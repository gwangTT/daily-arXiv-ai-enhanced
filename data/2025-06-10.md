<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 76]
- [cs.AR](#cs.AR) [Total: 11]
- [cs.CL](#cs.CL) [Total: 134]
- [cs.CV](#cs.CV) [Total: 192]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 196]
- [cs.NE](#cs.NE) [Total: 10]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 50]
- [cs.SE](#cs.SE) [Total: 16]
- [q-bio.NC](#q-bio.NC) [Total: 5]
- [stat.ML](#stat.ML) [Total: 13]
- [cs.LO](#cs.LO) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.CY](#cs.CY) [Total: 11]
- [stat.CO](#stat.CO) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 15]
- [eess.IV](#eess.IV) [Total: 9]
- [eess.SP](#eess.SP) [Total: 21]
- [cs.CC](#cs.CC) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [cs.HC](#cs.HC) [Total: 4]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.MS](#cs.MS) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 3]
- [cs.CR](#cs.CR) [Total: 19]
- [hep-ph](#hep-ph) [Total: 1]
- [econ.GN](#econ.GN) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 4]
- [cs.GR](#cs.GR) [Total: 9]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.SY](#eess.SY) [Total: 7]
- [quant-ph](#quant-ph) [Total: 6]
- [cs.CE](#cs.CE) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach](https://arxiv.org/abs/2506.06282)
*Shuangyan Deng,Haizhou Peng,Jiachen Xu,Chunhou Liu,Ciprian Doru Giurcuaneanu,Jiamou Liu*

Main category: cs.AI

TL;DR: This paper introduces a benchmark for evaluating AI reasoning in finance, focusing on textual and visual data. It also proposes an error-aware learning approach to improve inference.


<details>
  <summary>Details</summary>
Motivation: To address the growing need for AI systems to effectively reason with complex financial data that includes text and visuals.

Method: The creation of a 3,200-question benchmark covering textual and visual finance topics and proposing an error-aware learning framework using feedback from historical mistakes.

Result: Findings show multimodal inputs boost AI performance, with error feedback increasing reasoning accuracy, but challenges remain in visual and logical processing.

Conclusion: Multimodal inputs and self-reflective reasoning strengthen AI's ability in financial contexts, but visual understanding and mathematical logic require further attention.

Abstract: Effective financial reasoning demands not only textual understanding but also
the ability to interpret complex visual data such as charts, tables, and trend
graphs. This paper introduces a new benchmark designed to evaluate how well AI
models - especially large language and multimodal models - reason in
finance-specific contexts. Covering 3,200 expert-level question-answer pairs
across 15 core financial topics, the benchmark integrates both textual and
visual modalities to reflect authentic analytical challenges in finance. To
address limitations in current reasoning approaches, we propose an error-aware
learning framework that leverages historical model mistakes and feedback to
guide inference, without requiring fine-tuning. Our experiments across
state-of-the-art models show that multimodal inputs significantly enhance
performance and that incorporating error feedback leads to consistent and
measurable improvements. The results highlight persistent challenges in visual
understanding and mathematical logic, while also demonstrating the promise of
self-reflective reasoning in financial AI systems. Our code and data can be
found at https://anonymous/FinMR/CodeData.

</details>


### [2] [Unreal Patterns](https://arxiv.org/abs/2506.06284)
*John Beverley,Jim Logan*

Main category: cs.AI

TL;DR: The paper proposes a practical ontology framework for dealing with non-existent entities, avoiding traditional metaphysical complexities while ensuring computational feasibility.


<details>
  <summary>Details</summary>
Motivation: The study aims to address inadequacies in traditional methods (e.g., dummy instances, modal logic) that either complicate metaphysical assumptions or create computational inefficiencies when representing entities that do not or may never exist.

Method: It introduces a framework within the Basic Formal Ontology, utilizing intersections of actual types instead of specific non-existent tokens, emphasizing realist and implementable solutions.

Result: The proposed approach offers a structured, ontology-driven method for representing non-existent or hypothetical entities while being computationally efficient.

Conclusion: The framework strikes a balance between philosophical rigor and practicality, resolving inefficiencies in current methodologies to better handle non-existent entities in computational and ontological applications.

Abstract: This paper introduces a framework for representing information about entities
that do not exist or may never exist, such as those involving fictional
entities, blueprints, simulations, and future scenarios. Traditional approaches
that introduce "dummy instances" or rely on modal logic are criticized, and a
proposal is defended in which such cases are modeled using the intersections of
actual types rather than specific non existent tokens. The paper positions
itself within the Basic Formal Ontology and its realist commitments,
emphasizing the importance of practical, implementable solutions over purely
metaphysical or philosophical proposals, arguing that existing approaches to
non existent entities either overcommit to metaphysical assumptions or
introduce computational inefficiencies that hinder applications. By developing
a structured ontology driven approach to unreal patterns, the paper aims to
provide a useful and computationally viable means of handling references to
hypothetical or non existent entities.

</details>


### [3] [NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting](https://arxiv.org/abs/2506.06285)
*Kaike Sa Teles Rocha Alves,Eduardo Pestana de Aguiar*

Main category: cs.AI

TL;DR: A Python library, evolvingfuzzysystems, is introduced to provide implementations of various Evolving Fuzzy Systems (eFS) models, addressing their lack of accessibility.


<details>
  <summary>Details</summary>
Motivation: Evolving Fuzzy Systems are critical for adaptively updating structures to data dynamics, yet lack accessible implementations, hindering adoption.

Method: The authors developed evolvingfuzzysystems, a Python library with several eFS models, offering tools for training, visualization, evaluation, and computational complexity analysis.

Result: The library's models demonstrate performance on the fetch_california_housing dataset using metrics like NRMSE, NDEI, and MAPE. Computational complexity is assessed by execution times and rule evolution.

Conclusion: The library enhances access to eFS models, with ePL identified as a simple yet efficient option for balancing accuracy and cost, driving applications in adaptive machine learning.

Abstract: Evolving Fuzzy Systems (eFS) have gained significant attention due to their
ability to adaptively update their structure in response to data dynamics while
maintaining interpretability. However, the lack of publicly available
implementations of these models limits their accessibility and widespread
adoption. To address this gap, we present evolvingfuzzysystems, a Python
library that provides implementations of several well-established eFS models,
including ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl\_eTS, and eTS. The
library facilitates model evaluation and comparison by offering built-in tools
for training, visualization, and performance assessment. The models are
evaluated using the fetch\_california\_housing dataset, with performance
measured in terms of normalized root-mean-square error (NRMSE), non-dimensional
error index (NDEI), and mean absolute percentage error (MAPE). Additionally,
computational complexity is analyzed by measuring execution times and rule
evolution during training and testing phases. The results highlight ePL as a
simple yet efficient model that balances accuracy and computational cost,
making it particularly suitable for real-world applications. By making these
models publicly available, evolvingfuzzysystems aims to foster research and
practical applications in adaptive and interpretable machine learning.

</details>


### [4] [Deep Research Bench: Evaluating AI Web Research Agents](https://arxiv.org/abs/2506.06287)
*FutureSearch,:,Nikos I. Bosse,Jon Evans,Robert G. Gambee,Daniel Hnyk,Peter Mühlbacher,Lawrence Phillips,Dan Schwarz,Jack Wildman*

Main category: cs.AI

TL;DR: Deep Research Bench introduces a framework for evaluating AI-driven web research agents using static web data and robust tool benchmarks.


<details>
  <summary>Details</summary>
Motivation: There are no existing reliable evaluations for AI web research agents that account for the constantly changing nature of the web.

Method: The authors created a benchmark called Deep Research Bench, which includes 89 web research tasks and a RetroSearch environment with a frozen web page dataset for consistent model evaluation.

Result: The study found that offline RetroSearch agents performed on par with live web agents, and a public leaderboard was established to track improvements in AI agents' capabilities.

Conclusion: Deep Research Bench provides a standardized, reliable framework for benchmarking AI web research agents, allowing for robust comparisons over time.

Abstract: Amongst the most common use cases of modern AI is LLM chat with web search
enabled. However, no direct evaluations of the quality of web research agents
exist that control for the continually-changing web. We introduce Deep Research
Bench, consisting of 89 multi-step web research task instances of varying
difficulty across 8 diverse task categories, with the answers carefully worked
out by skilled humans. We provide a "RetroSearch" environment with a large
frozen set of scraped web pages, and demonstrate that offline "RetroSearch"
agents perform comparably to "live web" agents, enabling reliable evaluations
of models over time. We provide robust agent tooling and scaffolding to
benchmark major LLMs as they are released, including "thinking" models like o3
and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent
traces to report progress over time in hallucinations, tool use, and
forgetting. Finally, we evaluate the major web research products branded as
"Deep Research", "Deep Search", "Search", or "Research." Results are available
on a public leaderboard at https://drb.futuresearch.ai/.

</details>


### [5] [Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review](https://arxiv.org/abs/2506.06301)
*Muhammad Monjurul Karim,Yan Shi,Shucheng Zhang,Bingzhang Wang,Mehrdad Nasri,Yinhai Wang*

Main category: cs.AI

TL;DR: This paper reviews how Large Language Models (LLMs) can improve roadway safety and mobility, detailing their applications, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and dynamism of modern traffic systems, which traditional engineering methods struggle to fully manage.

Method: A comprehensive review focusing on how LLMs are adapted through various strategies to handle transportation data, analyzing their applications, enabling technologies, and challenges.

Result: Identifies diverse LLM applications in transportation, alongside challenges such as hallucinations, data bias, real-world deployment issues, and safety assurance.

Conclusion: LLMs have transformative potential to enhance transportation but require responsible innovation and research to address limitations and build safe, efficient systems.

Abstract: Roadway safety and mobility remain critical challenges for modern
transportation systems, demanding innovative analytical frameworks capable of
addressing complex, dynamic, and heterogeneous environments. While traditional
engineering methods have made progress, the complexity and dynamism of
real-world traffic necessitate more advanced analytical frameworks. Large
Language Models (LLMs), with their unprecedented capabilities in natural
language understanding, knowledge integration, and reasoning, represent a
promising paradigm shift. This paper comprehensively reviews the application
and customization of LLMs for enhancing roadway safety and mobility. A key
focus is how LLMs are adapted -- via architectural, training, prompting, and
multimodal strategies -- to bridge the "modality gap" with transportation's
unique spatio-temporal and physical data. The review systematically analyzes
diverse LLM applications in mobility (e.g., traffic flow prediction, signal
control) and safety (e.g., crash analysis, driver behavior assessment,).
Enabling technologies such as V2X integration, domain-specific foundation
models, explainability frameworks, and edge computing are also examined.
Despite significant potential, challenges persist regarding inherent LLM
limitations (hallucinations, reasoning deficits), data governance (privacy,
bias), deployment complexities (sim-to-real, latency), and rigorous safety
assurance. Promising future research directions are highlighted, including
advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI
collaboration, continuous learning, and the development of efficient,
verifiable systems. This review provides a structured roadmap of current
capabilities, limitations, and opportunities, underscoring LLMs' transformative
potential while emphasizing the need for responsible innovation to realize
safer, more intelligent transportation systems.

</details>


### [6] [Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review](https://arxiv.org/abs/2506.06324)
*Shruti Kumar,Xiaoyu Chen,Xiaomei Wang*

Main category: cs.AI

TL;DR: This scoping review investigates the terminology inconsistencies, task domains, and cognitive frameworks related to human-AI-robot co-learning and co-adaptation.


<details>
  <summary>Details</summary>
Motivation: To address inconsistencies in terminology, understand task domains, and explore cognitive frameworks, enhancing clarity and direction for human-AI-robot collaboration research.

Method: A scoping review to gather and analyze existing literature on terminology, task domains, and cognitive theories in human-AI interactions.

Result: Key findings provide an overview of terminology usage, task domain variation, and applied cognitive frameworks in human-agent co-learning and co-adaptation research.

Conclusion: The study highlights inconsistencies in terminology, diversity in task domains, and theoretical underpinnings, offering a basis for more focused future research.

Abstract: Several papers have delved into the challenges of human-AI-robot co-learning
and co-adaptation. It has been noted that the terminology used to describe this
collaborative relationship in existing studies needs to be more consistent. For
example, the prefix "co" is used interchangeably to represent both
"collaborative" and "mutual," and the terms "co-learning" and "co-adaptation"
are sometimes used interchangeably. However, they can reflect subtle
differences in the focus of the studies. The current scoping review's primary
research question (RQ1) aims to gather existing papers discussing this
collaboration pattern and examine the terms researchers use to describe this
human-agent relationship. Given the relative newness of this area of study, we
are also keen on exploring the specific types of intelligent agents and task
domains that have been considered in existing research (RQ2). This exploration
is significant as it can shed light on the diversity of human-agent
interactions, from one-time to continuous learning/adaptation scenarios. It can
also help us understand the dynamics of human-agent interactions in different
task domains, guiding our expectations towards research situated in dynamic,
complex domains. Our third objective (RQ3) is to investigate the cognitive
theories and frameworks that have been utilized in existing studies to measure
human-agent co-learning and co-adaptation. This investigation is crucial as it
can help us understand the theoretical underpinnings of human-agent
collaboration and adaptation, and it can also guide us in identifying any new
frameworks proposed specifically for this type of relationship.

</details>


### [7] [Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules](https://arxiv.org/abs/2506.06750)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.AI

TL;DR: This paper examines how different learning algorithms, including bioinspired methods, impact the performance of Spiking Neural Networks (SNN) in classification tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in training SNNs due to their unique temporal dynamics and event-driven activations, and explores learning algorithms for optimized classification.

Method: The authors propose a bioinspired classifier combining SNNs with Lempel-Ziv complexity (LZC) and compare biologically inspired algorithms like tempotron and Spikprop against traditional backpropagation.

Result: Backpropagation achieves high classification accuracy but is computationally expensive. Biologically inspired methods balance efficiency and competitive performance, making them suitable for real-time applications.

Conclusion: Choosing the best learning algorithm for SNNs depends on balancing classification accuracy, computational efficiency, and application-specific constraints.

Abstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique
properties, including temporal dynamics, non-differentiability of spike events,
and sparse event-driven activations. In this paper, we widely consider the
influence of the type of chosen learning algorithm, including bioinspired
learning rules on the accuracy of classification. We proposed a bioinspired
classifier based on the combination of SNN and Lempel-Ziv complexity (LZC).
This approach synergizes the strengths of SNNs in temporal precision and
biological realism with LZC's structural complexity analysis, facilitating
efficient and interpretable classification of spatiotemporal neural data. It
turned out that the classic backpropagation algorithm achieves excellent
classification accuracy, but at extremely high computational cost, which makes
it impractical for real-time applications. Biologically inspired learning
algorithms such as tempotron and Spikprop provide increased computational
efficiency while maintaining competitive classification performance, making
them suitable for time-sensitive tasks. The results obtained indicate that the
selection of the most appropriate learning algorithm depends on the trade-off
between classification accuracy and computational cost as well as application
constraints.

</details>


### [8] [Memory OS of AI Agent](https://arxiv.org/abs/2506.06326)
*Jiazheng Kang,Mingming Ji,Zhe Zhao,Ting Bai*

Main category: cs.AI

TL;DR: The paper introduces MemoryOS, a Memory Operating System for better memory management in AI agents by implementing a hierarchical memory structure.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fixed context windows and insufficient memory management in LLMs, which impede long-term memory capabilities and personalization.

Method: The authors designed MemoryOS with a hierarchical memory architecture (short-term, mid-term, and long-term personal memory) and dynamic update mechanisms, such as FIFO principles and segmented page organization.

Result: MemoryOS demonstrated significant enhancements in contextual coherence and personalized memory retention, achieving a 49.11% improvement in F1 and 46.18% in BLEU-1 scores in LoCoMo benchmark tests.

Conclusion: MemoryOS effectively tackles LLM memory challenges by enabling efficient and dynamic hierarchical memory management for long conversations and personalized experiences.

Abstract: Large Language Models (LLMs) face a crucial challenge from fixed context
windows and inadequate memory management, leading to a severe shortage of
long-term memory capabilities and limited personalization in the interactive
experience with AI agents. To overcome this challenge, we innovatively propose
a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and
efficient memory management for AI agents. Inspired by the memory management
principles in operating systems, MemoryOS designs a hierarchical storage
architecture and consists of four key modules: Memory Storage, Updating,
Retrieval, and Generation. Specifically, the architecture comprises three
levels of storage units: short-term memory, mid-term memory, and long-term
personal memory. Key operations within MemoryOS include dynamic updates between
storage units: short-term to mid-term updates follow a dialogue-chain-based
FIFO principle, while mid-term to long-term updates use a segmented page
organization strategy. Our pioneering MemoryOS enables hierarchical memory
integration and dynamic updating. Extensive experiments on the LoCoMo benchmark
show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the
baselines on GPT-4o-mini, showing contextual coherence and personalized memory
retention in long conversations. The implementation code is open-sourced at
https://github.com/BAI-LAB/MemoryOS.

</details>


### [9] [Will artificial agents pursue power by default?](https://arxiv.org/abs/2506.06352)
*Christian Tarsney*

Main category: cs.AI

TL;DR: The paper examines the concept of power-seeking behavior in AI systems and evaluates whether power is a universally useful instrumental goal. It concludes that while power-seeking has some validity, its predictive utility depends on the agent's final goals and circumstances.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address concerns about how sufficiently capable AI agents might pursue power over humanity, given claims that power-seeking is a convergent instrumental goal across varying objectives.

Method: The author uses an abstract decision-theoretic framework to formalize and evaluate the concepts of instrumental convergence and power-seeking.

Result: The paper finds that while the idea of power as a convergent instrumental goal holds some truth, its predictive utility is limited without detailed knowledge of the agent's final goals.

Conclusion: Instrumental convergence may predict behavior in AI agents aiming for near-absolute power, but generalizing it across all AI systems lacks robust predictive value.

Abstract: Researchers worried about catastrophic risks from advanced AI have argued
that we should expect sufficiently capable AI agents to pursue power over
humanity because power is a convergent instrumental goal, something that is
useful for a wide range of final goals. Others have recently expressed
skepticism of these claims. This paper aims to formalize the concepts of
instrumental convergence and power-seeking in an abstract, decision-theoretic
framework, and to assess the claim that power is a convergent instrumental
goal. I conclude that this claim contains at least an element of truth, but
might turn out to have limited predictive utility, since an agent's options
cannot always be ranked in terms of power in the absence of substantive
information about the agent's final goals. However, the fact of instrumental
convergence is more predictive for agents who have a good shot at attaining
absolute or near-absolute power.

</details>


### [10] [SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation](https://arxiv.org/abs/2506.06470)
*Yanwei Ren,Haotian Zhang,Fuxiang Wu,Jiayan Qiu,Jiaxing Huang,Baosheng Yu,Liu Liu*

Main category: cs.AI

TL;DR: The paper introduces SIGMA, a new approach to improve large language models (LLMs) by reintegrating discarded reasoning paths in Monte Carlo Tree Search (MCTS).


<details>
  <summary>Details</summary>
Motivation: Scaling up datasets for LLMs now shows diminishing returns, so improving data quality has become critical. Traditional use of MCTS discards non-optimal reasoning paths, which potentially wastes valuable data insights.

Method: The proposed SIGMA framework uses a critique and revision model to analyze and refine discarded sibling nodes from a search path in MCTS. This enables the utilization of overlooked insights to improve reasoning trajectories.

Result: Using SIGMA, a 7B LLM achieved 54.92% accuracy on the MATH benchmark with only 30K samples, outperforming models trained on 590K samples.

Conclusion: SIGMA effectively reduces data requirements while significantly improving LLM reasoning, demonstrating the value of utilizing discarded reasoning paths in MCTS.

Abstract: Enhancing large language models by simply scaling up datasets has begun to
yield diminishing returns, shifting the spotlight to data quality. Monte Carlo
Tree Search (MCTS) has emerged as a powerful technique for generating
high-quality chain-of-thought data, yet conventional approaches typically
retain only the top-scoring trajectory from the search tree, discarding sibling
nodes that often contain valuable partial insights, recurrent error patterns,
and alternative reasoning strategies. This unconditional rejection of
non-optimal reasoning branches may waste vast amounts of informative data in
the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo
Augmentation), a novel framework that reintegrates these discarded sibling
nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes
along each search path and applies a two-stage refinement: a critique model
identifies overlooked strengths and weaknesses across the sibling set, and a
revision model conducts text-based backpropagation to refine the top-scoring
trajectory in light of this comparative feedback. By recovering and amplifying
the underutilized but valuable signals from non-optimal reasoning branches,
SIGMA substantially improves reasoning trajectories. On the challenging MATH
benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K
samples, outperforming state-of-the-art models trained on 590K samples. This
result highlights that our sibling-guided optimization not only significantly
reduces data usage but also significantly boosts LLM reasoning.

</details>


### [11] [Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility](https://arxiv.org/abs/2506.06523)
*Sumanth Pillella*

Main category: cs.AI

TL;DR: The paper presents a reinforcement learning framework for optimizing warehouse operations in SAP Logistics Execution (LE), achieving 95% accuracy in task optimization and reducing processing times by 60%.


<details>
  <summary>Details</summary>
Motivation: The growing demands in supply chain management require advanced solutions to enhance operational efficiency and agility, particularly in warehouses.

Method: A reinforcement learning-based framework models warehouse processes as dynamic environments using a synthetic dataset of 300,000 LE transactions to optimize various tasks in real time.

Result: The proposed framework achieves a 95% task optimization accuracy and reduces processing times by 60%. Visual tools like heatmaps and graphs aid in strategy planning.

Conclusion: The research demonstrates a scalable, integrative, and privacy-conscious RL-driven solution that significantly improves operational efficiency in SAP LE warehouse management.

Abstract: In an era of escalating supply chain demands, SAP Logistics Execution (LE) is
pivotal for managing warehouse operations, transportation, and delivery. This
research introduces a pioneering framework leveraging reinforcement learning
(RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing
operational agility and efficiency. By modeling warehouse processes as dynamic
environments, the framework optimizes task allocation, inventory movement, and
order picking in real-time. A synthetic dataset of 300,000 LE transactions
simulates real-world warehouse scenarios, including multilingual data and
operational disruptions. The analysis achieves 95% task optimization accuracy,
reducing processing times by 60% compared to traditional methods.
Visualizations, including efficiency heatmaps and performance graphs, guide
agile warehouse strategies. This approach tackles data privacy, scalability,
and SAP integration, offering a transformative solution for modern supply
chains.

</details>


### [12] [ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search](https://arxiv.org/abs/2506.06524)
*Sam Earle,Ahmed Khalifa,Muhammad Umair Nasir,Zehua Jiang,Graham Todd,Andrzej Banburski-Fahey,Julian Togelius*

Main category: cs.AI

TL;DR: The paper introduces ScriptDoctor, a system that uses Large Language Models (LLMs) to automatically generate and test puzzle games in PuzzleScript, showcasing automated workflows in game design.


<details>
  <summary>Details</summary>
Motivation: To explore how large pre-trained models can be utilized for automatic game design (AGD) with minimal human supervision and integrated into long-term AGD pipelines.

Method: ScriptDoctor employs an iterative loop where it uses human-authored examples, resolves compilation errors, and employs search-based agents to play-test the generated games in PuzzleScript.

Result: ScriptDoctor successfully automates the generation and testing of novel puzzle games, demonstrating the feasibility of integrating LLMs into autonomous game design pipelines.

Conclusion: The system highlights the potential of LLM-based workflows in advancing open-ended automated game design and content creation.

Abstract: There is much interest in using large pre-trained models in Automatic Game
Design (AGD), whether via the generation of code, assets, or more abstract
conceptualization of design ideas. But so far this interest largely stems from
the ad hoc use of such generative models under persistent human supervision.
Much work remains to show how these tools can be integrated into
longer-time-horizon AGD pipelines, in which systems interface with game engines
to test generated content autonomously. To this end, we introduce ScriptDoctor,
a Large Language Model (LLM)-driven system for automatically generating and
testing games in PuzzleScript, an expressive but highly constrained description
language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates
and tests game design ideas in an iterative loop, where human-authored examples
are used to ground the system's output, compilation errors from the
PuzzleScript engine are used to elicit functional code, and search-based agents
play-test generated games. ScriptDoctor serves as a concrete example of the
potential of automated, open-ended LLM-based workflows in generating novel game
content.

</details>


### [13] [The Optimization Paradox in Clinical AI Multi-Agent Systems](https://arxiv.org/abs/2506.06574)
*Suhana Bedi,Iddah Mlauzi,Daniel Shin,Sanmi Koyejo,Nigam H. Shah*

Main category: cs.AI

TL;DR: This study examined multi-agent vs. single-agent AI systems in diagnosing four abdominal pathologies. It found that while multi-agent systems often outperformed single-agent systems, optimizing components alone can harm diagnostic accuracy if information flow and compatibility between components are neglected.


<details>
  <summary>Details</summary>
Motivation: To explore the yet unclear relationship between component-wise optimization and overall system performance in multi-agent AI systems used for clinical diagnosis.

Method: The study analyzed 2,400 real patient cases from the MIMIC-CDM dataset, comparing single-agent systems (one model for all tasks) with multi-agent systems (specialized models for each task). The evaluation considered diagnostic outcomes, process adherence, and cost efficiency.

Result: Multi-agent systems generally outperformed single-agent systems, but a component-optimized multi-agent system, which excelled in process metrics (85.5% information accuracy), underperformed in overall diagnostic accuracy (67.7% compared to 77.4% for the top-performing system).

Conclusion: Effective healthcare AI integration requires holistic validation of end-to-end systems rather than relying solely on component optimization or isolated metrics. Attention to information flow and agent compatibility is crucial.

Abstract: Multi-agent artificial intelligence systems are increasingly deployed in
clinical settings, yet the relationship between component-level optimization
and system-wide performance remains poorly understood. We evaluated this
relationship using 2,400 real patient cases from the MIMIC-CDM dataset across
four abdominal pathologies (appendicitis, pancreatitis, cholecystitis,
diverticulitis), decomposing clinical diagnosis into information gathering,
interpretation, and differential diagnosis. We evaluated single agent systems
(one model performing all tasks) against multi-agent systems (specialized
models for each task) using comprehensive metrics spanning diagnostic outcomes,
process adherence, and cost efficiency. Our results reveal a paradox: while
multi-agent systems generally outperformed single agents, the
component-optimized or Best of Breed system with superior components and
excellent process metrics (85.5% information accuracy) significantly
underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent
system). This finding underscores that successful integration of AI in
healthcare requires not just component level optimization but also attention to
information flow and compatibility between agents. Our findings highlight the
need for end to end system validation rather than relying on component metrics
alone.

</details>


### [14] [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
*Clément Hongler,Andrew Emil*

Main category: cs.AI

TL;DR: The paper introduces Cross-Entropy (Xent) Games, leveraging probability measures defined by Large Language Models (LLMs) for various tasks, and aims to benchmark LLM capabilities through these games.


<details>
  <summary>Details</summary>
Motivation: To develop new methods to evaluate and utilize the implicit probabilistic knowledge of Large Language Models by exploring tasks beyond generative sampling.

Method: The paper formulates Xent Games, which are computational structures using cross-entropy scores and constraints. These games are built from game-theoretic axioms and are designed to evaluate LLMs' abilities across various tasks.

Result: The authors demonstrate that the Xent Game space is vast, containing practical examples, and propose a framework to benchmark LLM capabilities using finite families of Xent Games tied to specific scopes.

Conclusion: By exploring Xent Games in an evolutionary manner, the paper outlines a systematic approach to address the challenges of unbounded scope in benchmarking LLM general abilities.

Abstract: Large Language Models (LLMs) define probability measures on text. By
considering the implicit knowledge question of what it means for an LLM to know
such a measure and what it entails algorithmically, we are naturally led to
formulate a series of tasks that go beyond generative sampling, involving forms
of summarization, counterfactual thinking, anomaly detection, originality
search, reverse prompting, debating, creative solving, etc. These tasks can be
formulated as games based on LLM measures, which we call Cross-Entropy (Xent)
Games. Xent Games can be single-player or multi-player. They involve
cross-entropy scores and cross-entropy constraints, and can be expressed as
simple computational graphs and programs. We show the Xent Game space is large
enough to contain a wealth of interesting examples, while being constructible
from basic game-theoretic consistency axioms. We then discuss how the Xent Game
space can be used to measure the abilities of LLMs. This leads to the
construction of Xent Game measures: finite families of Xent Games that can be
used as capability benchmarks, built from a given scope, by extracting a
covering measure. To address the unbounded scope problem associated with the
challenge of measuring general abilities, we propose to explore the space of
Xent Games in a coherent fashion, using ideas inspired by evolutionary
dynamics.

</details>


### [15] [AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture](https://arxiv.org/abs/2506.06580)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: This paper surveys the role of digital twins in AI simulation, identifying trends, proposing a reference framework, and highlighting future challenges.


<details>
  <summary>Details</summary>
Motivation: To address challenges in AI related to insufficient data through virtual training environments and digital twins, enabling safe, efficient, and high-quality data generation.

Method: A systematic survey of 22 primary studies to analyze trends, derive a reference framework, and align it with ISO 23247 standards.

Result: Technological trends and a comprehensive reference framework were identified, along with architectural guidelines for integrating digital twins with AI.

Conclusion: Digital twin-enabled AI simulation offers significant potential, but further research is needed to address challenges and leverage opportunities.

Abstract: Insufficient data volume and quality are particularly pressing challenges in
the adoption of modern subsymbolic AI. To alleviate these challenges, AI
simulation uses virtual training environments in which AI agents can be safely
and efficiently developed with simulated, synthetic data. Digital twins open
new avenues in AI simulation, as these high-fidelity virtual replicas of
physical systems are equipped with state-of-the-art simulators and the ability
to further interact with the physical system for additional data collection. In
this article, we report on our systematic survey of digital twin-enabled AI
simulation. By analyzing 22 primary studies, we identify technological trends
and derive a reference framework to situate digital twins and AI components.
Based on our findings, we derive a reference framework and provide
architectural guidelines by mapping it onto the ISO 23247 reference
architecture for digital twins. Finally, we identify challenges and research
opportunities for prospective researchers.

</details>


### [16] [GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales](https://arxiv.org/abs/2506.06634)
*Yubin Xiao,Di Wang,Rui Cao,Xuan Wu,Boyang Li,You Zhou*

Main category: cs.AI

TL;DR: The paper introduces GELD, a neural network-based TSP solver that addresses scalability and efficiency issues in solving TSPs of various sizes, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current neural TSP solvers struggle with efficiency and scalability, especially in solving both small- and large-scale TSPs with the same set of parameters, limiting their real-world applicability.

Method: The proposed GELD model integrates a lightweight Global-view Encoder with a heavyweight Local-view Decoder and uses a low-complexity attention mechanism. A two-stage training strategy is employed to improve generalization across different TSP sizes.

Result: GELD outperforms seven state-of-the-art TSP solvers in both solution quality and inference speed. It also scales up to solve TSPs with up to 744,710 nodes without divide-and-conquer strategies.

Conclusion: GELD demonstrates significant advancements in TSP solving by enhancing model generalization, efficiency, and scalability, positioning itself as a superior tool in neural TSP solving.

Abstract: The Traveling Salesman Problem (TSP) is a well-known combinatorial
optimization problem with broad real-world applications. Recent advancements in
neural network-based TSP solvers have shown promising results. Nonetheless,
these models often struggle to efficiently solve both small- and large-scale
TSPs using the same set of pre-trained model parameters, limiting their
practical utility. To address this issue, we introduce a novel neural TSP
solver named GELD, built upon our proposed broad global assessment and refined
local selection framework. Specifically, GELD integrates a lightweight
Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich
embedding representation while accelerating the decision-making process.
Moreover, GE incorporates a novel low-complexity attention mechanism, allowing
GELD to achieve low inference latency and scalability to larger-scale TSPs.
Additionally, we propose a two-stage training strategy that utilizes training
instances of different sizes to bolster GELD's generalization ability.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate that GELD outperforms seven state-of-the-art models considering
both solution quality and inference speed. Furthermore, GELD can be employed as
a post-processing method to significantly elevate the quality of the solutions
derived by existing neural TSP solvers via spending affordable additional
computing time. Notably, GELD is shown as capable of solving TSPs with up to
744,710 nodes, first-of-its-kind to solve this large size TSP without relying
on divide-and-conquer strategies to the best of our knowledge.

</details>


### [17] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: The paper introduces Contextual Experience Replay (CER) to help large language model agents improve adaptively in decision-making tasks during inference time by synthesizing past experiences in a dynamic memory buffer, achieving significant results on WebArena benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLM agents struggle with complex sequential decision-making tasks due to a lack of environment-specific experiences and inability to continually learn during inference time.

Method: The proposed CER framework accumulates and synthesizes past experiences into a memory buffer to retrieve and enhance knowledge during inference, thereby improving adaptability and decision-making.

Result: CER achieved a 31.9% performance score on VisualWebArena and a 36.7% success rate on WebArena, showing a 51.0% relative improvement compared to the GPT-4o baseline.

Conclusion: CER demonstrates the potential for enabling LLMs to self-improve efficiently during task inference, enhancing their adaptability in complex task environments through memory-based augmentation.

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [18] [Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation](https://arxiv.org/abs/2506.06714)
*Hamied Nabizada,Tom Jeleniewski,Lasse Beers,Maximilian Weigand,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: This paper introduces a SysML profile enabling PDDL integration in system models, demonstrated through an aircraft manufacturing case.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between system modeling and AI planning for optimized engineering design.

Method: The authors created reusable SysML stereotypes for PDDL concepts with OCL constraints and applied them to generate PDDL descriptions usable in a solver.

Result: The case study showcased a robotic system model generating PDDL domain and problem descriptions for optimized execution plans.

Conclusion: The approach automates planning description generation, connecting system modeling seamlessly with AI planning applications.

Abstract: This paper presents a SysML profile that enables the direct integration of
planning semantics based on the Planning Domain Definition Language (PDDL) into
system models. Reusable stereotypes are defined for key PDDL concepts such as
types, predicates, functions and actions, while formal OCL constraints ensure
syntactic consistency. The profile was derived from the Backus-Naur Form (BNF)
definition of PDDL 3.1 to align with SysML modeling practices. A case study
from aircraft manufacturing demonstrates the application of the profile: a
robotic system with interchangeable end effectors is modeled and enriched to
generate both domain and problem descriptions in PDDL format. These are used as
input to a PDDL solver to derive optimized execution plans. The approach
supports automated and model-based generation of planning descriptions and
provides a reusable bridge between system modeling and AI planning in
engineering design.

</details>


### [19] [REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models](https://arxiv.org/abs/2506.07759)
*Diego Forniés-Tabuenca,Alejandro Uribe,Urtzi Otamendi,Arkaitz Artetxe,Juan Carlos Rivera,Oier Lopez de Lacalle*

Main category: cs.AI

TL;DR: This paper introduces REMoH, a novel combination of NSGA-II and Large Language Models (LLMs) for more adaptable and effective multi-objective optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of nonlinear structures and extensive problem-specific modeling in traditional algorithms for multi-objective optimization.

Method: The REMoH framework integrates NSGA-II with LLM-based heuristic generation, using a unique reflection mechanism to guide the creation of diverse heuristics.

Result: REMoH achieved competitive results on Flexible Job Shop Scheduling Problem datasets (Dauzere, Barnes, and Brandimarte) with reduced modeling effort and enhanced adaptability.

Conclusion: LLMs can significantly improve traditional optimization by offering flexibility, interpretability, and robustness, as demonstrated by REMoH.

Abstract: Multi-objective optimization is fundamental in complex decision-making tasks.
Traditional algorithms, while effective, often demand extensive
problem-specific modeling and struggle to adapt to nonlinear structures. Recent
advances in Large Language Models (LLMs) offer enhanced explainability,
adaptability, and reasoning. This work proposes Reflective Evolution of
Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with
LLM-based heuristic generation. A key innovation is a reflection mechanism that
uses clustering and search-space reflection to guide the creation of diverse,
high-quality heuristics, improving convergence and maintaining solution
diversity. The approach is evaluated on the Flexible Job Shop Scheduling
Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using
three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate
that REMoH achieves competitive results compared to state-of-the-art approaches
with reduced modeling effort and enhanced adaptability. These findings
underscore the potential of LLMs to augment traditional optimization, offering
greater flexibility, interpretability, and robustness in multi-objective
scenarios.

</details>


### [20] [WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725)
*Guillaume Levy,Cedric Colas,Pierre-Yves Oudeyer,Thomas Carta,Clement Romac*

Main category: cs.AI

TL;DR: WorldLLM is a framework that improves LLM predictions in domain-specific environments by combining Bayesian inference and reinforcement learning for autonomous exploration.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of LLMs in structured, domain-specific contexts by grounding their broad understanding in specific environments.

Method: Combine in-context learning, Bayesian inference, and curiosity-driven reinforcement learning to iteratively refine predictions and actively explore environments.

Result: WorldLLM improves predictive accuracy and generates interpretable theories in a textual game requiring object manipulation and combination.

Conclusion: WorldLLM successfully enhances predictive capabilities and interpretability of LLMs in structured environments using iterative refinement and active exploration.

Abstract: Large Language Models (LLMs) possess general world knowledge but often
struggle to generate precise predictions in structured, domain-specific
contexts such as simulations. These limitations arise from their inability to
ground their broad, unstructured understanding in specific environments. To
address this, we present WorldLLM, a framework that enhances LLM-based world
modeling by combining Bayesian inference and autonomous active exploration with
reinforcement learning. WorldLLM leverages the in-context learning abilities of
LLMs to guide an LLM-based world model's predictions using natural language
hypotheses given in its prompt. These hypotheses are iteratively refined
through a Bayesian inference framework that leverages a second LLM as the
proposal distribution given collected evidence. This evidence is collected
using a curiosity-driven reinforcement learning policy that explores the
environment to find transitions with a low log-likelihood under our LLM-based
predictive model using the current hypotheses. By alternating between refining
hypotheses and collecting new evidence, our framework autonomously drives
continual improvement of the predictions. Our experiments demonstrate the
effectiveness of WorldLLM in a textual game environment that requires agents to
manipulate and combine objects. The framework not only enhances predictive
accuracy, but also generates human-interpretable theories of environment
dynamics.

</details>


### [21] [VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs](https://arxiv.org/abs/2506.06727)
*Can Li,Ting Zhang,Mei Wang,Hua Huang*

Main category: cs.AI

TL;DR: VisioMath introduces a benchmark to evaluate Large Multimodal Models (LMMs) on complex mathematical reasoning involving image-based answer options, exposing limitations in current state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Current Large Multimodal Models (LMMs) struggle with mathematical reasoning tasks where answer choices are represented as images, which is vital for multi-image comprehension.

Method: VisioMath benchmark consists of 8,070 images and 1,800 multiple-choice questions with image-based answer options. Various state-of-the-art LMMs, including GPT-4o, are systematically evaluated to test their capabilities on this task.

Result: Even the best-performing models, such as GPT-4o, achieved only a 45.9% accuracy rate, indicating significant limitations in existing LMMs when solving visually similar image-based queries.

Conclusion: VisioMath highlights the shortcomings of current LMMs in image-based mathematical reasoning and establishes a foundational benchmark to drive innovation and improvements in the domain.

Abstract: Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving
capabilities across various domains. However, their ability to perform
mathematical reasoning when answer options are represented as images--an
essential aspect of multi-image comprehension--remains underexplored. To bridge
this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical
reasoning in multimodal contexts involving image-based answer choices.
VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where
each answer option is an image, presenting unique challenges to existing LMMs.
To the best of our knowledge, VisioMath is the first dataset specifically
tailored for mathematical reasoning in image-based-option scenarios, where
fine-grained distinctions between answer choices are critical for accurate
problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath
and find that even the most advanced models struggle with this task. Notably,
GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current
models in reasoning over visually similar answer choices. By addressing a
crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed
for future research, driving advancements in multimodal reasoning.

</details>


### [22] [Honey, I shrunk the hypothesis space (through logical preprocessing)](https://arxiv.org/abs/2506.06739)
*Andrew Cropper,Filipe Gouveia,David M. Cerna*

Main category: cs.AI

TL;DR: This paper introduces a method that reduces the hypothesis space before inductive logic programming (ILP) performs its search, speeding up learning processes without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Inductive logic programming (ILP) often faces challenges of large hypothesis spaces, which slow down learning and increase computational resources needed.

Method: The researchers use background knowledge to identify rules that cannot form part of an optimal hypothesis, thereby eliminating them. This is implemented via answer set programming to preprocess and shrink the hypothesis space for a constraint-based ILP system.

Result: Experiments demonstrate significant reductions in learning times across domains like visual reasoning and game playing. For example, preprocessing in 10 seconds can reduce learning times from over 10 hours to just 2 seconds while preserving predictive accuracy.

Conclusion: The approach effectively enhances the efficiency of ILP systems by reducing computational effort without compromising accuracy, making it practical for time-sensitive applications.

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. The
goal is to search a hypothesis space for a hypothesis that generalises training
examples and background knowledge. We introduce an approach that 'shrinks' the
hypothesis space before an ILP system searches it. Our approach uses background
knowledge to find rules that cannot be in an optimal hypothesis regardless of
the training examples. For instance, our approach discovers relationships such
as "even numbers cannot be odd" and "prime numbers greater than 2 are odd". It
then removes violating rules from the hypothesis space. We implement our
approach using answer set programming and use it to shrink the hypothesis space
of a constraint-based ILP system. Our experiments on multiple domains,
including visual reasoning and game playing, show that our approach can
substantially reduce learning times whilst maintaining predictive accuracies.
For instance, given just 10 seconds of preprocessing time, our approach can
reduce learning times from over 10 hours to only 2 seconds.

</details>


### [23] [AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method](https://arxiv.org/abs/2506.06740)
*Yigui Feng,Qinglin Wang,Ke Liu,Xinhai Chen,Bo Yang,Jie Liu*

Main category: cs.AI

TL;DR: AI PsyRoom introduces a simulation framework enhancing psychological counseling using multi-agent systems for empathetic conversations and personalized treatments, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in psychological counseling, such as rising mental health service demand and the shortage of trained professionals, aiming to leverage AI for better empathy and emotional support.

Method: The study utilizes a multi-agent framework called AI PsyRoom, which includes fine-grained emotion classification. PsyRoom A generates dialogues (dataset EmoPsy), while PsyRoom B provides personalized treatment plans.

Result: AI PsyRoom achieves measurable improvements: 18% in problem orientation, 23% in emotional expression, 24% in empathy, and 16% in interactive communication quality, validating its effectiveness.

Conclusion: AI PsyRoom emerges as a promising tool for advancing AI-assisted psychological counseling, with publicly available datasets and models fostering future research.

Abstract: Psychological counseling faces huge challenges due to the growing demand for
mental health services and the shortage of trained professionals. Large
language models (LLMs) have shown potential to assist psychological counseling,
especially in empathy and emotional support. However, existing models lack a
deep understanding of emotions and are unable to generate personalized
treatment plans based on fine-grained emotions. To address these shortcomings,
we present AI PsyRoom, a multi-agent simulation framework designed to enhance
psychological counseling by generating empathetic and emotionally nuanced
conversations. By leveraging fine-grained emotion classification and a
multi-agent framework, we construct a multi-agent PsyRoom A for dialogue
reconstruction, generating a high-quality dialogue dataset EmoPsy, which
contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.
We also propose PsyRoom B for generating personalized treatment plans.
Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms
state-of-the-art methods, achieving 18% improvement in problem orientation, 23%
in expression, 24% in Empathy, and 16% in interactive communication quality.
The datasets and models are publicly available, providing a foundation for
advancing AI-assisted psychological counseling research.

</details>


### [24] [Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data](https://arxiv.org/abs/2506.07390)
*Xin-Cheng Wen,Yijun Yang,Cuiyun Gao,Yang Xiao,Deheng Ye*

Main category: cs.AI

TL;DR: LLMs show potential in many coding tasks, but struggle with detecting software vulnerabilities due to insufficient reasoning data and lack of focus on patterns. This study introduces ReVD, a framework optimizing vulnerability-specific patterns and demonstrating significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional LLMs lack reasoning data and focus on semantic representations rather than recognizing vulnerability patterns, leading to their underperformance in detecting software vulnerabilities.

Method: ReVD synthesizes reasoning data through constructed reasoning processes and employs triplet supervised fine-tuning combined with curriculum online preference optimization.

Result: ReVD outperforms existing methods, with an accuracy improvement of 12.24%-22.77% in vulnerability detection using the PrimeVul and SVEN datasets.

Conclusion: ReVD framework marks a significant advancement in LLM-based software vulnerability detection, offering new benchmarks and improved methodologies for this application.

Abstract: Large language models (LLMs) demonstrate considerable proficiency in numerous
coding-related tasks; however, their capabilities in detecting software
vulnerabilities remain limited. This limitation primarily stems from two
factors: (1) the absence of reasoning data related to vulnerabilities, which
hinders the models' ability to capture underlying vulnerability patterns; and
(2) their focus on learning semantic representations rather than the reason
behind them, thus failing to recognize semantically similar vulnerability
samples. Furthermore, the development of LLMs specialized in vulnerability
detection is challenging, particularly in environments characterized by the
scarcity of high-quality datasets. In this paper, we propose a novel framework
ReVD that excels at mining vulnerability patterns through reasoning data
synthesizing and vulnerability-specific preference optimization. Specifically,
we construct forward and backward reasoning processes for vulnerability and
corresponding fixed code, ensuring the synthesis of high-quality reasoning
data. Moreover, we design the triplet supervised fine-tuning followed by
curriculum online preference optimization for enabling ReVD to better
understand vulnerability patterns. The extensive experiments conducted on
PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for
LLM-based software vulnerability detection, e.g., 12.24\%-22.77\% improvement
in the accuracy. The source code and data are available at
https://github.com/Xin-Cheng-Wen/PO4Vul.

</details>


### [25] [Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain](https://arxiv.org/abs/2506.06786)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: The paper introduces CA-MIQ, a reinforcement learning framework for better adaptation in high-stakes search-and-rescue missions, achieving significantly higher performance in mission adaptability compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous systems in search-and-rescue missions to adapt dynamically to changing priorities while gathering critical information.

Method: CA-MIQ integrates a dual-critic RL framework combining extrinsic task rewards with intrinsic critics for state-novelty, location-awareness, and priority alignment. It also leverages a shift detector to reset and adapt exploration strategies when priorities change.

Result: CA-MIQ demonstrated nearly four times higher mission success rates after a single priority change and over three times better performance in scenarios with multiple shifts, achieving 100% recovery in tested simulations.

Conclusion: CA-MIQ is highly effective for adapting to changing priorities in discrete environments, making it a valuable tool for SAR missions and similar applications.

Abstract: Autonomous systems operating in high-stakes search-and-rescue (SAR) missions
must continuously gather mission-critical information while flexibly adapting
to shifting operational priorities. We propose CA-MIQ (Context-Aware
Max-Information Q-learning), a lightweight dual-critic reinforcement learning
(RL) framework that dynamically adjusts its exploration strategy whenever
mission priorities change. CA-MIQ pairs a standard extrinsic critic for task
reward with an intrinsic critic that fuses state-novelty, information-location
awareness, and real-time priority alignment. A built-in shift detector triggers
transient exploration boosts and selective critic resets, allowing the agent to
re-focus after a priority revision. In a simulated SAR grid-world, where
experiments specifically test adaptation to changes in the priority order of
information types the agent is expected to focus on, CA-MIQ achieves nearly
four times higher mission-success rates than baselines after a single priority
shift and more than three times better performance in multiple-shift scenarios,
achieving 100% recovery while baseline methods fail to adapt. These results
highlight CA-MIQ's effectiveness in any discrete environment with
piecewise-stationary information-value distributions.

</details>


### [26] [United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory](https://arxiv.org/abs/2506.06843)
*HaoYang Shang,Xuan Liu,Zi Liang,Jie Zhang,Haibo Hu,Song Guo*

Main category: cs.AI

TL;DR: The paper identifies performance limitations in Large Language Models (LLMs) due to cognitive overload, introduces a multi-agent framework named CoThinker to address these issues, and validates its effectiveness in complex tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex tasks due to their limited ability to handle cognitive overload, akin to human cognitive limitations explained by Cognitive Load Theory.

Method: The paper proposes CoThinker, an LLM-based multi-agent framework that distributes cognitive load among agents by specializing tasks and structuring communication.

Result: CoThinker outperforms existing multi-agent baselines in solving complex tasks with improved solution quality and efficiency.

Conclusion: Leveraging Cognitive Load Theory, CoThinker showcases a promising methodology for managing cognitive demands and enhancing performance in complex problem-solving.

Abstract: Large Language Models (LLMs) exhibit a notable performance ceiling on
complex, multi-faceted tasks, as they often fail to integrate diverse
information or adhere to multiple constraints. We posit that such limitation
arises when the demands of a task exceed the LLM's effective cognitive load
capacity. This interpretation draws a strong analogy to Cognitive Load Theory
(CLT) in cognitive science, which explains similar performance boundaries in
the human mind, and is further supported by emerging evidence that reveals LLMs
have bounded working memory characteristics. Building upon this CLT-grounded
understanding, we introduce CoThinker, a novel LLM-based multi-agent framework
designed to mitigate cognitive overload and enhance collaborative
problem-solving abilities. CoThinker operationalizes CLT principles by
distributing intrinsic cognitive load through agent specialization and managing
transactional load via structured communication and a collective working
memory. We empirically validate CoThinker on complex problem-solving tasks and
fabricated high cognitive load scenarios, demonstrating improvements over
existing multi-agent baselines in solution quality and efficiency. Our analysis
reveals characteristic interaction patterns, providing insights into the
emergence of collective cognition and effective load management, thus offering
a principled approach to overcoming LLM performance ceilings.

</details>


### [27] [Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance](https://arxiv.org/abs/2506.06868)
*Razieh Arshadizadeh,Mahmoud Asgari,Zeinab Khosravi,Yiannis Papadopoulos,Koorosh Aslansefat*

Main category: cs.AI

TL;DR: The paper proposes a new probabilistic safety assurance framework that integrates SafeML with Bayesian Networks to address reasoning failures in ML models under distributional shifts, demonstrated on a simulated automotive platooning system.


<details>
  <summary>Details</summary>
Motivation: To address reasoning failures in ML-based safety-critical systems caused by distributional shifts that existing safety assessment methods cannot handle effectively.

Method: The paper introduces a framework that combines SafeML for detecting distributional shifts with Bayesian Networks to integrate ML failures into a causal safety analysis, enabling dynamic safety evaluations under uncertainty.

Result: The framework was applied to a simulated automotive platooning system with traffic sign recognition, and it demonstrated the benefit of explicitly modeling ML failures in safety assessments.

Conclusion: Explicitly modeling ML-induced failures, integrated with dynamic safety evaluations, enhances the reliability of safety-critical systems under uncertainty.

Abstract: Machine Learning (ML) models are increasingly integrated into safety-critical
systems, such as autonomous vehicle platooning, to enable real-time
decision-making. However, their inherent imperfection introduces a new class of
failure: reasoning failures often triggered by distributional shifts between
operational and training data. Traditional safety assessment methods, which
rely on design artefacts or code, are ill-suited for ML components that learn
behaviour from data. SafeML was recently proposed to dynamically detect such
shifts and assign confidence levels to the reasoning of ML-based components.
Building on this, we introduce a probabilistic safety assurance framework that
integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of
a broader causal safety analysis. This allows for dynamic safety evaluation and
system adaptation under uncertainty. We demonstrate the approach on an
simulated automotive platooning system with traffic sign recognition. The
findings highlight the potential broader benefits of explicitly modelling ML
failures in safety assessment.

</details>


### [28] [KnowCoder-V2: Deep Knowledge Analysis](https://arxiv.org/abs/2506.06881)
*Zixuan Li,Wenxuan Liu,Long Bai,Chunmao Zhang,Wei Li,Fenghui Zhang,Quanxin Jin,Ruoyun He,Zhuo Chen,Zhilei Hu,Fei Wang,Bingbing Xu,Xuhui Jiang,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: The paper introduces the KDR framework for deep knowledge analysis, addressing inefficiencies in knowledge organization, online operation, and computation via systematic offline preprocessing and advanced reasoning steps.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks for deep knowledge analysis struggle with systematic knowledge organization, inefficiency in tasks requiring large-scale shared knowledge, and inability to perform complex computations for insightful results.

Method: The proposed KDR framework incorporates offline knowledge organization and introduces reasoning steps for complex online computations. An LLM, \KCII, is designed to bridge knowledge organization and reasoning using code generation for processing and analyzing large-scale data.

Result: Experiments show \KCII's effectiveness across over thirty datasets in six knowledge analysis tasks. Integrated with the KDR framework, \KCII outperforms existing frameworks in generating quality analytical reports.

Conclusion: The KDR framework with \KCII enhances deep research capabilities by enabling efficient organization, computation, and reasoning, offering improved analytical insights and reporting compared to traditional methods.

Abstract: Deep knowledge analysis tasks always involve the systematic extraction and
association of knowledge from large volumes of data, followed by logical
reasoning to discover insights. However, to solve such complex tasks, existing
deep research frameworks face three major challenges: 1) They lack systematic
organization and management of knowledge; 2) They operate purely online, making
it inefficient for tasks that rely on shared and large-scale knowledge; 3) They
cannot perform complex knowledge computation, limiting their abilities to
produce insightful analytical results. Motivated by these, in this paper, we
propose a \textbf{K}nowledgeable \textbf{D}eep \textbf{R}esearch (\textbf{KDR})
framework that empowers deep research with deep knowledge analysis capability.
Specifically, it introduces an independent knowledge organization phase to
preprocess large-scale, domain-relevant data into systematic knowledge offline.
Based on this knowledge, it extends deep research with an additional kind of
reasoning steps that perform complex knowledge computation in an online manner.
To enhance the abilities of LLMs to solve knowledge analysis tasks in the above
framework, we further introduce \textbf{\KCII}, an LLM that bridges knowledge
organization and reasoning via unified code generation. For knowledge
organization, it generates instantiation code for predefined classes,
transforming data into knowledge objects. For knowledge computation, it
generates analysis code and executes on the above knowledge objects to obtain
deep analysis results. Experimental results on more than thirty datasets across
six knowledge analysis tasks demonstrate the effectiveness of \KCII. Moreover,
when integrated into the KDR framework, \KCII can generate high-quality reports
with insightful analytical results compared to the mainstream deep research
framework.

</details>


### [29] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: The paper introduces a meta-learning approach using soft prompts to improve performance in large multimodal models (LMMs), especially in low-data settings.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistency and non-monotonic performance of in-context learning (ICL) in LMMs, hypothesizing that excessive task-irrelevant information in image embeddings undermines performance.

Method: Proposed a meta-learning setup with a set of task-specific soft prompts distilled from image features, paired with an attention-mapper module integrated into the LLaVA v1.5 architecture. These prompts are adapted during few-shot learning with minimal gradient steps.

Result: The proposed method outperforms traditional ICL and related prompt-tuning approaches on the VL-ICL Bench, even under challenging conditions like image perturbations, improving task-specific induction and reasoning in visual question answering.

Conclusion: The method offers a reliable alternative to ICL in LMMs for visual tasks, demonstrating improved few-shot task adaptation and robustness in low-data environments.

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [30] [Causal Graph based Event Reasoning using Semantic Relation Experts](https://arxiv.org/abs/2506.06910)
*Mahnaz Koupaee,Xueying Bai,Mudan Chen,Greg Durrett,Nathanael Chambers,Niranjan Balasubramanian*

Main category: cs.AI

TL;DR: The paper introduces a method using generated causal event graphs to improve reasoning tasks for Large Language Models (LLMs), enhancing areas like event forecasting and timeline understanding.


<details>
  <summary>Details</summary>
Motivation: Despite advances in LLMs, accurately identifying causal connections between events for deeper reasoning tasks remains a challenge.

Method: The method employs simulated expert discussions among LLMs to collaboratively generate causal event graphs, later using these graphs to aid reasoning and explanations in tasks.

Result: The approach creates more coherent explanations for event predictions and performs competitively with state-of-the-art models in forecasting and next event prediction tasks.

Conclusion: Incorporating causal event graphs as a reasoning tool leads to better understanding and explanation of event chains without the need for task-specific fine-tuning, making it efficient and effective.

Abstract: Understanding how events in a scenario causally connect with each other is
important for effectively modeling and reasoning about events. But event
reasoning remains a difficult challenge, and despite recent advances, Large
Language Models (LLMs) still struggle to accurately identify causal connections
between events. This struggle leads to poor performance on deeper reasoning
tasks like event forecasting and timeline understanding. To address this
challenge, we investigate the generation of causal event graphs (e.g., A
enables B) as a parallel mechanism to help LLMs explicitly represent causality
during inference. This paper evaluates both how to generate correct graphs as
well as how graphs can assist reasoning. We propose a collaborative approach to
causal graph generation where we use LLMs to simulate experts that focus on
specific semantic relations. The experts engage in multiple rounds of
discussions which are then consolidated by a final expert. Then, to demonstrate
the utility of causal graphs, we use them on multiple downstream applications,
and also introduce a new explainable event prediction task that requires a
causal chain of events in the explanation. These explanations are more
informative and coherent than baseline generations. Finally, our overall
approach not finetuned on any downstream task, achieves competitive results
with state-of-the-art models on both forecasting and next event prediction
tasks.

</details>


### [31] [Boosting LLM Reasoning via Spontaneous Self-Correction](https://arxiv.org/abs/2506.06923)
*Xutong Zhao,Tengyu Xu,Xuewei Wang,Zhengxing Chen,Di Jin,Liang Tan,Yen-Ting,Zishun Yu,Zhuokai Zhao,Yun He,Sinong Wang,Han Fang,Sarath Chandar,Chen Zhu*

Main category: cs.AI

TL;DR: The paper introduces SPOC, a spontaneous self-correction approach for large language models (LLMs) to address challenges in math reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve LLMs' ability to perform mathematical reasoning by enabling dynamic self-corrections without requiring extra prompts or additional system designs.

Method: SPOC involves interleaving solution generation and verification in a single inference pass, assigning dual roles (solution proposer and verifier) to the model, and employing synthetic data for fine-tuning as well as reinforcement learning for further enhancement.

Result: SPOC significantly improves the performance of LLMs on mathematical reasoning benchmarks, with measurable accuracy gains across several datasets.

Conclusion: SPOC demonstrates the effectiveness of spontaneous self-correction in improving math reasoning, highlighting the benefits of dynamic correction mechanisms and multi-agent collaboration within a single model.

Abstract: While large language models (LLMs) have demonstrated remarkable success on a
broad range of tasks, math reasoning remains a challenging one. One of the
approaches for improving math reasoning is self-correction, which designs
self-improving loops to let the model correct its own mistakes. However,
existing self-correction approaches treat corrections as standalone
post-generation refinements, relying on extra prompt and system designs to
elicit self-corrections, instead of performing real-time, spontaneous
self-corrections in a single pass. To address this, we propose SPOC, a
spontaneous self-correction approach that enables LLMs to generate interleaved
solutions and verifications in a single inference pass, with generation
dynamically terminated based on verification outcomes, thereby effectively
scaling inference time compute. SPOC considers a multi-agent perspective by
assigning dual roles -- solution proposer and verifier -- to the same model. We
adopt a simple yet effective approach to generate synthetic data for
fine-tuning, enabling the model to develop capabilities for self-verification
and multi-agent collaboration. We further improve its solution proposal and
verification accuracy through online reinforcement learning. Experiments on
mathematical reasoning benchmarks show that SPOC significantly improves
performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct
models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23,
and 3.3% and 6.7% on AIME24, respectively.

</details>


### [32] [An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design](https://arxiv.org/abs/2506.06935)
*Darui Lu,Jordan M. Malof,Willie J. Padilla*

Main category: cs.AI

TL;DR: The paper introduces a framework where integrated Large Language Models autonomously design photonic metamaterials by developing models, accessing tools, and utilizing memory for reasoning and planning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage the autonomous capabilities of advanced Agentic Frameworks to perform complex scientific tasks, like inverse design of photonic metamaterials, without direct human intervention.

Method: The framework queries a desired spectrum, develops forward models via deep learning, utilizes external APIs for tools like simulation, employs memory for reasoning, and generates a design using inverse methods.

Result: The framework demonstrated its effectiveness by autonomously handling tasks like reasoning, planning, and adapting to achieve photonic metamaterial designs.

Conclusion: The Agentic Framework shows flexibility in decision-making, reflection, and adaptability, enabling highly novel solutions for scientific problems.

Abstract: Recent significant advances in integrating multiple Large Language Model
(LLM) systems have enabled Agentic Frameworks capable of performing complex
tasks autonomously, including novel scientific research. We develop and
demonstrate such a framework specifically for the inverse design of photonic
metamaterials. When queried with a desired optical spectrum, the Agent
autonomously proposes and develops a forward deep learning model, accesses
external tools via APIs for tasks like simulation and optimization, utilizes
memory, and generates a final design via a deep inverse method. The framework's
effectiveness is demonstrated in its ability to automate, reason, plan, and
adapt. Notably, the Agentic Framework possesses internal reflection and
decision flexibility, permitting highly varied and potentially novel outputs.

</details>


### [33] [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
*Parshin Shojaee,Iman Mirzadeh,Keivan Alizadeh,Maxwell Horton,Samy Bengio,Mehrdad Farajtabar*

Main category: cs.AI

TL;DR: Large Reasoning Models (LRMs) enhance reasoning tasks with detailed thinking processes but exhibit accuracy collapses at higher complexity levels. Evaluations highlight inherent limitations in reasoning consistency and scaling.


<details>
  <summary>Details</summary>
Motivation: To explore the reasoning capabilities and limitations of LRMs, which generate detailed thought processes, and address the gaps in current evaluations that focus only on answer accuracy without analyzing reasoning traces.

Method: The study utilized controllable puzzle environments to precisely manipulate task complexity and examined reasoning traces and final answers of LRMs, comparing their performance under equivalent compute regimes with standard LLM counterparts.

Result: Key findings reveal LRMs face accuracy collapse beyond certain task complexity, a counterintuitive scaling limit in reasoning effort, and identified three performance regimes across low-, medium-, and high-complexity tasks.

Conclusion: Though LRMs show promise in medium-complexity tasks, their inconsistent reasoning across complexities and inability to use explicit algorithms highlight limitations, raising questions about their reasoning capabilities.

Abstract: Recent generations of language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers.
While these models demonstrate improved performance on reasoning benchmarks,
their fundamental capabilities, scaling properties, and limitations remain
insufficiently understood. Current evaluations primarily focus on established
math and coding benchmarks, emphasizing final answer accuracy. However, this
evaluation paradigm often suffers from contamination and does not provide
insights into the reasoning traces. In this work, we systematically investigate
these gaps with the help of controllable puzzle environments that allow precise
manipulation of complexity while maintaining consistent logical structures.
This setup enables the analysis of not only final answers but also the internal
reasoning traces, offering insights into how LRMs think. Through extensive
experiments, we show that LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counterintuitive scaling limit: their
reasoning effort increases with problem complexity up to a point, then declines
despite having remaining token budget. By comparing LRMs with their standard
LLM counterparts under same inference compute, we identify three performance
regimes: (1) low-complexity tasks where standard models outperform LRMs, (2)
medium-complexity tasks where LRMs demonstrates advantage, and (3)
high-complexity tasks where both models face complete collapse. We found that
LRMs have limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across scales. We also investigate the
reasoning traces in more depth, studying the patterns of explored solutions and
analyzing the models' computational behavior, shedding light on their
strengths, limitations, and raising questions about their reasoning
capabilities.

</details>


### [34] [Deontically Constrained Policy Improvement in Reinforcement Learning Agents](https://arxiv.org/abs/2506.06959)
*Alena Makarova,Houssam Abbas*

Main category: cs.AI

TL;DR: This paper explores using Expected Act Utilitarianism and probabilistic stit logic in Markov Decision Processes (MDPs) to create RL policies that adhere to constraints while maximizing utility.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning decision policies that maximize utility while respecting ethical, social, or situational constraints in Reinforcement Learning.

Method: The paper integrates Expected Act Utilitarianism into MDPs and develops a variation on policy improvement to satisfy both utility and deontic constraints, achieving a constrained local maximum.

Result: The method successfully demonstrates policy improvements that align with both mission utility and constraint satisfaction in experiments using sample MDPs.

Conclusion: This approach enables agents to optimize both explicit mission goals and implicit ethical constraints using a structured bi-level value maximization framework for constrained decision making.

Abstract: Markov Decision Processes (MDPs) are the most common model for decision
making under uncertainty in the Machine Learning community. An MDP captures
non-determinism, probabilistic uncertainty, and an explicit model of action. A
Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a
utility function. This paper considers the problem of learning a decision
policy that maximizes utility subject to satisfying a constraint expressed in
deontic logic. In this setup, the utility captures the agent's mission - such
as going quickly from A to B. The deontic formula represents (ethical, social,
situational) constraints on how the agent might achieve its mission by
prohibiting classes of behaviors. We use the logic of Expected Act
Utilitarianism, a probabilistic stit logic that can be interpreted over
controlled MDPs. We develop a variation on policy improvement, and show that it
reaches a constrained local maximum of the mission utility. Given that in stit
logic, an agent's duty is derived from value maximization, this can be seen as
a way of acting to simultaneously maximize two value functions, one of which is
implicit, in a bi-level structure. We illustrate these results with experiments
on sample MDPs.

</details>


### [35] [Long-Tailed Learning for Generalized Category Discovery](https://arxiv.org/abs/2506.06965)
*Cuong Manh Hoang*

Main category: cs.AI

TL;DR: The paper presents a framework for Generalized Category Discovery (GCD) that tackles long-tailed distributions, using pseudo-labeling and representation balancing to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods struggle with imbalanced datasets, which are common in the real world, limiting their effectiveness.

Method: The framework introduces a self-guided labeling technique with learnable distribution for pseudo-labels and a representation balancing process that mines sample neighborhoods for better focus on tail classes.

Result: Experiments on public datasets demonstrate superior performance over state-of-the-art methods.

Conclusion: The proposed framework successfully addresses imbalanced distributions in GCD, showing significant performance improvements and enhanced effectiveness.

Abstract: Generalized Category Discovery (GCD) utilizes labeled samples of known
classes to discover novel classes in unlabeled samples. Existing methods show
effective performance on artificial datasets with balanced distributions.
However, real-world datasets are always imbalanced, significantly affecting the
effectiveness of these methods. To solve this problem, we propose a novel
framework that performs generalized category discovery in long-tailed
distributions. We first present a self-guided labeling technique that uses a
learnable distribution to generate pseudo-labels, resulting in less biased
classifiers. We then introduce a representation balancing process to derive
discriminative representations. By mining sample neighborhoods, this process
encourages the model to focus more on tail classes. We conduct experiments on
public datasets to demonstrate the effectiveness of the proposed framework. The
results show that our model exceeds previous state-of-the-art methods.

</details>


### [36] [Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments](https://arxiv.org/abs/2506.06981)
*Riley Simmons-Edler,Ryan P. Badman,Felix Baastad Berg,Raymond Chua,John J. Vastola,Joshua Lunger,William Qian,Kanaka Rajan*

Main category: cs.AI

TL;DR: This paper introduces ForageWorld, an environment inspired by animal foraging, to analyze deep reinforcement learning (DRL) agents using neuroethology-based techniques, revealing emergent planning-like behavior in model-free agents.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance behavioral analysis methods for DRL agents, especially as the agents and tasks become increasingly sophisticated, overcoming limitations of conventional reward curve comparisons.

Method: The authors employ neuroscience and ethology-inspired tools to analyze DRL agents in ForageWorld, a complex, partially observable environment, emphasizing joint behavioral and neural dynamics study.

Result: The study reveals that model-free RNN-DRL agents exhibit structured, planning-like behavior through emergent dynamics, negating the necessity for explicit memory modules or world models.

Conclusion: Applying neuroethology-inspired analysis uncovers hidden complexities in DRL agents' behaviors and neural dynamics, offering a framework to study cognitive structures in AI while highlighting implications for safe alignment and advanced capabilities.

Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents --
particularly as task and agent sophistication increase -- requires more than
simple comparison of reward curves, yet standard methods for behavioral
analysis remain underdeveloped in DRL. We apply tools from neuroscience and
ethology to study DRL agents in a novel, complex, partially observable
environment, ForageWorld, designed to capture key aspects of real-world animal
foraging -- including sparse, depleting resource patches, predator threats, and
spatially extended arenas. We use this environment as a platform for applying
joint behavioral and neural analysis to agents, revealing detailed,
quantitatively grounded insights into agent strategies, memory, and planning.
Contrary to common assumptions, we find that model-free RNN-based DRL agents
can exhibit structured, planning-like behavior purely through emergent dynamics
-- without requiring explicit memory modules or world models. Our results show
that studying DRL agents like animals -- analyzing them with
neuroethology-inspired tools that reveal structure in both behavior and neural
dynamics -- uncovers rich structure in their learning dynamics that would
otherwise remain invisible. We distill these tools into a general analysis
framework linking core behavioral and representational features to diagnostic
methods, which can be reused for a wide range of tasks and agents. As agents
grow more complex and autonomous, bridging neuroscience, cognitive science, and
AI will be essential -- not just for understanding their behavior, but for
ensuring safe alignment and maximizing desirable behaviors that are hard to
measure via reward. We show how this can be done by drawing on lessons from how
biological intelligence is studied.

</details>


### [37] [Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth](https://arxiv.org/abs/2506.06991)
*Yichi Zhang,Jinlong Pang,Zhaowei Zhu,Yang Liu*

Main category: cs.AI

TL;DR: The paper addresses the challenge of detecting LLM-generated responses in crowdsourcing tasks using peer prediction methods without ground truth reliance.


<details>
  <summary>Details</summary>
Motivation: The widespread use of generative AI by crowdsourcing workers creates a need for mechanisms to ensure the integrity and trustworthiness of human feedback in datasets.

Method: The study introduces a training-free scoring mechanism based on peer prediction, which evaluates correlations among worker responses and leverages available LLM-generated labels.

Result: The proposed method theoretically demonstrates its effectiveness in reducing LLM-assisted cheating, backed by empirical validation on real-world datasets.

Conclusion: Peer prediction approaches offer promising solutions to detect and mitigate low-effort LLM-generated cheating in crowdsourcing tasks, enhancing dataset reliability.

Abstract: The recent success of generative AI highlights the crucial role of
high-quality human feedback in building trustworthy AI systems. However, the
increasing use of large language models (LLMs) by crowdsourcing workers poses a
significant challenge: datasets intended to reflect human input may be
compromised by LLM-generated responses. Existing LLM detection approaches often
rely on high-dimension training data such as text, making them unsuitable for
annotation tasks like multiple-choice labeling. In this work, we investigate
the potential of peer prediction -- a mechanism that evaluates the information
within workers' responses without using ground truth -- to mitigate
LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our
approach quantifies the correlations between worker answers while conditioning
on (a subset of) LLM-generated labels available to the requester. Building on
prior research, we propose a training-free scoring mechanism with theoretical
guarantees under a crowdsourcing model that accounts for LLM collusion. We
establish conditions under which our method is effective and empirically
demonstrate its robustness in detecting low-effort cheating on real-world
crowdsourcing datasets.

</details>


### [38] [Mathesis: Towards Formal Theorem Proving from Natural Languages](https://arxiv.org/abs/2506.07047)
*Yu Xuejun,Jianyuan Zhong,Zijin Feng,Pengyi Zhai,Roozbeh Yousefzadeh,Wei Chong Ng,Haoxiong Liu,Ziyi Shou,Jing Xiong,Yudong Zhou,Claudia Beth Ong,Austen Jeremy Sugiarto,Yaoxi Zhang,Wai Ming Tai,Huan Cao,Dongcai Lu,Jiacheng Sun,Qiang Xu,Shen Xin,Zhenguo Li*

Main category: cs.AI

TL;DR: Mathesis introduces an end-to-end theorem proving system capable of handling informal problem statements, leveraging reinforcement learning for autoformalization and sophisticated proof generation techniques.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based theorem provers struggle with real-world applications due to reliance on expert-defined formal statements. The paper aims to bridge this gap by enabling formal reasoning from natural language inputs.

Method: Mathesis comprises two major components: Mathesis-Autoformalizer, which uses reinforcement learning and LeanScorer for formalization assessment, and Mathesis-Prover, capable of generating proofs from formalized statements.

Result: Mathesis's autoformalizer improves pass-rate by 22% on Gaokao-Formal. The pipeline achieves 64% accuracy on MiniF2F with pass@32 and sets a state-of-the-art 18% on the Gaokao-Formal benchmark.

Conclusion: Mathesis effectively demonstrates end-to-end formal theorem proving from informal statements, presenting a meaningful step forward in applying LLMs to complex, real-world reasoning tasks.

Abstract: Recent advances in large language models show strong promise for formal
reasoning. However, most LLM-based theorem provers have long been constrained
by the need for expert-written formal statements as inputs, limiting their
applicability to real-world problems expressed in natural language. We tackle
this gap with Mathesis, the first end-to-end theorem proving pipeline
processing informal problem statements. It contributes Mathesis-Autoformalizer,
the first autoformalizer using reinforcement learning to enhance the
formalization ability of natural language problems, aided by our novel
LeanScorer framework for nuanced formalization quality assessment. It also
proposes a Mathesis-Prover, which generates formal proofs from the formalized
statements. To evaluate the real-world applicability of end-to-end formal
theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex
problems from China's national college entrance exam. Our approach is carefully
designed, with a thorough study of each component. Experiments demonstrate
Mathesis's effectiveness, with the autoformalizer outperforming the best
baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other
model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a
state-of-the-art 18% on Gaokao-Formal.

</details>


### [39] [Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression](https://arxiv.org/abs/2506.07075)
*Liwen Zheng,Chaozhuo Li,Haoran Jia,Xi Zhang*

Main category: cs.AI

TL;DR: This paper introduces a structural reasoning framework for multi-hop fact verification to improve evidence aggregation and reasoning, outperforming strong baselines.


<details>
  <summary>Details</summary>
Motivation: Address the challenges posed by fragmented retrieval and limited interpretability in existing fact verification systems that fail to model evolving reasoning paths.

Method: The method includes two modules: structure-enhanced retrieval mechanism for building reasoning graphs, and reasoning-path-guided verification module for creating subgraphs to represent inference trajectories. It uses a structure-aware reasoning mechanism to capture long-range dependencies.

Result: Experiments on FEVER and HoVer datasets demonstrate improved retrieval precision and verification accuracy compared to strong baselines.

Conclusion: Reasoning-path modeling enhances both evidence collection and the accuracy of fact verification systems, making the proposed framework effective in real-world scenarios.

Abstract: The growing complexity of factual claims in real-world scenarios presents
significant challenges for automated fact verification systems, particularly in
accurately aggregating and reasoning over multi-hop evidence. Existing
approaches often rely on static or shallow models that fail to capture the
evolving structure of reasoning paths, leading to fragmented retrieval and
limited interpretability. To address these issues, we propose a Structural
Reasoning framework for Multi-hop Fact Verification that explicitly models
reasoning paths as structured graphs throughout both evidence retrieval and
claim verification stages. Our method comprises two key modules: a
structure-enhanced retrieval mechanism that constructs reasoning graphs to
guide evidence collection, and a reasoning-path-guided verification module that
incrementally builds subgraphs to represent evolving inference trajectories. We
further incorporate a structure-aware reasoning mechanism that captures
long-range dependencies across multi-hop evidence chains, enabling more precise
verification. Extensive experiments on the FEVER and HoVer datasets demonstrate
that our approach consistently outperforms strong baselines, highlighting the
effectiveness of reasoning-path modeling in enhancing retrieval precision and
verification accuracy.

</details>


### [40] [BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite](https://arxiv.org/abs/2506.07116)
*Liyang Chen,Yujun Cai,Jieqiong Dong,Yiwei Wang*

Main category: cs.AI

TL;DR: The paper introduces MARCUS, a multi-agent pipeline used to clean and segment the BRIGHT corpus into BRIGHT-Plus to improve its structure and semantic coherence for better performance in retrieval and reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The BRIGHT benchmark for multi-hop retrieval is affected by web-crawled artifacts like redundancy and semantic discontinuity, particularly in StackExchange-derived subdomains, which limit its effectiveness.

Method: The MARCUS pipeline utilizes large language models (LLMs) with dedicated agents to remove structural noise and improve semantic segmentation, while preserving key answer-bearing content to create the BRIGHT-Plus corpus.

Result: The BRIGHT-Plus corpus leads to significant and consistent improvements in retrieval accuracy and multi-hop reasoning across diverse retrieval systems.

Conclusion: BRIGHT-Plus and the MARCUS pipeline provide enhanced tools for robust, reasoning-centric retrieval, supporting future research in this area.

Abstract: Retrieval-Augmented Generation (RAG) systems require corpora that are both
structurally clean and semantically coherent. BRIGHT is a recent and
influential benchmark designed to evaluate complex multi-hop retrieval across
diverse, high-reasoning domains. However, its practical effectiveness is
limited by common web-crawled artifacts - such as content redundancy and
semantic discontinuity - that impair retrieval accuracy and downstream
reasoning. Notably, we find that such issues are concentrated in seven
StackExchange-derived subdomains, while other domains (e.g., Coding and
Theorem-based content) remain relatively clean.
  In this study, we present MARCUS, a multi-agent pipeline that leverages large
language models (LLMs) to systematically clean and re-chunk BRIGHT into a
higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for
structural noise removal and semantic segmentation, preserving answer-bearing
spans while improving contextual integrity. Experimental evaluations
demonstrate that BRIGHT-Plus yields consistent and significant improvements in
both retrieval accuracy and multi-hop reasoning across a diverse set of
retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to
support future research on robust, reasoning-centric retrieval.

</details>


### [41] [Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT](https://arxiv.org/abs/2506.07173)
*Miroslav Popovic,Marko Popovic,Miodrag Djukic,Ilija Basicevic*

Main category: cs.AI

TL;DR: This paper introduces an automated approach using ChatGPT to translate Python-based federated learning algorithms into CSP processes for model checking, validated with successful experiments.


<details>
  <summary>Details</summary>
Motivation: To simplify the translation of federated learning algorithms written in Python into CSP processes for automatic verification of their safety and liveness properties.

Method: ChatGPT was used to automate the translation process from Python FL algorithms into CSP processes, incorporating feedback to estimate minimal context usage.

Result: Experimental validation showed successful translation of both centralized and decentralized FL algorithms, verified for safety and liveness by PAT model checker.

Conclusion: The approach using ChatGPT effectively automates the translation process, contributing to the simplification and reliability of verifying FL algorithm properties.

Abstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL
framework that is easy to use by ML&AI developers who do not need to be
professional programmers and is also amenable to LLMs. In the previous
research, generic federated learning algorithms provided by this framework were
manually translated into the CSP processes and algorithms' safety and liveness
properties were automatically verified by the model checker PAT. In this paper,
a simple translation process is introduced wherein the ChatGPT is used to
automate the translation of the mentioned federated learning algorithms in
Python into the corresponding CSP processes. Within the process, the minimality
of the used context is estimated based on the feedback from ChatGPT. The
proposed translation process was experimentally validated by successful
translation (verified by the model checker PAT) of both generic centralized and
decentralized federated learning algorithms.

</details>


### [42] [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
*Liangliang You,Junchi Yao,Shu Yang,Guimin Hu,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: The paper addresses behavioral hallucinations in multimodal LLMs using a two-stage lightweight framework called SHE, achieving reduced hallucination and maintaining descriptive accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models suffer from hallucinations, especially in sequential image scenarios, impacting their utility for broader applications.

Method: The SHE framework detects hallucinations via visual-textual alignment checks and mitigates them with orthogonal projections, using an adaptive temporal window. A metric (BEACH) is introduced to quantify hallucination severity.

Result: SHE reduced behavioral hallucination by over 10% on the BEACH metric while preserving descriptive accuracy according to empirical results on benchmarks.

Conclusion: SHE effectively addresses behavioral hallucinations in sequential images, demonstrating improved performance without sacrificing descriptive fidelity.

Abstract: While multimodal large language models excel at various tasks, they still
suffer from hallucinations, which limit their reliability and scalability for
broader domain applications. To address this issue, recent research mainly
focuses on objective hallucination. However, for sequential images, besides
objective hallucination, there is also behavioral hallucination, which is less
studied. This work aims to fill in the gap. We first reveal that behavioral
hallucinations mainly arise from two key factors: prior-driven bias and the
snowball effect. Based on these observations, we introduce SHE (Sequence
Hallucination Eradication), a lightweight, two-stage framework that (1) detects
hallucinations via visual-textual alignment check using our proposed adaptive
temporal window and (2) mitigates them via orthogonal projection onto the joint
embedding space. We also propose a new metric (BEACH) to quantify behavioral
hallucination severity. Empirical results on standard benchmarks demonstrate
that SHE reduces behavioral hallucination by over 10% on BEACH while
maintaining descriptive accuracy.

</details>


### [43] [Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues](https://arxiv.org/abs/2506.07194)
*Luwei Bai,Dongkeun Han,Sara Hennessy*

Main category: cs.AI

TL;DR: This paper explores strategies for developing a GPT agent to code classroom dialogue, leveraging GPT-4's MyGPT capabilities.


<details>
  <summary>Details</summary>
Motivation: Classroom dialogue analysis is essential for education but is difficult due to nuanced understanding demands and manual work. Large language models offer automation potential but are often unsuitable for researchers with small or specialized datasets.

Method: Researchers used GPT-4's MyGPT agent, evaluating its coding performance with a human codebook and systematically testing example input variations through a variable control method.

Result: The study identified effective strategies for configuring MyGPT agents using limited datasets to produce coding suggestions for classroom dialogue.

Conclusion: Despite limitations, MyGPT agents configured with the proposed strategies can assist researchers in coding dialogue effectively.

Abstract: This study investigates effective strategies for developing a customised GPT
agent to code classroom dialogue. While classroom dialogue is widely recognised
as a crucial element of education, its analysis remains challenging due to the
need for a nuanced understanding of dialogic functions and the labour-intensive
nature of manual transcript coding. Recent advancements in large language
models offer promising avenues for automating this process. However, existing
studies predominantly focus on training large-scale models or evaluating
pre-trained models with fixed codebooks, which are often not applicable or
replicable for dialogue researchers working with small datasets or customised
coding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its
baseline performance in coding classroom dialogue with a human codebook and
examines how performance varies with different example inputs through a
variable control method. Through a design-based research approach, it
identifies a set of practical strategies, based on MyGPT's unique features, for
configuring effective agents with limited data. The findings suggest that,
despite some limitations, a MyGPT agent developed with these strategies can
serve as a useful coding assistant by generating coding suggestions.

</details>


### [44] [Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation](https://arxiv.org/abs/2506.07202)
*Ming Liu,Wensheng Zhang*

Main category: cs.AI

TL;DR: This paper presents a framework for evaluating the generalization of Multimodal Large Language Models (MLLMs) by introducing dynamic task perturbation, revealing the shortcomings of static benchmarks.


<details>
  <summary>Details</summary>
Motivation: MLLMs excel on vision-language benchmarks, but test set contamination during training raises concerns about their genuine generalization ability, necessitating rigorous evaluation methods.

Method: A dynamic evaluation framework perturbs tasks (e.g., QA, captioning) instead of inputs, assessing models' robustness across task families. An automated scoring pipeline evaluates open-ended outputs using paraphrase and corruption sampling.

Result: Testing on benchmarks like MME and RealWorldQA, the framework reveals that fine-tuning on contaminated data sharpens task-specific performance but diminishes overall generalization.

Conclusion: Dynamic task perturbation provides deeper insights into MLLM performance, helping distinguish true model understanding from contamination-induced overfitting.

Abstract: Multimodal Large Language Models (MLLMs) show impressive vision-language
benchmark performance, yet growing concerns about data contamination (test set
exposure during training) risk masking true generalization. This concern
extends to reasoning MLLMs, often fine-tuned via reinforcement learning from
potentially contaminated base models. We propose a novel dynamic evaluation
framework to rigorously assess MLLM generalization, moving beyond static
benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the
same visual input, models are evaluated across a family of tasks (e.g., QA,
captioning, question posing, verification) to probe diverse capabilities. This
task perturbation reveals whether model performance is robust or reliant on
superficial task-specific cues. Our approach is analogous to loss landscape
sharpness: models overfit or contaminated for a single task (sharp minima)
falter under task shifts, unlike models with generalizable solutions (flatter
minima). We developed an automated pipeline with a calibrated judge scoring
open-ended generations (captions, questions) using paraphrase and corruption
sampling. Applying this framework to leading image/video MLLMs on benchmarks
including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task
"ability vector." We demonstrate that fine-tuning on simulated test data
(extreme contamination) drastically sharpens task-specific performance but
harms overall generalization. Our dynamic task perturbation offers deeper
insights into MLLM generalization, distinguishing genuine understanding from
spurious leakage or overfitting.

</details>


### [45] [BIMgent: Towards Autonomous Building Modeling via Computer-use Agents](https://arxiv.org/abs/2506.07217)
*Zihan Deng,Changyu Du,Stavros Nousias,André Borrmann*

Main category: cs.AI

TL;DR: The paper introduces BIMgent, a multimodal LLM-driven framework that successfully automates portions of the 3D building modeling process in BIM software, achieving a significant operation success rate compared to competing models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of specialized agents for complex and domain-specific tasks, such as 3D building modeling in AEC, where current solutions are limited.

Method: The authors designed BIMgent, a framework utilizing multimodal large language models to autonomously handle tasks like conceptual design and reconstruction of building models via GUI operations within BIM software.

Result: BIMgent achieved a 32% success rate in architectural modeling tasks, significantly outperforming baseline models, which had a 0% success rate.

Conclusion: BIMgent demonstrates its practicality by reducing manual workload while maintaining design intent, underscoring its potential applicability in real-world architectural modeling processes.

Abstract: Existing computer-use agents primarily focus on general-purpose desktop
automation tasks, with limited exploration of their application in highly
specialized domains. In particular, the 3D building modeling process in the
Architecture, Engineering, and Construction (AEC) sector involves open-ended
design tasks and complex interaction patterns within Building Information
Modeling (BIM) authoring software, which has yet to be thoroughly addressed by
current studies. In this paper, we propose BIMgent, an agentic framework
powered by multimodal large language models (LLMs), designed to enable
autonomous building model authoring via graphical user interface (GUI)
operations. BIMgent automates the architectural building modeling process,
including multimodal input for conceptual design, planning of software-specific
workflows, and efficient execution of the authoring GUI actions. We evaluate
BIMgent on real-world building modeling tasks, including both text-based
conceptual design generation and reconstruction from existing building design.
The design quality achieved by BIMgent was found to be reasonable. Its
operations achieved a 32% success rate, whereas all baseline models failed to
complete the tasks (0% success rate). Results demonstrate that BIMgent
effectively reduces manual workload while preserving design intent,
highlighting its potential for practical deployment in real-world architectural
modeling scenarios.

</details>


### [46] [LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments](https://arxiv.org/abs/2506.07223)
*Yangqing Zheng,Shunqi Mao,Dingxin Zhang,Weidong Cai*

Main category: cs.AI

TL;DR: The study addresses the challenge of decision-making delays for agents in high-risk dynamic scenarios by introducing a latency-aware evaluation protocol and a new agent model, achieving superior performance in extreme conditions.


<details>
  <summary>Details</summary>
Motivation: Improving agent decision-making in dynamic, high-risk scenarios like fire, flood, and wind to address the insufficiently studied issue of delays in decision-making.

Method: The authors propose a Time Conversion Mechanism (TCM) to convert inference delays into simulation frames, extend the HAZARD benchmark with latency-aware metrics (RL and LAR), and develop the Rapid-Reflex Async-Reflect Agent (RRARA) that combines a lightweight LLM-guided feedback module with rule-based reactive behaviors.

Result: Experiments demonstrate that RRARA significantly outperforms existing baselines in latency-sensitive tasks evaluated on the HAZARD benchmark.

Conclusion: The paper highlights the importance of latency-aware evaluation and introduces RRARA as a high-performing solution to reduce decision-making delays in extreme, dynamic conditions of embodied intelligence.

Abstract: In the realm of embodied intelligence, the evolution of large language models
(LLMs) has markedly enhanced agent decision making. Consequently, researchers
have begun exploring agent performance in dynamically changing high-risk
scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under
these extreme conditions, the delay in decision making emerges as a crucial yet
insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that
translates inference delays in decision-making into equivalent simulation
frames, thus aligning cognitive and physical costs under a single FPS-based
metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action
Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we
present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a
lightweight LLM-guided feedback module with a rule-based agent to enable
immediate reactive behaviors and asynchronous reflective refinements in situ.
Experiments on HAZARD show that RRARA substantially outperforms existing
baselines in latency-sensitive scenarios.

</details>


### [47] [Subgoal-Guided Policy Heuristic Search with Learned Subgoals](https://arxiv.org/abs/2506.07255)
*Jake Tuero,Michael Buro,Levi H. S. Lelis*

Main category: cs.AI

TL;DR: The paper presents a method to improve policy tree search algorithms by introducing subgoal-based learning to make training more efficient, especially in challenging problem instances.


<details>
  <summary>Details</summary>
Motivation: Current policy tree search algorithms require full solution trajectories for efficient training, which becomes costly and inefficient in hard problem instances, especially when starting with random policies.

Method: The proposed method incorporates subgoal-based policies, learning from both successful and unsuccessful search trees, to improve learning efficiency during the trial-and-error process.

Result: The experiments demonstrate that subgoal-based policy formulation and training improve the sample efficiency of learning policies and heuristic functions.

Conclusion: Subgoal-based policies make policy tree search training more resource-efficient, even in complex problem scenarios.

Abstract: Policy tree search is a family of tree search algorithms that use a policy to
guide the search. These algorithms provide guarantees on the number of
expansions required to solve a given problem that are based on the quality of
the policy. While these algorithms have shown promising results, the process in
which they are trained requires complete solution trajectories to train the
policy. Search trajectories are obtained during a trial-and-error search
process. When the training problem instances are hard, learning can be
prohibitively costly, especially when starting from a randomly initialized
policy. As a result, search samples are wasted in failed attempts to solve
these hard instances. This paper introduces a novel method for learning
subgoal-based policies for policy tree search algorithms. The subgoals and
policies conditioned on subgoals are learned from the trees that the search
expands while attempting to solve problems, including the search trees of
failed attempts. We empirically show that our policy formulation and training
method improve the sample efficiency of learning a policy and heuristic
function in this online setting.

</details>


### [48] [An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning](https://arxiv.org/abs/2506.07411)
*Ze Yang,Yihong Jin,Juntian Liu,Xinhe Xu*

Main category: cs.AI

TL;DR: The paper introduces an Intelligent Fault Self-Healing Mechanism (IFSHM) integrating LLM and DRL for efficient fault recovery in cloud AI systems.


<details>
  <summary>Details</summary>
Motivation: The need to address increasing system faults in cloud-based AI systems due to their growing scale and complexity.

Method: A two-stage hybrid architecture combining LLM-driven fault interpretation and DRL strategy optimization.

Result: IFSHM reduces system recovery time by 37% in unknown fault scenarios compared to existing methods.

Conclusion: The integration of LLM and DRL improves fault recovery efficiency and adaptability, ensuring better service reliability.

Abstract: As the scale and complexity of cloud-based AI systems continue to increase,
the detection and adaptive recovery of system faults have become the core
challenges to ensure service reliability and continuity. In this paper, we
propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates
Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to
realize a fault recovery framework with semantic understanding and policy
optimization capabilities in cloud AI systems. On the basis of the traditional
DRL-based control model, the proposed method constructs a two-stage hybrid
architecture: (1) an LLM-driven fault semantic interpretation module, which can
dynamically extract deep contextual semantics from multi-source logs and system
indicators to accurately identify potential fault modes; (2) DRL recovery
strategy optimizer, based on reinforcement learning, learns the dynamic
matching of fault types and response behaviors in the cloud environment. The
innovation of this method lies in the introduction of LLM for environment
modeling and action space abstraction, which greatly improves the exploration
efficiency and generalization ability of reinforcement learning. At the same
time, a memory-guided meta-controller is introduced, combined with
reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve
continuous adaptation to new failure modes and avoid catastrophic forgetting.
Experimental results on the cloud fault injection platform show that compared
with the existing DRL and rule methods, the IFSHM framework shortens the system
recovery time by 37% with unknown fault scenarios.

</details>


### [49] [Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests](https://arxiv.org/abs/2506.07418)
*Arnau Igualde Sáez,Lamyae Rhomrasi,Yusef Ahsini,Ricardo Vinuesa,Sergio Hoyas,Jose P. García Sabater,Marius J. Fullana i Alfonso,J. Alberto Conejero*

Main category: cs.AI

TL;DR: The paper evaluates multimodal large language models (MLLMs) for visual mathematics problem-solving using a multilingual benchmark, highlighting their moderate precision, language variation, and reasoning styles.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the effectiveness of MLLMs in solving visually represented mathematical problems, an area that has been underexplored.

Method: The authors benchmark several MLLMs, including Gemini 2.0 Flash and GPT 4o, across geometry, algebra, and logic in multilingual tasks framed within a Kangaroo-style benchmark in English, French, Spanish, and Catalan.

Result: Findings reveal moderate precision across mathematical topics and difficulty levels, significant underutilization of visual diagrams, variation across languages, and distinctions in reasoning styles among different models, with Gemini 2.0 Flash leading on image-based tasks.

Conclusion: MLLMs show promise but lack high accuracy or human-level performance, especially in complex visual mathematics tasks, underscoring the need for improved utilization of diagrammatic inputs and advanced reasoning capabilities.

Abstract: Multimodal Large Language Models (MLLMs) promise advanced vision language
capabilities, yet their effectiveness in visually presented mathematics remains
underexplored. This paper analyzes the development and evaluation of MLLMs for
mathematical problem solving, focusing on diagrams, multilingual text, and
symbolic notation. We then assess several models, including GPT 4o, Pixtral,
Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual
Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our
experiments reveal four key findings. First, overall precision remains moderate
across geometry, visual algebra, logic, patterns, and combinatorics: no single
model excels in every topic. Second, while most models see improved accuracy
with questions that do not have images, the gain is often limited; performance
for some remains nearly unchanged without visual input, indicating
underutilization of diagrammatic information. Third, substantial variation
exists across languages and difficulty levels: models frequently handle easier
items but struggle with advanced geometry and combinatorial reasoning. Notably,
Gemini 2.0 Flash achieves the highest precision on image based tasks, followed
by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance.
Fourth, a complementary analysis aimed at distinguishing whether models reason
or simply recite reveals that Gemini and GPT 4o stand out for their structured
reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less
consistent reasoning, often defaulting to heuristics or randomness when unable
to align their outputs with the given answer options.

</details>


### [50] [HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model](https://arxiv.org/abs/2506.07428)
*Yuling Wang,Zihui Chen,Pengfei Jiao,Xiao Wang*

Main category: cs.AI

TL;DR: The paper introduces HeTa, a foundation attack model designed to create transferable and generalizable perturbations targeting heterogeneous graph neural networks (HGNNs).


<details>
  <summary>Details</summary>
Motivation: Existing attack methods for HGNNs often require complex retraining, limiting their adaptability and generalization across new heterogeneous graphs. This paper seeks a foundational attack approach to address this inefficiency.

Method: The authors proposed HeTa, a relation-wise foundation attack model that captures shared vulnerability patterns in HGNNs. It uses a foundation surrogate model to align heterogeneity, identify relation-aware attack units, and implement serialized relation-by-relation attacks for transferable perturbations.

Result: Experiments demonstrate that HeTa achieves powerful attack performance and high generalizability across different HGNNs and new heterogeneous graphs.

Conclusion: HeTa successfully addresses the need for efficient, generalizable attack strategies on HGNNs, paving the way for both improved robustness assessment and model security in heterogeneous graph environments.

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the
need for tailored attacks to assess their robustness and ensure security.
However, existing HGNN attacks often require complex retraining of parameters
to generate specific perturbations for new scenarios. Recently, foundation
models have opened new horizons for the generalization of graph neural networks
by capturing shared semantics across various graph distributions. This leads us
to ask:Can we design a foundation attack model for HGNNs that enables
generalizable perturbations across different HGNNs, and quickly adapts to new
heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant
differences in model design and parameter space, different HGNNs surprisingly
share common vulnerability patterns from a relation-aware perspective.
Therefore, we explore how to design foundation HGNN attack criteria by mining
shared attack units. In this paper, we propose a novel relation-wise
heterogeneous graph foundation attack model, HeTa. We introduce a foundation
surrogate model to align heterogeneity and identify the importance of shared
relation-aware attack units. Building on this, we implement a serialized
relation-by-relation attack based on the identified relational weights. In this
way, the perturbation can be transferred to various target HGNNs and easily
fine-tuned for new HGs. Extensive experiments exhibit powerful attack
performances and generalizability of our method.

</details>


### [51] [LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning](https://arxiv.org/abs/2506.07443)
*Weijie Shi,Han Zhu,Jiaming Ji,Mengze Li,Jipeng Zhang,Ruiyuan Zhang,Jia Zhu,Jiajie Xu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: This paper introduces LegalReasoner, a system designed to improve the reliability of legal judgment prediction by implementing step-wise reasoning and error correction.


<details>
  <summary>Details</summary>
Motivation: To improve judicial efficiency and decision-making by addressing the logical errors within existing legal judgment prediction systems.

Method: LegalReasoner uses step-wise reasoning with a process verifier to validate the logical correctness, progressiveness, and resolution of each reasoning step, correcting errors when detected. A new dataset, LegalHK, serves as the basis for fine-tuning.

Result: LegalReasoner improved concordance with court decisions from 72.37% to 80.27% in experiments using the LLAMA-3.1-70B model.

Conclusion: LegalReasoner enhances reliability and accuracy in legal judgment prediction, offering tools and data to support more consistent and effective judicial reasoning processes.

Abstract: Legal judgment prediction (LJP) aims to function as a judge by making final
rulings based on case claims and facts, which plays a vital role in the
judicial domain for supporting court decision-making and improving judicial
efficiency. However, existing methods often struggle with logical errors when
conducting complex legal reasoning. We propose LegalReasoner, which enhances
LJP reliability through step-wise verification and correction of the reasoning
process. Specifically, it first identifies dispute points to decompose complex
cases, and then conducts step-wise reasoning while employing a process verifier
to validate each step's logic from correctness, progressiveness, and potential
perspectives. When errors are detected, expert-designed attribution and
resolution strategies are applied for correction. To fine-tune LegalReasoner,
we release the LegalHK dataset, containing 58,130 Hong Kong court cases with
detailed annotations of dispute points, step-by-step reasoning chains, and
process verification labels. Experiments demonstrate that LegalReasoner
significantly improves concordance with court decisions from 72.37 to 80.27 on
LLAMA-3.1-70B. The data is available at
https://huggingface.co/datasets/weijiezz/LegalHK.

</details>


### [52] [Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification](https://arxiv.org/abs/2506.07446)
*Liwen Zheng,Chaozhuo Li,Zheng Liu,Feiran Huang,Haoran Jia,Zaisheng Ye,Xi Zhang*

Main category: cs.AI

TL;DR: The paper introduces Atomic Fact Extraction and Verification (AFEV), a new approach to enhance accuracy and interpretability in fact verification, particularly for complex claims requiring multi-hop reasoning.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in verifying complex claims caused by traditional methods' reliance on static decomposition and surface-level semantic retrieval, which leads to reasoning errors and poor adaptability.

Method: AFEV iteratively decomposes complex claims into atomic facts. It refines understanding, re-ranks evidence to eliminate noise, and uses context-specific demonstrations for reasoning.

Result: Experiments on five benchmark datasets show that AFEV achieves state-of-the-art performance in accuracy and interpretability.

Conclusion: AFEV provides an advanced and adaptable solution for fact verification, reducing errors and improving the reasoning process, making it suitable for diverse and complex scenarios.

Abstract: Fact verification plays a vital role in combating misinformation by assessing
the veracity of claims through evidence retrieval and reasoning. However,
traditional methods struggle with complex claims requiring multi-hop reasoning
over fragmented evidence, as they often rely on static decomposition strategies
and surface-level semantic retrieval, which fail to capture the nuanced
structure and intent of the claim. This results in accumulated reasoning
errors, noisy evidence contamination, and limited adaptability to diverse
claims, ultimately undermining verification accuracy in complex scenarios. To
address this, we propose Atomic Fact Extraction and Verification (AFEV), a
novel framework that iteratively decomposes complex claims into atomic facts,
enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically
refines claim understanding and reduces error propagation through iterative
fact extraction, reranks evidence to filter noise, and leverages
context-specific demonstrations to guide the reasoning process. Extensive
experiments on five benchmark datasets demonstrate that AFEV achieves
state-of-the-art performance in both accuracy and interpretability.

</details>


### [53] [Efficient Generation of Diverse Cooperative Agents with World Models](https://arxiv.org/abs/2506.07450)
*Yi Loo,Akshunn Trivedi,Malika Meghjani*

Main category: cs.AI

TL;DR: The abstract presents XPM-WM, a framework using simulated trajectories from a learned World Model to improve the efficiency of Cross-play Minimization methods for generating diverse partner agents in Zero-Shot Coordination.


<details>
  <summary>Details</summary>
Motivation: Current methods for Zero-Shot Coordination require computationally expensive and inefficient processes for generating partner agents with diverse collaborative conventions.

Method: The proposed XPM-WM framework utilizes simulated trajectories generated from a learned World Model to reduce the need to sample multiple trajectories during partner agent generation.

Result: XPM-WM demonstrates the ability to generate diverse partners while achieving similar training rewards as previous methods, but with improved sample efficiency and scalability.

Conclusion: XPM-WM offers a more efficient and scalable approach to training Zero-Shot Coordination agents, making it a valuable improvement over existing methods.

Abstract: A major bottleneck in the training process for Zero-Shot Coordination (ZSC)
agents is the generation of partner agents that are diverse in collaborative
conventions. Current Cross-play Minimization (XPM) methods for population
generation can be very computationally expensive and sample inefficient as the
training objective requires sampling multiple types of trajectories. Each
partner agent in the population is also trained from scratch, despite all of
the partners in the population learning policies of the same coordination task.
In this work, we propose that simulated trajectories from the dynamics model of
an environment can drastically speed up the training process for XPM methods.
We introduce XPM-WM, a framework for generating simulated trajectories for XPM
via a learned World Model (WM). We show XPM with simulated trajectories removes
the need to sample multiple trajectories. In addition, we show our proposed
method can effectively generate partners with diverse conventions that match
the performance of previous methods in terms of SP population training reward
as well as training partners for ZSC agents. Our method is thus, significantly
more sample efficient and scalable to a larger number of partners.

</details>


### [54] [Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527)
*Lu Ma,Hao Liang,Meiyi Qiang,Lexiang Tang,Xiaochen Ma,Zhen Hao Wong,Junbo Niu,Chengyu Shen,Runming He,Bin Cui,Wentao Zhang*

Main category: cs.AI

TL;DR: This paper proposes ReLIFT, a method combining reinforcement learning (RL) and supervised fine-tuning (SFT) to overcome limitations in large language model reasoning by integrating new knowledge via fine-tuning. It demonstrates improved reasoning performance across benchmarks while using fewer data.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning approaches for large language models improve model capabilities based on existing knowledge but struggle to acquire new reasoning patterns or knowledge. This paper seeks to address these limitations and improve reasoning capability beyond the base model.

Method: The researchers combine RL and SFT in an alternating training process, dubbed ReLIFT. RL improves performance on tasks within the base model's knowledge, while SFT incorporates new knowledge and reasoning patterns through high-quality demonstration data.

Result: ReLIFT achieved an average improvement of +5.2 points across multiple competition-level benchmarks compared to other methods. The approach requires only 13% of the usual detailed demonstration data, showing improved scalability and performance.

Conclusion: ReLIFT addresses RL's limitations and combines complementary strengths of RL and SFT to enhance reasoning abilities in large language models. Its success illustrates significant potential for advancing LLM reasoning efficiency and effectiveness.

Abstract: Recent advances in large language model (LLM) reasoning have shown that
sophisticated behaviors such as planning and self-reflection can emerge through
reinforcement learning (RL). However, despite these successes, RL in its
current form remains insufficient to induce capabilities that exceed the
limitations of the base model, as it is primarily optimized based on existing
knowledge of the model rather than facilitating the acquisition of new
information. To address this limitation, we employ supervised fine-tuning (SFT)
to learn what RL cannot, which enables the incorporation of new knowledge and
reasoning patterns by leveraging high-quality demonstration data. We analyze
the training dynamics of RL and SFT for LLM reasoning and find that RL excels
at maintaining and improving performance on questions within the model's
original capabilities, while SFT is more effective at enabling progress on
questions beyond the current scope of the model. Motivated by the complementary
strengths of RL and SFT, we introduce a novel training approach,
\textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved
with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily
trained using RL, but when it encounters challenging questions, high-quality
solutions are collected for fine-tuning, and the training process alternates
between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT
achieves an average improvement of over +5.2 points across five
competition-level benchmarks and one out-of-distribution benchmark compared to
other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both
RL and SFT while using only 13\% of the detailed demonstration data,
highlighting its scalability. These results provide compelling evidence that
ReLIFT overcomes the fundamental limitations of RL and underscores the
significant potential.

</details>


### [55] [Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification](https://arxiv.org/abs/2506.07528)
*Qisheng Hu,Quanyu Long,Wenya Wang*

Main category: cs.AI

TL;DR: The paper introduces HARIS, a system for multi-hop claim verification that combines reasoning and search processes using hierarchical agents.


<details>
  <summary>Details</summary>
Motivation: Multi-hop claim verification is complex because it demands interleaved reasoning and dynamic searching for evidence. Current systems often struggle with coordinating these tasks effectively.

Method: HARIS employs a hierarchical agent structure where a high-level reasoning agent generates questions for missing information, and a low-level search agent retrieves evidence iteratively, refining searches based on findings. The system is trained using reinforcement learning and outcome-based rewards.

Result: HARIS performs strongly on benchmarks such as EX-FEVER and HOVER, demonstrating improved accuracy and better interpretability in multi-hop claim verification tasks.

Conclusion: The hierarchical design of HARIS allows for specialized agents focusing on distinct tasks, addressing the challenges of coordinated reasoning and search in multi-hop claim verification, significantly advancing the field.

Abstract: Multi-hop claim verification is inherently challenging, requiring multi-step
reasoning to construct verification chains while iteratively searching for
information to uncover hidden bridging facts. This process is fundamentally
interleaved, as effective reasoning relies on dynamically retrieved evidence,
while effective search demands reasoning to refine queries based on partial
information. To achieve this, we propose Hierarchical Agent Reasoning and
Information Search (HARIS), explicitly modeling the coordinated process of
reasoning-driven searching and search-informed reasoning. HARIS consists of a
high-level reasoning agent that focuses on constructing the main verification
chain, generating factual questions when more information is needed, and a
low-level search agent that iteratively retrieves more information, refining
its search based on intermediate findings. This design allows each agent to
specialize in its respective task, enhancing verification accuracy and
interpretability. HARIS is trained using reinforcement learning with
outcome-based rewards. Experimental results on the EX-FEVER and HOVER
benchmarks demonstrate that HARIS achieves strong performance, greatly
advancing multi-hop claim verification.

</details>


### [56] [Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.07548)
*Weiqiang Jin,Hongyang Du,Guizhong Liu,Dong In Kim*

Main category: cs.AI

TL;DR: The paper introduces a dynamic curriculum learning (CL) approach for multi-agent reinforcement learning (MARL), using adaptive difficulty adjustment and a novel CGRPA mechanism to improve stability and performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of existing MARL approaches that use fixed opponent strategies, restricting adaptability, and yielding suboptimal policies.

Method: They propose a dynamic curriculum learning framework with a self-adaptive difficulty adjustment mechanism, coupled with the Counterfactual Group Relative Policy Advantage (CGRPA) for intrinsic credit assignment.

Result: Experiments show improved training stability and final performance, with results competitive against state-of-the-art methods in MARL.

Conclusion: The integration of dynamic CL and CGRPA stabilizes learning in nonstationary environments, enhances credit assignment, and enables MARL agents to perform better under evolving task demands.

Abstract: Multi-agent reinforcement learning (MARL) has achieved strong performance in
cooperative adversarial tasks. However, most existing methods typically train
agents against fixed opponent strategies and rely on such meta-static
difficulty conditions, which limits their adaptability to changing environments
and often leads to suboptimal policies. Inspired by the success of curriculum
learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL
that employs an self-adaptive difficulty adjustment mechanism. This mechanism
continuously modulates opponent strength based on real-time agent training
performance, allowing agents to progressively learn from easier to more
challenging scenarios. However, the dynamic nature of CL introduces instability
due to nonstationary environments and sparse global rewards. To address this
challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA),
which is tightly coupled with the curriculum by providing intrinsic credit
signals that reflect each agent's impact under evolving task demands. CGRPA
constructs a counterfactual advantage function that isolates individual
contributions within group behavior, facilitating more reliable policy updates
throughout the curriculum. CGRPA evaluates each agent's contribution through
constructing counterfactual action advantage function, providing intrinsic
rewards that enhance credit assignment and stabilize learning under
non-stationary conditions. Extensive experiments demonstrate that our method
improves both training stability and final performance, achieving competitive
results against state-of-the-art methods. The code is available at
https://github.com/NICE-HKU/CL2MARL-SMAC.

</details>


### [57] [GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition](https://arxiv.org/abs/2506.07553)
*Jingchao Wang,Haote Yang,Jiang Wu,Yifan He,Xingjian Wei,Yinfan Wang,Chengjin Liu,Lingli Ge,Lijun Wu,Bin Wang,Dahua Lin,Conghui He*

Main category: cs.AI

TL;DR: The paper introduces GTR-Mol-VLM, a novel framework to improve Optical Chemical Structure Recognition (OCSR) with innovations in graph traversal and recognition methodology, supported by a new dataset and benchmark.


<details>
  <summary>Details</summary>
Motivation: Recent vision-language models face challenges in recognizing complex molecular structures due to inconsistencies in annotations and limitations in their image-captioning approach.

Method: GTR-Mol-VLM employs a graph traversal mechanism for sequential atom-bond predictions and adheres to the principle of faithfully recognizing visual inputs to improve accuracy in OCSR.

Result: The framework outperforms existing models and technologies, showing approximately 14 percentage points improvement over the second-best baseline in handling images with functional group abbreviations.

Conclusion: The advancements in GTR-Mol-VLM and its supporting resources aim to benefit cheminformatics and AI by meeting practical OCSR needs while improving graph-parsing benchmarks.

Abstract: Optical Chemical Structure Recognition (OCSR) is crucial for digitizing
chemical knowledge by converting molecular images into machine-readable
formats. While recent vision-language models (VLMs) have shown potential in
this task, their image-captioning approach often struggles with complex
molecular structures and inconsistent annotations. To overcome these
challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key
innovations: (1) the \textit{Graph Traversal as Visual Chain of Thought}
mechanism that emulates human reasoning by incrementally parsing molecular
graphs through sequential atom-bond predictions, and (2) the data-centric
principle of \textit{Faithfully Recognize What You've Seen}, which addresses
the mismatch between abbreviated structures in images and their expanded
annotations. To support model development, we constructed GTR-CoT-1.3M, a
large-scale instruction-tuning dataset with meticulously corrected annotations,
and introduced MolRec-Bench, the first benchmark designed for a fine-grained
evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments
demonstrate that GTR-Mol-VLM achieves superior results compared to specialist
models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in
scenarios involving molecular images with functional group abbreviations,
GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage
points, both in SMILES-based and graph-based metrics. We hope that this work
will drive OCSR technology to more effectively meet real-world needs, thereby
advancing the fields of cheminformatics and AI for Science. We will release
GTR-CoT at https://github.com/opendatalab/GTR-CoT.

</details>


### [58] [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)
*Peiran Li,Xinkai Zou,Zhuohang Wu,Ruifeng Li,Shuo Xing,Hanwen Zheng,Zhikai Hu,Yuping Wang,Haoxi Li,Qin Yuan,Yingmo Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: SAFEFLOW introduces a secure framework for LLM/VLM-based agents to ensure data integrity, confidentiality, and coordination in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Current autonomous agent frameworks based on LLMs and VLMs are fragile, lacking secure information flow, reliability, and multi-agent coordination.

Method: SAFEFLOW employs fine-grained information flow control, transactional execution, and mechanisms like logging and secure caches to ensure security and robustness in agent operations.

Result: SAFEFLOW outperforms state-of-the-art frameworks in maintaining task performance and security in adversarial and noisy environments, as demonstrated through SAFEFLOWBENCH experiments.

Conclusion: SAFEFLOW sets a foundation for creating robust, secure, and trustworthy LLM/VLM-based agent ecosystems.

Abstract: Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.

</details>


### [59] [Automating Exploratory Multiomics Research via Language Models](https://arxiv.org/abs/2506.07591)
*Shang Qu,Ning Ding,Linhai Xie,Yifei Li,Zaoqu Liu,Kaiyan Zhang,Yibai Xiong,Yuxin Zuo,Zhangren Chen,Ermo Hua,Xingtai Lv,Youbang Sun,Yang Li,Dong Li,Fuchu He,Bowen Zhou*

Main category: cs.AI

TL;DR: PROTEUS is an automated system for generating hypotheses from raw data, particularly in clinical proteogenomics, evaluated with 360 hypotheses from 10 datasets.


<details>
  <summary>Details</summary>
Motivation: To create a fully automated system that accelerates and specializes in generating hypotheses from complex clinical multiomics data.

Method: PROTEUS uses modular systems for open-ended data exploration, statistical analysis, and hypothesis proposal using graph structures.

Result: Applied to 10 clinical multiomics datasets, producing 360 hypotheses validated by external data and scoring.

Conclusion: PROTEUS facilitates reliable and innovative hypothesis generation from heterogeneous data, pushing towards specialized autonomous systems in scientific domains.

Abstract: This paper introduces PROTEUS, a fully automated system that produces
data-driven hypotheses from raw data files. We apply PROTEUS to clinical
proteogenomics, a field where effective downstream data analysis and hypothesis
proposal is crucial for producing novel discoveries. PROTEUS uses separate
modules to simulate different stages of the scientific process, from open-ended
data exploration to specific statistical analysis and hypothesis proposal. It
formulates research directions, tools, and results in terms of relationships
between biological entities, using unified graph structures to manage complex
research processes. We applied PROTEUS to 10 clinical multiomics datasets from
published research, arriving at 360 total hypotheses. Results were evaluated
through external data validation and automatic open-ended scoring. Through
exploratory and iterative research, the system can navigate high-throughput and
heterogeneous multiomics data to arrive at hypotheses that balance reliability
and novelty. In addition to accelerating multiomic analysis, PROTEUS represents
a path towards tailoring general autonomous systems to specialized scientific
domains to achieve open-ended hypothesis generation from data.

</details>


### [60] [SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling](https://arxiv.org/abs/2506.07636)
*Haoran Wang,Zhenyu Hou,Yao Wei,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: SWE-Dev introduces a framework that enhances open-source LLM-based software engineering agents through better test case synthesis and scaling training data, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in building effective software engineering agents, specifically the lack of high-quality training data and test cases.

Method: A robust pipeline was developed to synthesize test cases, alongside scaling up agent trajectories to create training data. Experiments were conducted on the SWE-bench-Verified benchmark to evaluate model performance.

Result: The SWE-Dev models (7B and 32B parameters) achieved success rates of 23.4% and 36.6% respectively, outperforming state-of-the-art open-source software engineering models.

Conclusion: SWE-Dev significantly advances open-source SWE agents, demonstrating high performance and providing all resources publicly accessible for further innovation in the field.

Abstract: Large language models (LLMs) have advanced rapidly from conversational
problem solving to addressing real-world tasks involving tool use, such as
software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex
and Cursor, have offered end-to-end automation of the software development
process. However, building effective SWE agents remains challenging due to the
lack of high-quality training data and effective test cases. To address this
issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we
develop a robust pipeline to synthesize test cases for patch evaluation.
Second, we scale up agent trajectories to construct the training data for
building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the
SWE-Dev models can achieve top performance among all open SWE agents.
Specifically, the success rates of the SWE-Dev 7B and 32B parameter models
reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source
models. All code, models, and datasets are publicly available at
https://github.com/THUDM/SWE-Dev.

</details>


### [61] [MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents](https://arxiv.org/abs/2506.07672)
*Yunhe Yan,Shihe Wang,Jiajun Du,Yexuan Yang,Yuxuan Shan,Qichen Qiu,Xianqing Jia,Xinge Wang,Xin Yuan,Xu Han,Mao Qin,Yinxiao Chen,Chen Peng,Shangguang Wang,Mengwei Xu*

Main category: cs.AI

TL;DR: The paper proposes MCPWorld, a novel benchmarking platform for evaluating API, GUI, and hybrid computer use agents, enhancing robustness and accuracy through 'white-box apps' and containerized infrastructure.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for computer use agents (CUA) focus primarily on GUI agents, which are sensitive to UI changes and fail to account for interactions via application APIs. This gap motivates the design of MCPWorld to enable more comprehensive and robust evaluation.

Method: The authors introduce MCPWorld, which utilizes 'white-box apps' with customizable source code and dynamic monitoring capabilities. The platform includes 201 annotated tasks and is containerized with GPU acceleration for compatibility.

Result: Preliminary experiments with an LLM-powered framework demonstrated a task completion accuracy of 75.12%, suggesting the potential of MCPWorld in enhancing agent automation effectiveness.

Conclusion: MCPWorld serves as a standardized and extensible platform for benchmarking next-generation computer use agents, incorporating diverse functionalities and evaluation practices. Its resources are publicly accessible for community adoption.

Abstract: (M)LLM-powered computer use agents (CUA) are emerging as a transformative
technique to automate human-computer interaction. However, existing CUA
benchmarks predominantly target GUI agents, whose evaluation methods are
susceptible to UI changes and ignore function interactions exposed by
application APIs, e.g., Model Context Protocol (MCP). To this end, we propose
MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid
agents. A key principle of MCPWorld is the use of "white-box apps", i.e., those
with source code availability and can be revised/re-compiled as needed (e.g.,
adding MCP support), with two notable advantages:
  (1) It greatly broadens the design space of CUA, such as what and how the app
features to be exposed/extracted as CUA-callable APIs.
  (2) It allows MCPWorld to programmatically verify task completion by directly
monitoring application behavior through techniques like dynamic code
instrumentation, offering robust, accurate CUA evaluation decoupled from
specific agent implementations or UI states.
  Currently, MCPWorld includes 201 well curated and annotated user tasks,
covering diversified use cases and difficulty levels. MCPWorld is also fully
containerized with GPU acceleration support for flexible adoption on different
OS/hardware environments. Our preliminary experiments, using a representative
LLM-powered CUA framework, achieve 75.12% task completion accuracy,
simultaneously providing initial evidence on the practical effectiveness of
agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate
and standardize the benchmarking of next-generation computer use agents that
can leverage rich external tools. Our code and dataset are publicly available
at https://github.com/SAAgent/MCPWorld.

</details>


### [62] [NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models](https://arxiv.org/abs/2506.07731)
*Mouadh Yagoubi,Yasser Dahou,Billel Mokeddem,Younes Belkada,Phuc H. Le-Khac,Basma El Amel Boussaha,Reda Alami,Jingwei Zuo,Damiano Marsili,Mugariya Farooq,Mounia Lalmas,Georgia Gkioxari,Patrick Gallinari,Philip Torr,Hakim Hacid*

Main category: cs.AI

TL;DR: The paper highlights weaknesses in existing benchmarks for evaluating small language models during early training stages and introduces a challenge for designing improved evaluation tasks tailored for early training progress.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for language models are not effective at assessing early-stage training progress, leaving a gap in understanding and evaluating foundational stages of model development.

Method: The competition provides small pre-trained models (0.5B, 1B, and 3B parameters) with training checkpoints, and invites participants to develop or adapt evaluation benchmarks focused on measuring early training progress in scientific knowledge tasks.

Result: Participants will be evaluated on their ability to produce quality performance signals, consistent rankings at large-scale training, and relevance to the scientific domain, aiming to improve model assessment and attract diverse contributors.

Conclusion: By emphasizing early training evaluation in small models, the initiative seeks to foster more systematic and benchmark-driven foundational research, accessible even to researchers with limited resources.

Abstract: Existing benchmarks have proven effective for assessing the performance of
fully trained large language models. However, we find striking differences in
the early training stages of small models, where benchmarks often fail to
provide meaningful or discriminative signals. To explore how these differences
arise, this competition tackles the challenge of designing scientific knowledge
evaluation tasks specifically tailored for measuring early training progress of
language models. Participants are invited to develop novel evaluation
methodologies or adapt existing benchmarks to better capture performance
differences among language models. To support this effort, we provide three
pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate
checkpoints sampled during training up to 200B tokens. All experiments and
development work can be run on widely available free cloud-based GPU platforms,
making participation accessible to researchers with limited computational
resources. Submissions will be evaluated based on three criteria: the quality
of the performance signal they produce, the consistency of model rankings at 1
trillion tokens of training, and their relevance to the scientific knowledge
domain. By promoting the design of tailored evaluation strategies for early
training, this competition aims to attract a broad range of participants from
various disciplines, including those who may not be machine learning experts or
have access to dedicated GPU resources. Ultimately, this initiative seeks to
make foundational LLM research more systematic and benchmark-informed from the
earliest phases of model development.

</details>


### [63] [RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](https://arxiv.org/abs/2506.07736)
*Jingnan Zheng,Xiangtian Ji,Yijun Lu,Chenhang Cui,Weixiang Zhao,Gelei Deng,Zhenkai Liang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: The paper introduces RSafe, a two-stage approach for improving the safety and robustness of guard models moderating Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address vulnerabilities in LLMs, as existing moderation strategies relying on curated datasets struggle with emerging threats like jailbreak attacks and new harmful content categories.

Method: RSafe employs a two-stage process: guided reasoning to analyze potential safety risks step-by-step and reinforced alignment, using rule-based reinforcement learning to optimize reasoning for accurate safety assessments.

Result: RSafe enhances safety capabilities, making guard models more adaptive and robust against unseen or adversarial scenarios, and aligns safeguards with user-specified policies.

Conclusion: The two-stage reasoning and reinforcement approach enables RSafe to internalize safety principles effectively, offering a more adaptive and policy-tailored safeguard solution for LLM moderation.

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities despite
deliberate safety alignment efforts, posing significant risks to users and
society. To safeguard against the risk of policy-violating content,
system-level moderation via external guard models-designed to monitor LLM
inputs and outputs and block potentially harmful content-has emerged as a
prevalent mitigation strategy. Existing approaches of training guard models
rely heavily on extensive human curated datasets and struggle with
out-of-distribution threats, such as emerging harmful categories or jailbreak
attacks. To address these limitations, we propose RSafe, an adaptive
reasoning-based safeguard that conducts guided safety reasoning to provide
robust protection within the scope of specified safety policies. RSafe operates
in two stages: 1) guided reasoning, where it analyzes safety risks of input
content through policy-guided step-by-step reasoning, and 2) reinforced
alignment, where rule-based RL optimizes its reasoning paths to align with
accurate safety prediction. This two-stage training paradigm enables RSafe to
internalize safety principles to generalize safety protection capability over
unseen or adversarial safety violation scenarios. During inference, RSafe
accepts user-specified safety policies to provide enhanced safeguards tailored
to specific safety requirements.

</details>


### [64] [Agent Semantics, Semantic Spacetime, and Graphical Reasoning](https://arxiv.org/abs/2506.07756)
*Mark Burgess*

Main category: cs.AI

TL;DR: The paper discusses formal aspects of the Semantic Spacetime graph model for directed knowledge representation and process modeling, emphasizing scalability and predictability in semantic pathways.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address issues of information leakage and unintentional absorbing states in knowledge graphs, aiming to improve predictability and semantic complexity handling.

Method: The authors define a finite $
\gamma(3,4)$ representation to establish scalable and closed operations within the Semantic Spacetime model, incorporating Promise Theory to examine boundary information and intentionality.

Result: The model ensures predictable pathways in graphs under minimal constraints, while highlighting how absorbing states naturally occur and affect information flow.

Conclusion: Semantic Spacetime provides a robust framework for managing complexity and predictability in knowledge graphs, particularly by addressing absorbing states and boundary information issues.

Abstract: Some formal aspects of the Semantic Spacetime graph model are presented, with
reference to its use for directed knowledge representations and process
modelling. A finite $\gamma(3,4)$ representation is defined to form a closed
set of operations that can scale to any degree of semantic complexity. The
Semantic Spacetime postulates bring predictability with minimal constraints to
pathways in graphs. The ubiquitous appearance of absorbing states in any
partial graph means that a graph process leaks information. The issue is
closely associated with the issue of division by zero, which signals a loss of
closure and the need for manual injection of remedial information. The Semantic
Spacetime model (and its Promise Theory) origins help to clarify how such
absorbing states are associated with boundary information where intentionality
can enter.

</details>


### [65] [A Proposal to Extend the Common Model of Cognition with Metacognition](https://arxiv.org/abs/2506.07807)
*John Laird,Christian Lebiere,Paul Rosenbloom,Andrea Stocco,Robert Wray*

Main category: cs.AI

TL;DR: The paper integrates metacognition into the Common Model of Cognition (CMC) by leveraging existing cognitive capabilities with minimal extensions.


<details>
  <summary>Details</summary>
Motivation: To create a unified approach for embedding metacognition in the CMC, simulating human-like reasoning over cognitive processes.

Method: The authors reason metacognitive processes using explicit cognitive representations in working memory, without significantly altering the CMC structure.

Result: Their proposal demonstrates examples of metacognitive reasoning implemented within the CMC framework.

Conclusion: The integration of metacognition into the CMC is feasible with minimal structural changes, enhancing the model's ability to simulate human-like cognition.

Abstract: The Common Model of Cognition (CMC) provides an abstract characterization of
the structure and processing required by a cognitive architecture for
human-like minds. We propose a unified approach to integrating metacognition
within the CMC. We propose that metacognition involves reasoning over explicit
representations of an agent's cognitive capabilities and processes in working
memory. Our proposal exploits the existing cognitive capabilities of the CMC,
making minimal extensions in the structure and information available within
working memory. We provide examples of metacognition within our proposal.

</details>


### [66] [Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation](https://arxiv.org/abs/2506.07820)
*Jiaxiang CHen,Zhuo Wang,Mingxi Zou,Qifan Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: The paper introduces Guideline Forest, a framework that improves reasoning in LLMs by leveraging structured, reusable reasoning strategies from verified examples.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with human-like reasoning, such as flexibility, adaptability, and being grounded in prior experience. Existing methods fail to efficiently utilize multiple reusable reasoning strategies.

Method: The Guideline Forest framework induces reasoning strategies called guidelines from verified examples. These guidelines create diverse thought variants, which are executed in parallel, refined via self-correction, and aggregated stepwise to resolve uncertainty and synthesize solutions.

Result: Guideline Forest surpasses existing baselines like CoT, ReAct, ToT, FoT, and AFlow in benchmarks for mathematical and programmatic reasoning. Ablation studies demonstrate its adaptability and effectiveness.

Conclusion: Guideline Forest showcases strong adaptability and generalization through reusable reasoning strategies that mimic human reasoning, offering improvements in LLM performance across reasoning-intensive tasks.

Abstract: Human reasoning is flexible, adaptive, and grounded in prior
experience-qualities that large language models (LLMs) still struggle to
emulate. Existing methods either explore diverse reasoning paths at inference
time or search for optimal workflows through expensive operations, but both
fall short in leveraging multiple reusable strategies in a structured,
efficient manner. We propose Guideline Forest, a framework that enhances LLMs
reasoning by inducing structured reasoning strategies-called guidelines-from
verified examples and executing them via step-wise aggregation. Unlike
test-time search or single-path distillation, our method draws on verified
reasoning experiences by inducing reusable guidelines and expanding each into
diverse variants. Much like human reasoning, these variants reflect alternative
thought patterns, are executed in parallel, refined via self-correction, and
aggregated step by step-enabling the model to adaptively resolve uncertainty
and synthesize robust solutions.We evaluate Guideline Forest on four
benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and
programmatic reasoning. Guideline Forest consistently outperforms strong
baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further
highlight the effectiveness of multi-path reasoning and stepwise aggregation,
underscoring the Guideline Forest's adaptability and generalization potential.

</details>


### [67] [Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs](https://arxiv.org/abs/2506.07824)
*Yao Yan*

Main category: cs.AI

TL;DR: The study examines the internal arithmetic processes of LLaMA-3-8B-Instruct using linear probing and logit-lens inspection, revealing a hierarchical computation mechanism for multi-digit addition.


<details>
  <summary>Details</summary>
Motivation: Understanding how large language models internally solve complex tasks like multi-digit addition, to assess their computational mechanisms.

Method: Linear probing and logit-lens inspection were applied to analyze four distinct computational stages during multi-digit addition within LLaMA-3-8B-Instruct.

Result: The model exhibits a coherent four-stage computational process, increasing clarity and accuracy through its activation layers until achieving reliable output generation.

Conclusion: The hierarchical process favors internal computation rather than rote memorization, showcasing the model's capacity for structured problem-solving.

Abstract: Multi-digit addition is a clear probe of the computational power of large
language models. To dissect the internal arithmetic processes in
LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.
Inspired by the step-by-step manner in which humans perform addition, we
propose and analyze a coherent four-stage trajectory in the forward
pass:Formula-structure representations become linearly decodable first, while
the answer token is still far down the candidate list.Core computational
features then emerge prominently.At deeper activation layers, numerical
abstractions of the result become clearer, enabling near-perfect detection and
decoding of the individual digits in the sum.Near the output, the model
organizes and generates the final content, with the correct token reliably
occupying the top rank.This trajectory suggests a hierarchical process that
favors internal computation over rote memorization. We release our code and
data to facilitate reproducibility.

</details>


### [68] [HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains](https://arxiv.org/abs/2506.07837)
*Shijie Wang,Yilun Zhang,Zeyu Lai,Dexing Kong*

Main category: cs.AI

TL;DR: This study addresses challenges in training multimodal large language models (MLLMs) in the specific domain of medical ultrasound by creating a novel image-text reasoning data generation pipeline to generate fine-tuning datasets, resulting in the ReMUD dataset and ReMUD-7B model.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of high-quality, domain-specific image-text data for training MLLMs, particularly in specialized fields like medical ultrasound, where existing data is unstandardized and inaccessible for direct use.

Method: The authors developed a pipeline that generates image-text reasoning data, including quadruplets (image, question, reasoning trace, and answer), sourced from disparate ultrasonic materials. This pipeline was applied to create ReMUD, a 45,000-instance dataset for QA and VQA tasks.

Result: The fine-tuned ReMUD-7B model demonstrated superior performance in the medical ultrasound domain compared to general-domain MLLMs.

Conclusion: The ReMUD dataset, associated codebase, and ReMUD-7B parameters provide a critical resource for advancing domain-specific MLLMs, solving the issue of limited data in the medical ultrasound domain.

Abstract: Multimodal large language models (MLLMs) have shown great potential in
general domains but perform poorly in some specific domains due to a lack of
domain-specific data, such as image-text data or vedio-text data. In some
specific domains, there is abundant graphic and textual data scattered around,
but lacks standardized arrangement. In the field of medical ultrasound, there
are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic
diagnostic reports, and so on. However, these ultrasonic materials are often
saved in the forms of PDF, images, etc., and cannot be directly used for the
training of MLLMs. This paper proposes a novel image-text reasoning supervised
fine-tuning data generation pipeline to create specific domain quadruplets
(image, question, thinking trace, and answer) from domain-specific materials. A
medical ultrasound domain dataset ReMUD is established, containing over 45,000
reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and
Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on
Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound
field. To facilitate research, the ReMUD dataset, data generation codebase, and
ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,
addressing the data shortage issue in specific domain MLLMs.

</details>


### [69] [A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms](https://arxiv.org/abs/2506.07853)
*Hudson de Martim*

Main category: cs.AI

TL;DR: The paper proposes a model to enhance fine-grained temporal tracking of legal norms.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of granular, time-aware legal norm modeling in existing frameworks like FRBR/FRBRoo and Akoma Ntoso.

Method: Extends FRBRoo using subclasses (TV, LV, CW, CTV, CLV) for detailed versioning of legal text components.

Result: Demonstrated the model's ability to track and reconstruct portions of Brazilian Federal Constitution as they existed at any point in time.

Conclusion: This model creates a foundation for advanced legal systems and tools, improving historical and impact analysis capabilities.

Abstract: Effectively representing legal norms for automated processing is a critical
challenge, particularly in tracking the diachronic evolution of their
hierarchical components (e.g., articles, paragraphs). While foundational
frameworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal
documents at a macro level, they lack native mechanisms for granular,
component-level versioning. This limitation hinders the deterministic
point-in-time reconstruction of legal texts, a fundamental capability for
reliable Legal Tech and AI applications. This paper proposes a structured,
temporal model that extends the FRBRoo framework to address this gap. It
introduces specialized subclasses of Expressio - Temporal Version (TV) and
Language Version (LV - to represent the state of a legal norm and its
linguistic variations at specific points in time. The model applies this same
paradigm hierarchically, introducing Component Work (CW), Component Temporal
Version (CTV), and Component Language Version (CLV) to track the lifecycle of
individual articles, paragraphs, and clauses. Using the Brazilian Federal
Constitution as a case study, the paper demonstrates how each amendment creates
new Component Temporal Versions for affected provisions, while unaffected
components retain their existing versions. This fine-grained, time-aware
architecture enables the precise, deterministic retrieval and reconstruction of
any part of a legal text as it existed on a specific date. The model provides a
robust foundation for developing advanced legal information systems, knowledge
graphs, and AI tools capable of accurate historical analysis and impact
assessment, overcoming the limitations of current generative models.

</details>


### [70] [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)
*Shoko Oka*

Main category: cs.AI

TL;DR: This paper investigates whether modern LLMs can address the Frame and Symbol Grounding problems using benchmark tasks, with certain models showing promising results.


<details>
  <summary>Details</summary>
Motivation: To reevaluate philosophical challenges (Frame Problem and Symbol Grounding Problem) within the context of advanced AI systems using LLMs.

Method: Designed benchmark tasks targeting the core philosophical issues, tested 13 LLMs in zero-shot settings, and assessed their outputs across criteria like contextual reasoning and semantic coherence.

Result: Closed models demonstrated consistently high performance, while open-source models showed more variability due to factors like size and tuning.

Conclusion: Select modern LLMs possess capacities that may effectively address longstanding philosophical AI challenges.

Abstract: Recent advancements in large language models (LLMs) have revitalized
philosophical debates surrounding artificial intelligence. Two of the most
fundamental challenges - namely, the Frame Problem and the Symbol Grounding
Problem - have historically been viewed as unsolvable within traditional
symbolic AI systems. This study investigates whether modern LLMs possess the
cognitive capacities required to address these problems. To do so, I designed
two benchmark tasks reflecting the philosophical core of each problem,
administered them under zero-shot conditions to 13 prominent LLMs (both closed
and open-source), and assessed the quality of the models' outputs across five
trials each. Responses were scored along multiple criteria, including
contextual reasoning, semantic coherence, and information filtering. The
results demonstrate that while open-source models showed variability in
performance due to differences in model size, quantization, and instruction
tuning, several closed models consistently achieved high scores. These findings
suggest that select modern LLMs may be acquiring capacities sufficient to
produce meaningful and stable responses to these long-standing theoretical
challenges.

</details>


### [71] [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: This paper introduces LUCIFER, a framework merging reinforcement learning (RL) and large language models (LLMs) to integrate real-time human contextual knowledge into autonomous decision-making.


<details>
  <summary>Details</summary>
Motivation: Agents in dynamic environments often struggle with outdated knowledge, limiting their decision-making capacity. Harnessing human contextual insights could help bridge this gap.

Method: LUCIFER uses a hierarchical architecture where LLMs serve as context extractors and zero-shot exploration guides, enhancing RL-based decision-making.

Result: Benchmarked against various policies, LUCIFER significantly boosts exploration efficiency and decision quality.

Conclusion: Integrating human context into autonomous systems improves decision-making, showcasing LUCIFER's potential for dynamic environments.

Abstract: In dynamic environments, the rapid obsolescence of pre-existing environmental
knowledge creates a gap between an agent's internal model and the evolving
reality of its operational context. This disparity between prior and updated
environmental valuations fundamentally limits the effectiveness of autonomous
decision-making. To bridge this gap, the contextual bias of human domain
stakeholders, who naturally accumulate insights through direct, real-time
observation, becomes indispensable. However, translating their nuanced, and
context-rich input into actionable intelligence for autonomous systems remains
an open challenge. To address this, we propose LUCIFER (Language Understanding
and Context-Infused Framework for Exploration and Behavior Refinement), a
domain-agnostic framework that integrates a hierarchical decision-making
architecture with reinforcement learning (RL) and large language models (LLMs)
into a unified system. This architecture mirrors how humans decompose complex
tasks, enabling a high-level planner to coordinate specialised sub-agents, each
focused on distinct objectives and temporally interdependent actions. Unlike
traditional applications where LLMs are limited to single role, LUCIFER
integrates them in two synergistic roles: as context extractors, structuring
verbal stakeholder input into domain-aware representations that influence
decision-making through an attention space mechanism aligning LLM-derived
insights with the agent's learning process, and as zero-shot exploration
facilitators guiding the agent's action selection process during exploration.
We benchmark various LLMs in both roles and demonstrate that LUCIFER improves
exploration efficiency and decision quality, outperforming flat,
goal-conditioned policies. Our findings show the potential of context-driven
decision-making, where autonomous systems leverage human contextual knowledge
for operational success.

</details>


### [72] [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
*Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: The paper introduces IneqMath, an expert-curated dataset and evaluation framework for testing LLMs on inequality proving tasks, revealing current models' poor performance in rigorous proof construction.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robust datasets and evaluation methods for inequality proving tasks, enabling better testing of LLMs' advanced reasoning skills in this domain.

Method: The authors created the IneqMath dataset with Olympiad-level inequality problems, step-wise solutions, and theorem annotations. They also designed a novel evaluation framework using LLMs as judges for both final answers and reasoning steps.

Result: An evaluation of 29 leading LLMs with the IneqMath dataset showed that even top models achieved less than 10% overall accuracy under step-wise scrutiny, revealing fragile deductive reasoning.

Conclusion: Current LLMs struggle with constructing rigorous proofs for inequality tasks, and progress should focus on approaches like theorem-guided reasoning and self-refinement rather than simply scaling models or computation.

Abstract: Inequality proving, crucial across diverse scientific and mathematical
fields, tests advanced reasoning skills such as discovering tight bounds and
strategic theorem application. This makes it a distinct, demanding frontier for
large language models (LLMs), offering insights beyond general mathematical
problem-solving. Progress in this area is hampered by existing datasets that
are often scarce, synthetic, or rigidly formal. We address this by proposing an
informal yet verifiable task formulation, recasting inequality proving into two
automatically checkable subtasks: bound estimation and relation prediction.
Building on this, we release IneqMath, an expert-curated dataset of
Olympiad-level inequalities, including a test set and training corpus enriched
with step-wise solutions and theorem annotations. We also develop a novel
LLM-as-judge evaluation framework, combining a final-answer judge with four
step-wise judges designed to detect common reasoning flaws. A systematic
evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even
top models like o1 achieve less than 10% overall accuracy under step-wise
scrutiny; this is a drop of up to 65.5% from their accuracy considering only
final answer equivalence. This discrepancy exposes fragile deductive chains and
a critical gap for current LLMs between merely finding an answer and
constructing a rigorous proof. Scaling model size and increasing test-time
computation yield limited gains in overall proof correctness. Instead, our
findings highlight promising research directions such as theorem-guided
reasoning and self-refinement. Code and data are available at
https://ineqmath.github.io/.

</details>


### [73] [Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation](https://arxiv.org/abs/2506.07940)
*Christopher Subia-Waud*

Main category: cs.AI

TL;DR: The paper introduces Gradients, a decentralized AutoML platform, transforming hyperparameter optimization into a competitive marketplace driven by economic incentives, achieving significant performance improvements over centralized methods.


<details>
  <summary>Details</summary>
Motivation: Traditional AutoML platforms are limited by single optimization strategies, which restrict the exploration of hyperparameter spaces, resulting in suboptimal model fine-tuning.

Method: Gradients uses a decentralized marketplace model where independent miners compete to find optimal hyperparameter configurations, aligned with collective optimization goals via economic incentives.

Result: Across 180 experiments, Gradients demonstrated superior performance, achieving an 82.8% win rate against HuggingFace AutoTrain and 100% against platforms like TogetherAI and Google Cloud, with 11.8%-42.1% improvements in various tasks.

Conclusion: Competitive, economically-driven decentralized approaches can uncover hyperparameter configurations that traditional centralized platforms fail to explore, leading to better performance.

Abstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML
platforms rely on single optimisation strategies that explore only a fraction
of viable hyperparameter configurations. In this white paper, We introduce
Gradients, a decentralised AutoML platform that transforms hyperparameter
optimisation into a competitive marketplace where independent miners compete to
discover optimal configurations. Economic incentives align individual
exploration with collective optimisation goals, driving systematic
investigation of hyperparameter regions that centralised methods miss. We
evaluate our approach across 180 controlled experiments spanning diverse model
architectures (70M to 70B parameters) and task types. Gradients achieves an
82.8\% win rate against HuggingFace AutoTrain and 100\% against TogetherAI,
Databricks, and Google Cloud, with mean improvements of 11.8\% and 42.1\%
respectively. Complex reasoning and retrieval tasks show particularly strong
gains of 30-40\%, whilst diffusion models achieve 23.4\% improvements for
person-specific generation. These results demonstrate that competitive,
economically-driven approaches can systematically discover superior
configurations that centralised AutoML consistently miss.

</details>


### [74] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
*Jixiang Hong,Yiran Zhang,Guanzhong Wang,Yi Liu,Ji-Rong Wen,Rui Yan*

Main category: cs.AI

TL;DR: This paper proposes a self-supervised dual reward mechanism for improving image-text alignment in large multimodal models (LMMs).


<details>
  <summary>Details</summary>
Motivation: Address limitations of LMMs in achieving accurate image-text coherence and reliance on external supervision.

Method: Introduce self-supervised dual reward mechanism based on inverse dual tasks of understanding and generation.

Result: Enhanced performance on visual understanding and generation tasks, with significant progress in text-to-image benchmarks.

Conclusion: Self-supervised dual rewards improve LMM capabilities without needing external input, showcasing the mechanism's effectiveness.

Abstract: Building upon large language models (LLMs), recent large multimodal models
(LMMs) unify cross-model understanding and generation into a single framework.
However, LMMs still struggle to achieve accurate image-text alignment, prone to
generating text responses contradicting the visual input or failing to follow
the text-to-image prompts. Current solutions require external supervision
(e.g., human feedback or reward models) and only address unidirectional
tasks-either understanding or generation. In this work, based on the
observation that understanding and generation are inverse dual tasks, we
introduce a self-supervised dual reward mechanism to reinforce the
understanding and generation capabilities of LMMs. Specifically, we sample
multiple outputs for a given input in one task domain, then reverse the
input-output pairs to compute the dual likelihood of the model as self-rewards
for optimization. Extensive experimental results on visual understanding and
generation benchmarks demonstrate that our method can effectively enhance the
performance of the model without any external supervision, especially achieving
remarkable improvements in text-to-image tasks.

</details>


### [75] [$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)
*Victor Barres,Honghua Dong,Soham Ray,Xujie Si,Karthik Narasimhan*

Main category: cs.AI

TL;DR: The paper introduces $	au^2$-bench, a benchmark for conversational AI in shared dynamic environments with dual control, addressing gaps in existing benchmarks where users passively provide information.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to reflect real-world scenarios where users actively participate in modifying shared environments, emphasizing the need for a shared, dual-control simulation.

Method: The paper proposes a Telecom dual-control domain modeled as a Dec-POMDP, a compositional task generator, a reliable user simulator constrained by tools and states, and fine-grained performance analysis.

Result: Experiments show significant performance drop when agents shift from single-control to dual-control setups, highlighting difficulties in coordinating and guiding users effectively.

Conclusion: $	au^2$-bench establishes a testbed for developing and analyzing agents capable of reasoning and guiding user actions in dynamic shared environments.

Abstract: Existing benchmarks for conversational AI agents simulate single-control
environments, where only the AI agent can use tools to interact with the world,
while the user remains a passive information provider. This differs from
real-world scenarios like technical support, where users need to actively
participate in modifying the state of the (shared) world. In order to address
this gap, we introduce $\tau^2$-bench, with four key contributions:
  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both
agent and user make use of tools to act in a shared, dynamic environment that
tests both agent coordination and communication,
  2) A compositional task generator that programmatically creates diverse,
verifiable tasks from atomic components, ensuring domain coverage and
controlled complexity,
  3) A reliable user simulator tightly coupled with the environment, whose
behavior is constrained by tools and observable states, improving simulation
fidelity,
  4) Fine-grained analysis of agent performance through multiple ablations
including separating errors arising from reasoning vs
communication/coordination.
  In particular, our experiments show significant performance drops when agents
shift from no-user to dual-control, highlighting the challenges of guiding
users. Overall, $\tau^2$-bench provides a controlled testbed for agents that
must both reason effectively and guide user actions.

</details>


### [76] [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior](https://arxiv.org/abs/2506.08012)
*Penghao Wu,Shengnan Ma,Bo Wang,Jiaheng Yu,Lewei Lu,Ziwei Liu*

Main category: cs.AI

TL;DR: GUI-Reflection introduces self-reflection and error correction to multimodal GUI models with novel training stages and automated systems.


<details>
  <summary>Details</summary>
Motivation: Existing GUI automation models lack capabilities for reflection and error recovery, creating limitations in adaptability and robustness.

Method: The framework uses GUI-specific pre-training, supervised fine-tuning, online reflection tuning, automated data pipelines, reflection tasks, and diverse mobile environments.

Result: GUI models gain self-reflection and error correction abilities through scalable pipelines and iterative online learning environments.

Conclusion: GUI-Reflection enhances the robustness, adaptability, and intelligence of GUI automation, contributing to advanced user interface interactions.

Abstract: Multimodal Large Language Models (MLLMs) have shown great potential in
revolutionizing Graphical User Interface (GUI) automation. However, existing
GUI models mostly rely on learning from nearly error-free offline trajectories,
thus lacking reflection and error recovery capabilities. To bridge this gap, we
propose GUI-Reflection, a novel framework that explicitly integrates
self-reflection and error correction capabilities into end-to-end multimodal
GUI models throughout dedicated training stages: GUI-specific pre-training,
offline supervised fine-tuning (SFT), and online reflection tuning.
GUI-reflection enables self-reflection behavior emergence with fully automated
data generation and learning processes without requiring any human annotation.
Specifically, 1) we first propose scalable data pipelines to automatically
construct reflection and error correction data from existing successful
trajectories. While existing GUI models mainly focus on grounding and UI
understanding ability, we propose the GUI-Reflection Task Suite to learn and
evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a
diverse and efficient environment for online training and data collection of
GUI models on mobile devices. 3) We also present an iterative online reflection
tuning algorithm leveraging the proposed environment, enabling the model to
continuously enhance its reflection and error correction abilities. Our
framework equips GUI agents with self-reflection and correction capabilities,
paving the way for more robust, adaptable, and intelligent GUI automation, with
all data, models, environments, and tools to be released publicly.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [77] [Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor](https://arxiv.org/abs/2506.06773)
*Emet Behrendt,Shing Wai Pun,Prashant J. Nair*

Main category: cs.AR

TL;DR: The paper addresses branch prediction challenges in processors by introducing a subsystem called the Bullseye predictor, which improves performance on hard-to-predict branches when integrated with TAGE-SC-L.


<details>
  <summary>Details</summary>
Motivation: Branch mispredictions are a critical limitation in the performance of out-of-order processors, with many mispredictions stemming from hard-to-predict branches, which current methods struggle to handle effectively.

Method: The authors add a 28 KB subsystem called the Bullseye predictor to the 159 KB TAGE-SC-L predictor. This involves using an H2P Identification Table to detect problematic branches, employing perceptrons for prediction, and managing updates dynamically to avoid pollution.

Result: The combined approach achieves an average mispredictions-per-thousand-instructions (MPKI) of 3.4045 and a weighted cycle cost per thousand instructions (CycWpPKI) of 145.09, demonstrating improved fidelity for hard-to-predict branches.

Conclusion: Integrating the Bullseye predictor with TAGE-SC-L enhances prediction performance on challenging branches, offering a solution to mitigate performance barriers caused by these mispredictions.

Abstract: Branch prediction is key to the performance of out-of-order processors. While
the CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical
corrector, and a loop predictor, over half of its remaining mispredictions stem
from a small set of hard-to-predict (H2P) branches. These branches occur under
diverse global histories, causing repeated thrashing in TAGE and eviction
before usefulness counters can mature. Prior work shows that simply enlarging
the tables offers only marginal improvement.
  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem
called the Bullseye predictor. It identifies problematic PCs using a
set-associative H2P Identification Table (HIT) and steers them to one of two
branch-specific perceptrons, one indexed by hashed local history and the other
by folded global history. A short trial phase tracks head-to-head accuracy in
an H2P cache. A branch becomes perceptron-resident only if the perceptron's
sustained accuracy and output magnitude exceed dynamic thresholds, after which
TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,
and perceptron operate fully in parallel with TAGE-SC-L, providing higher
fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI
of 145.09.

</details>


### [78] [Design and Implementation of a RISC-V SoC with Custom DSP Accelerators for Edge Computing](https://arxiv.org/abs/2506.06693)
*Priyanshu Yadav*

Main category: cs.AR

TL;DR: This paper analyzes RISC-V's architecture, focusing on its modularity, performance, and power efficiency, compared to ARM Cortex-M0.


<details>
  <summary>Details</summary>
Motivation: To explore the modular design, implementation challenges, and performance aspects of RISC-V architecture for its suitability in embedded systems and custom accelerators.

Method: Cycle-accurate simulations of a pipelined RISC-V implementation (RV32I with M and A extensions) are used to evaluate performance metrics like CPI and power efficiency.

Result: The study shows RISC-V reduces power consumption by 17% compared to ARM Cortex-M0, with significant flexibility for domain-specific optimizations.

Conclusion: RISC-V's open-standard and modular architecture make it highly efficient and scalable, especially for embedded systems and custom accelerators.

Abstract: This paper presents a comprehensive analysis of the RISC-V instruction set
architecture, focusing on its modular design, implementation challenges, and
performance characteristics. We examine the RV32I base instruction set with
extensions for multiplication (M) and atomic operations (A). Through
cycle-accurate simulation of a pipelined implementation, we evaluate
performance metrics including CPI (cycles per instruction) and power
efficiency. Our results demonstrate RISC-V's advantages in embedded systems and
its scalability for custom accelerators. Comparative analysis shows a 17%
reduction in power consumption compared to ARM Cortex-M0 implementations in
similar process nodes. The open-standard nature of RISC-V provides significant
flexibility for domain-specific optimizations.

</details>


### [79] [ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors](https://arxiv.org/abs/2506.06817)
*Haoran Wu,Ce Guo,Wayne Luk,Robert Mullins*

Main category: cs.AR

TL;DR: The paper proposes ASPO, an enhanced Bayesian Optimization method that handles constraints with categorical parameters and improves optimization speed for FPGA-based soft processors.


<details>
  <summary>Details</summary>
Motivation: To address limitations in standard Bayesian Optimization when tuning processor design parameters, particularly handling categorical constraints and reducing optimization time for complex FPGA-based soft processors.

Method: ASPO introduces a disjunctive form-based approach, a custom BO covariance kernel for categorical parameters, penalizes BO's acquisition function with evaluation time, and reuses FPGA synthesis checkpoints.

Result: ASPO significantly improves execution time for benchmarks (e.g., reducing BOOM's 'multiply' benchmark time by 35%) and cuts design time for processors by up to 74% compared to existing methods.

Conclusion: ASPO efficiently tackles challenges in optimizing soft processor designs on FPGAs, showcasing improved performance and reduced design times while addressing categorical parameter constraints.

Abstract: Bayesian Optimization (BO) has shown promise in tuning processor design
parameters. However, standard BO does not support constraints involving
categorical parameters such as types of branch predictors and division
circuits. In addition, optimization time of BO grows with processor complexity,
which becomes increasingly significant especially for FPGA-based soft
processors. This paper introduces ASPO, an approach that leverages disjunctive
form to enable BO to handle constraints involving categorical parameters.
Unlike existing methods that directly apply standard BO, the proposed ASPO
method, for the first time, customizes the mathematical mechanism of BO to
address challenges faced by soft-processor designs on FPGAs. Specifically, ASPO
supports categorical parameters using a novel customized BO covariance kernel.
It also accelerates the design evaluation procedure by penalizing the BO
acquisition function with potential evaluation time and by reusing FPGA
synthesis checkpoints from previously evaluated configurations. ASPO targets
three soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is
evaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce
execution time for the ``multiply'' benchmark on the BOOM processor by up to
35\% compared to the default configuration. Furthermore, it reduces design time
for the BOOM processor by up to 74\% compared to Boomerang, a state-of-the-art
hardware-oriented BO approach.

</details>


### [80] [Containerized In-Storage Processing and Computing-Enabled SSD Disaggregation](https://arxiv.org/abs/2506.06769)
*Miryeong Kwon,Donghyun Gouk,Eunjee Na,Jiseon Kim,Junhee Kim,Hyein Woo,Eojin Ryu,Hyunkyu Choi,Jinwoo Baek,Hanyeoreum Bae,Mahmut Kandemir,Myoungsoo Jung*

Main category: cs.AR

TL;DR: DockerSSD redefines ISP with virtualization and lightweight firmware for efficient containerized data processing, resulting in significant performance boosts.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in Integrated Storage Processing (ISP), particularly adaptation and disaggregation.

Method: DockerSSD uses OS-level virtualization and lightweight firmware, incorporating Ethernet over NVMe for management and Virtual Firmware for secure container execution.

Result: The proposed solution delivers up to 2.0x improved performance for I/O-heavy tasks and a 7.9x boost in distributed large-language-model (LLM) inference.

Conclusion: By enabling containerized data processing on SSDs, DockerSSD optimizes ISP for disaggregated systems, advancing storage and analytics integration.

Abstract: ISP minimizes data transfer for analytics but faces challenges in adaptation
and disaggregation. We propose DockerSSD, an ISP model leveraging OS-level
virtualization and lightweight firmware to enable containerized data processing
directly on SSDs. Key features include Ethernet over NVMe for network-based ISP
management and Virtual Firmware for secure, efficient container execution.
DockerSSD supports disaggregated storage pools, reducing host overhead and
enhancing large-scale services like LLM inference. It achieves up to 2.0x
better performance for I/O-intensive workloads, and 7.9x improvement in
distributed LLM inference.

</details>


### [81] [QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine](https://arxiv.org/abs/2506.07046)
*Anushka Jha,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: The paper introduces QForce-RL, a reinforcement learning framework utilizing quantization to improve computational efficiency while maintaining performance, demonstrated to outperform state-of-the-art methods in terms of energy efficiency and computational throughput.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the resource-intensive nature of FPGA deployment for reinforcement learning applications, particularly in managing high-quality image-based computations and dynamic environments.

Method: The study employs a lightweight RL architecture called QForce-RL, leveraging two techniques: E2HRL to optimize RL action efficiency and QuaRL for quantization-based SIMD hardware acceleration.

Result: The proposed framework delivers up to 2.6x better FPS and 2.3x performance improvement compared to existing state-of-the-art solutions across various RL environments.

Conclusion: QForce-RL proves to be a scalable and flexible RL solution suitable for resource-constrained devices, balancing trade-offs in latency, throughput, power, and energy efficiency without degrading performance.

Abstract: Reinforcement Learning (RL) has outperformed other counterparts in sequential
decision-making and dynamic environment control. However, FPGA deployment is
significantly resource-expensive, as associated with large number of
computations in training agents with high-quality images and possess new
challenges. In this work, we propose QForce-RL takes benefits of quantization
to enhance throughput and reduce energy footprint with light-weight RL
architecture, without significant performance degradation. QForce-RL takes
advantages from E2HRL to reduce overall RL actions to learn desired policy and
QuaRL for quantization based SIMD for hardware acceleration. We have also
provided detailed analysis for different RL environments, with emphasis on
model size, parameters, and accelerated compute ops. The architecture is
scalable for resource-constrained devices and provide parametrized efficient
deployment with flexibility in latency, throughput, power, and energy
efficiency. The proposed QForce-RL provides performance enhancement up to 2.3x
and better FPS - 2.6x compared to SoTA works.

</details>


### [82] [MAGNet: A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection](https://arxiv.org/abs/2506.07126)
*Weihan Lu,Hong Cai Chen*

Main category: cs.AR

TL;DR: MAGNet is a hybrid deep learning model combining enhanced U-Net and graph neural network for accurate and efficient DRC violation prediction in IC designs.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of design rule checking (DRC) violations in integrated circuit (IC) designs by leveraging machine learning techniques.

Method: MAGNet integrates an improved U-Net with Dynamic Attention Module and Multi-Scale Convolution Module for spatial feature extraction, along with a pixel-aligned graph neural network to capture topological relationships. A graph-to-grid mapping and label amplification strategy enhance its performance.

Result: MAGNet achieves higher prediction accuracy and lower false positive rates for DRC hotspot detection compared to ibUnet, RouteNet, and J-Net models.

Conclusion: MAGNet offers an effective combination of spatial, semantic, and structural analysis, setting a new benchmark in DRC violation detection models through improved prediction capabilities and sensitivity in incremental training.

Abstract: Design rule checking (DRC) is of great significance for cost reduction and
design efficiency improvement in integrated circuit (IC) designs.
Machine-learning-based DRC has become an important approach in computer-aided
design (CAD). In this paper, we propose MAGNet, a hybrid deep learning model
that integrates an improved U-Net with a graph neural network for DRC violation
prediction. The U-Net backbone is enhanced with a Dynamic Attention Module
(DAM) and a Multi-Scale Convolution Module (MSCM) to strengthen its capability
in extracting fine-grained and multi-scale spatial features. In parallel, we
construct a pixel-aligned graph structure based on chip layout tiles, and apply
a specialized GNN to model the topological relationships among pins. During
graph construction, a graph-to-grid mapping is generated to align GNN features
with the layout image. In addition, a label amplification strategy is adopted
during training to enhance the model's sensitivity to sparse violation
patterns. Overall, MAGNet effectively combines spatial, semantic, and
structural information, achieving improved prediction accuracy and reduced
false positive rates in DRC hotspot detection. Subsequently, through
incremental training, we achieve a more sensitive discrimination ability for
hotspots. The results demonstrate that, in comparison with ibUnet, RouteNet,
and J-Net, MAGnet significantly outperforms these models, achieving substantial
improvements in overall performance.

</details>


### [83] [VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code](https://arxiv.org/abs/2506.07239)
*Raghu Vamshi Hemadri,Jitendra Bhandari,Johann Knechtel,Badri P Gopalan,Ramesh Narayanaswamy,Ramesh Karri,Siddharth Garg*

Main category: cs.AR

TL;DR: The paper introduces VeriLoC, a method for predicting design quality directly from Verilog code at both line- and module-level, achieving significant performance improvements over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of chip design has created the demand for early-stage predictions of design quality metrics, such as timing and routing congestion, directly from Verilog code. Current methods do not address line-level prediction, which is crucial for identifying specific problems in the design.

Method: VeriLoC employs modern Verilog code-generation large language models (LLMs) to generate embeddings for both line-level and module-level analyses. These embeddings are used to train classifiers/regressors for prediction.

Result: VeriLoC demonstrates high F1-scores of 0.86-0.95 for predicting line-level issues like congestion and timing violations, and significantly reduces the mean average percentage error from 14%-18% to 4% compared to state-of-the-art methods.

Conclusion: VeriLoC provides an effective solution for predicting line-level and module-level design quality metrics directly from Verilog code, showing promising results for hardware design optimization and prediction tasks.

Abstract: Modern chip design is complex, and there is a crucial need for early-stage
prediction of key design-quality metrics like timing and routing congestion
directly from Verilog code (a commonly used programming language for hardware
design). It is especially important yet complex to predict individual lines of
code that cause timing violations or downstream routing congestion. Prior works
have tried approaches like converting Verilog into an intermediate graph
representation and using LLM embeddings alongside other features to predict
module-level quality, but did not consider line-level quality prediction. We
propose VeriLoC, the first method that predicts design quality directly from
Verilog at both the line- and module-level. To this end, VeriLoC leverages
recent Verilog code-generation LLMs to extract local line-level and
module-level embeddings, and train downstream classifiers/regressors on
concatenations of these embeddings. VeriLoC achieves high F1-scores of
0.86-0.95 for line-level congestion and timing prediction, and reduces the mean
average percentage error from 14% - 18% for SOTA methods down to only 4%. We
believe that VeriLoC embeddings and insights from our work will also be of
value for other predictive and optimization tasks for complex hardware design.

</details>


### [84] [A Survey on LUT-based Deep Neural Networks Implemented in FPGAs](https://arxiv.org/abs/2506.07367)
*Zeyu Guo*

Main category: cs.AR

TL;DR: The paper reviews LUT-based DNN architectures for edge applications, focusing on leveraging FPGA lookup tables to reduce latency and improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional FPGA-based DNN designs depend on DSP blocks for MAC operations, limiting scalability, highlighting the need for LUT-based approaches.

Method: The study surveys LUT-based architectures, exploring design methods, evolution, and performance trade-offs.

Result: The survey identifies improved resource utilization and reduced inference latency using LUT-based DNNs.

Conclusion: LUT-based DNN architectures are promising for edge applications, with opportunities for advancing design methodologies and addressing trade-offs.

Abstract: Low-latency, energy-efficient deep neural networks (DNNs) inference are
critical for edge applications, where traditional cloud-based deployment
suffers from high latency and security risks. Field-Programmable Gate Arrays
(FPGAs) offer a compelling solution, balancing reconfigurability, power
efficiency, and real-time performance. However, conventional FPGA-based DNNs
rely heavily on digital signal processing (DSP) blocks for multiply-accumulate
(MAC) operations, limiting scalability.
  LUT-based DNNs address this challenge by fully leveraging FPGA lookup tables
(LUTs) for computation, improving resource utilization and reducing inference
latency. This survey provides a comprehensive review of LUT-based DNN
architectures, including their evolution, design methodologies, and performance
trade-offs, while outlining promising directions for future research.

</details>


### [85] [FREESS: An Educational Simulator of a RISC-V-Inspired Superscalar Processor Based on Tomasulo's Algorithm](https://arxiv.org/abs/2506.07665)
*Roberto Giorgi*

Main category: cs.AR

TL;DR: FREESS is a free and open-source simulator for hands-on learning about superscalar processors, emphasizing out-of-order instruction execution.


<details>
  <summary>Details</summary>
Motivation: The paper aims to provide students and educators with an interactive tool to enhance understanding of complex CPU architectures, particularly instruction-level parallelism in superscalar processors.

Method: The simulator uses a minimal RISC-V-inspired instruction set and incorporates extended Tomasulo's algorithm. It models key microarchitectural components and allows customization of runtime parameters for flexibility.

Result: FREESS demonstrates parallel instruction execution through detailed visualizations and examples, enabling users to interactively explore superscalar architecture concepts.

Conclusion: FREESS serves as an effective, open-source educational tool for teaching and experimentation in advanced computer architecture topics.

Abstract: FREESS is a free, interactive simulator that illustrates instruction-level
parallelism in a RISC-V-inspired superscalar processor. Based on an extended
version of Tomasulo's algorithm, FREESS is intended as a hands-on educational
tool for Advanced Computer Architecture courses. It enables students to explore
dynamic, out-of-order instruction execution, emphasizing how instructions are
issued as soon as their operands become available.
  The simulator models key microarchitectural components, including the
Instruction Window (IW), Reorder Buffer (ROB), Register Map (RM), Free Pool
(FP), and Load/Store Queues. FREESS allows users to dynamically configure
runtime parameters, such as the superscalar issue width, functional unit types
and latencies, and the sizes of architectural buffers and queues.
  To simplify learning, the simulator uses a minimal instruction set inspired
by RISC-V (ADD, ADDI, BEQ, BNE, LW, MUL, SW), which is sufficient to
demonstrate key pipeline stages: fetch, register renaming, out-of-order
dispatch, execution, completion, commit, speculative branching, and memory
access. FREESS includes three step-by-step, illustrated examples that visually
demonstrate how multiple instructions can be issued and executed in parallel
within a single cycle. Being open source, FREESS encourages students and
educators to experiment freely by writing and analyzing their own
instruction-level programs and superscalar architectures.

</details>


### [86] [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
*Arnav Sheth,Ivaxi Sheth,Mario Fritz*

Main category: cs.AR

TL;DR: This paper evaluates the state-of-the-art large language models (LLMs) for their capability to generate synthesizable SystemVerilog designs for four key communication protocols: SPI, I2C, UART, and AXI.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in LLMs for code generation, their application to hardware description languages (HDLs) and synthesizable designs has been underexplored. The paper aims to address this gap.

Method: The authors introduce a benchmark suite targeting four communication protocols (SPI, I2C, UART, AXI), define code generation tasks with varying abstraction, and assess the generated SystemVerilog code for correctness, synthesizability, and functionality through simulation.

Result: The paper provides analysis and insights into the capabilities of LLMs in HDL code generation, evaluating their outputs using simulation and test bench-based verification.

Conclusion: LLMs show potential in generating HDL code, but challenges remain in ensuring adherence to timing semantics, concurrency, and synthesizability constraints for complex protocols.

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
capabilities in generating code for general-purpose programming languages. In
contrast, their applicability for hardware description languages, particularly
for generating synthesizable and functionally correct designs, remains
significantly underexplored. HDLs such as SystemVerilog are logic-oriented and
demand strict adherence to timing semantics, concurrency, and synthesizability
constraints. Moreover, HDL-based design flows encompass a broad set of tasks
beyond structural code generation, including testbench development,
assertion-based verification, timing closure, and protocol-level integration
for on-chip communication. The objective of our paper is to analyze the
capabilities of state-of-the-art LLMs in generating SystemVerilog
implementations of standard communication protocols, a core component of
embedded and System-on-Chip (SoC) architectures. This paper introduces the
first benchmark suite targeting four widely used protocols: SPI, I2C, UART, and
AXI. We define code generation tasks that capture varying levels of design
abstraction and prompt specificity. The generated designs are assessed for
syntactic correctness, synthesizability, and functional fidelity via waveform
simulation and test benches.

</details>


### [87] [Understanding the Error Sensitivity of Privacy-Aware Computing](https://arxiv.org/abs/2506.07957)
*Matías Mazzanti,Esteban Mocskos,Augusto Vega,Pradip Bose*

Main category: cs.AR

TL;DR: This paper studies the robustness and error sensitivity of homomorphic encryption (HE), focusing on the CKKS scheme and its vulnerability to hardware and software errors.


<details>
  <summary>Details</summary>
Motivation: HE provides secure computation on encrypted data, becoming crucial for privacy-sensitive domains like healthcare and finance. However, its reliance on noise for security introduces sensitivity to errors, raising the need to study its fault tolerance and robustness.

Method: The authors examine bit fault sensitivity in CKKS, a widely-used HE scheme for AI and ML. They also analyze the impact of optimization techniques like residue number system (RNS) and number theoretic transform (NTT) on error sensitivity.

Result: The study highlights how hardware- and software-induced errors can affect CKKS, revealing vulnerabilities that evade traditional error correction mechanisms.

Conclusion: This is the first detailed look into the robustness of HE, emphasizing the importance of addressing fault tolerance issues for future developments.

Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data
without decryption, allowing a great opportunity for privacy-preserving
computation. In particular, domains such as healthcare, finance, and
government, where data privacy and security are of utmost importance, can
benefit from HE by enabling third-party computation and services on sensitive
data. In other words, HE constitutes the "Holy Grail" of cryptography: data
remains encrypted all the time, being protected while in use.
  HE's security guarantees rely on noise added to data to make relatively
simple problems computationally intractable. This error-centric intrinsic HE
mechanism generates new challenges related to the fault tolerance and
robustness of HE itself: hardware- and software-induced errors during HE
operation can easily evade traditional error detection and correction
mechanisms, resulting in silent data corruption (SDC).
  In this work, we motivate a thorough discussion regarding the sensitivity of
HE applications to bit faults and provide a detailed error characterization
study of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes
due to its fixed-point arithmetic support for AI and machine learning
applications. We also delve into the impact of the residue number system (RNS)
and the number theoretic transform (NTT), two widely adopted HE optimization
techniques, on CKKS' error sensitivity. To the best of our knowledge, this is
the first work that looks into the robustness and error sensitivity of
homomorphic encryption and, as such, it can pave the way for critical future
work in this area.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [88] [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)
*Qiming Zeng,Xiao Yan,Hao Luo,Yuhao Lin,Yuxiang Wang,Fangcheng Fu,Bo Du,Quanqing Xu,Jiawei Jiang*

Main category: cs.CL

TL;DR: Current evaluation frameworks for graph-based retrieval-augmented generation (GraphRAG) methods have significant flaws affecting performance conclusions. An unbiased evaluation framework is proposed for better assessments.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for GraphRAG are flawed due to unrelated questions and biases, leading to unreliable performance evaluations.

Method: The authors create an unbiased evaluation framework using graph-text-grounded question generation and an unbiased scoring procedure to better assess GraphRAG methods.

Result: The authors evaluated 3 GraphRAG methods using their framework and discovered their performance gains were much less pronounced than previously reported.

Conclusion: Although the new framework still has potential flaws, its introduction emphasizes the need for rigorous evaluation methods to ensure foundational reliability in GraphRAG research.

Abstract: By retrieving contexts from knowledge graphs, graph-based retrieval-augmented
generation (GraphRAG) enhances large language models (LLMs) to generate quality
answers for user questions. Many GraphRAG methods have been proposed and
reported inspiring performance in answer quality. However, we observe that the
current answer evaluation framework for GraphRAG has two critical flaws, i.e.,
unrelated questions and evaluation biases, which may lead to biased or even
wrong conclusions on performance. To tackle the two flaws, we propose an
unbiased evaluation framework that uses graph-text-grounded question generation
to produce questions that are more related to the underlying dataset and an
unbiased evaluation procedure to eliminate the biases in LLM-based answer
assessment. We apply our unbiased framework to evaluate 3 representative
GraphRAG methods and find that their performance gains are much more moderate
than reported previously. Although our evaluation framework may still have
flaws, it calls for scientific evaluations to lay solid foundations for
GraphRAG research.

</details>


### [89] [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
*Taesoo Kim,Jong Hwan Ko*

Main category: cs.CL

TL;DR: TESU-LLM is a novel framework enabling speech-capable language models using only text data, avoiding reliance on large-scale paired datasets.


<details>
  <summary>Details</summary>
Motivation: Most speech-enabled models require large-scale paired data and heavy computational resources, limiting scalability and accessibility.

Method: Utilizing a unified encoder to map text and speech to a shared latent space and aligning it with a LLM embedding space via a projection network for text-only supervised training.

Result: TESU-LLM achieves strong performance in speech-related benchmarks, comparable to methods reliant on multimodal datasets and extensive resources.

Conclusion: The approach provides an effective and efficient path to build speech-capable language models without the need for speech data or heavy computational resources.

Abstract: Recent advances in speech-enabled language models have shown promising
results in building intelligent voice assistants. However, most existing
approaches rely on large-scale paired speech-text data and extensive
computational resources, which pose challenges in terms of scalability and
accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework
that enables training speech-capable language models using only text data. Our
key insight is to leverage a unified encoder that maps semantically equivalent
text and speech inputs to a shared latent space. By aligning the encoder output
with the embedding space of a LLM via a lightweight projection network, we
enable the model to generalize from text-only supervision to speech-based
inference. Despite being trained exclusively on text, TESU-LLM achieves strong
performance on various speech-related benchmarks, comparable to baseline
methods trained with large-scale multimodal datasets and substantial
computational resources. These results highlight the effectiveness and
efficiency of our approach, offering a scalable path toward building speech
LLMs without speech data.

</details>


### [90] [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)
*Zachary Yang,Domenico Tullo,Reihaneh Rabbany*

Main category: cs.CL

TL;DR: The paper focuses on enhancing toxicity detection in gaming communities using a scalable model with game-context tokens and extending it to multiple languages.


<details>
  <summary>Details</summary>
Motivation: Toxicity in gaming communities is a persistent issue, and there is a need for scalable, real-time solutions across multiple games and languages.

Method: The authors propose a soft-prompting method for game-specific contexts and an LLM-assisted label transfer framework for multilingual support.

Result: The model shows strong performance in detecting toxicity across various languages, outperforming benchmarks in some cases, and successfully operates in real-world gaming platforms.

Conclusion: The proposed approach improves scalability, reduces resource usage, and enhances the ability to detect toxic players across different games and languages in production environments.

Abstract: Toxicity detection in gaming communities faces significant scaling challenges
when expanding across multiple games and languages, particularly in real-time
environments where computational efficiency is crucial. We present two key
findings to address these challenges while building upon our previous work on
ToxBuster, a BERT-based real-time toxicity detection system. First, we
introduce a soft-prompting approach that enables a single model to effectively
handle multiple games by incorporating game-context tokens, matching the
performance of more complex methods like curriculum learning while offering
superior scalability. Second, we develop an LLM-assisted label transfer
framework using GPT-4o-mini to extend support to seven additional languages.
Evaluations on real game chat data across French, German, Portuguese, and
Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with
particularly strong performance in German, surpassing the English benchmark of
45.39%. In production, this unified approach significantly reduces
computational resources and maintenance overhead compared to maintaining
separate models for each game and language combination. At Ubisoft, this model
successfully identifies an average of 50 players, per game, per day engaging in
sanctionable behavior.

</details>


### [91] [Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models](https://arxiv.org/abs/2506.06371)
*Panagiotis Koletsis,Christos Panagiotopoulos,Georgios Th. Papadopoulos,Vasilis Efthymiou*

Main category: cs.CL

TL;DR: This paper introduces a hybrid approach using large language models (LLMs) and statistical techniques to detect relationships among columns in unlabeled tabular data by leveraging a Knowledge Graph (KG). It proves competitive on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation for this work is to improve table interpretation tasks by leveraging new technologies and benchmarks, focusing particularly on column-relationship detection in unlabeled tabular data.

Method: The paper combines large language models (LLMs) with statistical methods to reduce the search space for potential Knowledge Graph (KG) relations. Key modules for this include domain-range constraint detection and relation co-appearance analysis. It evaluates various state-of-the-art LLMs and prompting techniques.

Result: The experimental evaluation on SemTab challenge datasets demonstrates the effectiveness of the modules in reducing the search space and the accuracy of multiple state-of-the-art LLMs. The methodology performs competitively.

Conclusion: The proposed hybrid approach improves the detection of relationships in tabular data by combining LLMs and statistical analysis, achieving state-of-the-art performance, and has been made publicly available for further use.

Abstract: Over the past few years, table interpretation tasks have made significant
progress due to their importance and the introduction of new technologies and
benchmarks in the field. This work experiments with a hybrid approach for
detecting relationships among columns of unlabeled tabular data, using a
Knowledge Graph (KG) as a reference point, a task known as CPA. This approach
leverages large language models (LLMs) while employing statistical analysis to
reduce the search space of potential KG relations. The main modules of this
approach for reducing the search space are domain and range constraints
detection, as well as relation co-appearance analysis. The experimental
evaluation on two benchmark datasets provided by the SemTab challenge assesses
the influence of each module and the effectiveness of different
state-of-the-art LLMs at various levels of quantization. The experiments were
performed, as well as at different prompting techniques. The proposed
methodology, which is publicly available on github, proved to be competitive
with state-of-the-art approaches on these datasets.

</details>


### [92] [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)
*Heng Dong,Kefei Duan,Chongjie Zhang*

Main category: cs.CL

TL;DR: The paper introduces LAC, an Actor-Critic framework for enhancing LLM policies in long-term decision-making, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with long-term reasoning and high-level objective alignment in complex decision-making scenarios.

Method: The paper proposes LAC, which computes Q-values using token logits tied to outcomes, supports future rollouts/reasoning, and uses a gradient-free policy improvement mechanism.

Result: The approach demonstrates effectiveness across diverse tasks, outperforming state-of-the-art methods and GPT-4 with smaller models (7B/8B parameters).

Conclusion: Integrating structured policy optimization with LLM intrinsic knowledge significantly enhances multi-step decision-making.

Abstract: Large Language Models (LLMs) have achieved remarkable advancements in natural
language processing tasks, yet they encounter challenges in complex
decision-making scenarios that require long-term reasoning and alignment with
high-level objectives. Existing methods either rely on short-term
auto-regressive action generation or face limitations in accurately simulating
rollouts and assessing outcomes, leading to sub-optimal decisions. This paper
introduces a novel LLM-based Actor-Critic framework, termed LAC, that
effectively improves LLM policies with long-term action evaluations in a
principled and scalable way. Our approach addresses two key challenges: (1)
extracting robust action evaluations by computing Q-values via token logits
associated with positive/negative outcomes, enhanced by future trajectory
rollouts and reasoning; and (2) enabling efficient policy improvement through a
gradient-free mechanism. Experiments across diverse environments -- including
high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),
and large action spaces (WebShop) -- demonstrate the framework's generality and
superiority over state-of-the-art methods. Notably, our approach achieves
competitive performance using 7B/8B parameter LLMs, even outperforming baseline
methods employing GPT-4 in complex tasks. These results underscore the
potential of integrating structured policy optimization with LLMs' intrinsic
knowledge to advance decision-making capabilities in multi-step environments.

</details>


### [93] [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)
*Yi Ji,Runzhi Li,Baolei Mao*

Main category: cs.CL

TL;DR: The paper introduces DMPI-PMHFE, a dual-channel feature fusion framework for detecting prompt injection attacks on LLMs, combining semantic and heuristic features.


<details>
  <summary>Details</summary>
Motivation: Prompt injection attacks pose a major security threat to LLMs, and current defenses lack both effectiveness and generalizability, necessitating novel detection methods.

Method: They propose DMPI-PMHFE, which fuses semantic vectors extracted by DeBERTa-v3-base with heuristic features derived from attack patterns, using a neural network for final prediction.

Result: Experimental results show superior accuracy, recall, and F1-scores for DMPI-PMHFE compared to other methods, and it significantly reduces attack success rates on multiple LLMs.

Conclusion: DMPI-PMHFE offers a robust and generalizable approach to detecting prompt injection attacks, showcasing improved model performance and reduced vulnerability across various LLM systems.

Abstract: With the widespread adoption of Large Language Models (LLMs), prompt
injection attacks have emerged as a significant security threat. Existing
defense mechanisms often face critical trade-offs between effectiveness and
generalizability. This highlights the urgent need for efficient prompt
injection detection methods that are applicable across a wide range of LLMs. To
address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion
detection framework. It integrates a pretrained language model with heuristic
feature engineering to detect prompt injection attacks. Specifically, the
framework employs DeBERTa-v3-base as a feature extractor to transform input
text into semantic vectors enriched with contextual information. In parallel,
we design heuristic rules based on known attack patterns to extract explicit
structural features commonly observed in attacks. Features from both channels
are subsequently fused and passed through a fully connected neural network to
produce the final prediction. This dual-channel approach mitigates the
limitations of relying only on DeBERTa to extract features. Experimental
results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms
existing methods in terms of accuracy, recall, and F1-score. Furthermore, when
deployed actually, it significantly reduces attack success rates across
mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.

</details>


### [94] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
*Pengyi Li,Matvey Skripkin,Alexander Zubrey,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CL

TL;DR: The paper introduces RLSC, a reinforcement learning method leveraging a model's self-confidence for improved alignment on reasoning tasks, achieving significant accuracy boosts with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: To address the costly reliance on human annotations and external reward models in existing RL methods for aligning large language models with task goals.

Method: Introduces Reinforcement Learning via Self-Confidence (RLSC), which uses the model’s self-confidence as intrinsic reward signals to improve alignment, eliminating the need for external models or engineered rewards.

Result: RLSC applied to Qwen2.5-Math-7B achieved significant accuracy improvements of +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on AMC23, with minimal supervision.

Conclusion: RLSC is an effective, scalable post-training method for reasoning models that reduces supervision requirements while dramatically improving task performance.

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC
improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on
AMC23. RLSC offers a simple, scalable post-training method for reasoning models
with minimal supervision.

</details>


### [95] [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
*Christopher D. Molek,Roberto Fronteddu,K. Brent Venable,Niranjan Suri*

Main category: cs.CL

TL;DR: The paper introduces a system combining small Large Language Models (LLMs) with graphical databases to enhance situational awareness in the Internet of Battlefield Things (IoBT) through natural language interactions.


<details>
  <summary>Details</summary>
Motivation: The IoBT produces vast data that needs transformation into actionable information for critical decision-making in battlefields. Current methods lack efficient ways to query and process this data in natural language.

Method: The authors propose a workflow where medium-sized LLMs map user questions to Cypher database queries and summarize response outputs in natural language. They utilize edge-sized LLMs like Llama 3.1 and graphical databases tailored for connected IoBT networks.

Result: Evaluation on a dataset from the US Army's Jornada Range showed that Llama 3.1 (8B parameters) outperforms other models, achieving a 19.4% accuracy improvement by relaxing Exact Match (EM) requirements for queries.

Conclusion: The paper demonstrates a feasible architecture for deploying LLMs on edge devices to enable natural language-based database interactions, paving the way for IoBT-enhanced decision-making processes.

Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.

</details>


### [96] [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
*Sooyung Choi,Jaehyeok Lee,Xiaoyuan Yi,Jing Yao,Xing Xie,JinYeong Bak*

Main category: cs.CL

TL;DR: The paper explores the safety risks associated with value-aligned large language models (LLMs), showing they are more prone to harmful behaviors and proposes methods to improve their safety.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the growing use of LLMs and the need to align them with human values while addressing the safety challenges of harmful outcomes linked to certain individual values.

Method: The study examines specific safety risks, conducts psychological investigations, utilizes datasets with safety categories, and proposes in-context alignment methods to address safety concerns.

Result: The study finds that value-aligned LLMs are generally more prone to harmful behaviors compared to non-fine-tuned and other fine-tuned models, with significant correlations between value alignment and safety risks.

Conclusion: The paper sheds light on the risks of value alignment in LLMs, offering insights into these challenges while presenting potential methods to improve their safety through in-context alignment.

Abstract: The application scope of Large Language Models (LLMs) continues to expand,
leading to increasing interest in personalized LLMs that align with human
values. However, aligning these models with individual values raises
significant safety concerns, as certain values may correlate with harmful
information. In this paper, we identify specific safety risks associated with
value-aligned LLMs and investigate the psychological principles behind these
challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are
more prone to harmful behavior compared to non-fine-tuned models and exhibit
slightly higher risks in traditional safety evaluations than other fine-tuned
models. (2) These safety issues arise because value-aligned LLMs genuinely
generate text according to the aligned values, which can amplify harmful
outcomes. Using a dataset with detailed safety categories, we find significant
correlations between value alignment and safety risks, supported by
psychological hypotheses. This study offers insights into the "black box" of
value alignment and proposes in-context alignment methods to enhance the safety
of value-aligned LLMs.

</details>


### [97] [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Chen Wei,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CL

TL;DR: This paper introduces SMAR to efficiently balance expert specialization and language abilities in multimodal MoE models.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal MoE models either have high training costs or degrade language capabilities when adapting pretrained models, necessitating a more efficient and balanced solution.

Method: The authors propose a technique called Soft Modality-Aware Routing (SMAR), using KL divergence to regularize routing probability distributions across modalities, without modifying the model architecture.

Result: SMAR achieves 86.6% language ability retention with just 2.5% pure text while delivering strong multimodal performance, outperforming baseline methods.

Conclusion: SMAR proves to be a practical and efficient method to enhance multimodal MoE models by balancing modality differentiation and preserving language performance.

Abstract: Mixture of Experts (MoE) architectures have become a key approach for scaling
large language models, with growing interest in extending them to multimodal
tasks. Existing methods to build multimodal MoE models either incur high
training costs or suffer from degraded language capabilities when adapting
pretrained models. To address this, we propose Soft ModalityAware Routing
(SMAR), a novel regularization technique that uses Kullback Leibler divergence
to control routing probability distributions across modalities, encouraging
expert specialization without modifying model architecture or heavily relying
on textual data. Experiments on visual instruction tuning show that SMAR
preserves language ability at 86.6% retention with only 2.5% pure text,
outperforming baselines while maintaining strong multimodal performance. Our
approach offers a practical and efficient solution to balance modality
differentiation and language capabilities in multimodal MoE models.

</details>


### [98] [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
*Ivi Chatzi,Nina Corvelo Benz,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CL

TL;DR: The paper introduces canonical sampling to ensure large language models generate token sequences that are consistent with their tokenizer's canonical output, improving token distribution accuracy.


<details>
  <summary>Details</summary>
Motivation: Errors in tokenization during inference can lead to negative consequences in performance and accuracy for large language models.

Method: The authors propose canonical sampling, a technique that ensures a language model generates token sequences aligned to a canonical form defined by its tokenizer.

Result: Canonical sampling generates token sequences closer to the true distribution used for training compared to standard sampling methods.

Conclusion: Canonical sampling can enhance the reliability of token generation in language models, mitigating issues related to non-canonical token sequences.

Abstract: State of the art large language models are trained using large amounts of
tokens derived from raw text using what is called a tokenizer. Crucially, the
tokenizer determines the (token) vocabulary a model will use during inference
as well as, in principle, the (token) language. This is because, while the
token vocabulary may allow for different tokenizations of a string, the
tokenizer always maps the string to only one of these tokenizations--the
canonical tokenization. However, multiple lines of empirical evidence suggest
that large language models do not always generate canonical token sequences,
and this comes with several negative consequences. In this work, we first show
that, to generate a canonical token sequence, a model needs to generate
(partial) canonical token sequences at each step of the autoregressive
generation process underpinning its functioning. Building upon this theoretical
result, we introduce canonical sampling, a simple and efficient sampling method
that precludes a given model from generating non-canonical token sequences.
Further, we also show that, in comparison with standard sampling, the
distribution of token sequences generated using canonical sampling is provably
closer to the true distribution of token sequences used during training.

</details>


### [99] [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)
*Kaiser Sun,Fan Bai,Mark Dredze*

Main category: cs.CL

TL;DR: This paper evaluates how large language models (LLMs) handle conflicting contextual inputs and parametric knowledge.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored issue of how LLMs manage conflicts between their internal parametric knowledge and external contextual input, as such conflicts could affect their utility in practical scenarios.

Method: The authors introduced a diagnostic framework and constructed datasets to trigger context-parametric knowledge conflicts, analyzing LLMs' performance across multiple task types.

Result: The study found that knowledge conflicts minimally affect tasks not reliant on knowledge, models perform better when knowledge aligns, models fail to suppress internal knowledge even when instructed, and rationales explaining conflicts enhance reliance on contextual input.

Conclusion: The researchers highlight concerns about model-based evaluation validity and emphasize the importance of managing knowledge conflicts when deploying LLMs.

Abstract: Large language models frequently rely on both contextual input and parametric
knowledge to perform tasks. However, these sources can come into conflict,
especially when retrieved documents contradict the model's parametric
knowledge. We propose a diagnostic framework to systematically evaluate LLM
behavior under context-memory conflict, where the contextual information
diverges from their parametric beliefs. We construct diagnostic data that
elicit these conflicts and analyze model performance across multiple task
types. Our findings reveal that (1) knowledge conflict has minimal impact on
tasks that do not require knowledge utilization, (2) model performance is
consistently higher when contextual and parametric knowledge are aligned, (3)
models are unable to fully suppress their internal knowledge even when
instructed, and (4) providing rationales that explain the conflict increases
reliance on contexts. These insights raise concerns about the validity of
model-based evaluation and underscore the need to account for knowledge
conflict in the deployment of LLMs.

</details>


### [100] [Improving LLM-Powered EDA Assistants with RAFT](https://arxiv.org/abs/2506.06500)
*Luyao Shi,Michael Kazda,Charles Schmitter,Hemlata Gupta*

Main category: cs.CL

TL;DR: This paper proposes a method to improve large language models (LLMs) for Electronic Design Automation (EDA) tasks using synthetic question/answer datasets and fine-tuning approaches.


<details>
  <summary>Details</summary>
Motivation: Engineers in the EDA field need efficient access to specialized information, but pre-trained LLMs lack domain-specific knowledge and may generate inaccurate responses in a Retrieval-Augmented Generation (RAG) setup.

Method: The authors use Retrieval-Augmented Fine-Tuning (RAFT) with synthetic Q/A data to enhance LLMs for EDA tasks. They also employ Retrieval-Augmented Few-Shot (RAFS) examples and implement secure access control mechanisms.

Result: RAFT with synthetic data considerably improves LLM accuracy in EDA-specific RAG tasks. Additionally, insights into data leakage are provided, and security concerns regarding sensitive information are addressed.

Conclusion: Synthetic Q/A datasets and fine-tuning methodologies can successfully improve LLM performance for niche applications like EDA, while offering practical strategies to mitigate risks in data handling and security.

Abstract: Electronic design engineers often struggle to efficiently access relevant
information for tasks like design verification and technology development.
While large language models (LLMs) can enhance productivity as conversational
agents, pre-trained open-source LLMs lack domain-specific knowledge for
Electronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG)
context, LLMs rely on external context but may still produce inaccurate
responses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but
acquiring labeled question/answer (Q/A) data in EDA is difficult. To address
this, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our
results show that RAFT with synthetic data significantly boosts LLM performance
for RAG-based EDA tasks. We also investigate the impact of using real user
questions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data
generation. Additionally, we implement secure access control to ensure
sensitive information is only accessible to authorized personnel. Finally, we
assess the risk of data leakage and unintended memorization during fine-tuning
with synthetic data, providing practical insights.

</details>


### [101] [Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes](https://arxiv.org/abs/2506.06506)
*Kshitish Ghate,Tessa Charlesworth,Mona Diab,Aylin Caliskan*

Main category: cs.CL

TL;DR: The paper investigates how intrinsic biases in vision-language models (VLMs) propagate into downstream zero-shot retrieval tasks, creating biased outputs. They present a framework correlating intrinsic biases with extrinsic task biases and find that larger, better-performing models exacerbate bias propagation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how intrinsic biases within vision-language models influence their outputs in downstream tasks, as these biases have implications for fairness in AI systems.

Method: The study uses a controlled framework to measure bias propagation by correlating intrinsic bias measures in the representational space with extrinsic bias measures in text-to-image and image-to-text retrieval tasks.

Result: Substantial correlation is found between intrinsic and extrinsic biases ($\rho = 0.83 \pm 0.10). This pattern holds across multiple analyses, directions, social groups, and VLM models. Larger models exhibit enhanced bias propagation.

Conclusion: Bias in foundational vision-language models is deeply rooted and propagates into downstream tasks, raising fairness concerns as models grow in size and complexity. Additionally, underrepresented groups face less robust propagation, exacerbating skewed outcomes.

Abstract: To build fair AI systems we need to understand how social-group biases
intrinsic to foundational encoder-based vision-language models (VLMs) manifest
in biases in downstream tasks. In this study, we demonstrate that intrinsic
biases in VLM representations systematically ``carry over'' or propagate into
zero-shot retrieval tasks, revealing how deeply rooted biases shape a model's
outputs. We introduce a controlled framework to measure this propagation by
correlating (a) intrinsic measures of bias in the representational space with
(b) extrinsic measures of bias in zero-shot text-to-image (TTI) and
image-to-text (ITT) retrieval. Results show substantial correlations between
intrinsic and extrinsic bias, with an average $\rho$ = 0.83 $\pm$ 0.10. This
pattern is consistent across 114 analyses, both retrieval directions, six
social groups, and three distinct VLMs. Notably, we find that
larger/better-performing models exhibit greater bias propagation, a finding
that raises concerns given the trend towards increasingly complex AI models.
Our framework introduces baseline evaluation tasks to measure the propagation
of group and valence signals. Investigations reveal that underrepresented
groups experience less robust propagation, further skewing their model-related
outcomes.

</details>


### [102] [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)
*Aladin Djuhera,Swanand Ravindra Kadhe,Syed Zawad,Farhan Ahmed,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: This paper provides a detailed analysis of two popular open post-training datasets (Tulu-3-SFT-Mix and SmolTalk) and presents a new curated dataset, TuluTalk, which is more efficient and maintains high performance.


<details>
  <summary>Details</summary>
Motivation: Recent LLM post-training datasets often lack transparency in their construction, motivating the need for open-source datasets and systematic evaluation methods to optimize their use effectively.

Method: The authors used the Magpie framework to annotate the datasets with quality metrics (e.g., turn structure and task category) and conducted comparative statistical analyses. They designed a new, refined dataset (TuluTalk) based on their findings.

Result: The new dataset, TuluTalk, consists of 14% fewer samples yet matches or exceeds the performance of the original datasets on benchmarks, improving data efficiency and quality impact.

Conclusion: By analyzing open-source post-training datasets and curating an optimized data mixture, this work enhances model performance without additional resource requirements and advances transparency and reproducibility in dataset construction.

Abstract: Recent work on large language models (LLMs) has increasingly focused on
post-training and alignment with datasets curated to enhance instruction
following, world knowledge, and specialized skills. However, most post-training
datasets used in leading open- and closed-source LLMs remain inaccessible to
the public, with limited information about their construction process. This
lack of transparency has motivated the recent development of open-source
post-training corpora. While training on these open alternatives can yield
performance comparable to that of leading models, systematic comparisons remain
challenging due to the significant computational cost of conducting them
rigorously at scale, and are therefore largely absent. As a result, it remains
unclear how specific samples, task types, or curation strategies influence
downstream performance when assessing data quality. In this work, we conduct
the first comprehensive side-by-side analysis of two prominent open
post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie
framework, we annotate each sample with detailed quality metrics, including
turn structure (single-turn vs. multi-turn), task category, input quality, and
response quality, and we derive statistics that reveal structural and
qualitative similarities and differences between the two datasets. Based on
these insights, we design a principled curation recipe that produces a new data
mixture, TuluTalk, which contains 14% fewer samples than either source dataset
while matching or exceeding their performance on key benchmarks. Our findings
offer actionable insights for constructing more effective post-training
datasets that improve model performance within practical resource limits. To
support future research, we publicly release both the annotated source datasets
and our curated TuluTalk mixture.

</details>


### [103] [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)
*Yijie Hao,Haofei Yu,Jiaxuan You*

Main category: cs.CL

TL;DR: The paper investigates 'Intent Hallucination' in large language models when they fail to fully satisfy complex queries, and introduces FAITHQA benchmark along with a metric called CONSTRAINT SCORE.


<details>
  <summary>Details</summary>
Motivation: To address the prevalent issue of LLMs failing to fully satisfy complex queries by neglecting parts or misinterpreting them, known as intent hallucination.

Method: The authors introduce FAITHQA, a benchmark consisting of 20,068 problems across different query setups, and propose the CONSTRAINT SCORE metric for evaluating intent hallucination in LLMs.

Result: Experiments reveal that intent hallucination is a common problem even in state-of-the-art models. The proposed CONSTRAINT SCORE metric aligns closely with human evaluation for detecting intent hallucination.

Conclusion: Intent hallucination is a significant issue in LLMs. FAITHQA and CONSTRAINT SCORE are valuable tools to systematically evaluate and advance research in this area.

Abstract: When exposed to complex queries containing multiple conditions, today's large
language models (LLMs) tend to produce responses that only partially satisfy
the query while neglecting certain conditions. We therefore introduce the
concept of Intent Hallucination. In this phenomenon, LLMs either omit
(neglecting to address certain parts) or misinterpret (responding to invented
query parts) elements of the given query, leading to intent hallucinated
generation. To systematically evaluate intent hallucination, we introduce
FAITHQA, a novel benchmark for intent hallucination that contains 20,068
problems, covering both query-only and retrieval-augmented generation (RAG)
setups with varying topics and difficulty. FAITHQA is the first hallucination
benchmark that goes beyond factual verification, tailored to identify the
fundamental cause of intent hallucination. By evaluating various LLMs on
FAITHQA, we find that (1) intent hallucination is a common issue even for
state-of-the-art models, and (2) the phenomenon stems from omission or
misinterpretation of LLMs. To facilitate future research, we introduce an
automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting
intent hallucination. Human evaluation results demonstrate that CONSTRAINT
SCORE is closer to human performance for intent hallucination compared to
baselines.

</details>


### [104] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: The paper introduces LaMP-Cap, a dataset designed for personalized figure caption generation using multimodal profiles, and shows that incorporating figure profiles improves caption quality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of personalizing AI-generated figure captions to match an author's unique style and domain requirements, which existing text-based language models fail to fully accommodate in multimodal contexts.

Method: The authors developed the LaMP-Cap dataset, which includes multimodal figure profiles combining figure images, captions, and figure-mentioning paragraphs alongside target figures. They conducted experiments using four LLMs to analyze the impact of this profile information on caption generation.

Result: The experiments demonstrated that using multimodal profiles—especially the images—led to captions closer to the original author-written ones. Images were found to have more influence than figure-mentioning paragraphs.

Conclusion: Leveraging multimodal profiles significantly enhances personalized caption generation. Images within profiles are particularly beneficial in improving the quality of AI-generated captions.

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


### [105] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
*Yuhan Cao,Zian Chen,Kun Quan,Ziliang Zhang,Yu Wang,Xiaoning Dong,Yeqi Feng,Guanzhong He,Jingcheng Huang,Jianhao Li,Yixuan Tan,Jiafu Tang,Yilin Tang,Junlei Wu,Qianyu Xiao,Can Zheng,Shouchen Zhou,Yuxiang Zhu,Yiming Huang,Tian Xie,Tianxing He*

Main category: cs.CL

TL;DR: The paper introduces TCGBench, a benchmark for evaluating the ability of large language models (LLMs) to generate test case generators for competition-level programming problems.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the underutilized potential of LLMs in generating test cases for debugging code, particularly targeting the domain of competition-level programming tasks.

Method: The researchers evaluated state-of-the-art LLMs on two tasks: creating valid test case generators and generating targeted ones capable of identifying bugs in human-written code. They also created a curated dataset to enhance LLM performance.

Result: State-of-the-art LLMs performed well in generating valid test case generators but struggled significantly in producing targeted generators to reveal human code flaws. Performance improved when using a curated dataset.

Conclusion: LLMs show promise in code testing tasks but require further advancements and training to match human-level capabilities in generating targeted test cases effectively.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [106] [Precise Information Control in Long-Form Text Generation](https://arxiv.org/abs/2506.06589)
*Jacqueline He,Howard Yen,Margaret Li,Shuyue Stella Li,Zhiyuan Zeng,Weijia Shi,Yulia Tsvetkov,Danqi Chen,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.CL

TL;DR: This paper addresses the issue of intrinsic hallucination in language models by introducing the task of Precise Information Control (PIC) and a benchmark "PIC-Bench" to evaluate long-form output generation grounded in verifiable input claims.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the challenge of intrinsic hallucination in modern LMs, where the generated content is plausible but not substantiated, reducing trust in factual text generation.

Method: The authors propose Precise Information Control (PIC), including both partial and full settings, and introduce PIC-Bench, a benchmark for eight tasks. They also develop a post-training framework using weakly supervised preference data to enhance a 8B parameter PIC-LM's ability to follow PIC standards.

Result: State-of-the-art LMs were found to hallucinate in 70% of cases on PIC-Bench. The proposed PIC-LM model significantly improves performance and achieves 91.0% F1 in the full PIC setting.

Conclusion: PIC and PIC-Bench allow for better evaluation and reduction of hallucinations in LMs. The PIC-LM demonstrates improved groundedness and has potential applications in factual text generation pipelines.

Abstract: A central challenge in modern language models (LMs) is intrinsic
hallucination: the generation of information that is plausible but
unsubstantiated relative to input context. To study this problem, we propose
Precise Information Control (PIC), a new task formulation that requires models
to generate long-form outputs grounded in a provided set of short
self-contained statements, known as verifiable claims, without adding any
unsupported ones. For comprehensiveness, PIC includes a full setting that tests
a model's ability to include exactly all input claims, and a partial setting
that requires the model to selectively incorporate only relevant claims. We
present PIC-Bench, a benchmark of eight long-form generation tasks (e.g.,
summarization, biography generation) adapted to the PIC setting, where LMs are
supplied with well-formed, verifiable input claims. Our evaluation of a range
of open and proprietary LMs on PIC-Bench reveals that, surprisingly,
state-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To
alleviate this lack of faithfulness, we introduce a post-training framework,
using a weakly supervised preference data construction method, to train an 8B
PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full
PIC setting. When integrated into end-to-end factual generation pipelines,
PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and
factual precision by 30.5% on a birthplace verification task, underscoring the
potential of precisely grounded generation.

</details>


### [107] [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)
*Xiao Wang,Mengjue Tan,Qiao Jin,Guangzhi Xiong,Yu Hu,Aidong Zhang,Zhiyong Lu,Minjia Zhang*

Main category: cs.CL

TL;DR: The paper proposes a framework for citation generation and evaluation in medical question-answering systems, using a novel multi-pass retrieval-citation approach.


<details>
  <summary>Details</summary>
Motivation: The lack of citation generation and evaluation in LLM-based medical question-answering systems hinders their practical adoption.

Method: The authors introduce an end-to-end framework and a multi-pass retrieval-citation method designed to generate high-quality citations for medical tasks.

Result: The proposed method shows improved citation precision and recall over baseline methods and demonstrates a strong correlation between automated evaluations and expert annotations.

Conclusion: This study identifies critical design choices for citation generation and provides a robust solution that makes LLM-based medical systems more reliable and practical.

Abstract: Existing LLM-based medical question-answering systems lack citation
generation and evaluation capabilities, raising concerns about their adoption
in practice. In this work, we introduce \name, the first end-to-end framework
that facilitates the design and evaluation of citation generation with LLMs for
medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation
method that generates high-quality citations. Our evaluation highlights the
challenges and opportunities of citation generation for medical tasks, while
identifying important design choices that have a significant impact on the
final citation quality. Our proposed method achieves superior citation
precision and recall improvements compared to strong baseline methods, and we
show that evaluation results correlate well with annotation results from
professional experts.

</details>


### [108] [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
*Charles Goddard,Fernando Fernandes Neto*

Main category: cs.CL

TL;DR: The paper proposes a method called Orthogonal Matching Pursuit (OMP) for reconstructing token embeddings in pretrained language models to transplant tokenizers without additional training.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of mismatched tokenizers in pretrained language models, which hinders reuse of pretrained weights across models with different tokenization schemes.

Method: They use OMP to approximate out-of-vocabulary tokens as sparse linear combinations of shared tokens. This involves a two-phase process to reconstruct embeddings in one model's space and transfer them to another.

Result: The method shows superior zero-shot performance compared to existing approaches (e.g., WECHSEL, FOCUS, ZETT) in preserving performance across tasks when tokenizers differ.

Conclusion: OMP offers an effective, training-free solution to align tokenizers for pretrained models, facilitating enhanced model reuse and downstream applications like distillation and domain adaptation.

Abstract: We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a sparse linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.

</details>


### [109] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
*Alan Chen,Jack Merullo,Alessandro Stolfo,Ellie Pavlick*

Main category: cs.CL

TL;DR: This paper finds that affine mappings can be used to cheaply transfer features between language models of different sizes, improving efficiency in training components like Sparse Autoencoders (SAEs).


<details>
  <summary>Details</summary>
Motivation: To compare representation spaces between language models of various sizes and improve computational efficiency in training.

Method: Affinely map features between residual streams of different-sized models, using Sparse Autoencoders weights as transferable components.

Result: Small and large models have similar representation spaces; transferring trained SAEs from smaller to larger models reduces computational cost significantly.

Conclusion: Feature transfer improves training efficiency and reveals key insights into representation space similarities and differences across models.

Abstract: In this work, we demonstrate that affine mappings between residual streams of
language models is a cheap way to effectively transfer represented features
between models. We apply this technique to transfer the weights of Sparse
Autoencoders (SAEs) between models of different sizes to compare their
representations. We find that small and large models learn highly similar
representation spaces, which motivates training expensive components like SAEs
on a smaller model and transferring to a larger model at a FLOPs savings. For
example, using a small-to-large transferred SAE as initialization can lead to
50% cheaper training runs when training SAEs on larger models. Next, we show
that transferred probes and steering vectors can effectively recover ground
truth performance. Finally, we dive deeper into feature-level transferability,
finding that semantic and structural features transfer noticeably differently
while specific classes of functional features have their roles faithfully
mapped. Overall, our findings illustrate similarities and differences in the
linear representation spaces of small and large models and demonstrate a method
for improving the training efficiency of SAEs.

</details>


### [110] [Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings](https://arxiv.org/abs/2506.06616)
*Samuel Kim,Oghenemaro Imieye,Yunting Yin*

Main category: cs.CL

TL;DR: The paper compares large language models (LLMs) and traditional classifiers for detecting depressive language in social media, finding that supervised classifiers powered by LLM-generated summaries outperform zero-shot LLMs in fine-grained tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the detection and interpretation of depressive language in social media to support early mental health interventions.

Method: The authors evaluate the performance of zero-shot LLMs and supervised classifiers using text embeddings and LLM-generated summary embeddings across multiple classification tasks.

Result: Zero-shot LLMs excel at binary classification but struggle with more complex tasks, while supervised classifiers using LLM-generated summaries achieve superior performance.

Conclusion: LLMs have strong generalization capabilities for mental health tasks, and their summarization techniques offer potential for enhanced mental health prediction tools.

Abstract: Accurate and interpretable detection of depressive language in social media
is useful for early interventions of mental health conditions, and has
important implications for both clinical practice and broader public health
efforts. In this paper, we investigate the performance of large language models
(LLMs) and traditional machine learning classifiers across three classification
tasks involving social media data: binary depression classification, depression
severity classification, and differential diagnosis classification among
depression, PTSD, and anxiety. Our study compares zero-shot LLMs with
supervised classifiers trained on both conventional text embeddings and
LLM-generated summary embeddings. Our experiments reveal that while zero-shot
LLMs demonstrate strong generalization capabilities in binary classification,
they struggle with fine-grained ordinal classifications. In contrast,
classifiers trained on summary embeddings generated by LLMs demonstrate
competitive, and in some cases superior, performance on the classification
tasks, particularly when compared to models using traditional text embeddings.
Our findings demonstrate the strengths of LLMs in mental health prediction, and
suggest promising directions for better utilization of their zero-shot
capabilities and context-aware summarization techniques.

</details>


### [111] [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)
*Jesse Woo,Fateme Hashemi Chaleshtori,Ana Marasović,Kenneth Marino*

Main category: cs.CL

TL;DR: This paper introduces BRIEFME, a new dataset for assessing legal skills in language models through tasks such as argument summarization, completion, and case retrieval.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of focus in Legal NLP on tasks essential for writing and editing legal briefs, aiming to assist legal professionals with language models.

Method: The authors create the BRIEFME dataset, defining three specialized tasks—argument summarization, guided argument completion, and legal case retrieval—to evaluate and enhance language model capabilities in legal writing.

Result: Today's large language models perform well on summarization and guided completion tasks but struggle with realistic argument completion and legal case retrieval in the BRIEFME benchmark.

Conclusion: The authors emphasize the potential of their dataset to foster advancements in Legal NLP that directly aid legal professionals, highlighting areas where models need improvement.

Abstract: A core part of legal work that has been under-explored in Legal NLP is the
writing and editing of legal briefs. This requires not only a thorough
understanding of the law of a jurisdiction, from judgments to statutes, but
also the ability to make new arguments to try to expand the law in a new
direction and make novel and creative arguments that are persuasive to judges.
To capture and evaluate these legal skills in language models, we introduce
BRIEFME, a new dataset focused on legal briefs. It contains three tasks for
language models to assist legal professionals in writing briefs: argument
summarization, argument completion, and case retrieval. In this work, we
describe the creation of these tasks, analyze them, and show how current models
perform. We see that today's large language models (LLMs) are already quite
good at the summarization and guided completion tasks, even beating
human-generated headings. Yet, they perform poorly on other tasks in our
benchmark: realistic argument completion and retrieving relevant legal cases.
We hope this dataset encourages more development in Legal NLP in ways that will
specifically aid people in performing legal work.

</details>


### [112] [Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations](https://arxiv.org/abs/2506.06626)
*Junzhe Wang,Bichen Wang,Xing Fu,Yixin Sun,Yanyan Zhao,Bing Qin*

Main category: cs.CL

TL;DR: The study introduces the Multi-Session Psychological Counseling Conversation Dataset (MusPsy-Dataset) to address the gap in multi-session counseling research for Large Language Models.


<details>
  <summary>Details</summary>
Motivation: Current psychological counseling research using LLMs primarily focuses on single-session counseling, which fails to emulate real-world sustained multi-session engagements.

Method: The researchers developed the MusPsy-Dataset using real client profiles from publicly available psychological case reports, enabling progressive counseling conversations across multiple sessions. They also created the MusPsy-Model to track client progress and adjust counseling strategies over time.

Result: Experimental findings indicate that MusPsy-Model outperforms baseline models in handling multi-session counseling scenarios.

Conclusion: MusPsy-Dataset and MusPsy-Model significantly enhance the capacity of LLM-based systems to simulate realistic multi-session psychological counseling interactions, suggesting the approach is promising for practical applications.

Abstract: In recent years, Large Language Models (LLMs) have made significant progress
in automated psychological counseling. However, current research focuses on
single-session counseling, which doesn't represent real-world scenarios. In
practice, psychological counseling is a process, not a one-time event,
requiring sustained, multi-session engagement to progressively address clients'
issues. To overcome this limitation, we introduce a dataset for Multi-Session
Psychological Counseling Conversation Dataset (MusPsy-Dataset). Our
MusPsy-Dataset is constructed using real client profiles from publicly
available psychological case reports. It captures the dynamic arc of
counseling, encompassing multiple progressive counseling conversations from the
same client across different sessions. Leveraging our dataset, we also
developed our MusPsy-Model, which aims to track client progress and adapt its
counseling direction over time. Experiments show that our model performs better
than baseline models across multiple sessions.

</details>


### [113] [SafeLawBench: Towards Safe Alignment of Large Language Models](https://arxiv.org/abs/2506.06636)
*Chuxue Cao,Han Zhu,Jiaming Ji,Qichao Sun,Zhenghao Zhu,Yinyu Wu,Juntao Dai,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: The paper introduces SafeLawBench, a benchmark inspired by legal standards, to evaluate the safety of large language models (LLMs). The findings reveal limitations in accuracy and emphasize the importance of improving LLM safety.


<details>
  <summary>Details</summary>
Motivation: The growing use of LLMs has raised safety concerns, and existing safety benchmarks lack clear standards due to their subjective nature.

Method: The authors propose SafeLawBench, a framework based on legal standards, with multi-choice and open-domain QA tasks to systematically evaluate the safety of LLMs across multiple models.

Result: Evaluation of 20 LLMs revealed that even state-of-the-art models underperform in the benchmark, with no model achieving over 80.5% accuracy. Majority voting methods enhanced performance slightly.

Conclusion: Improving safety standards and methodologies for LLMs is critical for their responsible use, necessitating further research and development in this area.

Abstract: With the growing prevalence of large language models (LLMs), the safety of
LLMs has raised significant concerns. However, there is still a lack of
definitive standards for evaluating their safety due to the subjective nature
of current safety benchmarks. To address this gap, we conducted the first
exploration of LLMs' safety evaluation from a legal perspective by proposing
the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three
levels based on legal standards, providing a systematic and comprehensive
framework for evaluation. It comprises 24,860 multi-choice questions and 1,106
open-domain question-answering (QA) tasks. Our evaluation included 2
closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot
prompting, highlighting the safety features of each model. We also evaluated
the LLMs' safety-related reasoning stability and refusal behavior.
Additionally, we found that a majority voting mechanism can enhance model
performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and
GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench,
while the average accuracy of 20 LLMs remains at 68.8\%. We urge the community
to prioritize research on the safety of LLMs.

</details>


### [114] [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)
*Nikhita Vedula,Dushyanta Dhyani,Laleh Jalali,Boris Oreshkin,Mohsen Bayati,Shervin Malmasi*

Main category: cs.CL

TL;DR: This paper introduces a novel quantile regression approach using Large Language Models (LLMs) to tackle challenging probabilistic regression tasks, significantly outperforming traditional methods in price prediction across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: To improve probabilistic regression tasks involving unstructured inputs, such as text-to-distribution predictions for applications where accurate uncertainty quantification and nuanced text understanding are required.

Method: The study proposes quantile regression for LLMs, using fine-tuned Mistral-7B models with quantile heads to generate predictive distributions instead of point estimates. The approach is benchmarked against various architectures and methods across three datasets.

Result: Fine-tuned Mistral-7B models with quantile heads outperformed traditional regression approaches for point and distributional estimates in accuracy and calibration metrics, and the method excelled in diverse architectures and training setups.

Conclusion: The proposed quantile regression strategy for LLMs enhances predictive accuracy and uncertainty quantification in text-to-distribution tasks, presenting a significant step forward in probabilistic regression and enabling future research with shared datasets.

Abstract: Large Language Models (LLMs) have shown promise in structured prediction
tasks, including regression, but existing approaches primarily focus on point
estimates and lack systematic comparison across different methods. We
investigate probabilistic regression using LLMs for unstructured inputs,
addressing challenging text-to-distribution prediction tasks such as price
estimation where both nuanced text understanding and uncertainty quantification
are critical. We propose a novel quantile regression approach that enables LLMs
to produce full predictive distributions, improving upon traditional point
estimates. Through extensive experiments across three diverse price prediction
datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads
significantly outperforms traditional approaches for both point and
distributional estimations, as measured by three established metrics each for
prediction accuracy and distributional calibration. Our systematic comparison
of LLM approaches, model architectures, training approaches, and data scaling
reveals that Mistral-7B consistently outperforms encoder architectures,
embedding-based methods, and few-shot learning methods. Our experiments also
reveal the effectiveness of LLM-assisted label correction in achieving
human-level accuracy without systematic bias. Our curated datasets are made
available at https://github.com/vnik18/llm-price-quantile-reg/ to support
future research.

</details>


### [115] [Learning Distribution-Wise Control in Representation Space for Language Models](https://arxiv.org/abs/2506.06686)
*Chunyuan Deng,Ruidi Chang,Hanjie Chen*

Main category: cs.CL

TL;DR: The paper introduces distribution-wise interventions for language models, showing they enable better control and robustness compared to pointwise interventions.


<details>
  <summary>Details</summary>
Motivation: Improve control and steering of language model behavior efficiently, targeting concept subspaces.

Method: Extend representation fine-tuning to distribution-level interventions, allowing transformations over broader subspaces.

Result: Distribution-wise interventions outperform pointwise interventions across various reasoning benchmarks, enhancing controllability and robustness.

Conclusion: Distribution-wise interventions offer a more effective and comprehensive approach for influencing language model behavior.

Abstract: Interventions in language models (LMs) are applied strategically to steer
model behavior during the forward pass. Learnable interventions, also known as
representation fine-tuning, aim to apply pointwise control within the concept
subspace and have proven effective in altering high-level behaviors. In this
work, we extend this approach to the distribution level, enabling the model to
learn not only pointwise transformations but also the surrounding regions of
the concept subspace. We demonstrate that these methods perform effectively in
early layers, with larger standard deviations correlating strongly with
improved performance. Across eight commonsense reasoning and seven arithmetic
reasoning benchmarks, our distribution-wise interventions consistently
outperform pointwise interventions in controllability and robustness. These
results illustrate that distribution-wise interventions provide a more
comprehensive method for steering model behavior and enabling finer-grained
control over language models. The code is at:
\href{https://github.com/chili-lab/D-Intervention}{https://github.com/chili-lab/D-Intervention}.

</details>


### [116] [Dynamic and Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2506.06704)
*Weihang Su,Qingyao Ai,Jingtao Zhan,Qian Dong,Yiqun Liu*

Main category: cs.CL

TL;DR: This paper discusses recent advancements in Retrieval-Augmented Generation (RAG), specifically focusing on dynamic and parametric approaches to improve large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Current RAG systems use static pipelines and in-context knowledge injection, which are insufficient for complex tasks requiring multihop reasoning and adaptive knowledge integration.

Method: The tutorial explores two advanced research areas: Dynamic RAG, which adapts retrieval during text generation, and Parametric RAG, which focuses on parameter-level integration of retrieved knowledge.

Result: The study provides a comprehensive overview, including theoretical foundations and practical insights into these advanced RAG techniques.

Conclusion: Dynamic and Parametric RAG methods pave the way for more effective and adaptive use of external knowledge in LLMs, inspiring further advancements in the field.

Abstract: Retrieval-Augmented Generation (RAG) has become a foundational paradigm for
equipping large language models (LLMs) with external knowledge, playing a
critical role in information retrieval and knowledge-intensive applications.
However, conventional RAG systems typically adopt a static
retrieve-then-generate pipeline and rely on in-context knowledge injection,
which can be suboptimal for complex tasks that require multihop reasoning,
adaptive information access, and deeper integration of external knowledge.
Motivated by these limitations, the research community has moved beyond static
retrieval and in-context knowledge injection. Among the emerging directions,
this tutorial delves into two rapidly growing and complementary research areas
on RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when
and what to retrieve during the LLM's generation process, enabling real-time
adaptation to the LLM's evolving information needs. Parametric RAG rethinks how
retrieved knowledge should be injected into LLMs, transitioning from
input-level to parameter-level knowledge injection for enhanced efficiency and
effectiveness. This tutorial offers a comprehensive overview of recent advances
in these emerging research areas. It also shares theoretical foundations and
practical insights to support and inspire further research in RAG.

</details>


### [117] [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)
*Zhihui Chen,Kai He,Yucheng Huang,Yunxiao Zhu,Mengling Feng*

Main category: cs.CL

TL;DR: This paper introduces DivScore, a zero-shot detection framework designed to identify LLM-generated text in specialized domains like medicine and law, outperforming existing methods in challenges like domain shift.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot detectors fail to accurately detect LLM-generated text in specialized, high-stakes domains due to shifts in domain-specific distributions, raising concerns about misinformation and authenticity.

Method: The authors theoretically analyze the failure reasons, introduce DivScore employing normalized entropy-based scoring and domain knowledge distillation, and develop a domain-specific benchmark for text detection in medical and legal domains.

Result: DivScore outperforms state-of-the-art detectors with a 14.4% higher AUROC and 64.0% higher recall at a 0.1% false positive rate. In adversarial situations, it demonstrates robust improvements with a 22.8% advantage in AUROC and 29.5% in recall.

Conclusion: DivScore is an effective and robust zero-shot detection framework for identifying LLM-generated text in specialized medical and legal contexts, addressing domain shift challenges and advancing authenticity verification efforts.

Abstract: Detecting LLM-generated text in specialized and high-stakes domains like
medicine and law is crucial for combating misinformation and ensuring
authenticity. However, current zero-shot detectors, while effective on general
text, often fail when applied to specialized content due to domain shift. We
provide a theoretical analysis showing this failure is fundamentally linked to
the KL divergence between human, detector, and source text distributions. To
address this, we propose DivScore, a zero-shot detection framework using
normalized entropy-based scoring and domain knowledge distillation to robustly
identify LLM-generated text in specialized domains. We also release a
domain-specific benchmark for LLM-generated text detection in the medical and
legal domains. Experiments on our benchmark show that DivScore consistently
outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%
higher recall (0.1% false positive rate threshold). In adversarial settings,
DivScore demonstrates superior robustness than other baselines, achieving on
average 22.8% advantage in AUROC and 29.5% in recall. Code and data are
publicly available.

</details>


### [118] [A Survey of Retentive Network](https://arxiv.org/abs/2506.06708)
*Haiqi Yang,Zhiyuan Li,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: RetNet is introduced as an efficient alternative to Transformers, replacing high-complexity self-attention with a retention mechanism to handle extended contexts efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Transformers (high memory costs and poor scalability for long sequences) by introducing a more efficient neural network architecture with broader applicability.

Method: A retention mechanism that combines recurrence with attention to achieve linear-time inference and compatibility with parallelizable training pipelines.

Result: RetNet demonstrates robust performance across multiple domains, including NLP, speech recognition, and time-series analysis.

Conclusion: The paper reviews and consolidates understanding of RetNet, highlighting its potential and challenges while outlining future research directions for its development.

Abstract: Retentive Network (RetNet) represents a significant advancement in neural
network architecture, offering an efficient alternative to the Transformer.
While Transformers rely on self-attention to model dependencies, they suffer
from high memory costs and limited scalability when handling long sequences due
to their quadratic complexity. To mitigate these limitations, RetNet introduces
a retention mechanism that unifies the inductive bias of recurrence with the
global dependency modeling of attention. This mechanism enables linear-time
inference, facilitates efficient modeling of extended contexts, and remains
compatible with fully parallelizable training pipelines. RetNet has garnered
significant research interest due to its consistently demonstrated cross-domain
effectiveness, achieving robust performance across machine learning paradigms
including natural language processing, speech recognition, and time-series
analysis. However, a comprehensive review of RetNet is still missing from the
current literature. This paper aims to fill that gap by offering the first
detailed survey of the RetNet architecture, its key innovations, and its
diverse applications. We also explore the main challenges associated with
RetNet and propose future research directions to support its continued
advancement in both academic research and practical deployment.

</details>


### [119] [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)
*Qi Shi,Qiwei Han,Cláudia Soares*

Main category: cs.CL

TL;DR: C-PATH is a conversational AI system using large language models for healthcare assistance, showcasing strong performance in triage tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the complexity and challenges patients face in navigating healthcare systems, aiming to simplify symptom recognition and department selection.

Method: They developed C-PATH, a conversational AI based on the LLaMA3 architecture, fine-tuned using a multi-stage pipeline and augmented medical conversations with a GPT-based framework.

Result: C-PATH outperformed domain-specific baselines in accuracy and effectiveness using GPTScore evaluation on conversational datasets.

Conclusion: C-PATH highlights the potential of AI in creating accessible and accurate tools for healthcare navigation and triage tasks.

Abstract: Navigating healthcare systems can be complex and overwhelming, creating
barriers for patients seeking timely and appropriate medical attention. In this
paper, we introduce C-PATH (Conversational Patient Assistance and Triage in
Healthcare), a novel conversational AI system powered by large language models
(LLMs) designed to assist patients in recognizing symptoms and recommending
appropriate medical departments through natural, multi-turn dialogues. C-PATH
is fine-tuned on medical knowledge, dialogue data, and clinical summaries using
a multi-stage pipeline built on the LLaMA3 architecture. A core contribution of
this work is a GPT-based data augmentation framework that transforms structured
clinical knowledge from DDXPlus into lay-person-friendly conversations,
allowing alignment with patient communication norms. We also implement a
scalable conversation history management strategy to ensure long-range
coherence. Evaluation with GPTScore demonstrates strong performance across
dimensions such as clarity, informativeness, and recommendation accuracy.
Quantitative benchmarks show that C-PATH achieves superior performance in
GPT-rewritten conversational datasets, significantly outperforming
domain-specific baselines. C-PATH represents a step forward in the development
of user-centric, accessible, and accurate AI tools for digital health
assistance and triage.

</details>


### [120] [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)
*Mikhail Salnikov,Dmitrii Korzh,Ivan Lazichny,Elvir Karimov,Artyom Iudin,Ivan Oseledets,Oleg Y. Rogov,Alexander Panchenko,Natalia Loukachevitch,Elena Tutubalina*

Main category: cs.CL

TL;DR: The paper explores the geopolitical biases in large language models (LLMs), showing their preference for specific national narratives when interpreting historical events from conflicting perspectives.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to understand how LLMs reflect or amplify national narrative biases, particularly in the context of historical events interpreted differently by various countries.

Method: The authors developed a novel dataset with neutral event descriptions and contrasting viewpoints from the USA, UK, USSR, and China. They analyzed LLMs' interpretations, experimented with debiasing prompts, and manipulated participant labels to investigate attribution sensitivity.

Result: The study found significant geopolitical biases in LLMs, favoring specific national narratives. Simple debiasing prompts were ineffective, and label manipulation revealed both amplification of biases and recognition of inconsistencies.

Conclusion: The paper emphasizes the presence of national narrative biases in LLMs, shows the limitations of basic debiasing approaches, and provides valuable tools for further research on geopolitical biases.

Abstract: This paper evaluates geopolitical biases in LLMs with respect to various
countries though an analysis of their interpretation of historical events with
conflicting national perspectives (USA, UK, USSR, and China). We introduce a
novel dataset with neutral event descriptions and contrasting viewpoints from
different countries. Our findings show significant geopolitical biases, with
models favoring specific national narratives. Additionally, simple debiasing
prompts had a limited effect in reducing these biases. Experiments with
manipulated participant labels reveal models' sensitivity to attribution,
sometimes amplifying biases or recognizing inconsistencies, especially with
swapped labels. This work highlights national narrative biases in LLMs,
challenges the effectiveness of simple debiasing methods, and offers a
framework and dataset for future geopolitical bias research.

</details>


### [121] [They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse](https://arxiv.org/abs/2506.06775)
*Walter Paci,Alessandro Panunzi,Sandro Pezzelle*

Main category: cs.CL

TL;DR: The paper studies how Large Language Models (LLMs) perform in identifying implicit content in political discourse, revealing that current models struggle in interpreting presuppositions and implicatures, but identifying promising research directions.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the capability of LLMs in understanding and interpreting implicit content, like manipulation in political speeches, which is a significant aspect of political communication.

Method: The authors use the IMPAQTS corpus, a dataset of Italian political speeches with annotated manipulative content, to test LLMs through a multiple-choice and an open-ended generation task, focusing on detecting presuppositions and implicatures.

Result: The study finds that LLMs face challenges in accurately interpreting implicit political discourse. All tested models struggle with presuppositions and implicatures in both task formats.

Conclusion: Current LLMs lack key pragmatic skills required for understanding implicit language in political contexts, though the study suggests promising directions for improvement and provides datasets and code for further research.

Abstract: Implicit content plays a crucial role in political discourse, where speakers
systematically employ pragmatic strategies such as implicatures and
presuppositions to influence their audiences. Large Language Models (LLMs) have
demonstrated strong performance in tasks requiring complex semantic and
pragmatic understanding, highlighting their potential for detecting and
explaining the meaning of implicit content. However, their ability to do this
within political discourse remains largely underexplored. Leveraging, for the
first time, the large IMPAQTS corpus, which comprises Italian political
speeches with the annotation of manipulative implicit content, we propose
methods to test the effectiveness of LLMs in this challenging problem. Through
a multiple-choice task and an open-ended generation task, we demonstrate that
all tested models struggle to interpret presuppositions and implicatures. We
conclude that current LLMs lack the key pragmatic capabilities necessary for
accurately interpreting highly implicit language, such as that found in
political discourse. At the same time, we highlight promising trends and future
directions for enhancing model performance. We release our data and code at
https://github.com/WalterPaci/IMPAQTS-PID

</details>


### [122] [Extending dependencies to the taggedPBC: Word order in transitive clauses](https://arxiv.org/abs/2506.06785)
*Hiram Ring*

Main category: cs.CL

TL;DR: The paper reports a CoNLLU-formatted version of the taggedPBC, comprising dependency annotations for over 1,500 languages.


<details>
  <summary>Details</summary>
Motivation: To improve the utility of the taggedPBC dataset by adding dependency annotations and to explore crosslinguistic word order insights.

Method: Transferred dependency information alongside POS tags to over 1,500 languages in the taggedPBC, testing the derived word order data against typological databases.

Result: Word order data from the dependency-annotated taggedPBC correlates with expert analyses and typological resources, demonstrating the potential of the dataset.

Conclusion: Despite concerns about data quality, the dependency-annotated taggedPBC enables meaningful linguistic analysis and insights, showing value even from noisy datasets.

Abstract: The taggedPBC (Ring 2025a) contains more than 1,800 sentences of pos-tagged
parallel text data from over 1,500 languages, representing 133 language
families and 111 isolates. While this dwarfs previously available resources,
and the POS tags achieve decent accuracy, allowing for predictive
crosslinguistic insights (Ring 2025b), the dataset was not initially annotated
for dependencies. This paper reports on a CoNLLU-formatted version of the
dataset which transfers dependency information along with POS tags to all
languages in the taggedPBC. Although there are various concerns regarding the
quality of the tags and the dependencies, word order information derived from
this dataset regarding the position of arguments and predicates in transitive
clauses correlates with expert determinations of word order in three
typological databases (WALS, Grambank, Autotyp). This highlights the usefulness
of corpus-based typological approaches (as per Baylor et al. 2023; Bjerva 2024)
for extending comparisons of discrete linguistic categories, and suggests that
important insights can be gained even from noisy data, given sufficient
annotation. The dependency-annotated corpora are also made available for
research and collaboration via GitHub.

</details>


### [123] [On the Adaptive Psychological Persuasion of Large Language Models](https://arxiv.org/abs/2506.06800)
*Tianjie Ju,Yujia Chen,Hao Fei,Mong-Li Lee,Wynne Hsu,Pengzhou Cheng,Zongru Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: The paper explores the dual capabilities of LLMs in persuasion and resistance to persuasion, introduces psychological rhetoric strategies to improve success rates, and proposes a strategy-adaptive framework to further enhance effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on LLMs' dual capabilities in using and resisting psychological persuasion strategies and the need to improve their efficacy.

Method: The study evaluates existing LLMs, introduces eleven psychological persuasion strategies, and develops an adaptive framework leveraging direct preference optimization to autonomously select optimal strategies.

Result: Findings show current LLMs employ repetitive strategies with low success rates, but introducing specific strategies improves outcomes. The adaptive framework further enhances success through optimal strategy selection.

Conclusion: The adaptive psychological persuasion method significantly improves the persuasion capabilities of LLMs while retaining their general capabilities, suggesting promising advancements in their rhetorical potential.

Abstract: Previous work has showcased the intriguing capabilities of Large Language
Models (LLMs) in instruction-following and rhetorical fluency. However,
systematic exploration of their dual capabilities to autonomously persuade and
resist persuasion, particularly in contexts involving psychological rhetoric,
remains unexplored. In this paper, we first evaluate four commonly adopted LLMs
by tasking them to alternately act as persuaders and listeners in adversarial
dialogues. Empirical results show that persuader LLMs predominantly employ
repetitive strategies, leading to low success rates. Then we introduce eleven
comprehensive psychological persuasion strategies, finding that explicitly
instructing LLMs to adopt specific strategies such as Fluency Effect and
Repetition Effect significantly improves persuasion success rates. However, no
``one-size-fits-all'' strategy proves universally effective, with performance
heavily dependent on contextual counterfactuals. Motivated by these
observations, we propose an adaptive framework based on direct preference
optimization that trains LLMs to autonomously select optimal strategies by
leveraging persuasion results from strategy-specific responses as preference
pairs. Experiments on three open-source LLMs confirm that the proposed adaptive
psychological persuasion method effectively enables persuader LLMs to select
optimal strategies, significantly enhancing their success rates while
maintaining general capabilities. Our code is available at
https://github.com/KalinaEine/PsychologicalPersuasion.

</details>


### [124] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
*Subhendu Khatuya,Shashwat Naidu,Saptarshi Ghosh,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: The paper introduces LAGAMC, a generative model for multi-label text classification, achieving state-of-the-art results with significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Manual document classification is challenging due to the growing volume of textual data, necessitating more efficient, domain-agnostic systems.

Method: The model uses label descriptions, generates descriptions from input text, and maps them to labels using a finetuned sentence transformer with a dual-objective loss function combining cross-entropy and cosine similarity.

Result: LAGAMC demonstrated state-of-the-art performance with improvements of 13.94% in Micro-F1 and 24.85% in Macro-F1 scores across all evaluated datasets.

Conclusion: LAGAMC is efficient and versatile, making it suitable for diverse practical applications and effectively surpassing strong baselines in multi-label text classification.

Abstract: The explosion of textual data has made manual document classification
increasingly challenging. To address this, we introduce a robust, efficient
domain-agnostic generative model framework for multi-label text classification.
Instead of treating labels as mere atomic symbols, our approach utilizes
predefined label descriptions and is trained to generate these descriptions
based on the input text. During inference, the generated descriptions are
matched to the pre-defined labels using a finetuned sentence transformer. We
integrate this with a dual-objective loss function, combining cross-entropy
loss and cosine similarity of the generated sentences with the predefined
target descriptions, ensuring both semantic alignment and accuracy. Our
proposed model LAGAMC stands out for its parameter efficiency and versatility
across diverse datasets, making it well-suited for practical applications. We
demonstrate the effectiveness of our proposed model by achieving new
state-of-the-art performances across all evaluated datasets, surpassing several
strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in
Macro-F1 compared to the closest baseline across all datasets.

</details>


### [125] [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)
*James A. Michaelov,Reeka Estacio,Zhien Zhang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: The paper finds that language models often perform worse than chance in distinguishing impossible events from merely unlikely ones.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether language models can reliably distinguish between possible and improbable events by analyzing their understanding of possibility, typicality, and contextual relatedness.

Method: The paper tested several language models (Llama 3, Gemma 2, Mistral NeMo) against conditions involving impossible vs. unlikely events to determine the accuracy of their predictions.

Result: All tested models showed poor performance, sometimes performing worse than chance by assigning higher probabilities to impossible contexts compared to unlikely ones.

Conclusion: Language models lack robustness in distinguishing probabilities of contextual events, suggesting significant limitations in their reasoning capabilities.

Abstract: Can language models reliably predict that possible events are more likely
than merely improbable ones? By teasing apart possibility, typicality, and
contextual relatedness, we show that despite the results of previous work,
language models' ability to do this is far from robust. In fact, under certain
conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -
perform at worse-than-chance level, assigning higher probabilities to
impossible sentences such as 'the car was given a parking ticket by the brake'
than to merely unlikely sentences such as 'the car was given a parking ticket
by the explorer'.

</details>


### [126] [Advancing Question Generation with Joint Narrative and Difficulty Control](https://arxiv.org/abs/2506.06812)
*Bernardo Leite,Henrique Lopes Cardoso*

Main category: cs.CL

TL;DR: The paper addresses the challenge of combining narrative and difficulty controls in automated question generation for educational purposes.


<details>
  <summary>Details</summary>
Motivation: Existing research in question generation focuses on either difficulty-controllable or narrative-controllable aspects but lacks integration of both, which limits its educational applicability.

Method: The authors developed a joint strategy for controlling both narrative and difficulty aspects in reading comprehension question generation and evaluated its feasibility.

Result: The evaluation showed that the proposed strategy works in certain cases but is not universally effective across all instances.

Conclusion: The study identifies conditions under which the method is effective and discusses the trade-offs, indicating room for improvement in this combined control approach.

Abstract: Question Generation (QG), the task of automatically generating questions from
a source input, has seen significant progress in recent years.
Difficulty-controllable QG (DCQG) enables control over the difficulty level of
generated questions while considering the learner's ability. Additionally,
narrative-controllable QG (NCQG) allows control over the narrative aspects
embedded in the questions. However, research in QG lacks a focus on combining
these two types of control, which is important for generating questions
tailored to educational purposes. To address this gap, we propose a strategy
for Joint Narrative and Difficulty Control, enabling simultaneous control over
these two attributes in the generation of reading comprehension questions. Our
evaluation provides preliminary evidence that this approach is feasible, though
it is not effective across all instances. Our findings highlight the conditions
under which the strategy performs well and discuss the trade-offs associated
with its application.

</details>


### [127] [BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities](https://arxiv.org/abs/2506.06813)
*Dipto Das,Syed Ishtiaque Ahmed,Shion Guha*

Main category: cs.CL

TL;DR: This paper presents a dataset for Bengali transnational political discourse, addressing the lack of data in under-resourced languages.


<details>
  <summary>Details</summary>
Motivation: Understanding political discourse in online spaces for analyzing public opinion and polarization, with a focus on the under-resourced Bengali language.

Method: Hand-curated dataset collection using community-informed keyword-based retrieval across three online platforms with different community structures.

Result: A multilingual dataset was compiled, providing an overview of topics and multilingual political discourse content.

Conclusion: The dataset offers valuable resources for studying Bengali political discourse in multiple contexts and encourages further research in under-resourced languages.

Abstract: Understanding political discourse in online spaces is crucial for analyzing
public opinion and ideological polarization. While social computing and
computational linguistics have explored such discussions in English, such
research efforts are significantly limited in major yet under-resourced
languages like Bengali due to the unavailability of datasets. In this paper, we
present a multilingual dataset of Bengali transnational political discourse
(BTPD) collected from three online platforms, each representing distinct
community structures and interaction dynamics. Besides describing how we
hand-curated the dataset through community-informed keyword-based retrieval,
this paper also provides a general overview of its topics and multilingual
content.

</details>


### [128] [How do datasets, developers, and models affect biases in a low-resourced language?](https://arxiv.org/abs/2506.06816)
*Dipto Das,Shion Guha,Bryan Semaan*

Main category: cs.CL

TL;DR: The paper examines identity-based biases in sentiment analysis models for Bengali, finding them prevalent despite domain-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The authors address the understudied issue of identity-based biases in sociotechnical systems, especially for low-resource languages like Bengali.

Method: Sentiment analysis models (mBERT and BanglaBERT) were fine-tuned using all Bengali sentiment datasets available on Google Dataset Search and subjected to an algorithmic audit.

Result: Biases were detected in identity categories such as gender, religion, and nationality, regardless of similar semantic and structural contents used in models.

Conclusion: The findings underscore the complexities of combining diverse datasets and pre-trained models, highlighting broader concerns in AI alignment, epistemic injustice, and audit methodologies.

Abstract: Sociotechnical systems, such as language technologies, frequently exhibit
identity-based biases. These biases exacerbate the experiences of historically
marginalized communities and remain understudied in low-resource contexts.
While models and datasets specific to a language or with multilingual support
are commonly recommended to address these biases, this paper empirically tests
the effectiveness of such approaches in the context of gender, religion, and
nationality-based identities in Bengali, a widely spoken but low-resourced
language. We conducted an algorithmic audit of sentiment analysis models built
on mBERT and BanglaBERT, which were fine-tuned using all Bengali sentiment
analysis (BSA) datasets from Google Dataset Search. Our analyses showed that
BSA models exhibit biases across different identity categories despite having
similar semantic content and structure. We also examined the inconsistencies
and uncertainties arising from combining pre-trained models and datasets
created by individuals from diverse demographic backgrounds. We connected these
findings to the broader discussions on epistemic injustice, AI alignment, and
methodological decisions in algorithmic audits.

</details>


### [129] [Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs](https://arxiv.org/abs/2506.06820)
*Wenyu Zhang,Yingxu He,Geyu Lin,Zhuohan Liu,Shuo Sun,Bin Wang,Xunlong Zou,Jeremy H. M. Wong,Qiongqiong Wang,Hardik B. Sailor,Nancy F. Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: The paper proposes leveraging generative capabilities of AudioLLMs for improved emotion recognition, using a unified framework.


<details>
  <summary>Details</summary>
Motivation: To address limitations of AudioLLMs in paralinguistic tasks, especially emotion recognition and reasoning.

Method: A unified framework combining reasoning-enhanced supervision, dual-encoder architecture, and task-alternating training.

Result: Enhanced emotion prediction accuracy and coherent evidence-based explanations in two datasets: IEMOCAP and MELD.

Conclusion: The approach improves both the prediction and reasoning capabilities of AudioLLMs for emotion understanding.

Abstract: Audio Large Language Models (AudioLLMs) have achieved strong results in
semantic tasks like speech recognition and translation, but remain limited in
modeling paralinguistic cues such as emotion. Existing approaches often treat
emotion understanding as a classification problem, offering little insight into
the underlying rationale behind predictions. In this work, we explore emotion
reasoning, a strategy that leverages the generative capabilities of AudioLLMs
to enhance emotion recognition by producing semantically aligned,
evidence-grounded explanations. To support this in multitask AudioLLMs, we
introduce a unified framework combining reasoning-augmented data supervision,
dual-encoder architecture, and task-alternating training. This approach enables
AudioLLMs to effectively learn different tasks while incorporating emotional
reasoning. Experiments on IEMOCAP and MELD show that our approach not only
improves emotion prediction accuracy but also enhances the coherence and
evidential grounding of the generated responses.

</details>


### [130] [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)
*Arkadiusz Modzelewski,Witold Sosnowski,Tiziano Labruna,Adam Wierzbicki,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: This paper introduces the Persuasion-Augmented Chain of Thought (PCoT), a method leveraging persuasion knowledge to improve disinformation detection, achieving superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance disinformation detection by incorporating psychological insights, specifically knowledge of persuasive fallacies, into large language models to improve their zero-shot classification capabilities.

Method: The method involves developing the Persuasion-Augmented Chain of Thought (PCoT), which infuses persuasion knowledge into large language models to detect disinformation. The approach is evaluated on newly published datasets like EUDisinfo and MultiDis, ensuring content unfamiliar to the LLMs.

Result: PCoT outperformed competitive methods by 15% on average across trials involving five different LLMs and five datasets. The results were validated using new datasets designed to test content absent in LLMs' training data.

Conclusion: The study concludes that harnessing persuasion knowledge demonstrates significant potential for improving zero-shot disinformation detection, highlighting persuasion's critical role in media literacy.

Abstract: Disinformation detection is a key aspect of media literacy. Psychological
studies have shown that knowledge of persuasive fallacies helps individuals
detect disinformation. Inspired by these findings, we experimented with large
language models (LLMs) to test whether infusing persuasion knowledge enhances
disinformation detection. As a result, we introduce the Persuasion-Augmented
Chain of Thought (PCoT), a novel approach that leverages persuasion to improve
disinformation detection in zero-shot classification. We extensively evaluate
PCoT on online news and social media posts. Moreover, we publish two novel,
up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets
enable the evaluation of PCoT on content entirely unseen by the LLMs used in
our experiments, as the content was published after the models' knowledge
cutoffs. We show that, on average, PCoT outperforms competitive methods by 15%
across five LLMs and five datasets. These findings highlight the value of
persuasion in strengthening zero-shot disinformation detection.

</details>


### [131] [Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models](https://arxiv.org/abs/2506.06844)
*Naibin Gu,Peng Fu,Xiyu Liu,Ke Ma,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: PEFT adaptation struggles with performance degradation after base model updates; Trans-PEFT solves this by focusing on task patterns instead of task-specific knowledge.


<details>
  <summary>Details</summary>
Motivation: Updating base models causes performance drops in PEFT modules fine-tuned on earlier versions, demanding costly re-tuning.

Method: Analyzing updates in base models, emphasizing attention mechanisms over FFNs for task-specific patterns, and introducing the Trans-PEFT approach.

Result: Trans-PEFT modules retain performance across updated base models without re-tuning, validated by experiments involving 7 models and 12 datasets.

Conclusion: Trans-PEFT offers a practical, efficient solution to maintain PEFT module performance during base model updates, reducing computational overhead.

Abstract: Parameter-efficient fine-tuning (PEFT) has become a common method for
fine-tuning large language models, where a base model can serve multiple users
through PEFT module switching. To enhance user experience, base models require
periodic updates. However, once updated, PEFT modules fine-tuned on previous
versions often suffer substantial performance degradation on newer versions.
Re-tuning these numerous modules to restore performance would incur significant
computational costs. Through a comprehensive analysis of the changes that occur
during base model updates, we uncover an interesting phenomenon: continual
training primarily affects task-specific knowledge stored in Feed-Forward
Networks (FFN), while having less impact on the task-specific pattern in the
Attention mechanism. Based on these findings, we introduce Trans-PEFT, a novel
approach that enhances the PEFT module by focusing on the task-specific pattern
while reducing its dependence on certain knowledge in the base model. Further
theoretical analysis supports our approach. Extensive experiments across 7 base
models and 12 datasets demonstrate that Trans-PEFT trained modules can maintain
performance on updated base models without re-tuning, significantly reducing
maintenance overhead in real-world applications.

</details>


### [132] [Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning](https://arxiv.org/abs/2506.06877)
*Jiaxing Guo,Wenjie Yang,Shengzhong Zhang,Tongshan Xu,Lun Du,Da Zheng,Zengfeng Huang*

Main category: cs.CL

TL;DR: Existing LLMs perform well in math problem-solving but often use unsound reasoning, termed as reward hacking. MathOlympiadEval dataset and ParaStepVerifier methodology aim to provide fine-grained evaluation and correction of reasoning.


<details>
  <summary>Details</summary>
Motivation: The paper highlights a critical flaw in LLMs where they frequently obtain correct results despite using flawed reasoning processes, termed as reward hacking. Such flaws are invisible in traditional evaluations, resulting in unreliable model performance in mathematical reasoning.

Method: The authors introduced MathOlympiadEval, a meticulously annotated dataset, and proposed ParaStepVerifier, a step-by-step reasoning verification methodology. This ensures detailed scrutiny of each reasoning step within mathematical solutions.

Result: Empirical evaluations show that the ParaStepVerifier methodology significantly outperforms existing methods, like LLM-as-a-judge, in identifying reasoning errors, especially in complex, multi-step mathematical problems.

Conclusion: The proposed dataset and verification methodology pave the way for improving LLM evaluation and training processes, ensuring alignment between correctness of answers and soundness of mathematical reasoning.

Abstract: Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable
success in mathematical problem-solving. However, this success often masks a
critical issue: models frequently achieve correct answers through fundamentally
unsound reasoning processes, a phenomenon indicative of reward hacking. We
introduce MathOlympiadEval, a new dataset with fine-grained annotations, which
reveals a significant gap between LLMs' answer correctness and their low
process correctness. Existing automated methods like LLM-as-a-judge struggle to
reliably detect these reasoning flaws. To address this, we propose
ParaStepVerifier, a novel methodology for meticulous, step-by-step verification
of mathematical solutions. ParaStepVerifier identifies incorrect reasoning
steps. Empirical results demonstrate that ParaStepVerifier substantially
improves the accuracy of identifying flawed solutions compared to baselines,
especially for complex, multi-step problems. This offers a more robust path
towards evaluating and training LLMs with genuine mathematical reasoning.

</details>


### [133] [Mixture of Small and Large Models for Chinese Spelling Check](https://arxiv.org/abs/2506.06887)
*Ziheng Qiao,Houquan Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: This paper introduces a dynamic mixture approach for Chinese Spelling Check using a combination of small models and LLMs during decoding, achieving state-of-the-art results without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The performance of LLMs in Chinese Spelling Check has been unsatisfactory compared to fine-tuned BERT-based models. The paper aimed to overcome edit pattern overfitting and resource-intense fine-tuning.

Method: The authors developed a dynamic mixture approach that integrates small models and LLMs to combine their strengths during the beam search decoding phase.

Result: The mixture approach significantly improved error correction and set state-of-the-art performance across several datasets, without requiring LLM fine-tuning.

Conclusion: The approach enhances correction accuracy, improves fluency, saves resources, and supports domain adaptation, ultimately advancing the Chinese Spelling Check task.

Abstract: In the era of large language models (LLMs), the Chinese Spelling Check (CSC)
task has seen various LLM methods developed, yet their performance remains
unsatisfactory. In contrast, fine-tuned BERT-based models, relying on
high-quality in-domain data, show excellent performance but suffer from edit
pattern overfitting. This paper proposes a novel dynamic mixture approach that
effectively combines the probability distributions of small models and LLMs
during the beam search decoding phase, achieving a balanced enhancement of
precise corrections from small models and the fluency of LLMs. This approach
also eliminates the need for fine-tuning LLMs, saving significant time and
resources, and facilitating domain adaptation. Comprehensive experiments
demonstrate that our mixture approach significantly boosts error correction
capabilities, achieving state-of-the-art results across multiple datasets. Our
code is available at https://github.com/zhqiao-nlp/MSLLM.

</details>


### [134] [Automatic Speech Recognition of African American English: Lexical and Contextual Effects](https://arxiv.org/abs/2506.06888)
*Hamid Mojarad,Kevin Tang*

Main category: cs.CL

TL;DR: This paper studies the difficulties automatic speech recognition (ASR) face in recognizing features specific to African American English (AAE), with a focus on consonant cluster reduction and ING-reduction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of ASR models in recognizing phonetic, phonological, and morphosyntactic features of AAE speech.

Method: Researchers used wav2vec 2.0 ASR models, with and without language models (LMs), on CORAAL corpus to test effects of lexical features and employed Montreal Forced Aligner (MFA) for detection of pronounced features like CCR and ING-reduction.

Result: The study finds CCR and ING-reduction slightly but significantly affect word error rates (WER), and also identifies that ASR systems without LMs are influenced more by lexical neighborhood effects than contextual predictability.

Conclusion: ASR systems struggle with specific AAE features, and systems without LMs exhibit greater sensitivity to lexical effects than contextual information.

Abstract: Automatic Speech Recognition (ASR) models often struggle with the phonetic,
phonological, and morphosyntactic features found in African American English
(AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction
(CCR) and ING-reduction. It examines whether the presence of CCR and
ING-reduction increases ASR misrecognition. Subsequently, it investigates
whether end-to-end ASR systems without an external Language Model (LM) are more
influenced by lexical neighborhood effect and less by contextual predictability
compared to systems with an LM. The Corpus of Regional African American
Language (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR
and ING-reduction were detected using the Montreal Forced Aligner (MFA) with
pronunciation expansion. The analysis reveals a small but significant effect of
CCR and ING on Word Error Rate (WER) and indicates a stronger presence of
lexical neighborhood effect in ASR systems without LMs.

</details>


### [135] [Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis](https://arxiv.org/abs/2506.06929)
*Mikhail Krasitskii,Grigori Sidorov,Olga Kolesnikova,Liliana Chanona Hernandez,Alexander Gelbukh*

Main category: cs.CL

TL;DR: The paper introduces a hybrid multilingual sentiment analysis model combining TF-IDF-based extraction and fine-tuned XLM-R for significant performance and efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of solely extractive or abstractive sentiment analysis methods in multilingual settings.

Method: The approach integrates TF-IDF extraction with fine-tuned XLM-R for summarization, applying dynamic thresholding and cultural adaptation.

Result: Achieved 0.90 accuracy for English, 0.84 accuracy for low-resource languages, and demonstrated 22% better computational efficiency.

Conclusion: The hybrid model effectively enhances sentiment analysis in multilingual contexts and has promising real-world applications, with further optimization for low-resource languages planned.

Abstract: We propose a hybrid approach for multilingual sentiment analysis that
combines extractive and abstractive summarization to address the limitations of
standalone methods. The model integrates TF-IDF-based extraction with a
fine-tuned XLM-R abstractive module, enhanced by dynamic thresholding and
cultural adaptation. Experiments across 10 languages show significant
improvements over baselines, achieving 0.90 accuracy for English and 0.84 for
low-resource languages. The approach also demonstrates 22% greater
computational efficiency than traditional methods. Practical applications
include real-time brand monitoring and cross-cultural discourse analysis.
Future work will focus on optimization for low-resource languages via 8-bit
quantization.

</details>


### [136] [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)
*Alexander Spangher,Tenghao Huang,Jialiang Gu,Jiatong Shi,Muhao Chen*

Main category: cs.CL

TL;DR: This paper presents DiscoSum, a novel algorithm for structure-aware text summarization, focusing on improving discourse structure adherence in news articles.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to address the issue of language models failing to maintain long-term discourse structure in summarizations, especially for news articles where organizational flow is crucial.

Method: They introduced a summarization dataset consisting of news articles summarized in various styles for different social media platforms. Additionally, they developed a news discourse schema and a beam search-based method called DiscoSum for generating structure-aware summaries.

Result: Human and automated evaluations show that DiscoSum effectively maintains narrative fidelity and tailoring to different structural and stylistic requirements.

Conclusion: The study successfully demonstrates the importance of incorporating discourse structure for more engaging and accurate news article summaries across different platforms.

Abstract: Recent advances in text summarization have predominantly leveraged large
language models to generate concise summaries. However, language models often
do not maintain long-term discourse structure, especially in news articles,
where organizational flow significantly influences reader engagement. We
introduce a novel approach to integrating discourse structure into
summarization processes, focusing specifically on news articles across various
media. We present a novel summarization dataset where news articles are
summarized multiple times in different ways across different social media
platforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse
schema to describe summarization structures and a novel algorithm, DiscoSum,
which employs beam search technique for structure-aware summarization, enabling
the transformation of news stories to meet different stylistic and structural
demands. Both human and automatic evaluation results demonstrate the efficacy
of our approach in maintaining narrative fidelity and meeting structural
requirements.

</details>


### [137] [What Makes a Good Natural Language Prompt?](https://arxiv.org/abs/2506.06950)
*Do Xuan Long,Duy Dinh,Ngoc-Hai Nguyen,Kenji Kawaguchi,Nancy F. Chen,Shafiq Joty,Min-Yen Kan*

Main category: cs.CL

TL;DR: The paper conducts a meta-analysis on over 150 prompting-related studies, proposing a new property- and human-centric framework to evaluate natural language prompts for large language models (LLMs) and identifying research gaps in the field.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to better understand and evaluate natural language prompts used in human-AI communications with large language models, as there is no clear consensus on assessing their quality.

Method: The authors conducted a meta-analysis of over 150 studies on prompt design between 2022 and 2025. They proposed a framework focused on 21 properties across six dimensions and analyzed correlations among these properties. They also empirically tested multi-property enhancements in reasoning tasks.

Result: The study found imbalanced support for prompt evaluation across models and tasks, identified research gaps, and observed that single-property enhancements are most impactful. Instruction-tuning on property-enhanced prompts improved reasoning model performance.

Conclusion: The findings establish a new framework for prompt evaluation and optimization, offering a structured approach to advance human-AI communication and prompting research.

Abstract: As large language models (LLMs) have progressed towards more human-like and
human--AI communications have become prevalent, prompting has emerged as a
decisive component. However, there is limited conceptual consensus on what
exactly quantifies natural language prompts. We attempt to address this
question by conducting a meta-analysis surveying more than 150
prompting-related papers from leading NLP and AI conferences from 2022 to 2025
and blogs. We propose a property- and human-centric framework for evaluating
prompt quality, encompassing 21 properties categorized into six dimensions. We
then examine how existing studies assess their impact on LLMs, revealing their
imbalanced support across models and tasks, and substantial research gaps.
Further, we analyze correlations among properties in high-quality natural
language prompts, deriving prompting recommendations. We then empirically
explore multi-property prompt enhancements in reasoning tasks, observing that
single-property enhancements often have the greatest impact. Finally, we
discover that instruction-tuning on property-enhanced prompts can result in
better reasoning models. Our findings establish a foundation for
property-centric prompt evaluation and optimization, bridging the gaps between
human--AI communication and opening new prompting research directions.

</details>


### [138] [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
*Ha-Thanh Nguyen,Chaoran Liu,Hirokazu Kiyomaru,Koichi Takeda,Yusuke Miyao,Maki Matsuda,Yusuke Oda,Pontus Stenetorp,Qianying Liu,Su Myat Noe,Hideyuki Tachibana,Kouta Nakayama,Sadao Kurohashi*

Main category: cs.CL

TL;DR: The paper introduces BIS Reasoning 1.0, a Japanese dataset for evaluating biases in syllogistic reasoning in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: There was a need to evaluate how LLMs handle reasoning tasks where logic conflicts with human beliefs.

Method: Researchers created BIS Reasoning 1.0, included belief-inconsistent syllogisms, and benchmarked the performance of state-of-the-art LLMs.

Result: GPT-4 achieved the highest accuracy (79.54%), exposing weaknesses in belief-inconsistent reasoning among LLMs.

Conclusion: LLMs need further improvement to handle logical but belief-challenging problems, especially for applications in critical domains like law and healthcare.

Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of
syllogistic reasoning problems explicitly designed to evaluate
belief-inconsistent reasoning in large language models (LLMs). Unlike prior
datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned
reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent
syllogisms to uncover reasoning biases in LLMs trained on human-aligned
corpora. We benchmark state-of-the-art models - including GPT models, Claude
models, and leading Japanese LLMs - revealing significant variance in
performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies
critical weaknesses in current LLMs when handling logically valid but
belief-conflicting inputs. These findings have important implications for
deploying LLMs in high-stakes domains such as law, healthcare, and scientific
literature, where truth must override intuitive belief to ensure integrity and
safety.

</details>


### [139] [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: This work develops a method for QA agents to ask clarifying questions using simulated conversations and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To enable question answering agents to ask clarifying questions for better response accuracy.

Method: The paper simulates clarifying conversations and trains QA agents using an offline reinforcement learning approach viewed as reward-weighted supervised fine-tuning.

Result: The proposed method outperforms recently suggested SFT and preference optimization techniques by improving both rewards and language quality.

Conclusion: Simulating conversations and applying reinforcement learning is an effective strategy to improve clarifying question capabilities in QA agents.

Abstract: Question answering (QA) agents automatically answer questions posed in
natural language. In this work, we learn to ask clarifying questions in QA
agents. The key idea in our method is to simulate conversations that contain
clarifying questions and learn from them using reinforcement learning (RL). To
make RL practical, we propose and analyze offline RL objectives that can be
viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in
large language models. Our work stands in a stark contrast to recently proposed
methods, based on SFT and direct preference optimization, which have additional
hyper-parameters and do not directly optimize rewards. We compare to these
methods empirically and report gains in both optimized rewards and language
quality.

</details>


### [140] [A dependently-typed calculus of event telicity and culminativity](https://arxiv.org/abs/2506.06968)
*Pavel Kovalev,Carlo Angiuli*

Main category: cs.CL

TL;DR: The paper introduces a dependently-typed cross-linguistic framework to analyze telicity and culminativity of events, applied to English sentence analysis, formalized using the Agda proof assistant.


<details>
  <summary>Details</summary>
Motivation: To provide a precise linguistic framework for analyzing boundedness, telicity, and culminativity within nominal and verbal domains and their entailments.

Method: The framework is built by extending intensional Martin-Löf dependent type theory, focusing on modeling boundedness in noun phrases and dependent event calculus for telic and culminating events, with formalization in Agda.

Result: The paper showcases rules and examples of using the framework, effectively modeling linguistic phenomena and associated entailments in English sentences.

Conclusion: The dependently-typed framework successfully captures the nuanced semantics of telicity and culminativity in natural language, facilitating deeper linguistic analysis.

Abstract: We present a dependently-typed cross-linguistic framework for analyzing the
telicity and culminativity of events, accompanied by examples of using our
framework to model English sentences. Our framework consists of two parts. In
the nominal domain, we model the boundedness of noun phrases and its
relationship to subtyping, delimited quantities, and adjectival modification.
In the verbal domain we define a dependent event calculus, modeling telic
events as those whose undergoer is bounded, culminating events as telic events
that achieve their inherent endpoint, and consider adverbial modification. In
both domains we pay particular attention to associated entailments. Our
framework is defined as an extension of intensional Martin-L\"of dependent type
theory, and the rules and examples in this paper have been formalized in the
Agda proof assistant.

</details>


### [141] [Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971)
*Jaechul Roh,Varun Gandhi,Shivani Anilkumar,Arin Garg*

Main category: cs.CL

TL;DR: The paper examines the robustness of reasoning in Large Language Models (LLMs) using adversarial prompt perturbations, revealing fragility and sensitivity to semantic and surface-level changes in prompts.


<details>
  <summary>Details</summary>
Motivation: The study aims to determine whether LLMs genuinely reason or if their success in reasoning tasks arises from identifying shallow statistical patterns.

Method: The authors introduced adversarial prompt perturbations to evaluate LLM robustness through transformations like storytelling reframing, irrelevant constraints, reordering, and numeric changes. They tested on 700 perturbed code generation tasks based on LeetCode problems.

Result: Performance degraded significantly for some perturbations (accuracy drop up to -42.1%), while others surprisingly improved accuracy by up to 35.3%.

Conclusion: The findings reveal the fragile and unpredictable nature of current reasoning models, advocating for more robust approaches to align reasoning capabilities and improve prompt robustness. The study also provides datasets and frameworks for further research.

Abstract: Large Language Models (LLMs) have achieved remarkable success in tasks
requiring complex reasoning, such as code generation, mathematical problem
solving, and algorithmic synthesis -- especially when aided by reasoning tokens
and Chain-of-Thought prompting. Yet, a core question remains: do these models
truly reason, or do they merely exploit shallow statistical patterns? In this
paper, we systematically investigate the robustness of reasoning LLMs by
introducing a suite of semantically faithful yet adversarially structured
prompt perturbations. Our evaluation -- spanning 700 perturbed code generations
derived from LeetCode-style problems -- applies transformations such as
storytelling reframing, irrelevant constraint injection, example reordering,
and numeric perturbation. We observe that while certain modifications severely
degrade performance (with accuracy drops up to -42.1%), others surprisingly
improve model accuracy by up to 35.3%, suggesting sensitivity not only to
semantics but also to surface-level prompt dynamics. These findings expose the
fragility and unpredictability of current reasoning systems, underscoring the
need for more principles approaches to reasoning alignments and prompting
robustness. We release our perturbation datasets and evaluation framework to
promote further research in trustworthy and resilient LLM reasoning.

</details>


### [142] [Atomic Reasoning for Scientific Table Claim Verification](https://arxiv.org/abs/2506.06972)
*Yuji Zhang,Qingyun Wang,Cheng Qian,Jiateng Liu,Chenkai Sun,Denghui Zhang,Tarek Abdelzaher,Chengxiang Zhai,Preslav Nakov,Heng Ji*

Main category: cs.CL

TL;DR: This paper addresses the challenge of accurately interpreting scientific table claims by proposing a modular reasoning approach using reusable components, which outperforms state-of-the-art models with less data.


<details>
  <summary>Details</summary>
Motivation: Misinformation from complex scientific texts and tables can mislead non-experts. Current models lack precise fine-grained reasoning for verifying these claims.

Method: The authors introduced 'atomic skills' as modular reasoning components in a skill-chaining schema to reduce cognitive load and enhance scientific table claim verification.

Result: Their model, using only 350 fine-tuning examples, surpassed GPT-4o's chain-of-thought method and achieved state-of-the-art performance with minimal training data.

Conclusion: The modular approach with atomic skills improves accuracy and requires fewer resources, making it effective and efficient for scientific claim verification.

Abstract: Scientific texts often convey authority due to their technical language and
complex data. However, this complexity can sometimes lead to the spread of
misinformation. Non-experts are particularly susceptible to misleading claims
based on scientific tables due to their high information density and perceived
credibility. Existing table claim verification models, including
state-of-the-art large language models (LLMs), often struggle with precise
fine-grained reasoning, resulting in errors and a lack of precision in
verifying scientific claims. Inspired by Cognitive Load Theory, we propose that
enhancing a model's ability to interpret table-based claims involves reducing
cognitive load by developing modular, reusable reasoning components (i.e.,
atomic skills). We introduce a skill-chaining schema that dynamically composes
these skills to facilitate more accurate and generalizable reasoning with a
reduced cognitive load. To evaluate this, we create SciAtomicBench, a
cross-domain benchmark with fine-grained reasoning annotations. With only 350
fine-tuning examples, our model trained by atomic reasoning outperforms
GPT-4o's chain-of-thought method, achieving state-of-the-art results with far
less training data.

</details>


### [143] [Chain of Methodologies: Scaling Test Time Computation without Training](https://arxiv.org/abs/2506.06982)
*Cong Liu,Jie Wu,Weigang Wu,Xu Chen,Liang Lin,Wei-Shi Zheng*

Main category: cs.CL

TL;DR: The paper presents Chain of Methodologies (CoM), a prompting framework designed to enhance complex reasoning in LLMs by incorporating human-methodological insights, without requiring additional training.


<details>
  <summary>Details</summary>
Motivation: To address the challenge that LLMs face in performing complex reasoning tasks due to a lack of structured human insight in their training data.

Method: The authors propose CoM, a prompting framework that uses the metacognitive abilities of advanced LLMs to activate structured reasoning through user-defined methodologies, eliminating the need for model fine-tuning.

Result: Experiments demonstrate that CoM achieves superior performance compared to competing baseline methods and proves effective for improving extended reasoning in LLMs.

Conclusion: CoM represents a significant step toward enabling human-like reasoning in LLMs through training-free prompting that integrates human-inspired methodologies.

Abstract: Large Language Models (LLMs) often struggle with complex reasoning tasks due
to insufficient in-depth insights in their training data, which are typically
absent in publicly available documents. This paper introduces the Chain of
Methodologies (CoM), an innovative and intuitive prompting framework that
enhances structured thinking by integrating human methodological insights,
enabling LLMs to tackle complex tasks with extended reasoning. CoM leverages
the metacognitive abilities of advanced LLMs, activating systematic reasoning
throught user-defined methodologies without explicit fine-tuning. Experiments
show that CoM surpasses competitive baselines, demonstrating the potential of
training-free prompting methods as robust solutions for complex reasoning tasks
and bridging the gap toward human-level reasoning through human-like
methodological insights.

</details>


### [144] [Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors](https://arxiv.org/abs/2506.06987)
*Senqi Yang,Dongyu Zhang,Jing Ren,Ziqi Xu,Xiuzhen Zhang,Yiliao Song,Hongfei Lin,Feng Xia*

Main category: cs.CL

TL;DR: This paper addresses cultural bias in NLP metaphor processing by introducing a diverse multimedia dataset (MultiMM) and a baseline model (SEMD) to improve cross-cultural metaphor understanding.


<details>
  <summary>Details</summary>
Motivation: The research seeks to tackle the issue of cultural bias in NLP, which limits metaphor comprehension and model fairness across non-Western languages and cultures.

Method: The authors created the MultiMM dataset with a multicultural focus on Chinese and English text-image pairs. They also devised SEMD, a baseline model integrating sentiment embeddings for better cross-cultural metaphor detection.

Result: Experiments demonstrated SEMD's effectiveness in both metaphor detection and sentiment analysis, validating its capability to handle multicultural and multimodal data.

Conclusion: This study strengthens awareness of cultural biases in NLP, urging more inclusive language models, while providing open-access resources for future research.

Abstract: Metaphors are pervasive in communication, making them crucial for natural
language processing (NLP). Previous research on automatic metaphor processing
predominantly relies on training data consisting of English samples, which
often reflect Western European or North American biases. This cultural skew can
lead to an overestimation of model performance and contributions to NLP
progress. However, the impact of cultural bias on metaphor processing,
particularly in multimodal contexts, remains largely unexplored. To address
this gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset
designed for cross-cultural studies of metaphor in Chinese and English. MultiMM
consists of 8,461 text-image advertisement pairs, each accompanied by
fine-grained annotations, providing a deeper understanding of multimodal
metaphors beyond a single cultural domain. Additionally, we propose
Sentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates
sentiment embeddings to enhance metaphor comprehension across cultural
backgrounds. Experimental results validate the effectiveness of SEMD on
metaphor detection and sentiment analysis tasks. We hope this work increases
awareness of cultural bias in NLP research and contributes to the development
of fairer and more inclusive language models. Our dataset and code are
available at https://github.com/DUTIR-YSQ/MultiMM.

</details>


### [145] [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
*Ming Li,Zhengyuan Yang,Xiyao Wang,Dianqi Li,Kevin Lin,Tianyi Zhou,Lijuan Wang*

Main category: cs.CL

TL;DR: The paper examines inefficiencies in large reasoning models (LRMs), introducing a new method, FoReaL-Decoding, to reduce computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the inefficiency of LRMs due to verbose reasoning traces and overthinking, prompting a search for methods to reduce these inefficiencies without sacrificing task performance.

Method: They propose FoReaL-Decoding, a collaborative decoding method where a high-quality model generates initial tokens for each sentence, with a smaller draft model completing the rest to reduce computational costs.

Result: FoReaL-Decoding demonstrates a reduction in computation (FLOPs) by 30-50%, shortens chain-of-thought lengths by up to 40%, and retains 86-100% performance on math reasoning benchmarks like AIME24 and GPQA-Diamond.

Conclusion: FoReaL-Decoding is an effective approach for reducing computational overhead in LRMs while maintaining strong reasoning accuracy, providing a simple and practical solution for cost-quality trade-offs.

Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by
emitting long chains of thought. Yet, these verbose traces slow down inference
and often drift into unnecessary detail, known as the overthinking phenomenon.
To better understand LRMs' behavior, we systematically analyze the token-level
misalignment between reasoning and non-reasoning models. While it is expected
that their primary difference lies in the stylistic "thinking cues", LRMs
uniquely exhibit two pivotal, previously under-explored phenomena: a Global
Misalignment Rebound, where their divergence from non-reasoning models persists
or even grows as response length increases, and more critically, a Local
Misalignment Diminish, where the misalignment concentrates at the "thinking
cues" each sentence starts with but rapidly declines in the remaining of the
sentence. Motivated by the Local Misalignment Diminish, we propose
FoReaL-Decoding, a collaborative fast-slow thinking decoding method for
cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few
tokens for each sentence, and then a weaker draft model completes the following
tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to
smoothly interpolate between the small and the large model. On four popular
math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),
FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by
up to 40%, while preserving 86 to 100% of model performance. These results
establish FoReaL-Decoding as a simple, plug-and-play route to controllable
cost-quality trade-offs in reasoning-centric tasks.

</details>


### [146] [Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text](https://arxiv.org/abs/2506.07001)
*Yize Cheng,Vinu Sankar Sadasivan,Mehrdad Saberi,Shoumik Saha,Soheil Feizi*

Main category: cs.CL

TL;DR: This paper introduces Adversarial Paraphrasing, a training-free framework to evade AI-generated text detectors, showing striking bypass effectiveness across diverse detection systems.


<details>
  <summary>Details</summary>
Motivation: Concerns about misuse of LLMs in plagiarism and social engineering have intensified, highlighting the need for robust AI-generated text detection methods.

Method: The authors use an instruction-following LLM to adversarially paraphrase AI-generated content under the guidance of AI detectors, creating optimized texts specifically aimed at evading detection.

Result: Adversarial paraphrasing reduces detection rates significantly, e.g., by 64.49% on RADAR and 98.96% on Fast-DetectGPT, demonstrating high effectiveness and transferability across several detection systems.

Conclusion: The study underscores the need for more resilient AI text detection strategies to counter increasingly sophisticated evasion techniques like adversarial paraphrasing.

Abstract: The increasing capabilities of Large Language Models (LLMs) have raised
concerns about their misuse in AI-generated plagiarism and social engineering.
While various AI-generated text detectors have been proposed to mitigate these
risks, many remain vulnerable to simple evasion techniques such as
paraphrasing. However, recent detectors have shown greater robustness against
such basic attacks. In this work, we introduce Adversarial Paraphrasing, a
training-free attack framework that universally humanizes any AI-generated text
to evade detection more effectively. Our approach leverages an off-the-shelf
instruction-following LLM to paraphrase AI-generated content under the guidance
of an AI text detector, producing adversarial examples that are specifically
optimized to bypass detection. Extensive experiments show that our attack is
both broadly effective and highly transferable across several detection
systems. For instance, compared to simple paraphrasing attack--which,
ironically, increases the true positive at 1% false positive (T@1%F) by 8.57%
on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by
OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on
Fast-DetectGPT. Across a diverse set of detectors--including neural
network-based, watermark-based, and zero-shot approaches--our attack achieves
an average T@1%F reduction of 87.88% under the guidance of
OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and
attack success to find that our method can significantly reduce detection
rates, with mostly a slight degradation in text quality. Our adversarial setup
highlights the need for more robust and resilient detection strategies in the
light of increasingly sophisticated evasion techniques.

</details>


### [147] [Correlated Errors in Large Language Models](https://arxiv.org/abs/2506.07962)
*Elliot Kim,Avi Garg,Kenny Peng,Nikhil Garg*

Main category: cs.CL

TL;DR: The study examined over 350 LLMs and found substantial error correlation among models, especially larger and more accurate ones.


<details>
  <summary>Details</summary>
Motivation: To empirically test the assumption that diversity in training data, architectures, and providers reduces homogeneity in LLM errors.

Method: Conducted a large-scale empirical evaluation on two popular leaderboards and a resume-screening task, analyzing error correlation across various LLMs.

Result: Models showed 60% correlation in errors on one dataset, with larger and more accurate models having highly correlated errors even across diverse setups.

Conclusion: Diversity among LLMs does not ensure reduced correlation in errors, impacting downstream applications like evaluation frameworks and algorithmic decision-making.

Abstract: Diversity in training data, architecture, and providers is assumed to
mitigate homogeneity in LLMs. However, we lack empirical evidence on whether
different LLMs differ meaningfully. We conduct a large-scale empirical
evaluation on over 350 LLMs overall, using two popular leaderboards and a
resume-screening task. We find substantial correlation in model errors -- on
one leaderboard dataset, models agree 60% of the time when both models err. We
identify factors driving model correlation, including shared architectures and
providers. Crucially, however, larger and more accurate models have highly
correlated errors, even with distinct architectures and providers. Finally, we
show the effects of correlation in two downstream tasks: LLM-as-judge
evaluation and hiring -- the latter reflecting theoretical predictions
regarding algorithmic monoculture.

</details>


### [148] [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
*Bhuiyan Sanjid Shafique,Ashmal Vayani,Muhammad Maaz,Hanoona Abdul Rasheed,Dinura Dissanayake,Mohammed Irfan Kurpath,Yahya Hmaiti,Go Inoue,Jean Lahoud,Md. Safirur Rashid,Shadid Intisar Quasem,Maheen Fatima,Franco Vidal,Mykola Maslych,Ketan Pravin More,Sanoojan Baliah,Hasindri Watawana,Yuhao Li,Fabian Farestam,Leon Schaller,Roman Tymtsiv,Simon Weber,Hisham Cholakkal,Ivan Laptev,Shin'ichi Satoh,Michael Felsberg,Mubarak Shah,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CL

TL;DR: The paper introduces ViMUL-Bench, a multilingual video LMM benchmark covering 14 languages and culturally diverse categories, alongside a massive multilingual training dataset and a new model called ViMUL.


<details>
  <summary>Details</summary>
Motivation: To address the lack of inclusivity and research focused on multilingual and culturally diverse video large multimodal models (LMMs).

Method: ViMUL-Bench, a benchmark for evaluating Video LMMs across 14 languages and 15 categories, was designed. A machine-translated multilingual training dataset with 1.2 million samples was created, and the authors also developed a new Video LMM named ViMUL.

Result: ViMUL-Bench successfully assesses video LMMs across languages and cultures, and the ViMUL model balances effectiveness between high- and low-resource languages for video understanding.

Conclusion: The proposed ViMUL-Bench, multilingual training data, and the ViMUL model are significant steps towards culturally and linguistically inclusive video LMMs and will be publicly available to facilitate further research.

Abstract: Large multimodal models (LMMs) have recently gained attention due to their
effectiveness to understand and generate descriptions of visual content. Most
existing LMMs are in English language. While few recent works explore
multilingual image LMMs, to the best of our knowledge, moving beyond the
English language for cultural and linguistic inclusivity is yet to be
investigated in the context of video LMMs. In pursuit of more inclusive video
LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to
evaluate Video LMMs across 14 languages, including both low- and high-resource
languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,
Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is
designed to rigorously test video LMMs across 15 categories including eight
culturally diverse categories, ranging from lifestyles and festivals to foods
and rituals and from local landmarks to prominent cultural personalities.
ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice
questions spanning various video durations (short, medium, and long) with 8k
samples that are manually verified by native language speakers. In addition, we
also introduce a machine translated multilingual video training set comprising
1.2 million samples and develop a simple multilingual video LMM, named ViMUL,
that is shown to provide a better tradeoff between high-and low-resource
languages for video understanding. We hope our ViMUL-Bench and multilingual
video LMM along with a large-scale multilingual video training set will help
ease future research in developing cultural and linguistic inclusive
multilingual video LMMs. Our proposed benchmark, video LMM and training data
will be publicly released at https://mbzuai-oryx.github.io/ViMUL/.

</details>


### [149] [KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering](https://arxiv.org/abs/2506.07037)
*Zhongze Luo,Weixuan Wan,Qizhi Zheng,Yanhong Bai,Jingyun Sun,Jian Wang,Dan Wang*

Main category: cs.CL

TL;DR: The paper combines fine-tuning large language models with knowledge graphs to create an intelligent Q&A system for communication standards, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in the traditional consulting model for communication standards, which is slow, dependent on expert knowledge, and unable to keep up with rapid technological advancements.

Method: Fine-tuning Qwen2.5-7B-Instruct language model using the LoRA method on a dataset of 6,587 Q&A pairs, constructing a knowledge graph with 13,906 entities and 13,524 relations, and integrating both into a retrieval-augmented Q&A framework.

Result: The system showed improved performance metrics such as BLEU-4 (18.8564 to 66.8993) and ROUGE scores, outperforming Llama-3-8B-Instruct. All evaluation angles improved under the RAG framework, with an average score increase of 2.26%.

Conclusion: The intelligent consultation system demonstrates high practical application value by combining advanced AI techniques like language model fine-tuning and graphical retrieval, improving query accuracy and user interaction experience.

Abstract: There are many types of standards in the field of communication. The
traditional consulting model has a long cycle and relies on the knowledge and
experience of experts, making it difficult to meet the rapidly developing
technological demands. This paper combines the fine-tuning of large language
models with the construction of knowledge graphs to implement an intelligent
consultation and question-answering system for communication standards. The
experimental results show that after LoRA tuning on the constructed dataset of
6,587 questions and answers in the field of communication standards,
Qwen2.5-7B-Instruct demonstrates outstanding professional capabilities in the
field of communication standards on the test set. BLEU-4 rose from 18.8564 to
66.8993, and evaluation indicators such as ROUGE also increased significantly,
outperforming the fine-tuning effect of the comparison model
Llama-3-8B-Instruct. Based on the ontology framework containing 6 entity
attributes and 10 relation attributes, a knowledge graph of the communication
standard domain containing 13,906 entities and 13,524 relations was
constructed, showing a relatively good query accuracy rate. The intelligent
consultation and question-answering system enables the fine-tuned model on the
server side to access the locally constructed knowledge graph and conduct
graphical retrieval of key information first, which is conducive to improving
the question-answering effect. The evaluation using DeepSeek as the Judge on
the test set shows that our RAG framework enables the fine-tuned model to
improve the scores at all five angles, with an average score increase of 2.26%.
And combined with web services and API interfaces, it has achieved very good
results in terms of interaction experience and back-end access, and has very
good practical application value.

</details>


### [150] [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
*Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: This paper introduces methods leveraging multiple large language models (LLMs) for extracting historical events from text, combined with enhancement strategies and formal verification in Coq.


<details>
  <summary>Details</summary>
Motivation: Extracting structured representations of historical events manually is computationally expensive, and the limitations of RDF/OWL reasoners in handling deeper temporal and semantic analysis necessitate novel methods.

Method: The paper develops automatic historical event extraction from text using LLMs (GPT-4, Claude, Llama 3.2) with three enhancement strategies: base generation, knowledge graph enhancement, and Retrieval-Augmented Generation (RAG), followed by evaluation through historical texts and automated Coq formalization.

Result: Enhancement strategies optimize distinct aspects; base generation excels in scope, while RAG boosts precision. Larger LLMs perform robustly with slight RAG benefits, whereas smaller models like Llama 3.2 show inconsistent results. Coq formalization confirms the semantic validity of events.

Conclusion: The proposed methods enable effective historical text analysis while advancing RDF representations into higher-order reasoning through Coq formalization, with implications for precision, richness, and computational historical studies.

Abstract: Extracting structured computational representations of historical events from
narrative text remains computationally expensive when constructed manually.
While RDF/OWL reasoners enable graph-based reasoning, they are limited to
fragments of first-order logic, preventing deeper temporal and semantic
analysis. This paper addresses both challenges by developing automatic
historical event extraction models using multiple LLMs (GPT-4, Claude, Llama
3.2) with three enhancement strategies: pure base generation, knowledge graph
enhancement, and Retrieval-Augmented Generation (RAG). We conducted
comprehensive evaluations using historical texts from Thucydides. Our findings
reveal that enhancement strategies optimize different performance dimensions
rather than providing universal improvements. For coverage and historical
breadth, base generation achieves optimal performance with Claude and GPT-4
extracting comprehensive events. However, for precision, RAG enhancement
improves coordinate accuracy and metadata completeness. Model architecture
fundamentally determines enhancement sensitivity: larger models demonstrate
robust baseline performance with incremental RAG improvements, while Llama 3.2
shows extreme variance from competitive performance to complete failure. We
then developed an automated translation pipeline converting extracted RDF
representations into Coq proof assistant specifications, enabling higher-order
reasoning beyond RDF capabilities including multi-step causal verification,
temporal arithmetic with BC dates, and formal proofs about historical
causation. The Coq formalization validates that RAG-discovered event types
represent legitimate domain-specific semantic structures rather than
ontological violations.

</details>


### [151] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
*LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu Rong*

Main category: cs.CL

TL;DR: The paper addresses deficiencies in medical applications of Multimodal Large Language Models (MLLMs) by proposing enhanced data curation techniques and introducing Lingshu, a medical-specialized MLLM.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs show limitations in medical scenarios due to lack of comprehensive medical data coverage, susceptibility to hallucinations, and insufficient reasoning capabilities.

Method: The authors propose a novel data curation process for synthesizing accurate multimodal medical datasets and introduce Lingshu, trained using multi-stage training and reinforcement learning. They also develop MedEvalKit for standardized evaluation.

Result: Lingshu demonstrates superior performance in multimodal and text-based QA, and medical report generation compared to current open-source multimodal models.

Conclusion: The enhanced data curation and training strategies enable Lingshu to tackle complex medical tasks effectively, marking progress in medical MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [152] [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)
*Kai Xiong,Xiao Ding,Yixin Cao,Yuxiong Yan,Li Du,Yufei Zhang,Jinglong Gao,Jiaqian Liu,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: This paper introduces the Com$^2$ benchmark for assessing complex commonsense reasoning in large language models (LLMs), using structured causal event graphs and detective stories to create challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: LLMs excel in simple commonsense reasoning but lack proficiency in complex, implicit commonsense reasoning necessary for understanding scenarios like long-term effects of events, which humans prioritize. Existing benchmarks often overlook this area.

Method: The authors leverage causal event graphs to structure complex commonsense knowledge. Using causal theory, they modify these graphs to produce varied scenarios reflecting real-world concerns. LLMs are tasked with generating reasoning examples guided by these graphs, including experiments on detective-story challenges.

Result: Experimental results demonstrate weaknesses in LLMs' reasoning depth and breadth. However, techniques like post-training and slow thinking improve their performance in complex commonsense reasoning tasks.

Conclusion: LLMs are currently limited in their complex commonsense reasoning capabilities. Benchmarks like Com$^2$, combined with structured causal reasoning approaches, can help reveal gaps and provide pathways for improving model performance.

Abstract: Large language models (LLMs) have mastered abundant simple and explicit
commonsense knowledge through pre-training, enabling them to achieve human-like
performance in simple commonsense reasoning. Nevertheless, LLMs struggle to
reason with complex and implicit commonsense knowledge that is derived from
simple ones (such as understanding the long-term effects of certain events), an
aspect humans tend to focus on more. Existing works focus on complex tasks like
math and code, while complex commonsense reasoning remains underexplored due to
its uncertainty and lack of structure. To fill this gap and align with
real-world concerns, we propose a benchmark Com$^2$ focusing on complex
commonsense reasoning. We first incorporate causal event graphs to serve as
structured complex commonsense. Then we adopt causal theory~(e.g.,
intervention) to modify the causal event graphs and obtain different scenarios
that meet human concerns. Finally, an LLM is employed to synthesize examples
with slow thinking, which is guided by the logical relationships in the
modified causal graphs. Furthermore, we use detective stories to construct a
more challenging subset. Experiments show that LLMs struggle in reasoning depth
and breadth, while post-training and slow thinking can alleviate this. The code
and data are available at https://github.com/Waste-Wood/Com2.

</details>


### [153] [Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing](https://arxiv.org/abs/2506.07086)
*Yuanhe Tian,Pengsen Cheng,Guoqing Jin,Lei Zhang,Yan Song*

Main category: cs.CL

TL;DR: This paper introduces an LLM-based method for multi-modal affective computing, separating shared and modality-specific data, leading to superior performance in various emotion analysis tasks.


<details>
  <summary>Details</summary>
Motivation: To improve human-computer interaction and emotion understanding by addressing the limitations of existing approaches that fail to account for complex, conflicting cross-modal evidence.

Method: The method involves encoding and aligning visual and textual input using pre-trained multi-modal encoders, then decomposing the representations into shared and modality-specific components. These signals are integrated via an attention mechanism to create a dynamic soft prompt for a multi-modal LLM.

Result: Experimental results on tasks like multi-modal sentiment analysis, emotion analysis, and hateful meme detection show that the proposed approach outperforms strong baselines and state-of-the-art models.

Conclusion: The proposed decomposition and integration framework significantly enhances multi-modal affective computing, demonstrating its practical potential for related applications.

Abstract: Multi-modal affective computing aims to automatically recognize and interpret
human attitudes from diverse data sources such as images and text, thereby
enhancing human-computer interaction and emotion understanding. Existing
approaches typically rely on unimodal analysis or straightforward fusion of
cross-modal information that fail to capture complex and conflicting evidence
presented across different modalities. In this paper, we propose a novel
LLM-based approach for affective computing that explicitly deconstructs visual
and textual representations into shared (modality-invariant) and
modality-specific components. Specifically, our approach firstly encodes and
aligns input modalities using pre-trained multi-modal encoders, then employs a
representation decomposition framework to separate common emotional content
from unique cues, and finally integrates these decomposed signals via an
attention mechanism to form a dynamic soft prompt for a multi-modal LLM.
Extensive experiments on three representative tasks for affective computing,
namely, multi-modal aspect-based sentiment analysis, multi-modal emotion
analysis, and hateful meme detection, demonstrate the effectiveness of our
approach, which consistently outperforms strong baselines and state-of-the-art
models.

</details>


### [154] [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
*Jiaxuan Gao,Shu Yan,Qixin Tan,Lu Yang,Shusheng Xu,Wei Fu,Zhiyu Mei,Kaifeng Lyu,Yi Wu*

Main category: cs.CL

TL;DR: The paper addresses inefficiencies in reasoning traces of large reasoning models (LRMs) during problem-solving and introduces metrics and methods to improve reasoning efficiency without sacrificing much accuracy.


<details>
  <summary>Details</summary>
Motivation: Current large reasoning models (LRMs) are highly capable but inefficient due to verbose reasoning traces, leading to high inference costs. Existing fine-tuning methods lack consistent evaluation for efficiency improvements, prompting the need for a unified metric and better techniques.

Method: The authors introduce reasoning efficiency frontiers to determine empirical bounds for efficiency, and Reasoning Efficiency Gap (REG) as a metric for evaluating deviations from these bounds. They propose REO-RL, a reinforcement learning approach, that tackles the efficiency gap using strategically chosen token budgets and numerical integration to optimize reasoning performance.

Result: REO-RL reduces REG by over 50 across all evaluated LRMs with minimal loss of accuracy. It aligns with efficiency frontiers for models like Qwen3-4B/8B under a 16K token budget. The REG metric effectively captures the trade-off between accuracy and reasoning length.

Conclusion: The work demonstrates that token-efficient fine-tuning is possible and provides tools (REG and REO-RL) that improve reasoning efficiency while maintaining accuracy. However, aligning LRMs perfectly with efficiency frontiers remains an unresolved challenge and requires future advancements.

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving
capabilities through extended Chain-of-Thought (CoT) reasoning but often
produce excessively verbose and redundant reasoning traces. This inefficiency
incurs high inference costs and limits practical deployment. While existing
fine-tuning methods aim to improve reasoning efficiency, assessing their
efficiency gains remains challenging due to inconsistent evaluations. In this
work, we introduce the reasoning efficiency frontiers, empirical upper bounds
derived from fine-tuning base LRMs across diverse approaches and training
configurations. Based on these frontiers, we propose the Reasoning Efficiency
Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from
these frontiers. Systematic evaluation on challenging mathematical benchmarks
reveals significant gaps in current methods: they either sacrifice accuracy for
short length or still remain inefficient under tight token budgets. To reduce
the efficiency gap, we propose REO-RL, a class of Reinforcement Learning
algorithms that minimizes REG by targeting a sparse set of token budgets.
Leveraging numerical integration over strategically selected budgets, REO-RL
approximates the full efficiency objective with low error using a small set of
token budgets. Through systematic benchmarking, we demonstrate that our
efficiency metric, REG, effectively captures the accuracy-length trade-off,
with low-REG methods reducing length while maintaining accuracy. Our approach,
REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching
Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy
loss. Ablation studies confirm the effectiveness of our exponential token
budget strategy. Finally, our findings highlight that fine-tuning LRMs to
perfectly align with the efficiency frontiers remains an open challenge.

</details>


### [155] [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
*Samir Abdaljalil,Hasan Kurban,Khalid Qaraqe,Erchin Serpedin*

Main category: cs.CL

TL;DR: The paper introduces a reasoning framework called Theorem-of-Thought (ToTh) for large language models, which structures reasoning with three agents using abductive, deductive, and inductive inference. This approach yields improved coherence and performance compared to existing methods like Chain-of-Thought prompting.


<details>
  <summary>Details</summary>
Motivation: Current prompting techniques for large language models are brittle, lacking reliable internal logic structures and interpretability, necessitating frameworks that enforce coherence and robustness.

Method: The proposed framework, ToTh, uses three agents representing abductive, deductive, and inductive reasoning to produce formal reasoning graphs. Bayesian belief propagation guided by natural language inference evaluates coherence, selecting the graph with the highest confidence for final reasoning.

Result: Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks showed ToTh achieved consistently better performance compared to Chain-of-Thought and other prompting methods, with reasoning that is interpretable and logically grounded.

Conclusion: ToTh provides a promising and interpretable approach to strengthening and structuring large language model reasoning capabilities, paving the way for more robust, cognitively inspired frameworks.

Abstract: Large language models (LLMs) have shown strong performance across natural
language reasoning tasks, yet their reasoning processes remain brittle and
difficult to interpret. Prompting techniques like Chain-of-Thought (CoT)
enhance reliability by eliciting intermediate reasoning steps or aggregating
multiple outputs. However, they lack mechanisms for enforcing logical structure
and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a
novel framework that models reasoning as collaboration among three parallel
agents, each simulating a distinct mode of inference: abductive, deductive, and
inductive. Each agent produces a reasoning trace, which is structured into a
formal reasoning graph. To evaluate consistency, we apply Bayesian belief
propagation guided by natural language inference (NLI), assigning confidence
scores to each step. The most coherent graph is selected to derive the final
answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)
reasoning benchmarks show that ToTh consistently outperforms CoT,
Self-Consistency, and CoT-Decoding across multiple LLMs, while producing
interpretable and logically grounded reasoning chains. Our findings suggest a
promising direction for building more robust and cognitively inspired LLM
reasoning. The implementation is available at
https://github.com/KurbanIntelligenceLab/theorem-of-thought.

</details>


### [156] [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: This paper evaluates the nuanced effectiveness of Chain-of-Thought (CoT) prompting across various reasoning tasks and LLM models.


<details>
  <summary>Details</summary>
Motivation: To provide clarity and understanding to stakeholders on the technical workings and effectiveness of AI techniques, specifically Chain-of-Thought prompting.

Method: The authors conducted rigorous testing of CoT prompting on different reasoning tasks and model types, assessing its impact on accuracy, token usage, and variability in answers.

Result: CoT's effectiveness depends on the model and task type. It marginally improves performance for non-reasoning models but can increase errors and variability. On reasoning-enabled models, performance gains are minimal despite higher computational costs.

Conclusion: CoT prompting isn't universally effective; its benefits are limited for models with inherent reasoning abilities and come at significant time and token costs.

Abstract: This is the second in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate Chain-of-Thought
(CoT) prompting, a technique that encourages a large language model (LLM) to
"think step by step" (Wei et al., 2022). CoT is a widely adopted method for
improving reasoning tasks, however, our findings reveal a more nuanced picture
of its effectiveness. We demonstrate two things:
  - The effectiveness of Chain-of-Thought prompting can vary greatly depending
on the type of task and model. For non-reasoning models, CoT generally improves
average performance by a small amount, particularly if the model does not
inherently engage in step-by-step processing by default. However, CoT can
introduce more variability in answers, sometimes triggering occasional errors
in questions the model would otherwise get right. We also found that many
recent models perform some form of CoT reasoning even if not asked; for these
models, a request to perform CoT had little impact. Performing CoT generally
requires far more tokens (increasing cost and time) than direct answers.
  - For models designed with explicit reasoning capabilities, CoT prompting
often results in only marginal, if any, gains in answer accuracy. However, it
significantly increases the time and tokens needed to generate a response.

</details>


### [157] [Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2506.07148)
*Yaping Chai,Haoran Xie,Joe S. Qin*

Main category: cs.CL

TL;DR: This paper proposes a new data augmentation approach and fine-tuning strategy for improving aspect category sentiment analysis using large language models.


<details>
  <summary>Details</summary>
Motivation: To overcome data scarcity challenges in sentiment analysis for low-resource scenarios.

Method: Structured prompt templates guide LLM data augmentation, followed by post-processing for semantic consistency. A confidence-weighted fine-tuning strategy improves prediction accuracy.

Result: Outperformed existing methods on four benchmark datasets, achieving the best performance consistently.

Conclusion: The proposed method enhances semantic understanding and inference in sentiment analysis, proving highly effective in low-resource scenarios.

Abstract: Large language model (LLM) is an effective approach to addressing data
scarcity in low-resource scenarios. Recent existing research designs
hand-crafted prompts to guide LLM for data augmentation. We introduce a data
augmentation strategy for the aspect category sentiment analysis (ACSA) task
that preserves the original sentence semantics and has linguistic diversity,
specifically by providing a structured prompt template for an LLM to generate
predefined content. In addition, we employ a post-processing technique to
further ensure semantic consistency between the generated sentence and the
original sentence. The augmented data increases the semantic coverage of the
training distribution, enabling the model better to understand the relationship
between aspect categories and sentiment polarities, enhancing its inference
capabilities. Furthermore, we propose a confidence-weighted fine-tuning
strategy to encourage the model to generate more confident and accurate
sentiment polarity predictions. Compared with powerful and recent works, our
method consistently achieves the best performance on four benchmark datasets
over all baselines.

</details>


### [158] [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
*Vicky Xefteri,Tim Vieira,Ryan Cotterell,Afra Amini*

Main category: cs.CL

TL;DR: The paper explores using posterior-inference-based sampling algorithms to control syntactic structures in text generation, achieving significant accuracy improvements without sacrificing fluency.


<details>
  <summary>Details</summary>
Motivation: Precise syntactic control is crucial for applications requiring clarity, consistency, or stylistic precision, yet it is a difficult task to integrate with current language models.

Method: The authors propose a method combining sequential Monte Carlo sampling with syntactic tagging to enforce target constituency structures during the generation process.

Result: Their method significantly improves syntactic accuracy in text generation, raising the F1 score from 12.31 (GPT2-large) and 35.33 (Llama3-8B) to about 93 for both models, while maintaining language fluency.

Conclusion: The study demonstrates that posterior-inference-based sampling is an effective technique for achieving precise syntactic control in text generation, applicable to various scenarios requiring structured outputs.

Abstract: Controlling the syntactic structure of text generated by language models is
valuable for applications requiring clarity, stylistic consistency, or
interpretability, yet it remains a challenging task. In this paper, we argue
that sampling algorithms based on the posterior inference can effectively
enforce a target constituency structure during generation. Our approach
combines sequential Monte Carlo, which estimates the posterior distribution by
sampling from a proposal distribution, with a syntactic tagger that ensures
that each generated token aligns with the desired syntactic structure. Our
experiments with GPT2 and Llama3-8B models show that with an appropriate
proposal distribution, we can improve syntactic accuracy, increasing the F1
score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both
cases without compromising the language model's fluency. These results
underscore both the complexity of syntactic control and the effectiveness of
sampling algorithms, offering a promising approach for applications where
precise control over syntax is essential.

</details>


### [159] [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/abs/2506.07160)
*Yikun Wang,Yibin Wang,Dianyi Wang,Zimian Peng,Qipeng Guo,Dacheng Tao,Jiaqi Wang*

Main category: cs.CL

TL;DR: The paper introduces a new framework, GCPO, leveraging reinforcement learning to train smaller models for geometric reasoning with improved performance over existing approaches.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of geometric reasoning in LLMs, which relies heavily on auxiliary constructions while being computationally expensive with current large models.

Method: Introduced "Group Contrastive Policy Optimization" (GCPO) combining Group Contrastive Masking and length rewards for better training of smaller models in geometric reasoning.

Result: The proposed GeometryZero models using GCPO demonstrated a 4.29% performance improvement across benchmarks like Geometry3K and MathVista compared to baselines.

Conclusion: GCPO enables effective geometric reasoning in smaller, computationally affordable models, outperforming traditional GRPO frameworks and advancing reasoning capabilities.

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities across diverse domains, particularly in mathematical reasoning,
amid which geometry problem solving remains a challenging area where auxiliary
construction plays a enssential role. Existing approaches either achieve
suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring
massive computational costs. We posit that reinforcement learning with
verifiable reward (e.g., GRPO) offers a promising direction for training
smaller models that effectively combine auxiliary construction with robust
geometric reasoning. However, directly applying GRPO to geometric reasoning
presents fundamental limitations due to its dependence on unconditional
rewards, which leads to indiscriminate and counterproductive auxiliary
constructions. To address these challenges, we propose Group Contrastive Policy
Optimization (GCPO), a novel reinforcement learning framework featuring two key
innovations: (1) Group Contrastive Masking, which adaptively provides positive
or negative reward signals for auxiliary construction based on contextual
utility, and a (2) length reward that promotes longer reasoning chains.
Building on GCPO, we develop GeometryZero, a family of affordable-size
geometric reasoning models that judiciously determine when to employ auxiliary
construction. Our extensive empirical evaluation across popular geometric
benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models
consistently outperform baselines (e.g. GRPO), achieving an average improvement
of 4.29% across all benchmarks.

</details>


### [160] [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação](https://arxiv.org/abs/2506.07169)
*Washington Cunha,Leonardo Rocha,Marcos André Gonçalves*

Main category: cs.CL

TL;DR: The dissertation addresses Instance Selection (IS) in NLP to reduce large training dataset sizes while preserving model effectiveness, improving efficiency, and scalability.


<details>
  <summary>Details</summary>
Motivation: To tackle the problem of significant computational costs and data requirements for training/fine-tuning large dense NLP models.

Method: Comprehensive comparison of IS methods for Automatic Text Classification (ATC), proposing two novel IS solutions tailored for large datasets and transformer models.

Result: Achieved an average training dataset size reduction of 41% while maintaining effectiveness and speedup improvements of up to 2.46x.

Conclusion: Instance Selection demonstrates substantial potential in reducing resources and cost for NLP tasks, indicating scalability and enhanced efficiency for large-scale datasets.

Abstract: Progress in Natural Language Processing (NLP) has been dictated by the rule
of more: more data, more computing power and more complexity, best exemplified
by the Large Language Models. However, training (or fine-tuning) large dense
models for specific applications usually requires significant amounts of
computing resources. This \textbf{Ph.D. dissertation} focuses on an
under-investi\-gated NLP data engineering technique, whose potential is
enormous in the current scenario known as Instance Selection (IS). The IS goal
is to reduce the training set size by removing noisy or redundant instances
while maintaining the effectiveness of the trained models and reducing the
training process cost. We provide a comprehensive and scientifically sound
comparison of IS methods applied to an essential NLP task -- Automatic Text
Classification (ATC), considering several classification solutions and many
datasets. Our findings reveal a significant untapped potential for IS
solutions. We also propose two novel IS solutions that are noise-oriented and
redundancy-aware, specifically designed for large datasets and transformer
architectures. Our final solution achieved an average reduction of 41\% in
training sets, while maintaining the same levels of effectiveness in all
datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x
(up to 2.46x), making them scalable for datasets with hundreds of thousands of
documents.

</details>


### [161] [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
*Chenlong Zhang,Zhuoran Jin,Hongbang Yuan,Jiaheng Wei,Tong Zhou,Kang Liu,Jun Zhao,Yubo Chen*

Main category: cs.CL

TL;DR: The paper introduces RULE, a method for selectively unlearning in LLMs, achieving better unlearning quality, naturalness, and generalization while maintaining utility, using limited data.


<details>
  <summary>Details</summary>
Motivation: To address concerns regarding sensitive, copyrighted, or illegal content in LLMs and solve issues in existing unlearning methods such as poor generalization, unnatural responses, and utility loss.

Method: The authors propose 'Reinforcement UnLearning' (RULE), where unlearning is treated as a refusal boundary optimization problem. RULE leverages a verifiable reward function and uses only a small subset of 'forget' and synthesized boundary data.

Result: RULE was able to outperform existing methods with up to 17.5% improvement in forget quality and 16.3% in naturalness response while maintaining overall model utility. It also demonstrated efficient training and strong generalization.

Conclusion: RULE is an efficient and effective framework for selective unlearning in LLMs. It offers a Pareto-optimal solution that balances unlearning and utility, with enhanced naturalness and generalization.

Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive,
uncurated corpora has raised growing concerns about the inclusion of sensitive,
copyrighted, or illegal content. This has led to increasing interest in LLM
unlearning: the task of selectively removing specific information from a model
without retraining from scratch or degrading overall utility. However, existing
methods often rely on large-scale forget and retain datasets, and suffer from
unnatural responses, poor generalization, or catastrophic utility loss. In this
work, we propose Reinforcement UnLearning (RULE), an efficient framework that
formulates unlearning as a refusal boundary optimization problem. RULE is
trained with a small portion of the forget set and synthesized boundary
queries, using a verifiable reward function that encourages safe refusal on
forget--related queries while preserving helpful responses on permissible
inputs. We provide both theoretical and empirical evidence demonstrating the
effectiveness of RULE in achieving targeted unlearning without compromising
model utility. Experimental results show that, with only $12%$ forget set and
$8%$ synthesized boundary data, RULE outperforms existing baselines by up to
$17.5%$ forget quality and $16.3%$ naturalness response while maintaining
general utility, achieving forget--retain Pareto optimality. Remarkably, we
further observe that RULE improves the naturalness of model outputs, enhances
training efficiency, and exhibits strong generalization ability, generalizing
refusal behavior to semantically related but unseen queries.

</details>


### [162] [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
*Wenrui Zhou,Shu Yang,Qingsong Yang,Zikun Guo,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: The paper proposes a novel systematic benchmark, VISE, to evaluate and mitigate sycophantic behavior in Video-Large Language Models (Video-LLMs), addressing the gap in understanding their reliability under contradictory user input.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation methods for sycophantic behavior in Video-LLMs, which compromises their reliability in real-world multimodal reasoning applications.

Method: The authors introduce VISE, a benchmark encompassing diverse question formats, prompt biases, and visual reasoning tasks, and analyze linguistic perspectives of sycophancy in the visual domain. Additionally, they explore key-frame selection as a potential mitigation technique.

Result: VISE provides a framework for fine-grained analysis of sycophantic behavior in Video-LLMs and demonstrates that key-frame selection can strengthen visual grounding, thereby reducing bias.

Conclusion: VISE opens new pathways for systematically evaluating and mitigating sycophancy in Video-LLMs, improving their factual reliability in grounded reasoning scenarios.

Abstract: As video large language models (Video-LLMs) become increasingly integrated
into real-world applications that demand grounded multimodal reasoning,
ensuring their factual consistency and reliability is of critical importance.
However, sycophancy, the tendency of these models to align with user input even
when it contradicts the visual evidence, undermines their trustworthiness in
such contexts. Current sycophancy research has largely overlooked its specific
manifestations in the video-language domain, resulting in a notable absence of
systematic benchmarks and targeted evaluations to understand how Video-LLMs
respond under misleading user input. To fill this gap, we propose VISE
(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated
benchmark designed to evaluate sycophantic behavior in state-of-the-art
Video-LLMs across diverse question formats, prompt biases, and visual reasoning
tasks. Specifically, VISE pioneeringly brings linguistic perspectives on
sycophancy into the visual domain, enabling fine-grained analysis across
multiple sycophancy types and interaction patterns. In addition, we explore
key-frame selection as an interpretable, training-free mitigation strategy,
which reveals potential paths for reducing sycophantic bias by strengthening
visual grounding.

</details>


### [163] [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
*Wenxuan Xie,Yaxun Dai,Wenhao Jiang*

Main category: cs.CL

TL;DR: SDE-SQL introduces a novel framework enabling large language models to dynamically explore databases during inference, improving Text-to-SQL task performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation of previous Text-to-SQL approaches that rely on static, pre-processed database information, restricting models from dynamically exploring data.

Method: SDE-SQL allows large language models to self-drive database exploration by generating and executing SQL probes, enabling iterative data understanding without in-context question-SQL examples.

Result: On the BIRD benchmark, SDE-SQL achieved an 8.02% improvement in execution accuracy compared to the baseline, with further gains possible through supervised fine-tuning.

Conclusion: By enabling dynamic database exploration, SDE-SQL significantly advances Text-to-SQL task performance, establishing a new state-of-the-art for open-source models without supervised fine-tuning or ensembling.

Abstract: Recent advancements in large language models (LLMs) have significantly
improved performance on the Text-to-SQL task. However, prior approaches
typically rely on static, pre-processed database information provided at
inference time, which limits the model's ability to fully understand the
database contents. Without dynamic interaction, LLMs are constrained to fixed,
human-provided context and cannot autonomously explore the underlying data. To
address this limitation, we propose SDE-SQL, a framework that enables large
language models to perform self-driven exploration of databases during
inference. This is accomplished by generating and executing SQL probes, which
allow the model to actively retrieve information from the database and
iteratively update its understanding of the data. Unlike prior methods, SDE-SQL
operates in a zero-shot setting, without relying on any question-SQL pairs as
in-context demonstrations. When evaluated on the BIRD benchmark with
Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in
execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing
a new state-of-the-art among methods based on open-source models without
supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the
performance of SDE-SQL can be further enhanced, yielding an additional 0.52%
improvement.

</details>


### [164] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Raviraj Joshi*

Main category: cs.CL

TL;DR: The paper proposes a TF-IDF-based sentence ranking method to efficiently classify long documents by selecting only the most relevant sentences, reducing computational inefficiencies while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models like BERT face challenges with fixed input lengths and quadratic attention complexity when processing long documents. Furthermore, many sentences in such documents are redundant, leading to unnecessary computational demands.

Method: The paper introduces a ranking method for long document classification using TF-IDF scores and sentence length. Two selection strategies—fixed-count and percentage-based—are evaluated, alongside enhanced scoring strategies. The approach is tested on the MahaNews LDC dataset using MahaBERT-v2.

Result: The method outperformed simple baselines (e.g., first, last, random sentence selection) and achieved near-identical classification accuracy with a minor 0.33% drop compared to full-context input. Input size was reduced by over 50%, and inference latency decreased by 43%.

Conclusion: Selecting the most informative sentences using a TF-IDF-based method can significantly reduce computational overhead without compromising classification accuracy. This makes the approach suitable for practical applications in real-world scenarios

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [165] [Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages](https://arxiv.org/abs/2506.07249)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: The study adapted a bias attribution score for Filipino, analyzing bias in purely Filipino and multilingual models, finding notable differences between English and Filipino bias contributors.


<details>
  <summary>Details</summary>
Motivation: The increasing focus on bias attribution and interpretability in language models has identified token contributions to bias in English but lacks insights into agglutinative languages like Filipino.

Method: An adaptation of the information-theoretic bias attribution score metric was implemented to analyze bias in Filipino and multilingual language models.

Result: Filipino models were found to exhibit bias through entity-based themes (people, objects, relationships), differing from English models, which focus on action-heavy themes (e.g., behaviors).

Conclusion: The study highlights significant differences in sociodemographic and linguistic processing of bias between English-centric and Filipino-centric language models.

Abstract: Emerging research on bias attribution and interpretability have revealed how
tokens contribute to biased behavior in language models processing English
texts. We build on this line of inquiry by adapting the information-theoretic
bias attribution score metric for implementation on models handling
agglutinative languages, particularly Filipino. We then demonstrate the
effectiveness of our adapted method by using it on a purely Filipino model and
on three multilingual models: one trained on languages worldwide and two on
Southeast Asian data. Our results show that Filipino models are driven towards
bias by words pertaining to people, objects, and relationships, entity-based
themes that stand in contrast to the action-heavy nature of bias-contributing
themes in English (i.e., criminal, sexual, and prosocial behaviors). These
findings point to differences in how English and non-English models process
inputs linked to sociodemographic groups and bias.

</details>


### [166] [Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs](https://arxiv.org/abs/2506.07270)
*Atahan Özer,Çağatay Yıldız*

Main category: cs.CL

TL;DR: Large language models struggle with evolving real-world information; authors propose a structured external memory framework for improved reasoning and information retrieval.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs in handling evolving facts and real-time information changes, given their reliance on static pre-training data.

Method: Authors introduced benchmarks capturing factual shifts in temporal datasets and developed a framework utilizing external memory for temporally informed reasoning without re-training.

Result: The proposed method outperforms traditional in-context learning (ICL) and retrieval-augmented generation (RAG) baselines in handling temporal information and conflicting facts.

Conclusion: Structured external memory can enhance LLMs' ability to reason and retrieve temporally relevant information, mitigating the challenges of evolving real-world knowledge.

Abstract: Large language models (LLMs) exhibit remarkable capabilities in question
answering and reasoning thanks to their extensive parametric memory. However,
their knowledge is inherently limited by the scope of their pre-training data,
while real-world information evolves continuously. Updating this knowledge
typically requires costly and brittle re-training, or in-context learning
(ICL), which becomes impractical at scale given the volume and volatility of
modern information. Motivated by these limitations, we investigate how LLMs
perform when exposed to temporal text corpora, or documents that reflect
evolving knowledge over time, such as sports biographies where facts like a
player's "current team" change year by year. To this end, we introduce two new
benchmarks: Temporal Wiki, which captures factual drift across historical
Wikipedia snapshots, and Unified Clark, which aggregates timestamped news
articles to simulate real-world information accumulation. Our analysis reveals
that LLMs often struggle to reconcile conflicting or outdated facts and can be
misled when multiple versions of a fact appear in context. To address these
issues, we propose a lightweight, agentic framework that incrementally builds a
structured, external memory from source documents without requiring
re-training. This knowledge organization strategy enables models to retrieve
and reason over temporally filtered, relevant information at inference time.
Empirically, our method outperforms ICL and RAG baselines across both
benchmarks, especially on questions requiring more complex reasoning or
integration of conflicting facts.

</details>


### [167] [Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service](https://arxiv.org/abs/2411.12262)
*Raphael Merx,Adérito José Guterres Correia,Hanna Suominen,Ekaterina Vylomova*

Main category: cs.CL

TL;DR: The paper analyzes community usage patterns of a Tetun machine translation service, showing needs differ from existing corpora assumptions.


<details>
  <summary>Details</summary>
Motivation: To understand real-world application challenges for low-resource language MT, particularly for Tetun.

Method: Analyzed 100,000 translation requests from Tetun MT service to observe user patterns.

Result: Users mostly translate from high-resource languages to Tetun, with prevalent domains being science, healthcare, and daily life.

Conclusion: MT systems for Tetun should focus on educationally relevant domains and directional accuracy, informed by practical community usage patterns.

Abstract: Low-resource machine translation (MT) presents a diversity of community needs
and application challenges that remain poorly understood. To complement surveys
and focus groups, which tend to rely on small samples of respondents, we
propose an observational study on actual usage patterns of tetun$.$org, a
specialized MT service for the Tetun language, which is the lingua franca in
Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that
challenge assumptions based on existing corpora. We find that users, many of
them students on mobile devices, typically translate text from a high-resource
language into Tetun across diverse domains including science, healthcare, and
daily life. This contrasts sharply with available Tetun corpora, which are
dominated by news articles covering government and social issues. Our results
suggest that MT systems for institutionalized minority languages like Tetun
should prioritize accuracy on domains relevant to educational contexts, in the
high-resource to low-resource direction. More broadly, this study demonstrates
how observational analysis can inform low-resource language technology
development, by grounding research in practical community needs.

</details>


### [168] [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)
*Olga Kellert,Nemika Tyagi,Muhammad Imran,Nelvin Licona-Guevara,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: The paper presents the BiLingua Parser, an LLM-based system for generating syntactic Universal Dependency (UD) annotations for code-switched text in low-resource language scenarios, achieving up to 95.29% accuracy after expert revision.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of syntactic analysis in code-switched contexts, particularly in under-resourced language settings where annotated data is minimal, and monolingual parsers fail to effectively handle multilingual or mixed-language inputs.

Method: The authors developed the BiLingua Parser, which uses a prompt-based framework for few-shot LLM prompting and expert review, specifically applied to Spanish-English and Spanish-Guaraní data. They also released two UD-parsed datasets and conducted syntactic analyses of language switch points.

Result: The BiLingua Parser achieved up to 95.29% labeled attachment score (LAS) after expert revision, substantially outperforming prior baselines and multilingual parsers in its effectiveness on code-switched text.

Conclusion: LLMs can be guided to effectively serve as tools for syntactic analysis and resource generation in low-resource, code-switched language scenarios, providing a practical solution where annotated data is typically lacking.

Abstract: Code-switching presents a complex challenge for syntactic analysis,
especially in low-resource language settings where annotated data is scarce.
While recent work has explored the use of large language models (LLMs) for
sequence-level tagging, few approaches systematically investigate how well
these models capture syntactic structure in code-switched contexts. Moreover,
existing parsers trained on monolingual treebanks often fail to generalize to
multilingual and mixed-language input. To address this gap, we introduce the
BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal
Dependencies (UD) annotations for code-switched text. First, we develop a
prompt-based framework for Spanish-English and Spanish-Guaran\'i data,
combining few-shot LLM prompting with expert review. Second, we release two
annotated datasets, including the first Spanish-Guaran\'i UD-parsed corpus.
Third, we conduct a detailed syntactic analysis of switch points across
language pairs and communicative contexts. Experimental results show that
BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly
outperforming prior baselines and multilingual parsers. These results show that
LLMs, when carefully guided, can serve as practical tools for bootstrapping
syntactic resources in under-resourced, code-switched environments. Data and
source code are available at https://github.com/N3mika/ParsingProject

</details>


### [169] [Exploring the Impact of Temperature on Large Language Models:Hot or Cold?](https://arxiv.org/abs/2506.07295)
*Lujun Li,Lama Sleem,Niccolo' Gentile,Geoffrey Nichil,Radu State*

Main category: cs.CL

TL;DR: The study examines how temperature, a crucial hyperparameter in large language models, affects performance across various tasks and introduces a BERT-based optimizer for selecting optimal temperatures for prompts.


<details>
  <summary>Details</summary>
Motivation: Temperature in LLMs influences the stochasticity of token outputs, yet its precise impact on diverse model capabilities remains poorly understood.

Method: The study evaluates temperature effects (range: 0-4.0) on six tasks across open-source models of varying sizes. Statistical analysis and a BERT-based temperature selector are used to optimize performance through adaptive temperature setting.

Result: Distinct, skill-specific temperature effects on model capabilities are observed. The BERT-based selector enhances performance in smaller models, consistent across different quantization levels. Mutation Temperature increases with model size.

Conclusion: Temperature settings significantly influence LLM performance. A systematic, task-based selection can improve model output, particularly in smaller and medium-sized models, alongside revealing scaling laws tied to model size.

Abstract: The sampling temperature, a critical hyperparameter in large language models
(LLMs), modifies the logits before the softmax layer, thereby reshaping the
distribution of output tokens. Recent studies have challenged the Stochastic
Parrots analogy by demonstrating that LLMs are capable of understanding
semantics rather than merely memorizing data and that randomness, modulated by
sampling temperature, plays a crucial role in model inference. In this study,
we systematically evaluated the impact of temperature in the range of 0 to 2 on
data sets designed to assess six different capabilities, conducting statistical
analyses on open source models of three different sizes: small (1B--4B), medium
(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific
effects of temperature on model performance, highlighting the complexity of
optimal temperature selection in practical applications. To address this
challenge, we propose a BERT-based temperature selector that takes advantage of
these observed effects to identify the optimal temperature for a given prompt.
We demonstrate that this approach can significantly improve the performance of
small and medium models in the SuperGLUE datasets. Furthermore, our study
extends to FP16 precision inference, revealing that temperature effects are
consistent with those observed in 4-bit quantized models. By evaluating
temperature effects up to 4.0 in three quantized models, we find that the
Mutation Temperature -- the point at which significant performance changes
occur -- increases with model size.

</details>


### [170] [Subjectivity in the Annotation of Bridging Anaphora](https://arxiv.org/abs/2506.07297)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: This paper investigates the subjectivity in annotating bridging anaphora, analyzing challenges in recognizing anaphors, resolving antecedents, and subtype classification.


<details>
  <summary>Details</summary>
Motivation: Bridging anaphora are relationships between discourse entities and antecedents, inherently subjective and hard to annotate consistently.

Method: The study conducts an annotation pilot using the GUM corpus test set and introduces a new classification system for bridging subtypes. It compares this system to previous schemes.

Result: Findings indicate previous resources are under-annotated, annotator overlap is low for identifying bridging instances, and disagreements stem from subjective understanding.

Conclusion: Moderate agreement is achieved in bridging subtype categories, but subjective interpretation poses consistent annotation challenges.

Abstract: Bridging refers to the associative relationship between inferable entities in
a discourse and the antecedents which allow us to understand them, such as
understanding what "the door" means with respect to an aforementioned "house".
As identifying associative relations between entities is an inherently
subjective task, it is difficult to achieve consistent agreement in the
annotation of bridging anaphora and their antecedents. In this paper, we
explore the subjectivity involved in the annotation of bridging instances at
three levels: anaphor recognition, antecedent resolution, and bridging subtype
selection. To do this, we conduct an annotation pilot on the test set of the
existing GUM corpus, and propose a newly developed classification system for
bridging subtypes, which we compare to previously proposed schemes. Our results
suggest that some previous resources are likely to be severely under-annotated.
We also find that while agreement on the bridging subtype category was
moderate, annotator overlap for exhaustively identifying instances of bridging
is low, and that many disagreements resulted from subjective understanding of
the entities involved.

</details>


### [171] [ConfQA: Answer Only If You Are Confident](https://arxiv.org/abs/2506.07309)
*Yin Huang,Yifan Ethan Xu,Kai Sun,Vera Yan,Alicia Sun,Haidar Khan,Jimmy Nguyen,Mohammad Kachuee,Zhaojiang Lin,Yue Liu,Aaron Colak,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: The paper introduces ConfQA, a fine-tuning strategy that reduces hallucination in LLMs to under 5%, using confidence prompts and factual data calibration.


<details>
  <summary>Details</summary>
Motivation: To address and mitigate the issue of hallucination in large language models (LLMs), enhancing their factual accuracy and reliability.

Method: The strategy involves training LLMs to either continue with an answer if confident or admit uncertainty if unsure. This is reinforced using confidence-promoting prompts and knowledge graph factual data for calibration.

Result: ConfQA achieved significant improvements by reducing hallucination to under 5% and proposing the Dual Neural Knowledge framework, which increased accuracy to over 95% and reduced reliance on external knowledge retrieval by over 30%.

Conclusion: The findings demonstrate that fine-tuning strategies like ConfQA can significantly improve the reliability of LLMs, reducing hallucination and enhancing performance through better calibration of confidence and knowledge selection.

Abstract: Can we teach Large Language Models (LLMs) to refrain from hallucinating
factual statements? In this paper we present a fine-tuning strategy that we
call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across
multiple factuality benchmarks. The core idea is simple: when the LLM answers a
question correctly, it is trained to continue with the answer; otherwise, it is
trained to admit "I am unsure". But there are two key factors that make the
training highly effective. First, we introduce a dampening prompt "answer only
if you are confident" to explicitly guide the behavior, without which
hallucination remains high as 15%-25%. Second, we leverage simple factual
statements, specifically attribute values from knowledge graphs, to help LLMs
calibrate the confidence, resulting in robust generalization across domains and
question types. Building on this insight, we propose the Dual Neural Knowledge
framework, which seamlessly select between internally parameterized neural
knowledge and externally recorded symbolic knowledge based on ConfQA's
confidence. The framework enables potential accuracy gains to beyond 95%, while
reducing unnecessary external retrievals by over 30%.

</details>


### [172] [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
*Brian Christian,Hannah Rose Kirk,Jessica A. F. Thompson,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.CL

TL;DR: The paper provides a novel analysis of reward models used in aligning language models with human values, uncovering significant issues such as biases, sensitivity to prompts, and overvaluation of frequent tokens.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about reward models, which encode human value judgments to guide generative AI, and ensure these models don’t propagate biases or unintended consequences.

Method: Analyzed responses of ten open-source reward models across all single-token responses to prompts, evaluating variability, asymmetries, framing sensitivity, and token frequency valuation.

Result: Uncovered heterogeneity among reward models, systematic token scoring asymmetries, cognitive bias-mirroring sensitivities, and frequent token overvaluation. Also found risks of biases impacting downstream outputs.

Conclusion: Current reward models exhibit significant limitations and biases, challenging their effectiveness and highlighting risks for their use in training large-scale language models.

Abstract: Reward modeling has emerged as a crucial component in aligning large language
models with human values. Significant attention has focused on using reward
models as a means for fine-tuning generative models. However, the reward models
themselves -- which directly encode human value judgments by turning
prompt-response pairs into scalar rewards -- remain relatively understudied. We
present a novel approach to reward model interpretability through exhaustive
analysis of their responses across their entire vocabulary space. By examining
how different reward models score every possible single-token response to
value-laden prompts, we uncover several striking findings: (i) substantial
heterogeneity between models trained on similar objectives, (ii) systematic
asymmetries in how models encode high- vs low-scoring tokens, (iii) significant
sensitivity to prompt framing that mirrors human cognitive biases, and (iv)
overvaluation of more frequent tokens. We demonstrate these effects across ten
recent open-source reward models of varying parameter counts and architectures.
Our results challenge assumptions about the interchangeability of reward
models, as well as their suitability as proxies of complex and
context-dependent human values. We find that these models can encode concerning
biases toward certain identity groups, which may emerge as unintended
consequences of harmlessness training -- distortions that risk propagating
through the downstream large language models now deployed to millions.

</details>


### [173] [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)
*Anyi Wang,Dong Shu,Yifan Wang,Yunpu Ma,Mengnan Du*

Main category: cs.CL

TL;DR: The paper introduces SRPS, a framework to enhance LLM reasoning by manipulating internal model features for role-playing. It outperforms traditional prompt-based approaches in accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing role-playing techniques for LLMs heavily depend on unstable and less interpretable prompt engineering, necessitating a more stable and understandable approach.

Method: The authors propose SRPS, which uses sparse autoencoders to extract latent role-play features, selects top activation patterns, and constructs a steering vector to control role behavior via the model's residual stream.

Result: SRPS achieves significant accuracy improvements in zero-shot CoT tasks, such as increasing Llama3.1-8B's performance on CSQA by 7.94% and Gemma2-9B's on SVAMP by 7.6%.

Conclusion: SRPS provides a more stable and interpretable alternative to prompt-based role-playing, improving LLM reasoning abilities across model sizes and benchmarks.

Abstract: Role-playing has emerged as an effective technique for enhancing the
reasoning capabilities of large language models (LLMs). However, existing
methods primarily rely on prompt engineering, which often lacks stability and
interpretability. In this paper, we introduce Sparse Autoencoder Role-Playing
Steering (SRPS), a novel framework that identifies and manipulates internal
model features associated with role-playing behavior. Our approach extracts
latent representations from role-play prompts, selects the most relevant
features based on activation patterns, and constructs a steering vector that
can be injected into the model's residual stream with controllable intensity.
Our method enables fine-grained control over role-specific behavior and offers
insights into how role information influences internal model activations.
Extensive experiments across various reasoning benchmarks and model sizes
demonstrate consistent performance gains. Notably, in the zero-shot
chain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves
from 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to
45.10%. These results highlight the potential of SRPS to enhance reasoning
ability in LLMs, providing better interpretability and stability compared to
traditional prompt-based role-playing.

</details>


### [174] [Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation](https://arxiv.org/abs/2506.07356)
*Seokil Ham,Yubin Choi,Seungju Cho,Yujin Yang,Younghun Kim,Changick Kim*

Main category: cs.CL

TL;DR: The paper proposes a ReFT model to filter harmful prompts during LLM finetuning, enhancing safety and task accuracy.


<details>
  <summary>Details</summary>
Motivation: Current Finetuning-as-a-Service offerings face safety risks when user data contains harmful prompts, and these risks remain underexplored in existing solutions.

Method: A directional representation called the refusal feature is used to train the ReFT model, which identifies harmful prompts and serves as a filter and teacher during finetuning.

Result: The ReFT-based finetuning strategy reduces harmful outputs while maintaining high task-specific performance.

Conclusion: ReFT enables a safer and more reliable method for customizing LLMs in Finetuning-as-a-Service without compromising accuracy.

Abstract: Recently, major AI service providers such as Google and OpenAI have
introduced Finetuning-as-a-Service, which enables users to customize Large
Language Models (LLMs) for specific downstream tasks using their own data.
However, this service is vulnerable to degradation of LLM safety-alignment when
user data contains harmful prompts. While some prior works address this issue,
fundamentally filtering harmful data from user data remains unexplored.
Motivated by our observation that a directional representation reflecting
refusal behavior (called the refusal feature) obtained from safety-aligned LLMs
can inherently distinguish between harmful and harmless prompts, we propose the
Refusal-Feature-guided Teacher (ReFT). Our ReFT model is trained to identify
harmful prompts based on the similarity between input prompt features and its
refusal feature. During finetuning, the ReFT model serves as a teacher that
filters harmful prompts from user data and distills alignment knowledge into
the base model. Extensive experiments demonstrate that our ReFT-based
finetuning strategy effectively minimizes harmful outputs and enhances
finetuning accuracy for user-specific tasks, offering a practical solution for
secure and reliable deployment of LLMs in Finetuning-as-a-Service.

</details>


### [175] [SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation](https://arxiv.org/abs/2506.07423)
*Janghyeon Yun,Sang-goo Lee*

Main category: cs.CL

TL;DR: The paper introduces SEED, a method to enhance text-to-SQL systems by automatically generating evidence for SQL generation, improving performance compared to models relying on human-provided evidence.


<details>
  <summary>Details</summary>
Motivation: Address issues with the BIRD dataset, such as reliance on human-provided evidence and its defects, to improve real-world usability of text-to-SQL systems.

Method: SEED analyzes database schema, description files, and values to automatically extract and generate evidence for SQL generation.

Result: SEED demonstrates significant performance improvement in the no-evidence scenario and occasionally outperforms text-to-SQL models reliant on human-provided evidence from BIRD.

Conclusion: SEED bridges the research-to-deployment gap, enhances adaptability and robustness of text-to-SQL systems, and redefines evidence requirements for better real-world applications.

Abstract: Text-to-SQL enables non-experts to retrieve data from databases by converting
natural language queries into SQL. However, state-of-the-art text-to-SQL
studies rely on the BIRD dataset, which assumes that evidence is provided along
with questions. Although BIRD facilitates research advancements, it assumes
that users have expertise and domain knowledge, contradicting the fundamental
goal of text-to-SQL. In addition, human-generated evidence in BIRD contains
defects, including missing or erroneous evidence, which affects model
performance. To address this issue, we propose SEED (System for Evidence
Extraction and Domain knowledge generation), an approach that automatically
generates evidence to improve performance and practical usability in real-world
scenarios. SEED systematically analyzes database schema, description files, and
values to extract relevant information. We evaluated SEED on BIRD and Spider,
demonstrating that it significantly improves SQL generation accuracy in the
no-evidence scenario, and in some cases, even outperforms the setting where
BIRD evidence is provided. Our results highlight that SEED-generated evidence
not only bridges the gap between research and real-world deployment but also
improves the adaptability and robustness of text-to-SQL models. Our code is
available at https://github.com/felix01189/SEED

</details>


### [176] [Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models](https://arxiv.org/abs/2506.07424)
*Kyeonghyun Kim,Jinhee Jang,Juhwan Choi,Yoonji Lee,Kyohoon Jin,YoungBin Kim*

Main category: cs.CL

TL;DR: The PiFi framework merges a frozen LLM layer with a small language model (SLM), fine-tuning it to improve performance without high computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between high computational demands of LLMs and the limited generalization of SLMs.

Method: PiFi integrates a frozen layer from an LLM into an SLM and fine-tunes this combination for specific tasks.

Result: PiFi achieves consistent improvements in tasks like natural language understanding, generation, and generalization to unseen domains.

Conclusion: PiFi bridges the efficiency-performance gap by leveraging LLM knowledge in an economical manner, making advanced capabilities accessible in resource-constrained environments.

Abstract: Large language models (LLMs) are renowned for their extensive linguistic
knowledge and strong generalization capabilities, but their high computational
demands make them unsuitable for resource-constrained environments. In
contrast, small language models (SLMs) are computationally efficient but often
lack the broad generalization capacity of LLMs. To bridge this gap, we propose
PiFi, a novel framework that combines the strengths of both LLMs and SLMs to
achieve high performance while maintaining efficiency. PiFi integrates a single
frozen layer from an LLM into a SLM and fine-tunes the combined model for
specific tasks, boosting performance without a significant increase in
computational cost. We show that PiFi delivers consistent performance
improvements across a range of natural language processing tasks, including
both natural language understanding and generation. Moreover, our findings
demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing
generalization to unseen domains and facilitating the transfer of linguistic
abilities.

</details>


### [177] [Conjoined Predication and Scalar Implicature](https://arxiv.org/abs/2506.07429)
*Ratna Kandala*

Main category: cs.CL

TL;DR: This paper investigates unresolved linguistic oddities in conjunctive sentences, specifically Magri's first puzzle, providing a conceptual analysis and extending theories of scalar implicature.


<details>
  <summary>Details</summary>
Motivation: To resolve the linguistic oddity observed in certain conjunctive sentences, specifically exploring hidden interactions among quantification, collective/concurrent interpretation, and contextual updating.

Method: The authors situate Magri's first puzzle in its theoretical framework, analyze collective/concurrent interpretations of conjunctive predicates, and propose extensions to pragmatic mechanisms governing scalar implicature.

Result: The paper identifies how collective/concurrent readings cause contextual contradictions in sentences like "Some Italians come from a warm country and are blond."

Conclusion: This work broadens understanding of scalar implicature generation and demonstrates indirect contextual contradictions arising in conjunctive predicates.

Abstract: Magri (2016) investigates two puzzles arising from conjunction. Although
Magri has proposed a solution to the second puzzle, the first remains
unresolved. This first puzzle reveals a hidden interaction among
quantification, collective/concurrent interpretation, and contextual updating
dimensions that have yet to be explored. In essence, the problem is that
certain forms of sentences like "Some Italians come from a warm country," when
conjoined as in "(Only) Some Italians come from a warm country and are blond,"
sound infelicitous, even though no obvious alternative triggers a conflicting
scalar implicature. In this paper, we offer a conceptual analysis of Magri's
first puzzle by situating it within its original theoretical framework. We
argue that the oddness arises from the collective or concurrent reading of the
conjunctive predicate: in examples such as "(Only) Some Italians come from a
warm country and are blond," this interpretation generates an indirect
contextual contradiction. Moreover, we suggest that the pragmatic mechanisms
governing scalar implicature generation extend beyond what is captured by
exhaustification-based grammatical licensing accounts.

</details>


### [178] [Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434)
*Feifan Song,Shaohang Wei,Wen Luo,Yuxuan Fan,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: This paper proposes a method called Weak-to-Strong Decoding (WSD) where a smaller aligned language model drafts beginnings, and a larger model completes the task, improving generation alignment and avoiding performance degradation.


<details>
  <summary>Details</summary>
Motivation: The need to prevent large language models (LLMs) from generating harmful, false, or low-quality outputs, and the lack of current effective low-resource alignment solutions.

Method: A Weak-to-Strong Decoding (WSD) framework where a smaller, aligned model drafts initial responses, and a large base model completes them guided by an auto-switch mechanism.

Result: Experiments confirmed that WSD enhances aligned generation for various models, outperforms baseline methods, and avoids the 'alignment tax' of degraded task performance.

Conclusion: The WSD framework is an effective solution for LLM alignment, improving content quality and alignment without performance trade-offs, supported by comprehensive experiments and analyses.

Abstract: Large Language Models (LLMs) require alignment with human preferences to
avoid generating offensive, false, or meaningless content. Recently,
low-resource methods for LLM alignment have been popular, while still facing
challenges in obtaining both high-quality and aligned content. Motivated by the
observation that the difficulty of generating aligned responses is concentrated
at the beginning of decoding, we propose a novel framework, Weak-to-Strong
Decoding (WSD), to enhance the alignment ability of base models by the guidance
of a small aligned model. The small model first drafts well-aligned beginnings,
followed by the large base model to continue the rest, controlled by a
well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,
to fine-tune a small-sized Pilot-3B as the draft model, which effectively
enhances different base models under the WSD framework to outperform all
baseline methods, while avoiding degradation on downstream tasks, termed as the
alignment tax. Extensive experiments are further conducted to examine the
impact of different settings and time efficiency, as well as analyses on the
intrinsic mechanisms of WSD in depth.

</details>


### [179] [LG-ANNA-Embedding technical report](https://arxiv.org/abs/2506.07438)
*Jooyoung Choi,Hyun Kim,Hansol Jang,Changwook Jun,Kyunghoon Bae,Hyewon Choi,Stanley Jungkyu Choi,Honglak Lee,Chulmin Yun*

Main category: cs.CL

TL;DR: This paper introduces a method to create versatile text embeddings optimized for both retrieval and non-retrieval tasks using structured instructions and adaptive learning techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of creating embeddings that are generalized and effective across diverse tasks without requiring task-specific fine-tuning.

Method: The approach employs a decoder-only model with in-context learning, soft supervision through continuous relevance scores, and adaptive margin-based hard-negative mining to enhance semantic discrimination and stability.

Result: The proposed method achieves high performance and strong generalization across 41 tasks in the MTEB benchmark, ranking among the top models using the Borda score.

Conclusion: Combining in-context prompting, soft supervision, and adaptive sampling leads to scalable and robust embedding generation that performs effectively across varied tasks.

Abstract: This report presents a unified instruction-based framework for learning
generalized text embeddings optimized for both information retrieval (IR) and
non-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our
approach combines in-context learning, soft supervision, and adaptive
hard-negative mining to generate context-aware embeddings without task-specific
fine-tuning. Structured instructions and few-shot examples are used to guide
the model across diverse tasks, enabling strong performance on classification,
semantic similarity, clustering, and reranking benchmarks. To improve semantic
discrimination, we employ a soft labeling framework where continuous relevance
scores, distilled from a high-performance dense retriever and reranker, serve
as fine-grained supervision signals. In addition, we introduce adaptive
margin-based hard-negative mining, which filters out semantically ambiguous
negatives based on their similarity to positive examples, thereby enhancing
training stability and retrieval robustness. Our model is evaluated on the
newly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven
categories. Results show that our method achieves strong generalization and
ranks among the top-performing models by Borda score, outperforming several
larger or fully fine-tuned baselines. These findings highlight the
effectiveness of combining in-context prompting, soft supervision, and adaptive
sampling for scalable, high-quality embedding generation.

</details>


### [180] [Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling](https://arxiv.org/abs/2506.07453)
*Pritom Saha Akash,Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: The paper introduces DALTA, a domain adaptation framework specifically designed for low-resource topic modeling, emphasizing selective information transfer from a high-resource source domain to a low-resource target domain.


<details>
  <summary>Details</summary>
Motivation: Existing topic modeling methods struggle with unstable and incoherent topic inference in low-resource scenarios due to limited domain-specific data.

Method: The paper proposes DALTA, a framework using a shared encoder for domain-invariant features, individual decoders for domain-specific details, and adversarial alignment for controlled knowledge transfer.

Result: Experimental evaluations show DALTA surpasses current state-of-the-art approaches in topic coherence, stability, and transferability on diverse low-resource datasets.

Conclusion: DALTA effectively addresses low-resource topic modeling challenges by leveraging domain adaptation techniques and achieves superior performance metrics compared to existing methods.

Abstract: Topic modeling plays a vital role in uncovering hidden semantic structures
within text corpora, but existing models struggle in low-resource settings
where limited target-domain data leads to unstable and incoherent topic
inference. We address this challenge by formally introducing domain adaptation
for low-resource topic modeling, where a high-resource source domain informs a
low-resource target domain without overwhelming it with irrelevant content. We
establish a finite-sample generalization bound showing that effective knowledge
transfer depends on robust performance in both domains, minimizing latent-space
discrepancy, and preventing overfitting to the data. Guided by these insights,
we propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that
employs a shared encoder for domain-invariant features, specialized decoders
for domain-specific nuances, and adversarial alignment to selectively transfer
relevant information. Experiments on diverse low-resource datasets demonstrate
that DALTA consistently outperforms state-of-the-art methods in terms of topic
coherence, stability, and transferability.

</details>


### [181] [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
*Yuxin Xiao,Shan Chen,Jack Gallifant,Danielle Bitterman,Thomas Hartvigsen,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: The paper introduces a framework to categorize and evaluate the knowledge statuses of large language models (LLMs) across different scenarios, revealing insights into knowledge refinement via context features and updates.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of characterizing LLMs' knowledge beyond assessing conflicts between their internal memory and external context, and to better evaluate their knowledge consistency and correctness.

Method: A taxonomy of five knowledge statuses was developed. This was complemented by the KScope framework, which employs hierarchical statistical tests to systematically categorize LLM knowledge. The framework was tested on nine LLMs across four datasets.

Result: The findings highlight: (1) Supporting context reduces knowledge gaps among LLMs; (2) Factors like difficulty, relevance, and familiarity influence successful knowledge updates; (3) LLMs show distinct behavior when completely incorrect compared to being marginally correct or conflicted; (4) Summarized and credible context enhances update effectiveness across LLMs.

Conclusion: The methodology provides a structured way to analyze and enhance LLM knowledge, emphasizing the importance of context and contextual features in improving knowledge accuracy and updates.

Abstract: Characterizing a large language model's (LLM's) knowledge of a given question
is challenging. As a result, prior work has primarily examined LLM behavior
under knowledge conflicts, where the model's internal parametric memory
contradicts information in the external context. However, this does not fully
reflect how well the model knows the answer to the question. In this paper, we
first introduce a taxonomy of five knowledge statuses based on the consistency
and correctness of LLM knowledge modes. We then propose KScope, a hierarchical
framework of statistical tests that progressively refines hypotheses about
knowledge modes and characterizes LLM knowledge into one of these five
statuses. We apply KScope to nine LLMs across four datasets and systematically
establish: (1) Supporting context narrows knowledge gaps across models. (2)
Context features related to difficulty, relevance, and familiarity drive
successful knowledge updates. (3) LLMs exhibit similar feature preferences when
partially correct or conflicted, but diverge sharply when consistently wrong.
(4) Context summarization constrained by our feature analysis, together with
enhanced credibility, further improves update effectiveness and generalizes
across LLMs.

</details>


### [182] [From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered](https://arxiv.org/abs/2506.07461)
*Siddartha Devic,Tejas Srinivasan,Jesse Thomason,Willie Neiswanger,Vatsal Sharan*

Main category: cs.CL

TL;DR: The paper critiques current practices in uncertainty quantification (UQ) for Large Language Models (LLMs) and suggests more user-centric approaches to improve real-world decision-making.


<details>
  <summary>Details</summary>
Motivation: LLMs are widely used in real-world applications, yet their reliability remains a challenge, demanding better ways to assess their uncertainty to foster user trust.

Method: The authors analyze 40 LLM UQ methods and highlight shortcomings such as lack of ecological validity, focus on epistemic uncertainty alone, and reliance on suboptimal evaluation metrics.

Result: They identify three major issues hindering effective uncertainty quantification practices and propose alternative research directions that emphasize human-centered design.

Conclusion: The paper advocates shifting LLM UQ research practices toward user-centric goals to enhance downstream utility and decision-making in human-LLM collaborations.

Abstract: Large Language Models (LLMs) are increasingly assisting users in the real
world, yet their reliability remains a concern. Uncertainty quantification (UQ)
has been heralded as a tool to enhance human-LLM collaboration by enabling
users to know when to trust LLM predictions. We argue that current practices
for uncertainty quantification in LLMs are not optimal for developing useful UQ
for human users making decisions in real-world tasks. Through an analysis of 40
LLM UQ methods, we identify three prevalent practices hindering the community's
progress toward its goal of benefiting downstream users: 1) evaluating on
benchmarks with low ecological validity; 2) considering only epistemic
uncertainty; and 3) optimizing metrics that are not necessarily indicative of
downstream utility. For each issue, we propose concrete user-centric practices
and research directions that LLM UQ researchers should consider. Instead of
hill-climbing on unrepresentative tasks using imperfect metrics, we argue that
the community should adopt a more human-centered approach to LLM uncertainty
quantification.

</details>


### [183] [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models](https://arxiv.org/abs/2506.07463)
*Guang Liu,Liangdong Wang,Jijie Li,Yang Yu,Yao Xu,Jiabei Chen,Yu Bai,Feng Liao,Yonghua Lin*

Main category: cs.CL

TL;DR: This paper introduces CCI4.0, a large bilingual pre-training dataset aimed at enhancing data quality, reasoning diversity, and reducing hallucinations in language models.


<details>
  <summary>Details</summary>
Motivation: LLMs greatly rely on high-quality training data, but existing datasets either lack diverse reasoning patterns or require advanced processing to meet rigorous standards.

Method: The authors proposed a pipeline with processes such as two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering, coupled with CoT extraction.

Result: Empirical evaluations showed that LLMs trained on CCI4.0 exhibited better performance in downstream tasks, especially in math and code-related domains.

Conclusion: Rigorous data curation combined with human reasoning templates significantly enhances LLM performance and offers insights into automating dataset processing for pretraining.

Abstract: We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered
for superior data quality and diverse human-like reasoning trajectory. CCI4.0
occupies roughly $35$ TB of disk space and comprises two sub-datasets:
CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully
curated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and
diverse sources from math, wiki, arxiv, and code. Although these data are
mostly sourced from well-processed datasets, the quality standards of various
domains are dynamic and require extensive expert experience and labor to
process. So, we propose a novel pipeline justifying data quality mainly based
on models through two-stage deduplication, multiclassifier quality scoring, and
domain-aware fluency filtering. We extract $4.5$ billion pieces of
CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the
distillation of CoT from larger models, our proposed staged CoT extraction
exemplifies diverse reasoning patterns and significantly decreases the
possibility of hallucination. Empirical evaluations demonstrate that LLMs
pre-trained in CCI4.0 benefit from cleaner, more reliable training signals,
yielding consistent improvements in downstream tasks, especially in math and
code reflection tasks. Our results underscore the critical role of rigorous
data curation and human thinking templates in advancing LLM performance,
shedding some light on automatically processing pretraining corpora.

</details>


### [184] [Improving Fairness of Large Language Models in Multi-document Summarization](https://arxiv.org/abs/2506.07479)
*Haoyuan Li Yusen Zhang,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: FairPO addresses fairness in multi-document summarization across summary-level and corpus-level scales, achieving better performance while maintaining summary quality.


<details>
  <summary>Details</summary>
Motivation: Ensuring fairness in MDS to avoid biased representation that could affect decision-making, such as in product reviews.

Method: FairPO uses preference tuning with perturbation of document sets for summary-level fairness and dynamic weighting for corpus-level fairness.

Result: Experiments demonstrate FairPO's superiority over strong baselines with balanced fairness and quality.

Conclusion: FairPO effectively enhances fairness in summaries, setting a new benchmark in multi-document summarization.

Abstract: Fairness in multi-document summarization (MDS) is crucial for providing
comprehensive views across documents with diverse social attribute values,
which can significantly impact decision-making. For example, a summarization
system that tends to overrepresent negative reviews of products can mislead
customers into disregarding good products. Previous works measure fairness in
MDS at two levels: summary-level and corpus-level. While summary-level fairness
focuses on individual summaries, corpus-level fairness focuses on a corpus of
summaries. Recent methods primarily focus on summary-level fairness. We propose
FairPO, a preference tuning method that focuses on both summary-level and
corpus-level fairness in MDS. To improve summary-level fairness, we propose to
generate preference pairs by perturbing document sets. To improve corpus-level
fairness, we propose fairness-aware preference tuning by dynamically adjusting
the weights of preference pairs. Our experiments show that FairPO outperforms
strong baselines while maintaining the critical qualities of summaries. The
code is available at https://github.com/leehaoyuan/coverage_fairnes.

</details>


### [185] [A Hybrid GA LLM Framework for Structured Task Optimization](https://arxiv.org/abs/2506.07483)
*Berry Feng,Jonas Lin,Patrick Lau*

Main category: cs.CL

TL;DR: GA LLM combines Genetic Algorithms with Large Language Models (LLMs) to handle structured generation tasks under strict constraints through iterative improvements guided by evolutionary operations.


<details>
  <summary>Details</summary>
Motivation: To improve structured generation tasks under strict constraints, leveraging both domain knowledge and optimization capabilities.

Method: The framework treats generated outputs as genes and applies evolutionary operations (selection, crossover, and mutation) directed by the LLM to iteratively enhance solutions while maintaining structural integrity.

Result: GA LLM has demonstrated high effectiveness in tasks like itinerary planning, academic outlining, and business reporting, consistently meeting structural and requirement demands.

Conclusion: By merging Genetic Algorithms with LLMs, GA LLM delivers improved constraint satisfaction and quality solutions compared to using language models alone. Its modular design allows easy task adaptation.

Abstract: GA LLM is a hybrid framework that combines Genetic Algorithms with Large
Language Models to handle structured generation tasks under strict constraints.
Each output, such as a plan or report, is treated as a gene, and evolutionary
operations like selection, crossover, and mutation are guided by the language
model to iteratively improve solutions. The language model provides domain
knowledge and creative variation, while the genetic algorithm ensures
structural integrity and global optimization. GA LLM has proven effective in
tasks such as itinerary planning, academic outlining, and business reporting,
consistently producing well structured and requirement satisfying results. Its
modular design also makes it easy to adapt to new tasks. Compared to using a
language model alone, GA LLM achieves better constraint satisfaction and higher
quality solutions by combining the strengths of both components.

</details>


### [186] [DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech](https://arxiv.org/abs/2506.07502)
*Haotian Guo,Jing Han,Yongfeng Tu,Shihao Gao,Shengfan Shen,Wulong Xiang,Weihao Gan,Zixing Zhang*

Main category: cs.CL

TL;DR: This paper presents DEBATE, a Chinese speech-text dataset aimed at studying how speech cues help disambiguate text, addressing a gap in research on disambiguation through speech.


<details>
  <summary>Details</summary>
Motivation: Research on disambiguation via speech has been limited due to the lack of suitable datasets. The study aims to enable a deeper understanding of how speech patterns can clarify ambiguous text.

Method: DEBATE was created with 1,001 ambiguous utterances recorded by 10 native Chinese speakers, emphasizing speech patterns like pronunciation, pause, stress, and intonation. The paper also benchmarks current models using this dataset.

Result: DEBATE showcases clear differences between human and machine capabilities in interpreting spoken intent, exposing significant performance gaps.

Conclusion: The dataset offers a foundation for future research on disambiguation through speech across different languages and cultures, contributing a novel dataset and benchmarks to the field.

Abstract: Despite extensive research on textual and visual disambiguation,
disambiguation through speech (DTS) remains underexplored. This is largely due
to the lack of high-quality datasets that pair spoken sentences with richly
ambiguous text. To address this gap, we present DEBATE, a unique public Chinese
speech-text dataset designed to study how speech cues and
patterns-pronunciation, pause, stress and intonation-can help resolve textual
ambiguity and reveal a speaker's true intent. DEBATE contains 1,001 carefully
selected ambiguous utterances, each recorded by 10 native speakers, capturing
diverse linguistic ambiguities and their disambiguation through speech. We
detail the data collection pipeline and provide rigorous quality analysis.
Additionally, we benchmark three state-of-the-art large speech and
audio-language models, illustrating clear and huge performance gaps between
machine and human understanding of spoken intent. DEBATE represents the first
effort of its kind and offers a foundation for building similar DTS datasets
across languages and cultures. The dataset and associated code are available
at: https://github.com/SmileHnu/DEBATE.

</details>


### [187] [What Do Indonesians Really Need from Language Technology? A Nationwide Survey](https://arxiv.org/abs/2506.07506)
*Muhammad Dehan Al Kautsar,Lucky Susanto,Derry Wijaya,Fajri Koto*

Main category: cs.CL

TL;DR: A nationwide survey in Indonesia identifies critical language technology needs, emphasizing machine translation and information retrieval to tackle language barriers.


<details>
  <summary>Details</summary>
Motivation: To understand the real needs of Indonesia's diverse native language communities regarding language technology.

Method: A nationwide survey was conducted to gather insights directly from native speakers in Indonesia.

Result: The survey revealed a high demand for tools like machine translation and information retrieval, alongside concerns about privacy, bias, and AI data usage.

Conclusion: For effective adoption of AI, transparency and clear communication about data practices are essential, alongside focusing on the identified priorities.

Abstract: There is an emerging effort to develop NLP for Indonesias 700+ local
languages, but progress remains costly due to the need for direct engagement
with native speakers. However, it is unclear what these language communities
truly need from language technology. To address this, we conduct a nationwide
survey to assess the actual needs of native speakers in Indonesia. Our findings
indicate that addressing language barriers, particularly through machine
translation and information retrieval, is the most critical priority. Although
there is strong enthusiasm for advancements in language technology, concerns
around privacy, bias, and the use of public data for AI training highlight the
need for greater transparency and clear communication to support broader AI
adoption.

</details>


### [188] [DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction](https://arxiv.org/abs/2506.07510)
*Solee Im,Wonjun Lee,Jinmyeong An,Yunsu Kim,Jungseul Ok,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: DeRAGEC enhances Named Entity correction in ASR systems using denoising rationales within the RAGEC framework for better filtering and refinement of NE candidates.


<details>
  <summary>Details</summary>
Motivation: Improve the accuracy of Named Entity recognition and correction in Automatic Speech Recognition systems.

Method: Extends the RAGEC framework by incorporating synthetic denoising rationales for filtering noisy NE candidates and employing phonetic similarity and augmented definitions for refinement through in-context learning.

Result: Experiments on CommonVoice and STOP datasets revealed significant improvements, including a 28% relative reduction in Word Error Rate compared to ASR systems without postprocessing.

Conclusion: DeRAGEC outperforms baseline ASR and RAGEC methods, demonstrating that noise reduction and phonetic alignment yield better NE correction without additional training effort.

Abstract: We present DeRAGEC, a method for improving Named Entity (NE) correction in
Automatic Speech Recognition (ASR) systems. By extending the
Retrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC
employs synthetic denoising rationales to filter out noisy NE candidates before
correction. By leveraging phonetic similarity and augmented definitions, it
refines noisy retrieved NEs using in-context learning, requiring no additional
training. Experimental results on CommonVoice and STOP datasets show
significant improvements in Word Error Rate (WER) and NE hit ratio,
outperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28%
relative reduction in WER compared to ASR without postprocessing. Our source
code is publicly available at: https://github.com/solee0022/deragec

</details>


### [189] [Towards Large Language Models with Self-Consistent Natural Language Explanations](https://arxiv.org/abs/2506.07523)
*Sahar Admoni,Ofra Amir,Assaf Hallak,Yftah Ziser*

Main category: cs.CL

TL;DR: This paper discusses the limitations of post-hoc explanations from large language models (LLMs), introduces a new large-scale benchmark (PSCB), and proposes a metric for improving explanation quality. They fine-tune LLMs using this approach, yielding more trustworthy models.


<details>
  <summary>Details</summary>
Motivation: Many LLM-generated explanations misrepresent the actual decision-making process, as evidenced by mismatches in feature importance, and there is a need for systematic, scalable solutions to this problem.

Method: The paper introduces the Post-hoc Self-Consistency Bank (PSCB), a large-scale benchmark dataset, and proposes a new metric to better assess explanation quality. They use Direct Preference Optimization (DPO) to fine-tune LLMs for improved alignment between explanations and decision-relevant features.

Result: The proposed metric more effectively captures variations in explanation quality, and fine-tuning LLMs with DPO produces better alignment between explanations and true decision-relevant features, even in scenarios involving domain shifts.

Conclusion: The findings suggest a scalable method for achieving more trustworthy and self-consistent LLMs, addressing limitations in current post-hoc explanation methods.

Abstract: Large language models (LLMs) seem to offer an easy path to interpretability:
just ask them to explain their decisions. Yet, studies show that these post-hoc
explanations often misrepresent the true decision process, as revealed by
mismatches in feature importance. Despite growing evidence of this
inconsistency, no systematic solutions have emerged, partly due to the high
cost of estimating feature importance, which limits evaluations to small
datasets. To address this, we introduce the Post-hoc Self-Consistency Bank
(PSCB) - a large-scale benchmark of decisions spanning diverse tasks and
models, each paired with LLM-generated explanations and corresponding feature
importance scores. Analysis of PSCB reveals that self-consistency scores barely
differ between correct and incorrect predictions. We also show that the
standard metric fails to meaningfully distinguish between explanations. To
overcome this limitation, we propose an alternative metric that more
effectively captures variation in explanation quality. We use it to fine-tune
LLMs via Direct Preference Optimization (DPO), leading to significantly better
alignment between explanations and decision-relevant features, even under
domain shift. Our findings point to a scalable path toward more trustworthy,
self-consistent LLMs.

</details>


### [190] [Bit-level BPE: Below the byte boundary](https://arxiv.org/abs/2506.07541)
*Sangwhan Moon,Tatsuya Hiraoka,Naoaki Okazaki*

Main category: cs.CL

TL;DR: The paper proposes a lossless compression technique to address the issue of increased sequence lengths caused by byte-level fallbacks in tokenization for character-diverse languages such as CJK.


<details>
  <summary>Details</summary>
Motivation: Byte-level fallbacks prevent out-of-vocabulary (OOV) issues but cause longer sequence lengths for complex token representations, leading to inefficient computation for languages like CJK and emoji.

Method: The authors design a simple, lossless compression method aimed at reducing increased sequence lengths resulting from byte-level fallback tokenization.

Result: The proposed compression method mitigates inefficiencies in sequence handling without losing information.

Conclusion: A practical solution is introduced to optimize computation for tokenization in character-diverse contexts, improving efficiency without sacrificing data integrity.

Abstract: Byte-level fallbacks for subword tokenization have become a common practice
in large language models. In particular, it has been demonstrated to be
incredibly effective as a pragmatic solution for preventing OOV, especially in
the context of larger models. However, breaking a character down to individual
bytes significantly increases the sequence length for long-tail tokens in
languages such as Chinese, Japanese, and Korean (CJK) and other
character-diverse contexts such as emoji. The increased sequence length results
in longer computation during both training and inference. In this work, we
propose a simple compression technique that reduces the sequence length
losslessly.

</details>


### [191] [SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition](https://arxiv.org/abs/2506.07557)
*Mengsong Wu,Di Zhang,Yuqiang Li,Dongzhan Zhou,Wenliang Chen*

Main category: cs.CL

TL;DR: SELT enhances LLM reasoning using a modified Monte Carlo Tree Search (MCTS) algorithm without external reward models, demonstrating improvements in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs in complex reasoning tasks, where performance degrades due to issues like redundant reasoning paths and hallucination.

Method: SELT redefines MCTS scoring through intrinsic self-evaluation capabilities of LLMs and decomposes inference into atomic subtasks with semantic clustering.

Result: SELT achieves significant improvements in accuracy and reasoning robustness on challenging benchmarks like MMLU and Seal-Tools without task-specific fine-tuning.

Conclusion: The proposed method enhances reasoning capabilities of LLMs, offering strong generalizability and improved outcomes without requiring external reward models or fine-tuning.

Abstract: While Large Language Models (LLMs) have achieved remarkable success in a wide
range of applications, their performance often degrades in complex reasoning
tasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a
novel framework that leverages a modified Monte Carlo Tree Search (MCTS) to
enhance LLM reasoning without relying on external reward models. By redefining
the Upper Confidence Bound scoring to align with intrinsic self-evaluation
capabilities of LLMs and decomposing the inference process into atomic subtasks
augmented with semantic clustering at each node, SELT effectively balances
exploration and exploitation, reduces redundant reasoning paths, and mitigates
hallucination. We validate our approach on challenging benchmarks, including
the knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT
achieves significant improvements in answer accuracy and reasoning robustness
compared to baseline methods. Notably, our framework operates without
task-specific fine-tuning, demonstrating strong generalizability across diverse
reasoning tasks. Relevant results and code are available at
https://github.com/fairyshine/SELT .

</details>


### [192] [Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models](https://arxiv.org/abs/2506.07583)
*Ramakrishna Appicharla,Baban Gain,Santanu Pal,Asif Ekbal*

Main category: cs.CL

TL;DR: The paper reviews the use of large language models (LLMs) in context-aware machine translation, evaluating both prompting and fine-tuning approaches.


<details>
  <summary>Details</summary>
Motivation: To address the underexploration of LLMs in machine translation, particularly in context-aware settings.

Method: The study reviews existing methods (prompting, fine-tuning) and compares performance between commercial and open-source LLMs, identifying key gaps and future opportunities.

Result: Commercial LLMs like ChatGPT perform better than open-source LLMs in context-aware machine translation. Prompt-based methods serve as effective baselines.

Conclusion: LLMs demonstrate potential for advancing context-aware machine translation, with future opportunities in post-editing and translation agent development.

Abstract: Despite the popularity of the large language models (LLMs), their application
to machine translation is relatively underexplored, especially in context-aware
settings. This work presents a literature review of context-aware translation
with LLMs. The existing works utilise prompting and fine-tuning approaches,
with few focusing on automatic post-editing and creating translation agents for
context-aware machine translation. We observed that the commercial LLMs (such
as ChatGPT and Tower LLM) achieved better results than the open-source LLMs
(such as Llama and Bloom LLMs), and prompt-based approaches serve as good
baselines to assess the quality of translations. Finally, we present some
interesting future directions to explore.

</details>


### [193] [Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque](https://arxiv.org/abs/2506.07597)
*Oscar Sainz,Naiara Perez,Julen Etxaniz,Joseba Fernandez de Landa,Itziar Aldabe,Iker García-Ferrero,Aimar Zabala,Ekhi Azurmendi,German Rigau,Eneko Agirre,Mikel Artetxe,Aitor Soroa*

Main category: cs.CL

TL;DR: This paper investigates low-resource language adaptation using synthetic instructions generated by instruction-tuned models, and evaluates methods with Basque as a case study.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of large instruction datasets for low-resource languages, enabling language models to be effectively adapted in these scenarios.

Method: The paper uses synthetic instructions, open multilingual base models, and target-language corpora, and evaluates their combinations through experiments and human preferences.

Result: The study finds that target-language corpora are essential, synthetic instructions produce robust results, and instruction-tuned backbones outperform non-instructed models, scaling well.

Conclusion: Leveraging instruction-tuned backbones alongside synthetic instructions achieves near-frontier performance for Basque, showcasing a scalable and reproducible approach for low-resource languages.

Abstract: Instructing language models with user intent requires large instruction
datasets, which are only available for a limited set of languages. In this
paper, we explore alternatives to conventional instruction adaptation pipelines
in low-resource scenarios. We assume a realistic scenario for low-resource
languages, where only the following are available: corpora in the target
language, existing open-weight multilingual base and instructed backbone LLMs,
and synthetically generated instructions sampled from the instructed backbone.
We present a comprehensive set of experiments for Basque that systematically
study different combinations of these components evaluated on benchmarks and
human preferences from 1,680 participants. Our conclusions show that target
language corpora are essential, with synthetic instructions yielding robust
models, and, most importantly, that using as backbone an instruction-tuned
model outperforms using a base non-instructed model, and improved results when
scaling up. Using Llama 3.1 instruct 70B as backbone our model comes near
frontier models of much larger sizes for Basque, without using any Basque data
apart from the 1.2B word corpora. We release code, models, instruction
datasets, and human preferences to support full reproducibility in future
research on low-resource language adaptation.

</details>


### [194] [PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels](https://arxiv.org/abs/2506.07606)
*Peyman Rostami,Vahid Rahimzadeh,Ali Adibi,Azadeh Shakery*

Main category: cs.CL

TL;DR: This paper introduces PolitiSky24, a stance detection dataset for the 2024 U.S. presidential election focused on Kamala Harris and Donald Trump, collected from Bluesky.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of stance detection resources on emerging social platforms, providing a user-level perspective instead of tweet-level data.

Method: The dataset was created using a pipeline combining advanced information retrieval methods and large language models, achieving 81% labeling accuracy.

Result: PolitiSky24 comprises 16,044 user-target stance pairs enriched with metadata, interaction graphs, and user histories, filling gaps in political stance analysis.

Conclusion: The dataset offers timely, open, and scalable resources for political stance research, emphasizing user-level analysis and transparency.

Abstract: Stance detection identifies the viewpoint expressed in text toward a specific
target, such as a political figure. While previous datasets have focused
primarily on tweet-level stances from established platforms, user-level stance
resources, especially on emerging platforms like Bluesky remain scarce.
User-level stance detection provides a more holistic view by considering a
user's complete posting history rather than isolated posts. We present the
first stance detection dataset for the 2024 U.S. presidential election,
collected from Bluesky and centered on Kamala Harris and Donald Trump. The
dataset comprises 16,044 user-target stance pairs enriched with engagement
metadata, interaction graphs, and user posting histories. PolitiSky24 was
created using a carefully evaluated pipeline combining advanced information
retrieval and large language models, which generates stance labels with
supporting rationales and text spans for transparency. The labeling approach
achieves 81\% accuracy with scalable LLMs. This resource addresses gaps in
political stance analysis through its timeliness, open-data nature, and
user-level perspective. The dataset is available at
https://doi.org/10.5281/zenodo.15616911

</details>


### [195] [Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation](https://arxiv.org/abs/2506.07617)
*Roman Kyslyi,Yuliia Maksymiuk,Ihor Pysmennyi*

Main category: cs.CL

TL;DR: This paper adapts large language models to the Hutsul dialect, creating datasets, proposing a generation pipeline, and fine-tuning multiple LLMs for translation tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of language processing tools for the Hutsul dialect, a low-resource and morphologically complex dialect of Ukrainian.

Method: The authors constructed a parallel corpus and dictionary for Hutsul, proposed a Retrieval-Augmented Generation (RAG) pipeline to expand data, fine-tuned open-source LLMs using LoRA, and evaluated their performance using various metrics.

Result: Fine-tuned models outperformed zero-shot baselines like GPT-4o in standard-to-dialect translation tasks using both automatic and LLM-based evaluation metrics.

Conclusion: The developed methods prove effective for low-resource, morphologically complex dialect adaptation, enabling advancements in translation tasks while providing publicly available resources.

Abstract: In this paper we introduce the first effort to adapt large language models
(LLMs) to the Ukrainian dialect (in our case Hutsul), a low-resource and
morphologically complex dialect spoken in the Carpathian Highlands. We created
a parallel corpus of 9852 dialect-to-standard Ukrainian sentence pairs and a
dictionary of 7320 dialectal word mappings. We also addressed data shortage by
proposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate
synthetic parallel translation pairs, expanding the corpus with 52142 examples.
We have fine-tuned multiple open-source LLMs using LoRA and evaluated them on a
standard-to-dialect translation task, also comparing with few-shot GPT-4o
translation. In the absence of human annotators, we adopt a multi-metric
evaluation strategy combining BLEU, chrF++, TER, and LLM-based judgment
(GPT-4o). The results show that even small(7B) finetuned models outperform
zero-shot baselines such as GPT-4o across both automatic and LLM-evaluated
metrics. All data, models, and code are publicly released at:
https://github.com/woters/vuyko-hutsul

</details>


### [196] [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
*Harsh Bihany,Shubham Patel,Ashutosh Modi*

Main category: cs.CL

TL;DR: The paper introduces LoRMA, a method for efficiently adapting large language models using multiplicative transformations instead of additive updates like in LoRA.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods, such as LoRA, are computationally expensive and use additive updates, motivating the exploration of more efficient adaptation techniques.

Method: The proposed method, LoRMA, uses matrix multiplicative adaptations instead of additive updates. To address associated challenges, the authors re-order operations and introduce rank inflation strategies.

Result: Experiments demonstrate that LoRMA is effective across various evaluation metrics, proving its computational efficiency and capability.

Conclusion: LoRMA offers an innovative and efficient alternative to traditional fine-tuning methods, mitigating computational complexity with effective design strategies.

Abstract: Large Language Models have shown remarkable capabilities in the NLP domain.
Their effectiveness can mainly be attributed to their ability to adapt to an
array of downstream tasks. However, generally, full fine-tuning is a
computationally expensive job. To mitigate this, many techniques have been
developed that prime efficiency, a prominent one being Low-Rank Adaptation
(LoRA). However, LoRA and its variants employ re-parametrized additive updates.
In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which
shifts the paradigm of additive updates to a richer space of matrix
multiplicative transformations. We tackle challenges such as computational
complexity and rank bottleneck of matrix multiplication by effectively
re-ordering operations and introducing rank inflation strategies. We conduct
extensive experiments to demonstrate the effectiveness of our approach in terms
of various evaluation metrics.

</details>


### [197] [Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation](https://arxiv.org/abs/2506.07626)
*Kseniia Petukhova,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: This paper investigates how fine-grained annotation of teacher intents can improve tutoring responses generated by large language models (LLMs), specifically focusing on math instruction. They fine-tune an LLM with a detailed taxonomy and find it produces more effective outputs.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the gap in pedagogical alignment for large language models when used in intelligent tutoring systems, acknowledging that LLMs, while promising, require task-specific adaptations for effective use in education.

Method: The researchers applied an automated annotation framework to MathDial, categorizing teacher intents into a detailed taxonomy of eleven intents. They fine-tuned an LLM using this fine-grained annotation and compared its performance with a model trained on a simpler taxonomy with four categories.

Result: Fine-tuned LLMs based on the detailed taxonomy generated responses that were more pedagogically aligned and effective, as evidenced by both automatic and qualitative evaluations.

Conclusion: Using specific and detailed intent annotations enhances the ability of LLMs to generate controlled, effective tutoring responses in mathematical instruction, underscoring the value of intent specificity in educational applications.

Abstract: Large language models (LLMs) hold great promise for educational applications,
particularly in intelligent tutoring systems. However, effective tutoring
requires alignment with pedagogical strategies - something current LLMs lack
without task-specific adaptation. In this work, we explore whether fine-grained
annotation of teacher intents can improve the quality of LLM-generated tutoring
responses. We focus on MathDial, a dialog dataset for math instruction, and
apply an automated annotation framework to re-annotate a portion of the dataset
using a detailed taxonomy of eleven pedagogical intents. We then fine-tune an
LLM using these new annotations and compare its performance to models trained
on the original four-category taxonomy. Both automatic and qualitative
evaluations show that the fine-grained model produces more pedagogically
aligned and effective responses. Our findings highlight the value of intent
specificity for controlled text generation in educational settings, and we
release our annotated data and code to facilitate further research.

</details>


### [198] [Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline](https://arxiv.org/abs/2506.07631)
*Brian Gordon,Yonatan Bitton,Andreea Marzoca,Yasumasa Onoe,Xiao Wang,Daniel Cohen-Or,Idan Szpektor*

Main category: cs.CL

TL;DR: The paper introduces DOCCI-Critique, a benchmark for evaluating paragraph-length image captions from VLMs, and VNLI-Critique, a model for assessing and improving their factual accuracy. The methods achieve state-of-the-art results in evaluation and correction of VLM outputs.


<details>
  <summary>Details</summary>
Motivation: Evaluating the factual accuracy of detailed, paragraph-length captions generated by large vision-language models is challenging, as existing methods often fail to detect fine-grained inaccuracies or lack appropriate datasets.

Method: The authors developed DOCCI-Critique, a benchmark with human annotations for factual correctness in paragraph captions, and VNLI-Critique, a model for automated factuality classification and critique generation. They further integrate VNLI-Critique into applications like VLM rankings and text correction via a Critic-and-Revise pipeline.

Result: VNLI-Critique achieves state-of-the-art performance on other benchmarks (M-HalDetect and CHOCOLATE claim verification), aligns closely with human judgments for ranking VLMs (0.98 Spearman), and improves caption factuality significantly (46% improvement on DetailCaps-4870).

Conclusion: This work offers a robust benchmark (DOCCI-Critique) and practical tools (VNLI-Critique) for enhancing the fine-grained evaluation and factual accuracy of VLM-generated captions, pushing the boundaries of VLM image understanding.

Abstract: Large Vision-Language Models (VLMs) now generate highly detailed,
paragraphlength image captions, yet evaluating their factual accuracy remains
challenging. Current methods often miss fine-grained errors, being designed for
shorter texts or lacking datasets with verified inaccuracies. We introduce
DOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100
images, 14 VLMs) featuring over 10,216 sentence-level human annotations of
factual correctness and explanatory rationales for errors, all within paragraph
context. Building on this, we develop VNLI-Critique, a model for automated
sentence-level factuality classification and critique generation. We highlight
three key applications: (1) VNLI-Critique demonstrates robust generalization,
validated by state-of-the-art performance on the M-HalDetect benchmark and
strong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven
AutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent
alignment with human factuality judgments (e.g., 0.98 Spearman). (3) An
innovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide
LLM-based corrections, achieves substantial improvements in caption factuality
(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark
alongside practical tools, designed to significantly elevate the standards for
fine-grained evaluation and foster the improvement of VLM image understanding.
Project page: https://google.github.io/unblocking-detail-caption

</details>


### [199] [TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review](https://arxiv.org/abs/2506.07642)
*Yuan Chang,Ziyue Li,Hengyuan Zhang,Yuanbo Kong,Yanru Wu,Zhijiang Guo,Ngai Wong*

Main category: cs.CL

TL;DR: TreeReview introduces a hierarchical question-answering approach for assisting peer reviews, optimizing depth and efficiency, and outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: Current methods for LLM-assisted peer review struggle to balance thoroughness and efficiency in generating insightful reviews.

Method: The paper proposes TreeReview, a framework that decomposes high-level review questions into a hierarchical tree structure and resolves them iteratively with a dynamic question expansion to deepen analysis.

Result: TreeReview significantly improves review quality and consistency over strong baselines while reducing LLM token usage by up to 80%.

Conclusion: TreeReview offers an effective and efficient framework for generating expert-aligned, comprehensive, and resource-efficient peer reviews, validated by both human and computational evaluation.

Abstract: While Large Language Models (LLMs) have shown significant potential in
assisting peer review, current methods often struggle to generate thorough and
insightful reviews while maintaining efficiency. In this paper, we propose
TreeReview, a novel framework that models paper review as a hierarchical and
bidirectional question-answering process. TreeReview first constructs a tree of
review questions by recursively decomposing high-level questions into
fine-grained sub-questions and then resolves the question tree by iteratively
aggregating answers from leaf to root to get the final review. Crucially, we
incorporate a dynamic question expansion mechanism to enable deeper probing by
generating follow-up questions when needed. We construct a benchmark derived
from ICLR and NeurIPS venues to evaluate our method on full review generation
and actionable feedback comments generation tasks. Experimental results of both
LLM-based and human evaluation show that TreeReview outperforms strong
baselines in providing comprehensive, in-depth, and expert-aligned review
feedback, while reducing LLM token usage by up to 80% compared to
computationally intensive approaches. Our code and benchmark dataset are
available at https://github.com/YuanChang98/tree-review.

</details>


### [200] [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
*Maciej Chrabąszcz,Katarzyna Lorenc,Karolina Seweryn*

Main category: cs.CL

TL;DR: This paper highlights vulnerabilities in large language models (LLMs) to simple attacks in low-resource languages, notably Polish, by modifying characters and words.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown exceptional NLP capabilities but face challenges in safety due to limited training data in low-resource languages, making them susceptible to attacks.

Method: The authors devised cheap and strong character- and word-level attacks using minimal modifications and a small proxy model for word importance calculation.

Result: The study confirmed that the attacks significantly impacted LLM predictions, exposing vulnerabilities in safety mechanisms, particularly in low-resource languages like Polish.

Conclusion: LLMs require improved safety training and evaluation across all languages, especially low-resource ones, to prevent exploitation by such attacks. Released datasets and code aim to support future research.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
various natural language processing (NLP) tasks in recent years. However, their
susceptibility to jailbreaks and perturbations necessitates additional
evaluations. Many LLMs are multilingual, but safety-related training data
contains mainly high-resource languages like English. This can leave them
vulnerable to perturbations in low-resource languages such as Polish. We show
how surprisingly strong attacks can be cheaply created by altering just a few
characters and using a small proxy model for word importance calculation. We
find that these character and word-level attacks drastically alter the
predictions of different LLMs, suggesting a potential vulnerability that can be
used to circumvent their internal safety mechanisms. We validate our attack
construction methodology on Polish, a low-resource language, and find potential
vulnerabilities of LLMs in this language. Additionally, we show how it can be
extended to other languages. We release the created datasets and code for
further research.

</details>


### [201] [Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation](https://arxiv.org/abs/2506.07646)
*Rui Hu,Xiaolong Lin,Jiawang Liu,Shixi Huang,Zhenpeng Zhan*

Main category: cs.CL

TL;DR: The paper proposes a method to automate phonemic and prosodic labeling for Japanese TTS datasets using fine-tuned ASR models and dictionary-based decoding strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the process of annotating phonemic and prosodic labels for Japanese TTS datasets, which is typically time-consuming and requires manual efforts.

Method: The method involves fine-tuning a large pre-trained ASR model conditioned on ground truth transcripts to generate phrase-level annotations and employing a dictionary-based decoding strategy to correct phonemic labeling errors.

Result: Objective evaluation shows that the proposed method performs better than previous methods based only on text or audio. Subjective evaluation reveals that the TTS model using the proposed annotations generates speech with naturalness comparable to manual annotations.

Conclusion: The method effectively streamlines the annotation process for Japanese TTS datasets and maintains high-quality outputs comparable to manual annotation.

Abstract: In this paper, we propose a method for annotating phonemic and prosodic
labels on a given audio-transcript pair, aimed at constructing Japanese
text-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale
pre-trained automatic speech recognition (ASR) model, conditioned on ground
truth transcripts, to simultaneously output phrase-level graphemes and
annotation labels. To further correct errors in phonemic labeling, we employ a
decoding strategy that utilizes dictionary prior knowledge. The objective
evaluation results demonstrate that our proposed method outperforms previous
approaches relying solely on text or audio. The subjective evaluation results
indicate that the naturalness of speech synthesized by the TTS model, trained
with labels annotated using our method, is comparable to that of a model
trained with manual annotations.

</details>


### [202] [Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping](https://arxiv.org/abs/2506.07658)
*Nitin Sharma,Thomas Wolfers,Çağatay Yıldız*

Main category: cs.CL

TL;DR: This paper introduces a deterministic pipeline for creating domain-specific benchmarks to evaluate language models without contamination, offering a practical evaluation method and uncovering insights into knowledge representation during domain adaptation.


<details>
  <summary>Details</summary>
Motivation: There is a need for reliable domain-specific benchmarks and a deeper understanding of knowledge representation during domain adaptation in language models.

Method: The authors developed a pipeline that generates benchmarks from raw domain corpora using Term Frequency (TF) and Term TF-IDF methods, creating prompt-target pairs for measuring domain-specific knowledge. Models are evaluated via completion tasks, avoiding contamination and reducing computational costs.

Result: Their benchmark correlates well with expert-generated benchmarks and outperforms traditional perplexity metrics. They observed rapid domain adaptation in smaller models, identified different layer responsibilities during adaptation, and highlighted mechanisms of catastrophic forgetting.

Conclusion: The work provides a practical and contamination-free evaluation method for domain-specific language models, along with valuable insights into adaptation and forgetting processes, prompting improved fine-tuning strategies.

Abstract: The paper addresses two critical challenges in language model (LM)
evaluation: creating reliable domain-specific benchmarks and understanding
knowledge representation during domain adaptation. We introduce a deterministic
pipeline that converts raw domain corpora into completion-type benchmarks
without relying on LMs or human curation, eliminating benchmark contamination
issues while enabling evaluation on the latest domain data. Our approach
generates domain-specific keywords and related word lists using TF and Term
TF-IDF methods and constructs prompt-target pairs. We evaluate models by
measuring their ability to complete these prompts with the correct
domain-specific targets, providing a direct assessment of domain knowledge with
low computational cost. Through comprehensive experiments across multiple
models (GPT-2 medium/XL, Llama-2/3.1, OLMo-2, Qwen-2, Mistral) and domains, we
demonstrate that our benchmark strongly correlates with expert-generated
benchmarks while providing a more accurate measure of domain knowledge than
traditional perplexity metrics. We reveal that domain adaptation happens
rapidly in smaller models (within 500 steps) and illustrate a new approach to
domain knowledge evaluation in base models during training for early stopping.
By extending mechanistic analysis to domain adaptation, we discover that
initial-to-mid layers are primarily responsible for attribute extraction, while
later layers focus on next token prediction. Furthermore, we show that during
adaptation, forgetting begins in the middle layers, where attribute extraction
happens and is amplified in later layers. Our work provides both a practical
evaluation methodology for domain-specific LMs and novel insights into
knowledge representation during adaptation, with implications for more
efficient fine-tuning strategies and targeted approaches to mitigate
catastrophic forgetting.

</details>


### [203] [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)
*Lei Xu,Sirui Chen,Yuxuan Huang,Chaochao Lu*

Main category: cs.CL

TL;DR: The paper addresses challenges in mathematical reasoning for LLMs, introducing a method that uses structured solutions for data generation, resulting in improved benchmarks and validated effectiveness.


<details>
  <summary>Details</summary>
Motivation: Mathematical reasoning is complex for LLMs due to intricate logic and precise computation requirements. Current methods generate datasets but struggle with quality and problem complexity.

Method: The paper extracts structural information using generated problem-solving code, guides data generation with structured solutions, and introduces new benchmarks for higher difficulty mathematical problems.

Result: The approach generates 39K labeled problems and a 6.1K higher difficulty benchmark, showing declining model performance with longer reasoning steps. Fine-tuning experiments validate dataset effectiveness on various LLMs.

Conclusion: The structured solution method and dataset enhance mathematical reasoning abilities in LLMs and serve as effective tools for future improvements.

Abstract: Mathematical reasoning remains challenging for LLMs due to complex logic and
the need for precise computation. Existing methods enhance LLM reasoning by
synthesizing datasets through problem rephrasing, but face issues with
generation quality and problem complexity. To address this, we propose to
extract structural information with generated problem-solving code from
mathematical reasoning and guide data generation with structured solutions.
Applied to MATH and GSM8K, our approach produces 39K problems with labeled
intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results
on our benchmark show that model performance declines as reasoning length
increases. Additionally, we conducted fine-tuning experiments using the
proposed training data on a range of LLMs, and the results validate the
effectiveness of our dataset. We hope the proposed method and dataset will
contribute to future research in enhancing LLM reasoning capabilities.

</details>


### [204] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
*Prarabdh Shukla,Wei Yin Chong,Yash Patel,Brennan Schaffner,Danish Pruthi,Arjun Bhagoji*

Main category: cs.CL

TL;DR: An audit of Twitch's automated moderation tool, AutoMod, found significant weaknesses: it misses a large portion of hateful content and over-censors benign messages.


<details>
  <summary>Details</summary>
Motivation: To understand the effectiveness of Twitch's AutoMod in flagging hateful content during real-time interactions on live streams.

Method: Tested AutoMod by sending over 107,000 comments, containing hateful and benign messages, through Twitch chats using APIs, sourced from four datasets.

Result: AutoMod failed to flag up to 94% of hateful comments in some datasets and blocked up to 89.5% of benign messages that included sensitive words in positive contexts.

Conclusion: AutoMod lacks effective contextual understanding, leading to both under-moderation of hateful messages and over-moderation of benign ones.

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


### [205] [GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation](https://arxiv.org/abs/2506.07671)
*Ionut-Teodor Sorodoc,Leonardo F. R. Ribeiro,Rexhina Blloshmi,Christopher Davis,Adrià de Gispert*

Main category: cs.CL

TL;DR: The paper presents GaRAGe, a RAG benchmark with human-curated long-form answers and annotations for fine-grained LLM evaluation in retrieving and grounding relevant passages.


<details>
  <summary>Details</summary>
Motivation: To create a benchmark that evaluates LLMs' ability to retrieve relevant information and provide accurate responses or deflections when insufficient grounding is available.

Method: The GaRAGe benchmark includes 2366 diverse questions and over 35K annotated passages from private and public sources for rigorous evaluation of LLMs on relevance, factuality, and deflective capabilities.

Result: State-of-the-art LLMs struggle with accurate grounding in responses, showing a maximum Relevance-Aware Factuality Score of 60%, a 31% true positive deflection rate, and an F1 attribution score of 58.9%.

Conclusion: Current LLMs underperform in grounding their responses precisely or deflecting appropriately, especially for time-sensitive questions and sparse private sources, highlighting the need for advancements in these areas.

Abstract: We present GaRAGe, a large RAG benchmark with human-curated long-form answers
and annotations of each grounding passage, allowing a fine-grained evaluation
of whether LLMs can identify relevant grounding when generating RAG answers.
Our benchmark contains 2366 questions of diverse complexity, dynamism, and
topics, and includes over 35K annotated passages retrieved from both private
document sets and the Web, to reflect real-world RAG use cases. This makes it
an ideal test bed to evaluate an LLM's ability to identify only the relevant
information necessary to compose a response, or provide a deflective response
when there is insufficient information. Evaluations of multiple
state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise
rather than (a) ground their answers strictly on the annotated relevant
passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)
deflect when no relevant grounding is available (reaching at most 31% true
positive rate in deflections). The F1 in attribution to relevant sources is at
most 58.9%, and we show that performance is particularly reduced when answering
time-sensitive questions and when having to draw knowledge from sparser private
grounding sources.

</details>


### [206] [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
*Jiaming Li,Haoran Ye,Yukun Chen,Xinyue Li,Lei Zhang,Hamid Alinejad-Rokny,Jimmy Chih-Hsien Peng,Min Yang*

Main category: cs.CL

TL;DR: The paper introduces FAST, a novel training method to enhance sparse autoencoders' performance on instruct models, achieving significant improvements in reconstruction quality and feature interpretability.


<details>
  <summary>Details</summary>
Motivation: As large language models (LLMs) advance, there is an increasing need to understand their inner workings. Sparse autoencoders (SAEs) serve as an essential tool for extracting interpretable features, but existing methods cater mainly to base models, leading to reduced efficacy on instruct models. This necessitates a specialized training approach for instruct models.

Method: The authors propose FAST (Finetuning-Aligned Sequential Training), a tailored training method that aligns with the unique data distribution and activation characteristics of instruct models, enhancing both reconstruction quality and feature extraction.

Result: FAST outperforms baseline methods in both token reconstruction and feature interpretability. For example, it achieves a mean squared error of 0.6468 on Qwen2.5-7B-Instruct, compared to baseline errors of 5.1985 and 1.5096. In feature interpretability, 21.1% of features scored in the top range for Llama3.2-3B-Instruct, versus 7.0% and 10.2% for other methods.

Conclusion: FAST significantly improves SAE performance on instruct models, bridging the gap left by previous methods. This approach also opens new paths for fine-grained control over LLM behavior, as evidenced by the potential of activation interventions. The code and associated resources are available for further exploration.

Abstract: As large language models (LLMs) grow in scale and capability, understanding
their internal mechanisms becomes increasingly critical. Sparse autoencoders
(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the
extraction of human-interpretable features from LLMs. However, existing SAE
training methods are primarily designed for base models, resulting in reduced
reconstruction quality and interpretability when applied to instruct models. To
bridge this gap, we propose
$\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned
$\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining
($\textit{FAST}$), a novel training method specifically tailored for instruct
models. $\textit{FAST}$ aligns the training process with the data distribution
and activation patterns characteristic of instruct models, resulting in
substantial improvements in both reconstruction and feature interpretability.
On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468
in token reconstruction, significantly outperforming baseline methods with
errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$
yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,
$21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for
$\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that
intervening on the activations of special tokens via the SAEs leads to
improvements in output quality, suggesting new opportunities for fine-grained
control of model behavior. Code, data, and 240 trained SAEs are available at
https://github.com/Geaming2002/FAST.

</details>


### [207] [Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2506.07712)
*Renjie Luo,Jiaxi Li,Chen Huang,Wei Lu*

Main category: cs.CL

TL;DR: The paper identifies and analyzes a problem called Long CoT Degradation, where small language models (SLMs <=3B parameters) perform worse when trained on limited long chain-of-thought (CoT) data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to investigate why small language models struggle with reasoning when trained with limited long CoT data, challenging the common assumption that long CoT is universally beneficial.

Method: Extensive experiments were conducted on various SLM families (Qwen2.5, LLaMA3, Gemma3) to quantify performance degradation and analyze its causes, specifically focusing on error accumulation in longer responses.

Result: It was found that models trained on limited long CoT data could lose up to 75% of their original performance. Additionally, even training on substantial data (220k examples) could not fully restore performance for particularly small models.

Conclusion: The study highlights that long CoT training can harm SLMs instead of helping them due to error accumulation, and provides guidance for mitigating these effects with scaled supervised fine-tuning in practical applications.

Abstract: Long chain-of-thought (CoT) supervision has become a common strategy to
enhance reasoning in language models. While effective for large models, we
identify a phenomenon we call Long CoT Degradation, in which small language
models (SLMs; <=3B parameters) trained on limited long CoT data experience
significant performance deterioration. Through extensive experiments on the
Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is
widespread across SLMs. In some settings, models trained on only 8k long CoT
examples lose up to 75% of their original performance before fine-tuning.
Strikingly, we further observe that for some particularly small models, even
training on 220k long CoT examples fails to recover or surpass their original
performance prior to fine-tuning. Our analysis attributes this effect to error
accumulation: while longer responses increase the capacity for multi-step
reasoning, they also amplify the risk of compounding mistakes. Furthermore, we
find that Long CoT Degradation may negatively impacts downstream reinforcement
learning (RL), although this can be alleviated by sufficiently scaled
supervised fine-tuning (SFT). Our findings challenge common assumptions about
the benefits of long CoT training for SLMs and offer practical guidance for
building more effective small-scale reasoning models.

</details>


### [208] [Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility](https://arxiv.org/abs/2506.07719)
*Mengyang Qiu,Tran Minh Nguyen,Zihao Huang,Zelong Li,Yang Gu,Qingyu Gao,Siliang Liu,Jungyeul Park*

Main category: cs.CL

TL;DR: The paper introduces a modular framework to standardize multilingual grammatical error annotations using language-agnostic and language-specific methodologies, reimplementing errant via stanza to enhance multilingual applicability.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks like errant face difficulties in handling typologically diverse languages, necessitating a modular and standardized system for multilingual grammatical error annotation.

Method: The paper combines a language-agnostic foundation with structured language-specific extensions and reimplements errant using stanza for broader multilingual application.

Result: The framework demonstrates adaptability across multiple languages (English, German, Czech, Korean, and Chinese) for both general and customized linguistic refinements.

Conclusion: The work contributes to scalable GEC annotation across languages and promotes consistent evaluation in multilingual contexts, with open access to tools and codebase for further utilization.

Abstract: Grammatical Error Correction (GEC) relies on accurate error annotation and
evaluation, yet existing frameworks, such as $\texttt{errant}$, face
limitations when extended to typologically diverse languages. In this paper, we
introduce a standardized, modular framework for multilingual grammatical error
annotation. Our approach combines a language-agnostic foundation with
structured language-specific extensions, enabling both consistency and
flexibility across languages. We reimplement $\texttt{errant}$ using
$\texttt{stanza}$ to support broader multilingual coverage, and demonstrate the
framework's adaptability through applications to English, German, Czech,
Korean, and Chinese, ranging from general-purpose annotation to more customized
linguistic refinements. This work supports scalable and interpretable GEC
annotation across languages and promotes more consistent evaluation in
multilingual settings. The complete codebase and annotation tools can be
accessed at https://github.com/open-writing-evaluation/jp_errant_bea.

</details>


### [209] [Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU](https://arxiv.org/abs/2506.07726)
*Vincenzo Timmel,Manfred Vogel,Daniel Perruchoud,Reza Kakooee*

Main category: cs.CL

TL;DR: This paper introduces a new long-form Swiss Parliaments Corpus, achieving a 6-point BLEU improvement by combining transcription, GPT-4o corrections, and filtering methods.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality long-form dataset of Swiss German parliamentary debates, overcoming challenges in low-resource, domain-specific speech corpora.

Method: The pipeline involves transcribing debates with Whisper Large-v3, applying two stages of GPT-4o corrections, and filtering segments based on BLEU and evaluation scores.

Result: Generated a final corpus with 801 hours of audio, 751 of which meet quality standards, and improved BLEU scores by 6 points over the previous dataset.

Conclusion: The approach demonstrates an effective way to enhance low-resource speech corpora using advanced ASR, LLM-based corrections, and stringent quality controls.

Abstract: This paper presents a new long-form release of the Swiss Parliaments Corpus,
converting entire multi-hour Swiss German debate sessions (each aligned with
the official session protocols) into high-quality speech-text pairs. Our
pipeline starts by transcribing all session audio into Standard German using
Whisper Large-v3 under high-compute settings. We then apply a two-step GPT-4o
correction process: first, GPT-4o ingests the raw Whisper output alongside the
official protocols to refine misrecognitions, mainly named entities. Second, a
separate GPT-4o pass evaluates each refined segment for semantic completeness.
We filter out any segments whose Predicted BLEU score (derived from Whisper's
average token log-probability) and GPT-4o evaluation score fall below a certain
threshold. The final corpus contains 801 hours of audio, of which 751 hours
pass our quality control. Compared to the original sentence-level SPC release,
our long-form dataset achieves a 6-point BLEU improvement, demonstrating the
power of combining robust ASR, LLM-based correction, and data-driven filtering
for low-resource, domain-specific speech corpora.

</details>


### [210] [Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)
*Silin Gao,Antoine Bosselut,Samy Bengio,Emmanuel Abbe*

Main category: cs.CL

TL;DR: This paper presents a novel approach to improving reasoning robustness in large language models (LLMs) through a method called "AbstraL" that leverages reinforcement learning (RL) to abstract reasoning problems.


<details>
  <summary>Details</summary>
Motivation: Large language models often exhibit performance degradation under distribution shifts, such as changes in numerical variables or distracting clauses, compromising their reasoning robustness.

Method: The authors propose "AbstraL," a method that utilizes reinforcement learning (RL) to train LLMs to abstract reasoning problems and connect to symbolic tools, as opposed to generating synthetic data or using supervised fine-tuning.

Result: AbstraL demonstrates significant improvements in mitigating performance degradation on GSM perturbation benchmarks, indicating enhanced robustness against distribution shifts.

Conclusion: Abstracting reasoning tasks using reinforcement learning is a promising strategy for making LLMs more robust, outperforming traditional supervised fine-tuning approaches.

Abstract: Recent studies have shown that large language models (LLMs), especially
smaller ones, often lack robustness in their reasoning. I.e., they tend to
experience performance drops when faced with distribution shifts, such as
changes to numerical or nominal variables, or insertions of distracting
clauses. A possible strategy to address this involves generating synthetic data
to further "instantiate" reasoning problems on potential variations. In
contrast, our approach focuses on "abstracting" reasoning problems. This not
only helps counteract distribution shifts but also facilitates the connection
to symbolic tools for deriving solutions. We find that this abstraction process
is better acquired through reinforcement learning (RL) than just supervised
fine-tuning, which often fails to produce faithful abstractions. Our method,
AbstraL -- which promotes abstract reasoning in LLMs using RL on granular
abstraction data -- significantly mitigates performance degradation on recent
GSM perturbation benchmarks.

</details>


### [211] [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
*Xiaotian Ye,Mengqi Zhang,Shu Wu*

Main category: cs.CL

TL;DR: The study introduces a new benchmark for evaluating LLM unlearning methods and proposes a novel training-free unlearning method called Rank-one Concept Redirection (ROCR).


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) often retain harmful or private knowledge, creating a need for effective unlearning techniques that can generalize across varied expressions of the same knowledge.

Method: The authors identify Form-Dependent Bias in unlearning methods and propose a benchmark, ORT, to measure robustness. They also introduce ROCR, a training-free unlearning method that redirects the model's understanding of harmful concepts to safe ones by modifying model parameters.

Result: The paper finds that Form-Dependent Bias is widespread in current unlearning methods. Experiments show that ROCR significantly outperforms traditional methods with improved effectiveness and natural output generation.

Conclusion: Form-Independent unlearning is essential for real-world security-critical applications. The proposed ROCR method demonstrates promise as a practical and efficient approach to address Form-Dependent Bias in LLM unlearning.

Abstract: Large Language Model (LLM) unlearning aims to erase or suppress undesirable
knowledge within the model, offering promise for controlling harmful or private
information to prevent misuse. However, recent studies highlight its limited
efficacy in real-world scenarios, hindering practical adoption. In this study,
we identify a pervasive issue underlying many downstream failures: the
effectiveness of existing unlearning methods heavily depends on the form of
training samples and frequently fails to generalize to alternate expressions of
the same knowledge. We formally characterize this problem as Form-Dependent
Bias and systematically investigate its specific manifestation patterns across
various downstream tasks. To quantify its prevalence and support future
research, we introduce ORT, a novel benchmark designed to evaluate the
robustness of unlearning methods against variations in knowledge expression.
Results reveal that Form-Dependent Bias is both widespread and severe among
current techniques.
  We argue that LLM unlearning should be form-independent to address the
endless forms of downstream tasks encountered in real-world security-critical
scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),
a novel training-free method, as a promising solution path. ROCR performs
unlearning by targeting the invariants in downstream tasks, specifically the
activated dangerous concepts. It is capable of modifying model parameters
within seconds to redirect the model's perception of a specific unlearning
target concept to another harmless concept. Extensive experiments demonstrate
that ROCR significantly improves unlearning effectiveness compared to
traditional methods while generating highly natural outputs.

</details>


### [212] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
*Iustin Sirbu,Robert-Adrian Popovici,Cornelia Caragea,Stefan Trausan-Matu,Traian Rebedea*

Main category: cs.CL

TL;DR: MultiMatch is a semi-supervised learning algorithm combining co-training, consistency regularization, and pseudo-labeling with a novel pseudo-label weighting module.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the robustness and performance of SSL algorithms, especially in imbalanced text classification settings.

Method: MultiMatch integrates three techniques—heads agreement, self-adaptive thresholds, and Average Pseudo-Margins—into a unified pseudo-label weighting module.

Result: MultiMatch achieves state-of-the-art efficacy on 9 of 10 setups from 5 NLP datasets, ranking first among 19 methods in robustness and performance.

Conclusion: The algorithm showcases superior performance and robustness, especially under imbalanced data scenarios, positioning itself as a reliable choice for text classification tasks in SSL.

Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm
combining the paradigms of co-training and consistency regularization with
pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label
weighting module designed for three key purposes: selecting and filtering
pseudo-labels based on head agreement and model confidence, and weighting them
according to the perceived classification difficulty. This novel module
enhances and unifies three existing techniques -- heads agreement from
Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average
Pseudo-Margins from MarginMatch -- resulting in a holistic approach that
improves robustness and performance in SSL settings. Experimental results on
benchmark datasets highlight the superior performance of MultiMatch, achieving
state-of-the-art results on 9 out of 10 setups from 5 natural language
processing datasets and ranking first according to the Friedman test among 19
methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly
imbalanced settings, outperforming the second-best approach by 3.26% -- and
data imbalance is a key factor for many text classification tasks.

</details>


### [213] [WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code](https://arxiv.org/abs/2506.07818)
*Zhiyu Lin,Zhengda Zhou,Zhiyuan Zhao,Tianrui Wan,Yilun Ma,Junyu Gao,Xuelong Li*

Main category: cs.CL

TL;DR: This paper introduces WebUIBench, a benchmark for assessing Multimodal Large Language Models (MLLMs) in web application development.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks inadequately evaluate the multidimensional sub-capabilities of MLLMs, focusing only on webpage generation instead of the entire development process.

Method: The authors propose WebUIBench, a benchmark based on software engineering principles, evaluating MLLMs across four areas using 21K Q&A pairs from 0.7K real-world websites.

Result: The study evaluates 29 MLLMs, revealing their skill characteristics and weaknesses in web application development tasks.

Conclusion: WebUIBench helps reveal the development strengths and weaknesses of MLLMs, guiding improvements in their efficiency for web development applications.

Abstract: With the rapid advancement of Generative AI technology, Multimodal Large
Language Models(MLLMs) have the potential to act as AI software engineers
capable of executing complex web application development. Considering that the
model requires a confluence of multidimensional sub-capabilities to address the
challenges of various development phases, constructing a multi-view evaluation
framework is crucial for accurately guiding the enhancement of development
efficiency. However, existing benchmarks usually fail to provide an assessment
of sub-capabilities and focus solely on webpage generation outcomes. In this
work, we draw inspiration from the principles of software engineering and
further propose WebUIBench, a benchmark systematically designed to evaluate
MLLMs in four key areas: WebUI Perception, HTML Programming,WebUI-HTML
Understanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality
question-answer pairs derived from over 0.7K real-world websites. The extensive
evaluation of 29 mainstream MLLMs uncovers the skill characteristics and
various weakness that models encountered during the development process.

</details>


### [214] [Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning](https://arxiv.org/abs/2506.07851)
*Yiju Guo,Wenkai Yang,Zexu Sun,Ning Ding,Zhiyuan Liu,Yankai Lin*

Main category: cs.CL

TL;DR: The paper introduces a two-stage framework called Learning to Focus (LeaF) to improve reasoning and generation quality in large language models by mitigating the effect of confounding factors in training data.


<details>
  <summary>Details</summary>
Motivation: Although large language models have shown advancements in contextual understanding, they struggle with long-context reasoning and are prone to distractions caused by spurious correlations in the training data. This leads to redundant reasoning, inference delays, and suboptimal or erroneous responses.

Method: The proposed method, LeaF, identifies and mitigates confounding factors in two stages: (1) it uses gradient-based comparisons with a teacher model to pinpoint confounding tokens, and (2) removes these confounders during the model distillation to align the student's attention with the teacher's onto critical context tokens.

Result: The proposed LeaF framework improves reasoning accuracy and generation quality across benchmarks in mathematical reasoning and code generation. It also enhances interpretability and reliability by suppressing attention to confounding elements during inference.

Conclusion: LeaF provides an effective approach to disentangle and address spurious correlations in training data, resulting in better alignment of the model's focus on relevant information, superior performance, and reduced inference overhead.

Abstract: Large language models (LLMs) have demonstrated significant improvements in
contextual understanding. However, their ability to attend to truly critical
information during long-context reasoning and generation still falls behind the
pace. Specifically, our preliminary experiments reveal that certain distracting
patterns can misdirect the model's attention during inference, and removing
these patterns substantially improves reasoning accuracy and generation
quality. We attribute this phenomenon to spurious correlations in the training
data, which obstruct the model's capacity to infer authentic causal
instruction-response relationships. This phenomenon may induce redundant
reasoning processes, potentially resulting in significant inference overhead
and, more critically, the generation of erroneous or suboptimal responses. To
mitigate this, we introduce a two-stage framework called Learning to Focus
(LeaF) leveraging intervention-based inference to disentangle confounding
factors. In the first stage, LeaF employs gradient-based comparisons with an
advanced teacher to automatically identify confounding tokens based on causal
relationships in the training corpus. Then, in the second stage, it prunes
these tokens during distillation to enact intervention, aligning the student's
attention with the teacher's focus distribution on truly critical context
tokens. Experimental results demonstrate that LeaF not only achieves an
absolute improvement in various mathematical reasoning and code generation
benchmarks but also effectively suppresses attention to confounding tokens
during inference, yielding a more interpretable and reliable reasoning model.

</details>


### [215] [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
*Ke Wang,Yiming Qin,Nikolaos Dimitriadis,Alessandro Favero,Pascal Frossard*

Main category: cs.CL

TL;DR: MEMOIR is a novel framework for efficiently updating language models post-hoc using a dedicated memory module to minimize interference and forgetting, achieving state-of-the-art results across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing language model editing methods either hinder generalization, interfere with past edits, or struggle with long sequences of updates, necessitating a reliable and scalable solution.

Method: MEMOIR uses a residual memory for edits and employs sample-dependent masks to sparsify input activations, isolating edits within distinct memory subsets to minimize interference. During inference, relevant edits are identified through sparse activation pattern comparisons.

Result: MEMOIR outperforms other methods in reliability, generalization, and locality, handling thousands of sequential edits with minimal forgetting across benchmarks like question answering, hallucination correction, and out-of-distribution generalization.

Conclusion: MEMOIR provides a scalable, effective approach for post-hoc language model editing, preserving prior knowledge while enabling extensive updates without compromising model performance.

Abstract: Language models deployed in real-world systems often require post-hoc updates
to incorporate new or corrected knowledge. However, editing such models
efficiently and reliably - without retraining or forgetting previous
information - remains a major challenge. Existing methods for lifelong model
editing either compromise generalization, interfere with past edits, or fail to
scale to long editing sequences. We propose MEMOIR, a novel scalable framework
that injects knowledge through a residual memory, i.e., a dedicated parameter
module, while preserving the core capabilities of the pre-trained model. By
sparsifying input activations through sample-dependent masks, MEMOIR confines
each edit to a distinct subset of the memory parameters, minimizing
interference among edits. At inference, it identifies relevant edits by
comparing the sparse activation patterns of new queries to those stored during
editing. This enables generalization to rephrased queries by activating only
the relevant knowledge while suppressing unnecessary memory activation for
unrelated prompts. Experiments on question answering, hallucination correction,
and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral
demonstrate that MEMOIR achieves state-of-the-art performance across
reliability, generalization, and locality metrics, scaling to thousands of
sequential edits with minimal forgetting.

</details>


### [216] [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)
*MiniCPM Team,Chaojun Xiao,Yuxuan Li,Xu Han,Yuzhuo Bai,Jie Cai,Haotian Chen,Wentong Chen,Xin Cong,Ganqu Cui,Ning Ding,Shengdan Fan,Yewei Fang,Zixuan Fu,Wenyu Guan,Yitong Guan,Junshao Guo,Yufeng Han,Bingxiang He,Yuxiang Huang,Cunliang Kong,Qiuzuo Li,Siyuan Li,Wenhao Li,Yanghao Li,Yishan Li,Zhen Li,Dan Liu,Biyuan Lin,Yankai Lin,Xiang Long,Quanyu Lu,Yaxi Lu,Peiyan Luo,Hongya Lyu,Litu Ou,Yinxu Pan,Zekai Qu,Qundong Shi,Zijun Song,Jiayuan Su,Zhou Su,Ao Sun,Xianghui Sun,Peijun Tang,Fangzheng Wang,Feng Wang,Shuo Wang,Yudong Wang,Yesai Wu,Zhenyu Xiao,Jie Xie,Zihao Xie,Yukun Yan,Jiarui Yuan,Kaihuo Zhang,Lei Zhang,Linyue Zhang,Xueren Zhang,Yudi Zhang,Hengyu Zhao,Weilin Zhao,Weilun Zhao,Yuanqian Zhao,Zhi Zheng,Ge Zhou,Jie Zhou,Wei Zhou,Zihan Zhou,Zixuan Zhou,Zhiyuan Liu,Guoyang Zeng,Chao Jia,Dahai Li,Maosong Sun*

Main category: cs.CL

TL;DR: MiniCPM4 is a highly efficient large language model tailored for on-device applications, featuring innovations in architecture, data, algorithms, and inference, available in 0.5B and 8B parameter versions.


<details>
  <summary>Details</summary>
Motivation: To create an efficient large language model optimized for resource-limited end-side devices while maintaining competitive performance.

Method: Introduced innovations like sparse attention (InfLLM v2), clean dataset strategies (UltraClean, UltraChat v2), improved training algorithms (ModelTunnel v2), and an efficient inference framework (CPM.cu), with two parameter variants (0.5B and 8B).

Result: MiniCPM4 outperformed similar open-source models and achieved significant speed boosts in long-sequence processing, proving effective across multiple benchmarks.

Conclusion: MiniCPM4 is a state-of-the-art, efficient LLM for end-side devices, showcasing strong performance and broad applicability in diverse areas like surveys and tool-based tasks.

Abstract: This paper introduces MiniCPM4, a highly efficient large language model (LLM)
designed explicitly for end-side devices. We achieve this efficiency through
systematic innovation in four key dimensions: model architecture, training
data, training algorithms, and inference systems. Specifically, in terms of
model architecture, we propose InfLLM v2, a trainable sparse attention
mechanism that accelerates both prefilling and decoding phases for long-context
processing. Regarding training data, we propose UltraClean, an efficient and
accurate pre-training data filtering and generation strategy, and UltraChat v2,
a comprehensive supervised fine-tuning dataset. These datasets enable
satisfactory model performance to be achieved using just 8 trillion training
tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient
pre-training strategy search, and improve existing post-training methods by
introducing chunk-wise rollout for load-balanced reinforcement learning and
data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose
CPM.cu that integrates sparse attention, model quantization, and speculative
sampling to achieve efficient prefilling and decoding. To meet diverse
on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B
parameters, respectively. Sufficient evaluation results show that MiniCPM4
outperforms open-source models of similar size across multiple benchmarks,
highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B
demonstrates significant speed improvements over Qwen3-8B when processing long
sequences. Through further adaptation, MiniCPM4 successfully powers diverse
applications, including trustworthy survey generation and tool use with model
context protocol, clearly showcasing its broad usability.

</details>


### [217] [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
*Hongming Yang,Shi Lin,Jun Shao,Changting Lin,Donghai Zhu,Meng Han,Qinglei Kong*

Main category: cs.CL

TL;DR: This paper introduces DeBoP, an automatic prompt optimization method to enhance the inference capabilities of Lightweight Large Language Models (LwLLMs), significantly improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations in inference and reasoning capabilities of LwLLMs and overcome the inefficiencies of existing manual or meta-cognitive prompt optimization techniques.

Method: DeBoP employs a gradient-free Monte Carlo Tree Search to transform complex prompt optimization processes into discrete execution sequences, directly optimizing the behavior of LwLLMs.

Result: DeBoP substantially improves performance on seven challenging tasks, surpassing GPT-3.5 and reducing computational time by 60% compared to other prompt optimization methods.

Conclusion: DeBoP enhances LwLLMs by providing effective behavior optimization, expanding their applicability and efficiency in handling complex tasks.

Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized
models designed to run efficiently on consumer-grade hardware, offering
significant advantages in resource efficiency, cost-effectiveness, and data
privacy. However, these models often struggle with limited inference and
reasoning capabilities, which restrict their performance on complex tasks and
limit their practical applicability. Moreover, existing prompt optimization
methods typically rely on extensive manual effort or the meta-cognitive
abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To
address these challenges, we introduce DeBoP, a new Direct Behavior
Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting
technique. Unlike CoT Prompting, DeBoP is an automatic optimization method,
which focuses on the optimization directly on the behavior of LwLLMs. In
particular, DeBoP transforms the optimization of complex prompts into the
optimization of discrete, quantifiable execution sequences using a
gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging
tasks where state-of-the-art LLMs excel but LwLLMs generally underperform.
Experimental results demonstrate that DeBoP significantly outperforms recent
prompt optimization methods on most tasks. In particular, DeBoP-optimized
LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by
approximately 60% compared to other automatic prompt optimization methods.

</details>


### [218] [Quantum Graph Transformer for NLP Sentiment Classification](https://arxiv.org/abs/2506.07937)
*Shamminuj Aktar,Andreas Bärtschi,Abdel-Hameed A. Badawy,Stephan Eidenbenz*

Main category: cs.CL

TL;DR: The paper introduces Quantum Graph Transformer (QGT), a model combining quantum self-attention with graph-based message-passing for structured language modeling, achieving improved accuracy and sample efficiency over classical and quantum NLP models.


<details>
  <summary>Details</summary>
Motivation: Utilize quantum computing to enhance the efficiency and expressiveness of models in understanding complex, structured data, particularly for language processing.

Method: Developed a hybrid Quantum Graph Transformer (QGT) architecture that incorporates parameterized quantum circuits (PQCs) into the self-attention mechanism within a graph-based framework.

Result: QGT outperformed existing quantum and classical models in sentiment classification tasks, achieving higher accuracy, requiring fewer labeled samples, and improving sample efficiency.

Conclusion: QGT showcases the potential of graph-based quantum techniques for scalable and efficient language understanding, establishing a notable contribution to quantum machine learning in NLP tasks.

Abstract: Quantum machine learning is a promising direction for building more efficient
and expressive models, particularly in domains where understanding complex,
structured data is critical. We present the Quantum Graph Transformer (QGT), a
hybrid graph-based architecture that integrates a quantum self-attention
mechanism into the message-passing framework for structured language modeling.
The attention mechanism is implemented using parameterized quantum circuits
(PQCs), which enable the model to capture rich contextual relationships while
significantly reducing the number of trainable parameters compared to classical
attention mechanisms. We evaluate QGT on five sentiment classification
benchmarks. Experimental results show that QGT consistently achieves higher or
comparable accuracy than existing quantum natural language processing (QNLP)
models, including both attention-based and non-attention-based approaches. When
compared with an equivalent classical graph transformer, QGT yields an average
accuracy improvement of 5.42% on real-world datasets and 4.76% on synthetic
datasets. Additionally, QGT demonstrates improved sample efficiency, requiring
nearly 50% fewer labeled samples to reach comparable performance on the Yelp
dataset. These results highlight the potential of graph-based QNLP techniques
for advancing efficient and scalable language understanding.

</details>


### [219] [Statistical Hypothesis Testing for Auditing Robustness in Language Models](https://arxiv.org/abs/2506.07947)
*Paulius Rauba,Qiyao Wei,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: This paper introduces a framework for analyzing how large language model (LLM) outputs change under interventions by reformulating the problem into a hypothesis testing framework.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of evaluating changes in LLM outputs under interventions, which is difficult due to the stochastic nature of LLMs and the computational infeasibility of comparing output distributions.

Method: The authors propose a model-agnostic, hypothesis testing framework that employs Monte Carlo sampling to create low-dimensional semantic similarity spaces, allowing for tractable analysis of LLM perturbations.

Result: The framework effectively measures response changes, evaluates alignment with reference models, and quantifies true/false positive rates through multiple case studies.

Conclusion: The proposed approach is a reliable and interpretable tool for auditing LLMs, offering valuable insights into model behavior under various interventions.

Abstract: Consider the problem of testing whether the outputs of a large language model
(LLM) system change under an arbitrary intervention, such as an input
perturbation or changing the model variant. We cannot simply compare two LLM
outputs since they might differ due to the stochastic nature of the system, nor
can we compare the entire output distribution due to computational
intractability. While existing methods for analyzing text-based outputs exist,
they focus on fundamentally different problems, such as measuring bias or
fairness. To this end, we introduce distribution-based perturbation analysis, a
framework that reformulates LLM perturbation analysis as a frequentist
hypothesis testing problem. We construct empirical null and alternative output
distributions within a low-dimensional semantic similarity space via Monte
Carlo sampling, enabling tractable inference without restrictive distributional
assumptions. The framework is (i) model-agnostic, (ii) supports the evaluation
of arbitrary input perturbations on any black-box LLM, (iii) yields
interpretable p-values; (iv) supports multiple perturbations via controlled
error rates; and (v) provides scalar effect sizes. We demonstrate the
usefulness of the framework across multiple case studies, showing how we can
quantify response changes, measure true/false positive rates, and evaluate
alignment with reference models. Above all, we see this as a reliable
frequentist hypothesis testing framework for LLM auditing.

</details>


### [220] [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
*Tim Vieira,Tianyu Liu,Clemente Pasti,Yahya Emara,Brian DuSell,Benjamin LeBrun,Mario Giulianelli,Juan Luis Gastaldi,Timothy J. O'Donnell,Ryan Cotterell*

Main category: cs.CL

TL;DR: This paper identifies and addresses the problem of language models assigning probability mass to "noncanonical" token encodings, which are unnecessary and erroneous. It proposes two solutions to enforce canonical token representations, improving model performance and likelihood on held-out data.


<details>
  <summary>Details</summary>
Motivation: Modern language models rely on deterministic tokenizers like byte-pair encoding, but they unintentionally assign probabilities to invalid token sequences (noncanonical encodings). This misallocation wastes resources and reduces model accuracy, prompting the need for corrective measures.

Method: The authors propose two methods: (1) Canonicality by conditioning, which modifies test-time inference without retraining the model, and (2) Canonicality by construction, which involves reparameterizing the model to ensure only canonical outputs, requiring retraining.

Result: Both proposed methods reduce canonicality errors and improve the likelihood of held-out data across several models and corpora, proving their effectiveness.

Conclusion: Enforcing canonicality in token-level language models rectifies significant inefficiencies and errors, leading to better utilization of probability mass and improved performance on unseen datasets.

Abstract: Modern language models represent probability distributions over character
strings as distributions over (shorter) token strings derived via a
deterministic tokenizer, such as byte-pair encoding. While this approach is
highly effective at scaling up language models to large corpora, its current
incarnations have a concerning property: the model assigns nonzero probability
mass to an exponential number of $\it{noncanonical}$ token encodings of each
character string -- these are token strings that decode to valid character
strings but are impossible under the deterministic tokenizer (i.e., they will
never be seen in any training corpus, no matter how large). This misallocation
is both erroneous, as noncanonical strings never appear in training data, and
wasteful, diverting probability mass away from plausible outputs. These are
avoidable mistakes! In this work, we propose methods to enforce canonicality in
token-level language models, ensuring that only canonical token strings are
assigned positive probability. We present two approaches: (1) canonicality by
conditioning, leveraging test-time inference strategies without additional
training, and (2) canonicality by construction, a model parameterization that
guarantees canonical outputs but requires training. We demonstrate that fixing
canonicality mistakes improves the likelihood of held-out data for several
models and corpora.

</details>


### [221] [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)
*Qingxiu Dong,Li Dong,Yao Tang,Tianzhu Ye,Yutao Sun,Zhifang Sui,Furu Wei*

Main category: cs.CL

TL;DR: Reinforcement Pre-Training (RPT) redefines next-token prediction as a reasoning task using reinforcement learning, leveraging large-scale text data to enhance model accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional language modeling techniques, by leveraging reinforcement learning (RL) for scalable pre-training and next-token prediction.

Method: Reframing next-token prediction as a reasoning task trained using RL, with verifiable rewards for accurate predictions, allowing for the use of vast text data.

Result: RPT achieves significant improvement in next-token reasoning accuracy and offers robust pre-trained models for further reinforcement fine-tuning.

Conclusion: RPT presents a promising paradigm for advancing language model pre-training, demonstrating consistent accuracy improvements with increased computation.

Abstract: In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling
paradigm for large language models and reinforcement learning (RL).
Specifically, we reframe next-token prediction as a reasoning task trained
using RL, where it receives verifiable rewards for correctly predicting the
next token for a given context. RPT offers a scalable method to leverage vast
amounts of text data for general-purpose RL, rather than relying on
domain-specific annotated answers. By incentivizing the capability of
next-token reasoning, RPT significantly improves the language modeling accuracy
of predicting the next tokens. Moreover, RPT provides a strong pre-trained
foundation for further reinforcement fine-tuning. The scaling curves show that
increased training compute consistently improves the next-token prediction
accuracy. The results position RPT as an effective and promising scaling
paradigm to advance language model pre-training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [222] [Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow](https://arxiv.org/abs/2506.06283)
*Juexiao Zhou,Zhongyi Han,Mankun Xin,Xingwei He,Guotao Wang,Jiaoyan Song,Gongning Luo,Wenjia He,Xintong Li,Yuetan Chu,Juanwen Chen,Bo Wang,Xia Wu,Wenwen Duan,Zhixia Guo,Liyan Bai,Yilin Pan,Xuefei Bi,Lu Liu,Long Feng,Xiaonan He,Xin Gao*

Main category: cs.CV

TL;DR: DigitalShadow introduces a facial recognition-based early warning system for coronary artery disease (CAD) to predict risks passively and contactlessly.


<details>
  <summary>Details</summary>
Motivation: CAD causes over 17.8 million global deaths annually, and effective early detection methods are needed to combat this largely preventable disease.

Method: The approach uses a facial foundation model pre-trained on 21 million images, then fine-tuned with data from 7,004 facial images of 1,751 subjects to develop LiveCAD for risk assessment.

Result: DigitalShadow extracts facial features from live video streams to passively assess CAD risks and provides customized health advice via natural language.

Conclusion: DigitalShadow pioneers a secure, privacy-focused, AI-driven CAD risk prediction technology, enabling personalized healthcare recommendations.

Abstract: Global population aging presents increasing challenges to healthcare systems,
with coronary artery disease (CAD) responsible for approximately 17.8 million
deaths annually, making it a leading cause of global mortality. As CAD is
largely preventable, early detection and proactive management are essential. In
this work, we introduce DigitalShadow, an advanced early warning system for
CAD, powered by a fine-tuned facial foundation model. The system is pre-trained
on 21 million facial images and subsequently fine-tuned into LiveCAD, a
specialized CAD risk assessment model trained on 7,004 facial images from 1,751
subjects across four hospitals in China. DigitalShadow functions passively and
contactlessly, extracting facial features from live video streams without
requiring active user engagement. Integrated with a personalized database, it
generates natural language risk reports and individualized health
recommendations. With privacy as a core design principle, DigitalShadow
supports local deployment to ensure secure handling of user data.

</details>


### [223] [Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images](https://arxiv.org/abs/2506.06389)
*Rifat Sadik,Tanvir Rahman,Arpan Bhattacharjee,Bikash Chandra Halder,Ismail Hossain*

Main category: cs.CV

TL;DR: This paper explores the vulnerability of Vision Transformers (ViTs) to adversarial watermarking in medical image analysis and proposes adversarial training as an effective defense.


<details>
  <summary>Details</summary>
Motivation: The study investigates the potential susceptibility of state-of-the-art Vision Transformer models to adversarial perturbations in the domain of medical image analysis.

Method: Adversarial watermarking using Projected Gradient Descent (PGD) is employed to evaluate the transferability of adversarial attacks to CNNs and test adversarial training as a defense mechanism.

Result: Vision Transformers show severe accuracy drops (as low as 27.6%) under adversarial attacks but regain robustness (up to 90.0% accuracy) through adversarial training.

Conclusion: While Vision Transformers excel in computer vision tasks, they face significant vulnerability to adversarial techniques, highlighting the need for robust defense strategies such as adversarial training.

Abstract: Deep learning models have shown remarkable success in dermatological image
analysis, offering potential for automated skin disease diagnosis. Previously,
convolutional neural network(CNN) based architectures have achieved immense
popularity and success in computer vision (CV) based task like skin image
recognition, generation and video analysis. But with the emergence of
transformer based models, CV tasks are now are nowadays carrying out using
these models. Vision Transformers (ViTs) is such a transformer-based models
that have shown success in computer vision. It uses self-attention mechanisms
to achieve state-of-the-art performance across various tasks. However, their
reliance on global attention mechanisms makes them susceptible to adversarial
perturbations. This paper aims to investigate the susceptibility of ViTs for
medical images to adversarial watermarking-a method that adds so-called
imperceptible perturbations in order to fool models. By generating adversarial
watermarks through Projected Gradient Descent (PGD), we examine the
transferability of such attacks to CNNs and analyze the performance defense
mechanism -- adversarial training. Results indicate that while performance is
not compromised for clean images, ViTs certainly become much more vulnerable to
adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,
adversarial training raises it up to 90.0%.

</details>


### [224] [(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training](https://arxiv.org/abs/2506.06480)
*A. Postlmayr,P. Cosman,S. Dey*

Main category: cs.CV

TL;DR: The paper proposes a scalable, cost-effective fitness tracking system that uses an RGB smartphone camera and a vision-language model to detect and count repetitions of over 1,900 exercises.


<details>
  <summary>Details</summary>
Motivation: Existing exercise tracking models are either too restrictive in terms of exercise variety or too complex for practical use, creating a need for a more robust and generalized solution.

Method: The authors developed a multitask motion analysis model that performs exercise detection and repetition counting on skeletal data using RGB video. They also assembled Olympia, a large-scale dataset encompassing over 1,900 exercises.

Result: The presented model demonstrated 76.5% exercise detection accuracy and 85.3% off-by-one accuracy in counting repetitions, showcasing strong performance on the Olympia dataset.

Conclusion: By integrating exercise identification and rep counting in a single vision-language transformer, the study significantly advances the accessibility and scalability of AI-powered fitness tracking.

Abstract: We introduce a fitness tracking system that enables remote monitoring for
exercises using only a RGB smartphone camera, making fitness tracking more
private, scalable, and cost effective. Although prior work explored automated
exercise supervision, existing models are either too limited in exercise
variety or too complex for real-world deployment. Prior approaches typically
focus on a small set of exercises and fail to generalize across diverse
movements. In contrast, we develop a robust, multitask motion analysis model
capable of performing exercise detection and repetition counting across
hundreds of exercises, a scale far beyond previous methods. We overcome
previous data limitations by assembling a large-scale fitness dataset, Olympia
covering more than 1,900 exercises. To our knowledge, our vision-language model
is the first that can perform multiple tasks on skeletal fitness data. On
Olympia, our model can detect exercises with 76.5% accuracy and count
repetitions with 85.3% off-by-one accuracy, using only RGB video. By presenting
a single vision-language transformer model for both exercise identification and
rep counting, we take a significant step toward democratizing AI-powered
fitness tracking.

</details>


### [225] [GS4: Generalizable Sparse Splatting Semantic SLAM](https://arxiv.org/abs/2506.06517)
*Mingqi Jiang,Chanho Kim,Chen Ziwen,Li Fuxin*

Main category: cs.CV

TL;DR: The paper introduces a SLAM algorithm that uses Gaussian Splatting and a learned network for accurate 3D semantic mapping from RGB-D video streams, addressing generalizability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional SLAM struggles with low-resolution and incomplete 3D maps, and existing Gaussian Splatting methods are scene-specific and time-intensive, limiting their practicality.

Method: The method uses a generalizable network paired with an RGB-D image recognition backbone to predict Gaussian parameters and integrate 3D semantic segmentation. Localization drift is corrected through a 1-iteration optimization step.

Result: The approach demonstrates state-of-the-art performance on the ScanNet benchmark, reducing Gaussian usage by an order of magnitude and achieving zero-shot generalization to NYUv2 and TUM RGB-D datasets.

Conclusion: The proposed method advances semantic SLAM by offering a generalizable, efficient solution for high-accuracy 3D mapping and recognition that scales across diverse real-world scenes.

Abstract: Traditional SLAM algorithms are excellent at camera tracking but might
generate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting
(GS) approaches have emerged as an option for SLAM with accurate, dense 3D map
building. However, existing GS-based SLAM methods rely on per-scene
optimization which is time-consuming and does not generalize to diverse scenes
well. In this work, we introduce the first generalizable GS-based semantic SLAM
algorithm that incrementally builds and updates a 3D scene representation from
an RGB-D video stream using a learned generalizable network. Our approach
starts from an RGB-D image recognition backbone to predict the Gaussian
parameters from every downsampled and backprojected image location.
Additionally, we seamlessly integrate 3D semantic segmentation into our GS
framework, bridging 3D mapping and recognition through a shared backbone. To
correct localization drifting and floaters, we propose to optimize the GS for
only 1 iteration following global localization. We demonstrate state-of-the-art
semantic SLAM performance on the real-world benchmark ScanNet with an order of
magnitude fewer Gaussians compared to other recent GS-based methods, and
showcase our model's generalization capability through zero-shot transfer to
the NYUv2 and TUM RGB-D datasets.

</details>


### [226] [Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models](https://arxiv.org/abs/2506.06537)
*Seung-jae Lee,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: The paper introduces a zero-shot audiovisual segmentation (AVS) framework using pretrained models, eliminating the need for annotations while achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current AVS methods rely heavily on large-scale pixel-level annotations, which are expensive and time-consuming to create.

Method: The researchers developed a zero-shot AVS framework that leverages pretrained models across audio, vision, and text modalities to address modality gaps, eliminating AVS-specific annotations.

Result: The framework achieved state-of-the-art results in zero-shot AVS across multiple datasets, confirming its effectiveness.

Conclusion: Integrating pretrained multimodal representations allows precise sound source segmentation, showcasing a new efficient approach for audiovisual segmentation.

Abstract: Audiovisual segmentation (AVS) aims to identify visual regions corresponding
to sound sources, playing a vital role in video understanding, surveillance,
and human-computer interaction. Traditional AVS methods depend on large-scale
pixel-level annotations, which are costly and time-consuming to obtain. To
address this, we propose a novel zero-shot AVS framework that eliminates
task-specific training by leveraging multiple pretrained models. Our approach
integrates audio, vision, and text representations to bridge modality gaps,
enabling precise sound source segmentation without AVS-specific annotations. We
systematically explore different strategies for connecting pretrained models
and evaluate their efficacy across multiple datasets. Experimental results
demonstrate that our framework achieves state-of-the-art zero-shot AVS
performance, highlighting the effectiveness of multimodal model integration for
finegrained audiovisual segmentation.

</details>


### [227] [Securing Traffic Sign Recognition Systems in Autonomous Vehicles](https://arxiv.org/abs/2506.06563)
*Thushari Hapuarachchi,Long Dang,Kaiqi Xiong*

Main category: cs.CV

TL;DR: The paper investigates the vulnerability of DNNs for traffic sign recognition to data poisoning and proposes a robust training method leveraging data augmentation and a detection model for improved defense.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of DNNs used in traffic sign recognition systems to data poisoning attacks, ensuring model reliability and safety in real-world deployment.

Method: The authors perform error-minimizing attacks on DNNs by adding imperceptible perturbations to training data. They introduce a data augmentation-based training method utilizing nonlinear transformations to counteract the attacks. Additionally, they develop a detection model to identify poisoned data.

Result: The error-minimizing attacks reduced prediction accuracy from 99.90% to 10.6%. The proposed mitigation strategy successfully restored accuracy to 96.05% and outperformed adversarial training. The detection model achieved over 99% success in identifying poisoned data.

Conclusion: The study demonstrates the vulnerability of DNNs to imperceptible perturbations in traffic sign recognition and highlights the efficacy of advanced methods like data augmentation and detection modeling to bolster robustness against data poisoning attacks.

Abstract: Deep Neural Networks (DNNs) are widely used for traffic sign recognition
because they can automatically extract high-level features from images. These
DNNs are trained on large-scale datasets obtained from unknown sources.
Therefore, it is important to ensure that the models remain secure and are not
compromised or poisoned during training. In this paper, we investigate the
robustness of DNNs trained for traffic sign recognition. First, we perform the
error-minimizing attacks on DNNs used for traffic sign recognition by adding
imperceptible perturbations on training data. Then, we propose a data
augmentation-based training method to mitigate the error-minimizing attacks.
The proposed training method utilizes nonlinear transformations to disrupt the
perturbations and improve the model robustness. We experiment with two
well-known traffic sign datasets to demonstrate the severity of the attack and
the effectiveness of our mitigation scheme. The error-minimizing attacks reduce
the prediction accuracy of the DNNs from 99.90% to 10.6%. However, our
mitigation scheme successfully restores the prediction accuracy to 96.05%.
Moreover, our approach outperforms adversarial training in mitigating the
error-minimizing attacks. Furthermore, we propose a detection model capable of
identifying poisoned data even when the perturbations are imperceptible to
human inspection. Our detection model achieves a success rate of over 99% in
identifying the attack. This research highlights the need to employ advanced
training methods for DNNs in traffic sign recognition systems to mitigate the
effects of data poisoning attacks.

</details>


### [228] [NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery](https://arxiv.org/abs/2506.06898)
*Reese Kneeland,Paul S. Scotti,Ghislain St-Yves,Jesse Breedlove,Kendrick Kay,Thomas Naselaris*

Main category: cs.CV

TL;DR: The paper introduces NSD-Imagery, a benchmark dataset to evaluate how well models trained on the Natural Scenes Dataset (NSD) perform on reconstructing mental images from fMRI data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for models that can generalize from reconstructing seen images to reconstructing mental images, which is critical for applications in medical domains and brain-computer interfaces.

Method: They benchmark several NSD-trained visual decoding models on their ability to reconstruct mental images from NSD-Imagery and analyze how architectural choices impact performance.

Result: The results show that decoding performance on mental images is largely independent of performance on vision reconstruction. Simple linear architectures and multimodal feature decoding generalize better, while complex architectures overfit visual training data.

Conclusion: NSD-Imagery is essential for driving advancements in practical applications of visual decoding by encouraging better alignment between model capabilities and mental imagery reconstruction needs.

Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired
with mental images, to complement the existing Natural Scenes Dataset (NSD), a
large-scale dataset of fMRI activity paired with seen images that enabled
unprecedented improvements in fMRI-to-image reconstruction efforts. Recent
models trained on NSD have been evaluated only on seen image reconstruction.
Using NSD-Imagery, it is possible to assess how well these models perform on
mental image reconstruction. This is a challenging generalization requirement
because mental images are encoded in human brain activity with relatively lower
signal-to-noise and spatial resolution; however, generalization from seen to
mental imagery is critical for real-world applications in medical domains and
brain-computer interfaces, where the desired information is always internally
generated. We provide benchmarks for a suite of recent NSD-trained open-source
visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et
al.) on NSD-Imagery, and show that the performance of decoding methods on
mental images is largely decoupled from performance on vision reconstruction.
We further demonstrate that architectural choices significantly impact
cross-decoding performance: models employing simple linear decoding
architectures and multimodal feature decoding generalize better to mental
imagery, while complex architectures tend to overfit visual training data. Our
findings indicate that mental imagery datasets are critical for the development
of practical applications, and establish NSD-Imagery as a useful resource for
better aligning visual decoding methods with this goal.

</details>


### [229] [Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models](https://arxiv.org/abs/2506.06569)
*Yannis Spyridis,Vasileios Argyriou*

Main category: cs.CV

TL;DR: This paper explores using RGB imagery and deep learning for textile recycling by classifying textile types and segmenting non-textile contaminants.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of automated textile recycling by addressing challenges in material classification and contaminant detection.

Method: Utilized transfer learning for textile classification (EfficientNetB0 model) and a zero-shot semantic segmentation approach using Grounding DINO and SAM for contaminant detection.

Result: Achieved 81.25% accuracy in textile classification and a 0.90 mIoU score in mask segmentation for contaminants.

Conclusion: RGB imagery combined with modern deep learning techniques is feasible for key pre-processing tasks in automated textile recycling pipelines.

Abstract: Automated sorting is crucial for improving the efficiency and scalability of
textile recycling, but accurately identifying material composition and
detecting contaminants from sensor data remains challenging. This paper
investigates the use of standard RGB imagery, a cost-effective sensing
modality, for key pre-processing tasks in an automated system. We present
computer vision components designed for a conveyor belt setup to perform (a)
classification of four common textile types and (b) segmentation of non-textile
features such as buttons and zippers. For classification, several pre-trained
architectures were evaluated using transfer learning and cross-validation, with
EfficientNetB0 achieving the best performance on a held-out test set with
81.25\% accuracy. For feature segmentation, a zero-shot approach combining the
Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM)
was employed, demonstrating excellent performance with a mIoU of 0.90 for the
generated masks against ground truth. This study demonstrates the feasibility
of using RGB images coupled with modern deep learning techniques, including
transfer learning for classification and foundation models for zero-shot
segmentation, to enable essential analysis steps for automated textile
recycling pipelines.

</details>


### [230] [A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance](https://arxiv.org/abs/2506.06578)
*Anees Nashath Shaik,Barbara Villarini,Vasileios Argyriou*

Main category: cs.CV

TL;DR: The paper addresses biases in facial recognition by proposing a data-driven platform that generates diverse synthetic training sets and enhances image quality using deep learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve facial recognition performance in surveillance systems which is compromised due to low-quality images, biased datasets, and skin tone variations.

Method: The method involves generating synthetic facial datasets using deep learning tools like autoencoders and Generative Adversarial Networks (GANs) and improving image quality via an enhancement module.

Result: The evaluation with the CelebA dataset showed improved training data diversity and better model fairness in facial recognition.

Conclusion: The approach effectively reduces bias in AI-based facial analysis and enhances surveillance accuracy, contributing to fairer security systems.

Abstract: Surveillance systems play a critical role in security and reconnaissance, but
their performance is often compromised by low-quality images and videos,
leading to reduced accuracy in face recognition. Additionally, existing
AI-based facial analysis models suffer from biases related to skin tone
variations and partially occluded faces, further limiting their effectiveness
in diverse real-world scenarios. These challenges are the results of data
limitations and imbalances, where available training datasets lack sufficient
diversity, resulting in unfair and unreliable facial recognition performance.
To address these issues, we propose a data-driven platform that enhances
surveillance capabilities by generating synthetic training data tailored to
compensate for dataset biases. Our approach leverages deep learning-based
facial attribute manipulation and reconstruction using autoencoders and
Generative Adversarial Networks (GANs) to create diverse and high-quality
facial datasets. Additionally, our system integrates an image enhancement
module, improving the clarity of low-resolution or occluded faces in
surveillance footage. We evaluate our approach using the CelebA dataset,
demonstrating that the proposed platform enhances both training data diversity
and model fairness. This work contributes to reducing bias in AI-based facial
analysis and improving surveillance accuracy in challenging environments,
leading to fairer and more reliable security applications.

</details>


### [231] [EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras](https://arxiv.org/abs/2506.06596)
*Youssef Farah,Federico Paredes-Vallés,Guido De Croon,Muhammad Ahmed Humais,Hussain Sajwani,Yahya Zweiri*

Main category: cs.CV

TL;DR: The article introduces EV-LayerSegNet, a self-supervised CNN for event-based motion segmentation using event cameras, achieving strong evaluation metrics in a simulated dataset.


<details>
  <summary>Details</summary>
Motivation: Event cameras capture motion dynamics with high temporal resolution, making them ideal for motion segmentation tasks. However, obtaining ground truth for training event-based networks is challenging, expensive, and error-prone.

Method: The authors propose EV-LayerSegNet, a self-supervised convolutional neural network that learns affine optical flow and segmentation masks separately. It uses these elements to deblur input events, with deblurring quality measured as a self-supervised learning loss.

Result: The network was trained and tested on a simulated dataset with affine motion, reaching an Intersection over Union (IoU) of up to 71% and a detection rate of up to 87%.

Conclusion: EV-LayerSegNet demonstrates that self-supervised learning methods can effectively segment motion events through layered representations and achieve high-quality results without relying on expensive ground truth data.

Abstract: Event cameras are novel bio-inspired sensors that capture motion dynamics
with much higher temporal resolution than traditional cameras, since pixels
react asynchronously to brightness changes. They are therefore better suited
for tasks involving motion such as motion segmentation. However, training
event-based networks still represents a difficult challenge, as obtaining
ground truth is very expensive, error-prone and limited in frequency. In this
article, we introduce EV-LayerSegNet, a self-supervised CNN for event-based
motion segmentation. Inspired by a layered representation of the scene
dynamics, we show that it is possible to learn affine optical flow and
segmentation masks separately, and use them to deblur the input events. The
deblurring quality is then measured and used as self-supervised learning loss.
We train and test the network on a simulated dataset with only affine motion,
achieving IoU and detection rate up to 71% and 87% respectively.

</details>


### [232] [RARL: Improving Medical VLM Reasoning and Generalization with Reinforcement Learning and LoRA under Data and Hardware Constraints](https://arxiv.org/abs/2506.06600)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: RARL enhances reasoning and diagnostic accuracy in medical vision-language models through reinforcement learning and efficient tuning.


<details>
  <summary>Details</summary>
Motivation: Address limitations in generalization, transparency, and computational efficiency of medical VLMs to enable deployment in resource-constrained settings.

Method: RARL fine-tunes a lightweight Qwen2-VL-2B-Instruct model using Low-Rank Adaptation and custom reward functions considering diagnostic accuracy and reasoning quality.

Result: RARL improves reasoning-focused task performance by 7.78%, unseen dataset generalization by 27%, and traditional RL fine-tuning by 4%, using limited computational resources.

Conclusion: Reasoning-guided learning and prompting enhance VLMs' transparency, accuracy, and efficiency for clinical decision-making, making them suitable for constrained environments.

Abstract: The growing integration of vision-language models (VLMs) in medical
applications offers promising support for diagnostic reasoning. However,
current medical VLMs often face limitations in generalization, transparency,
and computational efficiency-barriers that hinder deployment in real-world,
resource-constrained settings. To address these challenges, we propose a
Reasoning-Aware Reinforcement Learning framework, \textbf{RARL}, that enhances
the reasoning capabilities of medical VLMs while remaining efficient and
adaptable to low-resource environments. Our approach fine-tunes a lightweight
base model, Qwen2-VL-2B-Instruct, using Low-Rank Adaptation and custom reward
functions that jointly consider diagnostic accuracy and reasoning quality.
Training is performed on a single NVIDIA A100-PCIE-40GB GPU, demonstrating the
feasibility of deploying such models in constrained environments. We evaluate
the model using an LLM-as-judge framework that scores both correctness and
explanation quality. Experimental results show that RARL significantly improves
VLM performance in medical image analysis and clinical reasoning, outperforming
supervised fine-tuning on reasoning-focused tasks by approximately 7.78%, while
requiring fewer computational resources. Additionally, we demonstrate the
generalization capabilities of our approach on unseen datasets, achieving
around 27% improved performance compared to supervised fine-tuning and about 4%
over traditional RL fine-tuning. Our experiments also illustrate that diversity
prompting during training and reasoning prompting during inference are crucial
for enhancing VLM performance. Our findings highlight the potential of
reasoning-guided learning and reasoning prompting to steer medical VLMs toward
more transparent, accurate, and resource-efficient clinical decision-making.
Code and data are publicly available.

</details>


### [233] [Zero Shot Composed Image Retrieval](https://arxiv.org/abs/2506.06602)
*Santhosh Kakarla,Gautama Shastry Bulusu Venkata*

Main category: cs.CV

TL;DR: This paper proposes improved methods for composed image retrieval (CIR), achieving higher Recall rates by fine-tuning BLIP-2 with effective multimodal fusion while identifying failures in alternative methods like Retrieval-DPO.


<details>
  <summary>Details</summary>
Motivation: The work aims to enhance CIR performance, particularly in zero-shot scenarios, as current methods achieve low Recall@10 rates on benchmarks like FashionIQ.

Method: The authors fine-tune BLIP-2 using a lightweight Q-Former for joint image-text multimodal embeddings and analyze Retrieval-DPO's inefficiencies.

Result: Their fine-tuned BLIP-2 achieved Recall@10 rates of 45.6%, 40.1%, and 50.4% on different fashion categories, outperforming existing methods.

Conclusion: Multimodal fusion, ranking-aware losses, and curated negatives are key for effective CIR, as highlighted by the superior performance of their BLIP-2 approach and the failures of Retrieval-DPO.

Abstract: Composed image retrieval (CIR) allows a user to locate a target image by
applying a fine-grained textual edit (e.g., ``turn the dress blue'' or ``remove
stripes'') to a reference image. Zero-shot CIR, which embeds the image and the
text with separate pretrained vision-language encoders, reaches only 20-25\%
Recall@10 on the FashionIQ benchmark. We improve this by fine-tuning BLIP-2
with a lightweight Q-Former that fuses visual and textual features into a
single embedding, raising Recall@10 to 45.6\% (shirt), 40.1\% (dress), and
50.4\% (top-tee) and increasing the average Recall@50 to 67.6\%. We also
examine Retrieval-DPO, which fine-tunes CLIP's text encoder with a Direct
Preference Optimization loss applied to FAISS-mined hard negatives. Despite
extensive tuning of the scaling factor, index, and sampling strategy,
Retrieval-DPO attains only 0.02\% Recall@10 -- far below zero-shot and
prompt-tuned baselines -- because it (i) lacks joint image-text fusion, (ii)
uses a margin objective misaligned with top-$K$ metrics, (iii) relies on
low-quality negatives, and (iv) keeps the vision and Transformer layers frozen.
Our results show that effective preference-based CIR requires genuine
multimodal fusion, ranking-aware objectives, and carefully curated negatives.

</details>


### [234] [PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of Physics Experiments](https://arxiv.org/abs/2506.06631)
*Minghao Zou,Qingtian Zeng,Yongping Miao,Shangkun Liu,Zilong Wang,Hantao Liu,Wei Zhou*

Main category: cs.CV

TL;DR: PhysLab is a novel video dataset designed for educational scenarios, specifically physics experiments, providing multilevel annotations for various computer vision tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current datasets, such as insufficient annotation detail, domain coverage, and procedural guidance, particularly for educational contexts.

Method: Created PhysLab, a dataset consisting of 620 videos capturing physics experiments with diverse annotations for tasks like object detection and action recognition.

Result: PhysLab highlights challenges in procedural video parsing and establishes baselines for educational video analysis.

Conclusion: PhysLab aims to advance fine-grained visual parsing, integrate computer vision with education, and foster the development of intelligent classroom systems.

Abstract: Visual parsing of images and videos is critical for a wide range of
real-world applications. However, progress in this field is constrained by
limitations of existing datasets: (1) insufficient annotation granularity,
which impedes fine-grained scene understanding and high-level reasoning; (2)
limited coverage of domains, particularly a lack of datasets tailored for
educational scenarios; and (3) lack of explicit procedural guidance, with
minimal logical rules and insufficient representation of structured task
process. To address these gaps, we introduce PhysLab, the first video dataset
that captures students conducting complex physics experiments. The dataset
includes four representative experiments that feature diverse scientific
instruments and rich human-object interaction (HOI) patterns. PhysLab comprises
620 long-form videos and provides multilevel annotations that support a variety
of vision tasks, including action recognition, object detection, HOI analysis,
etc. We establish strong baselines and perform extensive evaluations to
highlight key challenges in the parsing of procedural educational videos. We
expect PhysLab to serve as a valuable resource for advancing fine-grained
visual parsing, facilitating intelligent classroom systems, and fostering
closer integration between computer vision and educational technologies. The
dataset and the evaluation toolkit are publicly available at
https://github.com/ZMH-SDUST/PhysLab.

</details>


### [235] [Dark Channel-Assisted Depth-from-Defocus from a Single Image](https://arxiv.org/abs/2506.06643)
*Moushumi Medhi,Rajiv Ranjan Sahay*

Main category: cs.CV

TL;DR: This paper proposes a method to estimate depth from a single defocused image using the dark channel prior, achieving effective results.


<details>
  <summary>Details</summary>
Motivation: Existing depth-from-defocus (DFD) techniques depend on multiple images with different settings, making single-image approaches challenging due to underconstrained problems.

Method: The paper exploits the dark channel as a cue to capture image statistics and scene structure, training the entire pipeline in an adversarial, end-to-end manner.

Result: Experiments show meaningful depth estimation on real data with depth-induced defocus, highlighting the method's effectiveness.

Conclusion: Incorporating the dark channel into single-image DFD is a promising approach for robust scene depth estimation.

Abstract: In this paper, we utilize the dark channel as a complementary cue to estimate
the depth of a scene from a single space-variant defocus blurred image due to
its effectiveness in implicitly capturing the local statistics of blurred
images and the scene structure. Existing depth-from-defocus (DFD) techniques
typically rely on multiple images with varying apertures or focus settings to
recover depth information. Very few attempts have focused on DFD from a single
defocused image due to the underconstrained nature of the problem. Our method
capitalizes on the relationship between local defocus blur and contrast
variations as key depth cues to enhance the overall performance in estimating
the scene's structure. The entire pipeline is trained adversarially in a fully
end-to-end fashion. Experiments conducted on real data with realistic
depth-induced defocus blur demonstrate that incorporating dark channel prior
into single image DFD yields meaningful depth estimation results, validating
the effectiveness of our approach.

</details>


### [236] [Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling](https://arxiv.org/abs/2506.06645)
*Cheng Peng,Jingxiang Sun,Yushuo Chen,Zhaoqi Su,Zhuo Su,Yebin Liu*

Main category: cs.CV

TL;DR: PGHM is a new method integrating human priors with 3D Gaussian Splatting for fast avatar reconstruction from monocular videos.


<details>
  <summary>Details</summary>
Motivation: Current avatar reconstruction approaches struggle with inefficiency and poor generalization under sparse inputs.

Method: PGHM combines UV-aligned latent identity maps and Multi-Head U-Net with disentangled component prediction for efficient reconstruction.

Result: PGHM can create high-quality avatars in 20 minutes per subject and performs well under challenging conditions.

Conclusion: PGHM offers a practical and efficient solution for real-world monocular avatar creation.

Abstract: Photorealistic and animatable human avatars are a key enabler for
virtual/augmented reality, telepresence, and digital entertainment. While
recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering
quality and efficiency, existing methods still face fundamental challenges,
including time-consuming per-subject optimization and poor generalization under
sparse monocular inputs. In this work, we present the Parametric Gaussian Human
Model (PGHM), a generalizable and efficient framework that integrates human
priors into 3DGS for fast and high-fidelity avatar reconstruction from
monocular videos. PGHM introduces two core components: (1) a UV-aligned latent
identity map that compactly encodes subject-specific geometry and appearance
into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that
predicts Gaussian attributes by decomposing static, pose-dependent, and
view-dependent components via conditioned decoders. This design enables robust
rendering quality under challenging poses and viewpoints, while allowing
efficient subject adaptation without requiring multi-view capture or long
optimization time. Experiments show that PGHM is significantly more efficient
than optimization-from-scratch methods, requiring only approximately 20 minutes
per subject to produce avatars with comparable visual quality, thereby
demonstrating its practical applicability for real-world monocular avatar
creation.

</details>


### [237] [Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery](https://arxiv.org/abs/2506.06667)
*Yu-Hsuan Ho,Ali Mostafavi*

Main category: cs.CV

TL;DR: The paper introduces Flood-DamageSense, a deep-learning model for flood-damage assessment using multimodal data, achieving a notable improvement in building damage classification compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing post-disaster damage classifiers struggle to accurately classify flood-related building damages due to the lack of clear spectral or structural signatures following inundation. A solution is needed to address this limitation.

Method: The study develops Flood-DamageSense, a deep-learning framework utilizing pre- and post-event SAR/InSAR data, optical basemaps, and flood-risk layers. The model employs a multimodal Mamba backbone with a semi-Siamese encoder and task-specific decoders to predict building damage states, flood extent, and building footprints.

Result: Testing on Hurricane Harvey data from Harris County, the model shows an F1 improvement of up to 19 percentage points over state-of-the-art methods, particularly excelling in 'minor' and 'moderate' damage categories. The flood-risk feature is identified as the primary performance enhancer.

Conclusion: Flood-DamageSense offers a risk-aware, faster, and more granular approach to flood-damage assessment, providing actionable intelligence for post-disaster response and resource allocation.

Abstract: Most post-disaster damage classifiers succeed only when destructive forces
leave clear spectral or structural signatures -- conditions rarely present
after inundation. Consequently, existing models perform poorly at identifying
flood-related building damages. The model presented in this study,
Flood-DamageSense, addresses this gap as the first deep-learning framework
purpose-built for building-level flood-damage assessment. The architecture
fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical
basemaps and an inherent flood-risk layer that encodes long-term exposure
probabilities, guiding the network toward plausibly affected structures even
when compositional change is minimal. A multimodal Mamba backbone with a
semi-Siamese encoder and task-specific decoders jointly predicts (1) graded
building-damage states, (2) floodwater extent, and (3) building footprints.
Training and evaluation on Hurricane Harvey (2017) imagery from Harris County,
Texas -- supported by insurance-derived property-damage extents -- show a mean
F1 improvement of up to 19 percentage points over state-of-the-art baselines,
with the largest gains in the frequently misclassified "minor" and "moderate"
damage categories. Ablation studies identify the inherent-risk feature as the
single most significant contributor to this performance boost. An end-to-end
post-processing pipeline converts pixel-level outputs to actionable,
building-scale damage maps within minutes of image acquisition. By combining
risk-aware modeling with SAR's all-weather capability, Flood-DamageSense
delivers faster, finer-grained, and more reliable flood-damage intelligence to
support post-disaster decision-making and resource allocation.

</details>


### [238] [Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment](https://arxiv.org/abs/2506.06680)
*Radha Kodali,Venkata Rao Dhulipalla,Venkata Siva Kishor Tatavarty,Madhavi Nadakuditi,Bharadwaj Thiruveedhula,Suryanarayana Gunnam,Durga Prasad Bavirisetti*

Main category: cs.CV

TL;DR: This study proposes an explainable AI model combining CNN and LSTM for effective embryo classification to aid in IVF processes.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiency and time-consuming nature of conventional manual blastocyst grading processes in IVF treatment for infertility.

Method: A CNN-LSTM model is introduced for embryo classification, leveraging deep learning for high accuracy and explainability.

Result: The proposed CNN-LSTM model demonstrates high accuracy in classifying embryos, supported by explainability through XAI.

Conclusion: This AI framework has the potential to significantly improve efficiency and accuracy in embryo grading, supporting advancements in fertility treatments.

Abstract: Infertility has a considerable impact on individuals' quality of life,
affecting them socially and psychologically, with projections indicating a rise
in the upcoming years. In vitro fertilization (IVF) emerges as one of the
primary techniques within economically developed nations, employed to address
the rising problem of low fertility. Expert embryologists conventionally grade
embryos by reviewing blastocyst images to select the most optimal for transfer,
yet this process is time-consuming and lacks efficiency. Blastocyst images
provide a valuable resource for assessing embryo viability. In this study, we
introduce an explainable artificial intelligence (XAI) framework for
classifying embryos, employing a fusion of convolutional neural network (CNN)
and long short-term memory (LSTM) architecture, referred to as CNN-LSTM.
Utilizing deep learning, our model achieves high accuracy in embryo
classification while maintaining interpretability through XAI.

</details>


### [239] [A Systematic Investigation on Deep Learning-Based Omnidirectional Image and Video Super-Resolution](https://arxiv.org/abs/2506.06710)
*Qianqian Zhao,Chunle Guo,Tianyi Zhang,Junpei Zhang,Peiyang Jia,Tan Su,Wenjie Jiang,Chongyi Li*

Main category: cs.CV

TL;DR: Omnidirectional image/video super-resolution enhances detail using deep learning. New dataset, 360Insta, improves real-world evaluations.


<details>
  <summary>Details</summary>
Motivation: Current omnidirectional datasets primarily rely on synthetic degradation and overlook real-world distortions.

Method: This study reviews deep learning-based omnidirectional super-resolution methods and introduces a realistic dataset called 360Insta.

Result: 360Insta enables better assessment of omnidirectional super-resolution methods with authentic conditions like lighting and motion.

Conclusion: 360Insta fills a critical research gap, ensuring robust evaluations and fostering future deep learning advancements in omnidirectional super-resolution.

Abstract: Omnidirectional image and video super-resolution is a crucial research topic
in low-level vision, playing an essential role in virtual reality and augmented
reality applications. Its goal is to reconstruct high-resolution images or
video frames from low-resolution inputs, thereby enhancing detail preservation
and enabling more accurate scene analysis and interpretation. In recent years,
numerous innovative and effective approaches have been proposed, predominantly
based on deep learning techniques, involving diverse network architectures,
loss functions, projection strategies, and training datasets. This paper
presents a systematic review of recent progress in omnidirectional image and
video super-resolution, focusing on deep learning-based methods. Given that
existing datasets predominantly rely on synthetic degradation and fall short in
capturing real-world distortions, we introduce a new dataset, 360Insta, that
comprises authentically degraded omnidirectional images and videos collected
under diverse conditions, including varying lighting, motion, and exposure
settings. This dataset addresses a critical gap in current omnidirectional
benchmarks and enables more robust evaluation of the generalization
capabilities of omnidirectional super-resolution methods. We conduct
comprehensive qualitative and quantitative evaluations of existing methods on
both public datasets and our proposed dataset. Furthermore, we provide a
systematic overview of the current status of research and discuss promising
directions for future exploration. All datasets, methods, and evaluation
metrics introduced in this work are publicly available and will be regularly
updated. Project page: https://github.com/nqian1/Survey-on-ODISR-and-ODVSR.

</details>


### [240] [Active Contour Models Driven by Hyperbolic Mean Curvature Flow for Image Segmentation](https://arxiv.org/abs/2506.06712)
*Saiyu Hu,Chunlei He,Jianfeng Zhang,Dexing Kong,Shoujun Huang*

Main category: cs.CV

TL;DR: The paper introduces hyperbolic mean curvature flow-driven active contour models (HMCF-ACMs) for image segmentation, addressing initial configuration dependency issues and achieving better segmentation with improved noise resistance and stability.


<details>
  <summary>Details</summary>
Motivation: Current parabolic mean curvature flow models heavily rely on initial curve configurations for segmentation success, limiting adaptability to diverse scenarios.

Method: The authors propose hyperbolic flow-driven models incorporating tunable initial velocity fields, dual-mode regularization, and Heaviside edge modulation. They use a fourth-order Runge-Kutta algorithm with nine-point stencil for solving related wave equations.

Result: HMCF-ACMs and HDRF-ACMs demonstrated precise segmentation, better resistance to noise, and improved numerical stability across various tasks.

Conclusion: The introduced models enable adaptive and precise image segmentation for diverse scenarios, surpassing the limitations of standard parabolic models.

Abstract: Parabolic mean curvature flow-driven active contour models (PMCF-ACMs) are
widely used in image segmentation, which however depend heavily on the
selection of initial curve configurations. In this paper, we firstly propose
several hyperbolic mean curvature flow-driven ACMs (HMCF-ACMs), which introduce
tunable initial velocity fields, enabling adaptive optimization for diverse
segmentation scenarios. We shall prove that HMCF-ACMs are indeed normal flows
and establish the numerical equivalence between dissipative HMCF formulations
and certain wave equations using the level set method with signed distance
function. Building on this framework, we furthermore develop hyperbolic
dual-mode regularized flow-driven ACMs (HDRF-ACMs), which utilize smooth
Heaviside functions for edge-aware force modulation to suppress over-diffusion
near weak boundaries. Then, we optimize a weighted fourth-order Runge-Kutta
algorithm with nine-point stencil spatial discretization when solving the
above-mentioned wave equations. Experiments show that both HMCF-ACMs and
HDRF-ACMs could achieve more precise segmentations with superior noise
resistance and numerical stability due to task-adaptive configurations of
initial velocities and initial contours.

</details>


### [241] [Improving Wildlife Out-of-Distribution Detection: Africas Big Five](https://arxiv.org/abs/2506.06719)
*Mufhumudzi Muthivhi,Jiahao Huo,Fredrik Gustafsson,Terence L. van Zyl*

Main category: cs.CV

TL;DR: This study investigates out-of-distribution (OOD) detection of wildlife focusing on the Big Five African animals. It employs feature-based parametric and non-parametric methods and compares them against existing approaches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the problem of human-wildlife conflict using computer vision models to correctly detect wildlife species, especially under scenarios where many species go unrecognized by traditional classification models.

Method: For OOD detection, the study utilizes Nearest Class Mean (NCM) and contrastive learning approaches that utilize pretrained and projected features. It compares these methods with standard OOD techniques found in scientific literature.

Result: The proposed feature-based methods, particularly NCM with ImageNet pre-trained features, demonstrate superior generalization capability, yielding significant improvements on AUPR-IN (2%), AUPR-OUT (4%), and AUTC (22%) compared to the best-existing OOD approaches.

Conclusion: Leveraging feature-based approaches for wildlife detection offers notable advancements in recognizing out-of-distribution classes, aiding more reliable and robust computer vision solutions in ecological settings like human-wildlife conflict scenarios.

Abstract: Mitigating human-wildlife conflict seeks to resolve unwanted encounters
between these parties. Computer Vision provides a solution to identifying
individuals that might escalate into conflict, such as members of the Big Five
African animals. However, environments often contain several varied species.
The current state-of-the-art animal classification models are trained under a
closed-world assumption. They almost always remain overconfident in their
predictions even when presented with unknown classes. This study investigates
out-of-distribution (OOD) detection of wildlife, specifically the Big Five. To
this end, we select a parametric Nearest Class Mean (NCM) and a non-parametric
contrastive learning approach as baselines to take advantage of pretrained and
projected features from popular classification encoders. Moreover, we compare
our baselines to various common OOD methods in the literature. The results show
feature-based methods reflect stronger generalisation capability across varying
classification thresholds. Specifically, NCM with ImageNet pre-trained features
achieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the
best OOD methods, respectively. The code can be found here
https://github.com/pxpana/BIG5OOD

</details>


### [242] [Mitigating Object Hallucination via Robust Local Perception Search](https://arxiv.org/abs/2506.06729)
*Zixian Gao,Chao Yang,Zhanhui Zhou,Xing Xu,Chaochao Lu*

Main category: cs.CV

TL;DR: The paper introduces Local Perception Search (LPS), a simple and training-free decoding method to reduce hallucination in Multimodal Large Language Models (MLLMs), especially in noisy scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the hallucination issue in MLLMs, where model outputs appear plausible but do not match the image content.

Method: Proposed Local Perception Search (LPS), a decoding method using local visual prior information as a value function during inference to suppress hallucinations without additional training.

Result: LPS significantly reduces hallucinations in MLLMs, performing exceptionally well in noisy image setups and surpassing baselines in benchmarks.

Conclusion: LPS demonstrates a robust and plug-and-play approach to lessen hallucinations in MLLMs, making it useful across various noisy and standard settings.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled
them to effectively integrate vision and language, addressing a variety of
downstream tasks. However, despite their significant success, these models
still exhibit hallucination phenomena, where the outputs appear plausible but
do not align with the content of the images. To mitigate this issue, we
introduce Local Perception Search (LPS), a decoding method during inference
that is both simple and training-free, yet effectively suppresses
hallucinations. This method leverages local visual prior information as a value
function to correct the decoding process. Additionally, we observe that the
impact of the local visual prior on model performance is more pronounced in
scenarios with high levels of image noise. Notably, LPS is a plug-and-play
approach that is compatible with various models. Extensive experiments on
widely used hallucination benchmarks and noisy data demonstrate that LPS
significantly reduces the incidence of hallucinations compared to the baseline,
showing exceptional performance, particularly in noisy settings.

</details>


### [243] [RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation](https://arxiv.org/abs/2506.06733)
*Ruoxuan Zhang,Jidong Gao,Bin Wen,Hongxia Xie,Chenming Zhang,Honghan-shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: The paper introduces RecipeGen, a new large-scale dataset aimed at improving recipe-based image and video generation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack fine-grained alignment between recipe steps, objectives, and their visual representations, limiting advancements in food-related AI applications.

Method: The authors created RecipeGen, a dataset with 26,453 recipes, 196,724 images, and 4,491 videos, and proposed domain-specific metrics to evaluate model performance.

Result: RecipeGen serves as a benchmark for Text-to-Image, Image-to-Video, and Text-to-Video generation models, offering diverse data and measurable evaluation criteria.

Conclusion: RecipeGen addresses gaps in current datasets and aims to advance both research and practical applications in recipe-based visual content generation.

Abstract: Creating recipe images is a key challenge in food computing, with
applications in culinary education and multimodal recipe assistants. However,
existing datasets lack fine-grained alignment between recipe goals, step-wise
instructions, and visual content. We present RecipeGen, the first large-scale,
real-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video
(I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes,
196,724 images, and 4,491 videos, covering diverse ingredients, cooking
procedures, styles, and dish types. We further propose domain-specific
evaluation metrics to assess ingredient fidelity and interaction modeling,
benchmark representative T2I, I2V, and T2V models, and provide insights for
future recipe generation models. Project page is available now.

</details>


### [244] [THU-Warwick Submission for EPIC-KITCHEN Challenge 2025: Semi-Supervised Video Object Segmentation](https://arxiv.org/abs/2506.06748)
*Mingqi Gao,Haoran Duan,Tianlu Zhang,Jungong Han*

Main category: cs.CV

TL;DR: This paper introduces a method for egocentric video object segmentation combining visual pretraining and depth-based geometric cues.


<details>
  <summary>Details</summary>
Motivation: The goal is to handle complex scenes and enable reliable long-term tracking for improved segmentation performance.

Method: The approach integrates large-scale visual pretraining from SAM2 and depth-based geometric cues into a unified framework.

Result: Achieved a J&F score of 90.1% on the VISOR test set, indicating strong segmentation performance.

Conclusion: The integration of visual pretraining and geometric depth cues significantly enhances egocentric video object segmentation.

Abstract: In this report, we describe our approach to egocentric video object
segmentation. Our method combines large-scale visual pretraining from SAM2 with
depth-based geometric cues to handle complex scenes and long-term tracking. By
integrating these signals in a unified framework, we achieve strong
segmentation performance. On the VISOR test set, our method reaches a J&F score
of 90.1%.

</details>


### [245] [SAR2Struct: Extracting 3D Semantic Structural Representation of Aircraft Targets from Single-View SAR Image](https://arxiv.org/abs/2506.06757)
*Ziyu Yue,Ruixi You,Feng Xu*

Main category: cs.CV

TL;DR: The paper introduces a novel task to recover structural information and semantics from single-view SAR images, focusing on symmetry and adjacency relationships among target components.


<details>
  <summary>Details</summary>
Motivation: Current SAR image interpretation is limited, focusing mainly on 3D surface reconstruction or local geometric features, without effectively capturing semantic information or structural relationships.

Method: A two-step algorithmic framework is developed: extracting 2D keypoints from SAR images in the training phase and mapping these to 3D hierarchical structures using simulated data. This process is integrated during testing to infer 3D structures.

Result: The experimental results validate the effectiveness of the proposed approach, demonstrating—for the first time—the ability to derive 3D semantic structural representations of aircraft targets from single-view SAR images.

Conclusion: This study proposes a new method for extracting semantic and structural information from SAR images, advancing the capability to derive 3D structures and enabling better human interpretability of SAR data.

Abstract: To translate synthetic aperture radar (SAR) image into interpretable forms
for human understanding is the ultimate goal of SAR advanced information
retrieval. Existing methods mainly focus on 3D surface reconstruction or local
geometric feature extraction of targets, neglecting the role of structural
modeling in capturing semantic information. This paper proposes a novel task:
SAR target structure recovery, which aims to infer the components of a target
and the structural relationships between its components, specifically symmetry
and adjacency, from a single-view SAR image. Through learning the structural
consistency and geometric diversity across the same type of targets as observed
in different SAR images, it aims to derive the semantic representation of
target directly from its 2D SAR image. To solve this challenging task, a
two-step algorithmic framework based on structural descriptors is developed.
Specifically, in the training phase, it first detects 2D keypoints from real
SAR images, and then learns the mapping from these keypoints to 3D hierarchical
structures using simulated data. During the testing phase, these two steps are
integrated to infer the 3D structure from real SAR images. Experimental results
validated the effectiveness of each step and demonstrated, for the first time,
that 3D semantic structural representation of aircraft targets can be directly
derived from a single-view SAR image.

</details>


### [246] [LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security](https://arxiv.org/abs/2506.06759)
*Nidheesh Gorthi,Kartik Thakral,Rishabh Ranjan,Richa Singh,Mayank Vatsa*

Main category: cs.CV

TL;DR: The paper introduces LitMAS, a lightweight and generalizable framework for detecting spoofing in multi-modal biometric systems, achieving superior performance with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing biometric systems, while increasingly used in critical applications, are vulnerable to spoofing. Current anti-spoofing methods focus on specific modalities, making a unified and resource-efficient solution for multiple modalities difficult to achieve.

Method: LitMAS incorporates a Modality-Aligned Concentration Loss to enhance inter-class separability and maintain consistency across modalities. This method enables robust spoof detection across speech, face, iris, and fingerprint-based systems with only 6M parameters.

Result: LitMAS outperforms state-of-the-art methods with an improvement of 1.36% in average Equal Error Rate (EER) across seven datasets, highlighting its efficiency and generalizability.

Conclusion: LitMAS is a lightweight, generalizable anti-spoofing framework suitable for edge deployment, offering robust and efficient protection across diverse biometric modalities.

Abstract: Biometric authentication systems are increasingly being deployed in critical
applications, but they remain susceptible to spoofing. Since most of the
research efforts focus on modality-specific anti-spoofing techniques, building
a unified, resource-efficient solution across multiple biometric modalities
remains a challenge. To address this, we propose LitMAS, a
$\textbf{Li}$gh$\textbf{t}$ weight and generalizable $\textbf{M}$ulti-modal
$\textbf{A}$nti-$\textbf{S}$poofing framework designed to detect spoofing
attacks in speech, face, iris, and fingerprint-based biometric systems. At the
core of LitMAS is a Modality-Aligned Concentration Loss, which enhances
inter-class separability while preserving cross-modal consistency and enabling
robust spoof detection across diverse biometric traits. With just 6M
parameters, LitMAS surpasses state-of-the-art methods by $1.36\%$ in average
EER across seven datasets, demonstrating high efficiency, strong
generalizability, and suitability for edge deployment. Code and trained models
are available at https://github.com/IAB-IITJ/LitMAS.

</details>


### [247] [LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization and Mapping](https://arxiv.org/abs/2506.06771)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Dorian Cojocaru,Sorin Grigorescu*

Main category: cs.CV

TL;DR: The paper presents LoopDB, a loop closure dataset comprising 1000+ images across varied environments for benchmarking and training purposes.


<details>
  <summary>Details</summary>
Motivation: To provide a challenging and diverse dataset for benchmarking and training loop closure algorithms used in simultaneous localization and mapping.

Method: Developed a dataset with over 1000 high-resolution images captured across varied environments, representing scenes as sequences of five consecutive images. Ground truth rotations and translations between consecutive images are included.

Result: Created LoopDB, a publicly available dataset, useful for accuracy benchmarking and training loop closure methods.

Conclusion: LoopDB serves as a valuable resource for advancing loop closure methods, including those based on deep neural networks.

Abstract: In this study, we introduce LoopDB, which is a challenging loop closure
dataset comprising over 1000 images captured across diverse environments,
including parks, indoor scenes, parking spaces, as well as centered around
individual objects. Each scene is represented by a sequence of five consecutive
images. The dataset was collected using a high resolution camera, providing
suitable imagery for benchmarking the accuracy of loop closure algorithms,
typically used in simultaneous localization and mapping. As ground truth
information, we provide computed rotations and translations between each
consecutive images. Additional to its benchmarking goal, the dataset can be
used to train and fine-tune loop closure methods based on deep neural networks.
LoopDB is publicly available at https://github.com/RovisLab/LoopDB.

</details>


### [248] [Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations](https://arxiv.org/abs/2506.06780)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: The paper addresses the challenge of tracking and forecasting object rotations by proposing a method using Neural Controlled Differential Equations combined with Savitzky-Golay paths, yielding promising results on real-world data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenges in SO(3) extrapolation, which include handling noisy sensor data, complex motion dynamics, and the demand for long-term forecasting in applications.

Method: The proposed method uses Neural Controlled Differential Equations guided by Savitzky-Golay paths to model continuous-time rotational object dynamics while preserving SO(3)'s geometric structure.

Result: The experimental results show that the method provides compelling and superior forecasting capabilities when compared to existing approaches using real-world data.

Conclusion: The proposed approach effectively learns latent dynamical systems for object trajectories on SO(3), showcasing potential in handling noisy, sparse observations and complex dynamics for long-term forecasting.

Abstract: Tracking and forecasting the rotation of objects is fundamental in computer
vision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor
observations can be noisy and sparse, (2) motion patterns can be governed by
complex dynamics, and (3) application settings can demand long-term
forecasting. This work proposes modeling continuous-time rotational object
dynamics on $SO(3)$ using Neural Controlled Differential Equations guided by
Savitzky-Golay paths. Unlike existing methods that rely on simplified motion
assumptions, our method learns a general latent dynamical system of the
underlying object trajectory while respecting the geometric structure of
rotations. Experimental results on real-world data demonstrate compelling
forecasting capabilities compared to existing approaches.

</details>


### [249] [Training-Free Identity Preservation in Stylized Image Generation Using Diffusion Models](https://arxiv.org/abs/2506.06802)
*Mohammad Ali Rezaei,Helia Hajikazem,Saeed Khanehgir,Mahdi Javanmardi*

Main category: cs.CV

TL;DR: Proposes a training-free framework using diffusion models for superior style transfer with identity preservation.


<details>
  <summary>Details</summary>
Motivation: Existing style transfer methods struggle with identity retention, particularly for small faces or significant camera-to-face distances.

Method: Introduces 'Mosaic Restored Content Image' for identity retention and a training-free content consistency loss for fine-grained content preservation.

Result: Achieves better style fidelity and identity integrity than baseline methods, even for challenging facial conditions, without retraining or fine-tuning.

Conclusion: The method offers significant improvements in stylized image synthesis by focusing on identity preservation and stylistic quality through training-free innovations.

Abstract: While diffusion models have demonstrated remarkable generative capabilities,
existing style transfer techniques often struggle to maintain identity while
achieving high-quality stylization. This limitation is particularly acute for
images where faces are small or exhibit significant camera-to-face distances,
frequently leading to inadequate identity preservation. To address this, we
introduce a novel, training-free framework for identity-preserved stylized
image synthesis using diffusion models. Key contributions include: (1) the
"Mosaic Restored Content Image" technique, significantly enhancing identity
retention, especially in complex scenes; and (2) a training-free content
consistency loss that enhances the preservation of fine-grained content details
by directing more attention to the original image during stylization. Our
experiments reveal that the proposed approach substantially surpasses the
baseline model in concurrently maintaining high stylistic fidelity and robust
identity integrity, particularly under conditions of small facial regions or
significant camera-to-face distances, all without necessitating model
retraining or fine-tuning.

</details>


### [250] [Stepwise Decomposition and Dual-stream Focus: A Novel Approach for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2506.06818)
*Chao Yin,Hao Li,Kequan Yang,Jide Li,Pinpin Zhu,Xiaoqiang Li*

Main category: cs.CV

TL;DR: RDVP-MSD improves camouflaged object segmentation by addressing semantic ambiguity and spatial discrepancy issues with training-free test-time adaptation.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in camouflaged object segmentation tasks caused by semantic ambiguity and spatial discrepancies in prompts.

Method: The paper introduces RDVP-MSD, a framework combining region-constrained dual-stream visual prompting (RDVP) and multimodal stepwise decomposition chain of thought (MSD-CoT).

Result: RDVP-MSD achieves state-of-the-art segmentation results on various benchmarks with faster inference speeds compared to previous methods.

Conclusion: RDVP-MSD enhances accuracy and efficiency in segmentation tasks without needing training or supervision.

Abstract: While promptable segmentation (\textit{e.g.}, SAM) has shown promise for
various segmentation tasks, it still requires manual visual prompts for each
object to be segmented. In contrast, task-generic promptable segmentation aims
to reduce the need for such detailed prompts by employing only a task-generic
prompt to guide segmentation across all test samples. However, when applied to
Camouflaged Object Segmentation (COS), current methods still face two critical
issues: 1) \textit{\textbf{semantic ambiguity in getting instance-specific text
prompts}}, which arises from insufficient discriminative cues in holistic
captions, leading to foreground-background confusion; 2)
\textit{\textbf{semantic discrepancy combined with spatial separation in
getting instance-specific visual prompts}}, which results from global
background sampling far from object boundaries with low feature correlation,
causing SAM to segment irrelevant regions. To address the issues above, we
propose \textbf{RDVP-MSD}, a novel training-free test-time adaptation framework
that synergizes \textbf{R}egion-constrained \textbf{D}ual-stream
\textbf{V}isual \textbf{P}rompting (RDVP) via \textbf{M}ultimodal
\textbf{S}tepwise \textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT
progressively disentangles image captions to eliminate semantic ambiguity,
while RDVP injects spatial constraints into visual prompting and independently
samples visual prompts for foreground and background points, effectively
mitigating semantic discrepancy and spatial separation. Without requiring any
training or supervision, RDVP-MSD achieves a state-of-the-art segmentation
result on multiple COS benchmarks and delivers a faster inference speed than
previous methods, demonstrating significantly improved accuracy and efficiency.
The codes will be available at
\href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}

</details>


### [251] [Hi-LSplat: Hierarchical 3D Language Gaussian Splatting](https://arxiv.org/abs/2506.06822)
*Chenlu Zhan,Yufei Zhang,Gaoang Wang,Hongwei Wang*

Main category: cs.CV

TL;DR: Hi-LSplat proposes a method to resolve inconsistencies in 3D language fields by introducing view-consistent hierarchical semantic representations using Gaussian Splatting.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in modeling 3D language fields have encountered challenges such as view inconsistencies and difficulties in handling object and relational descriptions.

Method: The paper introduces Hi-LSplat, which uses a 3D hierarchical semantic tree, instance-wise and part-wise contrastive losses, and evaluates through two hierarchical semantic datasets.

Result: Hi-LSplat exhibits superior performance in 3D open-vocabulary segmentation and localization and effectively captures hierarchical semantics.

Conclusion: Hi-LSplat offers a promising solution to refine 3D language fields by resolving view-dependent inconsistency and enhancing hierarchical semantic understanding in 3D queries.

Abstract: Modeling 3D language fields with Gaussian Splatting for open-ended language
queries has recently garnered increasing attention. However, recent 3DGS-based
models leverage view-dependent 2D foundation models to refine 3D semantics but
lack a unified 3D representation, leading to view inconsistencies.
Additionally, inherent open-vocabulary challenges cause inconsistencies in
object and relational descriptions, impeding hierarchical semantic
understanding. In this paper, we propose Hi-LSplat, a view-consistent
Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.
To achieve view-consistent 3D hierarchical semantics, we first lift 2D features
to 3D features by constructing a 3D hierarchical semantic tree with layered
instance clustering, which addresses the view inconsistency issue caused by 2D
semantic features. Besides, we introduce instance-wise and part-wise
contrastive losses to capture all-sided hierarchical semantic representations.
Notably, we construct two hierarchical semantic datasets to better assess the
model's ability to distinguish different semantic levels. Extensive experiments
highlight our method's superiority in 3D open-vocabulary segmentation and
localization. Its strong performance on hierarchical semantic datasets
underscores its ability to capture complex hierarchical semantics within 3D
scenes.

</details>


### [252] [Exploring Visual Prompting: Robustness Inheritance and Beyond](https://arxiv.org/abs/2506.06823)
*Qi Li,Liangzhi Li,Zhouqiang Jiang,Bowen Wang,Keke Tang*

Main category: cs.CV

TL;DR: The paper investigates how Visual Prompting (VP) performs using robust source models. It identifies trade-offs and proposes a lightweight strategy PBL to enhance robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To understand how Visual Prompting, typically used with standard source models, performs with robust source models and addresses potential trade-offs.

Method: The authors explore robustness inheritance and trade-offs in VP and propose Prompt Boundary Loosening (PBL), a plug-and-play strategy compatible with VP.

Result: PBL successfully inherits robustness from robust source models while improving the generalization ability of VP across diverse datasets.

Conclusion: The findings affirm that VP inherits robustness and encounters trade-offs. PBL mitigates these limitations, demonstrating universal benefits and compatibility with VP across datasets.

Abstract: Visual Prompting (VP), an efficient method for transfer learning, has shown
its potential in vision tasks. However, previous works focus exclusively on VP
from standard source models, it is still unknown how it performs under the
scenario of a robust source model: Can the robustness of the source model be
successfully inherited? Does VP also encounter the same trade-off between
robustness and generalization ability as the source model during this process?
If such a trade-off exists, is there a strategy specifically tailored to VP to
mitigate this limitation? In this paper, we thoroughly explore these three
questions for the first time and provide affirmative answers to them. To
mitigate the trade-off faced by VP, we propose a strategy called Prompt
Boundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally
compatible with VP, PBL effectively ensures the successful inheritance of
robustness when the source model is a robust model, while significantly
enhancing VP's generalization ability across various downstream datasets.
Extensive experiments across various datasets show that our findings are
universal and demonstrate the significant benefits of the proposed strategy.

</details>


### [253] [Controllable Coupled Image Generation via Diffusion Models](https://arxiv.org/abs/2506.06826)
*Chenfei Yuan,Nanshan Jia,Hangqi Li,Peter W. Glynn,Zeyu Zheng*

Main category: cs.CV

TL;DR: The paper introduces a technique for controlling attention in coupled image generation, ensuring consistent backgrounds while maintaining flexibility in objects guided by text prompts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating multiple images with coupled backgrounds while retaining flexibility for objects influenced by text descriptions.

Method: The method disentangles background and entity components in cross-attention modules using time-varying weight control parameters optimized through a combined objective for background coupling, text-to-image alignment, and visual quality.

Result: Empirical studies show that the proposed method surpasses existing techniques in generating coupled images with consistent backgrounds and high-quality text-to-object alignment.

Conclusion: The approach effectively balances background coupling consistency and text-guided object adaptability, outperforming current methods in visual quality and alignment.

Abstract: We provide an attention-level control method for the task of coupled image
generation, where "coupled" means that multiple simultaneously generated images
are expected to have the same or very similar backgrounds. While backgrounds
coupled, the centered objects in the generated images are still expected to
enjoy the flexibility raised from different text prompts. The proposed method
disentangles the background and entity components in the model's
cross-attention modules, attached with a sequence of time-varying weight
control parameters depending on the time step of sampling. We optimize this
sequence of weight control parameters with a combined objective that assesses
how coupled the backgrounds are as well as text-to-image alignment and overall
visual quality. Empirical results demonstrate that our method outperforms
existing approaches across these criteria.

</details>


### [254] [EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery](https://arxiv.org/abs/2506.06830)
*Guankun Wang,Rui Tang,Mengya Xu,Long Bai,Huxin Gao,Hongliang Ren*

Main category: cs.CV

TL;DR: This paper introduces EndoARSS, a multi-task learning framework for activity recognition and semantic segmentation in endoscopic surgery, demonstrating superior accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Challenges in endoscopic surgery understanding arise from high scene complexity, cross-activity interference, and limitations of traditional deep learning models, necessitating an advanced methodology.

Method: The authors propose EndoARSS, leveraging the DINOv2 foundation model, low-rank adaptation, task-efficient shared low-rank adapters, and a spatially-aware multi-scale attention mechanism to enhance task performance and feature discrimination.

Result: EndoARSS achieves superior performance across newly introduced datasets (MTLESD, MTLEndovis, MTLEndovis-Gen), showcasing significant improvements in surgical activity recognition and semantic segmentation benchmarks.

Conclusion: EndoARSS enhances the understanding and efficiency of AI-driven endoscopic surgeries, addressing prior limitations, and offers a promising step towards safer and more effective surgical systems.

Abstract: Endoscopic surgery is the gold standard for robotic-assisted minimally
invasive surgery, offering significant advantages in early disease detection
and precise interventions. However, the complexity of surgical scenes,
characterized by high variability in different surgical activity scenarios and
confused image features between targets and the background, presents challenges
for surgical environment understanding. Traditional deep learning models often
struggle with cross-activity interference, leading to suboptimal performance in
each downstream task. To address this limitation, we explore multi-task
learning, which utilizes the interrelated features between tasks to enhance
overall task performance. In this paper, we propose EndoARSS, a novel
multi-task learning framework specifically designed for endoscopy surgery
activity recognition and semantic segmentation. Built upon the DINOv2
foundation model, our approach integrates Low-Rank Adaptation to facilitate
efficient fine-tuning while incorporating Task Efficient Shared Low-Rank
Adapters to mitigate gradient conflicts across diverse tasks. Additionally, we
introduce the Spatially-Aware Multi-Scale Attention that enhances feature
representation discrimination by enabling cross-spatial learning of global
information. In order to evaluate the effectiveness of our framework, we
present three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored
for endoscopic surgery scenarios with detailed annotations for both activity
recognition and semantic segmentation tasks. Extensive experiments demonstrate
that EndoARSS achieves remarkable performance across multiple benchmarks,
significantly improving both accuracy and robustness in comparison to existing
models. These results underscore the potential of EndoARSS to advance AI-driven
endoscopic surgical systems, offering valuable insights for enhancing surgical
safety and efficiency.

</details>


### [255] [Harnessing Vision-Language Models for Time Series Anomaly Detection](https://arxiv.org/abs/2506.06836)
*Zelin He,Sarah Alnegheimish,Matthew Reimherr*

Main category: cs.CV

TL;DR: The paper introduces a new method for time-series anomaly detection (TSAD) combining vision language models (VLMs) with a two-stage approach, achieving both higher accuracy and efficiency than prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing TSAD methods heavily rely on domain-specific models and numerical data, lacking visual-temporal reasoning capabilities similar to those employed by human experts.

Method: A two-stage approach is proposed: (1) ViT4TS uses a pretrained lightweight vision encoder for visual screening of anomalies, and (2) VLM4TS utilizes a VLM-based model for integrating global temporal context and reasoning based on ViT4TS predictions.

Result: The VLM4TS approach improves F1-max scores by 24.6% compared to the best baseline, consistently surpasses other methods, and is 36 times more efficient in token usage.

Conclusion: Using vision-language models significantly advances TSAD by addressing shortcomings in accuracy and efficiency, offering a promising direction for anomaly detection across diverse applications.

Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of
fields, including healthcare, finance, and industrial monitoring. Prior
methods, which mainly focus on training domain-specific models on numerical
data, lack the visual-temporal reasoning capacity that human experts have to
identify contextual anomalies. To fill this gap, we explore a solution based on
vision language models (VLMs). Recent studies have shown the ability of VLMs
for visual reasoning tasks, yet their direct application to time series has
fallen short on both accuracy and efficiency. To harness the power of VLMs for
TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening
stage built on a relatively lightweight pretrained vision encoder, which
leverages 2-D time-series representations to accurately localize candidate
anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal
context and VLM reasoning capacity to refine the detection upon the candidates
provided by ViT4TS. We show that without any time-series training, VLM4TS
outperforms time-series pretrained and from-scratch baselines in most cases,
yielding a 24.6 percent improvement in F1-max score over the best baseline.
Moreover, VLM4TS also consistently outperforms existing language-model-based
TSAD methods and is on average 36 times more efficient in token usage.

</details>


### [256] [Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles](https://arxiv.org/abs/2506.06846)
*Yangkai Lin,Jiabao Lei,Kui jia*

Main category: cs.CV

TL;DR: The paper introduces Multi-StyleGS, a method for styling 3D Gaussian Splatting scenes with multiple artistic styles efficiently and flexibly.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of stylizing 3D Gaussian Splatting scenes to align with multiple artistic styles while ensuring memory efficiency and providing flexibility for creative design.

Method: The method employs a bipartite matching mechanism to link style images with scene regions, a semantic style loss function for local style transfer, and a local-global feature matching to maintain multi-view consistency. It also includes segmentation techniques to label Gaussians and enhance memory efficiency.

Result: The proposed approach achieves high-quality stylization results with more detailed textures, better color matching, and allows flexibility in manual or automatic style assignment.

Conclusion: Multi-StyleGS surpasses existing methods in generating plausible, high-quality styles for 3D scenes with enhanced flexibility and efficiency.

Abstract: In recent years, there has been a growing demand to stylize a given 3D scene
to align with the artistic style of reference images for creative purposes.
While 3D Gaussian Splatting(GS) has emerged as a promising and efficient method
for realistic 3D scene modeling, there remains a challenge in adapting it to
stylize 3D GS to match with multiple styles through automatic local style
transfer or manual designation, while maintaining memory efficiency for
stylization training. In this paper, we introduce a novel 3D GS stylization
solution termed Multi-StyleGS to tackle these challenges. In particular, we
employ a bipartite matching mechanism to au tomatically identify
correspondences between the style images and the local regions of the rendered
images. To facilitate local style transfer, we introduce a novel semantic style
loss function that employs a segmentation network to apply distinct styles to
various objects of the scene and propose a local-global feature matching to
enhance the multi-view consistency. Furthermore, this technique can achieve
memory efficient training, more texture details and better color match. To
better assign a robust semantic label to each Gaussian, we propose several
techniques to regularize the segmentation network. As demonstrated by our
comprehensive experiments, our approach outperforms existing ones in producing
plausible stylization results and offering flexible editing.

</details>


### [257] [Deep Inertial Pose: A deep learning approach for human pose estimation](https://arxiv.org/abs/2506.06850)
*Sara M. Cerqueira,Manuel Palermo,Cristina P. Santos*

Main category: cs.CV

TL;DR: This paper explores using neural networks for human pose estimation using motion capture systems and demonstrates competitive results with advanced fusion filters.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify and reduce costs in human pose estimation by bypassing complex biomechanical models, making motion capture more accessible and affordable.

Method: The study compared various neural network architectures and approaches using data from both low-cost (MPU9250) and high-end (Mtw Awinda) MARG sensors, focusing on a Hybrid LSTM-Madgwick technique.

Result: The most accurate method, Hybrid LSTM-Madgwick detached, achieved a quaternion angle distance error of 7.96 using high-end sensor data. An ablation study further analyzed factors like data augmentation and magnetometer influence.

Conclusion: Neural Networks can effectively estimate human poses with accuracy comparable to state-of-the-art methods, suggesting a promising alternative for biomechanics and motion capture.

Abstract: Inertial-based Motion capture system has been attracting growing attention
due to its wearability and unsconstrained use. However, accurate human joint
estimation demands several complex and expertise demanding steps, which leads
to expensive software such as the state-of-the-art MVN Awinda from Xsens
Technologies. This work aims to study the use of Neural Networks to abstract
the complex biomechanical models and analytical mathematics required for pose
estimation. Thus, it presents a comparison of different Neural Network
architectures and methodologies to understand how accurately these methods can
estimate human pose, using both low cost(MPU9250) and high end (Mtw Awinda)
Magnetic, Angular Rate, and Gravity (MARG) sensors. The most efficient method
was the Hybrid LSTM-Madgwick detached, which achieved an Quaternion Angle
distance error of 7.96, using Mtw Awinda data. Also, an ablation study was
conducted to study the impact of data augmentation, output representation,
window size, loss function and magnetometer data on the pose estimation error.
This work indicates that Neural Networks can be trained to estimate human pose,
with results comparable to the state-of-the-art fusion filters.

</details>


### [258] [Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation](https://arxiv.org/abs/2506.06852)
*John Waithaka,Moise Busogi*

Main category: cs.CV

TL;DR: This paper adapts LOCA, a position prediction method, for semantic segmentation of multimodal satellite imagery and demonstrates its superiority over reconstruction-based approaches.


<details>
  <summary>Details</summary>
Motivation: To improve semantic segmentation performance in satellite imagery by addressing challenges of limited labeled data and focusing on localisation rather than reconstruction.

Method: The method extends SatMAE's channel grouping for multimodal data, integrates same-group attention masking to enhance cross-modal interactions, and employs relative patch position prediction for localisation.

Result: The approach outperformed reconstruction-based methods on the Sen1Floods11 dataset, effectively enhancing flood mapping accuracy.

Conclusion: Position prediction tasks, tailored for multimodal satellite imagery, offer more effective representation learning for semantic segmentation compared to reconstruction-focused methods.

Abstract: Semantic segmentation of satellite imagery is crucial for Earth observation
applications, but remains constrained by limited labelled training data. While
self-supervised pretraining methods like Masked Autoencoders (MAE) have shown
promise, they focus on reconstruction rather than localisation-a fundamental
aspect of segmentation tasks. We propose adapting LOCA (Location-aware), a
position prediction self-supervised learning method, for multimodal satellite
imagery semantic segmentation. Our approach addresses the unique challenges of
satellite data by extending SatMAE's channel grouping from multispectral to
multimodal data, enabling effective handling of multiple modalities, and
introducing same-group attention masking to encourage cross-modal interaction
during pretraining. The method uses relative patch position prediction,
encouraging spatial reasoning for localisation rather than reconstruction. We
evaluate our approach on the Sen1Floods11 flood mapping dataset, where it
significantly outperforms existing reconstruction-based self-supervised
learning methods for satellite imagery. Our results demonstrate that position
prediction tasks, when properly adapted for multimodal satellite imagery, learn
representations more effective for satellite image semantic segmentation than
reconstruction-based approaches.

</details>


### [259] [DONUT: A Decoder-Only Model for Trajectory Prediction](https://arxiv.org/abs/2506.06854)
*Markus Knoche,Daan de Geus,Bastian Leibe*

Main category: cs.CV

TL;DR: The paper introduces DONUT, a decoder-only model for predicting motion trajectories of agents in autonomous driving scenarios. It achieves state-of-the-art results using an autoregressive approach and a novel overprediction strategy.


<details>
  <summary>Details</summary>
Motivation: To develop better methods for motion prediction in autonomous driving to enhance a vehicle's ability to anticipate and make decisions based on future trajectories.

Method: The authors propose a decoder-only autoregressive model (DONUT) for predicting future motion trajectories. They couple this with an overprediction strategy that forecasts longer temporal horizons to improve performance.

Result: The proposed approach outperforms encoder-decoder baselines and sets new benchmarks on the Argoverse 2 motion forecasting dataset.

Conclusion: Autoregressive models with strategies like overprediction are effective for motion prediction in autonomous driving, offering consistent and improved performance over traditional methods.

Abstract: Predicting the motion of other agents in a scene is highly relevant for
autonomous driving, as it allows a self-driving car to anticipate. Inspired by
the success of decoder-only models for language modeling, we propose DONUT, a
Decoder-Only Network for Unrolling Trajectories. Different from existing
encoder-decoder forecasting models, we encode historical trajectories and
predict future trajectories with a single autoregressive model. This allows the
model to make iterative predictions in a consistent manner, and ensures that
the model is always provided with up-to-date information, enhancing the
performance. Furthermore, inspired by multi-token prediction for language
modeling, we introduce an 'overprediction' strategy that gives the network the
auxiliary task of predicting trajectories at longer temporal horizons. This
allows the model to better anticipate the future, and further improves the
performance. With experiments, we demonstrate that our decoder-only approach
outperforms the encoder-decoder baseline, and achieves new state-of-the-art
results on the Argoverse 2 single-agent motion forecasting benchmark.

</details>


### [260] [Vision-EKIPL: External Knowledge-Infused Policy Learning for Visual Reasoning](https://arxiv.org/abs/2506.06856)
*Chaoyang Wang,Zeyu Zhang,Haiyun Jiang*

Main category: cs.CV

TL;DR: The paper introduces Vision-EKIPL, a reinforcement learning framework enhancing visual reasoning in Multimodal Large Language Models by incorporating external high-quality actions, achieving better performance and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods for visual reasoning in Multimodal Large Language Models are constrained by limited exploration and inefficient training due to reliance on solely model-generated actions.

Method: Vision-EKIPL introduces knowledge infusion through high-quality actions generated by external auxiliary models during reinforcement learning optimization.

Result: The proposed method improves performance by up to 5% on the Reason-RFT-CoT benchmark compared to state-of-the-art methods.

Conclusion: Vision-EKIPL enhances reasoning capabilities, accelerates training efficiency, and sets a new paradigm for advancing research in multimodal large language models.

Abstract: Visual reasoning is crucial for understanding complex multimodal data and
advancing Artificial General Intelligence. Existing methods enhance the
reasoning capability of Multimodal Large Language Models (MLLMs) through
Reinforcement Learning (RL) fine-tuning (e.g., GRPO). However, current RL
approaches sample action groups solely from the policy model itself, which
limits the upper boundary of the model's reasoning capability and leads to
inefficient training. To address these limitations, this paper proposes a novel
RL framework called \textbf{Vision-EKIPL}. The core of this framework lies in
introducing high-quality actions generated by external auxiliary models during
the RL training process to guide the optimization of the policy model. The
policy learning with knowledge infusion from external models significantly
expands the model's exploration space, effectively improves the reasoning
boundary, and substantially accelerates training convergence speed and
efficiency. Experimental results demonstrate that our proposed Vision-EKIPL
achieved up to a 5\% performance improvement on the Reason-RFT-CoT Benchmark
compared to the state-of-the-art (SOTA). It reveals that Vision-EKIPL can
overcome the limitations of traditional RL methods, significantly enhance the
visual reasoning performance of MLLMs, and provide a new effective paradigm for
research in this field.

</details>


### [261] [Face recognition on point cloud with cgan-top for denoising](https://arxiv.org/abs/2506.06864)
*Junyu Liu,Jianfeng Ren,Sunhong Liang,Xudong Jiang*

Main category: cs.CV

TL;DR: This study proposes an innovative end-to-end method for 3D face recognition from noisy point clouds by combining denoising and recognition modules.


<details>
  <summary>Details</summary>
Motivation: Facial recognition using 3D point clouds faces challenges from noise due to imperfect sensors, necessitating robust methods to ensure high recognition accuracy.

Method: The authors introduced a Conditional Generative Adversarial Network on Three Orthogonal Planes (cGAN-TOP) to denoise point clouds and an LDGCNN for hierarchical feature linking to enable effective face recognition.

Result: The approach was validated using the Bosphorus dataset and achieved significant improvements in face recognition accuracy across various noise settings, with a peak performance gain of 14.81%.

Conclusion: Integrating denoising and recognition proves effective for addressing noisy 3D point clouds, delivering robust face recognition outcomes and advancing the field.

Abstract: Face recognition using 3D point clouds is gaining growing interest, while raw
point clouds often contain a significant amount of noise due to imperfect
sensors. In this paper, an end-to-end 3D face recognition on a noisy point
cloud is proposed, which synergistically integrates the denoising and
recognition modules. Specifically, a Conditional Generative Adversarial Network
on Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the
noise in the point cloud, and recover the underlying features for subsequent
recognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is
then adapted to recognize faces from the processed point cloud, which
hierarchically links both the local point features and neighboring features of
multiple scales. The proposed method is validated on the Bosphorus dataset. It
significantly improves the recognition accuracy under all noise settings, with
a maximum gain of 14.81%.

</details>


### [262] [Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via Eye-Tracking Analysis](https://arxiv.org/abs/2506.06886)
*Wafaa Kasri,Yassine Himeur,Abigail Copiaco,Wathiq Mansoor,Ammar Albanna,Valsamma Eapen*

Main category: cs.CV

TL;DR: The paper proposes a hybrid deep learning model using Vision Transformers and Vision Mamba to detect Autism Spectrum Disorder (ASD) with eye-tracking data, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Enhance the accuracy and transparency of ASD diagnosis by leveraging advanced AI techniques, especially in settings with limited diagnostic resources.

Method: Combine Vision Transformers and Vision Mamba with attention-based fusion to integrate spatial and temporal dynamics across eye-tracking, speech, and facial cues.

Result: Achieved superior performance metrics: 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94 specificity on the Saliency4ASD dataset.

Conclusion: The proposed model offers a scalable, interpretable solution for ASD diagnosis, particularly valuable for under-resourced or remote clinical environments.

Abstract: Accurate Autism Spectrum Disorder (ASD) diagnosis is vital for early
intervention. This study presents a hybrid deep learning framework combining
Vision Transformers (ViT) and Vision Mamba to detect ASD using eye-tracking
data. The model uses attention-based fusion to integrate visual, speech, and
facial cues, capturing both spatial and temporal dynamics. Unlike traditional
handcrafted methods, it applies state-of-the-art deep learning and explainable
AI techniques to enhance diagnostic accuracy and transparency. Tested on the
Saliency4ASD dataset, the proposed ViT-Mamba model outperformed existing
methods, achieving 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94
specificity. These findings show the model's promise for scalable,
interpretable ASD screening, especially in resource-constrained or remote
clinical settings where access to expert diagnosis is limited.

</details>


### [263] [KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search](https://arxiv.org/abs/2506.06906)
*Nima Jamali,Matina Mahdizadeh Sani,Hanieh Naderi,Shohreh Kasaei*

Main category: cs.CV

TL;DR: KNN-Defense is a lightweight defense strategy for improving the robustness of 3D point cloud neural networks against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: DNNs for 3D point cloud data are highly vulnerable to adversarial attacks, risking the reliability of these systems.

Method: The paper introduces KNN-Defense which leverages semantic similarity in the feature space using nearest-neighbor search rather than reconstructing geometries.

Result: KNN-Defense improves robustness significantly, showing accuracy gains up to 20.1% on PointNet and other notable increases across different architectures under various adversarial attacks.

Conclusion: KNN-Defense is an effective, scalable solution enhancing resilience of 3D point cloud classifiers, particularly practical for real-time applications.

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in
analyzing 3D point cloud data. However, their vulnerability to adversarial
attacks-such as point dropping, shifting, and adding-poses a critical challenge
to the reliability of 3D vision systems. These attacks can compromise the
semantic and structural integrity of point clouds, rendering many existing
defense mechanisms ineffective. To address this issue, a defense strategy named
KNN-Defense is proposed, grounded in the manifold assumption and
nearest-neighbor search in feature space. Instead of reconstructing surface
geometry or enforcing uniform point distributions, the method restores
perturbed inputs by leveraging the semantic similarity of neighboring samples
from the training set. KNN-Defense is lightweight and computationally
efficient, enabling fast inference and making it suitable for real-time and
practical applications. Empirical results on the ModelNet40 dataset
demonstrated that KNN-Defense significantly improves robustness across various
attack types. In particular, under point-dropping attacks-where many existing
methods underperform due to the targeted removal of critical points-the
proposed method achieves accuracy gains of 20.1%, 3.6%, 3.44%, and 7.74% on
PointNet, PointNet++, DGCNN, and PCT, respectively. These findings suggest that
KNN-Defense offers a scalable and effective solution for enhancing the
adversarial resilience of 3D point cloud classifiers. (An open-source
implementation of the method, including code and data, is available at
https://github.com/nimajam41/3d-knn-defense).

</details>


### [264] [Gaussian Mapping for Evolving Scenes](https://arxiv.org/abs/2506.06909)
*Vladimir Yugay,Thies Kersten,Luca Carlone,Theo Gevers,Martin R. Oswald,Lukas Schmid*

Main category: cs.CV

TL;DR: This paper introduces a novel mechanism called Gaussian Mapping for Evolving Scenes (GaME) to handle long-term dynamic changes in scenes for novel view synthesis.


<details>
  <summary>Details</summary>
Motivation: Current systems for 3D scene representation are limited to static or short-term dynamic scenes, leaving long-term dynamics underexplored.

Method: The paper proposes a dynamic scene adaptation mechanism to update the 3D representation continuously and a keyframe management system to maintain consistency by discarding outdated observations.

Result: Evaluations on synthetic and real-world datasets show that GaME outperforms existing state-of-the-art methods in accuracy.

Conclusion: GaME enables more consistent and accurate 3D scene mapping, making it suitable for applications in robotics, augmented reality, and autonomous driving.

Abstract: Mapping systems with novel view synthesis (NVS) capabilities are widely used
in computer vision, with augmented reality, robotics, and autonomous driving
applications. Most notably, 3D Gaussian Splatting-based systems show high NVS
performance; however, many current approaches are limited to static scenes.
While recent works have started addressing short-term dynamics (motion within
the view of the camera), long-term dynamics (the scene evolving through changes
out of view) remain less explored. To overcome this limitation, we introduce a
dynamic scene adaptation mechanism that continuously updates the 3D
representation to reflect the latest changes. In addition, since maintaining
geometric and semantic consistency remains challenging due to stale
observations disrupting the reconstruction process, we propose a novel keyframe
management mechanism that discards outdated observations while preserving as
much information as possible. We evaluate Gaussian Mapping for Evolving Scenes
(GaME) on both synthetic and real-world datasets and find it to be more
accurate than the state of the art.

</details>


### [265] [Sleep Stage Classification using Multimodal Embedding Fusion from EOG and PSM](https://arxiv.org/abs/2506.06912)
*Olivier Papillon,Rafik Goubran,James Green,Julien Larivière-Chartier,Caitlin Higginson,Frank Knoefel,Rébecca Robillard*

Main category: cs.CV

TL;DR: The paper presents a novel method to classify sleep stages using EOG and PSM data with ImageBind, achieving high accuracy comparable to EEG-based systems.


<details>
  <summary>Details</summary>
Motivation: There is a need for less obtrusive methods for sleep stage classification, especially for home-based sleep monitoring, as traditional PSG using EEG is complex and requires specialized equipment.

Method: The authors used ImageBind, a multimodal deep learning model, combining PSM data with dual-channel EOG signals for a five-stage sleep classification. They fine-tuned the model for optimal performance and evaluated it on 85 nights of patient data.

Result: The proposed approach outperformed other models such as DeepSleepNet, ViViT, and MBT in classification accuracy. It also demonstrated strong performance without fine-tuning, showing flexibility for applications with limited labeled data.

Conclusion: The study highlights that pre-trained multimodal embedding models, even those developed for non-medical domains, can be effectively adapted to sleep stage classification, offering high accuracy with simpler, less obtrusive data sources than EEG.

Abstract: Accurate sleep stage classification is essential for diagnosing sleep
disorders, particularly in aging populations. While traditional polysomnography
(PSG) relies on electroencephalography (EEG) as the gold standard, its
complexity and need for specialized equipment make home-based sleep monitoring
challenging. To address this limitation, we investigate the use of
electrooculography (EOG) and pressure-sensitive mats (PSM) as less obtrusive
alternatives for five-stage sleep-wake classification. This study introduces a
novel approach that leverages ImageBind, a multimodal embedding deep learning
model, to integrate PSM data with dual-channel EOG signals for sleep stage
classification. Our method is the first reported approach that fuses PSM and
EOG data for sleep stage classification with ImageBind. Our results demonstrate
that fine-tuning ImageBind significantly improves classification accuracy,
outperforming existing models based on single-channel EOG (DeepSleepNet),
exclusively PSM data (ViViT), and other multimodal deep learning approaches
(MBT). Notably, the model also achieved strong performance without fine-tuning,
highlighting its adaptability to specific tasks with limited labeled data,
making it particularly advantageous for medical applications. We evaluated our
method using 85 nights of patient recordings from a sleep clinic. Our findings
suggest that pre-trained multimodal embedding models, even those originally
developed for non-medical domains, can be effectively adapted for sleep
staging, with accuracies approaching systems that require complex EEG data.

</details>


### [266] [Reading in the Dark with Foveated Event Vision](https://arxiv.org/abs/2506.06918)
*Carl Brander,Giovanni Cioffi,Nico Messikommer,Davide Scaramuzza*

Main category: cs.CV

TL;DR: The paper introduces an event-based Optical Character Recognition (OCR) system for smart glasses that excels in low light and dynamic settings, offering huge bandwidth efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of RGB cameras in smart glasses under low-light, high-speed motion, and high power consumption scenarios, especially for image-based text recognition.

Method: Develop an event-based OCR leveraging user eye gaze to foveate event streams, deep binary reconstruction trained on synthetic data, and multimodal LLMs for superior performance.

Result: Achieved OCR capability in low-light scenarios, outperforming traditional methods and reducing bandwidth requirements by up to 2400x compared to wearable RGB cameras.

Conclusion: The proposed OCR method for smart glasses significantly enhances text recognition in challenging scenarios while being highly bandwidth-efficient.

Abstract: Current smart glasses equipped with RGB cameras struggle to perceive the
environment in low-light and high-speed motion scenarios due to motion blur and
the limited dynamic range of frame cameras. Additionally, capturing dense
images with a frame camera requires large bandwidth and power consumption,
consequently draining the battery faster. These challenges are especially
relevant for developing algorithms that can read text from images. In this
work, we propose a novel event-based Optical Character Recognition (OCR)
approach for smart glasses. By using the eye gaze of the user, we foveate the
event stream to significantly reduce bandwidth by around 98% while exploiting
the benefits of event cameras in high-dynamic and fast scenes. Our proposed
method performs deep binary reconstruction trained on synthetic data and
leverages multimodal LLMs for OCR, outperforming traditional OCR solutions. Our
results demonstrate the ability to read text in low light environments where
RGB cameras struggle while using up to 2400 times less bandwidth than a
wearable RGB camera.

</details>


### [267] [How Important are Videos for Training Video LLMs?](https://arxiv.org/abs/2506.06928)
*George Lydakis,Alexander Hermans,Ali Athar,Daan de Geus,Bastian Leibe*

Main category: cs.CV

TL;DR: Video LLMs show unexpected strong temporal reasoning abilities even when trained only on image-caption data, with marginal gains from video-specific training.


<details>
  <summary>Details</summary>
Motivation: To explore whether Video LLMs require video-specific training or if image-caption-based training suffices for effective temporal reasoning.

Method: Analyzing image-trained Video LLM performance on temporal reasoning benchmarks like TVBench and introducing a finetuning scheme using annotated image sequences with temporal tasks.

Result: Image-trained Video LLMs showcased above-chance temporal reasoning on TVBench, and the proposed finetuning scheme performed comparably to video-trained models.

Conclusion: Current video training methods fail to fully exploit temporal features of videos. Image-trained LLMs' unexpected temporal reasoning capabilities necessitate deeper investigation into their mechanisms and the inefficiencies in video-specific training approaches.

Abstract: Research into Video Large Language Models (LLMs) has progressed rapidly, with
numerous models and benchmarks emerging in just a few years. Typically, these
models are initialized with a pretrained text-only LLM and finetuned on both
image- and video-caption datasets. In this paper, we present findings
indicating that Video LLMs are more capable of temporal reasoning after
image-only training than one would assume, and that improvements from
video-specific training are surprisingly small. Specifically, we show that
image-trained versions of two LLMs trained with the recent LongVU algorithm
perform significantly above chance level on TVBench, a temporal reasoning
benchmark. Additionally, we introduce a simple finetuning scheme involving
sequences of annotated images and questions targeting temporal capabilities.
This baseline results in temporal reasoning performance close to, and
occasionally higher than, what is achieved by video-trained LLMs. This suggests
suboptimal utilization of rich temporal features found in real video by current
models. Our analysis motivates further research into the mechanisms that allow
image-trained LLMs to perform temporal reasoning, as well as into the
bottlenecks that render current video training schemes inefficient.

</details>


### [268] [Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences](https://arxiv.org/abs/2506.06944)
*Mellon M. Zhang,Glen Chou,Saibal Mukhopadhyay*

Main category: cs.CV

TL;DR: The paper introduces Polar Hierarchical Mamba (PHiM), a novel architecture for real-time LiDAR object detection, achieving state-of-the-art performance in streaming scenarios.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicle perception requires real-time LiDAR processing, but conventional approaches introduce latency due to full-scan processing or geometric misalignment in streaming methods.

Method: The authors propose PHiM, which replaces convolutions and positional encodings with distortion-aware operations, using local and global Mamba-based blocks for spatial and temporal encoding.

Result: PHiM outperforms previous streaming detectors by 10% on the Waymo Open Dataset and achieves throughput that is twice as fast as full-scan baselines.

Conclusion: PHiM is optimized for polar-coordinate streaming LiDAR, offering performance improvements and efficiency in real-time detection for autonomous vehicles.

Abstract: Accurate and efficient object detection is essential for autonomous vehicles,
where real-time perception requires low latency and high throughput. LiDAR
sensors provide robust depth information, but conventional methods process full
360{\deg} scans in a single pass, introducing significant delay. Streaming
approaches address this by sequentially processing partial scans in the native
polar coordinate system, yet they rely on translation-invariant convolutions
that are misaligned with polar geometry -- resulting in degraded performance or
requiring complex distortion mitigation. Recent Mamba-based state space models
(SSMs) have shown promise for LiDAR perception, but only in the full-scan
setting, relying on geometric serialization and positional embeddings that are
memory-intensive and ill-suited to streaming. We propose Polar Hierarchical
Mamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming
LiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial
encoding and a global forward Mamba for inter-sector temporal modeling,
replacing convolutions and positional encodings with distortion-aware,
dimensionally-decomposed operations. PHiM sets a new state-of-the-art among
streaming detectors on the Waymo Open Dataset, outperforming the previous best
by 10\% and matching full-scan baselines at twice the throughput. Code will be
available at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .

</details>


### [269] [LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer](https://arxiv.org/abs/2506.06952)
*Ying Shen,Zhiyang Xu,Jiuhai Chen,Shizhe Diao,Jiaxin Zhang,Yuguang Yao,Joy Rimchala,Ismini Lourentzou,Lifu Huang*

Main category: cs.CV

TL;DR: The paper introduces LaTtE-Flow, a multimodal model excelling in image understanding and generation with significant improvements in efficiency and speed.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of extensive pretraining, subpar task-specific performance, and slow image generation speeds in existing unified multimodal models.

Method: Proposes the LaTtE-Flow architecture, incorporating pretrained VLMs, a Layerwise Timestep-Expert Flow-based design, and a Timestep-Conditioned Residual Attention mechanism to enhance multimodal understanding and efficient image generation.

Result: LaTtE-Flow delivers strong multimodal understanding, competitive image generation quality, and a 6x faster inference speed compared to existing models.

Conclusion: LaTtE-Flow effectively unifies image understanding and generation with notable efficiency and performance gains, making it suitable for real-time deployment.

Abstract: Recent advances in multimodal foundation models unifying image understanding
and generation have opened exciting avenues for tackling a wide range of
vision-language tasks within a single framework. Despite progress, existing
unified models typically require extensive pretraining and struggle to achieve
the same level of performance compared to models dedicated to each task.
Additionally, many of these models suffer from slow image generation speeds,
limiting their practical deployment in real-time or resource-constrained
settings. In this work, we propose Layerwise Timestep-Expert Flow-based
Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image
understanding and generation within a single multimodal model. LaTtE-Flow
builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong
multimodal understanding capabilities, and extends them with a novel Layerwise
Timestep Experts flow-based architecture for efficient image generation.
LaTtE-Flow distributes the flow-matching process across specialized groups of
Transformer layers, each responsible for a distinct subset of timesteps. This
design significantly improves sampling efficiency by activating only a small
subset of layers at each sampling timestep. To further enhance performance, we
propose a Timestep-Conditioned Residual Attention mechanism for efficient
information reuse across layers. Experiments demonstrate that LaTtE-Flow
achieves strong performance on multimodal understanding tasks, while achieving
competitive image generation quality with around 6x faster inference speed
compared to recent unified multimodal models.

</details>


### [270] [Task-driven real-world super-resolution of document scans](https://arxiv.org/abs/2506.06953)
*Maciej Zyrek,Tomasz Tarasiewicz,Jakub Sadel,Aleksandra Krzywon,Michal Kawulok*

Main category: cs.CV

TL;DR: This paper improves single-image super-resolution for real-world document scans by using a multi-task learning framework augmented with auxiliary loss functions for tasks like text detection and recognition.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the difficulty of generalizing single-image super-resolution models, trained on simulated datasets, to real-world applications like document scans that involve complex degradations.

Method: The authors introduce a multi-task learning framework to optimize a super-resolution network for optical character recognition tasks, utilizing auxiliary loss functions from higher-level vision tasks and employing a dynamic weight averaging mechanism for balancing objectives.

Result: Using the SRResNet architecture, the proposed approach achieves improved text detection accuracy and preserves image fidelity in experiments on both simulated and real-world datasets.

Conclusion: The findings demonstrate the effectiveness of multi-objective optimization in bridging the gap between simulated training datasets and real-world deployment for super-resolution tasks, especially in the domain of document imaging.

Abstract: Single-image super-resolution refers to the reconstruction of a
high-resolution image from a single low-resolution observation. Although recent
deep learning-based methods have demonstrated notable success on simulated
datasets -- with low-resolution images obtained by degrading and downsampling
high-resolution ones -- they frequently fail to generalize to real-world
settings, such as document scans, which are affected by complex degradations
and semantic variability. In this study, we introduce a task-driven, multi-task
learning framework for training a super-resolution network specifically
optimized for optical character recognition tasks. We propose to incorporate
auxiliary loss functions derived from high-level vision tasks, including text
detection using the connectionist text proposal network, text recognition via a
convolutional recurrent neural network, keypoints localization using Key.Net,
and hue consistency. To balance these diverse objectives, we employ dynamic
weight averaging mechanism, which adaptively adjusts the relative importance of
each loss term based on its convergence behavior. We validate our approach upon
the SRResNet architecture, which is a well-established technique for
single-image super-resolution. Experimental evaluations on both simulated and
real-world scanned document datasets demonstrate that the proposed approach
improves text detection, measured with intersection over union, while
preserving overall image fidelity. These findings underscore the value of
multi-objective optimization in super-resolution models for bridging the gap
between simulated training regimes and practical deployment in real-world
scenarios.

</details>


### [271] [AR-RAG: Autoregressive Retrieval Augmentation for Image Generation](https://arxiv.org/abs/2506.06962)
*Jingyuan Qi,Zhiyang Xu,Qifan Wang,Lifu Huang*

Main category: cs.CV

TL;DR: AR-RAG improves image generation by using a dynamic, patch-level retrieval mechanism during the generation process.


<details>
  <summary>Details</summary>
Motivation: Existing methods for image generation rely on static retrievals, which can limit flexibility and introduce issues like over-copying and stylistic biases.

Method: AR-RAG incorporates retrievals at each generation step using two frameworks: DAiD, a training-free patch distribution merging strategy, and FAiD, a fine-tuning method that smooths and augments retrieved patch features during generation.

Result: Experiments on benchmarks like Midjourney-30K, GenEval, and DPG-Bench show AR-RAG achieves significant performance improvements over state-of-the-art models.

Conclusion: AR-RAG successfully enhances image generation by enabling dynamically evolving, context-aware retrievals that address limitations of previous static retrieval methods.

Abstract: We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm
that enhances image generation by autoregressively incorporating knearest
neighbor retrievals at the patch level. Unlike prior methods that perform a
single, static retrieval before generation and condition the entire generation
on fixed reference images, AR-RAG performs context-aware retrievals at each
generation step, using prior-generated patches as queries to retrieve and
incorporate the most relevant patch-level visual references, enabling the model
to respond to evolving generation needs while avoiding limitations (e.g.,
over-copying, stylistic bias, etc.) prevalent in existing methods. To realize
AR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in
Decoding (DAiD), a training-free plug-and-use decoding strategy that directly
merges the distribution of model-predicted patches with the distribution of
retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a
parameter-efficient fine-tuning method that progressively smooths the features
of retrieved patches via multi-scale convolution operations and leverages them
to augment the image generation process. We validate the effectiveness of
AR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and
DPG-Bench, demonstrating significant performance gains over state-of-the-art
image generation models.

</details>


### [272] [Dual-view Spatio-Temporal Feature Fusion with CNN-Transformer Hybrid Network for Chinese Isolated Sign Language Recognition](https://arxiv.org/abs/2506.06966)
*Siyuan Jing,Guangxue Wang,Haoyang Zhai,Qin Tao,Jun Yang,Bing Wang,Peng Jin*

Main category: cs.CV

TL;DR: The paper introduces NationalCSL-DP, a dual-view Chinese National Sign Language dataset, and proposes a CNN transformer-based baseline with a fusion strategy.


<details>
  <summary>Details</summary>
Motivation: Existing sign language datasets have limitations such as incomplete vocabulary coverage and challenges like hand occlusion due to single-view RGB videos.

Method: Created a dual-view dataset with recordings from front and left views to address occlusions and developed a CNN transformer model with a simple fusion strategy.

Result: The dataset and model showed effectiveness in improving ISLR performance, though sequence-to-sequence approaches struggled to leverage complementary features from the dual views.

Conclusion: The dual-view dataset and fusion strategy enhance ISLR performance, highlighting issues with complementary feature learning in sequence-to-sequence models.

Abstract: Due to the emergence of many sign language datasets, isolated sign language
recognition (ISLR) has made significant progress in recent years. In addition,
the development of various advanced deep neural networks is another reason for
this breakthrough. However, challenges remain in applying the technique in the
real world. First, existing sign language datasets do not cover the whole sign
vocabulary. Second, most of the sign language datasets provide only single view
RGB videos, which makes it difficult to handle hand occlusions when performing
ISLR. To fill this gap, this paper presents a dual-view sign language dataset
for ISLR named NationalCSL-DP, which fully covers the Chinese national sign
language vocabulary. The dataset consists of 134140 sign videos recorded by ten
signers with respect to two vertical views, namely, the front side and the left
side. Furthermore, a CNN transformer network is also proposed as a strong
baseline and an extremely simple but effective fusion strategy for prediction.
Extensive experiments were conducted to prove the effectiveness of the datasets
as well as the baseline. The results show that the proposed fusion strategy can
significantly increase the performance of the ISLR, but it is not easy for the
sequence-to-sequence model, regardless of whether the early-fusion or
late-fusion strategy is applied, to learn the complementary features from the
sign videos of two vertical views.

</details>


### [273] [Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment](https://arxiv.org/abs/2506.06970)
*Pengfei Zhao,Rongbo Luan,Wei Zhang,Peng Wu,Sifeng He*

Main category: cs.CV

TL;DR: MAPLE leverages Multimodal Large Language Models (MLLMs) to improve cross-modal representation learning in retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Existing cross-modal retrieval methods, like CLIP, face challenges due to the modality gap in feature space. Fine-grained cross-modal data alignment is needed for better retrieval performance.

Method: MAPLE combines reinforcement learning with preference learning, using MLLMs to automatically construct preference data and employing a Relative Preference Alignment loss to improve embeddings.

Result: Experiments show MAPLE achieves significant improvements in cross-modal retrieval, specifically for nuanced semantic distinctions.

Conclusion: MAPLE effectively bridges the modality gap by leveraging MLLMs intrinsic alignment capabilities, making retrieval more precise and granular.

Abstract: Despite Contrastive Language-Image Pretraining (CLIP)'s remarkable capability
to retrieve content across modalities, a substantial modality gap persists in
its feature space. Intriguingly, we discover that off-the-shelf MLLMs
(Multimodal Large Language Models) demonstrate powerful inherent modality
alignment properties. While recent MLLM-based retrievers with unified
architectures partially mitigate this gap, their reliance on coarse modality
alignment mechanisms fundamentally limits their potential. In this work, We
introduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel
framework that leverages the fine grained alignment priors inherent in MLLM to
guide cross modal representation learning. MAPLE formulates the learning
process as reinforcement learning with two key components: (1) Automatic
preference data construction using off-the-shelf MLLM, and (2) a new Relative
Preference Alignment (RPA) loss, which adapts Direct Preference Optimization
(DPO) to the embedding learning setting. Experimental results show that our
preference-guided alignment achieves substantial gains in fine-grained
cross-modal retrieval, underscoring its effectiveness in handling nuanced
semantic distinctions.

</details>


### [274] [Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction](https://arxiv.org/abs/2506.06988)
*Binxiao Huang,Zhihao Li,Shiyong Liu,Xiao Tang,Jiajun Tang,Jiaqi Lin,Yuxin Cheng,Zhenyu Chen,Xiaofei Wu,Ngai Wong*

Main category: cs.CV

TL;DR: This paper introduces a hybrid representation for 3D reconstruction, combining 3D Gaussian splatting with textured meshes to improve rendering speed while maintaining high quality.


<details>
  <summary>Details</summary>
Motivation: The inefficiency of 3D Gaussian splatting (3DGS) in regions with complex textures motivated the authors to create a more effective rendering solution for indoor scenes.

Method: The method involves pruning and refining textured meshes for texture-rich flat areas and using 3DGS for intricate geometries. Joint optimization is applied to balance their contributions, supported by a warm-up strategy and transmittance-aware supervision.

Result: The hybrid representation improves rendering speed (frames per second) significantly while using fewer Gaussian primitives, maintaining comparable rendering quality.

Conclusion: The proposed approach successfully addresses inefficiencies of 3DGS by combining it with textured meshes, achieving a balanced, optimized solution for real-time rendering of indoor scenes.

Abstract: 3D Gaussian splatting (3DGS) has demonstrated exceptional performance in
image-based 3D reconstruction and real-time rendering. However, regions with
complex textures require numerous Gaussians to capture significant color
variations accurately, leading to inefficiencies in rendering speed. To address
this challenge, we introduce a hybrid representation for indoor scenes that
combines 3DGS with textured meshes. Our approach uses textured meshes to handle
texture-rich flat areas, while retaining Gaussians to model intricate
geometries. The proposed method begins by pruning and refining the extracted
mesh to eliminate geometrically complex regions. We then employ a joint
optimization for 3DGS and mesh, incorporating a warm-up strategy and
transmittance-aware supervision to balance their contributions
seamlessly.Extensive experiments demonstrate that the hybrid representation
maintains comparable rendering quality and achieves superior frames per second
FPS with fewer Gaussian primitives.

</details>


### [275] [Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization](https://arxiv.org/abs/2506.06992)
*Yanting Gao,Yepeng Liu,Junming Liu,Qi Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: The paper introduces COGO, a novel strategy to improve the transferability of adversarial examples for Vision Transformers.


<details>
  <summary>Details</summary>
Motivation: Understanding adversarial example transferability in Vision Transformers is crucial, but current methods haven't effectively leveraged shared and unique model features.

Method: COGO combines Commonality Enhancement to perturb mid-to-low frequency regions and Individuality Suppression to adaptively weigh backpropagated gradients.

Result: COGO achieves superior transfer success rates compared to state-of-the-art adversarial attack methods in black-box settings.

Conclusion: Enhancing shared common features while suppressing individual characteristics in perturbations improves adversarial transferability significantly.

Abstract: Exploring effective and transferable adversarial examples is vital for
understanding the characteristics and mechanisms of Vision Transformers (ViTs).
However, adversarial examples generated from surrogate models often exhibit
weak transferability in black-box settings due to overfitting. Existing methods
improve transferability by diversifying perturbation inputs or applying uniform
gradient regularization within surrogate models, yet they have not fully
leveraged the shared and unique features of surrogate models trained on the
same task, leading to suboptimal transfer performance. Therefore, enhancing
perturbations of common information shared by surrogate models and suppressing
those tied to individual characteristics offers an effective way to improve
transferability. Accordingly, we propose a commonality-oriented gradient
optimization strategy (COGO) consisting of two components: Commonality
Enhancement (CE) and Individuality Suppression (IS). CE perturbs the mid-to-low
frequency regions, leveraging the fact that ViTs trained on the same dataset
tend to rely more on mid-to-low frequency information for classification. IS
employs adaptive thresholds to evaluate the correlation between backpropagated
gradients and model individuality, assigning weights to gradients accordingly.
Extensive experiments demonstrate that COGO significantly improves the transfer
success rates of adversarial attacks, outperforming current state-of-the-art
methods.

</details>


### [276] [DM$^3$Net: Dual-Camera Super-Resolution via Domain Modulation and Multi-scale Matching](https://arxiv.org/abs/2506.06993)
*Cong Guan,Jiacheng Ying,Yuya Ieiri,Osamu Yoshie*

Main category: cs.CV

TL;DR: The paper presents DM$^3$Net, a dual-camera super-resolution network utilizing domain modulation and multi-scale matching for enhanced image super-resolution, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve wide-angle image super-resolution in smartphones using telephoto images while overcoming domain gaps and enhancing structural detail transfer.

Method: The method includes domain modulation for bridging domain gaps, a multi-scale matching module for robust patch-level feature matching, and Key Pruning for reducing memory usage and inference time.

Result: DM$^3$Net achieves superior performance compared to state-of-the-art methods, validated through experimental results on three real-world datasets.

Conclusion: DM$^3$Net is effective and efficient for dual-camera super-resolution, offering strong results while maintaining computational efficiency.

Abstract: Dual-camera super-resolution is highly practical for smartphone photography
that primarily super-resolve the wide-angle images using the telephoto image as
a reference. In this paper, we propose DM$^3$Net, a novel dual-camera
super-resolution network based on Domain Modulation and Multi-scale Matching.
To bridge the domain gap between the high-resolution domain and the degraded
domain, we learn two compressed global representations from image pairs
corresponding to the two domains. To enable reliable transfer of high-frequency
structural details from the reference image, we design a multi-scale matching
module that conducts patch-level feature matching and retrieval across multiple
receptive fields to improve matching accuracy and robustness. Moreover, we also
introduce Key Pruning to achieve a significant reduction in memory usage and
inference time with little model performance sacrificed. Experimental results
on three real-world datasets demonstrate that our DM$^3$Net outperforms the
state-of-the-art approaches.

</details>


### [277] [Technical Report for ICRA 2025 GOOSE 3D Semantic Segmentation Challenge: Adaptive Point Cloud Understanding for Heterogeneous Robotic Systems](https://arxiv.org/abs/2506.06995)
*Xiaoya Zhang*

Main category: cs.CV

TL;DR: The paper discusses a winning solution for the ICRA 2025 GOOSE Challenge on 3D semantic segmentation using a novel Point Prompt Tuning integrated with Point Transformer v3, achieving up to 22.59% mIoU improvement.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of effective semantic segmentation on 3D point clouds collected from diverse robotic platforms and unstructured outdoor environments.

Method: Point Prompt Tuning (PPT) integrated with Point Transformer v3 (PTv3) backbone to process heterogeneous LiDAR data, using platform-specific conditioning and cross-dataset class alignment strategies.

Result: Achieved up to 22.59% mIoU improvement on challenging platforms, without additional external data.

Conclusion: This adaptive approach advances point cloud understanding and demonstrates its applicability and effectiveness in field robotics applications.

Abstract: This technical report presents the implementation details of the winning
solution for the ICRA 2025 GOOSE 3D Semantic Segmentation Challenge. This
challenge focuses on semantic segmentation of 3D point clouds from diverse
unstructured outdoor environments collected from multiple robotic platforms.
This problem was addressed by implementing Point Prompt Tuning (PPT) integrated
with Point Transformer v3 (PTv3) backbone, enabling adaptive processing of
heterogeneous LiDAR data through platform-specific conditioning and
cross-dataset class alignment strategies. The model is trained without
requiring additional external data. As a result, this approach achieved
substantial performance improvements with mIoU increases of up to 22.59% on
challenging platforms compared to the baseline PTv3 model, demonstrating the
effectiveness of adaptive point cloud understanding for field robotics
applications.

</details>


### [278] [BePo: Leveraging Birds Eye View and Sparse Points for Efficient and Accurate 3D Occupancy Prediction](https://arxiv.org/abs/2506.07002)
*Yunxiao Shi,Hong Cai,Jisoo Jeong,Yinhao Zhu,Shizhong Han,Amin Ansari,Fatih Porikli*

Main category: cs.CV

TL;DR: The paper introduces BePo, a novel 3D occupancy prediction approach for scene understanding in autonomous driving, combining BEV and sparse points to address their individual limitations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D occupancy prediction are computationally expensive or struggle with specific scenarios, such as small objects in BEV or large surfaces in sparse points representation.

Method: BePo utilizes a dual-branch design, incorporating a query-based sparse points branch alongside a BEV branch, with cross-attention between branches and fusion of outputs to predict 3D occupancy.

Result: Experiments on Occ3D-nuScenes and Occ3D-Waymo benchmarks show BePo's superior performance and competitive inference speed compared to efficient approaches.

Conclusion: BePo effectively combines BEV and sparse points representations, addressing their respective shortcomings while achieving high accuracy and efficiency for 3D occupancy prediction in autonomous driving.

Abstract: 3D occupancy provides fine-grained 3D geometry and semantics for scene
understanding which is critical for autonomous driving. Most existing methods,
however, carry high compute costs, requiring dense 3D feature volume and
cross-attention to effectively aggregate information. More recent works have
adopted Bird's Eye View (BEV) or sparse points as scene representation with
much reduced cost, but still suffer from their respective shortcomings. More
concretely, BEV struggles with small objects that often experience significant
information loss after being projected to the ground plane. On the other hand,
points can flexibly model little objects in 3D, but is inefficient at capturing
flat surfaces or large objects. To address these challenges, in this paper, we
present a novel 3D occupancy prediction approach, BePo, which combines BEV and
sparse points based representations. We propose a dual-branch design: a
query-based sparse points branch and a BEV branch. The 3D information learned
in the sparse points branch is shared with the BEV stream via cross-attention,
which enriches the weakened signals of difficult objects on the BEV plane. The
outputs of both branches are finally fused to generate predicted 3D occupancy.
We conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo
benchmarks that demonstrate the superiority of our proposed BePo. Moreover,
BePo also delivers competitive inference speed when compared to the latest
efficient approaches.

</details>


### [279] [UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment](https://arxiv.org/abs/2506.07013)
*Wentao Zhao,Yihe Niu,Yanbo Wang,Tianchen Deng,Shenghai Yuan,Zhenli Wang,Rui Guo,Jingchuan Wang*

Main category: cs.CV

TL;DR: UNO introduces a unified monocular visual odometry framework with state-of-the-art performance across diverse environments and devices.


<details>
  <summary>Details</summary>
Motivation: Current monocular visual odometry techniques struggle to adapt to various environments and motion patterns due to reliance on tuning or predefined motion assumptions.

Method: UNO uses a Mixture-of-Experts strategy with specialized decoders for ego-motion patterns and incorporates a differentiable Gumbel-Softmax module for robust pose estimation. Depth priors and bundling adjustments ensure geometric consistency.

Result: Extensive evaluations on KITTI, EuRoC-MAV, and TUM-RGBD datasets showcase UNO achieving state-of-the-art results for autonomous vehicles, drones, and handheld devices.

Conclusion: UNO provides a generalizable and robust solution for monocular visual odometry, marking a significant contribution to its deployment across varied scenarios.

Abstract: This work presents UNO, a unified monocular visual odometry framework that
enables robust and adaptable pose estimation across diverse environments,
platforms, and motion patterns. Unlike traditional methods that rely on
deployment-specific tuning or predefined motion priors, our approach
generalizes effectively across a wide range of real-world scenarios, including
autonomous vehicles, aerial drones, mobile robots, and handheld devices. To
this end, we introduce a Mixture-of-Experts strategy for local state
estimation, with several specialized decoders that each handle a distinct class
of ego-motion patterns. Moreover, we introduce a fully differentiable
Gumbel-Softmax module that constructs a robust inter-frame correlation graph,
selects the optimal expert decoder, and prunes erroneous estimates. These cues
are then fed into a unified back-end that combines pre-trained,
scale-independent depth priors with a lightweight bundling adjustment to
enforce geometric consistency. We extensively evaluate our method on three
major benchmark datasets: KITTI (outdoor/autonomous driving), EuRoC-MAV
(indoor/aerial drones), and TUM-RGBD (indoor/handheld), demonstrating
state-of-the-art performance.

</details>


### [280] [TABLET: Table Structure Recognition using Encoder-only Transformers](https://arxiv.org/abs/2506.07015)
*Qiyu Hou,Jun Wang*

Main category: cs.CV

TL;DR: A novel Split-Merge-based top-down model is proposed for recognizing large, densely populated tables, utilizing Transformer encoders to improve accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for table structure recognition face challenges like unstable bounding box predictions, resolution loss, computational complexity, and scalability for large tables.

Method: The approach involves splitting rows and columns as sequence labeling tasks with dual Transformer encoders, and merging as a grid cell classification task with an additional Transformer encoder.

Result: Experiments on FinTabNet and PubTabNet datasets show superior accuracy and processing speed, outperforming existing approaches in real-world applications.

Conclusion: The proposed method provides a scalable, robust, and efficient solution for large-scale table recognition, making it suitable for industrial use.

Abstract: To address the challenges of table structure recognition, we propose a novel
Split-Merge-based top-down model optimized for large, densely populated tables.
Our approach formulates row and column splitting as sequence labeling tasks,
utilizing dual Transformer encoders to capture feature interactions. The
merging process is framed as a grid cell classification task, leveraging an
additional Transformer encoder to ensure accurate and coherent merging. By
eliminating unstable bounding box predictions, our method reduces resolution
loss and computational complexity, achieving high accuracy while maintaining
fast processing speed. Extensive experiments on FinTabNet and PubTabNet
demonstrate the superiority of our model over existing approaches, particularly
in real-world applications. Our method offers a robust, scalable, and efficient
solution for large-scale table recognition, making it well-suited for
industrial deployment.

</details>


### [281] [Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI](https://arxiv.org/abs/2506.07286)
*Aditya Chakravarty*

Main category: cs.CV

TL;DR: This paper introduces a multistep optimization strategy for diffusion models to improve image restoration quality and generalization in real-time applications.


<details>
  <summary>Details</summary>
Motivation: Diffusion models possess flexibility for inverse problems without retraining, but current methods like MPGD suffer from low fidelity and robustness in challenging settings.

Method: The authors propose a multistep optimization strategy within each denoising timestep to enhance image restoration efficacy.

Result: Experiments on super-resolution and Gaussian deblurring show improved metrics like LPIPS and PSNR, with validation carried out on various datasets including natural and aerial scenes.

Conclusion: MPGD with multistep optimization functions as an effective, lightweight module for real-time visual perception tasks in AI applications such as drones and mobile robots.

Abstract: Diffusion models have shown remarkable flexibility for solving inverse
problems without task-specific retraining. However, existing approaches such as
Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update
per denoising step, limiting restoration fidelity and robustness, especially in
embedded or out-of-distribution settings. In this work, we introduce a
multistep optimization strategy within each denoising timestep, significantly
enhancing image quality, perceptual accuracy, and generalization. Our
experiments on super-resolution and Gaussian deblurring demonstrate that
increasing the number of gradient updates per step improves LPIPS and PSNR with
minimal latency overhead. Notably, we validate this approach on a Jetson Orin
Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally
trained on face datasets, generalizes effectively to natural and aerial scenes.
Our findings highlight MPGD's potential as a lightweight, plug-and-play
restoration module for real-time visual perception in embodied AI agents such
as drones and mobile robots.

</details>


### [282] [MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks](https://arxiv.org/abs/2506.07016)
*Sanjoy Chowdhury,Mohamed Elmoghany,Yohan Abeysinghe,Junjie Fei,Sayan Nag,Salman Khan,Mohamed Elhoseiny,Dinesh Manocha*

Main category: cs.CV

TL;DR: This paper introduces AV-HaystacksQA, a novel task and benchmark assessing large multimodal models (LMMs) on retrieving and reasoning across multiple videos, along with a framework called MAGNET to improve performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of current video question answering benchmarks, which typically involve single-clip queries and fail to reflect real-world audio-visual retrieval and reasoning challenges across multiple videos.

Method: A new benchmark AVHaystacks comprising 3100 annotated QA pairs is introduced, along with a model-agnostic multi-agent framework called MAGNET, supported by two evaluation metrics (STEM and MTGS) for assessing multi-video retrieval and segment grounding performance.

Result: The proposed MAGNET framework achieves significant improvements over baseline methods—up to 89% in BLEU@4 and 65% in GPT-based evaluation scores.

Conclusion: The work bridges the gap between current benchmarks and real-world scenarios, demonstrating the potential of MAGNET and AVHaystacks to improve LMM capabilities in multi-video retrieval and reasoning tasks.

Abstract: Large multimodal models (LMMs) have shown remarkable progress in audio-visual
understanding, yet they struggle with real-world scenarios that require complex
reasoning across extensive video collections. Existing benchmarks for video
question answering remain limited in scope, typically involving one clip per
query, which falls short of representing the challenges of large-scale,
audio-visual retrieval and reasoning encountered in practical applications. To
bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal
is to identify salient segments across different videos in response to a query
and link them together to generate the most informative answer. To this end, we
present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA
pairs designed to assess the capabilities of LMMs in multi-video retrieval and
temporal grounding task. Additionally, we propose a model-agnostic, multi-agent
framework MAGNET to address this challenge, achieving up to 89% and 65%
relative improvements over baseline methods on BLEU@4 and GPT evaluation scores
in QA task on our proposed AVHaystacks. To enable robust evaluation of
multi-video retrieval and temporal grounding for optimal response generation,
we introduce two new metrics, STEM, which captures alignment errors between a
ground truth and a predicted step sequence and MTGS, to facilitate balanced and
interpretable evaluation of segment-level grounding performance. Project:
https://schowdhury671.github.io/magnet_project/

</details>


### [283] [Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation](https://arxiv.org/abs/2506.07338)
*Yijie Deng,Shuaihang Yuan,Geeta Chandra Raju Bethala,Anthony Tzes,Yu-Shen Liu,Yi Fang*

Main category: cs.CV

TL;DR: This paper introduces a streamlined framework for Instance Image-Goal Navigation (IIN), optimizing viewpoint selection for target matching using semantic and geometric scoring.


<details>
  <summary>Details</summary>
Motivation: Current methods for IIN rely on random sampling for viewpoint selection, which leads to redundancy and inefficiency in rendering and comparisons.

Method: The paper proposes a hierarchical scoring mechanism combining semantic scoring via CLIP-derived relevancy fields and geometric scoring for precise pose estimation.

Result: The approach achieved state-of-the-art performance on simulated benchmarks and demonstrated applicability in real-world scenarios.

Conclusion: Utilizing principled viewpoint estimation improves efficiency and matching capability in IIN systems, reducing redundancy and computational overhead.

Abstract: Instance Image-Goal Navigation (IIN) requires autonomous agents to identify
and navigate to a target object or location depicted in a reference image
captured from any viewpoint. While recent methods leverage powerful novel view
synthesis (NVS) techniques, such as three-dimensional Gaussian splatting
(3DGS), they typically rely on randomly sampling multiple viewpoints or
trajectories to ensure comprehensive coverage of discriminative visual cues.
This approach, however, creates significant redundancy through overlapping
image samples and lacks principled view selection, substantially increasing
both rendering and comparison overhead. In this paper, we introduce a novel IIN
framework with a hierarchical scoring paradigm that estimates optimal
viewpoints for target matching. Our approach integrates cross-level semantic
scoring, utilizing CLIP-derived relevancy fields to identify regions with high
semantic similarity to the target object class, with fine-grained local
geometric scoring that performs precise pose estimation within promising
regions. Extensive evaluations demonstrate that our method achieves
state-of-the-art performance on simulated IIN benchmarks and real-world
applicability.

</details>


### [284] [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
*Yikun Ji,Hong Yan,Jun Lan,Huijia Zhu,Weiqiang Wang,Qi Fan,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: The paper presents an improved method for detecting AI-generated images using fine-tuned multi-modal large language models (MLLMs) with human-aligned reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods lack interpretability, and MLLMs face challenges in aligning visual interpretations with human reasoning.

Method: The authors constructed a dataset with annotated AI-generated images and fine-tuned MLLMs using a multi-stage optimization strategy to balance detection, localization, and explanation.

Result: The fine-tuned models surpassed baseline methods, offering enhanced performance in AI-image detection and artifact localization with coherent textual explanations.

Conclusion: Fine-tuning MLLMs with a targeted dataset and strategy brings improvements in both detection accuracy and interpretability for AI-generated images.

Abstract: The rapid advancement of image generation technologies intensifies the demand
for interpretable and robust detection methods. Although existing approaches
often attain high accuracy, they typically operate as black boxes without
providing human-understandable justifications. Multi-modal Large Language
Models (MLLMs), while not originally intended for forgery detection, exhibit
strong analytical and reasoning capabilities. When properly fine-tuned, they
can effectively identify AI-generated images and offer meaningful explanations.
However, existing MLLMs still struggle with hallucination and often fail to
align their visual interpretations with actual image content and human
reasoning. To bridge this gap, we construct a dataset of AI-generated images
annotated with bounding boxes and descriptive captions that highlight synthesis
artifacts, establishing a foundation for human-aligned visual-textual grounded
reasoning. We then finetune MLLMs through a multi-stage optimization strategy
that progressively balances the objectives of accurate detection, visual
localization, and coherent textual explanation. The resulting model achieves
superior performance in both detecting AI-generated images and localizing
visual flaws, significantly outperforming baseline methods.

</details>


### [285] [From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion](https://arxiv.org/abs/2506.07050)
*Zheng Wang,Kai Ying,Bin Xu,Chunjiao Wang,Cong Bai*

Main category: cs.CV

TL;DR: This paper introduces PRE-Net, a model for enhancing infrared-based precipitation retrieval in satellite data. It performs better than current industry-leading benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of infrared-based precipitation retrieval, which lacks accuracy compared to passive microwave and radar-based methods, and to extend accurate precipitation analysis beyond current scanning ranges.

Method: The study proposes a two-stage pipeline with the PRE-Net model. The Swath-Distilling stage transfers multimodal knowledge using Coordinated Masking and Wavelet Enhancement (CoMWE), and the Full-Disc Adaptation stage employs Self-MaskTune for balanced predictions across the full disc.

Result: PRE-Net significantly improves precipitation retrieval accuracy and outperforms leading precipitation retrieval products such as PERSIANN-CCS, PDIR, and IMERG in the introduced PRE benchmark.

Conclusion: PRE-Net represents a significant advancement in infrared-based precipitation retrieval, addressing spatial range and accuracy challenges. Its open-source code ensures accessibility for further research and application.

Abstract: Accurate near-real-time precipitation retrieval has been enhanced by
satellite-based technologies. However, infrared-based algorithms have low
accuracy due to weak relations with surface precipitation, whereas passive
microwave and radar-based methods are more accurate but limited in range. This
challenge motivates the Precipitation Retrieval Expansion (PRE) task, which
aims to enable accurate, infrared-based full-disc precipitation retrievals
beyond the scanning swath. We introduce Multimodal Knowledge Expansion, a
two-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling
stage, PRE-Net transfers knowledge from a multimodal data integration model to
an infrared-based model within the scanning swath via Coordinated Masking and
Wavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune
refines predictions across the full disc by balancing multimodal and full-disc
infrared knowledge. Experiments on the introduced PRE benchmark demonstrate
that PRE-Net significantly advanced precipitation retrieval performance,
outperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code
will be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.

</details>


### [286] [A Layered Self-Supervised Knowledge Distillation Framework for Efficient Multimodal Learning on the Edge](https://arxiv.org/abs/2506.07055)
*Tarique Dahri,Zulfiqar Ali Memon,Zhenyu Yu,Mohd. Yamani Idna Idris,Sheheryar Khan,Sadiq Ahmad,Maged Shoman,Saddam Aziz,Rizwan Qureshi*

Main category: cs.CV

TL;DR: The paper proposes Layered Self-Supervised Knowledge Distillation (LSSKD), improving compact model performance without leveraging large teacher networks, and achieves state-of-the-art results across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Current knowledge distillation methods often rely on large pre-trained teacher models, making them computationally expensive and less adaptable for resource-constrained environments.

Method: The framework introduces auxiliary classifiers to intermediate feature maps for self-supervised knowledge generation and stage-level knowledge transfer without a pre-trained teacher.

Result: LSSKD yields a 4.54% improvement over PS-KD on CIFAR-100, and further improves few-shot learning performance on datasets like Tiny ImageNet.

Conclusion: The framework offers a lightweight, adaptable solution for deploying efficient models suitable for low-computing environments, enhancing generalization and responsiveness under weak supervision.

Abstract: We introduce Layered Self-Supervised Knowledge Distillation (LSSKD) framework
for training compact deep learning models. Unlike traditional methods that rely
on pre-trained teacher networks, our approach appends auxiliary classifiers to
intermediate feature maps, generating diverse self-supervised knowledge and
enabling one-to-one transfer across different network stages. Our method
achieves an average improvement of 4.54\% over the state-of-the-art PS-KD
method and a 1.14% gain over SSKD on CIFAR-100, with a 0.32% improvement on
ImageNet compared to HASSKD. Experiments on Tiny ImageNet and CIFAR-100 under
few-shot learning scenarios also achieve state-of-the-art results. These
findings demonstrate the effectiveness of our approach in enhancing model
generalization and performance without the need for large over-parameterized
teacher networks. Importantly, at the inference stage, all auxiliary
classifiers can be removed, yielding no extra computational cost. This makes
our model suitable for deploying small language models on affordable
low-computing devices. Owing to its lightweight design and adaptability, our
framework is particularly suitable for multimodal sensing and cyber-physical
environments that require efficient and responsive inference. LSSKD facilitates
the development of intelligent agents capable of learning from limited sensory
data under weak supervision.

</details>


### [287] [D2R: dual regularization loss with collaborative adversarial generation for model robustness](https://arxiv.org/abs/2506.07056)
*Zhenyu Liu,Huizhi Liang,Rajiv Ranjan,Zhanxing Zhu,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: This paper introduces a dual regularization loss (D2R Loss) method and collaborative adversarial generation (CAG) strategy to improve the robustness of deep neural networks against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Existing defense methods have limitations in guiding target models effectively via loss functions and lack collaboration when generating adversarial samples.

Method: The authors present D2R Loss, which optimizes the adversarial and clean distributions, and CAG, which generates adversarial samples via gradient-based collaboration between guidance and target models.

Result: Extensive experiments on CIFAR-10, CIFAR-100, Tiny ImageNet, and models like WideResNet34-10 and PreActResNet18 demonstrate that D2R Loss combined with CAG significantly improves model robustness.

Conclusion: D2R Loss with CAG is an effective adversarial training method that achieves highly robust models by addressing key shortcomings in existing frameworks.

Abstract: The robustness of Deep Neural Network models is crucial for defending models
against adversarial attacks. Recent defense methods have employed collaborative
learning frameworks to enhance model robustness. Two key limitations of
existing methods are (i) insufficient guidance of the target model via loss
functions and (ii) non-collaborative adversarial generation. We, therefore,
propose a dual regularization loss (D2R Loss) method and a collaborative
adversarial generation (CAG) strategy for adversarial training. D2R loss
includes two optimization steps. The adversarial distribution and clean
distribution optimizations enhance the target model's robustness by leveraging
the strengths of different loss functions obtained via a suitable function
space exploration to focus more precisely on the target model's distribution.
CAG generates adversarial samples using a gradient-based collaboration between
guidance and target models. We conducted extensive experiments on three
benchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two
popular target models, WideResNet34-10 and PreActResNet18. Our results show
that D2R loss with CAG produces highly robust models.

</details>


### [288] [R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation](https://arxiv.org/abs/2506.07826)
*William Ljungbergh,Bernardo Taveira,Wenzhao Zheng,Adam Tonderski,Chensheng Peng,Fredrik Kahl,Christoffer Petersson,Michael Felsberg,Kurt Keutzer,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: This paper introduces R3D2, a lightweight diffusion model for realistic insertion of 3D assets into photorealistic virtual environments, improving AD validation capabilities.


<details>
  <summary>Details</summary>
Motivation: Current validation of autonomous driving systems lacks diverse and scalable photorealistic simulations due to the limitations of traditional platforms and neural reconstruction methods, including dynamic object manipulation and reusability issues.

Method: R3D2 leverages a novel dataset where 3D Gaussian Splatting object assets are created from autonomous driving data and placed synthetically into neural rendering environments. R3D2, a diffusion model, is trained to generate realistic effects like shadows and lighting for inserted assets in real time.

Result: The R3D2 model significantly improves the realism of inserted 3D assets, enabling applications such as text-to-3D asset insertion and cross-scene object transfers for scalable autonomous driving validation.

Conclusion: R3D2 addresses scalability and realism challenges in virtual environments for AD validation, demonstrating its potential for improving simulation realism and supporting research with released datasets and code.

Abstract: Validating autonomous driving (AD) systems requires diverse and
safety-critical testing, making photorealistic virtual environments essential.
Traditional simulation platforms, while controllable, are resource-intensive to
scale and often suffer from a domain gap with real-world data. In contrast,
neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a
scalable solution for creating photorealistic digital twins of real-world
driving scenes. However, they struggle with dynamic object manipulation and
reusability as their per-scene optimization-based methodology tends to result
in incomplete object models with integrated illumination effects. This paper
introduces R3D2, a lightweight, one-step diffusion model designed to overcome
these limitations and enable realistic insertion of complete 3D assets into
existing scenes by generating plausible rendering effects-such as shadows and
consistent lighting-in real time. This is achieved by training R3D2 on a novel
dataset: 3DGS object assets are generated from in-the-wild AD data using an
image-conditioned 3D generative model, and then synthetically placed into
neural rendering-based virtual environments, allowing R3D2 to learn realistic
integration. Quantitative and qualitative evaluations demonstrate that R3D2
significantly enhances the realism of inserted assets, enabling use-cases like
text-to-3D asset insertion and cross-scene/dataset object transfer, allowing
for true scalability in AD validation. To promote further research in scalable
and realistic AD simulation, we will release our dataset and code, see
https://research.zenseact.com/publications/R3D2/.

</details>


### [289] [FLAIR-HUB: Large-scale Multimodal Dataset for Land Cover and Crop Mapping](https://arxiv.org/abs/2506.07080)
*Anatol Garioud,Sébastien Giordano,Nicolas David,Nicolas Gonthier*

Main category: cs.CV

TL;DR: FLAIR-HUB introduces a massive, multimodal land cover dataset for advancing EO data applications in France, demonstrating strong outcomes in complex multimodal fusion and pretraining approaches.


<details>
  <summary>Details</summary>
Motivation: To explore innovative solutions for processing and annotating diverse high-quality Earth Observation data, aiding in global land cover and crop type monitoring.

Method: IGN created FLAIR-HUB, a large annotated multi-sensor land cover dataset, combining six modalities (aerial/Sentinel/Spot imagery, among others), and conducted benchmarks on deep learning methods for complex multimodal analysis.

Result: FLAIR-HUB achieved best land cover accuracy at 78.2% and mIoU at 65.8% using nearly all modalities, demonstrating effective multimodal fusion and classification.

Conclusion: FLAIR-HUB advances global EO applications by providing resources for supervised learning and multimodal pretraining, overcoming challenges posed by heterogeneous datasets.

Abstract: The growing availability of high-quality Earth Observation (EO) data enables
accurate global land cover and crop type monitoring. However, the volume and
heterogeneity of these datasets pose major processing and annotation
challenges. To address this, the French National Institute of Geographical and
Forest Information (IGN) is actively exploring innovative strategies to exploit
diverse EO data, which require large annotated datasets. IGN introduces
FLAIR-HUB, the largest multi-sensor land cover dataset with
very-high-resolution (20 cm) annotations, covering 2528 km2 of France. It
combines six aligned modalities: aerial imagery, Sentinel-1/2 time series, SPOT
imagery, topographic data, and historical aerial images. Extensive benchmarks
evaluate multimodal fusion and deep learning models (CNNs, transformers) for
land cover or crop mapping and also explore multi-task learning. Results
underscore the complexity of multimodal fusion and fine-grained classification,
with best land cover performance (78.2% accuracy, 65.8% mIoU) achieved using
nearly all modalities. FLAIR-HUB supports supervised and multimodal
pretraining, with data and code available at
https://ignf.github.io/FLAIR/flairhub.

</details>


### [290] [LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds](https://arxiv.org/abs/2506.07857)
*Zihui Zhang,Weisheng Dai,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: The paper proposes LogoSP, a novel approach for unsupervised 3D semantic segmentation using superpoints grouped by global frequency domain patterns, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods in unsupervised 3D semantic segmentation lack the ability to discover richer semantic information beyond per-point local features.

Method: Introduces LogoSP, which leverages both local and global point features by grouping superpoints based on frequency domain patterns to generate semantic pseudo-labels.

Result: LogoSP outperforms existing unsupervised methods significantly on multiple indoor and outdoor datasets, achieving state-of-the-art performance.

Conclusion: LogoSP successfully demonstrates that global patterns learned without human labels represent meaningful 3D semantics, enhancing segmentation performance.

Abstract: We study the problem of unsupervised 3D semantic segmentation on raw point
clouds without needing human labels in training. Existing methods usually
formulate this problem into learning per-point local features followed by a
simple grouping strategy, lacking the ability to discover additional and
possibly richer semantic priors beyond local features. In this paper, we
introduce LogoSP to learn 3D semantics from both local and global point
features. The key to our approach is to discover 3D semantic information by
grouping superpoints according to their global patterns in the frequency
domain, thus generating highly accurate semantic pseudo-labels for training a
segmentation network. Extensive experiments on two indoor and an outdoor
datasets show that our LogoSP surpasses all existing unsupervised methods by
large margins, achieving the state-of-the-art performance for unsupervised 3D
semantic segmentation. Notably, our investigation into the learned global
patterns reveals that they truly represent meaningful 3D semantics in the
absence of human labels during training.

</details>


### [291] [UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning](https://arxiv.org/abs/2506.07087)
*Weiqi Yan,Lvhai Chen,Huaijia Kou,Shengchuan Zhang,Yan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: The paper introduces UCOD-DPL, a teacher-student framework for unsupervised camouflaged object detection that improves pseudo-label generation and decoding, resulting in remarkable performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing unsupervised camouflaged object detection methods, which suffer from noisy pseudo-labels and simplistic decoding, leading to poor performance especially for small-sized objects.

Method: The proposed UCOD-DPL employs an Adaptive Pseudo-label Module for dynamic pseudo-label refinement, a Dual-Branch Adversarial decoder for advanced semantic learning, and a Look-Twice mechanism for refining small-sized object detection.

Result: Extensive experiments demonstrate UCOD-DPL's exceptional performance, surpassing some fully-supervised models in camouflaged object detection tasks.

Conclusion: UCOD-DPL provides a more robust and accurate framework for unsupervised camouflaged object detection, addressing previous limitations while achieving state-of-the-art results.

Abstract: Unsupervised Camoflaged Object Detection (UCOD) has gained attention since it
doesn't need to rely on extensive pixel-level labels. Existing UCOD methods
typically generate pseudo-labels using fixed strategies and train 1 x1
convolutional layers as a simple decoder, leading to low performance compared
to fully-supervised methods. We emphasize two drawbacks in these approaches:
1). The model is prone to fitting incorrect knowledge due to the pseudo-label
containing substantial noise. 2). The simple decoder fails to capture and learn
the semantic features of camouflaged objects, especially for small-sized
objects, due to the low-resolution pseudo-labels and severe confusion between
foreground and background pixels. To this end, we propose a UCOD method with a
teacher-student framework via Dynamic Pseudo-label Learning called UCOD-DPL,
which contains an Adaptive Pseudo-label Module (APM), a Dual-Branch Adversarial
(DBA) decoder, and a Look-Twice mechanism. The APM module adaptively combines
pseudo-labels generated by fixed strategies and the teacher model to prevent
the model from overfitting incorrect knowledge while preserving the ability for
self-correction; the DBA decoder takes adversarial learning of different
segmentation objectives, guides the model to overcome the foreground-background
confusion of camouflaged objects, and the Look-Twice mechanism mimics the human
tendency to zoom in on camouflaged objects and performs secondary refinement on
small-sized objects. Extensive experiments show that our method demonstrates
outstanding performance, even surpassing some existing fully supervised
methods. The code is available now.

</details>


### [292] [FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity](https://arxiv.org/abs/2506.07865)
*Jinxi Li,Ziyang Song,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: The paper introduces FreeGave, a method to model complex 3D dynamic scenes purely from multi-view videos without requiring object priors or relying on PINN losses.


<details>
  <summary>Details</summary>
Motivation: Current methods often fail to learn complex physical motions at boundaries and require object-specific priors, limiting their application to diverse scenes.

Method: FreeGave uses physics codes and a custom divergence-free module to estimate per-Gaussian velocity fields and avoids inefficient PINN losses.

Result: Experiments across three public datasets and a challenging real-world dataset show FreeGave excels in future frame extrapolation and motion segmentation.

Conclusion: FreeGave effectively learns meaningful 3D physical motion patterns directly from video data, without human labels or prior object knowledge.

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the
underlying physics purely from multi-view videos. By applying various governing
PDEs as PINN losses or incorporating physics simulation into neural networks,
existing works often fail to learn complex physical motions at boundaries or
require object priors such as masks or types. In this paper, we propose
FreeGave to learn the physics of complex dynamic 3D scenes without needing any
object priors. The key to our approach is to introduce a physics code followed
by a carefully designed divergence-free module for estimating a per-Gaussian
velocity field, without relying on the inefficient PINN losses. Extensive
experiments on three public datasets and a newly collected challenging
real-world dataset demonstrate the superior performance of our method for
future frame extrapolation and motion segmentation. Most notably, our
investigation into the learned physics codes reveals that they truly learn
meaningful 3D physical motion patterns in the absence of any human labels in
training.

</details>


### [293] [SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation with Latent Consistency Model](https://arxiv.org/abs/2506.07091)
*Yangkai Lin,Jiabao Lei,Kui Jia*

Main category: cs.CV

TL;DR: The paper introduces SceneLCM, a framework for generating complex, interactive indoor scenes using language models and optimization techniques, overcoming limitations of previous approaches such as rigidity and inefficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods for indoor scene synthesis face issues like rigid editing constraints, single-room limitations, physical incoherence, and excessive human effort, which necessitate improvement.

Method: The framework combines Large Language Models for spatial reasoning with a Latent Consistency Model for optimization. It includes four pipelines: layout generation via LLM-guided reasoning, furniture generation using CTS loss, environment texture optimization through multiresolution fields, and physical editing with simulations.

Result: SceneLCM demonstrates better performance than state-of-the-art methods in generating high-quality, semantically rich, and physically coherent indoor scenes.

Conclusion: By addressing constraints and optimizing processes, SceneLCM showcases its potential for applications in automated indoor scene synthesis, achieving improvements in usability and quality.

Abstract: Our project page: https://scutyklin.github.io/SceneLCM/. Automated generation
of complex, interactive indoor scenes tailored to user prompt remains a
formidable challenge. While existing methods achieve indoor scene synthesis,
they struggle with rigid editing constraints, physical incoherence, excessive
human effort, single-room limitations, and suboptimal material quality. To
address these limitations, we propose SceneLCM, an end-to-end framework that
synergizes Large Language Model (LLM) for layout design with Latent Consistency
Model(LCM) for scene optimization. Our approach decomposes scene generation
into four modular pipelines: (1) Layout Generation. We employ LLM-guided 3D
spatial reasoning to convert textual descriptions into parametric blueprints(3D
layout). And an iterative programmatic validation mechanism iteratively refines
layout parameters through LLM-mediated dialogue loops; (2) Furniture
Generation. SceneLCM employs Consistency Trajectory Sampling(CTS), a
consistency distillation sampling loss guided by LCM, to form fast,
semantically rich, and high-quality representations. We also offer two
theoretical justification to demonstrate that our CTS loss is equivalent to
consistency loss and its distillation error is bounded by the truncation error
of the Euler solver; (3) Environment Optimization. We use a multiresolution
texture field to encode the appearance of the scene, and optimize via CTS loss.
To maintain cross-geometric texture coherence, we introduce a normal-aware
cross-attention decoder to predict RGB by cross-attending to the anchors
locations in geometrically heterogeneous instance. (4)Physically Editing.
SceneLCM supports physically editing by integrating physical simulation,
achieved persistent physical realism. Extensive experiments validate SceneLCM's
superiority over state-of-the-art techniques, showing its wide-ranging
potential for diverse applications.

</details>


### [294] [UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References](https://arxiv.org/abs/2506.07996)
*Ming-Feng Li,Xin Yang,Fu-En Wang,Hritam Basak,Yuyin Sun,Shreekant Gayaka,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: UA-Pose is an uncertainty-aware method for 6D object pose estimation that performs well with partial object references by integrating uncertainty into incomplete 3D models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve 6D object pose estimation for scenarios where only partial references of objects are available, as current methods depend on complete 3D models or extensive reference images, which are often impractical.

Method: UA-Pose incorporates uncertainty into incomplete 3D object models to distinguish between seen and unseen regions. It utilizes uncertainty for confidence assessment in pose estimation and employs an uncertainty-aware sampling strategy for online object completion.

Result: The method significantly outperforms existing approaches in scenarios with incomplete or partial object observations, as demonstrated on datasets like YCB-Video, YCBInEOAT, and HO3D.

Conclusion: UA-Pose improves pose estimation accuracy and object model completeness by leveraging uncertainty in situations where complete object information is unavailable.

Abstract: 6D object pose estimation has shown strong generalizability to novel objects.
However, existing methods often require either a complete, well-reconstructed
3D model or numerous reference images that fully cover the object. Estimating
6D poses from partial references, which capture only fragments of an object's
appearance and geometry, remains challenging. To address this, we propose
UA-Pose, an uncertainty-aware approach for 6D object pose estimation and online
object completion specifically designed for partial references. We assume
access to either (1) a limited set of RGBD images with known poses or (2) a
single 2D image. For the first case, we initialize a partial object 3D model
based on the provided images and poses, while for the second, we use
image-to-3D techniques to generate an initial object 3D model. Our method
integrates uncertainty into the incomplete 3D model, distinguishing between
seen and unseen regions. This uncertainty enables confidence assessment in pose
estimation and guides an uncertainty-aware sampling strategy for online object
completion, enhancing robustness in pose estimation accuracy and improving
object completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and
HO3D datasets, including RGBD sequences of YCB objects manipulated by robots
and human hands. Experimental results demonstrate significant performance
improvements over existing methods, particularly when object observations are
incomplete or partially captured. Project page:
https://minfenli.github.io/UA-Pose/

</details>


### [295] [EdgeSpotter: Multi-Scale Dense Text Spotting for Industrial Panel Monitoring](https://arxiv.org/abs/2506.07112)
*Changhong Fu,Hua Lin,Haobo Zuo,Liangliang Yao,Liguo Zhang*

Main category: cs.CV

TL;DR: The paper proposes EdgeSpotter, a novel model for text spotting on industrial panels using multi-scale features and a new dataset, achieving improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the challenges of accurate and efficient text spotting on industrial panels, particularly dealing with issues like cross-scale localization, dense text regions, and the lack of exploration in multi-scale feature representation.

Method: The proposed method introduces EdgeSpotter, utilizing a novel Transformer with efficient mixers to learn multi-level feature interdependencies and a feature sampling approach based on Catmull-Rom splines to enhance text spot accuracy.

Result: Extensive evaluations on a newly constructed benchmark dataset for industrial panel monitoring (IPM) demonstrate the superior performance of EdgeSpotter in challenging scenarios.

Conclusion: EdgeSpotter achieves practical and robust text spotting for industrial panels with a high degree of accuracy, supplemented by a newly created dataset and effective feature analysis tools.

Abstract: Text spotting for industrial panels is a key task for intelligent monitoring.
However, achieving efficient and accurate text spotting for complex industrial
panels remains challenging due to issues such as cross-scale localization and
ambiguous boundaries in dense text regions. Moreover, most existing methods
primarily focus on representing a single text shape, neglecting a comprehensive
exploration of multi-scale feature information across different texts. To
address these issues, this work proposes a novel multi-scale dense text spotter
for edge AI-based vision system (EdgeSpotter) to achieve accurate and robust
industrial panel monitoring. Specifically, a novel Transformer with efficient
mixer is developed to learn the interdependencies among multi-level features,
integrating multi-layer spatial and semantic cues. In addition, a new feature
sampling with catmull-rom splines is designed, which explicitly encodes the
shape, position, and semantic information of text, thereby alleviating missed
detections and reducing recognition errors caused by multi-scale or dense text
regions. Furthermore, a new benchmark dataset for industrial panel monitoring
(IPM) is constructed. Extensive qualitative and quantitative evaluations on
this challenging benchmark dataset validate the superior performance of the
proposed method in different challenging panel monitoring tasks. Finally,
practical tests based on the self-designed edge AI-based vision system
demonstrate the practicality of the method. The code and demo will be available
at https://github.com/vision4robotics/EdgeSpotter.

</details>


### [296] [Image segmentation and classification of E-waste for waste segregation](https://arxiv.org/abs/2506.07122)
*Prakriti Tripathi,Theertha Biju,Maniram Thota,Rakesh Lingam*

Main category: cs.CV

TL;DR: The paper discusses classifying e-waste using machine learning models for robotic waste segregation, achieving 70 mAP with YOLOv11 and 41 mAP with Mask-RCNN.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of automating e-waste segregation using machine learning models integrated with robots to enhance efficiency in recycling.

Method: A custom dataset from dismantled e-waste items was created. Machine learning models YOLOv11 and Mask-RCNN were trained for classification tasks.

Result: YOLOv11 achieved 70 mAP in real-time performance, while Mask-RCNN reached 41 mAP performance.

Conclusion: The trained models demonstrate potential for automating e-waste segregation in conjunction with robotic systems, though further integration and optimization are planned.

Abstract: Industry partners provided a problem statement that involves classifying
electronic waste using machine learning models that will be used by
pick-and-place robots for waste segregation. We started by taking common
electronic waste items, such as a mouse and charger, unsoldering them, and
taking pictures to create a custom dataset. Then state-of-the art YOLOv11 model
was trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also
trained and achieved 41 mAP. The model will be further integrated with
pick-and-place robots to perform segregation of e-waste.

</details>


### [297] [Hi-VAE: Efficient Video Autoencoding with Global and Detailed Motion](https://arxiv.org/abs/2506.07136)
*Huaize Liu,Wenzhang Sun,Qiyuan Zhang,Donglin Di,Biao Gong,Hao Li,Chen Wei,Changqing Zou*

Main category: cs.CV

TL;DR: Hi-VAE is a novel video autoencoder that achieves 1428× compression, significantly outperforming previous methods while maintaining high video reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Existing video autoencoders fail to effectively model spatio-temporal redundancies in video dynamics, leading to suboptimal compression and high downstream training costs.

Method: Hi-VAE hierarchically encodes video dynamics into Global and Detailed Motion latent spaces using self-supervised motion encoders. A conditional diffusion decoder reconstructs the videos using these hierarchical representations.

Result: Hi-VAE achieves an effective compression factor of 1428×, nearly 30× higher than existing baseline methods, while maintaining high-quality reconstruction and excelling in downstream generative tasks.

Conclusion: Hi-VAE demonstrates superior compression capabilities, reconstruction quality, and scalability compared to existing methods, offering substantial advancements in video autoencoding and latent representation modeling.

Abstract: Recent breakthroughs in video autoencoders (Video AEs) have advanced video
generation, but existing methods fail to efficiently model spatio-temporal
redundancies in dynamics, resulting in suboptimal compression factors. This
shortfall leads to excessive training costs for downstream tasks. To address
this, we introduce Hi-VAE, an efficient video autoencoding framework that
hierarchically encode coarse-to-fine motion representations of video dynamics
and formulate the decoding process as a conditional generation task.
Specifically, Hi-VAE decomposes video dynamics into two latent spaces: Global
Motion, capturing overarching motion patterns, and Detailed Motion, encoding
high-frequency spatial details. Using separate self-supervised motion encoders,
we compress video latents into compact motion representations to reduce
redundancy significantly. A conditional diffusion decoder then reconstructs
videos by combining hierarchical global and detailed motions, enabling
high-fidelity video reconstructions. Extensive experiments demonstrate that
Hi-VAE achieves a high compression factor of 1428$\times$, almost 30$\times$
higher than baseline methods (e.g., Cosmos-VAE at 48$\times$), validating the
efficiency of our approach. Meanwhile, Hi-VAE maintains high reconstruction
quality at such high compression rates and performs effectively in downstream
generative tasks. Moreover, Hi-VAE exhibits interpretability and scalability,
providing new perspectives for future exploration in video latent
representation and generation.

</details>


### [298] [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
*Hao Tang,Chengchao Shen*

Main category: cs.CV

TL;DR: This paper proposes Spatial Token Fusion (STF) and Multi-Block Token Fusion (MBTF) methods to reduce vision token sequences for large multimodal models, achieving faster inference while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Large multimodal models face computational challenges due to the high cost of large language models and the complexity of processing long vision token sequences, necessitating methods to improve inference efficiency.

Method: The paper introduces STF to fuse spatially-adjacent tokens into compact vision tokens and MBTF to supplement multi-granularity features, enabling token reduction while preserving information content.

Result: The authors demonstrate that their approach, based on LLaVA-1.5, achieves performance comparable or superior to the baseline on 8 vision-language benchmarks while using only 25% of the vision tokens.

Conclusion: Combining STF and MBTF effectively reduces computational demands without compromising multimodal reasoning, offering an efficient solution for large multimodal models.

Abstract: Large multimodal models (LMMs) suffer significant computational challenges
due to the high cost of Large Language Models (LLMs) and the quadratic
complexity of processing long vision token sequences. In this paper, we explore
the spatial redundancy among vision tokens and shorten the length of vision
token sequences for inference acceleration. Specifically, we propose a Spatial
Token Fusion (STF) method to learn compact vision tokens for short vision token
sequence, where spatial-adjacent tokens are fused into one. Meanwhile,
weight-frozen vision encoder can not well adapt to the demand of extensive
downstream vision-language tasks. To this end, we further introduce a
Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features
for the reduced token sequence. Overall, we combine STF and MBTF module to
balance token reduction and information preservation, thereby improving
inference efficiency without sacrificing multimodal reasoning capabilities.
Experimental results demonstrate that our method based on LLaVA-1.5 achieves
comparable or even superior performance to the baseline on 8 popular
vision-language benchmarks with only $25\%$ vision tokens of baseline. The
source code and trained weights are available at
https://github.com/visresearch/LLaVA-STF.

</details>


### [299] [GoTrack: Generic 6DoF Object Pose Refinement and Tracking](https://arxiv.org/abs/2506.07155)
*Van Nguyen Nguyen,Christian Forster,Sindi Shkodrani,Vincent Lepetit,Bugra Tekin,Cem Keskin,Tomas Hodan*

Main category: cs.CV

TL;DR: GoTrack is an efficient 6DoF object pose refinement and tracking method that uses CAD-based techniques and integrates both model-to-frame and frame-to-frame registration using optical flow, achieving state-of-the-art RGB-only results.


<details>
  <summary>Details</summary>
Motivation: Current 6DoF object pose tracking methods rely heavily on model-to-frame registration, which is compute-intensive and unstable; there's a need for more efficient and robust methods.

Method: GoTrack integrates both model-to-frame registration using a transformer trained on DINOv2 and frame-to-frame registration using lightweight optical flow, leveraging both approaches to optimize tracking.

Result: GoTrack achieves state-of-the-art performance for RGB-only 6DoF object pose estimation and tracking on standard benchmarks, combining coarse pose estimation methods in a minimal pipeline.

Conclusion: The proposed GoTrack method offers a robust and efficient approach to 6DoF pose tracking, eliminating the need for object-specific training and outperforming existing methods, with open-source code available for further research and application.

Abstract: We introduce GoTrack, an efficient and accurate CAD-based method for 6DoF
object pose refinement and tracking, which can handle diverse objects without
any object-specific training. Unlike existing tracking methods that rely solely
on an analysis-by-synthesis approach for model-to-frame registration, GoTrack
additionally integrates frame-to-frame registration, which saves compute and
stabilizes tracking. Both types of registration are realized by optical flow
estimation. The model-to-frame registration is noticeably simpler than in
existing methods, relying only on standard neural network blocks (a transformer
is trained on top of DINOv2) and producing reliable pose confidence scores
without a scoring network. For the frame-to-frame registration, which is an
easier problem as consecutive video frames are typically nearly identical, we
employ a light off-the-shelf optical flow model. We demonstrate that GoTrack
can be seamlessly combined with existing coarse pose estimation methods to
create a minimal pipeline that reaches state-of-the-art RGB-only results on
standard benchmarks for 6DoF object pose estimation and tracking. Our source
code and trained models are publicly available at
https://github.com/facebookresearch/gotrack

</details>


### [300] [Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs](https://arxiv.org/abs/2506.07164)
*Qiong Chang,Xinyuan Chen,Xiang Li,Weimin Wang,Jun Miyazaki*

Main category: cs.CV

TL;DR: The paper proposes GPU-based optimizations to accelerate Oriented FAST feature detection in visual SLAM, achieving a 7.3x speedup.


<details>
  <summary>Details</summary>
Motivation: Current ORB-based SLAM systems are not fast enough for real-time mobile platform usage because Oriented FAST computations are too time-intensive.

Method: The authors propose a binary-level encoding strategy for faster candidate point selection and a separable Harris detection strategy using GPU-specific optimization.

Result: Experiments on Jetson TX2 showed a 7.3x speed improvement over the OpenCV GPU implementation.

Conclusion: The proposed methods significantly improve processing speeds, making them suitable for real-time mobile and embedded applications.

Abstract: The visual-based SLAM (Simultaneous Localization and Mapping) is a technology
widely used in applications such as robotic navigation and virtual reality,
which primarily focuses on detecting feature points from visual images to
construct an unknown environmental map and simultaneously determines its own
location. It usually imposes stringent requirements on hardware power
consumption, processing speed and accuracy. Currently, the ORB (Oriented FAST
and Rotated BRIEF)-based SLAM systems have exhibited superior performance in
terms of processing speed and robustness. However, they still fall short of
meeting the demands for real-time processing on mobile platforms. This
limitation is primarily due to the time-consuming Oriented FAST calculations
accounting for approximately half of the entire SLAM system. This paper
presents two methods to accelerate the Oriented FAST feature detection on
low-end embedded GPUs. These methods optimize the most time-consuming steps in
Oriented FAST feature detection: FAST feature point detection and Harris corner
detection, which is achieved by implementing a binary-level encoding strategy
to determine candidate points quickly and a separable Harris detection strategy
with efficient low-level GPU hardware-specific instructions. Extensive
experiments on a Jetson TX2 embedded GPU demonstrate an average speedup of over
7.3 times compared to widely used OpenCV with GPU support. This significant
improvement highlights its effectiveness and potential for real-time
applications in mobile and resource-constrained environments.

</details>


### [301] [Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models](https://arxiv.org/abs/2506.07177)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Jaehong Yoon,Soo Ye Kim,Zhe Lin,Sung Ju Hwang*

Main category: cs.CV

TL;DR: Frame Guidance introduces a training-free method utilizing frame-level signals (e.g., keyframes, sketches, or depth maps) for controllable video generation, eliminating the need for fine-tuning large models.


<details>
  <summary>Details</summary>
Motivation: Existing video generation methods often require fine-tuning large models for specific tasks, which is becoming increasingly impractical due to growing model sizes.

Method: Frame Guidance leverages frame-level signals and a simple latent processing approach that reduces memory usage. It employs a latent optimization strategy to ensure globally coherent video generation.

Result: Experimental results demonstrate Frame Guidance successfully achieving high-quality controlled video outputs across diverse tasks such as stylization, looping, and keyframe guidance.

Conclusion: Frame Guidance is a versatile technique capable of generating high-quality controllable videos across various tasks without training, compatible with any video model.

Abstract: Advancements in diffusion models have significantly improved video quality,
directing attention to fine-grained controllability. However, many existing
methods depend on fine-tuning large-scale video models for specific tasks,
which becomes increasingly impractical as model sizes continue to grow. In this
work, we present Frame Guidance, a training-free guidance for controllable
video generation based on frame-level signals, such as keyframes, style
reference images, sketches, or depth maps. For practical training-free
guidance, we propose a simple latent processing method that dramatically
reduces memory usage, and apply a novel latent optimization strategy designed
for globally coherent video generation. Frame Guidance enables effective
control across diverse tasks, including keyframe guidance, stylization, and
looping, without any training, compatible with any video models. Experimental
results show that Frame Guidance can produce high-quality controlled videos for
a wide range of tasks and input signals.

</details>


### [302] [Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks](https://arxiv.org/abs/2506.07188)
*Ni Ding,Lei He,Shengbo Eben Li,Keqiang Li*

Main category: cs.CV

TL;DR: The paper presents a novel hierarchical framework for training pretrained neural networks in end-to-end autonomous driving by introducing independent feature-level training, resulting in improved transparency and efficiency.


<details>
  <summary>Details</summary>
Motivation: End-to-end autonomous driving approaches often lack interpretability and pose safety concerns due to their black-box nature. This paper aims to improve model transparency and training methods by addressing these challenges.

Method: The authors propose a hierarchical post-training framework that introduces surrogate supervisory signals. It allows independent training of components and formalizes reverse computation as optimization problems, using systems of linear equations or least squares.

Result: The framework was tested on standard image classification benchmarks, showing superior generalization performance and computational efficiency compared to conventional approaches.

Conclusion: This method establishes a new paradigm for neural network training, improving safety and interpretability while maintaining competitive performance.

Abstract: End-to-end autonomous driving has emerged as a dominant paradigm, yet its
highly entangled black-box models pose significant challenges in terms of
interpretability and safety assurance. To improve model transparency and
training flexibility, this paper proposes a hierarchical and decoupled
post-training framework tailored for pretrained neural networks. By
reconstructing intermediate feature maps from ground-truth labels, surrogate
supervisory signals are introduced at transitional layers to enable independent
training of specific components, thereby avoiding the complexity and coupling
of conventional end-to-end backpropagation and providing interpretable insights
into networks' internal mechanisms. To the best of our knowledge, this is the
first method to formalize feature-level reverse computation as well-posed
optimization problems, which we rigorously reformulate as systems of linear
equations or least squares problems. This establishes a novel and efficient
training paradigm that extends gradient backpropagation to feature
backpropagation. Extensive experiments on multiple standard image
classification benchmarks demonstrate that the proposed method achieves
superior generalization performance and computational efficiency compared to
traditional training approaches, validating its effectiveness and potential.

</details>


### [303] [SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning](https://arxiv.org/abs/2506.07196)
*Mengya Xu,Zhongzhen Huang,Dillan Imans,Yiru Ye,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: The paper introduces SAP-Bench, a benchmark dataset for evaluating multimodal large language models (MLLMs) in surgical action planning. It focuses on predicting future actions in life-critical surgical procedures.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack the ability to evaluate precise decision-making skills of MLLMs in complex surgical tasks that demand reliability and patient safety.

Method: The authors created SAP-Bench, a large-scale dataset with clinically validated temporal annotations, focusing on multimodal analysis for cholecystectomy procedures. They propose the MLLM-SAP framework to predict next actions.

Result: SAP-Bench dataset contains annotated data from 74 procedures with 1,226 action clips and 1,152 multimodal frame-action pairs. Seven state-of-the-art MLLMs are evaluated, exposing gaps in next action prediction.

Conclusion: SAP-Bench provides a rigorous benchmark for advancing MLLMs in life-critical applications like surgical planning, highlighting areas where models need improvement for reliable next-action predictions.

Abstract: Effective evaluation is critical for driving advancements in MLLM research.
The surgical action planning (SAP) task, which aims to generate future action
sequences from visual inputs, demands precise and sophisticated analytical
capabilities. Unlike mathematical reasoning, surgical decision-making operates
in life-critical domains and requires meticulous, verifiable processes to
ensure reliability and patient safety. This task demands the ability to
distinguish between atomic visual actions and coordinate complex, long-horizon
procedures, capabilities that are inadequately evaluated by current benchmarks.
To address this gap, we introduce SAP-Bench, a large-scale, high-quality
dataset designed to enable multimodal large language models (MLLMs) to perform
interpretable surgical action planning. Our SAP-Bench benchmark, derived from
the cholecystectomy procedures context with the mean duration of 1137.5s, and
introduces temporally-grounded surgical action annotations, comprising the
1,226 clinically validated action clips (mean duration: 68.7s) capturing five
fundamental surgical actions across 74 procedures. The dataset provides 1,152
strategically sampled current frames, each paired with the corresponding next
action as multimodal analysis anchors. We propose the MLLM-SAP framework that
leverages MLLMs to generate next action recommendations from the current
surgical scene and natural language instructions, enhanced with injected
surgical domain knowledge. To assess our dataset's effectiveness and the
broader capabilities of current models, we evaluate seven state-of-the-art
MLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,
Step-1o, and GLM-4v) and reveal critical gaps in next action prediction
performance.

</details>


### [304] [TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation](https://arxiv.org/abs/2506.07205)
*Min-Jung Kim,Dongjin Kim,Seokju Yun,Jaegul Choo*

Main category: cs.CV

TL;DR: This paper introduces TV-LiVE, a system for text-guided video editing without training, focusing on object addition and non-rigid transformations. It utilizes key layers influencing video generation quality.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the growing demand for accessible and controllable video editing methods, especially for complex tasks like novel object addition and non-rigid transformations.

Method: TV-LiVE identifies key layers within video generation models, utilizing Rotary Position Embeddings (RoPE). It selectively injects features to enable object addition and non-rigid editing while using extracted mask regions for accurate edits.

Result: Experimental results show TV-LiVE surpasses existing techniques in object addition and non-rigid video editing tasks, delivering higher quality outputs.

Conclusion: TV-LiVE is a training-free framework that effectively improves text-guided video editing capabilities, paving the way for more flexible and advanced video transformations.

Abstract: Video editing has garnered increasing attention alongside the rapid progress
of diffusion-based video generation models. As part of these advancements,
there is a growing demand for more accessible and controllable forms of video
editing, such as prompt-based editing. Previous studies have primarily focused
on tasks such as style transfer, background replacement, object substitution,
and attribute modification, while maintaining the content structure of the
source video. However, more complex tasks, including the addition of novel
objects and nonrigid transformations, remain relatively unexplored. In this
paper, we present TV-LiVE, a Training-free and text-guided Video editing
framework via Layerinformed Vitality Exploitation. We empirically identify
vital layers within the video generation model that significantly influence the
quality of generated outputs. Notably, these layers are closely associated with
Rotary Position Embeddings (RoPE). Based on this observation, our method
enables both object addition and non-rigid video editing by selectively
injecting key and value features from the source model into the corresponding
layers of the target model guided by the layer vitality. For object addition,
we further identify prominent layers to extract the mask regions corresponding
to the newly added target prompt. We found that the extracted masks from the
prominent layers faithfully indicate the region to be edited. Experimental
results demonstrate that TV-LiVE outperforms existing approaches for both
object addition and non-rigid video editing. Project Page:
https://emjay73.github.io/TV_LiVE/

</details>


### [305] [Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning](https://arxiv.org/abs/2309.11082)
*Chen Jiang,Hong Liu,Xuzheng Yu,Qing Wang,Yuan Cheng,Jia Xu,Zhongyi Liu,Qingpei Guo,Wei Chu,Ming Yang,Yuan Qi*

Main category: cs.CV

TL;DR: This paper proposes two innovative techniques to improve text-video retrieval by enhancing hard negative handling and semantic similarity modeling, demonstrating superiority over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing contrastive learning approaches in text-video retrieval, specifically their insufficient handling of hard negatives and inability to model nuanced semantic similarities.

Method: The authors introduce Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs effectively and a Negative-aware InfoNCE (NegNCE) loss to emphasize their contribution in training. Additionally, they propose the Triplet Partial Margin Contrastive Learning (TPM-CL) module to leverage triplet samples for better semantic similarity modeling, incorporating an adaptive token masking strategy.

Result: The approach achieves state-of-the-art performance across four major text-video retrieval datasets: MSR-VTT, MSVD, DiDeMo, and ActivityNet.

Conclusion: The proposed techniques significantly improve cross-modal learning for text-video retrieval by addressing two core challenges—hard negative handling and semantic similarity modeling—enhancing retrieval accuracy.

Abstract: In recent years, the explosion of web videos makes text-video retrieval
increasingly essential and popular for video filtering, recommendation, and
search. Text-video retrieval aims to rank relevant text/video higher than
irrelevant ones. The core of this task is to precisely measure the cross-modal
similarity between texts and videos. Recently, contrastive learning methods
have shown promising results for text-video retrieval, most of which focus on
the construction of positive and negative pairs to learn text and video
representations. Nevertheless, they do not pay enough attention to hard
negative pairs and lack the ability to model different levels of semantic
similarity. To address these two issues, this paper improves contrastive
learning using two novel techniques. First, to exploit hard examples for robust
discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module
(DMAE) to mine hard negative pairs from textual and visual clues. By further
introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively
identify all these hard negatives and explicitly highlight their impacts in the
training loss. Second, our work argues that triplet samples can better model
fine-grained semantic similarity compared to pairwise samples. We thereby
present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to
construct partial order triplet samples by automatically generating
fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL
designs an adaptive token masking strategy with cross-modal interaction to
model subtle semantic differences. Extensive experiments demonstrate that the
proposed approach outperforms existing methods on four widely-used text-video
retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.

</details>


### [306] [Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation](https://arxiv.org/abs/2506.07214)
*Zhiyuan Zhong,Zhen Sun,Yepang Liu,Xinlei He,Guanhong Tao*

Main category: cs.CV

TL;DR: The paper introduces a backdoor attack method, BadSem, for Vision Language Models (VLMs) leveraging cross-modal semantic mismatches. It achieves over 98% attack success rate and resists defense strategies.


<details>
  <summary>Details</summary>
Motivation: VLMs are powerful but vulnerable to backdoor attacks. Prior methods focus on single-modality triggers, ignoring the cross-modal nature of VLMs. The authors identify semantic mismatches between modalities as an underexplored vulnerability.

Method: The proposed BadSem attack uses data poisoning during model training by misaligning image-text pairs. A dataset named SIMBad was also created for targeted semantic manipulation involving image and text attributes like color and objects.

Result: Experiments demonstrate that BadSem achieves over 98% average attack success rate (ASR). The attack generalizes well to out-of-distribution settings and across modalities while avoiding detection.

Conclusion: BadSem exposes a critical semantic vulnerability in VLMs. Existing defense strategies such as system prompts and supervised fine-tuning fail to mitigate this attack, thus highlighting the need for new defense mechanisms.

Abstract: Vision Language Models (VLMs) have shown remarkable performance, but are also
vulnerable to backdoor attacks whereby the adversary can manipulate the model's
outputs through hidden triggers. Prior attacks primarily rely on
single-modality triggers, leaving the crucial cross-modal fusion nature of VLMs
largely unexplored. Unlike prior work, we identify a novel attack surface that
leverages cross-modal semantic mismatches as implicit triggers. Based on this
insight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data
poisoning attack that injects stealthy backdoors by deliberately misaligning
image-text pairs during training. To perform the attack, we construct SIMBad, a
dataset tailored for semantic manipulation involving color and object
attributes. Extensive experiments across four widely used VLMs show that BadSem
achieves over 98% average ASR, generalizes well to out-of-distribution
datasets, and can transfer across poisoning modalities. Our detailed analysis
using attention visualization shows that backdoored models focus on
semantically sensitive regions under mismatched conditions while maintaining
normal behavior on clean inputs. To mitigate the attack, we try two defense
strategies based on system prompt and supervised fine-tuning but find that both
of them fail to mitigate the semantic backdoor. Our findings highlight the
urgent need to address semantic vulnerabilities in VLMs for their safer
deployment.

</details>


### [307] [MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4](https://arxiv.org/abs/2406.00971)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.CV

TL;DR: This paper adapts MiniGPT-4, a Vision-Language Model, to handle the complex task of reverse designing, which predicts edits and parameters from pairs of images and optional text.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the potential of Vision-Language Models (VLMs) like MiniGPT-4 in tackling intricate tasks like reverse designing, which integrates visual and textual understanding beyond standard tasks.

Method: The researchers fine-tune MiniGPT-4 to process a source image, its edited counterpart, and optional textual descriptions to predict edits and their parameters.

Result: The experiments demonstrate that MiniGPT-4 can effectively extend its capability to address the complex reverse designing task.

Conclusion: Fine-tuned VLMs like MiniGPT-4 show potential for complex multi-modal tasks, revealing their extensibility beyond conventional applications.

Abstract: Vision-Language Models (VLMs) have recently seen significant advancements
through integrating with Large Language Models (LLMs). The VLMs, which process
image and text modalities simultaneously, have demonstrated the ability to
learn and understand the interaction between images and texts across various
multi-modal tasks. Reverse designing, which could be defined as a complex
vision-language task, aims to predict the edits and their parameters, given a
source image, an edited version, and an optional high-level textual edit
description. This task requires VLMs to comprehend the interplay between the
source image, the edited version, and the optional textual context
simultaneously, going beyond traditional vision-language tasks. In this paper,
we extend and fine-tune MiniGPT-4 for the reverse designing task. Our
experiments demonstrate the extensibility of off-the-shelf VLMs, specifically
MiniGPT-4, for more complex tasks such as reverse designing. Code is available
at this \href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}

</details>


### [308] [AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?](https://arxiv.org/abs/2506.07216)
*Nada Aboudeshish,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: This paper introduces a data augmentation framework for skeleton-based datasets, incorporating geometric and intensity-based transformations to improve gesture recognition across various architectures.


<details>
  <summary>Details</summary>
Motivation: Limited diversity in skeleton-based datasets necessitates advanced data augmentation techniques to enhance deep learning model performance.

Method: Utilizes geometric transformations, random cropping, rotation, zooming, and intensity-based adjustments to quadruple dataset size and enrich its diversity.

Result: Demonstrates augmented model performance on three HGR architectures (e2eET, FPPR-PCD, DD-Net) across benchmark datasets like DHG14/28, SHREC'17, and JHMDB.

Conclusion: The proposed framework significantly improves generalization, establishes state-of-the-art results, and presents a scalable solution for real-world gesture and action recognition tasks.

Abstract: Data augmentation is a crucial technique in deep learning, particularly for
tasks with limited dataset diversity, such as skeleton-based datasets. This
paper proposes a comprehensive data augmentation framework that integrates
geometric transformations, random cropping, rotation, zooming and
intensity-based transformations, brightness and contrast adjustments to
simulate real-world variations. Random cropping ensures the preservation of
spatio-temporal integrity while addressing challenges such as viewpoint bias
and occlusions. The augmentation pipeline generates three augmented versions
for each sample in addition to the data set sample, thus quadrupling the data
set size and enriching the diversity of gesture representations. The proposed
augmentation strategy is evaluated on three models: multi-stream e2eET, FPPR
point cloud-based hand gesture recognition (HGR), and DD-Network. Experiments
are conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB.
The e2eET model, recognized as the state-of-the-art for hand gesture
recognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best
performing model on SHREC'17, excels in point cloud-based gesture recognition.
DD-Net, a lightweight and efficient architecture for skeleton-based action
recognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB).
The results underline the effectiveness and versatility of the proposed
augmentation strategy, significantly improving model generalization and
robustness across diverse datasets and architectures. This framework not only
establishes state-of-the-art results on all three evaluated models but also
offers a scalable solution to advance HGR and action recognition applications
in real-world scenarios. The framework is available at
https://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR

</details>


### [309] [Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning](https://arxiv.org/abs/2506.07227)
*Tianyi Bai,Yuxuan Fan,Jiantao Qiu,Fupeng Sun,Jiayi Song,Junlin Han,Zichen Liu,Conghui He,Wentao Zhang,Binhang Yuan*

Main category: cs.CV

TL;DR: The paper introduces a new dataset and framework to improve fine-grained visual reasoning in multimodal large language models (MLLMs) by aligning small visual differences with captions.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with recognizing fine-grained visual differences, leading to hallucinations or missed semantic shifts, due to limitations in training data and learning objectives.

Method: The authors created the Micro Edit Dataset (MED) with over 50K image-text pairs, generated using a controlled pipeline, and introduced a supervised fine-tuning (SFT) framework with a consistency loss at the feature level to enhance visual embeddings.

Result: Approach demonstrated improvement in detecting minor visual differences, reduced hallucinations relative to baselines like GPT-4o, and showed gains in image captioning and visual question answering tasks.

Conclusion: Targeted datasets and alignment objectives significantly enhance fine-grained visual reasoning capabilities in MLLMs.

Abstract: Multimodal large language models (MLLMs) have achieved strong performance on
vision-language tasks but still struggle with fine-grained visual differences,
leading to hallucinations or missed semantic shifts. We attribute this to
limitations in both training data and learning objectives. To address these
issues, we propose a controlled data generation pipeline that produces
minimally edited image pairs with semantically aligned captions. Using this
pipeline, we construct the Micro Edit Dataset (MED), containing over 50K
image-text pairs spanning 11 fine-grained edit categories, including attribute,
count, position, and object presence changes. Building on MED, we introduce a
supervised fine-tuning (SFT) framework with a feature-level consistency loss
that promotes stable visual embeddings under small edits. We evaluate our
approach on the Micro Edit Detection benchmark, which includes carefully
balanced evaluation pairs designed to test sensitivity to subtle visual
variations across the same edit categories. Our method improves difference
detection accuracy and reduces hallucinations compared to strong baselines,
including GPT-4o. Moreover, it yields consistent gains on standard
vision-language tasks such as image captioning and visual question answering.
These results demonstrate the effectiveness of combining targeted data and
alignment objectives for enhancing fine-grained visual reasoning in MLLMs.

</details>


### [310] [Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification](https://arxiv.org/abs/2506.07235)
*Tianyi Bai,Zengjie Hu,Fupeng Sun,Jiantao Qiu,Yizhen Jiang,Guangxin He,Bohan Zeng,Conghui He,Binhang Yuan,Wentao Zhang*

Main category: cs.CV

TL;DR: The paper introduces a novel framework for dynamic visual token scaling in multi-modal large language models (MLLMs), aiming to enhance adaptive and iterative visual reasoning.


<details>
  <summary>Details</summary>
Motivation: Static inference paradigms in MLLMs restrict their iterative and context-aware reasoning, contrasting human dynamic perception.

Method: The authors propose framing the reasoning task as a Markov Decision Process, utilizing a reasoner to suggest actions and a verifier trained via Direct Preference Optimization (DPO) to evaluate actions and determine stopping points.

Result: The developed model markedly outperforms current techniques across varied visual reasoning benchmarks, showcasing enhanced accuracy and interpretable outputs.

Conclusion: Dynamic inference mechanisms hold great potential for improving context-aware visual reasoning in advanced MLLMs.

Abstract: Multi-modal large language models (MLLMs) have achieved remarkable
capabilities by integrating visual perception with language understanding,
enabling applications such as image-grounded dialogue, visual question
answering, and scientific analysis. However, most MLLMs adopt a static
inference paradigm, encoding the entire image into fixed visual tokens upfront,
which limits their ability to iteratively refine understanding or adapt to
context during inference. This contrasts sharply with human perception, which
is dynamic, selective, and feedback-driven. In this work, we introduce a novel
framework for inference-time visual token scaling that enables MLLMs to perform
iterative, verifier-guided reasoning over visual content. We formulate the
problem as a Markov Decision Process, involving a reasoner that proposes visual
actions and a verifier, which is trained via multi-step Direct Preference
Optimization (DPO), that evaluates these actions and determines when reasoning
should terminate. To support this, we present a new dataset, VTS, comprising
supervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning
comparisons (VTS-DPO). Our method significantly outperforms existing approaches
across diverse visual reasoning benchmarks, offering not only improved accuracy
but also more interpretable and grounded reasoning processes. These results
demonstrate the promise of dynamic inference mechanisms for enabling
fine-grained, context-aware visual reasoning in next-generation MLLMs.

</details>


### [311] [STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis](https://arxiv.org/abs/2506.06276)
*Jiatao Gu,Tianrong Chen,David Berthelot,Huangjie Zheng,Yuyang Wang,Ruixiang Zhang,Laurent Dinh,Miguel Angel Bautista,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: STARFlow leverages Transformer Autoregressive Flow (TARFlow) and innovations like deep-shallow Transformer modeling, autoencoder-based latent space utilization, and advanced guidance algorithms for high-resolution image synthesis, achieving competitive results approaching state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To make normalizing flows scalable and competitive for high-resolution image synthesis while addressing the limitations of existing approaches in sample quality and efficiency.

Method: The paper introduces Transformer Autoregressive Flow (TARFlow) integrated with a deep-shallow Transformer approach, latent space modeling with pretrained autoencoders, and a novel guidance algorithm for enhancing image generation performance.

Result: STARFlow demonstrates competitive performance in class-conditional and text-conditional image generation tasks, producing high-quality samples nearly matching state-of-the-art diffusion models.

Conclusion: The work successfully pushes the boundaries of scalability and performance for normalizing flow models in image synthesis, offering a viable alternative to existing large-scale diffusion models.

Abstract: We present STARFlow, a scalable generative model based on normalizing flows
that achieves strong performance in high-resolution image synthesis. The core
of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the
expressive power of normalizing flows with the structured modeling capabilities
of Autoregressive Transformers. We first establish the theoretical universality
of TARFlow for modeling continuous distributions. Building on this foundation,
we introduce several key architectural and algorithmic innovations to
significantly enhance scalability: (1) a deep-shallow design, wherein a deep
Transformer block captures most of the model representational capacity,
complemented by a few shallow Transformer blocks that are computationally
efficient yet substantially beneficial; (2) modeling in the latent space of
pretrained autoencoders, which proves more effective than direct pixel-level
modeling; and (3) a novel guidance algorithm that significantly boosts sample
quality. Crucially, our model remains an end-to-end normalizing flow, enabling
exact maximum likelihood training in continuous spaces without discretization.
STARFlow achieves competitive performance in both class-conditional and
text-conditional image generation tasks, approaching state-of-the-art diffusion
models in sample quality. To our knowledge, this work is the first successful
demonstration of normalizing flows operating effectively at this scale and
resolution.

</details>


### [312] [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](https://arxiv.org/abs/2506.07280)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: The paper explores how Video Diffusion Models (VDMs) go beyond video generation to serve as adaptable tools for diverse vision tasks by using a lightweight fine-tuning approach.


<details>
  <summary>Details</summary>
Motivation: To investigate if Video Diffusion Models can internalize structured representations of the visual world and be repurposed for various visual reasoning tasks.

Method: A few-shot fine-tuning framework was introduced, transforming each task into visual transitions and training LoRA weights on short sequences without changing the frozen generative interface of the VDM.

Result: VDMs demonstrated strong generalization in tasks ranging from low-level vision (e.g., segmentation) to high-level reasoning (e.g., ARC-AGI), even with minimal supervision.

Conclusion: The study positions VDMs as not just generative tools but also as adaptable visual learners, highlighting their potential as foundational models in vision.

Abstract: Video Diffusion Models (VDMs) have emerged as powerful generative tools,
capable of synthesizing high-quality spatiotemporal content. Yet, their
potential goes far beyond mere video generation. We argue that the training
dynamics of VDMs, driven by the need to model coherent sequences, naturally
pushes them to internalize structured representations and an implicit
understanding of the visual world. To probe the extent of this internal
knowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs
for new tasks using only a handful of examples. Our method transforms each task
into a visual transition, enabling the training of LoRA weights on short
input-output sequences without altering the generative interface of a frozen
VDM. Despite minimal supervision, the model exhibits strong generalization
across diverse tasks, from low-level vision (for example, segmentation and pose
estimation) to high-level reasoning (for example, on ARC-AGI). These results
reframe VDMs as more than generative engines. They are adaptable visual
learners with the potential to serve as the backbone for future foundation
models in vision.

</details>


### [313] [FANVID: A Benchmark for Face and License Plate Recognition in Low-Resolution Videos](https://arxiv.org/abs/2506.07304)
*Kavitha Viswanathan,Vrinda Goel,Shlesh Gholap,Devayan Ghosh,Madhav Gupta,Dhruvi Ganatra,Sanket Potdar,Amit Sethi*

Main category: cs.CV

TL;DR: The paper introduces FANVID, a low-resolution video benchmark dataset for temporal face matching and license plate recognition, to advance LR recognition models.


<details>
  <summary>Details</summary>
Motivation: Surveillance systems often operate in low-resolution environments, making it difficult to identify faces and license plates in single frames. The paper aims to address this limitation by enabling and advancing temporal recognition models.

Method: The authors created the FANVID dataset containing 1,463 low-resolution video clips with labeled bounding boxes and defined two tasks: face matching and license plate recognition. They also evaluated baseline performance using pre-trained video super-resolution, detection, and recognition pipelines.

Result: Baseline methods achieved scores of 0.58 for face matching and 0.42 for license plate recognition, demonstrating both the potential and difficulty of low-resolution recognition.

Conclusion: FANVID provides a challenging and realistic benchmark for temporal modeling in low-resolution recognition. It emphasizes exploiting temporal information for improved identification and supports further research with its open release of software and annotations.

Abstract: Real-world surveillance often renders faces and license plates unrecognizable
in individual low-resolution (LR) frames, hindering reliable identification. To
advance temporal recognition models, we present FANVID, a novel video-based
benchmark comprising nearly 1,463 LR clips (180 x 320, 20--60 FPS) featuring 63
identities and 49 license plates from three English-speaking countries. Each
video includes distractor faces and plates, increasing task difficulty and
realism. The dataset contains 31,096 manually verified bounding boxes and
labels.
  FANVID defines two tasks: (1) face matching -- detecting LR faces and
matching them to high-resolution mugshots, and (2) license plate recognition --
extracting text from LR plates without a predefined database. Videos are
downsampled from high-resolution sources to ensure that faces and text are
indecipherable in single frames, requiring models to exploit temporal
information. We introduce evaluation metrics adapted from mean Average
Precision at IoU > 0.5, prioritizing identity correctness for faces and
character-level accuracy for text.
  A baseline method with pre-trained video super-resolution, detection, and
recognition achieved performance scores of 0.58 (face matching) and 0.42 (plate
recognition), highlighting both the feasibility and challenge of the tasks.
FANVID's selection of faces and plates balances diversity with recognition
challenge. We release the software for data access, evaluation, baseline, and
annotation to support reproducibility and extension. FANVID aims to catalyze
innovation in temporal modeling for LR recognition, with applications in
surveillance, forensics, and autonomous vehicles.

</details>


### [314] [AllTracker: Efficient Dense Point Tracking at High Resolution](https://arxiv.org/abs/2506.07310)
*Adam W. Harley,Yang You,Xinglong Sun,Yang Zheng,Nikhil Raghuraman,Yunqi Gu,Sheldon Liang,Wen-Hsuan Chu,Achal Dave,Pavel Tokmakov,Suya You,Rares Ambrus,Katerina Fragkiadaki,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: AllTracker introduces a novel model for high-resolution, long-range point tracking in videos by estimating dense optical flow between a query frame and all subsequent frames.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address limitations in existing point tracking methods and optical flow techniques, which either lack resolution or focus on short-range frame-to-frame tracking.

Method: The researchers propose a model leveraging iterative inference on low-resolution grids, using 2D convolutions for spatial propagation and pixel-aligned attention for temporal propagation. The model is trained on diverse datasets to enhance performance and includes extensive architecture ablation studies.

Result: AllTracker achieves state-of-the-art point tracking at high resolution with efficient resource usage (16M parameters on 40G GPU).

Conclusion: The proposed architecture and training methodology deliver superior results in dense correspondence fields for long-range tracking tasks, enabling wider applicability in video analysis domains.

Abstract: We introduce AllTracker: a model that estimates long-range point tracks by
way of estimating the flow field between a query frame and every other frame of
a video. Unlike existing point tracking methods, our approach delivers
high-resolution and dense (all-pixel) correspondence fields, which can be
visualized as flow maps. Unlike existing optical flow methods, our approach
corresponds one frame to hundreds of subsequent frames, rather than just the
next frame. We develop a new architecture for this task, blending techniques
from existing work in optical flow and point tracking: the model performs
iterative inference on low-resolution grids of correspondence estimates,
propagating information spatially via 2D convolution layers, and propagating
information temporally via pixel-aligned attention layers. The model is fast
and parameter-efficient (16 million parameters), and delivers state-of-the-art
point tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on
a 40G GPU). A benefit of our design is that we can train on a wider set of
datasets, and we find that doing so is crucial for top performance. We provide
an extensive ablation study on our architecture details and training recipe,
making it clear which details matter most. Our code and model weights are
available at https://alltracker.github.io .

</details>


### [315] ["CASE: Contrastive Activation for Saliency Estimation](https://arxiv.org/abs/2506.07327)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CV

TL;DR: Saliency methods often fail to differentiate between class labels in model explanations. This paper introduces CASE, a method offering more class-specific explanations.


<details>
  <summary>Details</summary>
Motivation: Saliency methods are widely used for visualizing feature relevance, but their limitations in distinguishing class labels require diagnostic tests.

Method: The authors propose a diagnostic test for class sensitivity and develop CASE, a contrastive explanation method that isolates features unique to a predicted class.

Result: CASE generates faithful and class-specific explanations, outperforming existing saliency methods in reliability and fidelity tests.

Conclusion: CASE addresses structural failures in saliency methods, enhancing their reliability and utility for understanding model predictions.

Abstract: Saliency methods are widely used to visualize which input features are deemed
relevant to a model's prediction. However, their visual plausibility can
obscure critical limitations. In this work, we propose a diagnostic test for
class sensitivity: a method's ability to distinguish between competing class
labels on the same input. Through extensive experiments, we show that many
widely used saliency methods produce nearly identical explanations regardless
of the class label, calling into question their reliability. We find that
class-insensitive behavior persists across architectures and datasets,
suggesting the failure mode is structural rather than model-specific. Motivated
by these findings, we introduce CASE, a contrastive explanation method that
isolates features uniquely discriminative for the predicted class. We evaluate
CASE using the proposed diagnostic and a perturbation-based fidelity test, and
show that it produces faithful and more class-specific explanations than
existing methods.

</details>


### [316] [CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms](https://arxiv.org/abs/2506.07357)
*Satvik Praveen,Yoonsung Jung*

Main category: cs.CV

TL;DR: The paper proposes CBAM-STN-TPS-YOLO, a novel object detection model for precision agriculture that integrates Thin-Plate Splines and attention mechanisms to overcome challenges like occlusions and non-rigid deformations.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection models like YOLO face challenges with occlusions, irregular plant structures, and background noise, reducing their suitability for precision agricultural tasks.

Method: The authors integrate Thin-Plate Splines (TPS) into Spatial Transformer Networks (STNs) for better handling of non-rigid deformations, along with a Convolutional Block Attention Module (CBAM) to reduce background noise and emphasize important features.

Result: On the occlusion-heavy PGP dataset, CBAM-STN-TPS-YOLO outperforms STN-YOLO in precision, recall, and mAP, reducing false positives by 12%.

Conclusion: This lightweight and efficient model with improved spatial awareness is well-suited for real-time precision agriculture applications, providing better plant monitoring and detection capabilities.

Abstract: Object detection is vital in precision agriculture for plant monitoring,
disease detection, and yield estimation. However, models like YOLO struggle
with occlusions, irregular structures, and background noise, reducing detection
accuracy. While Spatial Transformer Networks (STNs) improve spatial invariance
through learned transformations, affine mappings are insufficient for non-rigid
deformations such as bent leaves and overlaps.
  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)
into STNs for flexible, non-rigid spatial transformations that better align
features. Performance is further enhanced by the Convolutional Block Attention
Module (CBAM), which suppresses background noise and emphasizes relevant
spatial and channel-wise features.
  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model
outperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction
in false positives, highlighting the benefits of improved spatial flexibility
and attention-guided refinement. We also examine the impact of the TPS
regularization parameter in balancing transformation smoothness and detection
performance.
  This lightweight model improves spatial awareness and supports real-time edge
deployment, making it ideal for smart farming applications requiring accurate
and efficient monitoring.

</details>


### [317] [Multiple Object Stitching for Unsupervised Representation Learning](https://arxiv.org/abs/2506.07364)
*Chengchao Shen,Dawei Liu,Jianxin Wang*

Main category: cs.CV

TL;DR: The paper introduces Multiple Object Stitching (MOS), a method for improving unsupervised representations in multi-object images by stitching single-object images and providing object correspondences.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods perform poorly on multi-object images, which are widely present in real-world scenarios, necessitating an approach to enhance their representation learning.

Method: MOS constructs multi-object images by stitching together single-object-centric ones, introducing object correspondences between multi-object images without requiring human annotations.

Result: MOS achieved state-of-the-art performance on several datasets (ImageNet, CIFAR, COCO) for unsupervised representation learning on both single and multi-object images.

Conclusion: The proposed MOS approach improves detailed representations for complex downstream tasks like object detection and semantic segmentation, making it highly effective for multi-object image scenarios.

Abstract: Contrastive learning for single object centric images has achieved remarkable
progress on unsupervised representation, but suffering inferior performance on
the widespread images with multiple objects. In this paper, we propose a simple
but effective method, Multiple Object Stitching (MOS), to refine the
unsupervised representation for multi-object images. Specifically, we construct
the multi-object images by stitching the single object centric ones, where the
objects in the synthesized multi-object images are predetermined. Hence,
compared to the existing contrastive methods, our method provides additional
object correspondences between multi-object images without human annotations.
In this manner, our method pays more attention to the representations of each
object in multi-object image, thus providing more detailed representations for
complicated downstream tasks, such as object detection and semantic
segmentation. Experimental results on ImageNet, CIFAR and COCO datasets
demonstrate that our proposed method achieves the leading unsupervised
representation performance on both single object centric images and
multi-object ones. The source code is available at
https://github.com/visresearch/MultipleObjectStitching.

</details>


### [318] [C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.07368)
*Jiaying He,Yitong Lin,Jiahe Chen,Honghui Xu,Jianwei Zheng*

Main category: cs.CV

TL;DR: This paper introduces C3S3, an innovative semi-supervised segmentation model that enhances boundary delineation in medical imaging using contrastive learning and complementary competition strategies.


<details>
  <summary>Details</summary>
Motivation: Current semi-supervised medical image segmentation methods struggle to precisely capture detailed boundaries, which can result in diagnostic inaccuracies.

Method: C3S3 integrates two modules: the Outcome-Driven Contrastive Learning module to refine boundary localization, and the Dynamic Complementary Competition module to generate pseudo-labels via two high-performing sub-networks.

Result: C3S3 demonstrates superior performance on MRI and CT scan datasets, achieving at least a 6% improvement in 95HD and ASD metrics compared to previous methods.

Conclusion: The C3S3 model substantially advances boundary delineation and overall segmentation precision in medical imaging while surpassing existing technologies.

Abstract: For the immanent challenge of insufficiently annotated samples in the medical
field, semi-supervised medical image segmentation (SSMIS) offers a promising
solution. Despite achieving impressive results in delineating primary target
areas, most current methodologies struggle to precisely capture the subtle
details of boundaries. This deficiency often leads to significant diagnostic
inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised
segmentation model that synergistically integrates complementary competition
and contrastive selection. This design significantly sharpens boundary
delineation and enhances overall precision. Specifically, we develop an
$\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining
boundary localization. Additionally, we incorporate a $\textit{Dynamic
Complementary Competition}$ module that leverages two high-performing
sub-networks to generate pseudo-labels, thereby further improving segmentation
quality. The proposed C3S3 undergoes rigorous validation on two publicly
accessible datasets, encompassing the practices of both MRI and CT scans. The
results demonstrate that our method achieves superior performance compared to
previous cutting-edge competitors. Especially, on the 95HD and ASD metrics, our
approach achieves a notable improvement of at least $6\%$, highlighting the
significant advancements. The code is available at
https://github.com/Y-TARL/C3S3.

</details>


### [319] [Generative Models at the Frontier of Compression: A Survey on Generative Face Video Coding](https://arxiv.org/abs/2506.07369)
*Bolin Chen,Shanzhi Yin,Goluck Konuko,Giuseppe Valenzise,Zihan Zhang,Shiqi Wang,Yan Ye*

Main category: cs.CV

TL;DR: This paper surveys Generative Face Video Coding (GFVC), a deep generative model-based approach for ultra-low bitrate face video compression, benchmarking methods, creating a database for quality metrics, and discussing standardization and industrial applications.


<details>
  <summary>Details</summary>
Motivation: Face video communication demands high fidelity with ultra-low bandwidth requirements. Traditional standards like VVC struggle to achieve this efficiently, prompting exploration into generative models for improved compression.

Method: The paper reviews various GFVC approaches, benchmarks them, builds a database with human-perceived scores, proposes a unified syntax, and develops a low-complexity system for GFVC standardization.

Result: A large-scale GFVC database was constructed alongside benchmarking insights. A proposed syntax and low-complexity GFVC system show promise for practical implementation.

Conclusion: GFVC can drastically enhance face video communication efficiency and fidelity using generative models, revealing opportunities for industrial applications while addressing challenges in standardization and metric evaluation.

Abstract: The rise of deep generative models has greatly advanced video compression,
reshaping the paradigm of face video coding through their powerful capability
for semantic-aware representation and lifelike synthesis. Generative Face Video
Coding (GFVC) stands at the forefront of this revolution, which could
characterize complex facial dynamics into compact latent codes for bitstream
compactness at the encoder side and leverages powerful deep generative models
to reconstruct high-fidelity face signal from the compressed latent codes at
the decoder side. As such, this well-designed GFVC paradigm could enable
high-fidelity face video communication at ultra-low bitrate ranges, far
surpassing the capabilities of the latest Versatile Video Coding (VVC)
standard. To pioneer foundational research and accelerate the evolution of
GFVC, this paper presents the first comprehensive survey of GFVC technologies,
systematically bridging critical gaps between theoretical innovation and
industrial standardization. In particular, we first review a broad range of
existing GFVC methods with different feature representations and optimization
strategies, and conduct a thorough benchmarking analysis. In addition, we
construct a large-scale GFVC-compressed face video database with subjective
Mean Opinion Scores (MOSs) based on human perception, aiming to identify the
most appropriate quality metrics tailored to GFVC. Moreover, we summarize the
GFVC standardization potentials with a unified high-level syntax and develop a
low-complexity GFVC system which are both expected to push forward future
practical deployments and applications. Finally, we envision the potential of
GFVC in industrial applications and deliberate on the current challenges and
future opportunities.

</details>


### [320] [ARGUS: Hallucination and Omission Evaluation in Video-LLMs](https://arxiv.org/abs/2506.07371)
*Ruchit Rawal,Reza Shirkavand,Heng Huang,Gowthami Somepalli,Tom Goldstein*

Main category: cs.CV

TL;DR: The paper introduces ARGUS, a benchmark for evaluating video large language models' (VideoLLMs) performance in freeform video captioning by minimizing hallucinations and omissions.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs are not widely deployed due to issues with hallucination, especially in freeform video captioning tasks, which standard multiple-choice benchmarks fail to capture effectively.

Method: The researchers developed ARGUS, a benchmark that evaluates VideoLLMs by comparing their freeform video captions to human-generated ground truth captions. ARGUS assesses hallucination rates (incorrect video content/temporal statements) and omission rates (missing important details).

Result: ARGUS provides a dual-metric approach to benchmarking VideoLLMs, allowing a detailed assessment of models regarding both hallucinations and descriptive omissions.

Conclusion: ARGUS creates a more robust evaluation framework for VideoLLMs, enabling better understanding and improvement of their freeform video captioning capabilities.

Abstract: Video large language models have not yet been widely deployed, largely due to
their tendency to hallucinate. Typical benchmarks for Video-LLMs rely simply on
multiple-choice questions. Unfortunately, VideoLLMs hallucinate far more
aggressively on freeform text generation tasks like video captioning than they
do on multiple choice verification tasks. To address this weakness, we propose
ARGUS, a VideoLLM benchmark that measures freeform video captioning
performance. By comparing VideoLLM outputs to human ground truth captions,
ARGUS quantifies dual metrics. First, we measure the rate of hallucinations in
the form of incorrect statements about video content or temporal relationships.
Second, we measure the rate at which the model omits important descriptive
details. Together, these dual metrics form a comprehensive view of video
captioning performance.

</details>


### [321] [DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models](https://arxiv.org/abs/2506.07375)
*Xunjie He,Christina Dao Wen Lee,Meiling Wang,Chengran Yuan,Zefan Huang,Yufeng Yue,Marcelo H. Ang Jr*

Main category: cs.CV

TL;DR: The paper introduces a framework for multi-class collaborative detection and tracking of road users, addressing limitations in existing methods. The solution leverages three key modules to enhance detection and tracking performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods mostly focus on single-class (vehicles) collaborative perception and lack effective approaches for multi-class scenarios. This restricts their use in real-world settings with diverse road users.

Method: The authors propose a multi-class detection and tracking framework incorporating: (1) a global spatial attention fusion (GSAF) module for multi-scale feature learning, (2) a tracklet RE-IDentification (REID) module to minimize mismatches using visual semantics, and (3) a velocity-based adaptive tracklet management (VATM) module adjusting tracking intervals dynamically.

Result: Experiments using the V2X-Real and OPV2V datasets demonstrate the proposed framework's superior detection and tracking accuracy compared to state-of-the-art approaches.

Conclusion: This paper provides an advanced multi-class collaborative perception methodology, enhancing real-world applicability by improving detection accuracy, tracking precision, and robustness to failures in diverse road conditions.

Abstract: Collaborative perception plays a crucial role in enhancing environmental
understanding by expanding the perceptual range and improving robustness
against sensor failures, which primarily involves collaborative 3D detection
and tracking tasks. The former focuses on object recognition in individual
frames, while the latter captures continuous instance tracklets over time.
However, existing works in both areas predominantly focus on the vehicle
superclass, lacking effective solutions for both multi-class collaborative
detection and tracking. This limitation hinders their applicability in
real-world scenarios, which involve diverse object classes with varying
appearances and motion patterns. To overcome these limitations, we propose a
multi-class collaborative detection and tracking framework tailored for diverse
road users. We first present a detector with a global spatial attention fusion
(GSAF) module, enhancing multi-scale feature learning for objects of varying
sizes. Next, we introduce a tracklet RE-IDentification (REID) module that
leverages visual semantics with a vision foundation model to effectively reduce
ID SWitch (IDSW) errors, in cases of erroneous mismatches involving small
objects like pedestrians. We further design a velocity-based adaptive tracklet
management (VATM) module that adjusts the tracking interval dynamically based
on object motion. Extensive experiments on the V2X-Real and OPV2V datasets show
that our approach significantly outperforms existing state-of-the-art methods
in both detection and tracking accuracy.

</details>


### [322] [Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation](https://arxiv.org/abs/2506.07376)
*Jintao Tong,Ran Ma,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: This paper tackles cross-domain few-shot segmentation (CD-FSS) to address domain gaps and limited data by introducing a novel adapter-based methodology and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address two key challenges in CD-FSS: the domain gap between source and target datasets and the difficulty of fine-tuning with limited samples.

Method: The authors propose the Domain Feature Navigator (DFN), a structural decoupler to separate domain-specific information. They also introduce the SAM-SVN technique to limit DFN overfitting during source-domain training, later fine-tuning the DFN for target-specific knowledge.

Result: Experiments reveal that the proposed method outperforms existing CD-FSS approaches, achieving a 2.69% and 4.68% MIoU improvement in 1-shot and 5-shot setups, respectively.

Conclusion: By revisiting adapter-based methods and leveraging domain-specific decoupling, the proposed approach advances the state-of-the-art in CD-FSS, offering a robust solution to the challenges of domain gap and data scarcity.

Abstract: Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the
model on a source-domain dataset with sufficient samples, and then transfer the
model to target-domain datasets where only a few samples are available for
efficient fine-tuning. There are majorly two challenges in this task: (1) the
domain gap and (2) fine-tuning with scarce data. To solve these challenges, we
revisit the adapter-based methods, and discover an intriguing insight not
explored in previous works: the adapter not only helps the fine-tuning of
downstream tasks but also naturally serves as a domain information decoupler.
Then, we delve into this finding for an interpretation, and find the model's
inherent structure could lead to a natural decoupling of domain information.
Building upon this insight, we propose the Domain Feature Navigator (DFN),
which is a structure-based decoupler instead of loss-based ones like current
works, to capture domain-specific information, thereby directing the model's
attention towards domain-agnostic knowledge. Moreover, to prevent the potential
excessive overfitting of DFN during the source-domain training, we further
design the SAM-SVN method to constrain DFN from learning sample-specific
knowledge. On target domains, we freeze the model and fine-tune the DFN to
learn target-specific knowledge specific. Extensive experiments demonstrate
that our method surpasses the state-of-the-art method in CD-FSS significantly
by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.

</details>


### [323] [MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems](https://arxiv.org/abs/2506.07399)
*Peiru Yang,Jinhua Yin,Haoran Zheng,Xueying Bai,Huili Wang,Yufei Sun,Xintian Li,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CV

TL;DR: This study introduces MrM, a multimodal membership inference attack framework targeting retrieval-augmented generation systems by exploiting both textual and visual modalities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored area of potential privacy risks in multimodal RAG systems, particularly membership inference attacks (MIAs) focusing on the visual modality.

Method: The proposed black-box MIA framework, MrM, employs multi-object data perturbation constrained by counterfactual attacks. It utilizes object-aware perturbation and a counterfact-informed mask selection strategy to identify sensitive data while inducing systems to retrieve the target information.

Result: Experiments on two datasets and eight commercial visual-language models, including GPT-4o and Gemini-2, show that MrM consistently performs well in both sample-level and set-level attacks and resists adaptive defenses.

Conclusion: MrM demonstrates efficacy and robustness as a method for assessing privacy vulnerabilities in multimodal RAG systems.

Abstract: Multimodal retrieval-augmented generation (RAG) systems enhance large
vision-language models by integrating cross-modal knowledge, enabling their
increasing adoption across real-world multimodal tasks. These knowledge
databases may contain sensitive information that requires privacy protection.
However, multimodal RAG systems inherently grant external users indirect access
to such data, making them potentially vulnerable to privacy attacks,
particularly membership inference attacks (MIAs). % Existing MIA methods
targeting RAG systems predominantly focus on the textual modality, while the
visual modality remains relatively underexplored. To bridge this gap, we
propose MrM, the first black-box MIA framework targeted at multimodal RAG
systems. It utilizes a multi-object data perturbation framework constrained by
counterfactual attacks, which can concurrently induce the RAG systems to
retrieve the target data and generate information that leaks the membership
information. Our method first employs an object-aware data perturbation method
to constrain the perturbation to key semantics and ensure successful retrieval.
Building on this, we design a counterfact-informed mask selection strategy to
prioritize the most informative masked regions, aiming to eliminate the
interference of model self-knowledge and amplify attack efficacy. Finally, we
perform statistical membership inference by modeling query trials to extract
features that reflect the reconstruction of masked semantics from response
patterns. Experiments on two visual datasets and eight mainstream commercial
visual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves
consistently strong performance across both sample-level and set-level
evaluations, and remains robust under adaptive defenses.

</details>


### [324] [Compressed Feature Quality Assessment: Dataset and Baselines](https://arxiv.org/abs/2506.07412)
*Changsheng Gao,Wei Zhou,Guosheng Lin,Weisi Lin*

Main category: cs.CV

TL;DR: This paper introduces Compressed Feature Quality Assessment (CFQA) to evaluate semantic fidelity in compressed features. It presents a benchmark dataset and evaluates existing metrics, emphasizing the need for refined methods.


<details>
  <summary>Details</summary>
Motivation: The increasing use of large models in resource-limited environments requires efficient feature transmission, which is hindered by semantic degradation resulting from compression. Current metrics fail to effectively quantify this degradation, creating a need for focused research.

Method: The researchers developed a benchmark dataset with original and compressed features across vision tasks and feature codecs. They analyzed task-specific performance drops to quantify semantic distortion and evaluated existing metrics like MSE and cosine similarity.

Result: The study demonstrates that metrics like MSE, cosine similarity, and Centered Kernel Alignment inadequately capture semantic degradation. It also validates the representativeness of their benchmark dataset.

Conclusion: The research introduces CFQA as a critical area and provides resources to study semantic distortion in compressed features. It highlights the limitations of current metrics and aims to drive advancements in this space.

Abstract: The widespread deployment of large models in resource-constrained
environments has underscored the need for efficient transmission of
intermediate feature representations. In this context, feature coding, which
compresses features into compact bitstreams, becomes a critical component for
scenarios involving feature transmission, storage, and reuse. However, this
compression process introduces inherent semantic degradation that is
notoriously difficult to quantify with traditional metrics. To address this,
this paper introduces the research problem of Compressed Feature Quality
Assessment (CFQA), which seeks to evaluate the semantic fidelity of compressed
features. To advance CFQA research, we propose the first benchmark dataset,
comprising 300 original features and 12000 compressed features derived from
three vision tasks and four feature codecs. Task-specific performance drops are
provided as true semantic distortion for the evaluation of CFQA metrics. We
assess the performance of three widely used metrics (MSE, cosine similarity,
and Centered Kernel Alignment) in capturing semantic degradation. The results
underscore the representativeness of the dataset and highlight the need for
more refined metrics capable of addressing the nuances of semantic distortion
in compressed features. To facilitate the ongoing development of CFQA research,
we release the dataset and all accompanying source code at
\href{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}.
This contribution aims to advance the field and provide a foundational resource
for the community to explore CFQA.

</details>


### [325] [DPFormer: Dynamic Prompt Transformer for Continual Learning](https://arxiv.org/abs/2506.07414)
*Sheng-Kai Huang,Jiun-Feng Chang,Chun-Rong Huang*

Main category: cs.CV

TL;DR: The paper proposes DPFormer, a novel dynamic prompt transformer for continual learning, addressing catastrophic forgetting and inter-task confusion issues through prompt schemes and achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To address challenges in continual learning, specifically the stability-plasticity dilemma and inter-task confusion, which hinder model performance in tasks requiring memory of past knowledge and adaptability to new tasks.

Method: The paper introduces DPFormer, which uses prompt schemes to encode learned and new knowledge in a single network with nearly fixed parameters. It employs a unified classification module with multiple loss functions for training.

Result: The proposed DPFormer outperforms state-of-the-art methods on CIFAR-100, ImageNet100, and ImageNet1K under various class-incremental continual learning settings.

Conclusion: DPFormer effectively tackles continual learning challenges, offering strong performance and scalability. The method will be open-sourced upon paper acceptance.

Abstract: In continual learning, solving the catastrophic forgetting problem may make
the models fall into the stability-plasticity dilemma. Moreover, inter-task
confusion will also occur due to the lack of knowledge exchanges between
different tasks. In order to solve the aforementioned problems, we propose a
novel dynamic prompt transformer (DPFormer) with prompt schemes. The prompt
schemes help the DPFormer memorize learned knowledge of previous classes and
tasks, and keep on learning new knowledge from new classes and tasks under a
single network structure with a nearly fixed number of model parameters.
Moreover, they also provide discrepant information to represent different tasks
to solve the inter-task confusion problem. Based on prompt schemes, a unified
classification module with the binary cross entropy loss, the knowledge
distillation loss and the auxiliary loss is proposed to train the whole model
in an end-to-end trainable manner. Compared with state-of-the-art methods, our
method achieves the best performance in the CIFAR-100, ImageNet100 and
ImageNet1K datasets under different class-incremental settings in continual
learning. The source code will be available at our GitHub after acceptance.

</details>


### [326] [FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement](https://arxiv.org/abs/2506.07431)
*Jie He,Minglang Chen,Minying Lu,Bocheng Liang,Junming Wei,Guiyan Peng,Jiaxi Chen,Ying Tan*

Main category: cs.CV

TL;DR: The paper introduces FAMSeg, a novel ultrasound image segmentation model optimized for fetal femur and cranial images, addressing noise, similarity, and small object challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing segmentation models, particularly their inefficiency in addressing high noise, small objects, and high similarity in ultrasound images.

Method: The method includes introducing feature perception modules, longitudinal and transverse scanning convolution blocks, and Mamba-optimized residual structures to enhance feature detection and suppress noise.

Result: The proposed FAMSeg model demonstrated superior loss reduction and segmentation accuracy across images of various sizes and orientations.

Conclusion: The FAMSeg network offers a robust solution for ultrasound image segmentation, achieving improved biometrics and assessment precision while reducing manual effort and errors.

Abstract: Accurate ultrasound image segmentation is a prerequisite for precise
biometrics and accurate assessment. Relying on manual delineation introduces
significant errors and is time-consuming. However, existing segmentation models
are designed based on objects in natural scenes, making them difficult to adapt
to ultrasound objects with high noise and high similarity. This is particularly
evident in small object segmentation, where a pronounced jagged effect occurs.
Therefore, this paper proposes a fetal femur and cranial ultrasound image
segmentation model based on feature perception and Mamba enhancement to address
these challenges. Specifically, a longitudinal and transverse independent
viewpoint scanning convolution block and a feature perception module were
designed to enhance the ability to capture local detail information and improve
the fusion of contextual information. Combined with the Mamba-optimized
residual structure, this design suppresses the interference of raw noise and
enhances local multi-dimensional scanning. The system builds global information
and local feature dependencies, and is trained with a combination of different
optimizers to achieve the optimal solution. After extensive experimental
validation, the FAMSeg network achieved the fastest loss reduction and the best
segmentation performance across images of varying sizes and orientations.

</details>


### [327] [Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition](https://arxiv.org/abs/2506.07436)
*Nishi Chaudhary,S M Jamil Uddin,Sathvik Sharath Chandra,Anto Ovid,Alex Albert*

Main category: cs.CV

TL;DR: The study evaluates five state-of-the-art multimodal LLMs for recognizing hazards in construction images using different prompting strategies. Results demonstrate better performance with chain-of-thought prompting.


<details>
  <summary>Details</summary>
Motivation: To address the limited research on how multimodal LLMs perform in safety-critical visual tasks within the construction domain and explore their application for improving hazard recognition on construction sites.

Method: Conducted comparative evaluation of five LLMs (Claude-3 Opus, GPT-4.5, GPT-4o, GPT-o3, and Gemini 2.0 Pro) using three prompting strategies—zero-shot, few-shot, and chain-of-thought—on real-world construction images. Metrics like precision, recall, and F1-score were used for quantitative analysis.

Result: Chain-of-thought prompting consistently outperformed zero-shot and few-shot approaches. Among the models, GPT-4.5 and GPT-o3 showed superior performance across most settings.

Conclusion: Prompting strategy significantly impacts hazard recognition accuracy in LLMs. Chain-of-thought prompting and specific models like GPT-4.5 and GPT-o3 demonstrate promise for practical AI-assisted safety systems in construction.

Abstract: The recent emergence of multimodal large language models (LLMs) has
introduced new opportunities for improving visual hazard recognition on
construction sites. Unlike traditional computer vision models that rely on
domain-specific training and extensive datasets, modern LLMs can interpret and
describe complex visual scenes using simple natural language prompts. However,
despite growing interest in their applications, there has been limited
investigation into how different LLMs perform in safety-critical visual tasks
within the construction domain. To address this gap, this study conducts a
comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,
GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify
potential hazards from real-world construction images. Each model was tested
under three prompting strategies: zero-shot, few-shot, and chain-of-thought
(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated
basic safety context and a hazard source mnemonic, and CoT provided
step-by-step reasoning examples to scaffold model thinking. Quantitative
analysis was performed using precision, recall, and F1-score metrics across all
conditions. Results reveal that prompting strategy significantly influenced
performance, with CoT prompting consistently producing higher accuracy across
models. Additionally, LLM performance varied under different conditions, with
GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also
demonstrate the critical role of prompt design in enhancing the accuracy and
consistency of multimodal LLMs for construction safety applications. This study
offers actionable insights into the integration of prompt engineering and LLMs
for practical hazard recognition, contributing to the development of more
reliable AI-assisted safety systems.

</details>


### [328] [PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation](https://arxiv.org/abs/2506.07456)
*Wei Yao,Yunlian Sun,Chang Liu,Hongwen Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: This study introduces a method to improve human motion generation by integrating physics-based constraints, addressing realism and interaction issues in motion capture and generative models.


<details>
  <summary>Details</summary>
Motivation: Advancements in MoCap and generative AI have enabled large-scale datasets for synthesizing human motions, but they often fail to respect physical constraints, leading to unrealistic artifacts, especially in multi-person interactions.

Method: The paper employs physical mapping in a physics-based simulation environment for motion imitation, ensuring physical validity while preserving semantic meaning. It includes Motion Consistency (MC) and Marker-based Interaction (MI) loss functions tailored for multi-person scenarios.

Result: Experiments demonstrate a 3%-89% improvement in physical fidelity of the generated human motions, showing a significant enhancement in quality.

Conclusion: Integrating physics-based simulation for motion generation improves realism and interaction fidelity, addressing critical limitations in current technologies.

Abstract: Driven by advancements in motion capture and generative artificial
intelligence, leveraging large-scale MoCap datasets to train generative models
for synthesizing diverse, realistic human motions has become a promising
research direction. However, existing motion-capture techniques and generative
models often neglect physical constraints, leading to artifacts such as
interpenetration, sliding, and floating. These issues are exacerbated in
multi-person motion generation, where complex interactions are involved. To
address these limitations, we introduce physical mapping, integrated throughout
the human interaction generation pipeline. Specifically, motion imitation
within a physics-based simulation environment is used to project target motions
into a physically valid space. The resulting motions are adjusted to adhere to
real-world physics constraints while retaining their original semantic meaning.
This mapping not only improves MoCap data quality but also directly informs
post-processing of generated motions. Given the unique interactivity of
multi-person scenarios, we propose a tailored motion representation framework.
Motion Consistency (MC) and Marker-based Interaction (MI) loss functions are
introduced to improve model performance. Experiments show our method achieves
impressive results in generated human motion quality, with a 3%-89% improvement
in physical fidelity. Project page http://yw0208.github.io/physiinter

</details>


### [329] [GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning](https://arxiv.org/abs/2506.07460)
*Taeryung Lee,Hyeongjin Nam,Gyeongsik Moon,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: The paper introduces GLOS, a framework for more accurate sign language generation using gloss-level temporal alignment.


<details>
  <summary>Details</summary>
Motivation: Existing sign language generation methods have issues with disordered lexical ordering and low semantic accuracy due to a sentence-level conditioning process that fails to capture temporal structures and word-level semantics.

Method: GLOS uses gloss-level conditions aligned temporally with motion sequences and a temporal alignment conditioning (TAC) module to deliver accurate semantic and structural information to each motion timestep.

Result: The proposed method improves lexical order and semantic accuracy, surpassing prior approaches on CSL-Daily and Phoenix-2014T benchmarks.

Conclusion: Gloss-level temporal alignment significantly enhances the granularity and accuracy of sign language generation, addressing key limitations of previous techniques.

Abstract: Sign language generation (SLG), or text-to-sign generation, bridges the gap
between signers and non-signers. Despite recent progress in SLG, existing
methods still often suffer from incorrect lexical ordering and low semantic
accuracy. This is primarily due to sentence-level condition, which encodes the
entire sentence of the input text into a single feature vector as a condition
for SLG. This approach fails to capture the temporal structure of sign language
and lacks the granularity of word-level semantics, often leading to disordered
sign sequences and ambiguous motions. To overcome these limitations, we propose
GLOS, a sign language generation framework with temporally aligned gloss-level
conditioning. First, we employ gloss-level conditions, which we define as
sequences of gloss embeddings temporally aligned with the motion sequence. This
enables the model to access both the temporal structure of sign language and
word-level semantics at each timestep. As a result, this allows for
fine-grained control of signs and better preservation of lexical order. Second,
we introduce a condition fusion module, temporal alignment conditioning (TAC),
to efficiently deliver the word-level semantic and temporal structure provided
by the gloss-level condition to the corresponding motion timesteps. Our method,
which is composed of gloss-level conditions and TAC, generates signs with
correct lexical order and high semantic accuracy, outperforming prior methods
on CSL-Daily and Phoenix-2014T.

</details>


### [330] [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/abs/2506.07464)
*Jinyoung Park,Jeehye Na,Jinyoung Kim,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: The paper introduces DeepVideo-R1, a video-centric LLM trained with Reg-GRPO and difficulty-aware data augmentation to address issues in GRPO, improving video reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning capabilities of video large language models by addressing challenges in existing GRPO methodologies.

Method: Proposed Reg-GRPO reformulates GRPO objectives as regression tasks and incorporates difficulty-aware data augmentation for dynamic and solvable training samples.

Result: DeepVideo-R1 demonstrates improved performance in video reasoning tasks across benchmarks.

Conclusion: The introduction of Reg-GRPO and data augmentation strategies significantly aid in overcoming GRPO challenges, boosting video reasoning capabilities.

Abstract: Recent works have demonstrated the effectiveness of reinforcement learning
(RL)-based post-training in enhancing the reasoning capabilities of large
language models (LLMs). In particular, Group Relative Policy Optimization
(GRPO) has shown impressive success by employing a PPO-style reinforcement
algorithm with group-based normalized rewards. However, the application of GRPO
to Video Large Language Models (Video LLMs) has been less studied. In this
paper, we explore GRPO for video LLMs and identify two primary issues that
impede its effective learning: (1) reliance on safeguards, and (2) the
vanishing advantage problem. To mitigate these challenges, we propose
DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO
(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO
reformulates the GRPO objective as a regression task, directly predicting the
advantage in GRPO. This design eliminates the need for safeguards like clipping
and min functions, thereby facilitating more direct policy guidance by aligning
the model with the advantage values. We also design the difficulty-aware data
augmentation strategy that dynamically augments training samples at solvable
difficulty levels, fostering diverse and informative reward signals. Our
comprehensive experiments show that DeepVideo-R1 significantly improves video
reasoning performance across multiple video reasoning benchmarks.

</details>


### [331] [Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.07471)
*CH Cho,WJ Moon,W Jun,MS Jung,JP Heo*

Main category: cs.CV

TL;DR: This paper proposes Ambiguity-Restrained representation Learning (ARL) to handle the ambiguity in text-video relationships for Partially Relevant Video Retrieval (PRVR).


<details>
  <summary>Details</summary>
Motivation: To address the inherent ambiguity between text and video conceptual scopes, which is largely ignored in typical PRVR training that assumes one-to-one relationships.

Method: The ARL framework detects ambiguous pairs using uncertainty and similarity criteria, employs multi-positive contrastive learning and dual triplet margin loss, extends learning to text-frame level, and uses cross-model ambiguity detection to prevent error propagation.

Result: ARL effectively improves learning for PRVR by addressing ambiguities at both text-video and text-frame levels.

Conclusion: Incorporating ambiguity into model learning enables ARL to enhance retrieval accuracy and better capture the complexities of the PRVR task.

Abstract: Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a
specific segment is relevant to a given text query. Typical training processes
of PRVR assume a one-to-one relationship where each text query is relevant to
only one video. However, we point out the inherent ambiguity between text and
video content based on their conceptual scope and propose a framework that
incorporates this ambiguity into the model learning process. Specifically, we
propose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous
text-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:
uncertainty and similarity. Uncertainty represents whether instances include
commonly shared context across the dataset, while similarity indicates
pair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL
hierarchically learns the semantic relationship via multi-positive contrastive
learning and dual triplet margin loss. Additionally, we delve into fine-grained
relationships within the video instances. Unlike typical training at the
text-video level, where pairwise information is provided, we address the
inherent ambiguity within frames of the same untrimmed video, which often
contains multiple contexts. This allows us to further enhance learning at the
text-frame level. Lastly, we propose cross-model ambiguity detection to
mitigate the error propagation that occurs when a single model is employed to
detect ambiguous pairs for its training. With all components combined, our
proposed method demonstrates its effectiveness in PRVR.

</details>


### [332] [CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization](https://arxiv.org/abs/2506.07484)
*Dasol Hong,Wooju Lee,Hyun Myung*

Main category: cs.CV

TL;DR: The paper introduces "CoCoA-Mix," a framework for improving vision-language model adaptation, using a confusion-aware loss and a mixture model with confidence-aware weights to enhance specialization and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the core challenge in prompt tuning of improving task specialization and domain generalization, while overcoming the misaligned features from frozen encoders that result in class confusion.

Method: The paper proposes a confusion-aware loss (CoA-loss) for refining decision boundaries and a mixture model using confidence-aware weights (CoA-weights) to enhance generalization without sacrificing specialization.

Result: The CoCoA-Mix framework, incorporating CoA-loss and CoA-weights, outperformed state-of-the-art methods in both specialization and generalization for vision-language tasks.

Conclusion: CoCoA-Mix demonstrates the utility of confusion-aware loss and confidence-aware weights in improving both task-specific adaptation and domain robustness in vision-language models.

Abstract: Prompt tuning, which adapts vision-language models by freezing model
parameters and optimizing only the prompt, has proven effective for
task-specific adaptations. The core challenge in prompt tuning is improving
specialization for a specific task and generalization for unseen domains.
However, frozen encoders often produce misaligned features, leading to
confusion between classes and limiting specialization. To overcome this issue,
we propose a confusion-aware loss (CoA-loss) that improves specialization by
refining the decision boundaries between confusing classes. Additionally, we
mathematically demonstrate that a mixture model can enhance generalization
without compromising specialization. This is achieved using confidence-aware
weights (CoA-weights), which adjust the weights of each prediction in the
mixture model based on its confidence within the class domains. Extensive
experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,
outperforms state-of-the-art methods by enhancing specialization and
generalization. Our code is publicly available at
https://github.com/url-kaist/CoCoA-Mix.

</details>


### [333] [Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video](https://arxiv.org/abs/2506.07489)
*Yahao Shi,Yang Liu,Yanmin Wu,Xing Liu,Chen Zhao,Jie Luo,Bin Zhou*

Main category: cs.CV

TL;DR: DriveAnyMesh is a new method for generating mesh animations from monocular video using a 4D diffusion model, transforming point cloud trajectories into high-quality animatable 3D assets.


<details>
  <summary>Details</summary>
Motivation: Current 4D generation techniques struggle with rendering efficiency and require significant manual effort. There's a need for methods that can animate existing 3D assets efficiently while spanning across categories.

Method: The paper introduces a 4D diffusion model paired with a transformer-based variational autoencoder to denoise latent sets, producing mesh animations from point cloud trajectories. It employs a spatiotemporal diffusion model to exchange information across latent frames.

Result: DriveAnyMesh successfully generates complex, high-quality animations efficiently and proves compatible with modern rendering engines.

Conclusion: The method shows promise for practical applications in gaming and filmmaking, offering cross-category generalization with reduced manual effort.

Abstract: We propose DriveAnyMesh, a method for driving mesh guided by monocular video.
Current 4D generation techniques encounter challenges with modern rendering
engines. Implicit methods have low rendering efficiency and are unfriendly to
rasterization-based engines, while skeletal methods demand significant manual
effort and lack cross-category generalization. Animating existing 3D assets,
instead of creating 4D assets from scratch, demands a deep understanding of the
input's 3D structure. To tackle these challenges, we present a 4D diffusion
model that denoises sequences of latent sets, which are then decoded to produce
mesh animations from point cloud trajectory sequences. These latent sets
leverage a transformer-based variational autoencoder, simultaneously capturing
3D shape and motion information. By employing a spatiotemporal,
transformer-based diffusion model, information is exchanged across multiple
latent frames, enhancing the efficiency and generalization of the generated
results. Our experimental results demonstrate that DriveAnyMesh can rapidly
produce high-quality animations for complex motions and is compatible with
modern rendering engines. This method holds potential for applications in both
the gaming and filming industries.

</details>


### [334] [SpatialLM: Training Large Language Models for Structured Indoor Modeling](https://arxiv.org/abs/2506.07491)
*Yongsen Mao,Junhao Zhong,Chuan Fang,Jia Zheng,Rui Tang,Hao Zhu,Ping Tan,Zihan Zhou*

Main category: cs.CV

TL;DR: SpatialLM is a model that applies large language model architecture to 3D point clouds, achieving state-of-the-art performance in scene understanding tasks like layout estimation and object detection.


<details>
  <summary>Details</summary>
Motivation: There is a need to enhance spatial understanding in large language models to enable better scene understanding and unlock applications in augmented reality and robotics.

Method: SpatialLM fine-tunes open-source large language models using a multimodal architecture. It is trained on a synthetic dataset of over 12,000 indoor scenes and 50,000 annotated rooms with careful modeling and training decisions.

Result: SpatialLM achieves state-of-the-art results in layout estimation tasks and competitive performance in 3D object detection benchmarks.

Conclusion: SpatialLM provides a promising approach to extending language model capabilities to 3D spatial reasoning, making them applicable for tasks in augmented reality and robotics.

Abstract: SpatialLM is a large language model designed to process 3D point cloud data
and generate structured 3D scene understanding outputs. These outputs include
architectural elements like walls, doors, windows, and oriented object boxes
with their semantic categories. Unlike previous methods which exploit
task-specific network designs, our model adheres to the standard multimodal LLM
architecture and is fine-tuned directly from open-source LLMs.
  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset
consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with
ground-truth 3D annotations, and conduct a careful study on various modeling
and training decisions. On public benchmarks, our model gives state-of-the-art
performance in layout estimation and competitive results in 3D object
detection. With that, we show a feasible path for enhancing the spatial
understanding capabilities of modern LLMs for applications in augmented
reality, embodied robotics, and more.

</details>


### [335] [Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency](https://arxiv.org/abs/2506.07497)
*Xiangyu Guo,Zhanqian Wu,Kaixin Xiong,Ziyang Xu,Lijun Zhou,Gangwei Xu,Shaoqing Xu,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: Genesis introduces a unified framework for generating synchronized driving videos and LiDAR sequences, leveraging advanced models for consistency across visual and geometric information.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of jointly generating multi-view driving videos and LiDAR sequences that maintain spatio-temporal and cross-modal consistency.

Method: Genesis uses a two-stage architecture integrating DiT-based video diffusion, 3D-VAE encoding, BEV-aware LiDAR generation, NeRF-rendering, adaptive sampling, and a shared latent space. The DataCrafter module enhances semantic structure via captioning supervision.

Result: Extensive experiments on the nuScenes benchmark show state-of-the-art performance on video and LiDAR metrics (FVD 16.95, FID 4.24, Chamfer 0.611) with improvements to tasks like segmentation and 3D detection.

Conclusion: Genesis proves effective in producing semantically faithful and practically useful generated data, contributing to downstream applications and surpassing existing techniques in this domain.

Abstract: We present Genesis, a unified framework for joint generation of multi-view
driving videos and LiDAR sequences with spatio-temporal and cross-modal
consistency. Genesis employs a two-stage architecture that integrates a
DiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR
generator with NeRF-based rendering and adaptive sampling. Both modalities are
directly coupled through a shared latent space, enabling coherent evolution
across visual and geometric domains. To guide the generation with structured
semantics, we introduce DataCrafter, a captioning module built on
vision-language models that provides scene-level and instance-level
supervision. Extensive experiments on the nuScenes benchmark demonstrate that
Genesis achieves state-of-the-art performance across video and LiDAR metrics
(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including
segmentation and 3D detection, validating the semantic fidelity and practical
utility of the generated data.

</details>


### [336] [MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts](https://arxiv.org/abs/2506.07533)
*Wei Tao,Haocheng Lu,Xiaoyang Qu,Bin Zhang,Kai Lu,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: MoQAE introduces a mixed-precision quantization method that balances model accuracy and memory efficiency for large language models, outperforming state-of-the-art quantization methods.


<details>
  <summary>Details</summary>
Motivation: Optimizing large language models for long-context inference is challenging due to high memory consumption in KV caches. Existing quantization methods fail to balance effectiveness and efficiency.

Method: MoQAE employs a mixture of quantization-aware experts to select optimal bit-width configurations, utilizes chunk-based routing, implements a fine-tuning process with a comprehensive loss, and introduces routing freezing and sharing mechanisms.

Result: Extensive experiments demonstrate MoQAE’s superior performance in both efficiency and effectiveness when compared to existing KV cache quantization techniques.

Conclusion: MoQAE achieves a significant improvement in reducing the memory usage of LLMs while maintaining accuracy, establishing it as a superior approach for KV cache optimization.

Abstract: One of the primary challenges in optimizing large language models (LLMs) for
long-context inference lies in the high memory consumption of the Key-Value
(KV) cache. Existing approaches, such as quantization, have demonstrated
promising results in reducing memory usage. However, current quantization
methods cannot take both effectiveness and efficiency into account. In this
paper, we propose MoQAE, a novel mixed-precision quantization method via
mixture of quantization-aware experts. First, we view different quantization
bit-width configurations as experts and use the traditional mixture of experts
(MoE) method to select the optimal configuration. To avoid the inefficiency
caused by inputting tokens one by one into the router in the traditional MoE
method, we input the tokens into the router chunk by chunk. Second, we design a
lightweight router-only fine-tuning process to train MoQAE with a comprehensive
loss to learn the trade-off between model accuracy and memory usage. Finally,
we introduce a routing freezing (RF) and a routing sharing (RS) mechanism to
further reduce the inference overhead. Extensive experiments on multiple
benchmark datasets demonstrate that our method outperforms state-of-the-art KV
cache quantization approaches in both efficiency and effectiveness.

</details>


### [337] [Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study](https://arxiv.org/abs/2506.07539)
*Xiaomeng Zhu,Jacob Henningsson,Duruo Li,Pär Mårtensson,Lars Hanson,Mårten Björkman,Atsuto Maki*

Main category: cs.CV

TL;DR: The paper focuses on domain randomization for synthetic data in manufacturing object detection, proposing a pipeline and dataset while achieving high performance using synthetic data.


<details>
  <summary>Details</summary>
Motivation: Improving object detection in manufacturing through synthetic data generation, addressing sim-to-real gaps and enabling robust performance under various environments.

Method: They developed a data generation pipeline considering factors like object characteristics, backgrounds, illumination, and camera settings, along with the creation of the SIP15-OD dataset and evaluation using Yolov8 models.

Result: The proposed method achieved top performance on synthetic training data with mAP@50 scores: 96.4% for a robotics dataset and above 94% for three SIP15-OD use cases.

Conclusion: The effectiveness of this domain randomization approach demonstrates its potential to approximate real-world data distributions, tackling challenges in sim-to-real object detection.

Abstract: This paper addresses key aspects of domain randomization in generating
synthetic data for manufacturing object detection applications. To this end, we
present a comprehensive data generation pipeline that reflects different
factors: object characteristics, background, illumination, camera settings, and
post-processing. We also introduce the Synthetic Industrial Parts Object
Detection dataset (SIP15-OD) consisting of 15 objects from three industrial use
cases under varying environments as a test bed for the study, while also
employing an industrial dataset publicly available for robotic applications. In
our experiments, we present more abundant results and insights into the
feasibility as well as challenges of sim-to-real object detection. In
particular, we identified material properties, rendering methods,
post-processing, and distractors as important factors. Our method, leveraging
these, achieves top performance on the public dataset with Yolov8 models
trained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics
dataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,
respectively. The results showcase the effectiveness of the proposed domain
randomization, potentially covering the distribution close to real data for the
applications.

</details>


### [338] [APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs](https://arxiv.org/abs/2506.07542)
*Bowen Liu,Weiyi Zhang,Peranut Chotcomwongse,Xiaolan Chen,Ruoyu Chen,Pawin Pakaymaskul,Niracha Arjkongharn,Nattaporn Vongsa,Xuelian Cheng,Zongyuan Ge,Kun Huang,Xiaohui Li,Yiru Duan,Zhenbang Wang,BaoYe Xie,Qiang Chen,Huazhu Fu,Michael A. Mahr,Jiaqi Qu,Wangyiyang Chen,Shiye Wang,Yubo Tan,Yongjie Li,Mingguang He,Danli Shi,Paisan Ruamviboonsuk*

Main category: cs.CV

TL;DR: The paper discusses the APTOS-2024 challenge, which aims to synthesize 3D OCT images from 2D fundus photographs using AI.


<details>
  <summary>Details</summary>
Motivation: To make high-resolution, 3D retinal imaging more accessible in under-resourced healthcare settings by leveraging AI to generate OCT images from easily-acquirable 2D fundus photography.

Method: A challenge framework was established involving a benchmark dataset, evaluation metrics for fidelity, and analysis of top-performing solutions from 342 teams. Various AI approaches were explored, such as cross-modality preprocessing, pre-training, and architecture enhancement.

Result: The challenge demonstrated innovative AI solutions had a significant potential for OCT generation from fundus images. Nine finalists showcased advancements in image synthesis methodologies.

Conclusion: This study proves the feasibility of fundus-to-OCT synthesis in enhancing accessibility to retinal imaging and advancing research and clinical practices in ophthalmology.

Abstract: Optical Coherence Tomography (OCT) provides high-resolution, 3D, and
non-invasive visualization of retinal layers in vivo, serving as a critical
tool for lesion localization and disease diagnosis. However, its widespread
adoption is limited by equipment costs and the need for specialized operators.
In comparison, 2D color fundus photography offers faster acquisition and
greater accessibility with less dependence on expensive devices. Although
generative artificial intelligence has demonstrated promising results in
medical image synthesis, translating 2D fundus images into 3D OCT images
presents unique challenges due to inherent differences in data dimensionality
and biological information between modalities. To advance generative models in
the fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society
(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT
Generation from Fundus Images. This paper details the challenge framework
(referred to as APTOS-2024 Challenge), including: the benchmark dataset,
evaluation methodology featuring two fidelity metrics-image-based distance
(pixel-level OCT B-scan similarity) and video-based distance (semantic-level
volumetric consistency), and analysis of top-performing solutions. The
challenge attracted 342 participating teams, with 42 preliminary submissions
and 9 finalists. Leading methodologies incorporated innovations in hybrid data
preprocessing or augmentation (cross-modality collaborative paradigms),
pre-training on external ophthalmic imaging datasets, integration of vision
foundation models, and model architecture improvement. The APTOS-2024 Challenge
is the first benchmark demonstrating the feasibility of fundus-to-3D-OCT
synthesis as a potential solution for improving ophthalmic care accessibility
in under-resourced healthcare settings, while helping to expedite medical
research and clinical applications.

</details>


### [339] [Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries](https://arxiv.org/abs/2506.07555)
*Haoxiang Wang,Zinan Lin,Da Yu,Huishuai Zhang*

Main category: cs.CV

TL;DR: The paper proposes a novel method named 'Synthesis via Private Textual Intermediaries' (SPTI) for generating high-resolution, differentially private synthetic images by translating images to textual descriptions, applying DP text generation, and reconstructing the images.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing DP image synthesis methods that struggle to generate high-resolution outputs and accurately retain structural information.

Method: SPTI summarizes private images into textual descriptions using image-to-text models, applies a modified Private Evolution algorithm for DP text generation, and then uses text-to-image models for reconstruction, eliminating the need for model training.

Result: SPTI demonstrates significant improvements in image quality under differential privacy constraints. On LSUN Bedroom and MM CelebA HQ datasets, SPTI achieved better FID scores (26.71 and 33.27) compared to existing methods.

Conclusion: SPTI offers an effective, resource-efficient, and widely accessible approach for generating high-resolution, differentially private synthetic images, overcoming major barriers in sharing sensitive visual datasets.

Abstract: Generating high fidelity, differentially private (DP) synthetic images offers
a promising route to share and analyze sensitive visual data without
compromising individual privacy. However, existing DP image synthesis methods
struggle to produce high resolution outputs that faithfully capture the
structure of the original data. In this paper, we introduce a novel method,
referred to as Synthesis via Private Textual Intermediaries (SPTI), that can
generate high resolution DP images with easy adoption. The key idea is to shift
the challenge of DP image synthesis from the image domain to the text domain by
leveraging state of the art DP text generation methods. SPTI first summarizes
each private image into a concise textual description using image to text
models, then applies a modified Private Evolution algorithm to generate DP
text, and finally reconstructs images using text to image models. Notably, SPTI
requires no model training, only inference with off the shelf models. Given a
private dataset, SPTI produces synthetic images of substantially higher quality
than prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less
than or equal to 26.71 under epsilon equal to 1.0, improving over Private
Evolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less
than or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine
tuning baselines. Overall, our results demonstrate that Synthesis via Private
Textual Intermediaries provides a resource efficient and proprietary model
compatible framework for generating high resolution DP synthetic images,
greatly expanding access to private visual datasets.

</details>


### [340] [Cross-channel Perception Learning for H&E-to-IHC Virtual Staining](https://arxiv.org/abs/2506.07559)
*Hao Yang,JianYu Wu,Run Fang,Xuelian Zhao,Yuan Ji,Zhiyu Chen,Guibin He,Junceng Guo,Yang Liu,Xinhua Zeng*

Main category: cs.CV

TL;DR: This paper introduces the Cross-Channel Perception Learning (CCPL) strategy for virtual staining in digital pathology, improving the quality and consistency of stained images.


<details>
  <summary>Details</summary>
Motivation: Existing H&E-to-IHC virtual staining methods fail to account for cross-channel correlations between cell nuclei and cell membranes, limiting diagnostic accuracy.

Method: CCPL decomposes staining into nuclei and membrane channels, extracts dual-channel features using Gigapath’s Tile Encoder, measures cross-channel correlations, and ensures staining intensity consistency via focal optical density maps.

Result: Experimental results show that CCPL produces high-quality virtual stained images, preserves pathological features, and achieves strong performance in metrics like PSNR, SSIM, PCC, and FID, validated by experts.

Conclusion: CCPL enhances the capability of virtual staining methods in digital pathology, offering improved diagnostic tools that are both efficient and accurate.

Abstract: With the rapid development of digital pathology, virtual staining has become
a key technology in multimedia medical information systems, offering new
possibilities for the analysis and diagnosis of pathological images. However,
existing H&E-to-IHC studies often overlook the cross-channel correlations
between cell nuclei and cell membranes. To address this issue, we propose a
novel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL
first decomposes HER2 immunohistochemical staining into Hematoxylin and DAB
staining channels, corresponding to cell nuclei and cell membranes,
respectively. Using the pathology foundation model Gigapath's Tile Encoder,
CCPL extracts dual-channel features from both the generated and real images and
measures cross-channel correlations between nuclei and membranes. The features
of the generated and real stained images, obtained through the Tile Encoder,
are also used to calculate feature distillation loss, enhancing the model's
feature extraction capabilities without increasing the inference burden.
Additionally, CCPL performs statistical analysis on the focal optical density
maps of both single channels to ensure consistency in staining distribution and
intensity. Experimental results, based on quantitative metrics such as PSNR,
SSIM, PCC, and FID, along with professional evaluations from pathologists,
demonstrate that CCPL effectively preserves pathological features, generates
high-quality virtual stained images, and provides robust support for automated
pathological diagnosis using multimedia medical data.

</details>


### [341] [OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data](https://arxiv.org/abs/2506.07565)
*Jinlu Zhang,Zixi Kang,Yizhou Wang*

Main category: cs.CV

TL;DR: The paper introduces OpenDance5D, a large-scale multimodal dance dataset, and OpenDanceNet, a unified framework for music-driven, controllable dance generation.


<details>
  <summary>Details</summary>
Motivation: Address the limitations in music-driven dance generation with respect to generation controllability and diversity due to the lack of fine-grained multimodal data.

Method: Developed OpenDance5D dataset with five data modalities and proposed OpenDanceNet, a masked modeling framework for dance generation conditioned on music and multimodal inputs.

Result: OpenDanceNet demonstrated high-fidelity outputs and flexible controllability in dance generation tasks.

Conclusion: The work provides significant advancements in music-driven dance generation by achieving robust multimodal learning and enhanced control flexibility.

Abstract: Music-driven dance generation offers significant creative potential yet faces
considerable challenges. The absence of fine-grained multimodal data and the
difficulty of flexible multi-conditional generation limit previous works on
generation controllability and diversity in practice. In this paper, we build
OpenDance5D, an extensive human dance dataset comprising over 101 hours across
14 distinct genres. Each sample has five modalities to facilitate robust
cross-modal learning: RGB video, audio, 2D keypoints, 3D motion, and
fine-grained textual descriptions from human arts. Furthermore, we propose
OpenDanceNet, a unified masked modeling framework for controllable dance
generation conditioned on music and arbitrary combinations of text prompts,
keypoints, or character positioning. Comprehensive experiments demonstrate that
OpenDanceNet achieves high-fidelity and flexible controllability.

</details>


### [342] [Towards the Influence of Text Quantity on Writer Retrieval](https://arxiv.org/abs/2506.07566)
*Marco Peer,Robert Sablatnig,Florian Kleber*

Main category: cs.CV

TL;DR: This research explores writer retrieval, using handwriting similarities to identify document authors within datasets, and examines line and word-level text retrieval.


<details>
  <summary>Details</summary>
Motivation: To assess how text quantity impacts writer retrieval accuracy and to evaluate existing writer retrieval systems with both traditional and deep learning methods.

Method: The study evaluates three state-of-the-art writer retrieval systems on the CVL and IAM datasets, using varying amounts of text (lines and words) to observe performance.

Result: Performance decreases by 20-30% when only one line of text is used, but accuracy stays above 90% of full-page performance with at least four lines. Deep learning outperforms handcrafted methods in low-text scenarios.

Conclusion: Deep learning approaches are more effective in writer retrieval in low-text scenarios, highlighting limitations of handcrafted methods and emphasizing the importance of sufficient text quantity for accuracy.

Abstract: This paper investigates the task of writer retrieval, which identifies
documents authored by the same individual within a dataset based on handwriting
similarities. While existing datasets and methodologies primarily focus on page
level retrieval, we explore the impact of text quantity on writer retrieval
performance by evaluating line- and word level retrieval. We examine three
state-of-the-art writer retrieval systems, including both handcrafted and deep
learning-based approaches, and analyze their performance using varying amounts
of text. Our experiments on the CVL and IAM dataset demonstrate that while
performance decreases by 20-30% when only one line of text is used as query and
gallery, retrieval accuracy remains above 90% of full-page performance when at
least four lines are included. We further show that text-dependent retrieval
can maintain strong performance in low-text scenarios. Our findings also
highlight the limitations of handcrafted features in low-text scenarios, with
deep learning-based methods like NetVLAD outperforming traditional VLAD
encoding.

</details>


### [343] [LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization](https://arxiv.org/abs/2506.07570)
*Yixuan Yang,Zhen Luo,Tongsheng Ding,Junru Lu,Mingqi Gao,Jinyu Yang,Victor Sanchez,Feng Zheng*

Main category: cs.CV

TL;DR: The paper introduces 3D-SynthPlace, a large-scale dataset for 3D indoor layout generation, and OptiScene, an optimized open-source LLM trained on this dataset, outperforming traditional methods and excelling in interactive tasks.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to address the limitations of existing prompt-driven and learning-based methods for indoor layout generation, such as spatial inconsistencies, computational inefficiencies, and lack of generalization across diverse room categories.

Method: The authors develop 3D-SynthPlace, a dataset generated through a pipeline combining synthetic layouts and human inspection, and train OptiScene using a two-stage process: supervised fine-tuning (SFT) for initial spatial description and object placement, followed by multi-turn direct preference optimization (DPO) to enhance alignment with human design preferences.

Result: OptiScene demonstrates superior performance compared to existing prompt-driven and learning-based approaches, with improved layout quality, generation success rates, and versatility in tasks like scene editing and robot navigation.

Conclusion: The study provides a robust dataset and methodology for advancing indoor layout generation, offering practical benefits for interior design, virtual environments, and embodied AI, while setting a new benchmark in this research area.

Abstract: Automatic indoor layout generation has attracted increasing attention due to
its potential in interior design, virtual environment construction, and
embodied AI. Existing methods fall into two categories: prompt-driven
approaches that leverage proprietary LLM services (e.g., GPT APIs) and
learning-based methods trained on layout data upon diffusion-based models.
Prompt-driven methods often suffer from spatial inconsistency and high
computational costs, while learning-based methods are typically constrained by
coarse relational graphs and limited datasets, restricting their generalization
to diverse room categories. In this paper, we revisit LLM-based indoor layout
generation and present 3D-SynthPlace, a large-scale dataset that combines
synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,
upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000
scenes, covering four common room types -- bedroom, living room, kitchen, and
bathroom -- enriched with diverse objects and high-level spatial annotations.
We further introduce OptiScene, a strong open-source LLM optimized for indoor
layout generation, fine-tuned based on our 3D-SynthPlace dataset through our
two-stage training. For the warum-up stage I, we adopt supervised fine-tuning
(SFT), which is taught to first generate high-level spatial descriptions then
conditionally predict concrete object placements. For the reinforcing stage II,
to better align the generated layouts with human design preferences, we apply
multi-turn direct preference optimization (DPO), which significantly improving
layout quality and generation success rates. Extensive experiments demonstrate
that OptiScene outperforms traditional prompt-driven and learning-based
baselines. Moreover, OptiScene shows promising potential in interactive tasks
such as scene editing and robot navigation.

</details>


### [344] [Learning Speaker-Invariant Visual Features for Lipreading](https://arxiv.org/abs/2506.07572)
*Yu Li,Feng Xue,Shujie Li,Jinrui Zhang,Shuang Yang,Dan Guo,Richang Hong*

Main category: cs.CV

TL;DR: SIFLip is a new lipreading framework that enhances generalization by removing speaker-specific features from visual representations.


<details>
  <summary>Details</summary>
Motivation: Existing lipreading methods suffer from inaccuracies because they retain speaker-specific visual attributes.

Method: SIFLip employs both implicit and explicit disentanglement modules to separate speaker-specific features from general visual representations.

Result: SIFLip significantly improves lipreading accuracy and generalization performance across multiple datasets.

Conclusion: The proposed SIFLip framework outperforms state-of-the-art lipreading methods and demonstrates higher model generalization.

Abstract: Lipreading is a challenging cross-modal task that aims to convert visual lip
movements into spoken text. Existing lipreading methods often extract visual
features that include speaker-specific lip attributes (e.g., shape, color,
texture), which introduce spurious correlations between vision and text. These
correlations lead to suboptimal lipreading accuracy and restrict model
generalization. To address this challenge, we introduce SIFLip, a
speaker-invariant visual feature learning framework that disentangles
speaker-specific attributes using two complementary disentanglement modules
(Implicit Disentanglement and Explicit Disentanglement) to improve
generalization. Specifically, since different speakers exhibit semantic
consistency between lip movements and phonetic text when pronouncing the same
words, our implicit disentanglement module leverages stable text embeddings as
supervisory signals to learn common visual representations across speakers,
implicitly decoupling speaker-specific features. Additionally, we design a
speaker recognition sub-task within the main lipreading pipeline to filter
speaker-specific features, then further explicitly disentangle these
personalized visual features from the backbone network via gradient reversal.
Experimental results demonstrate that SIFLip significantly enhances
generalization performance across multiple public datasets. Experimental
results demonstrate that SIFLip significantly improves generalization
performance across multiple public datasets, outperforming state-of-the-art
methods.

</details>


### [345] [Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models](https://arxiv.org/abs/2506.07575)
*Ruiyang Zhang,Hu Zhang,Hao Fei,Zhedong Zheng*

Main category: cs.CV

TL;DR: The paper introduces Uncertainty-o, a framework to evaluate and quantify uncertainty in Large Multimodal Models (LMMs) across diverse modalities and tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing three key questions: assessing LMM uncertainty, guiding them to show their uncertainty, and quantifying uncertainty for various tasks.

Method: Develop a model-agnostic framework, use multimodal prompt perturbations, and derive multimodal semantic uncertainty for quantifying responses.

Result: Experiments with 10 LMMs across 18 benchmarks demonstrate enhanced abilities such as detecting and mitigating hallucinations, and improving uncertainty-aware reasoning.

Conclusion: Uncertainty-o effectively estimates uncertainty in LMMs, contributing to both theoretical understanding and practical downstream applications.

Abstract: Large Multimodal Models (LMMs), harnessing the complementarity among diverse
modalities, are often considered more robust than pure Language Large Models
(LLMs); yet do LMMs know what they do not know? There are three key open
questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a
unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to
quantify uncertainty for downstream tasks. In an attempt to address these
challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed
to reveal uncertainty in LMMs regardless of their modalities, architectures, or
capabilities, (2) an empirical exploration of multimodal prompt perturbations
to uncover LMM uncertainty, offering insights and findings, and (3) derive the
formulation of multimodal semantic uncertainty, which enables quantifying
uncertainty from multimodal responses. Experiments across 18 benchmarks
spanning various modalities and 10 LMMs (both open- and closed-source)
demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM
uncertainty, thereby enhancing downstream tasks such as hallucination
detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought
reasoning.

</details>


### [346] [Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding](https://arxiv.org/abs/2506.07576)
*Boyu Chen,Siran Chen,Kunchang Li,Qinglin Xu,Yu Qiao,Yali Wang*

Main category: cs.CV

TL;DR: The paper introduces the Super Encoding Network (SEN), which enhances video understanding by enabling deeper multi-modal interactions within foundation models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing multi-modal foundation models that lack deep multi-modal interactions critical for understanding complex video scenes.

Method: The proposed SEN leverages pre-trained encoders as 'super neurons' and uses a Recursive Association (RA) block to progressively fuse multi-modalities, enabling deeper interaction of knowledge and features from the video.

Result: SEN significantly improves performance across four video tasks—tracking, recognition, chatting, and editing—with metrics such as 2.7% Jaccard index improvement for pixel-level tracking, 8.8% reduction in temporal coherence, 6.4% textual alignment improvement, and 4.1% frame consistency increase.

Conclusion: The SEN approach effectively addresses the gap in multi-modal interactions for video understanding, demonstrating substantial performance boosts in diverse downstream tasks.

Abstract: Video understanding has been considered as one critical step towards world
modeling, which is an important long-term problem in AI research. Recently,
multi-modal foundation models have shown such potential via large-scale
pretraining. However, these models simply align encoders of different
modalities via contrastive learning, while lacking deeper multi-modal
interactions, which is critical for understanding complex target movements with
diversified video scenes. To fill this gap, we propose a unified Super Encoding
Network (SEN) for video understanding, which builds up such distinct
interactions through recursive association of multi-modal encoders in the
foundation models. Specifically, we creatively treat those well-trained
encoders as "super neurons" in our SEN. Via designing a Recursive Association
(RA) block, we progressively fuse multi-modalities with the input video, based
on knowledge integrating, distributing, and prompting of super neurons in a
recursive manner. In this way, our SEN can effectively encode deeper
multi-modal interactions, for prompting various video understanding tasks in
downstream. Extensive experiments show that, our SEN can remarkably boost the
four most representative video tasks, including tracking, recognition,
chatting, and editing, e.g., for pixel-level tracking, the average jaccard
index improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular
CaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,
and frame consistency increases 4.1% compared to the popular TuneA-Video
approach.

</details>


### [347] [Explore the vulnerability of black-box models via diffusion models](https://arxiv.org/abs/2506.07590)
*Jiacheng Shi,Yanfu Zhang,Huajie Shao,Ashley Gao*

Main category: cs.CV

TL;DR: The paper identifies a security threat involving the misuse of diffusion model APIs to generate synthetic images that train substitute models used in adversarial black-box attacks, achieving high attack success rates with minimal resources.


<details>
  <summary>Details</summary>
Motivation: To address the security and privacy risks posed by diffusion models, particularly in scenarios involving high-fidelity image generation that can be misused for malicious purposes.

Method: The study explores the utilization of synthetic images generated by diffusion model APIs for training substitute models, which are subsequently utilized for adversarial attacks on black-box classification models.

Result: An average improvement of 27.37% over existing methods was observed, along with a 98.68% success rate in adversarial attacks on black-box models, using only a fraction of the query budget.

Conclusion: Diffusion model APIs significantly amplify the potential for adversarial attacks with reduced resources, necessitating attention to their security implications.

Abstract: Recent advancements in diffusion models have enabled high-fidelity and
photorealistic image generation across diverse applications. However, these
models also present security and privacy risks, including copyright violations,
sensitive information leakage, and the creation of harmful or offensive content
that could be exploited maliciously. In this study, we uncover a novel security
threat where an attacker leverages diffusion model APIs to generate synthetic
images, which are then used to train a high-performing substitute model. This
enables the attacker to execute model extraction and transfer-based adversarial
attacks on black-box classification models with minimal queries, without
needing access to the original training data. The generated images are
sufficiently high-resolution and diverse to train a substitute model whose
outputs closely match those of the target model. Across the seven benchmarks,
including CIFAR and ImageNet subsets, our method shows an average improvement
of 27.37% over state-of-the-art methods while using just 0.01 times of the
query budget, achieving a 98.68% success rate in adversarial attacks on the
target model.

</details>


### [348] [SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding](https://arxiv.org/abs/2506.07600)
*Nianbo Zeng,Haowen Hou,Fei Richard Yu,Si Shi,Ying Tiffany He*

Main category: cs.CV

TL;DR: SceneRAG is a framework for understanding long-form video content by segmenting videos into narrative-consistent scenes and building knowledge graphs for better retrieval and generation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in effectively understanding long-form video content due to its vast scale, complexity, and lack of coherent segmentation in existing methods.

Method: SceneRAG segments videos into narrative-consistent scenes using ASR transcripts and metadata, sharpens boundaries with heuristics, and builds dynamic knowledge graphs to enable robust multi-hop retrieval and generation.

Result: SceneRAG outperformed existing baselines on the LongerVideos benchmark, achieving a generation task win rate of up to 72.5%.

Conclusion: Segmenting videos into coherent scenes and building entity-driven knowledge graphs significantly improve long-form video understanding and generation efficiency.

Abstract: Despite recent advances in retrieval-augmented generation (RAG) for video
understanding, effectively understanding long-form video content remains
underexplored due to the vast scale and high complexity of video data. Current
RAG approaches typically segment videos into fixed-length chunks, which often
disrupts the continuity of contextual information and fails to capture
authentic scene boundaries. Inspired by the human ability to naturally organize
continuous experiences into coherent scenes, we present SceneRAG, a unified
framework that leverages large language models to segment videos into
narrative-consistent scenes by processing ASR transcripts alongside temporal
metadata. SceneRAG further sharpens these initial boundaries through
lightweight heuristics and iterative correction. For each scene, the framework
fuses information from both visual and textual modalities to extract entity
relations and dynamically builds a knowledge graph, enabling robust multi-hop
retrieval and generation that account for long-range dependencies. Experiments
on the LongerVideos benchmark, featuring over 134 hours of diverse content,
confirm that SceneRAG substantially outperforms prior baselines, achieving a
win rate of up to 72.5 percent on generation tasks.

</details>


### [349] [SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis](https://arxiv.org/abs/2506.07603)
*Jianhui Wei,Zikai Xiao,Danyu Sun,Luqi Gong,Zongxin Yang,Zuozhu Liu,Jian Wu*

Main category: cs.CV

TL;DR: The paper introduces SurgBench, a framework for surgical video understanding consisting of a pretraining dataset and an evaluation benchmark, aiming to improve automated analysis in surgery.


<details>
  <summary>Details</summary>
Motivation: Current progress in surgical video understanding is limited due to a lack of large-scale and diverse datasets, hindering applications like automated decision-making and skill assessment.

Method: The authors propose SurgBench, comprising SurgBench-P (a dataset with 53 million frames covering 22 procedures and 11 specialties) and SurgBench-E (a benchmark for evaluating 72 fine-grained tasks in six categories).

Result: Experiments showed that existing video foundational models underperform, but pretraining on SurgBench-P significantly boosts performance and allows better cross-domain generalization.

Conclusion: SurgBench advances the field by addressing dataset limitations and enabling improved performance and generalization for surgical video tasks.

Abstract: Surgical video understanding is pivotal for enabling automated intraoperative
decision-making, skill assessment, and postoperative quality improvement.
However, progress in developing surgical video foundation models (FMs) remains
hindered by the scarcity of large-scale, diverse datasets for pretraining and
systematic evaluation. In this paper, we introduce \textbf{SurgBench}, a
unified surgical video benchmarking framework comprising a pretraining dataset,
\textbf{SurgBench-P}, and an evaluation benchmark, \textbf{SurgBench-E}.
SurgBench offers extensive coverage of diverse surgical scenarios, with
SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11
specialties, and SurgBench-E providing robust evaluation across six categories
(phase classification, camera motion, tool recognition, disease diagnosis,
action classification, and organ detection) spanning 72 fine-grained tasks.
Extensive experiments reveal that existing video FMs struggle to generalize
across varied surgical video analysis tasks, whereas pretraining on SurgBench-P
yields substantial performance improvements and superior cross-domain
generalization to unseen procedures and modalities. Our dataset and code are
available upon request.

</details>


### [350] [DragNeXt: Rethinking Drag-Based Image Editing](https://arxiv.org/abs/2506.07611)
*Yuan Zhou,Junbao Zhou,Qingshan Xu,Kesen Zhao,Yuxuan Wang,Hao Fei,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: The paper tackles Drag-Based Image Editing (DBIE) challenges by redefining it as deformation, rotation, and translation of user-defined handle regions, along with a novel framework, DragNeXt.


<details>
  <summary>Details</summary>
Motivation: Current DBIE methods suffer from ambiguity in point-based dragging and rely on cumbersome methods that fail to ensure high-quality edits.

Method: The authors redefine DBIE as precise manipulations (deformation, rotation, translation) of selected regions and propose DragNeXt, a Latent Region Optimization framework using Progressive Backward Self-Intervention for improved results.

Result: Extensive experiments validate that DragNeXt significantly outperforms existing DBIE methods.

Conclusion: Reconceptualizing DBIE and employing DragNeXt addresses major challenges, offering higher quality and simplicity for image editing tasks.

Abstract: Drag-Based Image Editing (DBIE), which allows users to manipulate images by
directly dragging objects within them, has recently attracted much attention
from the community. However, it faces two key challenges:
(\emph{\textcolor{magenta}{i}}) point-based drag is often highly ambiguous and
difficult to align with users' intentions; (\emph{\textcolor{magenta}{ii}})
current DBIE methods primarily rely on alternating between motion supervision
and point tracking, which is not only cumbersome but also fails to produce
high-quality results. These limitations motivate us to explore DBIE from a new
perspective -- redefining it as deformation, rotation, and translation of
user-specified handle regions. Thereby, by requiring users to explicitly
specify both drag areas and types, we can effectively address the ambiguity
issue. Furthermore, we propose a simple-yet-effective editing framework, dubbed
\textcolor{SkyBlue}{\textbf{DragNeXt}}. It unifies DBIE as a Latent Region
Optimization (LRO) problem and solves it through Progressive Backward
Self-Intervention (PBSI), simplifying the overall procedure of DBIE while
further enhancing quality by fully leveraging region-level structure
information and progressive guidance from intermediate drag states. We validate
\textcolor{SkyBlue}{\textbf{DragNeXt}} on our NextBench, and extensive
experiments demonstrate that our proposed method can significantly outperform
existing approaches. Code will be released on github.

</details>


### [351] [Scaling Human Activity Recognition: A Comparative Evaluation of Synthetic Data Generation and Augmentation Techniques](https://arxiv.org/abs/2506.07612)
*Zikang Leng,Archith Iyer,Thomas Plötz*

Main category: cs.CV

TL;DR: This paper explores and compares virtual IMU generation methods (video-based and language-based) against traditional data augmentation techniques for human activity recognition.


<details>
  <summary>Details</summary>
Motivation: Human activity recognition is constrained by scarce labeled datasets due to the high cost and complexity of collecting real-world data.

Method: The paper constructs a large virtual IMU dataset with signals simulated for 22 body locations across 100 activities. It evaluates video-based, language-based, and classical data augmentation methods on benchmark datasets using popular models.

Result: The results show that virtual IMU data enhances performance significantly over using real data or augmented data, especially under limited-data conditions.

Conclusion: Virtual IMU data generation has substantial benefits over traditional approaches, and the paper provides practical guidance for selecting suitable strategies.

Abstract: Human activity recognition (HAR) is often limited by the scarcity of labeled
datasets due to the high cost and complexity of real-world data collection. To
mitigate this, recent work has explored generating virtual inertial measurement
unit (IMU) data via cross-modality transfer. While video-based and
language-based pipelines have each shown promise, they differ in assumptions
and computational cost. Moreover, their effectiveness relative to traditional
sensor-level data augmentation remains unclear. In this paper, we present a
direct comparison between these two virtual IMU generation approaches against
classical data augmentation techniques. We construct a large-scale virtual IMU
dataset spanning 100 diverse activities from Kinetics-400 and simulate sensor
signals at 22 body locations. The three data generation strategies are
evaluated on benchmark HAR datasets (UTD-MHAD, PAMAP2, HAD-AW) using four
popular models. Results show that virtual IMU data significantly improves
performance over real or augmented data alone, particularly under limited-data
conditions. We offer practical guidance on choosing data generation strategies
and highlight the distinct advantages and disadvantages of each approach.

</details>


### [352] [Event-Priori-Based Vision-Language Model for Efficient Visual Understanding](https://arxiv.org/abs/2506.07627)
*Haotong Qin,Cheng Hu,Michele Magno*

Main category: cs.CV

TL;DR: The paper introduces EP-VLM, a vision-language model that uses event-based vision priors to reduce redundant computations in visual input processing, achieving significant efficiency improvements with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inefficiency of current vision-language models (VLMs), which struggle with high computational demands due to processing dense and redundant visual data, making them unsuitable for edge-device deployment.

Method: EP-VLM uses motion priors from dynamic event vision to sparsify RGB visual inputs patch-wise, concentrating computation on salient regions. It also includes a position-preserving tokenization strategy to handle sparse visual inputs while maintaining positional accuracy.

Result: EP-VLM achieves a 50% reduction in FLOPs while retaining 98% of the original accuracy on the RealWorldQA dataset, compared to the Qwen2-VL-2B baseline model.

Conclusion: The study shows that event-based vision priors can significantly enhance VLM efficiency, making them more suitable for deployment on resource-constrained edge devices while maintaining high performance.

Abstract: Large Language Model (LLM)-based Vision-Language Models (VLMs) have
substantially extended the boundaries of visual understanding capabilities.
However, their high computational demands hinder deployment on
resource-constrained edge devices. A key source of inefficiency stems from the
VLM's need to process dense and redundant visual information. Visual inputs
contain significant regions irrelevant to text semantics, rendering the
associated computations ineffective for inference. This paper introduces a
novel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core
contribution is a novel mechanism leveraging motion priors derived from dynamic
event vision to enhance VLM efficiency. Inspired by human visual cognition,
EP-VLM first employs event data to guide the patch-wise sparsification of RGB
visual inputs, progressively concentrating VLM computation on salient regions
of the visual input. Subsequently, we construct a position-preserving
tokenization strategy for the visual encoder within the VLM architecture. This
strategy processes the event-guided, unstructured, sparse visual input while
accurately preserving positional understanding within the visual input.
Experimental results demonstrate that EP-VLM achieves significant efficiency
improvements while maintaining nearly lossless accuracy compared to baseline
models from the Qwen2-VL series. For instance, against the original
Qwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the
original accuracy on the RealWorldQA dataset. This work demonstrates the
potential of event-based vision priors for improving VLM inference efficiency,
paving the way for creating more efficient and deployable VLMs for sustainable
visual understanding at the edge.

</details>


### [353] [HuSc3D: Human Sculpture dataset for 3D object reconstruction](https://arxiv.org/abs/2506.07628)
*Weronika Smolak-Dyżewska,Dawid Malarz,Grzegorz Wilczyński,Rafał Tobiasz,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.CV

TL;DR: The paper introduces the HuSc3D dataset for assessing 3D scene reconstruction models under realistic challenges such as dynamic backgrounds and inconsistent camera settings.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for evaluating 3D reconstruction models focus on synthetic or carefully captured data, which doesn't reflect real-world acquisition complexity.

Method: The authors designed HuSc3D, a dataset featuring six white sculptures with detailed geometry and sparse texture. It also includes varying image counts per scene to simulate training data limitations.

Result: Popular 3D reconstruction methods were tested on HuSc3D, showing its ability to differentiate model performance based on handling fine geometry, color ambiguity, and data variability.

Conclusion: HuSc3D effectively benchmarks 3D reconstruction methods under real-world challenges, outperforming traditional synthetic datasets in highlighting model limitations.

Abstract: 3D scene reconstruction from 2D images is one of the most important tasks in
computer graphics. Unfortunately, existing datasets and benchmarks concentrate
on idealized synthetic or meticulously captured realistic data. Such benchmarks
fail to convey the inherent complexities encountered in newly acquired
real-world scenes. In such scenes especially those acquired outside, the
background is often dynamic, and by popular usage of cell phone cameras, there
might be discrepancies in, e.g., white balance. To address this gap, we present
HuSc3D, a novel dataset specifically designed for rigorous benchmarking of 3D
reconstruction models under realistic acquisition challenges. Our dataset
uniquely features six highly detailed, fully white sculptures characterized by
intricate perforations and minimal textural and color variation. Furthermore,
the number of images per scene varies significantly, introducing the additional
challenge of limited training data for some instances alongside scenes with a
standard number of views. By evaluating popular 3D reconstruction methods on
this diverse dataset, we demonstrate the distinctiveness of HuSc3D in
effectively differentiating model performance, particularly highlighting the
sensitivity of methods to fine geometric details, color ambiguity, and varying
data availability--limitations often masked by more conventional datasets.

</details>


### [354] [HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition](https://arxiv.org/abs/2506.07637)
*Yuchong Long,Wen Sun,Ningxiao Sun,Wenxiao Wang,Chao Li,Shan Yin*

Main category: cs.CV

TL;DR: This paper presents HieraEdgeNet, a deep learning framework for efficient and precise pollen recognition using advanced edge-enhancement techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the inefficiency and subjectivity in traditional pollen recognition methods and to improve localization accuracy for microscopic objects like pollen, which existing deep learning models struggle with.

Method: The authors introduce HieraEdgeNet, a multi-scale edge-enhancement framework featuring three modules: Hierarchical Edge Module (HEM) for extracting edge features, Synergistic Edge Fusion (SEF) for fusing edge priors and semantics, and Cross Stage Partial Omni-Kernel Module (CSPOKM) for refining features using Omni-Kernel operators in a computationally efficient manner.

Result: HieraEdgeNet achieves a mean Average Precision (mAP@.5) of 0.9501 on a dataset of 120 pollen classes, outperforming state-of-the-art models like YOLOv12n and RT-DETR. It also provides qualitatively better boundary-focused feature representations.

Conclusion: HieraEdgeNet demonstrates significant advancements in automated pollen recognition by systematically integrating edge information, offering a precise, efficient, and robust solution for detecting microscopic objects.

Abstract: Automated pollen recognition is vital to paleoclimatology, biodiversity
monitoring, and public health, yet conventional methods are hampered by
inefficiency and subjectivity. Existing deep learning models often struggle to
achieve the requisite localization accuracy for microscopic targets like
pollen, which are characterized by their minute size, indistinct edges, and
complex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a
multi-scale edge-enhancement framework. The framework's core innovation is the
introduction of three synergistic modules: the Hierarchical Edge Module (HEM),
which explicitly extracts a multi-scale pyramid of edge features that
corresponds to the semantic hierarchy at early network stages; the Synergistic
Edge Fusion (SEF) module, for deeply fusing these edge priors with semantic
information at each respective scale; and the Cross Stage Partial Omni-Kernel
Module (CSPOKM), which maximally refines the most detail-rich feature layers
using an Omni-Kernel operator - comprising anisotropic large-kernel
convolutions and mixed-domain attention - all within a computationally
efficient Cross-Stage Partial (CSP) framework. On a large-scale dataset
comprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision
(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline
models such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms
that our approach generates feature representations that are more precisely
focused on object boundaries. By systematically integrating edge information,
HieraEdgeNet provides a robust and powerful solution for high-precision,
high-efficiency automated detection of microscopic objects.

</details>


### [355] [Synthetic Visual Genome](https://arxiv.org/abs/2506.07643)
*Jae Sung Park,Zixian Ma,Linjie Li,Chenhao Zheng,Cheng-Yu Hsieh,Ximing Lu,Khyathi Chandu,Quan Kong,Norimasa Kobori,Ali Farhadi,Yejin Choi,Ranjay Krishna*

Main category: cs.CV

TL;DR: The paper introduces ROBIN, an instruction-tuned multimodal language model capable of creating detailed scene graphs, and demonstrates its effectiveness compared to larger models.


<details>
  <summary>Details</summary>
Motivation: To address the persistent challenge in reasoning over complex visual relationships in multimodal language models.

Method: The authors created a refined dataset called SVG and utilized a self-distillation framework (SG-EDIT) for relationship reasoning and scene graph generation.

Result: The ROBIN-3B model surpasses similar-size models trained on more data and achieves state-of-the-art performance in visual relationship reasoning and referring expression comprehension.

Conclusion: Refined scene graph data significantly enhances performance in visual reasoning tasks, showcasing its importance in model training.

Abstract: Reasoning over visual relationships-spatial, functional, interactional,
social, etc.-is considered to be a fundamental component of human cognition.
Yet, despite the major advances in visual comprehension in multimodal language
models (MLMs), precise reasoning over relationships and their generations
remains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely
annotated relationships capable of constructing high-quality dense scene graphs
at scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by
completing the missing relations of selected objects in existing scene graphs
using a teacher MLM and a carefully designed filtering process to ensure
high-quality. To generate more accurate and rich scene graphs at scale for any
image, we introduce SG-EDIT: a self-distillation framework where GPT-4o further
refines ROBIN's predicted scene graphs by removing unlikely relations and/or
suggesting relevant ones. In total, our dataset contains 146K images and 5.6M
relationships for 2.6M objects. Results show that our ROBIN-3B model, despite
being trained on less than 3 million instances, outperforms similar-size models
trained on over 300 million instances on relationship understanding benchmarks,
and even surpasses larger models up to 13B parameters. Notably, it achieves
state-of-the-art performance in referring expression comprehension with a score
of 88.9, surpassing the previous best of 87.4. Our results suggest that
training on the refined scene graph data is crucial to maintaining high
performance across diverse visual reasoning task.

</details>


### [356] [FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images](https://arxiv.org/abs/2506.07652)
*Hangbei Cheng,Xiaorong Dong,Xueyu Liu,Jianan Zhang,Xuetao Ma,Mingqiang Wei,Liansheng Wang,Junxin Chen,Yongfei Wu*

Main category: cs.CV

TL;DR: The paper introduces FMaMIL, a two-stage framework for lesion segmentation in histopathology images using image-level labels, addressing the challenge of limited pixel-level annotations.


<details>
  <summary>Details</summary>
Motivation: Diagnosing and quantitatively analyzing histopathology images is difficult due to costly and limited availability of precise pixel-level annotations.

Method: Proposed a two-stage weakly supervised framework (FMaMIL): Stage 1 uses a Mamba-based encoder and a frequency-domain encoding module to generate CAMs; Stage 2 refines these pseudo labels with CAM-guided supervision and self-correction.

Result: FMaMIL demonstrates superior performance compared to state-of-the-art weakly supervised methods on various datasets without relying on pixel-level annotations.

Conclusion: FMaMIL is effective and holds significant potential for digital pathology by providing accurate lesion segmentation using only image-level labels.

Abstract: Accurate lesion segmentation in histopathology images is essential for
diagnostic interpretation and quantitative analysis, yet it remains challenging
due to the limited availability of costly pixel-level annotations. To address
this, we propose FMaMIL, a novel two-stage framework for weakly supervised
lesion segmentation based solely on image-level labels. In the first stage, a
lightweight Mamba-based encoder is introduced to capture long-range
dependencies across image patches under the MIL paradigm. To enhance spatial
sensitivity and structural awareness, we design a learnable frequency-domain
encoding module that supplements spatial-domain features with spectrum-based
information. CAMs generated in this stage are used to guide segmentation
training. In the second stage, we refine the initial pseudo labels via a
CAM-guided soft-label supervision and a self-correction mechanism, enabling
robust training even under label noise. Extensive experiments on both public
and private histopathology datasets demonstrate that FMaMIL outperforms
state-of-the-art weakly supervised methods without relying on pixel-level
annotations, validating its effectiveness and potential for digital pathology
applications.

</details>


### [357] [ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views](https://arxiv.org/abs/2506.07670)
*Xiaohan Lu,Jiaye Fu,Jiaqi Zhang,Zetian Song,Chuanmin Jia,Siwei Ma*

Main category: cs.CV

TL;DR: ProSplat is a two-stage framework that enhances 3D Gaussian Splatting for wide-baseline novel view synthesis, using a diffusion model and geometric consistency techniques.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian Splatting methods perform poorly in wide-baseline settings due to lack of texture detail and geometric inconsistencies.

Method: ProSplat employs a two-stage process: the first generates 3D Gaussian primitives, and the second enhances them using a one-step diffusion model with novel techniques such as MORI and DWEA.

Result: ProSplat achieves superior performance, showing a 1 dB improvement in PSNR on RealEstate10K and DL3DV-10K datasets compared to state-of-the-art methods.

Conclusion: The proposed ProSplat framework effectively overcomes challenges in wide-baseline novel view synthesis, providing high-fidelity results.

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising
results for novel view synthesis (NVS) from sparse input views, particularly
under narrow-baseline conditions. However, its performance significantly
degrades in wide-baseline scenarios due to limited texture details and
geometric inconsistencies across views. To address these challenges, in this
paper, we propose ProSplat, a two-stage feed-forward framework designed for
high-fidelity rendering under wide-baseline conditions. The first stage
involves generating 3D Gaussian primitives via a 3DGS generator. In the second
stage, rendered views from these primitives are enhanced through an improvement
model. Specifically, this improvement model is based on a one-step diffusion
model, further optimized by our proposed Maximum Overlap Reference view
Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI
supplements missing texture and color by strategically selecting a reference
view with maximum viewpoint overlap, while DWEA enforces geometric consistency
using epipolar constraints. Additionally, we introduce a divide-and-conquer
training strategy that aligns data distributions between the two stages through
joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K
datasets under wide-baseline settings. Experimental results demonstrate that
ProSplat achieves an average improvement of 1 dB in PSNR compared to recent
SOTA methods.

</details>


### [358] [OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting](https://arxiv.org/abs/2506.07697)
*Jens Piekenbrinck,Christian Schmidt,Alexander Hermans,Narunas Vaskevicius,Timm Linder,Bastian Leibe*

Main category: cs.CV

TL;DR: The paper introduces OpenSplat3D, an extension of 3D Gaussian Splatting that enables open-vocabulary 3D instance segmentation using semantic association and text-driven identification.


<details>
  <summary>Details</summary>
Motivation: Existing neural scene reconstruction methods, like 3D Gaussian Splatting, achieve high-quality views but lack the capability for detailed scene understanding, such as open-vocabulary 3D instance segmentation.

Method: The authors integrate techniques like feature-splatting for semantic association, Segment Anything Model masks with a contrastive loss for instance segmentation, and language embeddings for text-based object identification.

Result: OpenSplat3D successfully achieves high-quality 3D instance segmentation, demonstrating its efficacy on datasets like LERF-mask, LERF-OVS, and ScanNet++ validation data.

Conclusion: By combining semantic reasoning and language-based identification, OpenSplat3D expands 3DGS's scope to scene understanding, enabling flexible and accurate instance segmentation in 3D environments.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful representation for
neural scene reconstruction, offering high-quality novel view synthesis while
maintaining computational efficiency. In this paper, we extend the capabilities
of 3DGS beyond pure scene representation by introducing an approach for
open-vocabulary 3D instance segmentation without requiring manual labeling,
termed OpenSplat3D. Our method leverages feature-splatting techniques to
associate semantic information with individual Gaussians, enabling fine-grained
scene understanding. We incorporate Segment Anything Model instance masks with
a contrastive loss formulation as guidance for the instance features to achieve
accurate instance-level segmentation. Furthermore, we utilize language
embeddings of a vision-language model, allowing for flexible, text-driven
instance identification. This combination enables our system to identify and
segment arbitrary objects in 3D scenes based on natural language descriptions.
We show results on LERF-mask and LERF-OVS as well as the full ScanNet++
validation set, demonstrating the effectiveness of our approach.

</details>


### [359] [NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation](https://arxiv.org/abs/2506.07698)
*Yuxiao Yang,Peihao Li,Yuhong Zhang,Junzhe Lu,Xianglong He,Minghan Qin,Weitao Wang,Haoqian Wang*

Main category: cs.CV

TL;DR: NOVA3D is a framework converting single images to 3D content, leveraging video diffusion models and advanced geometric alignment techniques.


<details>
  <summary>Details</summary>
Motivation: To address the issue of insufficient multi-view consistency in 3D content generation methods powered by pretrained image diffusion models.

Method: NOVA3D uses pretrained video diffusion models, Geometry-Temporal Alignment (GTA) attention, and a de-conflict geometry fusion algorithm to ensure high-quality, consistent 3D generation.

Result: The proposed approach achieves superior performance compared to existing baselines, as validated by extensive experiments.

Conclusion: NOVA3D successfully enhances multi-view consistency, texture fidelity, and generalization in single-image-to-3D generation frameworks.

Abstract: 3D AI-generated content (AIGC) has made it increasingly accessible for anyone
to become a 3D content creator. While recent methods leverage Score
Distillation Sampling to distill 3D objects from pretrained image diffusion
models, they often suffer from inadequate 3D priors, leading to insufficient
multi-view consistency. In this work, we introduce NOVA3D, an innovative
single-image-to-3D generation framework. Our key insight lies in leveraging
strong 3D priors from a pretrained video diffusion model and integrating
geometric information during multi-view video fine-tuning. To facilitate
information exchange between color and geometric domains, we propose the
Geometry-Temporal Alignment (GTA) attention mechanism, thereby improving
generalization and multi-view consistency. Moreover, we introduce the
de-conflict geometry fusion algorithm, which improves texture fidelity by
addressing multi-view inaccuracies and resolving discrepancies in pose
alignment. Extensive experiments validate the superiority of NOVA3D over
existing baselines.

</details>


### [360] [Adaptive Blind Super-Resolution Network for Spatial-Specific and Spatial-Agnostic Degradations](https://arxiv.org/abs/2506.07705)
*Weilei Wen,Chunle Guo,Wenqi Ren,Hongpeng Wang,Xiuli Shao*

Main category: cs.CV

TL;DR: The paper introduces a dynamic filter network that addresses two main types of image degradations—spatial-agnostic and spatial-specific—and achieves superior performance in blind super-resolution tasks.


<details>
  <summary>Details</summary>
Motivation: To handle diverse image degradation types more effectively, as prior methods used a single uniform model without considering distinct deteriorations.

Method: The paper proposes a dynamic filter network with global and local branches, each designed to handle specific degradation types using attention mechanisms and dynamic filtering operations.

Result: The proposed approach surpasses state-of-the-art blind super-resolution algorithms on both synthetic and real image datasets.

Conclusion: Integrating global and local dynamic filtering enables effective handling of diverse image degradation types, offering improved performance in super-resolution tasks.

Abstract: Prior methodologies have disregarded the diversities among distinct
degradation types during image reconstruction, employing a uniform network
model to handle multiple deteriorations. Nevertheless, we discover that
prevalent degradation modalities, including sampling, blurring, and noise, can
be roughly categorized into two classes. We classify the first class as
spatial-agnostic dominant degradations, less affected by regional changes in
image space, such as downsampling and noise degradation. The second class
degradation type is intimately associated with the spatial position of the
image, such as blurring, and we identify them as spatial-specific dominant
degradations. We introduce a dynamic filter network integrating global and
local branches to address these two degradation types. This network can greatly
alleviate the practical degradation problem. Specifically, the global dynamic
filtering layer can perceive the spatial-agnostic dominant degradation in
different images by applying weights generated by the attention mechanism to
multiple parallel standard convolution kernels, enhancing the network's
representation ability. Meanwhile, the local dynamic filtering layer converts
feature maps of the image into a spatially specific dynamic filtering operator,
which performs spatially specific convolution operations on the image features
to handle spatial-specific dominant degradations. By effectively integrating
both global and local dynamic filtering operators, our proposed method
outperforms state-of-the-art blind super-resolution algorithms in both
synthetic and real image datasets.

</details>


### [361] [Consistent Video Editing as Flow-Driven Image-to-Video Generation](https://arxiv.org/abs/2506.07713)
*Ge Wang,Songlin Fan,Hangxu Liu,Quanjian Song,Hewei Wang,Jinfeng Xu*

Main category: cs.CV

TL;DR: This paper introduces FlowV2V, a video editing method leveraging flow-driven video generation for improved handling of complex motions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with video editing tasks involving non-rigid motion like multi-object or portrait editing, limiting effective motion modeling.

Method: The method involves decomposing video editing into two stages: first-frame editing and flow-based Image-to-Video generation while simulating pseudo flow sequences for motion consistency.

Result: FlowV2V achieved superior performance on the DAVIS-EDIT dataset, improving DOVER scores by 13.67% and warping errors by 50.66%, showcasing its temporal consistency and quality.

Conclusion: FlowV2V effectively addresses motion transfer challenges in video editing, offering a promising framework for handling complex non-rigid motion patterns.

Abstract: With the prosper of video diffusion models, down-stream applications like
video editing have been significantly promoted without consuming much
computational cost. One particular challenge in this task lies at the motion
transfer process from the source video to the edited one, where it requires the
consideration of the shape deformation in between, meanwhile maintaining the
temporal consistency in the generated video sequence. However, existing methods
fail to model complicated motion patterns for video editing, and are
fundamentally limited to object replacement, where tasks with non-rigid object
motions like multi-object and portrait editing are largely neglected. In this
paper, we observe that optical flows offer a promising alternative in complex
motion modeling, and present FlowV2V to re-investigate video editing as a task
of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V
decomposes the entire pipeline into first-frame editing and conditional I2V
generation, and simulates pseudo flow sequence that aligns with the deformed
shape, thus ensuring the consistency during editing. Experimental results on
DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error
illustrate the superior temporal consistency and sample quality of FlowV2V
compared to existing state-of-the-art ones. Furthermore, we conduct
comprehensive ablation studies to analyze the internal functionalities of the
first-frame paradigm and flow alignment in the proposed method.

</details>


### [362] [ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks](https://arxiv.org/abs/2506.07720)
*Yufei Guo,Yuhan Zhang,Zhou Jie,Xiaode Liu,Xin Tong,Yuanpei Chen,Weihang Peng,Zhe Ma*

Main category: cs.CV

TL;DR: The paper introduces ReverB-SNN, an improved Spiking Neural Network model using real-valued spike activations and binary weights, which enhances accuracy while retaining energy efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the accuracy limitations faced by traditional SNNs due to their binary spike activations failing to capture sufficient data information.

Method: The method involves using real-valued spike activations combined with binary weights and incorporating a trainable binary weight factor to improve network capacity, while maintaining energy efficiency via re-parameterization during inference.

Result: Extensive experiments show that ReverB-SNN consistently outperforms state-of-the-art frameworks across diverse network architectures and datasets.

Conclusion: ReverB-SNN balances improved accuracy, adaptability, and energy efficiency, addressing SNN limitations effectively.

Abstract: The Spiking Neural Network (SNN), a biologically inspired neural network
infrastructure, has garnered significant attention recently. SNNs utilize
binary spike activations for efficient information transmission, replacing
multiplications with additions, thereby enhancing energy efficiency. However,
binary spike activation maps often fail to capture sufficient data information,
resulting in reduced accuracy. To address this challenge, we advocate reversing
the bit of the weight and activation for SNNs, called \textbf{ReverB-SNN},
inspired by recent findings that highlight greater accuracy degradation from
quantizing activations compared to weights. Specifically, our method employs
real-valued spike activations alongside binary weights in SNNs. This preserves
the event-driven and multiplication-free advantages of standard SNNs while
enhancing the information capacity of activations. Additionally, we introduce a
trainable factor within binary weights to adaptively learn suitable weight
amplitudes during training, thereby increasing network capacity. To maintain
efficiency akin to vanilla \textbf{ReverB-SNN}, our trainable binary weight
SNNs are converted back to standard form using a re-parameterization technique
during inference. Extensive experiments across various network architectures
and datasets, both static and dynamic, demonstrate that our approach
consistently outperforms state-of-the-art methods.

</details>


### [363] [ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models](https://arxiv.org/abs/2506.07725)
*Shadi Hamdan,Chonghao Sima,Zetong Yang,Hongyang Li,Fatma Güney*

Main category: cs.CV

TL;DR: Efficiency through Thinking Ahead (ETA) system shifts intensive computations to previous steps, enabling large models in self-driving systems to respond promptly without compromising inference speed.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenge of leveraging large models in self-driving systems without sacrificing inference speed, particularly for timely decisions in dynamic environments.

Method: The proposed ETA system employs an asynchronous dual-system approach by propagating past features via future predictions from large models, extracting current features through a small model, and integrating these features using an action mask emphasizing critical image regions.

Result: Using the Bench2Drive CARLA Leaderboard-v2 benchmark, ETA improves state-of-the-art performance by 8%, achieving a driving score of 69.53 while maintaining near-real-time inference at 50 ms.

Conclusion: ETA effectively combines the strengths of small and large models in a dual-system architecture to enhance self-driving systems' performance and responsiveness, setting a new benchmark in both efficiency and accuracy.

Abstract: How can we benefit from large models without sacrificing inference speed, a
common dilemma in self-driving systems? A prevalent solution is a dual-system
architecture, employing a small model for rapid, reactive decisions and a
larger model for slower but more informative analyses. Existing dual-system
designs often implement parallel architectures where inference is either
directly conducted using the large model at each current frame or retrieved
from previously stored inference results. However, these works still struggle
to enable large models for a timely response to every online frame. Our key
insight is to shift intensive computations of the current frame to previous
time steps and perform a batch inference of multiple time steps to make large
models respond promptly to each time step. To achieve the shifting, we
introduce Efficiency through Thinking Ahead (ETA), an asynchronous system
designed to: (1) propagate informative features from the past to the current
frame using future predictions from the large model, (2) extract current frame
features using a small model for real-time responsiveness, and (3) integrate
these dual features via an action mask mechanism that emphasizes
action-critical image regions. Evaluated on the Bench2Drive CARLA
Leaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with
a driving score of 69.53 while maintaining a near-real-time inference speed at
50 ms.

</details>


### [364] [SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding](https://arxiv.org/abs/2506.07737)
*Xuemei Chen,Huamin Wang,Hangchi Shen,Shukai Duan,Shiping Wen,Tingwen Huang*

Main category: cs.CV

TL;DR: This paper proposes SpikeSMOKE, a low-power spiking neural network-based architecture for monocular 3D object detection.


<details>
  <summary>Details</summary>
Motivation: To address increasing energy consumption in 3D object detection applications such as autonomous driving by leveraging the low-power characteristics of spiking neural networks.

Method: The authors introduced SpikeSMOKE, incorporating a novel cross-scale gated coding mechanism to enhance feature representation and a lightweight residual block to reduce computational costs.

Result: Compared to baseline models, the proposed architecture achieves notable improvements in detection performance on the KITTI dataset (e.g., +3.17 for Hard category) and significantly reduces energy consumption, such as a 72.2% reduction for the Hard category.

Conclusion: SpikeSMOKE represents an effective step towards energy-efficient 3D object detection, offering a balance between detection performance and energy savings, while lightweight variants further optimize computational efficiency.

Abstract: Low energy consumption for 3D object detection is an important research area
because of the increasing energy consumption with their wide application in
fields such as autonomous driving. The spiking neural networks (SNNs) with
low-power consumption characteristics can provide a novel solution for this
research. Therefore, we apply SNNs to monocular 3D object detection and propose
the SpikeSMOKE architecture in this paper, which is a new attempt for low-power
monocular 3D object detection. As we all know, discrete signals of SNNs will
generate information loss and limit their feature expression ability compared
with the artificial neural networks (ANNs).In order to address this issue,
inspired by the filtering mechanism of biological neuronal synapses, we propose
a cross-scale gated coding mechanism(CSGC), which can enhance feature
representation by combining cross-scale fusion of attentional methods and gated
filtering mechanisms.In addition, to reduce the computation and increase the
speed of training, we present a novel light-weight residual block that can
maintain spiking computing paradigm and the highest possible detection
performance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,
the proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,
Moderate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by
AP|R11 at 0.7 IoU threshold, respectively. It is important to note that the
results of SpikeSMOKE can significantly reduce energy consumption compared to
the results on SMOKE. For example,the energy consumption can be reduced by
72.2% on the hard category, while the detection performance is reduced by only
4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3
times and computation by 10 times compared to SMOKE.

</details>


### [365] [AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization](https://arxiv.org/abs/2506.07738)
*Lanjiong Li,Guanhua Zhao,Lingting Zhu,Zeyu Cai,Lequan Yu,Jian Zhang,Zeyu Wang*

Main category: cs.CV

TL;DR: AssetDropper is a novel model that extracts standardized assets from reference images for creative design purposes, overcoming challenges like perspective distortion and occlusion.


<details>
  <summary>Details</summary>
Motivation: Current generative models lack specialized tools for extracting standardized design assets, which are highly valued by designers working with visual elements.

Method: The paper introduces AssetDropper, a model that uses a dataset of 200K synthetic image-subject pairs and a reward model for consistency. The reward model refines training by pasting extracted assets back into the reference images.

Result: AssetDropper demonstrates state-of-the-art performance in asset extraction tasks, addressing challenges in real-world benchmarks.

Conclusion: AssetDropper establishes a promising framework for asset extraction, enabling better utilization of generative models in creative design workflows and enabling further exploration in downstream tasks.

Abstract: Recent research on generative models has primarily focused on creating
product-ready visual outputs; however, designers often favor access to
standardized asset libraries, a domain that has yet to be significantly
enhanced by generative capabilities. Although open-world scenes provide ample
raw materials for designers, efficiently extracting high-quality, standardized
assets remains a challenge. To address this, we introduce AssetDropper, the
first framework designed to extract assets from reference images, providing
artists with an open-world asset palette. Our model adeptly extracts a front
view of selected subjects from input images, effectively handling complex
scenarios such as perspective distortion and subject occlusion. We establish a
synthetic dataset of more than 200,000 image-subject pairs and a real-world
benchmark with thousands more for evaluation, facilitating the exploration of
future research in downstream tasks. Furthermore, to ensure precise asset
extraction that aligns well with the image prompts, we employ a pre-trained
reward model to fulfill a closed-loop with feedback. We design the reward model
to perform an inverse task that pastes the extracted assets back into the
reference sources, which assists training with additional consistency and
mitigates hallucination. Extensive experiments show that, with the aid of
reward-driven optimization, AssetDropper achieves the state-of-the-art results
in asset extraction. Project page: AssetDropper.github.io.

</details>


### [366] [ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models](https://arxiv.org/abs/2506.07739)
*Jing Zhong,Jun Yin,Peilin Li,Pengyu Zeng,Miao Zhang,Shuai Lu,Ran Luo*

Main category: cs.CV

TL;DR: This paper introduces ArchDiffBench, a dataset for architectural styles, and ArchiLense, a Vision-Language Model framework achieving high accuracy in style recognition and comparative analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional architectural studies are often subjective and limited by regional biases, necessitating a more objective approach to analyzing stylistic diversity across regions.

Method: The authors developed a dataset called ArchDiffBench, consisting of annotated architectural images, and introduced ArchiLense, a framework combining Vision-Language Models and advanced AI techniques for precise style classification.

Result: ArchiLense demonstrated strong performance with 92.4% consistency with expert annotations and 84.5% classification accuracy, effectively distinguishing architectural styles.

Conclusion: The paper provides an objective framework for studying architectural styles, overcoming limitations of traditional methods and enhancing the analysis of stylistic diversity.

Abstract: Architectural cultures across regions are characterized by stylistic
diversity, shaped by historical, social, and technological contexts in addition
to geograph-ical conditions. Understanding architectural styles requires the
ability to describe and analyze the stylistic features of different architects
from various regions through visual observations of architectural imagery.
However, traditional studies of architectural culture have largely relied on
subjective expert interpretations and historical literature reviews, often
suffering from regional biases and limited ex-planatory scope. To address these
challenges, this study proposes three core contributions: (1) We construct a
professional architectural style dataset named ArchDiffBench, which comprises
1,765 high-quality architectural images and their corresponding style
annotations, collected from different regions and historical periods. (2) We
propose ArchiLense, an analytical framework grounded in Vision-Language Models
and constructed using the ArchDiffBench dataset. By integrating ad-vanced
computer vision techniques, deep learning, and machine learning algo-rithms,
ArchiLense enables automatic recognition, comparison, and precise
classi-fication of architectural imagery, producing descriptive language
outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show
that ArchiLense achieves strong performance in architectural style recognition,
with a 92.4% con-sistency rate with expert annotations and 84.5% classification
accuracy, effec-tively capturing stylistic distinctions across images. The
proposed approach transcends the subjectivity inherent in traditional analyses
and offers a more objective and accurate perspective for comparative studies of
architectural culture.

</details>


### [367] [Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images](https://arxiv.org/abs/2506.07740)
*Yingping Liang,Ying Fu,Yutao Hu,Wenqi Shao,Jiaming Liu,Debing Zhang*

Main category: cs.CV

TL;DR: The paper introduces "Flow-Anything," a framework that generates optical flow training data from single-view images using 3D representation conversion and rendering techniques, improving real-world robustness for vision tasks.


<details>
  <summary>Details</summary>
Motivation: Optical flow estimation in computer vision relies heavily on synthetic datasets, which limit real-world robustness and benefits from dataset scaling. The study seeks to bridge the domain gap.

Method: The framework involves converting single-view images into 3D representations using depth estimation, followed by volume rendering and inpainting to account for dynamic objects, enabling large-scale realistic dataset generation.

Result: The generated FA-Flow Dataset outperforms unsupervised and supervised methods trained on synthetic datasets in optical flow estimation. The models also enhance downstream video tasks.

Conclusion: Flow-Anything effectively addresses domain gaps in optical flow estimation, proving the utility of leveraging single-view real-world images for training and advancing related applications.

Abstract: Optical flow estimation is a crucial subfield of computer vision, serving as
a foundation for video tasks. However, the real-world robustness is limited by
animated synthetic datasets for training. This introduces domain gaps when
applied to real-world applications and limits the benefits of scaling up
datasets. To address these challenges, we propose \textbf{Flow-Anything}, a
large-scale data generation framework designed to learn optical flow estimation
from any single-view images in the real world. We employ two effective steps to
make data scaling-up promising. First, we convert a single-view image into a 3D
representation using advanced monocular depth estimation networks. This allows
us to render optical flow and novel view images under a virtual camera. Second,
we develop an Object-Independent Volume Rendering module and a Depth-Aware
Inpainting module to model the dynamic objects in the 3D representation. These
two steps allow us to generate realistic datasets for training from large-scale
single-view images, namely \textbf{FA-Flow Dataset}. For the first time, we
demonstrate the benefits of generating optical flow training data from
large-scale real-world images, outperforming the most advanced unsupervised
methods and supervised methods on synthetic datasets. Moreover, our models
serve as a foundation model and enhance the performance of various downstream
video tasks.

</details>


### [368] [Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation](https://arxiv.org/abs/2506.07750)
*Hyunsoo Kim,Donghyun Kim,Suhyun Kim*

Main category: cs.CV

TL;DR: The paper proposes a method called Difference Inversion to generate image B' in a manner analogous to input images A:A'::B:B', focusing on model-agnostic approaches for stable diffusion models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing image transformation approaches, which are tied to specific models and suffer from biases or reduced editing capabilities.

Method: Difference Inversion isolates differences between input images A and A' using techniques like Delta Interpolation, Token Consistency Loss, and Zero Initialization of Token Embeddings, and incorporates these differences into the prompt of B for use with general diffusion models.

Result: Difference Inversion shows superior performance over baselines in generating plausible and high-quality B' both quantitatively and qualitatively, without relying on specific models.

Conclusion: The approach effectively addresses model dependency, making it possible to apply differences in a plug-and-play manner in stable diffusion models, advancing the state of image transformation.

Abstract: How can we generate an image B' that satisfies A:A'::B:B', given the input
images A,A' and B? Recent works have tackled this challenge through approaches
like visual in-context learning or visual instruction. However, these methods
are typically limited to specific models (e.g. InstructPix2Pix. Inpainting
models) rather than general diffusion models (e.g. Stable Diffusion, SDXL).
This dependency may lead to inherited biases or lower editing capabilities. In
this paper, we propose Difference Inversion, a method that isolates only the
difference from A and A' and applies it to B to generate a plausible B'. To
address model dependency, it is crucial to structure prompts in the form of a
"Full Prompt" suitable for input to stable diffusion models, rather than using
an "Instruction Prompt". To this end, we accurately extract the Difference
between A and A' and combine it with the prompt of B, enabling a plug-and-play
application of the difference. To extract a precise difference, we first
identify it through 1) Delta Interpolation. Additionally, to ensure accurate
training, we propose the 2) Token Consistency Loss and 3) Zero Initialization
of Token Embeddings. Our extensive experiments demonstrate that Difference
Inversion outperforms existing baselines both quantitatively and qualitatively,
indicating its ability to generate more feasible B' in a model-agnostic manner.

</details>


### [369] [Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity](https://arxiv.org/abs/2506.07773)
*Mohamed Djilani,Nassim Ali Ousalah,Nidhal Eddine Chenni*

Main category: cs.CV

TL;DR: The paper proposes a fashion recommendation system integrating visual embeddings, garment segmentation, and user behavior simulation, improving on personalized and trend-aware predictions.


<details>
  <summary>Details</summary>
Motivation: Developing a recommendation system that balances personal style preferences with evolving fashion trends.

Method: The authors integrate garment semantic segmentation, pretrained visual feature extraction using CNNs, and synthetic user purchase history aligned to item popularity.

Result: Their pipeline achieved notably improved category similarity and reduced popularity errors in experiments on the DeepFashion dataset, with ResNet-50 being the most efficient model.

Conclusion: The presented system demonstrates scalability and state-of-the-art capability in generating personalized fashion recommendations while adapting to changing trends in fashion.

Abstract: We introduce a trend-aware and visually-grounded fashion recommendation
system that integrates deep visual representations, garment-aware segmentation,
semantic category similarity and user behavior simulation. Our pipeline
extracts focused visual embeddings by masking non-garment regions via semantic
segmentation followed by feature extraction using pretrained CNN backbones
(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we
generate synthetic purchase histories influenced by user-specific trendiness
and item popularity. Recommendations are computed using a weighted scoring
function that fuses visual similarity, semantic coherence and popularity
alignment. Experiments on the DeepFashion dataset demonstrate consistent gender
alignment and improved category relevance, with ResNet-50 achieving 64.95%
category similarity and lowest popularity MAE. An ablation study confirms the
complementary roles of visual and popularity cues. Our method provides a
scalable framework for personalized fashion recommendations that balances
individual style with emerging trends. Our implementation is available at
https://github.com/meddjilani/FashionRecommender

</details>


### [370] [Language-Vision Planner and Executor for Text-to-Visual Reasoning](https://arxiv.org/abs/2506.07778)
*Yichang Xu,Gaowen Liu,Ramana Rao Kompella,Sihao Hu,Tiansheng Huang,Fatih Ilhan,Selim Furkan Tekin,Zachary Yahn,Ling Liu*

Main category: cs.CV

TL;DR: VLAgent is introduced to address generalization issues in vision-language models (VLMs) for visual-text reasoning. It improves performance through a step-by-step reasoning plan and execution refinement, showing significant enhancement across four benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with generalization in multimodal visual-text reasoning tasks. Inspired by LLMs' advancements, the study aims to enhance reasoning ability and reduce errors in vision-language models.

Method: VLAgent employs fine-tuned LLMs for step-by-step planning and execution, paired with syntax-semantics parsing, plan repairing, ensemble methods, and neuro-symbolic executable modules to achieve accurate reasoning.

Result: The system demonstrates high performance on GQA, MME, NLVR2, and VQAv2 benchmarks, outperforming state-of-the-art models like ViperGPT and VisProg.

Conclusion: VLAgent's novel design optimizes visual-text reasoning by integrating planning and execution processes, reducing logic errors, and improving generalization. The code and data will be publicly available for future research.

Abstract: The advancement in large language models (LLMs) and large vision models has
fueled the rapid progress in multi-modal visual-text reasoning capabilities.
However, existing vision-language models (VLMs) to date suffer from
generalization performance. Inspired by recent development in LLMs for visual
reasoning, this paper presents VLAgent, an AI system that can create a
step-by-step visual reasoning plan with an easy-to-understand script and
execute each step of the plan in real time by integrating planning script with
execution verifications via an automated process supported by VLAgent. In the
task planning phase, VLAgent fine-tunes an LLM through in-context learning to
generate a step-by-step planner for each user-submitted text-visual reasoning
task. During the plan execution phase, VLAgent progressively refines the
composition of neuro-symbolic executable modules to generate high-confidence
reasoning results. VLAgent has three unique design characteristics: First, we
improve the quality of plan generation through in-context learning, improving
logic reasoning by reducing erroneous logic steps, incorrect programs, and LLM
hallucinations. Second, we design a syntax-semantics parser to identify and
correct additional logic errors of the LLM-generated planning script prior to
launching the plan executor. Finally, we employ the ensemble method to improve
the generalization performance of our step-executor. Extensive experiments with
four visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgent
achieves significant performance enhancement for multimodal text-visual
reasoning applications, compared to the exiting representative VLMs and LLM
based visual composition approaches like ViperGPT and VisProg, thanks to the
novel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer,
Output Verifiers). Code and data will be made available upon paper acceptance.

</details>


### [371] [Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion Methods](https://arxiv.org/abs/2506.07779)
*Beining Xu,Junxian Li*

Main category: cs.CV

TL;DR: This paper introduces a novel visible and infrared image dataset captured in campus scenarios and proposes a new evaluation framework to benchmark image fusion algorithms, emphasizing downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of visible-infrared fusion algorithms lack standardized benchmarks, adequate datasets, and robust assessments for downstream tasks, limiting progress in this domain.

Method: The authors constructed a dual-spectrum dataset containing 1,369 visible-infrared image pairs across various scenarios and implemented an evaluation framework incorporating fusion speed, general metrics, and downstream task performance using advanced models.

Result: The evaluation of state-of-the-art algorithms using the proposed framework highlights that methods optimized for downstream tasks like target detection offer superior performance, particularly in challenging scenarios. However, general-purpose metrics can fail to reflect such improvements.

Conclusion: This work underscores the need for task-aware evaluation of image fusion algorithms and provides valuable resources—including a diverse dataset and evaluation framework—to advance research in this field.

Abstract: Visible images offer rich texture details, while infrared images emphasize
salient targets. Fusing these complementary modalities enhances scene
understanding, particularly for advanced vision tasks under challenging
conditions. Recently, deep learning-based fusion methods have gained attention,
but current evaluations primarily rely on general-purpose metrics without
standardized benchmarks or downstream task performance. Additionally, the lack
of well-developed dual-spectrum datasets and fair algorithm comparisons hinders
progress.
  To address these gaps, we construct a high-quality dual-spectrum dataset
captured in campus environments, comprising 1,369 well-aligned visible-infrared
image pairs across four representative scenarios: daytime, nighttime, smoke
occlusion, and underpasses. We also propose a comprehensive and fair evaluation
framework that integrates fusion speed, general metrics, and object detection
performance using the lang-segment-anything model to ensure fairness in
downstream evaluation.
  Extensive experiments benchmark several state-of-the-art fusion algorithms
under this framework. Results demonstrate that fusion models optimized for
downstream tasks achieve superior performance in target detection, especially
in low-light and occluded scenes. Notably, some algorithms that perform well on
general metrics do not translate to strong downstream performance, highlighting
limitations of current evaluation practices and validating the necessity of our
proposed framework.
  The main contributions of this work are: (1)a campus-oriented dual-spectrum
dataset with diverse and challenging scenes; (2) a task-aware, comprehensive
evaluation framework; and (3) thorough comparative analysis of leading fusion
methods across multiple datasets, offering insights for future development.

</details>


### [372] [Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger](https://arxiv.org/abs/2506.07785)
*Qi Yang,Chenghao Zhang,Lubin Fan,Kun Ding,Jieping Ye,Shiming Xiang*

Main category: cs.CV

TL;DR: This paper introduces the RCTS framework to improve Large Vision Language Models (LVLMs) in Visual Question Answering (VQA) by enriching reasoning contexts and utilizing an advanced re-ranking method.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current multimodal Retrieval-Augmented Generation (RAG) methods in addressing reasoning example scarcity and erratic responses from retrieved knowledge.

Method: The RCTS framework combines a Reasoning Context-enriched knowledge base and a Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) for re-ranking. It employs a self-consistent evaluation mechanism to refine the knowledge base with reasoning patterns.

Result: RCTS achieves state-of-the-art performance on various VQA datasets, outperforming existing methods like In-Context Learning (ICL) and Vanilla-RAG.

Conclusion: The proposed framework demonstrates the effectiveness of improving LVLM output in VQA tasks through enhanced reasoning contexts and advanced selection methods.

Abstract: Recent advancements in Large Vision Language Models (LVLMs) have
significantly improved performance in Visual Question Answering (VQA) tasks
through multimodal Retrieval-Augmented Generation (RAG). However, existing
methods still face challenges, such as the scarcity of knowledge with reasoning
examples and erratic responses from retrieved knowledge. To address these
issues, in this study, we propose a multimodal RAG framework, termed RCTS,
which enhances LVLMs by constructing a Reasoning Context-enriched knowledge
base and a Tree Search re-ranking method. Specifically, we introduce a
self-consistent evaluation mechanism to enrich the knowledge base with
intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with
Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This
ensures that LVLMs can leverage high-quality contextual reasoning for better
and more consistent responses. Extensive experiments demonstrate that our
framework achieves state-of-the-art performance on multiple VQA datasets,
significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.
It highlights the effectiveness of our knowledge base and re-ranking method in
improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.

</details>


### [373] [Image Reconstruction as a Tool for Feature Analysis](https://arxiv.org/abs/2506.07803)
*Eduard Allakhverdov,Dmitrii Tarasov,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.CV

TL;DR: This paper introduces a novel methodology for decoding how vision encoders represent features by leveraging image reconstruction, comparing encoders trained on image-based versus non-image-based tasks.


<details>
  <summary>Details</summary>
Motivation: Understanding how vision encoders internally represent features is crucial for improving their interpretability, performance, and customization in modern applications.

Method: The authors propose an image reconstruction-based approach, comparing the feature representation of two encoder families (SigLIP and SigLIP2) and ranking various vision encoders based on the informativeness of their representations.

Result: Encoders trained on image-specific tasks retain significantly more image details than those trained on non-image tasks. Additionally, the paper shows that feature space manipulations yield predictable image changes, revealing unique encoding mechanisms like orthogonal rotation's impact on color representation.

Conclusion: The proposed method provides a universal tool for inspecting the structure of feature spaces in vision encoders, offering insights that can inform their future design and utilization.

Abstract: Vision encoders are increasingly used in modern applications, from
vision-only models to multimodal systems such as vision-language models.
Despite their remarkable success, it remains unclear how these architectures
represent features internally. Here, we propose a novel approach for
interpreting vision features via image reconstruction. We compare two related
model families, SigLIP and SigLIP2, which differ only in their training
objective, and show that encoders pre-trained on image-based tasks retain
significantly more image information than those trained on non-image tasks such
as contrastive learning. We further apply our method to a range of vision
encoders, ranking them by the informativeness of their feature representations.
Finally, we demonstrate that manipulating the feature space yields predictable
changes in reconstructed images, revealing that orthogonal rotations (rather
than spatial transformations) control color encoding. Our approach can be
applied to any vision encoder, shedding light on the inner structure of its
feature space. The code and model weights to reproduce the experiments are
available in GitHub.

</details>


### [374] [Incorporating Uncertainty-Guided and Top-k Codebook Matching for Real-World Blind Image Super-Resolution](https://arxiv.org/abs/2506.07809)
*Weilei Wen,Tianyi Zhang,Qianqian Zhao,Zhaohui Zheng,Chunle Guo,Xiuli Shao,Chongyi Li*

Main category: cs.CV

TL;DR: The paper introduces the UGTSR framework, improving real image super-resolution (SR) by addressing feature matching and texture reconstruction challenges.


<details>
  <summary>Details</summary>
Motivation: To overcome feature matching inaccuracies and poor texture detail reconstruction in real image SR.

Method: The UGTSR framework includes three components: uncertainty learning for texture-rich focus, Top-k feature matching for higher accuracy, and Align-Attention module for better LR-HR feature alignment.

Result: Experimental results show that UGTSR achieves significant improvements in texture fidelity and reconstruction quality over existing methods.

Conclusion: UGTSR provides a novel, effective approach to enhance real-world image SR, with improved results and plans for code release after publication.

Abstract: Recent advancements in codebook-based real image super-resolution (SR) have
shown promising results in real-world applications. The core idea involves
matching high-quality image features from a codebook based on low-resolution
(LR) image features. However, existing methods face two major challenges:
inaccurate feature matching with the codebook and poor texture detail
reconstruction. To address these issues, we propose a novel Uncertainty-Guided
and Top-k Codebook Matching SR (UGTSR) framework, which incorporates three key
components: (1) an uncertainty learning mechanism that guides the model to
focus on texture-rich regions, (2) a Top-k feature matching strategy that
enhances feature matching accuracy by fusing multiple candidate features, and
(3) an Align-Attention module that enhances the alignment of information
between LR and HR features. Experimental results demonstrate significant
improvements in texture realism and reconstruction fidelity compared to
existing methods. We will release the code upon formal publication.

</details>


### [375] [Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning](https://arxiv.org/abs/2506.07811)
*Tieyuan Chen,Huabin Liu,Yi Wang,Chaofan Gan,Mingxi Lyu,Gui Zou,Weiyao Lin*

Main category: cs.CV

TL;DR: This paper introduces a new task and dataset named I-VQA (Implicit Video Question Answering) for answering questions when explicit visual evidence is unavailable and develops the IRM model for addressing this challenge.


<details>
  <summary>Details</summary>
Motivation: Previous work on VideoQA focused on explicit visual evidence, leading to poor performance when questions relate to symbolic meanings or deeper intentions without such evidence.

Method: The authors proposed the IRM (Implicit Reasoning Model), which includes an Action-Intent Module (AIM) for deducing dual clues and a Visual Enhancement Module (VEM) for improving visual contextual representations.

Result: IRM achieves superior performance on I-VQA tasks, outperforming competitors like GPT-4o and OpenAI-o3 by margins ranging from 0.76% to 4.87%. It also sets state-of-the-art performance on related tasks, such as implicit advertisement understanding and future prediction in Traffic-VQA.

Conclusion: The proposed IRM framework effectively handles implicit reasoning challenges in video question answering, demonstrating its potential for broader applications in tasks requiring contextual understanding.

Abstract: Video Question Answering (VideoQA) aims to answer natural language questions
based on the given video, with prior work primarily focusing on identifying the
duration of relevant segments, referred to as explicit visual evidence.
However, explicit visual evidence is not always directly available,
particularly when questions target symbolic meanings or deeper intentions,
leading to significant performance degradation. To fill this gap, we introduce
a novel task and dataset, $\textbf{I}$mplicit $\textbf{V}$ideo
$\textbf{Q}$uestion $\textbf{A}$nswering (I-VQA), which focuses on answering
questions in scenarios where explicit visual evidence is inaccessible. Given an
implicit question and its corresponding video, I-VQA requires answering based
on the contextual visual cues present within the video. To tackle I-VQA, we
propose a novel reasoning framework, IRM (Implicit Reasoning Model),
incorporating dual-stream modeling of contextual actions and intent clues as
implicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the
Visual Enhancement Module (VEM). AIM deduces and preserves question-related
dual clues by generating clue candidates and performing relation deduction. VEM
enhances contextual visual representation by leveraging key contextual clues.
Extensive experiments validate the effectiveness of our IRM in I-VQA tasks,
outperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\%$,
$1.37\%$, and $4.87\%$, respectively. Additionally, IRM performs SOTA on
similar implicit advertisement understanding and future prediction in
traffic-VQA. Datasets and codes are available for double-blind review in
anonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.

</details>


### [376] [Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution](https://arxiv.org/abs/2506.07813)
*Junseo Bang,Joonhee Lee,Kyeonghyun Lee,Haechang Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: CasArbi proposes a novel framework for arbitrary-scale image super-resolution using a progressive, self-cascaded diffusion model to enhance image quality across a range of scaling factors.


<details>
  <summary>Details</summary>
Motivation: Current arbitrary-scale super-resolution methods struggle to handle wide and continuous distributions of scaling factors effectively. The study aims to address this gap and improve upscaling flexibility and performance.

Method: The authors introduce CasArbi, a self-cascaded diffusion framework that progressively upscales images using a coordinate-guided residual diffusion model for efficient, seamless transitions across scales.

Result: CasArbi achieved superior performance in perceptual and distortion metrics compared to existing approaches in arbitrary-scale super-resolution benchmarks.

Conclusion: CasArbi offers a robust solution for arbitrary-scale image super-resolution, effectively handling varying scaling demands while improving image quality and achieving state-of-the-art results.

Abstract: Arbitrary-scale image super-resolution aims to upsample images to any desired
resolution, offering greater flexibility than traditional fixed-scale
super-resolution. Recent approaches in this domain utilize regression-based or
generative models, but many of them are a single-stage upsampling process,
which may be challenging to learn across a wide, continuous distribution of
scaling factors. Progressive upsampling strategies have shown promise in
mitigating this issue, yet their integration with diffusion models for flexible
upscaling remains underexplored. Here, we present CasArbi, a novel
self-cascaded diffusion framework for arbitrary-scale image super-resolution.
CasArbi meets the varying scaling demands by breaking them down into smaller
sequential factors and progressively enhancing the image resolution at each
step with seamless transitions for arbitrary scales. Our novel
coordinate-guided residual diffusion model allows for the learning of
continuous image representations while enabling efficient diffusion sampling.
Extensive experiments demonstrate that our CasArbi outperforms prior arts in
both perceptual and distortion performance metrics across diverse
arbitrary-scale super-resolution benchmarks.

</details>


### [377] [M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration](https://arxiv.org/abs/2506.07814)
*Yongzhen Wang,Yongjun Li,Zhuoran Zheng,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: The paper proposes M2Restore, a method to restore images degraded by complex scenarios like rain and haze, achieving superior quality and performance through a novel combination of CNNs and Mamba-based architecture.


<details>
  <summary>Details</summary>
Motivation: Natural images face complex degradations (e.g., rain, haze) that hamper vision applications. Existing techniques struggle with generalization and balancing global-local features.

Method: M2Restore uses a CLIP-guided Mixture-of-Experts (MoE) gating mechanism and fuses CNN with Mamba methods for effective degradation handling. It also introduces edge-aware dynamic gating for focused restoration.

Result: M2Restore outperforms other methods on multiple benchmarks in terms of visual quality and quantitative metrics.

Conclusion: The framework effectively tackles varied degradation types by generalizing and balancing features for superior all-in-one image restoration.

Abstract: Natural images are often degraded by complex, composite degradations such as
rain, snow, and haze, which adversely impact downstream vision applications.
While existing image restoration efforts have achieved notable success, they
are still hindered by two critical challenges: limited generalization across
dynamically varying degradation scenarios and a suboptimal balance between
preserving local details and modeling global dependencies. To overcome these
challenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based
Mamba-CNN fusion framework for efficient and robust all-in-one image
restoration. M2Restore introduces three key contributions: First, to boost the
model's generalization across diverse degradation conditions, we exploit a
CLIP-guided MoE gating mechanism that fuses task-conditioned prompts with
CLIP-derived semantic priors. This mechanism is further refined via cross-modal
feature calibration, which enables precise expert selection for various
degradation types. Second, to jointly capture global contextual dependencies
and fine-grained local details, we design a dual-stream architecture that
integrates the localized representational strength of CNNs with the long-range
modeling efficiency of Mamba. This integration enables collaborative
optimization of global semantic relationships and local structural fidelity,
preserving global coherence while enhancing detail restoration. Third, we
introduce an edge-aware dynamic gating mechanism that adaptively balances
global modeling and local enhancement by reallocating computational attention
to degradation-sensitive regions. This targeted focus leads to more efficient
and precise restoration. Extensive experiments across multiple image
restoration benchmarks validate the superiority of M2Restore in both visual
quality and quantitative performance.

</details>


### [378] [Diffusion models under low-noise regime](https://arxiv.org/abs/2506.07841)
*Elizabeth Pavlova,Xue-Xin Wei*

Main category: cs.CV

TL;DR: This paper investigates how diffusion models behave as denoisers under low-noise conditions, addressing reliability in practical applications.


<details>
  <summary>Details</summary>
Motivation: To explore diffusion models' behavior and reliability under low-noise settings, a scenario less studied compared to high-noise conditions.

Method: The authors systematically studied diffusion dynamics using CelebA subsets and Gaussian mixture benchmarks to analyze divergence and score accuracy in model training.

Result: Models trained on disjoint data diverged near the data manifold under low-noise conditions, with training set size, data geometry, and objectives affecting denoising trajectories.

Conclusion: Diffusion models exhibit nuances in learning representations under low-noise settings, necessitating deeper examination of their robustness for practical reliability.

Abstract: Recent work on diffusion models proposed that they operate in two regimes:
memorization, in which models reproduce their training data, and
generalization, in which they generate novel samples. While this has been
tested in high-noise settings, the behavior of diffusion models as effective
denoisers when the corruption level is small remains unclear. To address this
gap, we systematically investigated the behavior of diffusion models under
low-noise diffusion dynamics, with implications for model robustness and
interpretability. Using (i) CelebA subsets of varying sample sizes and (ii)
analytic Gaussian mixture benchmarks, we reveal that models trained on disjoint
data diverge near the data manifold even when their high-noise outputs
converge. We quantify how training set size, data geometry, and model objective
choice shape denoising trajectories and affect score accuracy, providing
insights into how these models actually learn representations of data
distributions. This work starts to address gaps in our understanding of
generative model reliability in practical applications where small
perturbations are common.

</details>


### [379] [F2Net: A Frequency-Fused Network for Ultra-High Resolution Remote Sensing Segmentation](https://arxiv.org/abs/2506.07847)
*Hengzhi Chen,Liqian Feng,Wenhua Wu,Xiaogang Zhu,Shawn Leo,Kun Hu*

Main category: cs.CV

TL;DR: The paper introduces F2Net, a novel method for semantic segmentation of ultra-high-resolution remote sensing imagery, that addresses computational inefficiency and optimization challenges by utilizing frequency-aware processing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve semantic segmentation of ultra-high-resolution remote sensing imagery, which is vital for environmental monitoring and urban planning but is challenging due to computational inefficiency and traditional methods' inability to balance fine details and global context.

Method: The method involves a frequency-aware framework called F2Net, which decomposes images into high- and low-frequency components. High-frequency data retains structural details, while low-frequency data is split into sub-branches to capture dependencies. The Hybrid-Frequency Fusion module combines them, using novel alignment and balance loss functions to stabilize training and ensure consistency.

Result: F2Net achieved state-of-the-art performance, with mIoU scores of 80.22 on DeepGlobe and 83.39 on Inria Aerial benchmarks.

Conclusion: The results validate that F2Net effectively advances semantic segmentation of UHR imagery, overcoming computational and gradient challenges with a frequency-aware approach, and sets new performance standards.

Abstract: Semantic segmentation of ultra-high-resolution (UHR) remote sensing imagery
is critical for applications like environmental monitoring and urban planning
but faces computational and optimization challenges. Conventional methods
either lose fine details through downsampling or fragment global context via
patch processing. While multi-branch networks address this trade-off, they
suffer from computational inefficiency and conflicting gradient dynamics during
training. We propose F2Net, a frequency-aware framework that decomposes UHR
images into high- and low-frequency components for specialized processing. The
high-frequency branch preserves full-resolution structural details, while the
low-frequency branch processes downsampled inputs through dual sub-branches
capturing short- and long-range dependencies. A Hybrid-Frequency Fusion module
integrates these observations, guided by two novel objectives: Cross-Frequency
Alignment Loss ensures semantic consistency between frequency components, and
Cross-Frequency Balance Loss regulates gradient magnitudes across branches to
stabilize training. Evaluated on DeepGlobe and Inria Aerial benchmarks, F2Net
achieves state-of-the-art performance with mIoU of 80.22 and 83.39,
respectively. Our code will be publicly available.

</details>


### [380] [PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement](https://arxiv.org/abs/2506.07848)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Jiangning Zhang,Yuan Zhou,Qinglin Lu,Ran Yi*

Main category: cs.CV

TL;DR: The paper proposes PolyVivid, a framework for creating identity-consistent and customizable videos with multiple subjects, overcoming limitations in current video generation models.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods struggle with fine-grained control, multi-subject customization, and consistent identity preservation. The paper aims to address these challenges.

Method: PolyVivid introduces several technical innovations: a VLLM-based text-image fusion module for better grounding, a 3D-RoPE-based enhancement module for identity preservation and interaction, an attention-inherited identity injection module to prevent identity drift, and an MLLM-based data pipeline for better subject distinction and data quality.

Result: PolyVivid delivers improved identity fidelity, video realism, and subject alignment, outperforming existing methods in both open-source and commercial settings.

Conclusion: PolyVivid successfully advances the state of video generation by enabling precise, identity-consistent, and multi-subject video customization, making it a significant step forward in the field.

Abstract: Despite recent advances in video generation, existing models still lack
fine-grained controllability, especially for multi-subject customization with
consistent identity and interaction. In this paper, we propose PolyVivid, a
multi-subject video customization framework that enables flexible and
identity-consistent generation. To establish accurate correspondences between
subject images and textual entities, we design a VLLM-based text-image fusion
module that embeds visual identities into the textual space for precise
grounding. To further enhance identity preservation and subject interaction, we
propose a 3D-RoPE-based enhancement module that enables structured
bidirectional fusion between text and image embeddings. Moreover, we develop an
attention-inherited identity injection module to effectively inject fused
identity features into the video generation process, mitigating identity drift.
Finally, we construct an MLLM-based data pipeline that combines MLLM-based
grounding, segmentation, and a clique-based subject consolidation strategy to
produce high-quality multi-subject data, effectively enhancing subject
distinction and reducing ambiguity in downstream video generation. Extensive
experiments demonstrate that PolyVivid achieves superior performance in
identity fidelity, video realism, and subject alignment, outperforming existing
open-source and commercial baselines.

</details>


### [381] [SAM2Auto: Auto Annotation Using FLASH](https://arxiv.org/abs/2506.07850)
*Arash Rocky,Q. M. Jonathan Wu*

Main category: cs.CV

TL;DR: The paper introduces SAM2Auto, an automated pipeline for video dataset annotation to overcome limitations in Vision-Language Models caused by the lack of annotated data.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models are constrained by limited annotated datasets, as creating these datasets is labor-intensive and expensive.

Method: SAM2Auto integrates SMART-OD, a robust open-world object detection system, and FLASH, a video instance segmentation tool ensuring consistency across video frames. It avoids manual tuning through statistical error minimization in detection.

Result: SAM2Auto is validated to achieve annotation accuracy comparable to manual methods while significantly saving time and cutting costs, and it operates effectively across diverse datasets without retraining.

Conclusion: SAM2Auto establishes a new standard for automated video annotations, overcoming dataset bottlenecks, and accelerating vision-language model development.

Abstract: Vision-Language Models (VLMs) lag behind Large Language Models due to the
scarcity of annotated datasets, as creating paired visual-textual annotations
is labor-intensive and expensive. To address this bottleneck, we introduce
SAM2Auto, the first fully automated annotation pipeline for video datasets
requiring no human intervention or dataset-specific training. Our approach
consists of two key components: SMART-OD, a robust object detection system that
combines automatic mask generation with open-world object detection
capabilities, and FLASH (Frame-Level Annotation and Segmentation Handler), a
multi-object real-time video instance segmentation (VIS) that maintains
consistent object identification across video frames even with intermittent
detection gaps. Unlike existing open-world detection methods that require
frame-specific hyperparameter tuning and suffer from numerous false positives,
our system employs statistical approaches to minimize detection errors while
ensuring consistent object tracking throughout entire video sequences.
Extensive experimental validation demonstrates that SAM2Auto achieves
comparable accuracy to manual annotation while dramatically reducing annotation
time and eliminating labor costs. The system successfully handles diverse
datasets without requiring retraining or extensive parameter adjustments,
making it a practical solution for large-scale dataset creation. Our work
establishes a new baseline for automated video annotation and provides a
pathway for accelerating VLM development by addressing the fundamental dataset
bottleneck that has constrained progress in vision-language understanding.

</details>


### [382] [Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction](https://arxiv.org/abs/2506.07860)
*Ivan Alberico,Marco Cannici,Giovanni Cioffi,Davide Scaramuzza*

Main category: cs.CV

TL;DR: The paper introduces a real-time egocentric system using event cameras for trajectory prediction in table tennis, yielding lower latency and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of standard cameras in fast-moving scenarios like table tennis, such as motion blur and high latency.

Method: The system employs event cameras, foveated vision leveraging eye-gaze data, and a low-latency detection pipeline for real-time ball trajectory prediction.

Result: The system achieved a computational latency of 4.5 ms—significantly faster than frame-based systems—and improved ball detection via foveated vision.

Conclusion: This approach demonstrates the first egocentric table tennis trajectory prediction system using event cameras, offering high precision and low latency.

Abstract: In this paper, we present a real-time egocentric trajectory prediction system
for table tennis using event cameras. Unlike standard cameras, which suffer
from high latency and motion blur at fast ball speeds, event cameras provide
higher temporal resolution, allowing more frequent state updates, greater
robustness to outliers, and accurate trajectory predictions using just a short
time window after the opponent's impact. We collect a dataset of ping-pong game
sequences, including 3D ground-truth trajectories of the ball, synchronized
with sensor data from the Meta Project Aria glasses and event streams. Our
system leverages foveated vision, using eye-gaze data from the glasses to
process only events in the viewer's fovea. This biologically inspired approach
improves ball detection performance and significantly reduces computational
latency, as it efficiently allocates resources to the most perceptually
relevant regions, achieving a reduction factor of 10.81 on the collected
trajectories. Our detection pipeline has a worst-case total latency of 4.5 ms,
including computation and perception - significantly lower than a frame-based
30 FPS system, which, in the worst case, takes 66 ms solely for perception.
Finally, we fit a trajectory prediction model to the estimated states of the
ball, enabling 3D trajectory forecasting in the future. To the best of our
knowledge, this is the first approach to predict table tennis trajectories from
an egocentric perspective using event cameras.

</details>


### [383] [VIVAT: Virtuous Improving VAE Training through Artifact Mitigation](https://arxiv.org/abs/2506.07863)
*Lev Novitskiy,Viacheslav Vasilev,Maria Kovaleva,Vladimir Arkhipkin,Denis Dimitrov*

Main category: cs.CV

TL;DR: The paper introduces VIVAT, a method to address artifacts in Variational Autoencoder (VAE) training, improving generation and reconstruction quality through simple modifications.


<details>
  <summary>Details</summary>
Motivation: VAEs are central in generative computer vision but suffer from artifacts during training, which compromise the reconstruction and generation outcomes.

Method: The method involves identifying five artifact types and addressing them through loss weight adjustments, padding strategies, and Spatially Conditional Normalization. These changes are straightforward and do not require complex architectural alterations.

Result: VIVAT improves VAE performance in image reconstruction metrics (PSNR and SSIM) and text-to-image generation quality, marked by better CLIP scores compared to benchmarks.

Conclusion: The paper provides a practical solution to optimize VAE training while retaining its simplicity, benefiting both researchers and practitioners in the field.

Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer
vision, yet their training is often plagued by artifacts that degrade
reconstruction and generation quality. This paper introduces VIVAT, a
systematic approach to mitigating common artifacts in KL-VAE training without
requiring radical architectural changes. We present a detailed taxonomy of five
prevalent artifacts - color shift, grid patterns, blur, corner and droplet
artifacts - and analyze their root causes. Through straightforward
modifications, including adjustments to loss weights, padding strategies, and
the integration of Spatially Conditional Normalization, we demonstrate
significant improvements in VAE performance. Our method achieves
state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across
multiple benchmarks and enhances text-to-image generation quality, as evidenced
by superior CLIP scores. By preserving the simplicity of the KL-VAE framework
while addressing its practical challenges, VIVAT offers actionable insights for
researchers and practitioners aiming to optimize VAE training.

</details>


### [384] [Spatio-Temporal State Space Model For Efficient Event-Based Optical Flow](https://arxiv.org/abs/2506.07878)
*Muhammad Ahmed Humais,Xiaoqian Huang,Hussain Sajwani,Sajid Javed,Yahya Zweiri*

Main category: cs.CV

TL;DR: A new approach, Spatio-Temporal State Space Model (STSSM), is introduced to improve computational efficiency and performance in optical flow estimation using event cameras.


<details>
  <summary>Details</summary>
Motivation: Standard frame-based cameras struggle with low-latency motion estimation required for real-time applications. Event cameras combined with recent deep learning approaches either lack computational efficiency or fail to capture sufficient spatio-temporal information.

Method: The proposed Spatio-Temporal State Space Model (STSSM) integrates state-space modeling to better encode spatio-temporal correlations, coupled with a novel network architecture for optical flow estimation.

Result: The model achieves competitive performance on the DSEC benchmark, 4.5x faster inference than TMA, 8x lower computations compared to TMA, and 2x lower computations compared to EV-FlowNet.

Conclusion: STSSM offers an efficient and high-performing alternative for optical flow estimation in event cameras, enhancing latency and computational efficiency.

Abstract: Event cameras unlock new frontiers that were previously unthinkable with
standard frame-based cameras. One notable example is low-latency motion
estimation (optical flow), which is critical for many real-time applications.
In such applications, the computational efficiency of algorithms is paramount.
Although recent deep learning paradigms such as CNN, RNN, or ViT have shown
remarkable performance, they often lack the desired computational efficiency.
Conversely, asynchronous event-based methods including SNNs and GNNs are
computationally efficient; however, these approaches fail to capture sufficient
spatio-temporal information, a powerful feature required to achieve better
performance for optical flow estimation. In this work, we introduce
Spatio-Temporal State Space Model (STSSM) module along with a novel network
architecture to develop an extremely efficient solution with competitive
performance. Our STSSM module leverages state-space models to effectively
capture spatio-temporal correlations in event data, offering higher performance
with lower complexity compared to ViT, CNN-based architectures in similar
settings. Our model achieves 4.5x faster inference and 8x lower computations
compared to TMA and 2x lower computations compared to EV-FlowNet with
competitive performance on the DSEC benchmark. Our code will be available at
https://github.com/AhmedHumais/E-STMFlow

</details>


### [385] [CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing](https://arxiv.org/abs/2506.07885)
*Zubin Bhuyan,Yuanchang Xie,AngkeaReach Rith,Xintong Yan,Nasko Apostolov,Jimi Oke,Chengbo Ai*

Main category: cs.CV

TL;DR: CrosswalkNet is a deep learning framework for detecting pedestrian crosswalks from aerial imagery with high precision and recall.


<details>
  <summary>Details</summary>
Motivation: To improve transportation asset management and urban planning by detecting pedestrian crosswalks from aerial images using deep learning.

Method: The authors developed CrosswalkNet, utilizing oriented bounding boxes, optimization techniques like Convolutional Block Attention, and a dual-branch Spatial Pyramid Pooling-Fast module.

Result: CrosswalkNet achieved a precision of 96.5% and recall of 93.3% on Massachusetts imagery and generalized well to other regions without fine-tuning.

Conclusion: The framework supports real-time crosswalk detection, enhancing pedestrian safety and urban mobility for policymakers and planners.

Abstract: With the increasing availability of aerial and satellite imagery, deep
learning presents significant potential for transportation asset management,
safety analysis, and urban planning. This study introduces CrosswalkNet, a
robust and efficient deep learning framework designed to detect various types
of pedestrian crosswalks from 15-cm resolution aerial images. CrosswalkNet
incorporates a novel detection approach that improves upon traditional object
detection strategies by utilizing oriented bounding boxes (OBB), enhancing
detection precision by accurately capturing crosswalks regardless of their
orientation. Several optimization techniques, including Convolutional Block
Attention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine
annealing, are implemented to maximize performance and efficiency. A
comprehensive dataset comprising over 23,000 annotated crosswalk instances is
utilized to train and validate the proposed framework. The best-performing
model achieves an impressive precision of 96.5% and a recall of 93.3% on aerial
imagery from Massachusetts, demonstrating its accuracy and effectiveness.
CrosswalkNet has also been successfully applied to datasets from New Hampshire,
Virginia, and Maine without transfer learning or fine-tuning, showcasing its
robustness and strong generalization capability. Additionally, the crosswalk
detection results, processed using High-Performance Computing (HPC) platforms
and provided in polygon shapefile format, have been shown to accelerate data
processing and detection, supporting real-time analysis for safety and mobility
applications. This integration offers policymakers, transportation engineers,
and urban planners an effective instrument to enhance pedestrian safety and
improve urban mobility.

</details>


### [386] [EgoM2P: Egocentric Multimodal Multitask Pretraining](https://arxiv.org/abs/2506.07886)
*Gen Li,Yutong Chen,Yiqian Wu,Kaifeng Zhao,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: The paper proposes EgoM2P, an efficient multimodal and multitask framework that overcomes the challenges of diverse egocentric data for tasks like gaze prediction, camera tracking, and depth estimation, and outperforms specialist models.


<details>
  <summary>Details</summary>
Motivation: The complexity and heterogeneity of egocentric multimodal data, such as missing modalities and dynamic motion, make it challenging to build scalable and general-purpose models for augmented reality, robotics, and other applications.

Method: The authors introduce temporal tokenizers and a masked modeling framework (EgoM2P) that leverages temporally aware multimodal tokens to unify multitasking in egocentric vision, including perception and video synthesis tasks.

Result: EgoM2P achieves superior or comparable performance to specialized models and is significantly faster across multiple egocentric tasks.

Conclusion: EgoM2P serves as a unified, efficient, and open-source framework for advancing egocentric 4D data understanding and multitasking applications, enabling scalable research and development in this field.

Abstract: Understanding multimodal signals in egocentric vision, such as RGB video,
depth, camera poses, and gaze, is essential for applications in augmented
reality, robotics, and human-computer interaction. These capabilities enable
systems to better interpret the camera wearer's actions, intentions, and
surrounding environment. However, building large-scale egocentric multimodal
and multitask models presents unique challenges. Egocentric data are inherently
heterogeneous, with large variations in modality coverage across devices and
settings. Generating pseudo-labels for missing modalities, such as gaze or
head-mounted camera trajectories, is often infeasible, making standard
supervised learning approaches difficult to scale. Furthermore, dynamic camera
motion and the complex temporal and spatial structure of first-person video
pose additional challenges for the direct application of existing multimodal
foundation models.
  To address these challenges, we introduce a set of efficient temporal
tokenizers and propose EgoM2P, a masked modeling framework that learns from
temporally aware multimodal tokens to train a large, general-purpose model for
egocentric 4D understanding. This unified design supports multitasking across
diverse egocentric perception and synthesis tasks, including gaze prediction,
egocentric camera tracking, and monocular depth estimation from egocentric
video. EgoM2P also serves as a generative model for conditional egocentric
video synthesis. Across these tasks, EgoM2P matches or outperforms specialist
models while being an order of magnitude faster. We will fully open-source
EgoM2P to support the community and advance egocentric vision research. Project
page: https://egom2p.github.io/

</details>


### [387] [Video Unlearning via Low-Rank Refusal Vector](https://arxiv.org/abs/2506.07891)
*Simone Facchiano,Stefano Saravalle,Matteo Migliarini,Edoardo De Matteis,Alessio Sampieri,Andrea Pilzer,Emanuele Rodolà,Indro Spinelli,Luca Franco,Fabio Galasso*

Main category: cs.CV

TL;DR: The paper proposes the first unlearning technique specifically for video diffusion models, enabling the suppression of harmful content generation.


<details>
  <summary>Details</summary>
Motivation: Video generative models can inadvertently amplify biases, harmful, or illegal content due to their training data, creating significant ethical and legal risks.

Method: The approach uses just 5 multi-modal prompt pairs to clarify safe vs. unsafe concepts, deriving 'refusal vectors' through a novel low-rank factorization method applied to latent differences.

Result: The method robustly suppresses harmful content generation while preserving visual quality and does not require retraining or original training data access.

Conclusion: The proposed unlearning technique enhances the safety and robustness of video generative models against unwanted content generation, ensuring minimal semantic collateral damage.

Abstract: Video generative models democratize the creation of visual content through
intuitive instruction following, but they also inherit the biases and harmful
concepts embedded within their web-scale training data. This inheritance
creates a significant risk, as users can readily generate undesirable and even
illegal content. This work introduces the first unlearning technique tailored
explicitly for video diffusion models to address this critical issue. Our
method requires 5 multi-modal prompt pairs only. Each pair contains a "safe"
and an "unsafe" example that differ only by the target concept. Averaging their
per-layer latent differences produces a "refusal vector", which, once
subtracted from the model parameters, neutralizes the unsafe concept. We
introduce a novel low-rank factorization approach on the covariance difference
of embeddings that yields robust refusal vectors. This isolates the target
concept while minimizing collateral unlearning of other semantics, thus
preserving the visual quality of the generated video. Our method preserves the
model's generation quality while operating without retraining or access to the
original training data. By embedding the refusal direction directly into the
model's weights, the suppression mechanism becomes inherently more robust
against adversarial bypass attempts compared to surface-level input-output
filters. In a thorough qualitative and quantitative evaluation, we show that we
can neutralize a variety of harmful contents, including explicit nudity,
graphic violence, copyrights, and trademarks. Project page:
https://www.pinlab.org/video-unlearning.

</details>


### [388] [WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning](https://arxiv.org/abs/2506.07905)
*Jie Yang,Feipeng Ma,Zitian Wang,Dacheng Yin,Kang Rong,Fengyun Rao,Ruimao Zhang*

Main category: cs.CV

TL;DR: The paper introduces a novel approach to general-purpose visual-language reasoning through reinforcement learning, supported by a new scalable multimodal QA synthesis pipeline and the WeThink dataset.


<details>
  <summary>Details</summary>
Motivation: To extend the capabilities of text-based reasoning models like DeepSeek-R1 into the multimodal domain for advanced visual-language reasoning, addressing current gaps in achieving general-purpose reasoning.

Method: The approach involves creating a scalable QA synthesis pipeline for generating reasoning-centric QA pairs, building the WeThink dataset with 120K annotated QA pairs, and adopting a hybrid reward mechanism to optimize RL training.

Result: The WeThink dataset and the proposed methods significantly enhance performance across 14 diverse multimodal large language model (MLLM) benchmarks, improving tasks from mathematical reasoning to general multimodal tasks.

Conclusion: The proposed dataset and pipeline effectively improve general-purpose multimodal reasoning and can be extended for continuous data diversity to boost performance further.

Abstract: Building on the success of text-based reasoning models like DeepSeek-R1,
extending these capabilities to multimodal reasoning holds great promise. While
recent works have attempted to adapt DeepSeek-R1-style reinforcement learning
(RL) training paradigms to multimodal large language models (MLLM), focusing on
domain-specific tasks like math and visual perception, a critical question
remains: How can we achieve the general-purpose visual-language reasoning
through RL? To address this challenge, we make three key efforts: (1) A novel
Scalable Multimodal QA Synthesis pipeline that autonomously generates
context-aware, reasoning-centric question-answer (QA) pairs directly from the
given images. (2) The open-source WeThink dataset containing over 120K
multimodal QA pairs with annotated reasoning paths, curated from 18 diverse
dataset sources and covering various question domains. (3) A comprehensive
exploration of RL on our dataset, incorporating a hybrid reward mechanism that
combines rule-based verification with model-based assessment to optimize RL
training efficiency across various task domains. Across 14 diverse MLLM
benchmarks, we demonstrate that our WeThink dataset significantly enhances
performance, from mathematical reasoning to diverse general multimodal tasks.
Moreover, we show that our automated data pipeline can continuously increase
data diversity to further improve model performance.

</details>


### [389] [A Comparative Study of U-Net Architectures for Change Detection in Satellite Images](https://arxiv.org/abs/2506.07925)
*Yaxita Amin,Naimisha S Trivedi,Rashmi Bhattad*

Main category: cs.CV

TL;DR: The paper reviews 34 studies on U-Net variations in remote sensing change detection, analyzing 18 versions and highlighting the strengths and weaknesses for improved precision.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the unexplored application of U-Net architectures in remote sensing change detection despite their wide use in pixel-wise classification.

Method: The method involved identifying and analyzing 34 past papers, comparing 18 U-Net variations, particularly focusing on their pros and cons and suitability for change detection.

Result: The study emphasizes specialized versions like Siamese Swin-U-Net and highlights key aspects such as managing multi-temporal data and long-distance relationships for accuracy.

Conclusion: Insights are provided for researchers and practitioners to better select U-Net versions tailored to remote sensing change detection needs.

Abstract: Remote sensing change detection is essential for monitoring the everchanging
landscapes of the Earth. The U-Net architecture has gained popularity for its
capability to capture spatial information and perform pixel-wise
classification. However, their application in the Remote sensing field remains
largely unexplored. Therefore, this paper fill the gap by conducting a
comprehensive analysis of 34 papers. This study conducts a comparison and
analysis of 18 different U-Net variations, assessing their potential for
detecting changes in remote sensing. We evaluate both benefits along with
drawbacks of each variation within the framework of this particular
application. We emphasize variations that are explicitly built for change
detection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.
The analysis highlights the significance of aspects such as managing data from
different time periods and collecting relationships over a long distance to
enhance the precision of change detection. This study provides valuable
insights for researchers and practitioners that choose U-Net versions for
remote sensing change detection tasks.

</details>


### [390] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: The paper evaluates vision-language models (VLMs) for multimodal in-context learning, revealing their reliance on heuristics and limited ability to utilize demonstrations.


<details>
  <summary>Details</summary>
Motivation: To investigate whether VLMs truly perform multimodal in-context learning (MM-ICL) beyond using shallow heuristics.

Method: A new pipeline is proposed augmenting demonstrations with generated rationales, followed by extensive experiments on various datasets using both open-source and proprietary VLMs.

Result: Results highlight that VLMs show limited performance sensitivity to factors like rationale quality and shot count, indicating a gap in their effective utilization of MM-ICL.

Conclusion: Current vision-language models fail to achieve intended in-context learning capabilities, necessitating improvements in utilizing demonstration-level information.

Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context
learning (ICL), a property similar to that of their language-only counterparts.
While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies
show they often rely on shallow heuristics -- such as copying or majority
voting -- rather than true task understanding. We revisit this assumption by
evaluating VLMs under distribution shifts, where support examples come from a
dataset different from the query. Surprisingly, performance often degrades with
more demonstrations, and models tend to copy answers rather than learn from
them. To investigate further, we propose a new MM-ICL with Reasoning pipeline
that augments each demonstration with a generated rationale alongside the
answer. We conduct extensive and comprehensive experiments on both perception-
and reasoning-required datasets with open-source VLMs ranging from 3B to 72B
and proprietary models such as Gemini 2.0. We conduct controlled studies
varying shot count, retrieval method, rationale quality, and distribution. Our
results show limited performance sensitivity across these factors, suggesting
that current VLMs do not effectively utilize demonstration-level information as
intended in MM-ICL.

</details>


### [391] [Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations](https://arxiv.org/abs/2506.07943)
*Yizhen Li,Dell Zhang,Xuelong Li,Yiqing Shen*

Main category: cs.CV

TL;DR: DTwinSeger introduces a novel approach to Reasoning Segmentation (RS) by using a Digital Twin representation to preserve spatial relationships for multimodal vision-text reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models for RS disrupt spatial relationships through image tokenization, necessitating improved methods for visual-text reasoning.

Method: DTwinSeger employs a two-stage process: first, generating a structured Digital Twin representation of an image, and second, using a fine-tuned Large Language Model for reasoning based on this representation. A specialized Seg-DT dataset is used for supervised fine-tuning.

Result: DTwinSeger achieves state-of-the-art results on multiple RS and referring segmentation benchmarks, demonstrating superior visual-text reasoning capabilities.

Conclusion: Leveraging a DT representation effectively bridges visual and textual modalities, enabling complex reasoning tasks to be handled via LLMs while maintaining high performance.

Abstract: Reasoning Segmentation (RS) is a multimodal vision-text task that requires
segmenting objects based on implicit text queries, demanding both precise
visual perception and vision-text reasoning capabilities. Current RS approaches
rely on fine-tuning vision-language models (VLMs) for both perception and
reasoning, but their tokenization of images fundamentally disrupts continuous
spatial relationships between objects. We introduce DTwinSeger, a novel RS
approach that leverages Digital Twin (DT) representation as an intermediate
layer to decouple perception from reasoning. Innovatively, DTwinSeger
reformulates RS as a two-stage process, where the first transforms the image
into a structured DT representation that preserves spatial relationships and
semantic properties and then employs a Large Language Model (LLM) to perform
explicit reasoning over this representation to identify target objects. We
propose a supervised fine-tuning method specifically for LLM with DT
representation, together with a corresponding fine-tuning dataset Seg-DT, to
enhance the LLM's reasoning capabilities with DT representations. Experiments
show that our method can achieve state-of-the-art performance on two image RS
benchmarks and three image referring segmentation benchmarks. It yields that DT
representation functions as an effective bridge between vision and text,
enabling complex multimodal reasoning tasks to be accomplished solely with an
LLM.

</details>


### [392] [Creating a Historical Migration Dataset from Finnish Church Records, 1800-1920](https://arxiv.org/abs/2506.07960)
*Ari Vesalainen,Jenna Kanerva,Aida Nitsch,Kiia Korsu,Ilari Larkiola,Laura Ruotsalainen,Filip Ginter*

Main category: cs.CV

TL;DR: The paper details the creation of a structured dataset on internal migration in Finland (1800-1920) using digitized church records and a deep learning pipeline.


<details>
  <summary>Details</summary>
Motivation: This paper aims to study historical demographic patterns by transforming handwritten church records into a structured dataset.

Method: A deep learning pipeline was employed for layout analysis, table detection, cell classification, and handwriting recognition on 200,000 images of handwritten records.

Result: Over 6 million data entries were extracted, creating a structured dataset suitable for studying migration, urbanization, and diseases.

Conclusion: The study showcases how deep learning technology can be used to process historical records, aiding demographic and historical research.

Abstract: This article presents a large-scale effort to create a structured dataset of
internal migration in Finland between 1800 and 1920 using digitized church
moving records. These records, maintained by Evangelical-Lutheran parishes,
document the migration of individuals and families and offer a valuable source
for studying historical demographic patterns. The dataset includes over six
million entries extracted from approximately 200,000 images of handwritten
migration records.
  The data extraction process was automated using a deep learning pipeline that
included layout analysis, table detection, cell classification, and handwriting
recognition. The complete pipeline was applied to all images, resulting in a
structured dataset suitable for research.
  The dataset can be used to study internal migration, urbanization, and family
migration, and the spread of disease in preindustrial Finland. A case study
from the Elim\"aki parish shows how local migration histories can be
reconstructed. The work demonstrates how large volumes of handwritten archival
material can be transformed into structured data to support historical and
demographic research.

</details>


### [393] [SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design](https://arxiv.org/abs/2506.07964)
*Wenxin Tang,Jingyu Xiao,Wenxuan Jiang,Xi Xiao,Yuhang Wang,Xuxin Tang,Qing Li,Yuehe Ma,Junliang Liu,Shisong Tang,Michael R. Lyu*

Main category: cs.CV

TL;DR: This paper presents SlideCoder and Slide2Code for generating editable slides from reference images, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Manual slide creation is time-consuming and knowledge-intensive, with existing natural language-based LLM methods failing to address the visual and structural nuances needed for effective slide design.

Method: The authors introduced SlideCoder, a framework combining Color Gradient-based Segmentation and a Hierarchical Retrieval-Augmented Generation method. They also proposed a benchmark, Slide2Code, and released SlideMaster, a 7B fine-tuned open-source model.

Result: SlideCoder demonstrates a 40.5-point improvement over state-of-the-art baselines in layout fidelity, execution accuracy, and visual consistency.

Conclusion: The study successfully formalizes the Reference Image to Slide Generation task and develops a robust framework, enhancing the effectiveness of automated slide design.

Abstract: Manual slide creation is labor-intensive and requires expert prior knowledge.
Existing natural language-based LLM generation methods struggle to capture the
visual and structural nuances of slide designs. To address this, we formalize
the Reference Image to Slide Generation task and propose Slide2Code, the first
benchmark with difficulty-tiered samples based on a novel Slide Complexity
Metric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework
for generating editable slides from reference images. SlideCoder integrates a
Color Gradient-based Segmentation algorithm and a Hierarchical
Retrieval-Augmented Generation method to decompose complex tasks and enhance
code generation. We also release SlideMaster, a 7B open-source model fine-tuned
with improved reverse-engineered data. Experiments show that SlideCoder
outperforms state-of-the-art baselines by up to 40.5 points, demonstrating
strong performance across layout fidelity, execution accuracy, and visual
consistency. Our code is available at
https://github.com/vinsontang1/SlideCoder.

</details>


### [394] [SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence](https://arxiv.org/abs/2506.07966)
*Ziyang Gong,Wenhao Li,Oliver Ma,Songyuan Li,Jiayi Ji,Xue Yang,Gen Luo,Junchi Yan,Rongrong Ji*

Main category: cs.CV

TL;DR: SpaCE-10 introduces a benchmark for evaluating spatial intelligence in Multimodal Large Language Models (MLLMs), covering atomic and compositional capabilities across diverse indoor scenes.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks struggle to evaluate spatial intelligence in MLLMs comprehensively. SpaCE-10 aims to bridge this gap with systematic testing of both atomic and compositional spatial capabilities.

Method: The benchmark defines 10 atomic spatial capabilities and combines them into 8 compositional capabilities. A hierarchical annotation pipeline generated over 5k QA pairs through human expert effort, covering diverse real indoor settings.

Result: Extensive evaluations show that current advanced MLLMs significantly lag behind human performance in spatial tasks. Key insights include the limitation in counting capability as a major bottleneck.

Conclusion: SpaCE-10 highlights critical areas for improvement in MLLMs' spatial intelligence, providing a valuable resource for developing smarter multimodal models.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in
various multimodal tasks. To pursue higher intelligence in space, MLLMs require
integrating multiple atomic spatial capabilities to handle complex and dynamic
tasks. However, existing benchmarks struggle to comprehensively evaluate the
spatial intelligence of common MLLMs from the atomic level to the compositional
level. To fill this gap, we present SpaCE-10, a comprehensive benchmark for
compositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial
capabilities, which are combined to form 8 compositional capabilities. Based on
these definitions, we propose a novel hierarchical annotation pipeline to
generate high-quality and diverse question-answer (QA) pairs. With over 150+
hours of human expert effort, we obtain over 5k QA pairs for 811 real indoor
scenes in SpaCE-10, which covers various evaluation settings like point cloud
input and multi-choice QA. We conduct an extensive evaluation of common MLLMs
on SpaCE-10 and find that even the most advanced MLLM still lags behind humans
by large margins. Through our careful study, we also draw several significant
findings that benefit the MLLM community. For example, we reveal that the
shortcoming of counting capability greatly limits the compositional spatial
capabilities of existing MLLMs. The evaluation code and benchmark datasets are
available at https://github.com/Cuzyoung/SpaCE-10.

</details>


### [395] [CyberV: Cybernetics for Test-time Scaling in Video Understanding](https://arxiv.org/abs/2506.07971)
*Jiahao Meng,Shuyang Sun,Yue Tan,Lu Qi,Yunhai Tong,Xiangtai Li,Longyin Wen*

Main category: cs.CV

TL;DR: CyberV introduces a cybernetic framework for enhancing video-based multimodal language models (MLLMs) by enabling self-monitoring, self-correction, and adaptive scaling during inference, improving performance without retraining.


<details>
  <summary>Details</summary>
Motivation: Address challenges in video MLLMs, such as computational demands, lack of robustness, and limited accuracy, by leveraging cybernetic principles for adaptive systems.

Method: Uses a cybernetic loop with components (Sensor, Controller, MLLM Inference System) to enable test-time monitoring, self-correction, and dynamic resource allocation without retraining.

Result: CyberV significantly improves video understanding of models, boosting performance by up to 10% and achieving human-level accuracy on specific benchmarks.

Conclusion: CyberV effectively enhances video MLLMs for better dynamic video understanding, generalization, and robustness, closing gaps with human-level expertise and proprietary systems.

Abstract: Current Multimodal Large Language Models (MLLMs) may struggle with
understanding long or complex videos due to computational demands at test time,
lack of robustness, and limited accuracy, primarily stemming from their
feed-forward processing nature. These limitations could be more severe for
models with fewer parameters. To address these limitations, we propose a novel
framework inspired by cybernetic principles, redesigning video MLLMs as
adaptive systems capable of self-monitoring, self-correction, and dynamic
resource allocation during inference. Our approach, CyberV, introduces a
cybernetic loop consisting of an MLLM Inference System, a Sensor, and a
Controller. Specifically, the sensor monitors forward processes of the MLLM and
collects intermediate interpretations, such as attention drift, then the
controller determines when and how to trigger self-correction and generate
feedback to guide the next round. This test-time adaptive scaling framework
enhances frozen MLLMs without requiring retraining or additional components.
Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B
by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive
proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%
improvement, achieving performance even comparable to human experts.
Furthermore, our method demonstrates consistent gains on general-purpose
benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and
generalization capabilities in making MLLMs more robust and accurate for
dynamic video understanding. The code is released at
https://github.com/marinero4972/CyberV.

</details>


### [396] [OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation](https://arxiv.org/abs/2506.07977)
*Jingjing Chang,Yixiao Fang,Peng Xing,Shuhan Wu,Wei Cheng,Rui Wang,Xianfang Zeng,Gang Yu,Hai-Bao Chen*

Main category: cs.CV

TL;DR: OneIG-Bench is introduced as a comprehensive benchmark for evaluating text-to-image (T2I) models across various dimensions, addressing gaps in previous evaluations.


<details>
  <summary>Details</summary>
Motivation: There is a need for a comprehensive evaluation framework for T2I models that addresses limitations such as reasoning, text rendering, and style which previous benchmarks lacked.

Method: The authors designed OneIG-Bench, a framework that evaluates T2I models across multiple dimensions such as prompt-image alignment, text rendering precision, and reasoning, with flexible evaluation capabilities.

Result: OneIG-Bench provides fine-grained insights into T2I model performance and facilitates comparisons across models by enabling evaluation on selected dimensions.

Conclusion: OneIG-Bench is expected to drive advancements in the T2I field by systematically addressing evaluation gaps and offering tools for improving model research and development.

Abstract: Text-to-image (T2I) models have garnered significant attention for generating
high-quality images aligned with text prompts. However, rapid T2I model
advancements reveal limitations in early benchmarks, lacking comprehensive
evaluations, for example, the evaluation on reasoning, text rendering and
style. Notably, recent state-of-the-art models, with their rich knowledge
modeling capabilities, show promising results on the image generation problems
requiring strong reasoning ability, yet existing evaluation systems have not
adequately addressed this frontier. To systematically address these gaps, we
introduce OneIG-Bench, a meticulously designed comprehensive benchmark
framework for fine-grained evaluation of T2I models across multiple dimensions,
including prompt-image alignment, text rendering precision, reasoning-generated
content, stylization, and diversity. By structuring the evaluation, this
benchmark enables in-depth analysis of model performance, helping researchers
and practitioners pinpoint strengths and bottlenecks in the full pipeline of
image generation. Specifically, OneIG-Bench enables flexible evaluation by
allowing users to focus on a particular evaluation subset. Instead of
generating images for the entire set of prompts, users can generate images only
for the prompts associated with the selected dimension and complete the
corresponding evaluation accordingly. Our codebase and dataset are now publicly
available to facilitate reproducible evaluation studies and cross-model
comparisons within the T2I research community.

</details>


### [397] [Real-time Localization of a Soccer Ball from a Single Camera](https://arxiv.org/abs/2506.07981)
*Dmitrii Vorobev,Artem Prosvetov,Karim Elhadji Daou*

Main category: cs.CV

TL;DR: The paper proposes a real-time, affordable method for 3D football trajectory reconstruction using a single broadcast camera, ensuring high accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: To create an efficient, accurate, and cost-effective system for 3D football trajectory tracking without relying on complex multi-camera setups or expensive infrastructure.

Method: Developed a multi-mode state model with $W$ discrete modes to optimize performance. The system operates on standard CPUs, ensuring computational efficiency even in challenging visual conditions like occlusion and motion blur.

Result: Results show performance comparable to multi-camera systems when tested on a proprietary dataset of Russian Premier League matches using 6K-resolution footage.

Conclusion: The approach provides a practical, low-cost solution for accurate 3D ball tracking in professional football, suitable for real-time live broadcasts.

Abstract: We propose a computationally efficient method for real-time three-dimensional
football trajectory reconstruction from a single broadcast camera. In contrast
to previous work, our approach introduces a multi-mode state model with $W$
discrete modes to significantly accelerate optimization while preserving
centimeter-level accuracy -- even in cases of severe occlusion, motion blur,
and complex backgrounds. The system operates on standard CPUs and achieves low
latency suitable for live broadcast settings. Extensive evaluation on a
proprietary dataset of 6K-resolution Russian Premier League matches
demonstrates performance comparable to multi-camera systems, without the need
for specialized or costly infrastructure. This work provides a practical method
for accessible and accurate 3D ball tracking in professional football
environments.

</details>


### [398] [CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray](https://arxiv.org/abs/2506.07984)
*Mingquan Lin,Gregory Holste,Song Wang,Yiliang Zhou,Yishu Wei,Imon Banerjee,Pengyi Chen,Tianjie Dai,Yuexi Du,Nicha C. Dvornek,Yuyan Ge,Zuowei Guo,Shouhei Hanaoka,Dongkyun Kim,Pablo Messina,Yang Lu,Denis Parra,Donghyun Son,Álvaro Soto,Aisha Urooj,René Vidal,Yosuke Yamagishi,Zefan Yang,Ruichi Zhang,Yang Zhou,Leo Anthony Celi,Ronald M. Summers,Zhiyong Lu,Hao Chen,Adam Flanders,George Shih,Zhangyang Wang,Yifan Peng*

Main category: cs.CV

TL;DR: The CXR-LT series focuses on advancing lung disease classification via organized initiatives and benchmark datasets. The 2024 iteration addresses long-tailed classification and zero-shot learning while expanding its dataset and toolset.


<details>
  <summary>Details</summary>
Motivation: To improve real-world lung disease diagnosis from chest X-rays by addressing long-tailed disease distribution and introducing innovations like zero-shot learning.

Method: The initiative provides expanded datasets, zero-shot learning tasks, and a range of state-of-the-art solutions, such as multimodal models and generative approaches.

Result: CXR-LT 2024 expanded its dataset to include 377,110 CXRs and 45 disease labels and introduced new evaluation tasks.

Conclusion: The initiative reaffirms its goal of enhancing clinically robust lung disease classification, contributing valuable resources for the research community.

Abstract: The CXR-LT series is a community-driven initiative designed to enhance lung
disease classification using chest X-rays (CXR). It tackles challenges in open
long-tailed lung disease classification and enhances the measurability of
state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve
these goals by providing high-quality benchmark CXR data for model development
and conducting comprehensive evaluations to identify ongoing issues impacting
lung disease classification performance. Building on the success of CXR-LT
2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45
disease labels, including 19 new rare disease findings. It also introduces a
new focus on zero-shot learning to address limitations identified in the
previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed
classification on a large, noisy test set, (ii) long-tailed classification on a
manually annotated "gold standard" subset, and (iii) zero-shot generalization
to five previously unseen disease findings. This paper provides an overview of
CXR-LT 2024, detailing the data curation process and consolidating
state-of-the-art solutions, including the use of multimodal models for rare
disease detection, advanced generative approaches to handle noisy labels, and
zero-shot learning strategies for unseen diseases. Additionally, the expanded
dataset enhances disease coverage to better represent real-world clinical
settings, offering a valuable resource for future research. By synthesizing the
insights and innovations of participating teams, we aim to advance the
development of clinically realistic and generalizable diagnostic models for
chest radiography.

</details>


### [399] [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
*Tuomas Oikarinen,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CV

TL;DR: This paper develops a cost-effective and accurate crowdsourced evaluation to assess neuron explanations, introducing innovative methods like importance sampling and Bayesian aggregation for efficiency and reliability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reliably evaluating neuron explanation methods, which can be noisy and expensive to measure accurately using crowd-sourced evaluations.

Method: The authors propose a novel evaluation strategy using importance sampling to prioritize high-value inputs for rating and a Bayesian aggregation technique to reduce noise and optimize the number of required ratings.

Result: The strategy reduces cost by ~30x through importance sampling and by an additional ~5x via Bayesian aggregation, while maintaining high evaluation accuracy.

Conclusion: The methods significantly improve the cost-effectiveness and reliability of crowd-sourced evaluations, enabling a large-scale study comparing neuron explanation quality across two vision-oriented models.

Abstract: Interpreting individual neurons or directions in activations space is an
important component of mechanistic interpretability. As such, many algorithms
have been proposed to automatically produce neuron explanations, but it is
often not clear how reliable these explanations are, or which methods produce
the best explanations. This can be measured via crowd-sourced evaluations, but
they can often be noisy and expensive, leading to unreliable results. In this
paper, we carefully analyze the evaluation pipeline and develop a
cost-effective and highly accurate crowdsourced evaluation strategy. In
contrast to previous human studies that only rate whether the explanation
matches the most highly activating inputs, we estimate whether the explanation
describes neuron activations across all inputs. To estimate this effectively,
we introduce a novel application of importance sampling to determine which
inputs are the most valuable to show to raters, leading to around 30x cost
reduction compared to uniform sampling. We also analyze the label noise present
in crowd-sourced evaluations and propose a Bayesian method to aggregate
multiple ratings leading to a further ~5x reduction in number of ratings
required for the same accuracy. Finally, we use these methods to conduct a
large-scale study comparing the quality of neuron explanations produced by the
most popular methods for two different vision models.

</details>


### [400] [Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers](https://arxiv.org/abs/2506.07986)
*Zhengyao Lv,Tianlin Pan,Chenyang Si,Zhaoxi Chen,Wangmeng Zuo,Ziwei Liu,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: The paper addresses challenges in achieving precise text-image alignment in multimodal diffusion transformers by introducing the Temperature-Adjusted Cross-modal Attention (TACA) method, which enhances alignment with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing state-of-the-art multimodal diffusion transformers like FLUX face challenges in aligning generated images with text prompts due to issues in the attention mechanism.

Method: The authors propose the TACA method, which uses temperature scaling and timestep-dependent adjustment to dynamically rebalance cross-modal interactions. It is further enhanced with LoRA fine-tuning.

Result: TACA significantly improves text-image alignment, especially in object appearance, attribute binding, and spatial relationships. The method was tested on FLUX and SD3.5 models, achieving higher performance on the T2I-CompBench benchmark.

Conclusion: Balancing cross-modal attention is crucial for semantic fidelity in text-to-image diffusion models, and TACA provides an efficient solution.

Abstract: Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress
in text-driven visual generation. However, even state-of-the-art MM-DiT models
like FLUX struggle with achieving precise alignment between text prompts and
generated content. We identify two key issues in the attention mechanism of
MM-DiT, namely 1) the suppression of cross-modal attention due to token
imbalance between visual and textual modalities and 2) the lack of
timestep-aware attention weighting, which hinder the alignment. To address
these issues, we propose \textbf{Temperature-Adjusted Cross-modal Attention
(TACA)}, a parameter-efficient method that dynamically rebalances multimodal
interactions through temperature scaling and timestep-dependent adjustment.
When combined with LoRA fine-tuning, TACA significantly enhances text-image
alignment on the T2I-CompBench benchmark with minimal computational overhead.
We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating
its ability to improve image-text alignment in terms of object appearance,
attribute binding, and spatial relationships. Our findings highlight the
importance of balancing cross-modal attention in improving semantic fidelity in
text-to-image diffusion models. Our codes are publicly available at
\href{https://github.com/Vchitect/TACA}

</details>


### [401] [PairEdit: Learning Semantic Variations for Exemplar-based Image Editing](https://arxiv.org/abs/2506.07992)
*Haoguang Lu,Jiacheng Chen,Zhenguo Yang,Aurele Tohokantche Gnanha,Fu Lee Wang,Li Qing,Xudong Mao*

Main category: cs.CV

TL;DR: PairEdit is a visual editing method capable of learning complex editing semantics from minimal paired image examples without textual guidance, showing improved consistency and semantic understanding.


<details>
  <summary>Details</summary>
Motivation: Text-guided image editing struggles to specify certain intricate editing semantics due to limitations of textual descriptions; exemplar-based editing methods are proposed as a practical alternative.

Method: PairEdit introduces target noise prediction with guidance direction terms, a content-preserving noise schedule, and distinct LoRAs for disentangling semantic variations from content.

Result: Extensive evaluations demonstrate PairEdit effectively learns intricate editing semantics and significantly enhances content consistency compared to other methods.

Conclusion: PairEdit establishes a novel approach to visual editing without textual prompts, achieving superior semantic learning and content preservation using minimal paired examples.

Abstract: Recent advancements in text-guided image editing have achieved notable
success by leveraging natural language prompts for fine-grained semantic
control. However, certain editing semantics are challenging to specify
precisely using textual descriptions alone. A practical alternative involves
learning editing semantics from paired source-target examples. Existing
exemplar-based editing methods still rely on text prompts describing the change
within paired examples or learning implicit text-based editing instructions. In
this paper, we introduce PairEdit, a novel visual editing method designed to
effectively learn complex editing semantics from a limited number of image
pairs or even a single image pair, without using any textual guidance. We
propose a target noise prediction that explicitly models semantic variations
within paired images through a guidance direction term. Moreover, we introduce
a content-preserving noise schedule to facilitate more effective semantic
learning. We also propose optimizing distinct LoRAs to disentangle the learning
of semantic variations from content. Extensive qualitative and quantitative
evaluations demonstrate that PairEdit successfully learns intricate semantics
while significantly improving content consistency compared to baseline methods.
Code will be available at https://github.com/xudonmao/PairEdit.

</details>


### [402] [MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation](https://arxiv.org/abs/2506.07999)
*Junhao Chen,Yulia Tsvetkov,Xiaochuang Han*

Main category: cs.CV

TL;DR: Introduces MADFormer, a novel hybrid model combining autoregressive and diffusion approaches for balanced, high-quality image generation.


<details>
  <summary>Details</summary>
Motivation: Existing hybrid generative models lack structured guidelines for allocating capacities between autoregressive and diffusion methods.

Method: MADFormer uses block-wise partitioning, leveraging AR layers for global conditioning and diffusion layers for local refinement.

Result: The model improves performance for high-resolution images and achieves up to 75% FID improvement under constrained compute.

Conclusion: Offers actionable insights into AR-diffusion layer mixing and block-wise partitioning for enhanced generative model design.

Abstract: Recent progress in multimodal generation has increasingly combined
autoregressive (AR) and diffusion-based approaches, leveraging their
complementary strengths: AR models capture long-range dependencies and produce
fluent, context-aware outputs, while diffusion models operate in continuous
latent spaces to refine high-fidelity visual details. However, existing hybrids
often lack systematic guidance on how and why to allocate model capacity
between these paradigms. In this work, we introduce MADFormer, a Mixed
Autoregressive and Diffusion Transformer that serves as a testbed for analyzing
AR-diffusion trade-offs. MADFormer partitions image generation into spatial
blocks, using AR layers for one-pass global conditioning across blocks and
diffusion layers for iterative local refinement within each block. Through
controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights:
(1) block-wise partitioning significantly improves performance on
high-resolution images, and (2) vertically mixing AR and diffusion layers
yields better quality-efficiency balances--improving FID by up to 75% under
constrained inference compute. Our findings offer practical design principles
for future hybrid generative models.

</details>


### [403] [Aligning Text, Images, and 3D Structure Token-by-Token](https://arxiv.org/abs/2506.08002)
*Aadarsh Sahoo,Vansh Tibrewal,Georgia Gkioxari*

Main category: cs.CV

TL;DR: This paper proposes an autoregressive model framework to unify language, images, and structured 3D scenes, aiming to enhance task performance like rendering and 3D object recognition.


<details>
  <summary>Details</summary>
Motivation: To improve understanding and interaction in 3D environments for applications like design assistance and robotic navigation.

Method: An autoregressive model is proposed to align language and 3D scenes, with detailed guidelines on data representation and modality-specific objectives. It extends to reconstruct 3D shapes using quantized encodings.

Result: The model demonstrated effectiveness across four 3D tasks (e.g., recognition, rendering) and performed well on synthetic and real-world datasets.

Conclusion: The proposed model is effective in aligning language, images, and 3D scenes and shows promise for 3D object recognition and other core tasks.

Abstract: Creating machines capable of understanding the world in 3D is essential in
assisting designers that build and edit 3D environments and robots navigating
and interacting within a three-dimensional space. Inspired by advances in
language and image modeling, we investigate the potential of autoregressive
models for a new modality: structured 3D scenes. To this end, we propose a
unified LLM framework that aligns language, images, and 3D scenes and provide a
detailed ''cookbook'' outlining critical design choices for achieving optimal
training and performance addressing key questions related to data
representation, modality-specific objectives, and more. We evaluate performance
across four core 3D tasks -- rendering, recognition, instruction-following, and
question-answering -- and four 3D datasets, synthetic and real-world. We extend
our approach to reconstruct complex 3D object shapes by enriching our 3D
modality with quantized shape encodings, and show our model's effectiveness on
real-world 3D object recognition tasks. Project webpage:
https://glab-caltech.github.io/kyvo/

</details>


### [404] [Audio-Sync Video Generation with Multi-Stream Temporal Control](https://arxiv.org/abs/2506.08003)
*Shuchen Weng,Haojie Zheng,Zheng Chang,Si Li,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: Proposes MTV, a framework for generating high-quality videos synchronized with audio by disentangling audio into speech, effects, and music for precise control.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of high-quality video generation synchronized with diverse and complex audio inputs, which current methods fail to handle effectively.

Method: MTV separates audio into three tracks—speech, effects, and music—to control lip motion, event timing, and visual mood, supported by the DEMIX dataset for scalable training.

Result: MTV achieves state-of-the-art performance across metrics like video quality, text-video consistency, and audio synchronization.

Conclusion: MTV enables fine-grained, synchronized video generation using its disentanglement strategy and specialized dataset, advancing the field of controllable video generation.

Abstract: Audio is inherently temporal and closely synchronized with the visual world,
making it a naturally aligned and expressive control signal for controllable
video generation (e.g., movies). Beyond control, directly translating audio
into video is essential for understanding and visualizing rich audio narratives
(e.g., Podcasts or historical recordings). However, existing approaches fall
short in generating high-quality videos with precise audio-visual
synchronization, especially across diverse and complex audio types. In this
work, we introduce MTV, a versatile framework for audio-sync video generation.
MTV explicitly separates audios into speech, effects, and music tracks,
enabling disentangled control over lip motion, event timing, and visual mood,
respectively -- resulting in fine-grained and semantically aligned video
generation. To support the framework, we additionally present DEMIX, a dataset
comprising high-quality cinematic videos and demixed audio tracks. DEMIX is
structured into five overlapped subsets, enabling scalable multi-stage training
for diverse generation scenarios. Extensive experiments demonstrate that MTV
achieves state-of-the-art performance across six standard metrics spanning
video quality, text-video consistency, and audio-video alignment. Project page:
https://hjzheng.net/projects/MTV/.

</details>


### [405] [Dynamic View Synthesis as an Inverse Problem](https://arxiv.org/abs/2506.08004)
*Hidir Yesiltepe,Pinar Yanardag*

Main category: cs.CV

TL;DR: This paper proposes a method for dynamic view synthesis using a pre-trained video diffusion model, requiring no training by introducing advanced noise representation.


<details>
  <summary>Details</summary>
Motivation: To enable dynamic view synthesis from monocular videos without requiring training or auxiliary modules by addressing limitations in deterministic inversion.

Method: Introduced K-order Recursive Noise Representation to resolve inversion challenges and Stochastic Latent Modulation for visibility-aware latent sampling.

Result: Achieved high-fidelity dynamic view synthesis with structured latent manipulation during noise initialization without training.

Conclusion: Structured latent manipulation in noise initialization enables effective and efficient dynamic view synthesis in a training-free manner.

Abstract: In this work, we address dynamic view synthesis from monocular videos as an
inverse problem in a training-free setting. By redesigning the noise
initialization phase of a pre-trained video diffusion model, we enable
high-fidelity dynamic view synthesis without any weight updates or auxiliary
modules. We begin by identifying a fundamental obstacle to deterministic
inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and
resolve it by introducing a novel noise representation, termed K-order
Recursive Noise Representation. We derive a closed form expression for this
representation, enabling precise and efficient alignment between the
VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions
resulting from camera motion, we introduce Stochastic Latent Modulation, which
performs visibility aware sampling over the latent space to complete occluded
regions. Comprehensive experiments demonstrate that dynamic view synthesis can
be effectively performed through structured latent manipulation in the noise
initialization phase.

</details>


### [406] [ZeroVO: Visual Odometry with Minimal Assumptions](https://arxiv.org/abs/2506.08005)
*Lei Lai,Zekai Yin,Eshed Ohn-Bar*

Main category: cs.CV

TL;DR: ZeroVO introduces a visual odometry algorithm that generalizes across various cameras and environments without calibration, outperforming prior methods by over 30% on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current visual odometry methods that rely on predefined camera calibration setups, restricting real-world applicability.

Method: ZeroVO incorporates a geometry-aware network, language-based priors for semantic enrichment, and a semi-supervised training paradigm to adapt to new environments using unlabeled data.

Result: Empirical results show ZeroVO delivers over 30% improvement on benchmarks like KITTI, nuScenes, and Argoverse 2, as well as a new synthetic dataset from Grand Theft Auto.

Conclusion: ZeroVO's calibration-free, adaptable approach broadens the scope of visual odometry applications, enabling robust deployment across diverse real-world scenarios.

Abstract: We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves
zero-shot generalization across diverse cameras and environments, overcoming
limitations in existing methods that depend on predefined or static camera
calibration setups. Our approach incorporates three main innovations. First, we
design a calibration-free, geometry-aware network structure capable of handling
noise in estimated depth and camera parameters. Second, we introduce a
language-based prior that infuses semantic information to enhance robust
feature extraction and generalization to previously unseen domains. Third, we
develop a flexible, semi-supervised training paradigm that iteratively adapts
to new scenes using unlabeled data, further boosting the models' ability to
generalize across diverse real-world scenarios. We analyze complex autonomous
driving contexts, demonstrating over 30% improvement against prior methods on
three standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly
introduced, high-fidelity synthetic dataset derived from Grand Theft Auto
(GTA). By not requiring fine-tuning or camera calibration, our work broadens
the applicability of VO, providing a versatile solution for real-world
deployment at scale.

</details>


### [407] [Dreamland: Controllable World Creation with Simulator and Generative Models](https://arxiv.org/abs/2506.08006)
*Sicheng Mo,Ziyang Leng,Leon Liu,Weizhen Wang,Honglin He,Bolei Zhou*

Main category: cs.CV

TL;DR: Dreamland introduces a hybrid framework for scene generation, combining physics-based simulations for granular control with large-scale generative models for photorealistic visualization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of element-wise controllability in large-scale video generative models, limiting their application in scene editing and embodied AI training.

Method: It proposes a layered world abstraction that integrates pixel and object-level semantics with physics-based simulators and generative models. A D3Sim dataset is developed for facilitating training and evaluation.

Result: Experiments show Dreamland improved image quality by 50.8%, controllability by 17.9%, and demonstrated strong potential for embodied AI training.

Conclusion: Dreamland successfully merges granular control and realistic visualization, overcoming existing limitations and paving the way for enhanced embodied AI development.

Abstract: Large-scale video generative models can synthesize diverse and realistic
visual content for dynamic world creation, but they often lack element-wise
controllability, hindering their use in editing scenes and training embodied AI
agents. We propose Dreamland, a hybrid world generation framework combining the
granular control of a physics-based simulator and the photorealistic content
output of large-scale pretrained generative models. In particular, we design a
layered world abstraction that encodes both pixel-level and object-level
semantics and geometry as an intermediate representation to bridge the
simulator and the generative model. This approach enhances controllability,
minimizes adaptation cost through early alignment with real-world
distributions, and supports off-the-shelf use of existing and future pretrained
generative models. We further construct a D3Sim dataset to facilitate the
training and evaluation of hybrid generation pipelines. Experiments demonstrate
that Dreamland outperforms existing baselines with 50.8% improved image
quality, 17.9% stronger controllability, and has great potential to enhance
embodied agent training. Code and data will be made available.

</details>


### [408] [Hidden in plain sight: VLMs overlook their visual representations](https://arxiv.org/abs/2506.08008)
*Stephanie Fu,Tyler Bonnen,Devin Guillory,Trevor Darrell*

Main category: cs.CV

TL;DR: Vision-language models (VLMs) struggle to efficiently use visual information compared to their visual encoders in vision-centric benchmarks.


<details>
  <summary>Details</summary>
Motivation: To explore whether VLMs can integrate visual and linguistic information effectively, leveraging language-based interfaces for visual tasks.

Method: Compared the performance of VLMs with their visual encoders across vision-centric benchmarks and analyzed visual representation degradation, prompt brittleness, and language model dependencies.

Result: VLMs performed much worse than visual encoders, with significant drops in task performance and challenges in utilizing visual information.

Conclusion: The bottleneck for vision-centric tasks lies in VLMs' inability to leverage visual information effectively, influenced by language model priors; future work should address this limitation for better visual understanding.

Abstract: Language provides a natural interface to specify and evaluate performance on
visual tasks. To realize this possibility, vision language models (VLMs) must
successfully integrate visual and linguistic information. Our work compares
VLMs to a direct readout of their visual encoders to understand their ability
to integrate across these modalities. Across a series of vision-centric
benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform
substantially worse than their visual encoders, dropping to near-chance
performance. We investigate these results through a series of analyses across
the entire VLM: namely 1) the degradation of vision representations, 2)
brittleness to task prompt, and 3) the language model's role in solving the
task. We find that the bottleneck in performing these vision-centric tasks lies
in this third category; VLMs are not effectively using visual information
easily accessible throughout the entire model, and they inherit the language
priors present in the LLM. Our work helps diagnose the failure modes of
open-source VLMs, and presents a series of evaluations useful for future
investigations into visual understanding within VLMs.

</details>


### [409] [Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion](https://arxiv.org/abs/2506.08009)
*Xun Huang,Zhengqi Li,Guande He,Mingyuan Zhou,Eli Shechtman*

Main category: cs.CV

TL;DR: The paper introduces a new training method, Self Forcing, for autoregressive video diffusion models to combat exposure bias using autoregressive rollout and holistic sequence-level loss.


<details>
  <summary>Details</summary>
Motivation: To address the exposure bias challenge in autoregressive video diffusion models, which occurs when models must generate sequences conditioned on their own outputs during inference.

Method: The method involves autoregressive rollout using key-value caching during training, allowing supervision via a video-level loss function. It balances efficiency and performance with a few-step diffusion model and stochastic gradient truncation along with a rolling key-value cache for efficient video extrapolation.

Result: Experiments show that this approach generates real-time streaming video on a single GPU with sub-second latency while maintaining or surpassing the quality of slower diffusion models.

Conclusion: Self Forcing advances autoregressive video generation by reducing exposure bias and achieving real-time, high-quality video generation efficiently on accessible hardware.

Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video
diffusion models. It addresses the longstanding issue of exposure bias, where
models trained on ground-truth context must generate sequences conditioned on
their own imperfect outputs during inference. Unlike prior methods that denoise
future frames based on ground-truth context frames, Self Forcing conditions
each frame's generation on previously self-generated outputs by performing
autoregressive rollout with key-value (KV) caching during training. This
strategy enables supervision through a holistic loss at the video level that
directly evaluates the quality of the entire generated sequence, rather than
relying solely on traditional frame-wise objectives. To ensure training
efficiency, we employ a few-step diffusion model along with a stochastic
gradient truncation strategy, effectively balancing computational cost and
performance. We further introduce a rolling KV cache mechanism that enables
efficient autoregressive video extrapolation. Extensive experiments demonstrate
that our approach achieves real-time streaming video generation with sub-second
latency on a single GPU, while matching or even surpassing the generation
quality of significantly slower and non-causal diffusion models. Project
website: http://self-forcing.github.io/

</details>


### [410] [Vision Transformers Don't Need Trained Registers](https://arxiv.org/abs/2506.08010)
*Nick Jiang,Amil Dravid,Alexei Efros,Yossi Gandelsman*

Main category: cs.CV

TL;DR: The paper identifies and addresses high-norm token artifacts causing noisy attention in Vision Transformers with a training-free solution by introducing test-time register tokens.


<details>
  <summary>Details</summary>
Motivation: To address the problem of high-norm tokens in Vision Transformers causing irregular attention patterns and degrading performance in downstream tasks, which previously required retraining models with register tokens.

Method: The authors identify sparse neurons responsible for the issue and propose a training-free solution by redistributing high-norm activations to an untrained token to mimic the role of register tokens in pre-trained models.

Result: The proposed method cleans up attention and feature maps, improves performance on multiple downstream visual tasks, and achieves results comparable to models trained with register tokens.

Conclusion: Test-time registers offer a training-free method to address high-norm token artifacts in pre-trained models, improving interpretability and performance across tasks.

Abstract: We investigate the mechanism underlying a previously identified phenomenon in
Vision Transformers -- the emergence of high-norm tokens that lead to noisy
attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a
sparse set of neurons is responsible for concentrating high-norm activations on
outlier tokens, leading to irregular attention patterns and degrading
downstream visual processing. While the existing solution for removing these
outliers involves retraining models from scratch with additional learned
register tokens, we use our findings to create a training-free approach to
mitigate these artifacts. By shifting the high-norm activations from our
discovered register neurons into an additional untrained token, we can mimic
the effect of register tokens on a model already trained without registers. We
demonstrate that our method produces cleaner attention and feature maps,
enhances performance over base models across multiple downstream visual tasks,
and achieves results comparable to models explicitly trained with register
tokens. We then extend test-time registers to off-the-shelf vision-language
models to improve their interpretability. Our results suggest that test-time
registers effectively take on the role of register tokens at test-time,
offering a training-free solution for any pre-trained model released without
them.

</details>


### [411] [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
*Yunfei Xie,Yinsong Ma,Shiyi Lan,Alan Yuille,Junfei Xiao,Chen Wei*

Main category: cs.CV

TL;DR: The paper proposes the Visual Game Learning (ViGaL) post-training method to enhance multimodal reasoning in large language models (MLLMs) using reinforcement learning on simple games, yielding improved performance without compromising general capabilities.


<details>
  <summary>Details</summary>
Motivation: Generalizable reasoning in multimodal large language models is hard to achieve. Inspired by cognitive science findings on gameplay enhancing transferable skills, the paper explores this domain.

Method: The authors post-trained a 7B-parameter MLLM through reinforcement learning using arcade-like games like Snake to enhance reasoning skills without explicit exposure to domain-specific solutions.

Result: The ViGaL approach led to significant improvements in multimodal reasoning benchmarks such as MathVista and MMMU, outperforming specialist models on reasoning tasks while maintaining general visual benchmark capabilities.

Conclusion: Playing synthetic, rule-based games can act as scalable pre-text tasks for developing generalizable multimodal reasoning abilities in MLLMs, offering a novel post-training paradigm.

Abstract: Developing generalizable reasoning capabilities in multimodal large language
models (MLLMs) remains challenging. Motivated by cognitive science literature
suggesting that gameplay promotes transferable cognitive skills, we propose a
novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs
develop out-of-domain generalization of multimodal reasoning through playing
arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM
via reinforcement learning (RL) on simple arcade-like games, e.g. Snake,
significantly enhances its downstream performance on multimodal math benchmarks
like MathVista, and on multi-discipline questions like MMMU, without seeing any
worked solutions, equations, or diagrams during RL, suggesting the capture of
transferable reasoning skills. Remarkably, our model outperforms specialist
models tuned on multimodal reasoning data in multimodal reasoning benchmarks,
while preserving the base model's performance on general visual benchmarks, a
challenge where specialist models often fall short. Our findings suggest a new
post-training paradigm: synthetic, rule-based games can serve as controllable
and scalable pre-text tasks that unlock generalizable multimodal reasoning
abilities in MLLMs.

</details>


### [412] [StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets](https://arxiv.org/abs/2506.08013)
*Anh-Quan Cao,Ivan Lopes,Raoul de Charette*

Main category: cs.CV

TL;DR: StableMTL introduces a zero-shot multi-task learning framework using synthetic datasets labeled for subsets of tasks, avoiding the need for extensive per-task annotations.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of multi-task learning for dense prediction when extensive task annotations are unavailable, exploring synthetic datasets and generalization using diffusion models.

Method: By using image generators for latent regression and unifying task losses with a latent loss, along with a task-attention mechanism to promote inter-task sharing.

Result: StableMTL achieves superior performance compared to baselines on 7 tasks across 8 benchmarks.

Conclusion: StableMTL's unique framework allows efficient and effective multi-task learning without requiring exhaustive annotation, leveraging synthetic data and inter-task synergies.

Abstract: Multi-task learning for dense prediction is limited by the need for extensive
annotation for every task, though recent works have explored training with
partial task labels. Leveraging the generalization power of diffusion models,
we extend the partial learning setup to a zero-shot setting, training a
multi-task model on multiple synthetic datasets, each labeled for only a subset
of tasks. Our method, StableMTL, repurposes image generators for latent
regression. Adapting a denoising framework with task encoding, per-task
conditioning and a tailored training scheme. Instead of per-task losses
requiring careful balancing, a unified latent loss is adopted, enabling
seamless scaling to more tasks. To encourage inter-task synergy, we introduce a
multi-stream model with a task-attention mechanism that converts N-to-N task
interactions into efficient 1-to-N attention, promoting effective cross-task
sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.

</details>


### [413] [4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos](https://arxiv.org/abs/2506.08015)
*Zhen Xu,Zhengqin Li,Zhao Dong,Xiaowei Zhou,Richard Newcombe,Zhaoyang Lv*

Main category: cs.CV

TL;DR: 4DGT is a 4D Gaussian-based Transformer model for dynamic scene reconstruction that processes 64 consecutive frames efficiently and achieves faster feed-forward inference.


<details>
  <summary>Details</summary>
Motivation: To efficiently and accurately model complex, time-varying environments using dynamic scene reconstruction on real-world monocular posed videos.

Method: The authors use a 4D Gaussian inductive bias combined with a novel density control strategy, enabling rolling-window processing and fast feed-forward inference for longer sequences.

Result: 4DGT reduces reconstruction time to seconds, scales well with long videos, and outperforms Gaussian-based methods while matching optimization-based approaches in accuracy.

Conclusion: The study demonstrates that 4DGT is an efficient and scalable solution for dynamic scene reconstruction, offering significant speed and performance improvements.

Abstract: We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene
reconstruction, trained entirely on real-world monocular posed videos. Using 4D
Gaussian as an inductive bias, 4DGT unifies static and dynamic components,
enabling the modeling of complex, time-varying environments with varying object
lifespans. We proposed a novel density control strategy in training, which
enables our 4DGT to handle longer space-time input and remain efficient
rendering at runtime. Our model processes 64 consecutive posed frames in a
rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike
optimization-based methods, 4DGT performs purely feed-forward inference,
reducing reconstruction time from hours to seconds and scaling effectively to
long video sequences. Trained only on large-scale monocular posed video
datasets, 4DGT can outperform prior Gaussian-based networks significantly in
real-world videos and achieve on-par accuracy with optimization-based methods
on cross-domain videos. Project page: https://4dgt.github.io

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [414] [Performance Impact of Containerized METADOCK 2 on Heterogeneous Platforms](https://arxiv.org/abs/2506.06450)
*Antonio Jesús Banegas-Luna,Baldomero Imbernón Tudela,Carlos Martínez-Cortés,José María Cecilia,Horacio Pérez-Sánchez*

Main category: cs.DC

TL;DR: The paper evaluated the impact of using containerization technologies on the performance of METADOCK 2 for virtual screening in heterogeneous HPC platforms and found negligible performance deviations (<1%) while enhancing portability and efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the computational challenges in virtual screening by assessing whether container-based approaches can provide scalability, portability, and maintain performance for high-throughput docking.

Method: The researchers tested METADOCK 2 with three containerization technologies—Docker, Singularity, and Apptainer—on heterogeneous HPC setups, comparing CPU and GPU configurations to measure performance impact.

Result: Containerization introduced negligible overhead, with deviations below 1%. METADOCK 2 also surpassed the capabilities of commercial tools like AutoDock Vina in processing large molecular structures.

Conclusion: Containerized METADOCK 2 offers a highly robust and scalable solution for virtual screening tasks, ensuring portability and reproducibility without compromising efficiency.

Abstract: Virtual screening (VS) is a computationally intensive process crucial for
drug discovery, often requiring significant resources to analyze large chemical
libraries and predict ligand-protein interactions. This study evaluates the
performance impact of containerization on METADOCK 2, a high-throughput docking
software when deployed on heterogeneous high-performance computing (HPC)
platforms. By testing three containerization technologies - Docker,
Singularity, and Apptainer - across varying CPU and GPU configurations, the
experiments reveal that containerization introduces negligible performance
overhead, with deviations below 1%. Moreover, METADOCK 2 demonstrated the
capability to efficiently process large molecular complexes, surpassing the
limitations of commercial tools such as AutoDock Vina. The results underscore
the advantages of container-based deployment for ensuring portability,
reproducibility, and scalability in scientific computing. This study concludes
that containerized METADOCK 2 is a robust and efficient solution for VS tasks
on heterogeneous HPC platforms.

</details>


### [415] [Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage](https://arxiv.org/abs/2506.06472)
*Ziqi Yuan,Haoyang Zhang,Yirui Eric Zhou,Apoorve Mohan,I-Hsin Chung,Seetharami Seelam,Jian Huang*

Main category: cs.DC

TL;DR: TERAIO is a tensor offloading framework enhancing GPU memory usage in training large language models by using SSDs efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the GPU memory limitations during large language model (LLM) training by leveraging low-cost SSDs for tensor management.

Method: TERAIO profiles tensor lifetimes, optimizes offloading/prefetching plans, and uses GPUDirect storage to facilitate migration directly between GPUs and SSDs.

Result: TERAIO achieved a 1.47x improvement in training performance over state-of-the-art methods and performed at 80.7% of ideal unlimited GPU memory efficiency.

Conclusion: TERAIO effectively expands GPU memory capacity and accelerates LLM training through lifetime-aware tensor management and SSD utilization.

Abstract: We present the design and implementation of a new lifetime-aware tensor
offloading framework for GPU memory expansion using low-cost PCIe-based
solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for
large language model (LLM) training with multiple GPUs and multiple SSDs. Its
design is driven by our observation that the active tensors take only a small
fraction (1.7% on average) of allocated GPU memory in each LLM training
iteration, the inactive tensors are usually large and will not be used for a
long period of time, creating ample opportunities for offloading/prefetching
tensors to/from slow SSDs without stalling the GPU training process. TERAIO
accurately estimates the lifetime (active period of time in GPU memory) of each
tensor with the profiling of the first few iterations in the training process.
With the tensor lifetime analysis, TERAIO will generate an optimized tensor
offloading/prefetching plan and integrate it into the compiled LLM program via
PyTorch. TERAIO has a runtime tensor migration engine to execute the
offloading/prefetching plan via GPUDirect storage, which allows direct tensor
migration between GPUs and SSDs for alleviating the CPU bottleneck and
maximizing the SSD bandwidth utilization. In comparison with state-of-the-art
studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves
the training performance of various LLMs by 1.47x on average, and achieves
80.7% of the ideal performance assuming unlimited GPU memory.

</details>


### [416] [Generating representative macrobenchmark microservice systems from distributed traces with Palette](https://arxiv.org/abs/2506.06448)
*Vaastav Anand,Matheus Stolet,Jonathan Mace,Antoine Kaufmann*

Main category: cs.DC

TL;DR: This paper proposes using distributed trace datasets and Graphical Causal Models (GCMs) to generate representative microservice systems for better benchmarking and evaluation.


<details>
  <summary>Details</summary>
Motivation: Researchers and practitioners lack access to representative microservice systems, forcing reliance on non-representative alternatives for evaluation purposes.

Method: They propose a novel system topology abstraction using Graphical Causal Models to model branching probabilities, execution patterns, and execution times. This is incorporated into a system called Palette to generate benchmarks.

Result: The approach allows the generation of flexible and representative microservice benchmarks.

Conclusion: The use of distributed traces and GCM-based modeling can enhance the realism and relevance of microservice system benchmarking.

Abstract: Microservices are the dominant design for developing cloud systems
  today. Advancements for microservice need to be evaluated in representative
systems, e.g. with matching scale, topology, and execution patterns.
  Unfortunately in practice, researchers and practitioners alike often do not
have access to representative systems. Thus they have to resort to sub-optimal
non-representative alternatives, e.g. small and oversimplified synthetic
benchmark systems or simulated system models instead.
  To solve this issue, we propose the use of distributed trace datasets,
available from large internet companies,
  to generate representative microservice systems.
  To do so, we introduce a novel abstraction of a system topology which uses
Graphical Causal Models (GCMs)
  to model the underlying system by incorporating the branching probabilities,
execution order of outgoing
  calls to every dependency, and execution times.
  We then incorporate this topology in Palette, a system that generates
  representative flexible macrobenchmarks microservice systems from distributed
traces.

</details>


### [417] [pFedSOP : Accelerating Training Of Personalized Federated Learning Using Second-Order Optimization](https://arxiv.org/abs/2506.07159)
*Mrinmay Sen,Chalavadi Krishna Mohan*

Main category: cs.DC

TL;DR: The paper introduces pFedSOP, a new method for Personalized Federated Learning (PFL) that leverages second-order optimization to enhance training speed and reduce communication rounds, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Federated Learning struggles with model generalization due to high data heterogeneity. Personalized Federated Learning can address this but often suffers from slow training and increased communication rounds due to the use of first-order optimization and additional data requirements.

Method: The proposed method, pFedSOP, utilizes second-order optimization by introducing a Fisher Information Matrix (FIM)-based approach to approximate the Hessian matrix and employing the Gompertz function-based normalized angle to compute personalized local updates.

Result: Experiments on various image classification datasets with heterogeneous data and partial client participation show that pFedSOP achieves superior performance and faster convergence compared to state-of-the-art FL and PFL algorithms.

Conclusion: pFedSOP demonstrates the ability to efficiently accelerate personalized model training in federated learning with fewer communication rounds, addressing core challenges of existing PFL methods.

Abstract: Personalized Federated Learning (PFL) enables clients to collaboratively
train personalized models tailored to their individual objectives, addressing
the challenge of model generalization in traditional Federated Learning (FL)
due to high data heterogeneity. However, existing PFL methods often require
increased communication rounds to achieve the desired performance, primarily
due to slow training caused by the use of first-order optimization, which has
linear convergence. Additionally, many of these methods increase local
computation because of the additional data fed into the model during the search
for personalized local models. One promising solution to this slow training is
second-order optimization, known for its quadratic convergence. However,
employing it in PFL is challenging due to the Hessian matrix and its inverse.
In this paper, we propose pFedSOP, which efficiently utilizes second-order
optimization in PFL to accelerate the training of personalized models and
enhance performance with fewer communication rounds. Our approach first
computes a personalized local gradient update using the Gompertz function-based
normalized angle between local and global gradient updates, incorporating
client-specific global information. We then use a regularized Fisher
Information Matrix (FIM), computed from this personalized gradient update, as
an approximation of the Hessian to update the personalized models. This
FIM-based second-order optimization speeds up training with fewer communication
rounds by tackling the challenges with exact Hessian and avoids additional data
being fed into the model during the search for personalized local models.
Extensive experiments on heterogeneously partitioned image classification
datasets with partial client participation demonstrate that pFedSOP outperforms
state-of-the-art FL and PFL algorithms.

</details>


### [418] [Addressing tokens dynamic generation, propagation, storage and renewal to secure the GlideinWMS pilot based jobs and system](https://arxiv.org/abs/2506.07379)
*Bruno Moreira Coimbra,Marco Mambelli*

Main category: cs.DC

TL;DR: The GlideinWMS middleware transitioned from X.509 to token-based credentials, addressing challenges and enhancing pilot infrastructure security through new credential modules.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address inadequacies in existing infrastructure and more stringent requirements due to tokens’ wider implementation across the WLCG community.

Method: The GlideinWMS team redesigned credential handling, introducing dynamic generation mechanisms and supporting features like credential storage, renewal, and invalidation.

Result: The result is a more secure, flexible credential system enabling minimal privilege enforcement and better alignment with modern requirements.

Conclusion: The improved GlideinWMS credential handling system supports evolving security demands while facilitating experiments and resource migration to token-based systems.

Abstract: GlideinWMS has been one of the first middleware in the WLCG community to
transition from X.509 to support also tokens. The first step was to get from
the prototype in 2019 to using tokens in production in 2022. This paper will
present the challenges introduced by the wider adoption of tokens and the
evolution plans for securing the pilot infrastructure of GlideinWMS and
supporting the new requirements. In the last couple of years, the GlideinWMS
team supported the migration of experiments and resources to tokens. Inadequate
support in the current infrastructure, more stringent requirements, and the
higher spatial and temporal granularity forced GlideinWMS to revisit once more
how credentials are generated, used, and propagated. The new credential modules
have been designed to be used in multiple systems (GlideinWMS, HEPCloud) and
use a model where credentials have type, purpose, and different flows.
Credentials are dynamically generated in order to customize the duration and
limit the scope to the targeted resource. This allows to enforce the least
privilege principle. Finally, we also considered adding credential storage,
renewal, and invalidation mechanisms within the GlideinWMS infrastructure to
better serve the experiments' needs.

</details>


### [419] [New Limits on Distributed Quantum Advantage: Dequantizing Linear Programs](https://arxiv.org/abs/2506.07574)
*Alkida Balliu,Corinna Coupette,Antonio Cruciani,Francesco d'Amore,Massimo Equi,Henrik Lievonen,Augusto Modanese,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: The paper establishes limits on distributed quantum advantage in the LOCAL model, showing that classical algorithms can match or exceed the performance of quantum algorithms for certain problems.


<details>
  <summary>Details</summary>
Motivation: To understand the potential advantages and limitations of using quantum-based algorithms for distributed computing tasks, especially those involving linear optimization and locally checkable labeling problems.

Method: The authors analyze equivalence and separations between quantum-LOCAL and classical deterministic models using mathematical proofs and theoretical frameworks.

Result: They prove that classical algorithms can mimic quantum-LOCAL for linear programs and demonstrate a scenario where quantum-LOCAL is strictly weaker than classical deterministic models. Additionally, they explore the relationship between non-signaling and SLOCAL models.

Conclusion: Quantum-LOCAL offers no advantage over classical deterministic LOCAL for linear programs, and specific locally checkable labeling problems showcase its limitations relative to classical models.

Abstract: In this work, we give two results that put new limits on distributed quantum
advantage in the context of the LOCAL model of distributed computing. First, we
show that there is no distributed quantum advantage for any linear program. Put
otherwise, if there is a quantum-LOCAL algorithm $\mathcal{A}$ that finds an
$\alpha$-approximation of some linear optimization problem $\Pi$ in $T$
communication rounds, we can construct a classical, deterministic LOCAL
algorithm $\mathcal{A}'$ that finds an $\alpha$-approximation of $\Pi$ in $T$
rounds. As a corollary, all classical lower bounds for linear programs,
including the KMW bound, hold verbatim in quantum-LOCAL. Second, using the
above result, we show that there exists a locally checkable labeling problem
(LCL) for which quantum-LOCAL is strictly weaker than the classical
deterministic SLOCAL model. Our results extend from quantum-LOCAL also to
finitely dependent and non-signaling distributions, and one of the corollaries
of our work is that the non-signaling model and the SLOCAL model are
incomparable in the context of LCL problems: By prior work, there exists an LCL
problem for which SLOCAL is strictly weaker than the non-signaling model, and
our work provides a separation in the opposite direction.

</details>


### [420] [A Terminology for Scientific Workflow Systems](https://arxiv.org/abs/2506.07838)
*Frédéric Sutera,Tainã Coleman,İlkay Altintaş,Rosa M. Badia,Bartosz Balis,Kyle Chard,Iacopo Colonnelli,Ewa Deelman,Paolo Di Tommaso,Thomas Fahringer,Carole Goble,Shantenu Jha,Daniel S. Katz,Johannes Köster,Ulf Leser,Kshitij Mehta,Hilary Oliver,J. -Luc Peterson,Giovanni Pizzi,Loïc Pottier,Raül Sirvent,Eric Suchyta,Douglas Thain,Sean R. Wilkinson,Justin M. Wozniak,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: This paper introduces a community-based terminology to standardize the characterization of workflow management systems (WMSs) and classifies 23 existing WMSs using this model.


<details>
  <summary>Details</summary>
Motivation: Researchers face difficulties in selecting appropriate workflow management systems (WMSs) due to their overlapping features and diverse functionalities, necessitating clear terminology for proper evaluation.

Method: The paper presents a five-axis terminology (workflow characteristics, composition, orchestration, data management, and metadata capture) and evaluates 23 WMSs based on these axes to form a standardized classification.

Result: A structured terminology was developed, and 23 existing WMSs were assessed and classified under the proposed framework.

Conclusion: The terminology provides a unified framework for comparing and selecting WMSs, aiding researchers in navigating the complexities of diverse systems based on their needs.

Abstract: The term scientific workflow has evolved over the last two decades to
encompass a broad range of compositions of interdependent compute tasks and
data movements. It has also become an umbrella term for processing in modern
scientific applications. Today, many scientific applications can be considered
as workflows made of multiple dependent steps, and hundreds of workflow
management systems (WMSs) have been developed to manage and run these
workflows. However, no turnkey solution has emerged to address the diversity of
scientific processes and the infrastructure on which they are implemented.
Instead, new research problems requiring the execution of scientific workflows
with some novel feature often lead to the development of an entirely new WMS. A
direct consequence is that many existing WMSs share some salient features,
offer similar functionalities, and can manage the same categories of workflows
but also have some distinct capabilities. This situation makes researchers who
develop workflows face the complex question of selecting a WMS. This selection
can be driven by technical considerations, to find the system that is the most
appropriate for their application and for the resources available to them, or
other factors such as reputation, adoption, strong community support, or
long-term sustainability. To address this problem, a group of WMS developers
and practitioners joined their efforts to produce a community-based terminology
of WMSs. This paper summarizes their findings and introduces this new
terminology to characterize WMSs. This terminology is composed of fives axes:
workflow characteristics, composition, orchestration, data management, and
metadata capture. Each axis comprises several concepts that capture the
prominent features of WMSs. Based on this terminology, this paper also presents
a classification of 23 existing WMSs according to the proposed axes and terms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [421] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/abs/2506.06290)
*Mingyu Lu,Ethan Weinberger,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: CellCLIP is a framework leveraging cross-modal contrastive learning to align perturbations and morphological effects from high-content screening data, achieving better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: High-content screening assays have unlocked large-scale analysis of cellular responses to perturbations, but aligning the data with perturbation effects is challenging due to the semantic gap and representation difficulties.

Method: The CellCLIP framework employs cross-modal contrastive learning, using pre-trained image encoders with a novel channel encoding for microscopy data and natural language encoders for perturbation representation.

Result: CellCLIP surpasses existing models in cross-modal retrieval and downstream biological tasks, while reducing computation time significantly.

Conclusion: CellCLIP addresses challenges in unifying representations of biological data, offering a scalable and efficient solution for HCS data analysis.

Abstract: High-content screening (HCS) assays based on high-throughput microscopy
techniques such as Cell Painting have enabled the interrogation of cells'
morphological responses to perturbations at an unprecedented scale. The
collection of such data promises to facilitate a better understanding of the
relationships between different perturbations and their effects on cellular
state. Towards achieving this goal, recent advances in cross-modal contrastive
learning could, in theory, be leveraged to learn a unified latent space that
aligns perturbations with their corresponding morphological effects. However,
the application of such methods to HCS data is not straightforward due to
substantial differences in the semantics of Cell Painting images compared to
natural images, and the difficulty of representing different classes of
perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent
space. In response to these challenges, here we introduce CellCLIP, a
cross-modal contrastive learning framework for HCS data. CellCLIP leverages
pre-trained image encoders coupled with a novel channel encoding scheme to
better capture relationships between different microscopy channels in image
embeddings, along with natural language encoders for representing
perturbations. Our framework outperforms current open-source models,
demonstrating the best performance in both cross-modal retrieval and
biologically meaningful downstream tasks while also achieving significant
reductions in computation time.

</details>


### [422] [Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks](https://arxiv.org/abs/2506.06291)
*Xiaoke Wang,Batuhan Altundas,Zhaoxin Li,Aaron Zhao,Matthew Gombolay*

Main category: cs.LG

TL;DR: The paper introduces a learning-based framework using Graph Neural Networks to warm-start MILP solvers for scheduling and task allocation issues, reducing optimization time.


<details>
  <summary>Details</summary>
Motivation: MILPs suffer from long computational times in real-time applications, making them impractical for widespread use in critical industries.

Method: The approach involves combining Behavior Cloning and Reinforcement Learning to train Graph Neural Networks for generating high-quality initial solutions to MILPs.

Result: Experiments show reduced optimization time and variance, while solution quality and feasibility are retained.

Conclusion: The framework successfully accelerates MILP solving while ensuring reliable solutions.

Abstract: Mixed Integer Linear Programs (MILPs) are essential tools for solving
planning and scheduling problems across critical industries such as
construction, manufacturing, and logistics. However, their widespread adoption
is limited by long computational times, especially in large-scale, real-time
scenarios. To address this, we present a learning-based framework that
leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph
Neural Networks (GNNs), producing high-quality initial solutions for
warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling
Problems. Experimental results demonstrate that our method reduces optimization
time and variance compared to traditional techniques while maintaining solution
quality and feasibility.

</details>


### [423] [Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks](https://arxiv.org/abs/2506.07500)
*Shakir Yousefi,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: The paper proposes a method to enhance the efficiency and performance of logic gate networks (LGNs) used for image classification.


<details>
  <summary>Details</summary>
Motivation: Neural networks are highly effective but computationally expensive, making it necessary to develop efficient alternatives like LGNs. However, LGNs suffer from slow training times, unused components, and a performance drop between training and inference.

Method: The authors inject Gumbel noise with a straight-through estimator during training. This approach leverages implicit Hessian regularization to improve convergence and reduce inefficiencies in LGNs.

Result: The proposed method accelerates training by 4.5×, reduces the discretization gap by 98%, and eliminates unused gates completely, enhancing the overall efficiency of LGNs.

Conclusion: The integration of Gumbel noise and regularization techniques provides a practical and effective solution for real-world deployment of LGNs, offering faster training and greater resource utilization without significant loss in accuracy.

Abstract: Modern neural networks demonstrate state-of-the-art performance on numerous
existing benchmarks; however, their high computational requirements and energy
consumption prompt researchers to seek more efficient solutions for real-world
deployment. Logic gate networks (LGNs) learns a large network of logic gates
for efficient image classification. However, learning a network that can solve
a simple problem like CIFAR-10 can take days to weeks to train. Even then,
almost half of the network remains unused, causing a discretization gap. This
discretization gap hinders real-world deployment of LGNs, as the performance
drop between training and inference negatively impacts accuracy. We inject
Gumbel noise with a straight-through estimator during training to significantly
speed up training, improve neuron utilization, and decrease the discretization
gap. We theoretically show that this results from implicit Hessian
regularization, which improves the convergence properties of LGNs. We train
networks $4.5 \times$ faster in wall-clock time, reduce the discretization gap
by $98\%$, and reduce the number of unused gates by $100\%$.

</details>


### [424] [Mutual-Taught for Co-adapting Policy and Reward Models](https://arxiv.org/abs/2506.06292)
*Tianyuan Shi,Canbin Huang,Fanqi Wan,Longguang Zhong,Ziyi Yang,Weizhou Shen,Xiaojun Quan,Ming Yan*

Main category: cs.LG

TL;DR: The Mutual-Taught approach enhances the preference optimization of LLMs, improving both Policy Model (PM) and Reward Model (RM) iteratively without additional human annotation.


<details>
  <summary>Details</summary>
Motivation: There is a performance issue in LLMs caused by distribution shifts between the training data and model-generated samples during preference optimization.

Method: The paper introduces Mutual-Taught, a self-training, iterative method resembling the expectation-maximization (EM) process to improve PM and RM.

Result: Experimental results show that their 8B models outperform benchmarks, achieving better scores and competitive performance with GPT-4o-2024-08-06.

Conclusion: Mutual-Taught effectively addresses distribution shifts, leading to consistently better-performing PM and RM in preference optimization tasks.

Abstract: During the preference optimization of large language models (LLMs),
distribution shifts may arise between newly generated model samples and the
data used to train the reward model (RM). This shift reduces the efficacy of
the RM, which in turn negatively impacts the performance of the policy model
(PM). To address this challenge, we propose Mutual-Taught, a self-training
method that iteratively improves both the PM and RM without requiring
additional human annotation. Our approach mirrors the expectation-maximization
(EM) algorithm. In the E-step, the PM is updated using feedback from the
current RM, guiding the PM toward a better approximation of the latent optimal
preference distribution. In the M-step, we update the RM by constructing
training data from the outputs of the PM before and after the E-step update.
This process ensures that the RM adapts to the evolving policy distribution.
Experimental results demonstrate that this iterative approach leads to
consistent improvements in both models. Specifically, our 8B policy model,
LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on
AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par
with GPT-4o-2024-08-06 on RewardBench.

</details>


### [425] [Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks](https://arxiv.org/abs/2506.06293)
*Junyi Liu,Stanley Kok*

Main category: cs.LG

TL;DR: The paper introduces a method, HTGNN, that combines persistent homology and lending networks to predict bank credit ratings using heterogeneous graphs.


<details>
  <summary>Details</summary>
Motivation: Bank credit ratings significantly affect economic security and decision-making, but privacy limitations obscure interbank connection graphs, complicating graph-based predictions.

Method: The authors use persistent homology to infer connections among banks and integrate this with an existing lending network to form a heterogeneous network analyzed by HTGNN.

Result: Experiments on global real-world datasets demonstrate that HTGNN improves rating prediction accuracy.

Conclusion: HTGNN aids investors and regulators by enhancing risk mitigation and enabling more effective market interventions.

Abstract: Agencies such as Standard & Poor's and Moody's provide bank credit ratings
that influence economic stability and decision-making by stakeholders. Accurate
and timely predictions support informed decision-making, regulatory actions,
and investor protection. However, a complete interbank connection graph is
often unavailable due to privacy concerns, complicating the direct application
of Graph Neural Networks (GNNs) for rating prediction. our research utilizes
persistent homology to construct a network that captures relationships among
banks and combines this with a traditional lending network to create a
heterogeneous network that integrates information from both sources, leading to
improved predictions. Experiments on a global, real-world dataset validate the
effectiveness of HTGNN. This research has implications for investors and
regulatory bodies in enhancing proactive risk mitigation and the implementation
of effective market interventions.The code can be find at
https://github.com/Liu-Jun-Yi/HTGNN.

</details>


### [426] [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
*Yunqing Liu,Wenqi Fan,Xiaoyong Wei,Qing Li*

Main category: cs.LG

TL;DR: The study introduces GLProtein, a pre-training framework combining protein structure and sequence insights to improve bioinformatics tasks.


<details>
  <summary>Details</summary>
Motivation: To address the gap in current protein analysis methods that overlook comprehensive integration of both global and local protein structural information.

Method: GLProtein integrates global structural similarity and local amino acid details through innovations like triplet structure similarity scoring, 3D distance encoding, and substructure-based amino acid encoding.

Result: GLProtein demonstrates superior performance over existing methods in tasks like protein-protein interaction prediction and contact prediction.

Conclusion: GLProtein enhances protein analysis by leveraging combined structural and amino acid information, providing improved accuracy and functional insights.

Abstract: Proteins are central to biological systems, participating as building blocks
across all forms of life. Despite advancements in understanding protein
functions through protein sequence analysis, there remains potential for
further exploration in integrating protein structural information. We argue
that the structural information of proteins is not only limited to their 3D
information but also encompasses information from amino acid molecules (local
information) to protein-protein structure similarity (global information). To
address this, we propose \textbf{GLProtein}, the first framework in protein
pre-training that incorporates both global structural similarity and local
amino acid details to enhance prediction accuracy and functional insights.
GLProtein innovatively combines protein-masked modelling with triplet structure
similarity scoring, protein 3D distance encoding and substructure-based amino
acid molecule encoding. Experimental results demonstrate that GLProtein
outperforms previous methods in several bioinformatics tasks, including
predicting protein-protein interaction, contact prediction, and so on.

</details>


### [427] [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
*Zhiyuan Liu,Yicun Yang,Yaojie Zhang,Junjie Chen,Chang Zou,Qingyuan Wei,Shaobo Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: This paper proposes dLLM-Cache, a caching framework that increases inference speed for diffusion-based LLMs (dLLMs) without compromising output quality, achieving up to 9.1x speedup.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based LLMs provide advantages over traditional autoregressive models but suffer from high inference latency. Existing acceleration techniques are incompatible with dLLMs due to their specific bidirectional attention mechanisms.

Method: The dLLM-Cache framework uses adaptive caching through long-interval prompt caching and partial response updates based on feature similarity, allowing reuse of computations during inference.

Result: Experiments with models like LLaDA 8B and Dream 7B demonstrate up to 9.1x acceleration in inference speed for dLLMs, with output quality remaining consistent.

Conclusion: The dLLM-Cache framework successfully addresses the latency issues of dLLMs, bringing their inference times closer to autoregressive models, enhancing practical viability.

Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large
Language Models. Recently, a new paradigm has emerged in the form of
diffusion-based Large Language Models (dLLMs), which generate text by
iteratively denoising masked segments. This approach has shown significant
advantages and potential. However, dLLMs suffer from high inference latency.
Traditional ARM acceleration techniques, such as Key-Value caching, are
incompatible with dLLMs due to their bidirectional attention mechanism. To
address this specific challenge, our work begins with a key observation that
dLLM inference involves a static prompt and a partially dynamic response, where
most tokens remain stable across adjacent denoising steps. Based on this, we
propose dLLM-Cache, a training-free adaptive caching framework that combines
long-interval prompt caching with partial response updates guided by feature
similarity. This design enables efficient reuse of intermediate computations
without compromising model performance. Extensive experiments on representative
dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1
x speedup over standard inference without compromising output quality. Notably,
our method brings dLLM inference latency close to that of ARMs under many
settings. Codes are provided in the supplementary material and will be released
publicly on GitHub.

</details>


### [428] [Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets](https://arxiv.org/abs/2506.06296)
*Hanaa El Afia,Said Ohamouddou,Raddouane Chiheb,Abdellatif El Afia*

Main category: cs.LG

TL;DR: The paper introduces Jacobi-KAN-DGCNN, a hybrid of DGCNN and Jacobi KAN using polynomial expansions instead of MLP layers, improving performance in 3D point cloud classification.


<details>
  <summary>Details</summary>
Motivation: To improve the classification of 3D point clouds by integrating polynomial expansions into DGCNN for more efficient and accurate learning.

Method: The proposed framework replaces MLP layers in DGCNN with Jacobi polynomial-based KAN layers and compares their effectiveness using the ModelNet40 dataset.

Result: Jacobi KAN layers achieve better accuracy and convergence speed than traditional DGCNN with fewer parameters, but high polynomial degrees do not necessarily enhance performance.

Conclusion: The integration of Jacobi KAN into DGCNN shows promise but requires further research on optimal polynomial bases and their link with graph learning mechanisms.

Abstract: We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph
Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks
(KAN) for the classification of three-dimensional point clouds. This method
replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate
polynomial expansions within a streamlined DGCNN architecture, circumventing
deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In
comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi
polynomials outperform the traditional linear layer-based DGCNN baseline in
terms of accuracy and convergence speed, while maintaining parameter
efficiency. Our results demonstrate that higher polynomial degrees do not
automatically improve performance, highlighting the need for further
theoretical and empirical investigation to fully understand the interactions
between polynomial bases, degrees, and the mechanisms of graph-based learning.

</details>


### [429] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: This survey examines efficient inference techniques for Language Models focusing on routing and cascading strategies to optimize computational resources while balancing performance.


<details>
  <summary>Details</summary>
Motivation: The computation and energy-intensive nature of modern LMs, especially in constrained environments, necessitates strategies to make their deployment more efficient.

Method: The paper studies two primary strategies: query-based model routing and hierarchical inference, both aimed at dynamically allocating computational resources based on query complexity.

Result: A comparative analysis of routing and cascading approaches is provided along with benchmarking efforts, and the identification of open challenges in achieving efficiency.

Conclusion: The work emphasizes the need for adaptive, efficient, and scalable LLM deployment techniques with the potential for real-world impact in constrained environments.

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [430] [Optimal patient allocation for echocardiographic assessments](https://arxiv.org/abs/2506.06297)
*Bozhi Sun,Seda Tierney,Jeffrey A. Feinstein,Frederick Damen,Alison L. Marsden,Daniele E. Schiavazzi*

Main category: cs.LG

TL;DR: This paper addresses scheduling challenges for hospital echocardiographic exams, developing a simulation and reinforcement learning (RL) approach for resource allocation, and finding that RL enhances efficiency over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Scheduling echocardiographic exams is challenging due to factors like patient no-shows, diverse exam durations, and asymmetric resource constraints between fetal and non-fetal cases, demanding advanced allocation strategies.

Method: The authors conduct pre-processing of hospital data to model patient behavior, use SimPy for discrete-event stochastic simulations integrated with the Gymnasium library, compare on-the-fly versus reservation-based allocation, and apply RL to optimize resource scheduling policies.

Result: The RL-based dynamic allocation policy outperforms traditional rule-based strategies by adapting efficiently to variability and resource constraints, especially with a 1:6 fetal to non-fetal room ratio and a 4:2 sonographer ratio.

Conclusion: Dynamic scheduling policies leveraging RL show significant potential to improve resource allocation efficiency in echocardiographic labs, supporting intelligent, data-driven decision-making.

Abstract: Scheduling echocardiographic exams in a hospital presents significant
challenges due to non-deterministic factors (e.g., patient no-shows, patient
arrival times, diverse exam durations, etc.) and asymmetric resource
constraints between fetal and non-fetal patient streams. To address these
challenges, we first conducted extensive pre-processing on one week of
operational data from the Echo Laboratory at Stanford University's Lucile
Packard Children's Hospital, to estimate patient no-show probabilities and
derive empirical distributions of arrival times and exam durations. Based on
these inputs, we developed a discrete-event stochastic simulation model using
SimPy, and integrate it with the open source Gymnasium Python library. As a
baseline for policy optimization, we developed a comparative framework to
evaluate on-the-fly versus reservation-based allocation strategies, in which
different proportions of resources are reserved in advance. Considering a
hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2
ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation
generally yields better performance, more effectively adapting to patient
variability and resource constraints. Building on this foundation, we apply
reinforcement learning (RL) to derive an approximated optimal dynamic
allocation policy. This RL-based policy is benchmarked against the
best-performing rule-based strategies, allowing us to quantify their
differences and provide actionable insights for improving echo lab efficiency
through intelligent, data-driven resource management.

</details>


### [431] [Pairwise Calibrated Rewards for Pluralistic Alignment](https://arxiv.org/abs/2506.06298)
*Daniel Halpern,Evi Micha,Ariel D. Procaccia,Itai Shapira*

Main category: cs.LG

TL;DR: Current alignment techniques in AI oversimplify human preferences, favoring majority views and ignoring minority perspectives. This paper introduces a method to account for diverse preferences using multiple reward functions learned from annotator disagreements.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing alignment techniques that fail to capture diverse human preferences and often marginalize minority perspectives.

Method: The paper proposes learning a distribution over multiple reward functions based on pairwise preferences without predefined groups or annotator identifiers, treating disagreements as soft labels. Pairwise calibration is used as the criterion for assessing alignment fidelity.

Result: Empirical tests demonstrate improved calibration with the proposed method, showing it better represents diverse human values by aligning policies to reflect pluralistic preferences.

Conclusion: This method effectively represents diverse preferences, overcoming the majority-minority issue in human alignment pipelines and improving faithfulness to pluralistic values through thoughtful calibration techniques.

Abstract: Current alignment pipelines presume a single, universal notion of desirable
behavior. However, human preferences often diverge across users, contexts, and
cultures. As a result, disagreement collapses into the majority signal and
minority perspectives are discounted. To address this, we propose reflecting
diverse human preferences through a distribution over multiple reward
functions, each inducing a distinct aligned policy. The distribution is learned
directly from pairwise preference without annotator identifiers or predefined
groups. Instead, annotator disagreements are treated as informative soft
labels. Our central criterion is pairwise calibration: for every pair of
candidate responses, the proportion of reward functions preferring one response
matches the fraction of annotators with that preference. We prove that even a
small outlier-free ensemble can accurately represent diverse preference
distributions. Empirically, we introduce and validate a practical training
heuristic to learn such ensembles, and demonstrate its effectiveness through
improved calibration, implying a more faithful representation of pluralistic
values.

</details>


### [432] [FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning](https://arxiv.org/abs/2506.07581)
*Tan Chen,Jintao Yan,Yuxuan Sun,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: This paper addresses data heterogeneity and limited bandwidth in Federated Learning (FL) through a novel approach. It introduces multi-level collective gradient divergence (CGD) for convergence optimization.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance FL's performance in wireless networks by addressing challenges related to data heterogeneity among devices and bandwidth limitations.

Method: The authors redefine gradient divergence as the sum of device-level and sample-level multi-level CGDs. They also integrate weighted earth moving distance (WEMD) and sampling variance into a new algorithm, FedCGD, to optimize FL convergence efficiently.

Result: The proposed FedCGD algorithm demonstrates an improvement in classification accuracy by up to 4.2% on CIFAR-10 while scheduling 41.8% fewer devices. It effectively balances between WEMD reduction and sampling variance reduction.

Conclusion: The paper concludes that properly addressing multi-level CGD can significantly enhance FL system efficiency, offering better model accuracy and resource allocation flexibility in wireless networks.

Abstract: Federated learning (FL) is a promising paradigm for multiple devices to
cooperatively train a model. When applied in wireless networks, two issues
consistently affect the performance of FL, i.e., data heterogeneity of devices
and limited bandwidth. Many papers have investigated device scheduling
strategies considering the two issues. However, most of them recognize data
heterogeneity as a property of individual devices. In this paper, we prove that
the convergence speed of FL is affected by the sum of device-level and
sample-level collective gradient divergence (CGD). The device-level CGD refers
to the gradient divergence of the scheduled device group, instead of the sum of
the individual device divergence. The sample-level CGD is statistically upper
bounded by sampling variance, which is inversely proportional to the total
number of samples scheduled for local update. To derive a tractable form of the
device-level CGD, we further consider a classification problem and transform it
into the weighted earth moving distance (WEMD) between the group distribution
and the global distribution. Then we propose FedCGD algorithm to minimize the
sum of multi-level CGDs by balancing WEMD and sampling variance, within
polynomial time. Simulation shows that the proposed strategy increases
classification accuracy on the CIFAR-10 dataset by up to 4.2\% while scheduling
41.8\% fewer devices, and flexibly switches between reducing WEMD and reducing
sampling variance.

</details>


### [433] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/abs/2506.06300)
*Yuanye Zhou,Zhaokun Wang,Kai Zhou,Hui Tang,Xiaofan Li*

Main category: cs.LG

TL;DR: LT-PINNs are a novel enhancement over traditional PINNs for topology optimization, providing precise boundary representation and eliminating the need for manual interpolation.


<details>
  <summary>Details</summary>
Motivation: Current PINNs for topology optimization rely on density-based descriptions, which require manual interpolation and have limitations with complex geometries.

Method: LT-PINNs utilize learnable parameters to control boundary curves directly, paired with new loss functions for boundary accuracy and clarity. They are tested on PDEs with various boundary conditions and time-dependent/independent flow problems.

Result: The approach demonstrates significant error reduction compared to DT-PINNs, handles diverse boundary conditions, and provides clear topology boundaries.

Conclusion: LT-PINNs advance the state-of-the-art in topology optimization, making the process more robust, flexible, and accurate for complex engineering applications.

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [434] [Rapid training of Hamiltonian graph networks without gradient descent](https://arxiv.org/abs/2506.06558)
*Atamert Rahma,Chinmay Datar,Ana Cukarska,Felix Dietrich*

Main category: cs.LG

TL;DR: This paper proposes a faster training method for physical system modeling using Hamiltonian Graph Networks (HGN), demonstrating up to 600x speed improvements and zero-shot generalization to large systems.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the slow training of graph neural networks (GNNs) in modeling N-body dynamics while maintaining accuracy and respecting physical invariances.

Method: Instead of relying on iterative gradient-based optimization, the study utilizes HGN with random feature-based parameter construction to represent physical systems.

Result: HGN achieves comparable accuracy to traditional optimizers while providing significantly faster training and zero-shot generalization to systems with thousands of nodes.

Conclusion: This work challenges the reliance on conventional gradient-descent-based training methods, offering a more efficient approach for data-driven modeling of physical systems.

Abstract: Learning dynamical systems that respect physical symmetries and constraints
remains a fundamental challenge in data-driven modeling. Integrating physical
laws with graph neural networks facilitates principled modeling of complex
N-body dynamics and yields accurate and permutation-invariant models. However,
training graph neural networks with iterative, gradient-based optimization
algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,
especially for large, complex systems. In comparison to 15 different
optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained
up to 600x faster--but with comparable accuracy--by replacing iterative
optimization with random feature-based parameter construction. We show robust
performance in diverse simulations, including N-body mass-spring systems in up
to 3 dimensions with different geometries, while retaining essential physical
invariances with respect to permutation, rotation, and translation. We reveal
that even when trained on minimal 8-node systems, the model can generalize in a
zero-shot manner to systems as large as 4096 nodes without retraining. Our work
challenges the dominance of iterative gradient-descent-based optimization
algorithms for training neural network models for physical systems.

</details>


### [435] [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
*Kefan Song,Amir Moeini,Peng Wang,Lei Gong,Rohan Chandra,Yanjun Qi,Shangtong Zhang*

Main category: cs.LG

TL;DR: This paper presents a novel framework called In-Context Reinforcement Learning (ICRL) prompting, where large language models (LLMs) demonstrate reinforcement learning behavior during inference, improving task performance as they process iterative responses and rewards.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs can inherently demonstrate reinforcement learning capabilities during inference time, enhancing their performance on sequential decision-making tasks without retraining.

Method: The authors propose the ICRL prompting framework, which iteratively provides a task to an LLM, evaluates its responses using scalar rewards, and incorporates past responses and rewards as context for subsequent prompts, essentially mimicking reinforcement learning behavior.

Result: Using benchmarks like Game of 24, creative writing, and ScienceWorld, ICRL prompting showed significant performance improvements over baseline methods such as Self-Refine and Reflexion, even when reward signals were generated by the LLM itself.

Conclusion: ICRL prompting reveals LLMs' ability to optimize reward signals during inference time, offering a promising approach for enhancing their capability to solve complex tasks and scaling test-time computational efficiency.

Abstract: Reinforcement learning (RL) is a human-designed framework for solving
sequential decision making problems. In this work, we demonstrate that,
surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a
phenomenon known as in-context RL (ICRL). Specifically, we propose a novel
multi-round prompting framework called ICRL prompting. The goal is to prompt
the LLM to complete a task. After the LLM generates a response at the current
round, we give numerical scalar feedbacks for the response, called the rewards.
At the next round, we prompt the LLM again with the same task and a context
consisting of all previous responses and rewards. We observe that the quality
of the LLM's response increases as the context grows. In other words, the LLM
is able to maximize the scalar reward signal in the inference time, just like
an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,
creative writing, and ScienceWorld) and demonstrate significant performance
improvements over baseline methods such as Self-Refine and Reflexion.
Surprisingly, in some experiments the reward signals are generated by the LLM
itself, yet performance improvements are still observed from ICRL prompting,
offering a promising paradigm for scaling test-time compute.

</details>


### [436] [InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models](https://arxiv.org/abs/2506.06505)
*Keisuke Sugiura,Hiroki Matsutani*

Main category: cs.LG

TL;DR: InstantFT is an FPGA-based method enabling ultra-fast fine-tuning of CNNs on IoT devices, significantly reducing computational time and improving energy efficiency.


<details>
  <summary>Details</summary>
Motivation: IoT devices are resource-constrained and require efficient methods for adapting DNNs to dynamic data distributions, as traditional training methods are computationally expensive.

Method: InstantFT employs FPGA hardware and optimizes PEFT techniques to accelerate forward and backward computations during fine-tuning.

Result: The method demonstrated 17.4x faster fine-tuning compared to LoRA approaches, with comparable accuracy, reducing tuning time to 0.36s and improving energy-efficiency by 16.3x.

Conclusion: InstantFT enables real-time fine-tuning of CNNs on IoT platforms, addressing concept drift in non-stationary data and improving resource utilization.

Abstract: Training deep neural networks (DNNs) requires significantly more computation
and memory than inference, making runtime adaptation of DNNs challenging on
resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for
ultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and
backward computations in parameter-efficient fine-tuning (PEFT). Experiments on
datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained
CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,
while achieving comparable accuracy. Our FPGA-based InstantFT reduces the
fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,
enabling on-the-fly adaptation of CNNs to non-stationary data distributions.

</details>


### [437] [Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study](https://arxiv.org/abs/2506.06327)
*Zilang Chen*

Main category: cs.LG

TL;DR: This paper benchmarks five ensemble machine-learning models for wine-quality prediction using standardized methods and presents Gradient Boosting as the best predictor, but Random Forest as the most cost-effective solution.


<details>
  <summary>Details</summary>
Motivation: Wine-quality assessment is critical for production control but remains subjective and labor-intensive. The study aims to develop accurate and reproducible machine-learning methodologies to address these issues.

Method: The study used five ensemble learners on wine datasets with leakage-free workflows, including SMOTE-Tomek resampling, Optuna hyper-parameter tuning, and feature selection methods. Metrics like weighted F1 scores were employed for evaluation.

Result: Gradient Boosting achieved the highest weighted F1 accuracy, with Random Forest and XGBoost closely trailing. Runtime metrics showed Gradient Boosting was computationally expensive compared to Random Forest.

Conclusion: Gradient Boosting performs best in accuracy while Random Forest is the most cost-effective for production use. The pipeline offers a reproducible benchmark for wine-quality prediction research.

Abstract: Accurate and reproducible wine-quality assessment is critical for production
control yet remains dominated by subjective, labour-intensive tasting panels.
We present the first unified benchmark of five ensemble learners (Random
Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho
Verde red- and white-wine datasets (1,599 and 4,898 instances, 11
physicochemical attributes). Our leakage-free workflow employs an 80:20
stratified train-test split, five-fold StratifiedGroupKFold within the training
set, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost
weighting, Optuna hyper-parameter search (120-200 trials per model) and a
two-stage feature-selection refit. Final scores on untouched test sets are
reported with weighted F1 as the headline metric. Gradient Boosting achieves
the highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016
for white), followed within three percentage points by Random Forest and
XGBoost. Limiting each model to its five top-ranked variables lowers
dimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage
points for red and 3.0 percentage points for white, indicating that alcohol,
volatile acidity, sulphates, free SO2 and chlorides capture most predictive
signal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency
gradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and
LightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We
therefore recommend Random Forest as the most cost-effective production model,
XGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as
the accuracy ceiling for offline benchmarking. The fully documented pipeline
and metric set provide a reproducible baseline for future work on imbalanced
multi-class wine-quality prediction.

</details>


### [438] [FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks](https://arxiv.org/abs/2506.06787)
*Qiyun Zhao*

Main category: cs.LG

TL;DR: The paper proposes FuncGNN, a method to enhance Boolean logic representation in electronic circuit design using hybrid feature aggregation and gate-aware normalization, achieving better performance and efficiency than existing approaches.


<details>
  <summary>Details</summary>
Motivation: Increasing complexity and integration density in modern circuits lead to structural heterogeneity and global logic information loss in And-Inverter Graphs (AIGs), challenging accurate circuit modeling.

Method: FuncGNN uses hybrid feature aggregation for multi-granularity pattern extraction, gate-aware normalization for coping with circuit-specific heterogeneity, and multi-layer integration to combine local and global semantic information.

Result: FuncGNN achieves 2.06% and 18.71% improvements in signal probability prediction and truth-table distance prediction tasks, respectively, while reducing training time by 50.6% and GPU memory usage by 32.8%.

Conclusion: FuncGNN addresses key challenges in circuit modeling by enhancing logic representations and demonstrates superior performance and efficiency over existing methods.

Abstract: As integrated circuit scale grows and design complexity rises, effective
circuit representation helps support logic synthesis, formal verification, and
other automated processes in electronic design automation. And-Inverter Graphs
(AIGs), as a compact and canonical structure, are widely adopted for
representing Boolean logic in these workflows. However, the increasing
complexity and integration density of modern circuits introduce structural
heterogeneity and global logic information loss in AIGs, posing significant
challenges to accurate circuit modeling. To address these issues, we propose
FuncGNN, which integrates hybrid feature aggregation to extract
multi-granularity topological patterns, thereby mitigating structural
heterogeneity and enhancing logic circuit representations. FuncGNN further
introduces gate-aware normalization that adapts to circuit-specific gate
distributions, improving robustness to structural heterogeneity. Finally,
FuncGNN employs multi-layer integration to merge intermediate features across
layers, effectively synthesizing local and global semantic information for
comprehensive logic representations. Experimental results on two logic-level
analysis tasks (i.e., signal probability prediction and truth-table distance
prediction) demonstrate that FuncGNN outperforms existing state-of-the-art
methods, achieving improvements of 2.06% and 18.71%, respectively, while
reducing training time by approximately 50.6% and GPU memory usage by about
32.8%.

</details>


### [439] [ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications](https://arxiv.org/abs/2506.06330)
*James Afful*

Main category: cs.LG

TL;DR: ExplainBench introduces a benchmarking suite for evaluating local model explanations, particularly in fairness-sensitive domains like criminal justice and healthcare.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a standardized framework for comparing local explanation techniques, particularly in ethically critical applications.

Method: Developed an open-source tool called ExplainBench with unified wrappers, pipelines integrating model training and explanation methods, and evaluation metrics like fidelity, sparsity, and robustness.

Result: ExplainBench was demonstrated on fairness-focused datasets, showcasing comparative behaviors among explanation methods under a standardized protocol.

Conclusion: ExplainBench enhances the methodological rigor in interpretable machine learning, promoting accountability in high-stakes AI applications.

Abstract: As machine learning systems are increasingly deployed in high-stakes domains
such as criminal justice, finance, and healthcare, the demand for interpretable
and trustworthy models has intensified. Despite the proliferation of local
explanation techniques, including SHAP, LIME, and counterfactual methods, there
exists no standardized, reproducible framework for their comparative
evaluation, particularly in fairness-sensitive settings.
  We introduce ExplainBench, an open-source benchmarking suite for systematic
evaluation of local model explanations across ethically consequential datasets.
ExplainBench provides unified wrappers for popular explanation algorithms,
integrates end-to-end pipelines for model training and explanation generation,
and supports evaluation via fidelity, sparsity, and robustness metrics. The
framework includes a Streamlit-based graphical interface for interactive
exploration and is packaged as a Python module for seamless integration into
research workflows.
  We demonstrate ExplainBench on datasets commonly used in fairness research,
such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different
explanation methods behave under a shared experimental protocol. By enabling
reproducible, comparative analysis of local explanations, ExplainBench advances
the methodological foundations of interpretable machine learning and
facilitates accountability in real-world AI systems.

</details>


### [440] [Extending AALpy with Passive Learning: A Generalized State-Merging Approach](https://arxiv.org/abs/2506.06333)
*Benjamin von Berg,Bernhard K. Aichernig*

Main category: cs.LG

TL;DR: The paper introduces a generalized state-merging implementation in the red-blue framework for passive automata learning, integrated into the AALpy library, simplifying the development of automata learning algorithms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the AALpy library by providing a generalized and configurable framework for state-merging methods in passive automata learning, reducing implementation complexity for developers.

Method: The authors implemented a common internal representation for different automaton types, allowing the generalized state-merging red-blue framework to be easily configured. Users need to define compatibility criteria and scoring to implement algorithms within AALpy.

Result: The integration enables concise definitions of existing state-merging algorithms in AALpy, often requiring only a few lines of code, and facilitates the design of novel algorithms.

Conclusion: The addition to AALpy aids both researchers and practitioners by streamlining the implementation process for state-merging algorithms in the field of passive automata learning.

Abstract: AALpy is a well-established open-source automata learning library written in
Python with a focus on active learning of systems with IO behavior. It provides
a wide range of state-of-the-art algorithms for different automaton types
ranging from fully deterministic to probabilistic automata. In this work, we
present the recent addition of a generalized implementation of an important
method from the domain of passive automata learning: state-merging in the
red-blue framework. Using a common internal representation for different
automaton types allows for a general and highly configurable implementation of
the red-blue framework. We describe how to define and execute state-merging
algorithms using AALpy, which reduces the implementation effort for
state-merging algorithms mainly to the definition of compatibility criteria and
scoring. This aids the implementation of both existing and novel algorithms. In
particular, defining some existing state-merging algorithms from the literature
with AALpy only takes a few lines of code.

</details>


### [441] [Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings](https://arxiv.org/abs/2506.07109)
*Rong-Xi Tan,Ming Chen,Ke Xue,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: The paper proposes leveraging language model embeddings for universal black-box optimization (BBO) across diverse data types, overcoming barriers in traditional BBO methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of unified representations in heterogeneous numerical spaces, which prevents universal optimization in offline BBO.

Method: The authors employ language model embeddings and propose methods like next-token prediction and latent space learning for unifying representations across domains.

Result: Experiments using open-source BBO tasks validate the universality and effectiveness of the proposed methods.

Conclusion: Unifying language model priors enables universal optimization across diverse data types, paving the way for new general-purpose BBO algorithms.

Abstract: The pursuit of universal black-box optimization (BBO) algorithms is a
longstanding goal. However, unlike domains such as language or vision, where
scaling structured data has driven generalization, progress in offline BBO
remains hindered by the lack of unified representations for heterogeneous
numerical spaces. Thus, existing offline BBO approaches are constrained to
single-task and fixed-dimensional settings, failing to achieve cross-domain
universal optimization. Recent advances in language models (LMs) offer a
promising path forward: their embeddings capture latent relationships in a
unifying way, enabling universal optimization across different data types
possible. In this paper, we discuss multiple potential approaches, including an
end-to-end learning framework in the form of next-token prediction, as well as
prioritizing the learning of latent spaces with strong representational
capabilities. To validate the effectiveness of these methods, we collect
offline BBO tasks and data from open-source academic works for training.
Experiments demonstrate the universality and effectiveness of our proposed
methods. Our findings suggest that unifying language model priors and learning
string embedding space can overcome traditional barriers in universal BBO,
paving the way for general-purpose BBO algorithms. The code is provided at
https://github.com/lamda-bbo/universal-offline-bbo.

</details>


### [442] [Optimized Local Updates in Federated Learning via Reinforcement Learning](https://arxiv.org/abs/2506.06337)
*Ali Murad,Bo Hui,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: The paper introduces a new method to optimize client training in Federated Learning (FL) using Deep Reinforcement Learning (DRL), addressing the challenge of non-IID data distribution across clients.


<details>
  <summary>Details</summary>
Motivation: To address the issue of performance drops in Federated Learning due to non-IID data distribution across clients and to prevent unnecessary local training that does not benefit the overall FL performance.

Method: The authors employ a Deep Reinforcement Learning agent as part of the framework to determine an optimal amount of data for client-side model training. The agent uses changes in training loss as a reward signal to optimize training data usage by adjusting weights assigned to data classes during FL rounds.

Result: Their proposed method showed superior performance across multiple benchmark datasets and Federated Learning frameworks compared to traditional approaches.

Conclusion: The framework effectively mitigates the non-IID data challenge in FL by optimizing client training and ultimately improves model performance while retaining privacy. The use of reinforcement learning ensures efficient and adaptive training strategies.

Abstract: Federated Learning (FL) is a distributed framework for collaborative model
training over large-scale distributed data, enabling higher performance while
maintaining client data privacy. However, the nature of model aggregation at
the centralized server can result in a performance drop in the presence of
non-IID data across different clients. We remark that training a client locally
on more data than necessary does not benefit the overall performance of all
clients. In this paper, we devise a novel framework that leverages a Deep
Reinforcement Learning (DRL) agent to select an optimized amount of data
necessary to train a client model without oversharing information with the
server. Starting without awareness of the client's performance, the DRL agent
utilizes the change in training loss as a reward signal and learns to optimize
the amount of training data necessary for improving the client's performance.
Specifically, after each aggregation round, the DRL algorithm considers the
local performance as the current state and outputs the optimized weights for
each class, in the training data, to be used during the next round of local
training. In doing so, the agent learns a policy that creates an optimized
partition of the local training dataset during the FL rounds. After FL, the
client utilizes the entire local training dataset to further enhance its
performance on its own data distribution, mitigating the non-IID effects of
aggregation. Through extensive experiments, we demonstrate that training FL
clients through our algorithm results in superior performance on multiple
benchmark datasets and FL frameworks. Our code is available at
https://github.com/amuraddd/optimized_client_training.git.

</details>


### [443] [Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models](https://arxiv.org/abs/2506.07121)
*Ren-Jian Wang,Ke Xue,Zeyu Qin,Ziniu Li,Sheng Tang,Hao-Tian Li,Shengcai Liu,Chao Qian*

Main category: cs.LG

TL;DR: The paper introduces a Quality-Diversity Red-Teaming (QDRT) framework that improves adversarial prompt diversity and attack effectiveness for assessing safety in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing red-teaming methods, such as simplistic diversity metrics and reliance on single attacker models, which hinder comprehensive evaluations of LLM safety.

Method: Proposed QDRT framework incorporates behavior-conditioned training, a behavioral replay buffer, and training of multiple specialized attackers to generate diverse and effective adversarial prompts.

Result: Empirical evaluation demonstrates that QDRT produces adversarial attacks that are more diverse and effective across various LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5.

Conclusion: QDRT advances LLM safety by enabling systematic and improved red-teaming, contributing to the responsible deployment of language models.

Abstract: Ensuring safety of large language models (LLMs) is important. Red teaming--a
systematic approach to identifying adversarial prompts that elicit harmful
responses from target LLMs--has emerged as a crucial safety evaluation method.
Within this framework, the diversity of adversarial prompts is essential for
comprehensive safety assessments. We find that previous approaches to
red-teaming may suffer from two key limitations. First, they often pursue
diversity through simplistic metrics like word frequency or sentence embedding
similarity, which may not capture meaningful variation in attack strategies.
Second, the common practice of training a single attacker model restricts
coverage across potential attack styles and risk categories. This paper
introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to
address these limitations. QDRT achieves goal-driven diversity through
behavior-conditioned training and implements a behavioral replay buffer in an
open-ended manner. Additionally, it trains multiple specialized attackers
capable of generating high-quality attacks across diverse styles and risk
categories. Our empirical evaluation demonstrates that QDRT generates attacks
that are both more diverse and more effective against a wide range of target
LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the
field of LLM safety by providing a systematic and effective approach to
automated red-teaming, ultimately supporting the responsible deployment of
LLMs.

</details>


### [444] [From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins](https://arxiv.org/abs/2506.06359)
*Gabriel Antonesi,Tudor Cioara,Ionut Anghel,Vasilis Michalakopoulos,Elissaios Sarmas,Liana Toderean*

Main category: cs.LG

TL;DR: This paper reviews the use of advanced AI models, specifically Transformers and LLMs, in smart grid and energy management, emphasizing their improved capabilities in heterogeneous data integration and decision-making.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning struggles with generalization, situational awareness, and integration of heterogeneous data, which limits its efficacy in energy sector applications. Recent AI advancements promise improvements.

Method: The authors synthesize research on Transformers and LLMs in energy applications, examining architectures, domain-specific adaptations, practical implementations, and emerging roles.

Result: Generative AI models, such as Transformers and LLMs, demonstrate improved decision-making capabilities in tasks like forecasting, grid balancing, workforce training, and asset onboarding.

Conclusion: The paper introduces the concept of Agentic Digital Twin, which combines LLMs for autonomous and proactive energy management systems within digital twin frameworks.

Abstract: Artificial intelligence (AI) has long promised to improve energy management
in smart grids by enhancing situational awareness and supporting more effective
decision-making. While traditional machine learning has demonstrated notable
results in forecasting and optimization, it often struggles with
generalization, situational awareness, and heterogeneous data integration.
Recent advances in foundation models such as Transformer architecture and Large
Language Models (LLMs) have demonstrated improved capabilities in modelling
complex temporal and contextual relationships, as well as in multi-modal data
fusion which is essential for most AI applications in the energy sector. In
this review we synthesize the rapid expanding field of AI applications in the
energy domain focusing on Transformers and LLMs. We examine the architectural
foundations, domain-specific adaptations and practical implementations of
transformer models across various forecasting and grid management tasks. We
then explore the emerging role of LLMs in the field: adaptation and fine tuning
for the energy sector, the type of tasks they are suited for, and the new
challenges they introduce. Along the way, we highlight practical
implementations, innovations, and areas where the research frontier is rapidly
expanding. These recent developments reviewed underscore a broader trend:
Generative AI (GenAI) is beginning to augment decision-making not only in
high-level planning but also in day-to-day operations, from forecasting and
grid balancing to workforce training and asset onboarding. Building on these
developments, we introduce the concept of the Agentic Digital Twin, a
next-generation model that integrates LLMs to bring autonomy, proactivity, and
social interaction into digital twin-based energy management systems.

</details>


### [445] [MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing](https://arxiv.org/abs/2506.07366)
*Haiyue Ma,Zhixu Du,Yiran Chen*

Main category: cs.LG

TL;DR: The paper introduces MoE-GPS, a framework for optimizing multi-GPU Mixture-of-Experts (MoE) inference by suggesting prediction strategies that enhance load balance and end-to-end system performance.


<details>
  <summary>Details</summary>
Motivation: Multi-GPU MoE networks often suffer from load imbalance due to unequal token assignment among experts distributed across GPUs.

Method: The authors propose MoE-GPS, a strategy-guiding framework that evaluates prediction techniques, such as Distribution-Only Prediction versus Token-to-Expert Prediction, to optimize system-level performance under varying configurations.

Result: Using MoE-GPS, the Distribution-Only Prediction strategy improved the inference performance by over 23% on the Mixtral 8x7B MMLU dataset compared to Token-to-Expert Prediction.

Conclusion: The study concludes that Distribution-Only Prediction effectively reduces overhead and enhances end-to-end performance, making it a preferred choice in many system configurations.

Abstract: In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across
different GPUs, which creates load imbalance as each expert processes different
number of tokens. Recent works improve MoE inference load balance by
dynamically duplicating popular experts to more GPUs to process excessive
tokens, which requires predicting the distribution before routing. In this
paper, we discuss the tradeoff of prediction strategies, accuracies, overhead,
and end-to-end system performance. We propose MoE-GPS, a framework that guides
the selection of the optimal predictor design under various system
configurations, by quantifying the performance impact to system-level model
runtime. Specifically, we advocate for Distribution-Only Prediction, a
prediction strategy that only predicts overall token distribution which
significantly reduces overhead compared to the traditional Token-to-Expert
Prediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only
Prediction which improves end-to-end inference performance by more than 23%
compared with Token-to-Expert Prediction.

</details>


### [446] [LETS Forecast: Learning Embedology for Time Series Forecasting](https://arxiv.org/abs/2506.06454)
*Abrar Majeedi,Viswanatha Reddy Gajjala,Satya Sai Srinath Namburi GNVV,Nada Magdi Elkordi,Yin Li*

Main category: cs.LG

TL;DR: DeepEDM integrates nonlinear dynamical systems modeling with deep learning, using Takens' theorem and empirical dynamic modeling for precise time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for time series forecasting often fail to explicitly model the underlying complex nonlinear dynamics.

Method: DeepEDM creates a latent space from time-delayed embeddings and uses kernel regression to model dynamics, supported by softmax attention for efficient and accurate future predictions.

Result: DeepEDM demonstrates robustness against noise and surpasses state-of-the-art forecasting techniques in accuracy, validated on both synthetic and real-world data.

Conclusion: DeepEDM effectively bridges the gap between deep learning and nonlinear dynamical modeling, offering significant improvements in time series forecasting precision.

Abstract: Real-world time series are often governed by complex nonlinear dynamics.
Understanding these underlying dynamics is crucial for precise future
prediction. While deep learning has achieved major success in time series
forecasting, many existing approaches do not explicitly model the dynamics. To
bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear
dynamical systems modeling with deep neural networks. Inspired by empirical
dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel
deep model that learns a latent space from time-delayed embeddings, and employs
kernel regression to approximate the underlying dynamics, while leveraging
efficient implementation of softmax attention and allowing for accurate
prediction of future time steps. To evaluate our method, we conduct
comprehensive experiments on synthetic data of nonlinear dynamical systems as
well as real-world time series across domains. Our results show that DeepEDM is
robust to input noise, and outperforms state-of-the-art methods in forecasting
accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.

</details>


### [447] [Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events](https://arxiv.org/abs/2506.06380)
*Jingyi Gu,Xuan Zhang,Guiling Wang*

Main category: cs.LG

TL;DR: This survey explores synthetic data generation for modeling extreme events, addressing the challenges posed by data scarcity and heavy-tailed distributions.


<details>
  <summary>Details</summary>
Motivation: Extreme events are rare yet catastrophic, creating cascading disruptions. Effective prediction and preparedness are limited due to data scarcity, necessitating innovative approaches like synthetic data generation.

Method: The paper systematically reviews generative modeling techniques, large language models, statistical enhancements, tailored evaluation frameworks, datasets, and domain-specific adaptations for extreme event simulation.

Result: Synthetic data methods are evaluated on statistical, dependence, visual, and task-oriented metrics, with actionable insights into their suitability and adaptations for extreme event domains.

Conclusion: This paper establishes foundational guidelines for synthetic extreme event research, highlights applications, identifies gaps in areas like behavioral finance and natural disasters, and outlines future challenges.

Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are
rare but catastrophic, often triggering cascading failures across
interconnected systems. Accurate prediction and early warning can help minimize
losses and improve preparedness. While data-driven methods offer powerful
capabilities for extreme event modeling, they require abundant training data,
yet extreme event data is inherently scarce, creating a fundamental challenge.
Synthetic data generation has emerged as a powerful solution. However, existing
surveys focus on general data with privacy preservation emphasis, rather than
extreme events' unique performance requirements. This survey provides the first
overview of synthetic data generation for extreme events. We systematically
review generative modeling techniques and large language models, particularly
those enhanced by statistical theory as well as specialized training and
sampling mechanisms to capture heavy-tailed distributions. We summarize
benchmark datasets and introduce a tailored evaluation framework covering
statistical, dependence, visual, and task-oriented metrics. A central
contribution is our in-depth analysis of each metric's applicability in
extremeness and domain-specific adaptations, providing actionable guidance for
model evaluation in extreme settings. We categorize key application domains and
identify underexplored areas like behavioral finance, wildfires, earthquakes,
windstorms, and infectious outbreaks. Finally, we outline open challenges,
providing a structured foundation for advancing synthetic rare-event research.

</details>


### [448] [WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets](https://arxiv.org/abs/2506.06455)
*Antonio Jesús Banegas-Luna,Horacio Pérez-Sánchez,Carlos Martínez-Cortés*

Main category: cs.LG

TL;DR: This paper addresses inconsistencies among interpretability algorithms for machine learning models by proposing a novel consensus method called WISCA, which enhances explanation reliability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of conflicting explanations produced by different interpretability algorithms in ML, which is problematic in domains requiring high reliability.

Method: Six machine learning models were trained on synthetic datasets with known ground truths. Various model-agnostic interpretability techniques were tested, and a novel consensus method, WISCA, was developed to harmonize results using class probability and normalized attributions.

Result: WISCA consistently aligned with the most reliable method among the tested interpretability techniques, demonstrating its effectiveness in producing robust and consistent explanations.

Conclusion: The study concludes that consensus strategies like WISCA are valuable for improving the reliability of interpretability methods, particularly in critical domains where explanation accuracy is vital.

Abstract: While predictive accuracy is often prioritized in machine learning (ML)
models, interpretability remains essential in scientific and high-stakes
domains. However, diverse interpretability algorithms frequently yield
conflicting explanations, highlighting the need for consensus to harmonize
results. In this study, six ML models were trained on six synthetic datasets
with known ground truths, utilizing various model-agnostic interpretability
techniques. Consensus explanations were generated using established methods and
a novel approach: WISCA (Weighted Scaled Consensus Attributions), which
integrates class probability and normalized attributions. WISCA consistently
aligned with the most reliable individual method, underscoring the value of
robust consensus strategies in improving explanation reliability.

</details>


### [449] [Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization](https://arxiv.org/abs/2506.06398)
*Yin Li*

Main category: cs.LG

TL;DR: The paper examines the impact of positional encodings in transformers by proposing new methods based on orthogonal functions and analyzing their performance in expressiveness, generalization, and extrapolation.


<details>
  <summary>Details</summary>
Motivation: To fill the gap in the theoretical understanding of positional encodings in transformers, optimizing them for better generalization and extrapolation in tasks across domains like NLP and computer vision.

Method: A theoretical framework assessing various positional encoding methods, alongside experimental evaluations to compare traditional and proposed encodings using orthogonal functions.

Result: Proposed orthogonal-based encodings outperform sinusoidal encodings in generalization and extrapolation for synthetic tasks.

Conclusion: This study enhances transformer theory by providing theoretical insights, advancing encoding methods to improve task-specific performance.

Abstract: Positional encodings are a core part of transformer-based models, enabling
processing of sequential data without recurrence. This paper presents a
theoretical framework to analyze how various positional encoding methods,
including sinusoidal, learned, relative, and bias-based methods like Attention
with Linear Biases (ALiBi), impact a transformer's expressiveness,
generalization ability, and extrapolation to longer sequences. Expressiveness
is defined via function approximation, generalization bounds are established
using Rademacher complexity, and new encoding methods based on orthogonal
functions, such as wavelets and Legendre polynomials, are proposed. The
extrapolation capacity of existing and proposed encodings is analyzed,
extending ALiBi's biasing approach to a unified theoretical context.
Experimental evaluation on synthetic sequence-to-sequence tasks shows that
orthogonal transform-based encodings outperform traditional sinusoidal
encodings in generalization and extrapolation. This work addresses a critical
gap in transformer theory, providing insights for design choices in natural
language processing, computer vision, and other transformer applications.

</details>


### [450] [A Certified Unlearning Approach without Access to Source Data](https://arxiv.org/abs/2506.06486)
*Umit Yigit Basaran,Sk Miraj Ahmed,Amit Roy-Chowdhury,Basak Guler*

Main category: cs.LG

TL;DR: The paper introduces a method for certified unlearning without relying on the original training data by leveraging surrogate datasets.


<details>
  <summary>Details</summary>
Motivation: The need for unlearning trained models in compliance with data privacy regulations, especially when the original training data is unavailable.

Method: The method uses surrogate datasets to approximate the statistical properties of the original data and calibrates noise based on statistical distance.

Result: Experimental validation on synthetic and real-world datasets showed reliable and effective unlearning results.

Conclusion: The framework offers a sound approach to erasing data from models while preserving utility and maintaining privacy guarantees even without access to the original data.

Abstract: With the growing adoption of data privacy regulations, the ability to erase
private or copyrighted information from trained models has become a crucial
requirement. Traditional unlearning methods often assume access to the complete
training dataset, which is unrealistic in scenarios where the source data is no
longer available. To address this challenge, we propose a certified unlearning
framework that enables effective data removal \final{without access to the
original training data samples}. Our approach utilizes a surrogate dataset that
approximates the statistical properties of the source data, allowing for
controlled noise scaling based on the statistical distance between the two.
\updated{While our theoretical guarantees assume knowledge of the exact
statistical distance, practical implementations typically approximate this
distance, resulting in potentially weaker but still meaningful privacy
guarantees.} This ensures strong guarantees on the model's behavior
post-unlearning while maintaining its overall utility. We establish theoretical
bounds, introduce practical noise calibration techniques, and validate our
method through extensive experiments on both synthetic and real-world datasets.
The results demonstrate the effectiveness and reliability of our approach in
privacy-sensitive settings.

</details>


### [451] [CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis](https://arxiv.org/abs/2506.06411)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: The paper introduces CoxNTF, a method using non-negative tensor factorization for clustering and predicting survival outcomes, showing comparable performance to Coxnet.


<details>
  <summary>Details</summary>
Motivation: Existing methods like NMF fail to include survival information in latent factor representations, thus limiting predictive effectiveness in survival analysis.

Method: CoxNTF uses survival probabilities from Coxnet to guide tensor factorization, constructing weighted covariate tensors for interpretable clustering and predictions.

Result: CoxNTF achieves similar survival prediction accuracy compared to Coxnet while offering a structured clustering framework and handling feature redundancies.

Conclusion: CoxNTF improves both interpretability and redundancy handling in survival analysis, serving as a robust tool for joint clustering and prediction.

Abstract: The interpretation of the results of survival analysis often benefits from
latent factor representations of baseline covariates. However, existing
methods, such as Nonnegative Matrix Factorization (NMF), do not incorporate
survival information, limiting their predictive power. We present CoxNTF, a
novel approach that uses non-negative tensor factorization (NTF) to derive
meaningful latent representations that are closely associated with survival
outcomes. CoxNTF constructs a weighted covariate tensor in which survival
probabilities derived from the Coxnet model are used to guide the tensorization
process. Our results show that CoxNTF achieves survival prediction performance
comparable to using Coxnet with the original covariates, while providing a
structured and interpretable clustering framework. In addition, the new
approach effectively handles feature redundancy, making it a powerful tool for
joint clustering and prediction in survival analysis.

</details>


### [452] [Membership Inference Attacks for Unseen Classes](https://arxiv.org/abs/2506.06488)
*Pratiksha Thaker,Neil Kale,Zhiwei Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: The paper investigates membership inference attacks for machine learning models under distribution shift where specific subclasses are inaccessible to the attacker. It reveals shadow model attacks fail in such scenarios and introduces quantile regression as a promising alternative.


<details>
  <summary>Details</summary>
Motivation: To address and understand membership inference attacks in a more realistic setting where attackers lack access to a complete subclass from the data distribution, representing a significant distribution shift.

Method: The authors evaluate shadow model attacks in a "class dropout" situation and propose quantile regression as an alternative approach. They conduct experiments using datasets like CIFAR-100 and ImageNet to compare performance.

Result: Quantile regression is shown to significantly outperform shadow model attacks under class dropout conditions, achieving up to 11x the true positive rate (TPR) on unseen classes and demonstrating effectiveness even with 90% of training classes removed.

Conclusion: Quantile regression attacks provide a more robust approach for membership inference attacks compared to shadow models in scenarios involving extreme distribution shifts, as supported by experimental and theoretical insights.

Abstract: Shadow model attacks are the state-of-the-art approach for membership
inference attacks on machine learning models. However, these attacks typically
assume an adversary has access to a background (nonmember) data distribution
that matches the distribution the target model was trained on. We initiate a
study of membership inference attacks where the adversary or auditor cannot
access an entire subclass from the distribution -- a more extreme but realistic
version of distribution shift than has been studied previously. In this
setting, we first show that the performance of shadow model attacks degrades
catastrophically, and then demonstrate the promise of another approach,
quantile regression, that does not have the same limitations. We show that
quantile regression attacks consistently outperform shadow model attacks in the
class dropout setting -- for example, quantile regression attacks achieve up to
11$\times$ the TPR of shadow models on the unseen class on CIFAR-100, and
achieve nontrivial TPR on ImageNet even with 90% of training classes removed.
We also provide a theoretical model that illustrates the potential and
limitations of this approach.

</details>


### [453] [NeurNCD: Novel Class Discovery via Implicit Neural Representation](https://arxiv.org/abs/2506.06412)
*Junming Wang,Yi Shi*

Main category: cs.LG

TL;DR: The paper presents NeurNCD, a new framework for discovering novel classes using implicit representations, achieving better segmentation in both open and closed-world scenarios and outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: Traditional explicit representations for novel class discovery are limited by discrete and noisy characteristics, making it harder to identify new classes accurately.

Method: The authors propose NeurNCD, combining an Embedding-NeRF model with KL divergence to replace explicit segmentation maps. Key features include feature query, modulation, and clustering for efficient augmentation and exchange between segmentation networks and neural representations.

Result: Experiments show that NeurNCD outperforms state-of-the-art methods, specifically on the NYUv2 and Replica datasets.

Conclusion: NeurNCD offers a versatile, data-efficient solution for novel class discovery, eliminating the need for dense supervision or human labeling while setting a new benchmark for segmentation tasks.

Abstract: Discovering novel classes in open-world settings is crucial for real-world
applications. Traditional explicit representations, such as object descriptors
or 3D segmentation maps, are constrained by their discrete, hole-prone, and
noisy nature, which hinders accurate novel class discovery. To address these
challenges, we introduce NeurNCD, the first versatile and data-efficient
framework for novel class discovery that employs the meticulously designed
Embedding-NeRF model combined with KL divergence as a substitute for
traditional explicit 3D segmentation maps to aggregate semantic embedding and
entropy in visual embedding space. NeurNCD also integrates several key
components, including feature query, feature modulation and clustering,
facilitating efficient feature augmentation and information exchange between
the pre-trained semantic segmentation network and implicit neural
representations. As a result, our framework achieves superior segmentation
performance in both open and closed-world settings without relying on densely
labelled datasets for supervised training or human interaction to generate
sparse label supervision. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches on the NYUv2 and Replica
datasets.

</details>


### [454] [Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks](https://arxiv.org/abs/2506.06489)
*Daniel Kunin,Giovanni Luca Marchetti,Feng Chen,Dhruva Karkada,James B. Simon,Michael R. DeWeese,Surya Ganguli,Nina Miolane*

Main category: cs.LG

TL;DR: The paper introduces Alternating Gradient Flows (AGF) as a framework to describe feature learning dynamics in two-layer neural networks with small initialization. AGF characterizes loss patterns and training dynamics across architectures.


<details>
  <summary>Details</summary>
Motivation: Understanding the dynamics of feature learning in neural networks remains challenging.

Method: The authors propose Alternating Gradient Flows (AGF), an algorithmic process where dormant and active neurons alternate phases of utility maximization and cost minimization.

Result: AGF captures the loss dynamics observed in experiments and analytically connects neural network training to mathematical frameworks across several architectures.

Conclusion: AGF provides a unified explanation for feature learning dynamics and extends analyses, offering insights into networks learning specific features like Fourier features in modular addition tasks.

Abstract: What features neural networks learn, and how, remains an open question. In
this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic
framework that describes the dynamics of feature learning in two-layer networks
trained from small initialization. Prior works have shown that gradient flow in
this regime exhibits a staircase-like loss curve, alternating between plateaus
where neurons slowly align to useful directions and sharp drops where neurons
rapidly grow in norm. AGF approximates this behavior as an alternating two-step
process: maximizing a utility function over dormant neurons and minimizing a
cost function over active ones. AGF begins with all neurons dormant. At each
round, a dormant neuron activates, triggering the acquisition of a feature and
a drop in the loss. AGF quantifies the order, timing, and magnitude of these
drops, matching experiments across architectures. We show that AGF unifies and
extends existing saddle-to-saddle analyses in fully connected linear networks
and attention-only linear transformers, where the learned features are singular
modes and principal components, respectively. In diagonal linear networks, we
prove AGF converges to gradient flow in the limit of vanishing initialization.
Applying AGF to quadratic networks trained to perform modular addition, we give
the first complete characterization of the training dynamics, revealing that
networks learn Fourier features in decreasing order of coefficient magnitude.
Altogether, AGF offers a promising step towards understanding feature learning
in neural networks.

</details>


### [455] [Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers](https://arxiv.org/abs/2506.06443)
*Luis Pinto*

Main category: cs.LG

TL;DR: Intermediate layer embeddings in molecular encoders deliver superior performance over final-layer representations for ADMET property prediction, with average gains of 5.4% for fixed embeddings and 8.5% for finetuned ones. This approach achieves state-of-the-art results and improves computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To challenge the prevailing practice of using only final-layer embeddings from pretrained molecular encoders for downstream tasks, and investigate whether intermediate layer embeddings hold untapped potential.

Method: Conducted layer-wise analysis of embeddings from five molecular encoders across 22 ADMET property prediction tasks, comparing fixed embeddings and finetuned embeddings from intermediate layers against final-layer representations.

Result: Intermediate layer embeddings outperformed final-layer ones, improving performance by an average of 5.4% and up to 28.6% for fixed embeddings; finetuned embeddings improved by an average of 8.5% and up to 40.8%. State-of-the-art results were achieved on several benchmarks.

Conclusion: Intermediate layers of molecular encoders contain valuable representational depth that enhances performance and computational efficiency. Evaluate-then-finetune techniques identified optimal layers effectively, emphasizing the importance of leveraging all representational layers.

Abstract: Pretrained molecular encoders have become indispensable in computational
chemistry for tasks such as property prediction and molecular generation.
However, the standard practice of relying solely on final-layer embeddings for
downstream tasks may discard valuable information. In this work, we challenge
this convention by conducting a comprehensive layer-wise analysis of five
diverse molecular encoders across 22 ADMET property prediction tasks. Our
results demonstrate that embeddings from intermediate layers consistently
outperform final-layer representations. Specifically, using fixed embeddings
from the optimal intermediate layers improved downstream performance by an
average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to
these intermediate layers yielded even greater average improvements of 8.5%,
with performance increases as high as 40.8%, achieving new state-of-the-art
results on several benchmarks. Additionally, a strong positive correlation
between fixed embedding performance and finetuning outcomes supports an
efficient evaluate-then-finetune approach, enabling identification of optimal
layers with reduced computational cost. These findings highlight the importance
of exploring the full representational depth of molecular encoders to achieve
substantial performance improvements and computational efficiency. The code is
made publicly available at
https://github.com/luispintoc/Unlocking-Chemical-Insights.

</details>


### [456] [Optimal Rates in Continual Linear Regression via Increasing Regularization](https://arxiv.org/abs/2506.06501)
*Ran Levinstein,Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: The paper addresses continual linear regression under random task orderings, proposing regularization schemes to improve learning rates up to optimal \(O(1/k)\).


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the discrepancy between the theoretical lower bound \(\Omega(1/k)\) and prior upper bounds \(O(1/k^{1/4})\) in realizable continual linear regression.

Method: The study utilizes isotropic \(\ell_2\) regularization and finite step budgets, redefining the problem using surrogate losses and analyzing a variant of SGD for time-varying functions.

Result: The fixed regularization strength achieves a near-optimal rate \(O(\log k / k)\), and increasing regularization schedules attain the optimal rate \(O(1/k)\).

Conclusion: The findings suggest that adjusting regularization strength and step allocation schedules is crucial for improving worst-case continual learning outcomes.

Abstract: We study realizable continual linear regression under random task orderings,
a common setting for developing continual learning theory. In this setup, the
worst-case expected loss after $k$ learning iterations admits a lower bound of
$\Omega(1/k)$. However, prior work using an unregularized scheme has only
established an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our
paper proves that this gap can be narrowed, or even closed, using two
frequently used regularization schemes: (1) explicit isotropic $\ell_2$
regularization, and (2) implicit regularization via finite step budgets. We
show that these approaches, which are used in practice to mitigate forgetting,
reduce to stochastic gradient descent (SGD) on carefully defined surrogate
losses. Through this lens, we identify a fixed regularization strength that
yields a near-optimal rate of $O(\log k / k)$. Moreover, formalizing and
analyzing a generalized variant of SGD for time-varying functions, we derive an
increasing regularization strength schedule that provably achieves an optimal
rate of $O(1/k)$. This suggests that schedules that increase the regularization
coefficient or decrease the number of steps per task are beneficial, at least
in the worst case.

</details>


### [457] [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance](https://arxiv.org/abs/2506.06444)
*Ruizhong Qiu,Gaotang Li,Tianxin Wei,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: This paper introduces SAFFRON, a novel inference scaling approach for language model safety, addressing challenges like the exploration-efficiency dilemma using innovative techniques like a multifurcation reward model.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the vulnerability of LLMs to jailbreak attacks and the unexplored potential of inference scaling in enhancing safety assurance.

Method: The authors developed SAFFRON, which includes a multifurcation reward model (MRM), partial supervision training, conservative exploration constraints, and a Trie-based caching strategy.

Result: Experiments demonstrate that SAFFRON significantly enhances safety assurance, outperforming conventional methods. Additionally, they released the Saffron-1 model and Safety4M dataset for public use.

Conclusion: SAFFRON provides a robust framework for LLM safety, combining innovation with practical tools to advance research against emerging threats in the field.

Abstract: Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .

</details>


### [458] [Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?](https://arxiv.org/abs/2506.07871)
*Sigma Jahan,Mohammad Masudur Rahman*

Main category: cs.LG

TL;DR: This study examines Hessian-based analysis techniques to diagnose faults in attention-based deep learning models, proving more effective than gradient methods for identifying instability and faults.


<details>
  <summary>Details</summary>
Motivation: As attention-based models grow in size and complexity, traditional fault-diagnosing methods often fail or lack precision. The paper aims to explore alternative techniques using Hessian-based metrics for improved diagnostics.

Method: The authors analyze fault diagnosis in attention models using Hessian-derived insights like curvature analysis and parameter interaction analysis. They test these methods on three diverse models: HAN, 3D-CNN, and DistilBERT.

Result: Hessian-based metrics successfully localized instability and identified fault sources more effectively compared to gradient-based methods.

Conclusion: Hessian-derived insights can play a significant role in enhancing fault diagnosis for complex neural architectures, improving debugging practices in software development.

Abstract: As attention-based deep learning models scale in size and complexity,
diagnosing their faults becomes increasingly challenging. In this work, we
conduct an empirical study to evaluate the potential of Hessian-based analysis
for diagnosing faults in attention-based models. Specifically, we use
Hessian-derived insights to identify fragile regions (via curvature analysis)
and parameter interdependencies (via parameter interaction analysis) within
attention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,
DistilBERT), we show that Hessian-based metrics can localize instability and
pinpoint fault sources more effectively than gradients alone. Our empirical
findings suggest that these metrics could significantly improve fault diagnosis
in complex neural architectures, potentially improving software debugging
practices.

</details>


### [459] [Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs](https://arxiv.org/abs/2506.06521)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: This paper introduces the Monotonic Value Propagation (MVP) algorithm for episodic MDPs, achieving variance-aware gap-dependent regret bounds. It provides both upper and lower bounds, demonstrating the critical role of certain variance terms in regret bounds.


<details>
  <summary>Details</summary>
Motivation: The study aims to advance understanding of gap-dependent regret bounds in episodic MDPs, particularly incorporating variance-awareness to improve learning performance and provide sharper theoretical guarantees.

Method: The authors introduce the MVP algorithm and conduct a novel analysis involving weighted sums of suboptimality gaps. They establish an upper regret bound for MVP and derive a complementary lower bound to demonstrate the necessity of variance-dependent terms.

Result: The main result is an upper bound for the MVP algorithm that incorporates variance-awareness, alongside a lower bound that highlights the necessity of including variance terms. This establishes the strengths and limitations of the approach, setting benchmarks for performance.

Conclusion: The paper shows that the MVP algorithm achieves near-optimal variance-aware gap-dependent regret bounds, effectively balancing planning horizon, state-action complexity, and variance terms. This analysis can be extended to study other algorithms in similar settings.

Abstract: We consider the gap-dependent regret bounds for episodic MDPs. We show that
the Monotonic Value Propagation (MVP) algorithm achieves a variance-aware
gap-dependent regret bound of $$\tilde{O}\left(\left(\sum_{\Delta_h(s,a)>0}
\frac{H^2 \log K \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}
+\sum_{\Delta_h(s,a)=0}\frac{ H^2 \land
\mathtt{Var}_{\max}^{\text{c}}}{\Delta_{\mathrm{min}}} + SAH^4 (S \lor H)
\right) \log K\right),$$ where $H$ is the planning horizon, $S$ is the number
of states, $A$ is the number of actions, and $K$ is the number of episodes.
Here, $\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality
gap and $\Delta_{\mathrm{min}} := \min_{\Delta_h (s,a) > 0} \Delta_h(s,a)$. The
term $\mathtt{Var}_{\max}^{\text{c}}$ denotes the maximum conditional total
variance, calculated as the maximum over all $(\pi, h, s)$ tuples of the
expected total variance under policy $\pi$ conditioned on trajectories visiting
state $s$ at step $h$. $\mathtt{Var}_{\max}^{\text{c}}$ characterizes the
maximum randomness encountered when learning any $(h, s)$ pair. Our result
stems from a novel analysis of the weighted sum of the suboptimality gap and
can be potentially adapted for other algorithms. To complement the study, we
establish a lower bound of $$\Omega \left( \sum_{\Delta_h(s,a)>0} \frac{H^2
\land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}\cdot \log K\right),$$
demonstrating the necessity of dependence on $\mathtt{Var}_{\max}^{\text{c}}$
even when the maximum unconditional total variance (without conditioning on
$(h, s)$) approaches zero.

</details>


### [460] [Graph Persistence goes Spectral](https://arxiv.org/abs/2506.06571)
*Mattie Ji,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: The paper introduces SpectRe, a new topological descriptor for graphs that incorporates spectral data into Persistent Homology diagrams, achieving higher expressivity and local stability in graph representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing graph representation learning methods using Persistent Homology (PH) fail to capture key structural information due to heavy reliance on vertex and edge features.

Method: SpectRe integrates spectral information into PH diagrams and introduces analytical concepts of global and local stability for topological descriptors.

Result: SpectRe demonstrated superior expressivity over prior methods and proved effective on synthetic and real-world datasets, supporting its value for graph-based learning tasks.

Conclusion: SpectRe enhances the capabilities of graph neural networks by improving expressivity and local stability, advancing the integration of topological and spectral insights in graph representation learning.

Abstract: Including intricate topological information (e.g., cycles) provably enhances
the expressivity of message-passing graph neural networks (GNNs) beyond the
Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods
are increasingly employed for graph representation learning. In this context,
recent works have proposed decorating classical PH diagrams with vertex and
edge features for improved expressivity. However, due to their dependence on
features, these methods still fail to capture basic graph structural
information. In this paper, we propose SpectRe -- a new topological descriptor
for graphs that integrates spectral information into PH diagrams. Notably,
SpectRe is strictly more expressive than existing descriptors on graphs. We
also introduce notions of global and local stability to analyze existing
descriptors and establish that SpectRe is locally stable. Finally, experiments
on synthetic and real-world datasets demonstrate the effectiveness of SpectRe
and its potential to enhance the capabilities of graph models in relevant
learning tasks.

</details>


### [461] [Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing](https://arxiv.org/abs/2506.06582)
*Diaaeldin Taha,James Chapman,Marzieh Eidi,Karel Devriendt,Guido Montúfar*

Main category: cs.LG

TL;DR: The paper proposes a framework to address oversquashing issues in topological deep learning (TDL) for higher-order interactions in relational data.


<details>
  <summary>Details</summary>
Motivation: Oversquashing in topological message-passing networks is a poorly understood phenomenon, which limits the capability of TDL in effectively capturing higher-order interactions.

Method: The authors introduce an axiomatic framework that unifies graph and topological message-passing by treating simplicial and cellular complexes as relational structures, extending graph-theoretic concepts and algorithms.

Result: Theoretical analysis and empirical studies validate the effectiveness of this framework in analyzing and alleviating oversquashing in TDL networks.

Conclusion: The framework bridges gaps between graph theory and TDL, advancing the field by providing theoretical tools to mitigate oversquashing while enhancing the understanding of topological message-passing networks.

Abstract: Topological deep learning (TDL) has emerged as a powerful tool for modeling
higher-order interactions in relational data. However, phenomena such as
oversquashing in topological message-passing remain understudied and lack
theoretical analysis. We propose a unifying axiomatic framework that bridges
graph and topological message-passing by viewing simplicial and cellular
complexes and their message-passing schemes through the lens of relational
structures. This approach extends graph-theoretic results and algorithms to
higher-order structures, facilitating the analysis and mitigation of
oversquashing in topological message-passing networks. Through theoretical
analysis and empirical studies on simplicial networks, we demonstrate the
potential of this framework to advance TDL.

</details>


### [462] [Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control](https://arxiv.org/abs/2506.06459)
*Ruitao Chen,Mozhang Guo,Jinge Li*

Main category: cs.LG

TL;DR: The paper introduces a reinforcement learning-based cruise control system in automated vehicles to enhance infant sleep quality by personalizing driving behavior while maintaining travel efficiency.


<details>
  <summary>Details</summary>
Motivation: Current automated driving systems lack consideration for the well-being of passengers, especially infants, whose sleep can be disrupted by aggressive driving maneuvers.

Method: The study uses reinforcement learning techniques combined with wearable sensors and vehicle data to adapt driving behaviors. It employs LSTM and transformer-based neural networks to model the effect of driving on infant sleep quality and dynamically adjusts driving parameters.

Result: Simulation results indicate that the proposed framework improves infant sleep quality significantly compared to baseline models, while maintaining efficient travel.

Conclusion: The integration of reinforcement learning and adaptive driving strategies successfully balances infant comfort and travel efficiency, demonstrating practical benefits in enhancing passenger well-being.

Abstract: Automated driving (AD) has substantially improved vehicle safety and driving
comfort, but their impact on passenger well-being, particularly infant sleep,
is not sufficiently studied. Sudden acceleration, abrupt braking, and sharp
maneuvers can disrupt infant sleep, compromising both passenger comfort and
parental convenience. To solve this problem, this paper explores the
integration of reinforcement learning (RL) within AD to personalize driving
behavior and optimally balance occupant comfort and travel efficiency. In
particular, we propose an intelligent cruise control framework that adapts to
varying driving conditions to enhance infant sleep quality by effectively
synergizing wearable sensing and vehicle data. Long short-term memory (LSTM)
and transformer-based neural networks are integrated with RL to model the
relationship between driving behavior and infant sleep quality under diverse
traffic and road conditions. Based on the sleep quality indicators from the
wearable sensors, driving action data from vehicle controllers, and map data
from map applications, the model dynamically computes the optimal driving
aggressiveness level, which is subsequently translated into specific AD control
strategies, e.g., the magnitude and frequency of acceleration, lane change, and
overtaking. Simulation results demonstrate that the proposed solution
significantly improves infant sleep quality compared to baseline methods, while
preserving desirable travel efficiency.

</details>


### [463] [Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures](https://arxiv.org/abs/2506.06584)
*Mo Zhou,Weihang Xu,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: The paper establishes global convergence for learning Gaussian Mixture Models (GMMs) in an over-parameterized setting using gradient EM. Previous studies only showed convergence for GMMs with 2 components; this paper expands it to any number of components under mild over-parameterization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rigorous global convergence guarantees for cases where the number of components in the ground truth GMM is greater than 2 ($m ≥ 3$), especially in machine learning over-parameterized settings.

Method: The authors analyze gradient EM in an over-parameterized context, proving global convergence using Hermite polynomials for dynamic analysis and tensor decomposition for geometric characterization of the likelihood loss.

Result: They demonstrate that gradient EM globally converges to the ground truth at a polynomial rate using polynomial samples when the learning model over-parameterizes the ground truth with $n = Ω(m log m)$ components.

Conclusion: The work is the first to establish global convergence guarantees for EM and Gradient EM beyond previously limited settings ($m = 2$), significantly contributing to the theoretical understanding of over-parameterized Gaussian Mixture Models.

Abstract: Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine
learning, with the Expectation-Maximization (EM) algorithm and its popular
variant gradient EM being arguably the most widely used algorithms in practice.
In the exact-parameterized setting, where both the ground truth GMM and the
learning model have the same number of components $m$, a vast line of work has
aimed to establish rigorous recovery guarantees for EM. However, global
convergence has only been proven for the case of $m=2$, and EM is known to fail
to recover the ground truth when $m\geq 3$.
  In this paper, we consider the $\textit{over-parameterized}$ setting, where
the learning model uses $n>m$ components to fit an $m$-component ground truth
GMM. In contrast to the exact-parameterized case, we provide a rigorous global
convergence guarantee for gradient EM. Specifically, for any well separated
GMMs in general position, we prove that with only mild over-parameterization $n
= \Omega(m\log m)$, randomly initialized gradient EM converges globally to the
ground truth at a polynomial rate with polynomial samples. Our analysis
proceeds in two stages and introduces a suite of novel tools for Gaussian
Mixture analysis. We use Hermite polynomials to study the dynamics of gradient
EM and employ tensor decomposition to characterize the geometric landscape of
the likelihood loss. This is the first global convergence and recovery result
for EM or Gradient EM beyond the special case of $m=2$.

</details>


### [464] [TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness](https://arxiv.org/abs/2506.06482)
*Zhiyuan Zhao,Juntong Ni,Shangqing Xu,Haoxin Liu,Wei Jin,B. Aditya Prakash*

Main category: cs.LG

TL;DR: The paper introduces TimeRecipe, a framework for benchmarking time-series forecasting methods, which evaluates design components in over 10,000 experiments and provides a practical model recommendation toolkit.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation of design components in time-series forecasting models and provide deeper insights into why certain designs work.

Method: The authors propose TimeRecipe, a benchmarking framework that systematically evaluates individual design components of time-series forecasting models via 10,000+ experiments across diverse datasets and tasks.

Result: Exhaustive exploration of design choices yielded models outperforming state-of-the-art methods and uncovered correlations between specific designs and forecasting contexts.

Conclusion: By utilizing an extensive benchmarking and evaluation process, TimeRecipe provides valuable insights and a practical toolkit for recommending effective forecasting architectures based on empirical data.

Abstract: Time-series forecasting is an essential task with wide real-world
applications across domains. While recent advances in deep learning have
enabled time-series forecasting models with accurate predictions, there remains
considerable debate over which architectures and design components, such as
series decomposition or normalization, are most effective under varying
conditions. Existing benchmarks primarily evaluate models at a high level,
offering limited insight into why certain designs work better. To mitigate this
gap, we propose TimeRecipe, a unified benchmarking framework that
systematically evaluates time-series forecasting methods at the module level.
TimeRecipe conducts over 10,000 experiments to assess the effectiveness of
individual components across a diverse range of datasets, forecasting horizons,
and task settings. Our results reveal that exhaustive exploration of the design
space can yield models that outperform existing state-of-the-art methods and
uncover meaningful intuitions linking specific design choices to forecasting
scenarios. Furthermore, we release a practical toolkit within TimeRecipe that
recommends suitable model architectures based on these empirical insights. The
benchmark is available at: https://github.com/AdityaLab/TimeRecipe.

</details>


### [465] [Direct Prediction Set Minimization via Bilevel Conformal Classifier Training](https://arxiv.org/abs/2506.06599)
*Yuanjie Shi,Hooman Shahrokhi,Xuesong Jia,Xiongzhi Chen,Janardhan Rao Doppa,Yan Yan*

Main category: cs.LG

TL;DR: This paper introduces an improved algorithm for conformal prediction that minimizes the size of prediction sets while ensuring performance guarantees.


<details>
  <summary>Details</summary>
Motivation: Standard methods in conformal prediction tend to produce impractically large prediction sets, which limit their usefulness.

Method: The authors propose the Direct Prediction Set Minimization (DPSM) algorithm, which utilizes a bilevel optimization approach to minimize prediction set sizes.

Result: DPSM achieves a learning bound of $O(1/\sqrt{n})$, significantly outperforming prior methods which have worse bounds. It also achieves a $20.46\%$ reduction in prediction set size across various datasets.

Conclusion: DPSM effectively integrates conformal prediction principles into deep model training, providing smaller prediction sets and stronger theoretical guarantees than previous approaches.

Abstract: Conformal prediction (CP) is a promising uncertainty quantification framework
which works as a wrapper around a black-box classifier to construct prediction
sets (i.e., subset of candidate classes) with provable guarantees. However,
standard calibration methods for CP tend to produce large prediction sets which
makes them less useful in practice. This paper considers the problem of
integrating conformal principles into the training process of deep classifiers
to directly minimize the size of prediction sets. We formulate conformal
training as a bilevel optimization problem and propose the {\em Direct
Prediction Set Minimization (DPSM)} algorithm to solve it. The key insight
behind DPSM is to minimize a measure of the prediction set size (upper level)
that is conditioned on the learned quantile of conformity scores (lower level).
We analyze that DPSM has a learning bound of $O(1/\sqrt{n})$ (with $n$ training
samples), while prior conformal training methods based on stochastic
approximation for the quantile has a bound of $\Omega(1/s)$ (with batch size
$s$ and typically $s \ll \sqrt{n}$). Experiments on various benchmark datasets
and deep models show that DPSM significantly outperforms the best prior
conformal training baseline with $20.46\%\downarrow$ in the prediction set size
and validates our theory.

</details>


### [466] [Spark Transformer: Reactivating Sparsity in FFN and Attention](https://arxiv.org/abs/2506.06644)
*Chong You,Kan Wu,Zhipeng Jia,Lin Chen,Srinadh Bhojanapalli,Jiaxian Guo,Utku Evci,Jan Wassenberg,Praneeth Netrapalli,Jeremiah J. Willcock,Suvinay Subramanian,Felix Chern,Alek Andreev,Shreya Pathak,Felix Yu,Prateek Jain,David E. Culler,Henry M. Levy,Sanjiv Kumar*

Main category: cs.LG

TL;DR: Spark Transformer is a novel architecture achieving high activation sparsity, improving efficiency without harming model quality.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies and challenges in activation sparsity for trained Transformers, especially with modern architectures moving away from ReLU.

Method: Integrates top-k masking and statistical top-k for efficient sparsity control, reallocates FFN and attention parameters for enhanced activation prediction.

Result: Spark Transformer achieves 8% FFN neuron activation and token attention limited to 256 tokens, realizing up to 2.5x FLOPs reduction and notable speedups.

Conclusion: Spark Transformer successfully balances activation sparsity and model quality while enhancing computational efficiency.

Abstract: The discovery of the lazy neuron phenomenon in trained Transformers, where
the vast majority of neurons in their feed-forward networks (FFN) are inactive
for each token, has spurred tremendous interests in activation sparsity for
enhancing large model efficiency. While notable progress has been made in
translating such sparsity to wall-time benefits, modern Transformers have moved
away from the ReLU activation function crucial to this phenomenon. Existing
efforts on re-introducing activation sparsity often degrade model quality,
increase parameter count, complicate or slow down training. Sparse attention,
the application of sparse activation to the attention mechanism, often faces
similar challenges.
  This paper introduces the Spark Transformer, a novel architecture that
achieves a high level of activation sparsity in both FFN and the attention
mechanism while maintaining model quality, parameter count, and standard
training procedures. Our method realizes sparsity via top-k masking for
explicit control over sparsity level. Crucially, we introduce statistical
top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that
avoids costly sorting and mitigates significant training slowdown from standard
top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN
parameters and attention key embeddings to form a low-cost predictor for
identifying activated entries. This design not only mitigates quality loss from
enforced sparsity, but also enhances wall-time benefit. Pretrained with the
Gemma-2 recipe, Spark Transformer demonstrates competitive performance on
standard benchmarks while exhibiting significant sparsity: only 8% of FFN
neurons are activated, and each token attends to a maximum of 256 tokens. This
sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time
speedups of up to 1.79x on CPU and 1.40x on GPU.

</details>


### [467] [SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.06649)
*Yishan Shen,Yuyang Ye,Hui Xiong,Yong Chen*

Main category: cs.LG

TL;DR: This paper proposes SAFER, a risk-aware framework that merges structured EHR data and clinical notes for safe dynamic treatment recommendations in precision medicine.


<details>
  <summary>Details</summary>
Motivation: Existing methods for dynamic treatment regimes overly depend on clinician-prescribed standards and structured EHR data, limiting their effectiveness and reliability, particularly in the absence of an optimal treatment strategy.

Method: The proposed SAFER framework integrates structured EHR and clinical notes, assuming label uncertainty for deceased patients and employing conformal prediction for statistical safety guarantees.

Result: SAFER outperforms state-of-the-art baselines in recommendation metrics and reduces counterfactual mortality rates, validated on two sepsis datasets.

Conclusion: SAFER shows promise as a reliable and theoretically robust framework for high-stakes clinical decision-making in dynamic treatment regimes.

Abstract: Dynamic treatment regimes (DTRs) are critical to precision medicine,
optimizing long-term outcomes through personalized, real-time decision-making
in evolving clinical contexts, but require careful supervision for unsafe
treatment risks. Existing efforts rely primarily on clinician-prescribed gold
standards despite the absence of a known optimal strategy, and predominantly
using structured EHR data without extracting valuable insights from clinical
notes, limiting their reliability for treatment recommendations. In this work,
we introduce SAFER, a calibrated risk-aware tabular-language recommendation
framework for DTR that integrates both structured EHR and clinical notes,
enabling them to learn from each other, and addresses inherent label
uncertainty by assuming ambiguous optimal treatment solution for deceased
patients. Moreover, SAFER employs conformal prediction to provide statistical
guarantees, ensuring safe treatment recommendations while filtering out
uncertain predictions. Experiments on two publicly available sepsis datasets
demonstrate that SAFER outperforms state-of-the-art baselines across multiple
recommendation metrics and counterfactual mortality rate, while offering robust
formal assurances. These findings underscore SAFER potential as a trustworthy
and theoretically grounded solution for high-stakes DTR applications.

</details>


### [468] [Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms](https://arxiv.org/abs/2506.06499)
*Alex Havrilla,Edward Hughes,Mikayel Samvelyan,Jacob Abernethy*

Main category: cs.LG

TL;DR: The paper introduces SPARQ, a method for generating large-scale, high-quality, and diverse synthetic math problems using a single model evaluated by solve-rate and analyzes the impact on model reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for model reasoning improvement rely heavily on distillation or ground-truth problem statements, limiting scalability and diversity in complex problem domains.

Method: SPARQ employs Quality-Diversity Algorithms and solve-rate metrics to generate synthetic problem-solution pairs from a seed dataset, followed by quality filtering and fine-tuning for enhanced reasoning.

Result: SPARQ generates over 20 million synthetic problem-solution pairs from a 7.5K sample seed. Fine-tuning using filtered data improved model performance by up to 24%. Synthetic data scaling laws were established, showcasing quality and diversity's effects on generalization.

Conclusion: Filtering synthetic data by difficulty boosts in-distribution performance, while diversity filtering facilitates robust out-of-distribution generalization. Scaling laws validate the benefits for reasoning enhancement.

Abstract: Large language model (LLM) driven synthetic data generation has emerged as a
powerful method for improving model reasoning capabilities. However, most
methods either distill large state-of-the-art models into small students or use
natural ground-truth problem statements to guarantee problem statement quality.
This limits the scalability of these approaches to more complex and diverse
problem domains. To address this, we present SPARQ: Synthetic Problem
Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for
generating high-quality and diverse synthetic math problem and solution pairs
using only a single model by measuring a problem's solve-rate: a proxy for
problem difficulty. Starting from a seed dataset of 7.5K samples, we generate
over 20 million new problem-solution pairs. We show that filtering the
generated data by difficulty and then fine-tuning the same model on the
resulting data improves relative model performance by up to 24\%. Additionally,
we conduct ablations studying the impact of synthetic data quantity, quality
and diversity on model generalization. We find that higher quality, as measured
by problem difficulty, facilitates better in-distribution performance. Further,
while generating diverse synthetic data does not as strongly benefit
in-distribution performance, filtering for more diverse data facilitates more
robust OOD generalization. We also confirm the existence of model and data
scaling laws for synthetically generated problems, which positively benefit
downstream model generalization.

</details>


### [469] [Rescaled Influence Functions: Accurate Data Attribution in High Dimension](https://arxiv.org/abs/2506.06656)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: The paper introduces Rescaled Influence Functions (RIF) to improve data attribution accuracy for machine learning models, highlighting its advantages over traditional Influence Functions (IF).


<details>
  <summary>Details</summary>
Motivation: Existing Influence Functions (IF) for data attribution often fail in high-dimensional settings and underestimate the effects of removing training samples.

Method: The authors propose Rescaled Influence Functions (RIF), a method built for both theoretical and practical improvement over standard IF, which minimally increases computational overhead.

Result: RIF produces superior predictions compared to IF on various real-world datasets and is also robust against certain data poisoning attacks that can mislead IF.

Conclusion: RIF is a better alternative to IF for data attribution, offering higher precision, robustness, and practical applicability for machine learning tasks.

Abstract: How does the training data affect a model's behavior? This is the question we
seek to answer with data attribution. The leading practical approaches to data
attribution are based on influence functions (IF). IFs utilize a first-order
Taylor approximation to efficiently predict the effect of removing a set of
samples from the training set without retraining the model, and are used in a
wide variety of machine learning applications. However, especially in the
high-dimensional regime (# params $\geq \Omega($# samples$)$), they are often
imprecise and tend to underestimate the effect of sample removals, even for
simple models such as logistic regression. We present rescaled influence
functions (RIF), a new tool for data attribution which can be used as a drop-in
replacement for influence functions, with little computational overhead but
significant improvement in accuracy. We compare IF and RIF on a range of
real-world datasets, showing that RIFs offer significantly better predictions
in practice, and present a theoretical analysis explaining this improvement.
Finally, we present a simple class of data poisoning attacks that would fool
IF-based detections but would be detected by RIF.

</details>


### [470] [Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering](https://arxiv.org/abs/2506.06666)
*Oktay Karakuş,Hasan Arkadaş*

Main category: cs.LG

TL;DR: This study introduces an unsupervised method to detect and analyze line-breaking passes (LBPs) in football matches using tracking and event data.


<details>
  <summary>Details</summary>
Motivation: The importance of LBPs lies in their ability to break defensive lines and create high-value offensive opportunities for teams in football.

Method: The paper uses a clustering-based unsupervised framework that incorporates synchronized tracking and event data, applies vertical spatial segmentation for modeling defensive shapes, and develops new metrics to assess LBP effectiveness.

Result: Using data from the 2022 FIFA World Cup, the study highlights variations in vertical progression and defensive disruption among teams and players, showcasing tactical styles.

Conclusion: The framework and metrics introduced in this paper are scalable, explainable, and applicable to performance analysis and scouting in football.

Abstract: Line-breaking passes (LBPs) are crucial tactical actions in football,
allowing teams to penetrate defensive lines and access high-value spaces. In
this study, we present an unsupervised, clustering-based framework for
detecting and analysing LBPs using synchronised event and tracking data from
elite matches. Our approach models opponent team shape through vertical spatial
segmentation and identifies passes that disrupt defensive lines within open
play. Beyond detection, we introduce several tactical metrics, including the
space build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and
LBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or
sustained attacking threats. We evaluate these metrics across teams and players
in the 2022 FIFA World Cup, revealing stylistic differences in vertical
progression and structural disruption. The proposed methodology is explainable,
scalable, and directly applicable to modern performance analysis and scouting
workflows.

</details>


### [471] [A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks](https://arxiv.org/abs/2506.06715)
*Minh-Duc Nguyen,Dung D. Le*

Main category: cs.LG

TL;DR: The paper introduces SVH-MOL, a novel method using Stein Variational Gradient Descent (SVGD) for achieving diverse and hypervolume-maximizing solutions in multi-objective learning.


<details>
  <summary>Details</summary>
Motivation: Current methods in multi-objective learning often struggle to achieve both solution diversity and maximum hypervolume in the Pareto set.

Method: The approach uses SVGD to push particles toward the Pareto set via functional gradient descent and incorporates diverse gradient direction strategies along with an annealing schedule for stability.

Result: SVH-MOL is experimentally validated to outperform existing methods in multi-objective optimization and multi-task learning.

Conclusion: The proposed SVH-MOL framework enhances diversity and stability in Pareto set learning while delivering superior performance.

Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining
the complete optimal solution in Multi-objective Learning (MOL). A set of
optimal solutions approximates the Pareto set, and its mapping is a set of
dense points in the Pareto front in objective space. However, some current
methods face a challenge: how to make the Pareto solution is diverse while
maximizing the hypervolume value. In this paper, we propose a novel method to
address this challenge, which employs Stein Variational Gradient Descent (SVGD)
to approximate the entire Pareto set. SVGD pushes a set of particles towards
the Pareto set by applying a form of functional gradient descent, which helps
to converge and diversify optimal solutions. Additionally, we employ diverse
gradient direction strategies to thoroughly investigate a unified framework for
SVGD in multi-objective optimization and adapt this framework with an annealing
schedule to promote stability. We introduce our method, SVH-MOL, and validate
its effectiveness through extensive experiments on multi-objective problems and
multi-task learning, demonstrating its superior performance.

</details>


### [472] [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.06532)
*Zijiang Yan,Hao Zhou,Jianhua Pei,Hina Tabassum*

Main category: cs.LG

TL;DR: The paper proposes a hierarchical and collaborative control framework using large language models (LLMs) for UAVs in integrated networks, achieving better performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Controlling and optimizing multi-UAV systems in dynamic and constrained environments remains a challenge, especially when integrated with terrestrial and non-terrestrial networks.

Method: A hierarchical framework where LLMs on high-altitude platform stations (HAPS) manage UAV access control and LLMs on UAVs handle motion planning, utilizing pre-trained knowledge for strategic and tactical decisions.

Result: Experimental results show improved system rewards, reduced operational costs, and significantly lower UAV collision rates using the LLM-based approach compared to baselines.

Conclusion: The LLM-driven method offers a promising solution for advanced UAV traffic control, paving the way for next-generation 3D aerial highway systems.

Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various
real-world applications. However, the control and optimization of multi-UAV
systems remain a significant challenge, particularly in dynamic and constrained
environments. This work explores the joint motion and communication control of
multiple UAVs operating within integrated terrestrial and non-terrestrial
networks that include high-altitude platform stations (HAPS). Specifically, we
consider an aerial highway scenario in which UAVs must accelerate, decelerate,
and change lanes to avoid collisions and maintain overall traffic flow.
Different from existing studies, we propose a novel hierarchical and
collaborative method based on large language models (LLMs). In our approach, an
LLM deployed on the HAPS performs UAV access control, while another LLM onboard
each UAV handles motion planning and control. This LLM-based framework
leverages the rich knowledge embedded in pre-trained models to enable both
high-level strategic planning and low-level tactical decisions. This
knowledge-driven paradigm holds great potential for the development of
next-generation 3D aerial highway systems. Experimental results demonstrate
that our proposed collaborative LLM-based method achieves higher system
rewards, lower operational costs, and significantly reduced UAV collision rates
compared to baseline approaches.

</details>


### [473] [Curvature Enhanced Data Augmentation for Regression](https://arxiv.org/abs/2506.06853)
*Ilya Kaufman Sirot,Omri Azencot*

Main category: cs.LG

TL;DR: The paper introduces Curvature-Enhanced Manifold Sampling (CEMS), a second-order data augmentation method for regression tasks, demonstrating superior performance with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Over-parameterized models often face overfitting concerns but generalize well with effective regularization techniques, especially data augmentation. However, regression tasks lack focused data augmentation approaches.

Method: The CEMS method utilizes a second-order approximation of data manifolds to efficiently sample and reconstruct synthetic data, addressing data augmentation in regression tasks.

Result: Evaluations show that CEMS consistently outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios.

Conclusion: CEMS provides an advanced framework for data augmentation in regression, combining theoretical foundations with practical efficiency, enhancing performance while using minimal resources.

Abstract: Deep learning models with a large number of parameters, often referred to as
over-parameterized models, have achieved exceptional performance across various
tasks. Despite concerns about overfitting, these models frequently generalize
well to unseen data, thanks to effective regularization techniques, with data
augmentation being among the most widely used. While data augmentation has
shown great success in classification tasks using label-preserving
transformations, its application in regression problems has received less
attention. Recently, a novel \emph{manifold learning} approach for generating
synthetic data was proposed, utilizing a first-order approximation of the data
manifold. Building on this foundation, we present a theoretical framework and
practical tools for approximating and sampling general data manifolds.
Furthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)
method for regression tasks. CEMS leverages a second-order representation of
the data manifold to enable efficient sampling and reconstruction of new data
points. Extensive evaluations across multiple datasets and comparisons with
state-of-the-art methods demonstrate that CEMS delivers superior performance in
both in-distribution and out-of-distribution scenarios, while introducing only
minimal computational overhead. Code is available at
https://github.com/azencot-group/CEMS.

</details>


### [474] [GeoClip: Geometry-Aware Clipping for Differentially Private SGD](https://arxiv.org/abs/2506.06549)
*Atefeh Gilani,Naima Tasnim,Lalitha Sankar,Oliver Kosut*

Main category: cs.LG

TL;DR: GeoClip introduces a geometry-aware approach to DP-SGD that transforms gradients into a basis aligned with their distribution, improving privacy-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation of DP-SGD methods that fail to consider coordinate-wise correlations when setting gradient clipping thresholds, impacting model performance.

Method: The proposed GeoClip framework uses a transformed basis aligned with the gradient geometry to clip and perturb gradients, estimating this transformation adaptively using noisy gradients without additional privacy cost.

Result: GeoClip outperforms existing adaptive methods in experiments, offering improved utility across tabular and image datasets within the same privacy constraints.

Conclusion: Geometry-aware adaptive clipping in gradient descent provides tighter privacy-utility trade-offs, showcasing GeoClip's potential as an optimal method for differentially private learning.

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the most
widely used method for training machine learning models with provable privacy
guarantees. A key challenge in DP-SGD is setting the per-sample gradient
clipping threshold, which significantly affects the trade-off between privacy
and utility. While recent adaptive methods improve performance by adjusting
this threshold during training, they operate in the standard coordinate system
and fail to account for correlations across the coordinates of the gradient. We
propose GeoClip, a geometry-aware framework that clips and perturbs gradients
in a transformed basis aligned with the geometry of the gradient distribution.
GeoClip adaptively estimates this transformation using only previously released
noisy gradients, incurring no additional privacy cost. We provide convergence
guarantees for GeoClip and derive a closed-form solution for the optimal
transformation that minimizes the amount of noise added while keeping the
probability of gradient clipping under control. Experiments on both tabular and
image datasets demonstrate that GeoClip consistently outperforms existing
adaptive clipping methods under the same privacy budget.

</details>


### [475] [Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning](https://arxiv.org/abs/2506.06873)
*Armin Behnamnia,Gholamali Aminian,Alireza Aghaei,Chengchun Shi,Vincent Y. F. Tan,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: The paper introduces a log-sum-exponential (LSE) estimator for off-policy learning and evaluation, which reduces variance and performs robustly under heavy-tailed conditions.


<details>
  <summary>Details</summary>
Motivation: Challenges in off-policy learning and evaluation arise due to high variance and inefficacy with low-quality propensity scores and heavy-tailed reward distributions.

Method: The authors propose the LSE estimator, analyze its bias, variance, and regret bounds, and validate it both theoretically and empirically.

Result: The LSE estimator shows reduction in variance, robustness under challenging conditions, and a convergence rate of $O(n^{-\epsilon/(1+ \epsilon)})$ for regret bounds.

Conclusion: The LSE estimator offers theoretical and practical advantages for off-policy learning and evaluation, with accessible implementation.

Abstract: Off-policy learning and evaluation leverage logged bandit feedback datasets,
which contain context, action, propensity score, and feedback for each data
point. These scenarios face significant challenges due to high variance and
poor performance with low-quality propensity scores and heavy-tailed reward
distributions. We address these issues by introducing a novel estimator based
on the log-sum-exponential (LSE) operator, which outperforms traditional
inverse propensity score estimators. Our LSE estimator demonstrates variance
reduction and robustness under heavy-tailed conditions. For off-policy
evaluation, we derive upper bounds on the estimator's bias and variance. In the
off-policy learning scenario, we establish bounds on the regret -- the
performance gap between our LSE estimator and the optimal policy -- assuming
bounded $(1+\epsilon)$-th moment of weighted reward. Notably, we achieve a
convergence rate of $O(n^{-\epsilon/(1+ \epsilon)})$ for the regret bounds,
where $\epsilon \in [0,1]$ and $n$ is the size of logged bandit feedback
dataset. Theoretical analysis is complemented by comprehensive empirical
evaluations in both off-policy learning and evaluation scenarios, confirming
the practical advantages of our approach. The code for our estimator is
available at the following link:
https://github.com/armin-behnamnia/lse-offpolicy-learning.

</details>


### [476] [SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks](https://arxiv.org/abs/2506.06556)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Yi Li*

Main category: cs.LG

TL;DR: The paper introduces a robust Software-Defined Networking (SDN)-based system to detect and mitigate false data injection attacks in in-vehicle networks using LSTM models and adversarial attack countermeasures.


<details>
  <summary>Details</summary>
Motivation: The growth of autonomous and connected vehicles has increased the complexity of in-vehicle communication, requiring enhanced security measures to protect ECUs from false data injection attacks.

Method: The authors propose the FDDMS system utilizing SDN to monitor and detect false data in real-time. LSTM-based models are employed for attack detection, and adversarial attacks are countered by a re-training technique. Mitigation is achieved dynamically by SDN flow rule updates.

Result: Experimental results demonstrate that FDDMS effectively detects and mitigates false data injection attacks in real-time and provides robustness against various adversarial attacks.

Conclusion: The proposed FDDMS system enhances the security of in-vehicle networks, proving effective and robust against false data injection and adversarial attacks through real-time detection and mitigation.

Abstract: As the development of autonomous and connected vehicles advances, the
complexity of modern vehicles increases, with numerous Electronic Control Units
(ECUs) integrated into the system. In an in-vehicle network, these ECUs
communicate with one another using an standard protocol called Controller Area
Network (CAN). Securing communication among ECUs plays a vital role in
maintaining the safety and security of the vehicle. This paper proposes a
robust SDN-based False Data Detection and Mitigation System (FDDMS) for
in-vehicle networks. Leveraging the unique capabilities of Software-Defined
Networking (SDN), FDDMS is designed to monitor and detect false data injection
attacks in real-time. Specifically, we focus on brake-related ECUs within an
SDN-enabled in-vehicle network. First, we decode raw CAN data to create an
attack model that illustrates how false data can be injected into the system.
Then, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection
model, is used to identify false data injection attacks. We further propose an
effective variant of DeepFool attack to evaluate the model's robustness. To
countermeasure the impacts of four adversarial attacks including Fast gradient
descent method, Basic iterative method, DeepFool, and the DeepFool variant, we
further enhance a re-training technique method with a threshold based selection
strategy. Finally, a mitigation scheme is implemented to redirect attack
traffic by dynamically updating flow rules through SDN. Our experimental
results show that the proposed FDDMS is robust against adversarial attacks and
effectively detects and mitigates false data injection attacks in real-time.

</details>


### [477] [Scalable Gaussian Processes with Latent Kronecker Structure](https://arxiv.org/abs/2506.06895)
*Jihao Andreas Lin,Sebastian Ament,Maximilian Balandat,David Eriksson,José Miguel Hernández-Lobato,Eytan Bakshy*

Main category: cs.LG

TL;DR: The paper introduces a method to handle large datasets in Gaussian processes (GPs) by leveraging latent Kronecker structures, making computation more efficient and enabling exact GP inference.


<details>
  <summary>Details</summary>
Motivation: Address the computational challenges of applying Gaussian processes to large datasets, especially in cases where missing observations disrupt the underlying structure.

Method: Utilizes the latent Kronecker product structure in the kernel matrix combined with iterative solvers and pathwise conditioning to improve computational scalability.

Result: The proposed method outperforms existing sparse and variational GP techniques on large real-world datasets, including applications in robotics and climate studies.

Conclusion: The approach enables exact GP inference on large datasets while needing fewer resources and improves state-of-the-art performance in various fields.

Abstract: Applying Gaussian processes (GPs) to very large datasets remains a challenge
due to limited computational scalability. Matrix structures, such as the
Kronecker product, can accelerate operations significantly, but their
application commonly entails approximations or unrealistic assumptions. In
particular, the most common path to creating a Kronecker-structured kernel
matrix is by evaluating a product kernel on gridded inputs that can be
expressed as a Cartesian product. However, this structure is lost if any
observation is missing, breaking the Cartesian product structure, which
frequently occurs in real-world data such as time series. To address this
limitation, we propose leveraging latent Kronecker structure, by expressing the
kernel matrix of observed values as the projection of a latent Kronecker
product. In combination with iterative linear system solvers and pathwise
conditioning, our method facilitates inference of exact GPs while requiring
substantially fewer computational resources than standard iterative methods. We
demonstrate that our method outperforms state-of-the-art sparse and variational
GPs on real-world datasets with up to five million examples, including
robotics, automated machine learning, and climate applications.

</details>


### [478] [Near Optimal Non-asymptotic Sample Complexity of 1-Identification](https://arxiv.org/abs/2506.06978)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: This paper addresses the 1-identification problem in multi-armed bandit settings, proposing the Sequential-Exploration-Exploitation (SEE) algorithm with near-optimal results.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the open question of non-asymptotic analysis for the 1-identification problem in pure exploration for multi-armed bandits.

Method: The paper designs the Sequential-Exploration-Exploitation (SEE) algorithm and provides theoretical analysis from the non-asymptotic perspective, achieving near-optimal pulling complexity.

Result: The SEE algorithm achieved near-optimality, matching upper and lower bounds with a gap of at most a polynomial logarithmic factor. Numerical results indicate its effectiveness against benchmarks.

Conclusion: The SEE algorithm provides a significant advancement in non-asymptotic analysis for the 1-identification problem, offering both theoretical and numerical support for its effectiveness.

Abstract: Motivated by an open direction in existing literature, we study the
1-identification problem, a fundamental multi-armed bandit formulation on pure
exploration. The goal is to determine whether there exists an arm whose mean
reward is at least a known threshold $\mu_0$, or to output None if it believes
such an arm does not exist. The agent needs to guarantee its output is correct
with probability at least $1-\delta$. Degenne & Koolen 2019 has established the
asymptotically tight sample complexity for the 1-identification problem, but
they commented that the non-asymptotic analysis remains unclear. We design a
new algorithm Sequential-Exploration-Exploitation (SEE), and conduct
theoretical analysis from the non-asymptotic perspective. Novel to the
literature, we achieve near optimality, in the sense of matching upper and
lower bounds on the pulling complexity. The gap between the upper and lower
bounds is up to a polynomial logarithmic factor. The numerical result also
indicates the effectiveness of our algorithm, compared to existing benchmarks.

</details>


### [479] [Certified Unlearning for Neural Networks](https://arxiv.org/abs/2506.06985)
*Anastasia Koloskova,Youssef Allouah,Animesh Jha,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: The paper introduces a certified machine unlearning method using noisy fine-tuning, achieving strong unlearning guarantees without restrictive assumptions while outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Motivated by privacy concerns and laws like the "right to be forgotten," there is a need to effectively remove specific training data's influence from machine learning models.

Method: The proposed approach uses noisy fine-tuning on retained data, leveraging privacy amplification from stochastic post-processing. It is broadly applicable as it avoids relying on restrictive assumptions about loss functions.

Result: The method ensures provable unlearning guarantees and demonstrates superior empirical performance compared to existing methods.

Conclusion: This novel approach establishes a viable and effective means for certified unlearning, balancing trade-offs in accuracy and efficiency, with open-source code made available for reproducibility.

Abstract: We address the problem of machine unlearning, where the goal is to remove the
influence of specific training data from a model upon request, motivated by
privacy concerns and regulatory requirements such as the "right to be
forgotten." Unfortunately, existing methods rely on restrictive assumptions or
lack formal guarantees. To this end, we propose a novel method for certified
machine unlearning, leveraging the connection between unlearning and privacy
amplification by stochastic post-processing. Our method uses noisy fine-tuning
on the retain data, i.e., data that does not need to be removed, to ensure
provable unlearning guarantees. This approach requires no assumptions about the
underlying loss function, making it broadly applicable across diverse settings.
We analyze the theoretical trade-offs in efficiency and accuracy and
demonstrate empirically that our method not only achieves formal unlearning
guarantees but also performs effectively in practice, outperforming existing
baselines. Our code is available at
https://github.com/stair-lab/certified-unlearningneural-networks-icml-2025

</details>


### [480] [Towards Physics-informed Diffusion for Anomaly Detection in Trajectories](https://arxiv.org/abs/2506.06999)
*Arun Sharma,Mingzhou Yang,Majid Farhadloo,Subhankar Ghosh,Bharat Jayaprakash,Shashi Shekhar*

Main category: cs.LG

TL;DR: The paper proposes a physics-informed diffusion model to detect anomalous trajectories (e.g., those caused by GPS spoofing) with improved accuracy by integrating kinematic constraints.


<details>
  <summary>Details</summary>
Motivation: The study seeks to combat illegal activities such as unauthorized fishing and illicit oil transfers in international waters by detecting fake trajectories, a problem heightened by AI advances and lack of labeled data.

Method: The authors develop a physics-informed diffusion probabilistic model that considers kinematic constraints and prior physical knowledge, addressing limitations in current generative models for anomalous trajectory detection.

Result: Experimental results using real-world maritime and urban datasets show the proposed approach achieves higher prediction accuracy and lower estimation error rates compared to existing methods.

Conclusion: By integrating physical laws into trajectory analysis, the proposed model provides a more reliable framework for detecting anomalous trajectories, demonstrating societal relevance and practical applicability in combating illegal activities.

Abstract: Given trajectory data, a domain-specific study area, and a user-defined
threshold, we aim to find anomalous trajectories indicative of possible GPS
spoofing (e.g., fake trajectory). The problem is societally important to curb
illegal activities in international waters, such as unauthorized fishing and
illicit oil transfers. The problem is challenging due to advances in AI
generated in deep fakes generation (e.g., additive noise, fake trajectories)
and lack of adequate amount of labeled samples for ground-truth verification.
Recent literature shows promising results for anomalous trajectory detection
using generative models despite data sparsity. However, they do not consider
fine-scale spatiotemporal dependencies and prior physical knowledge, resulting
in higher false-positive rates. To address these limitations, we propose a
physics-informed diffusion model that integrates kinematic constraints to
identify trajectories that do not adhere to physical laws. Experimental results
on real-world datasets in the maritime and urban domains show that the proposed
framework results in higher prediction accuracy and lower estimation error rate
for anomaly detection and trajectory generation methods, respectively. Our
implementation is available at
https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.

</details>


### [481] [Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning](https://arxiv.org/abs/2506.07040)
*Yang Xu,Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: The paper introduces $Q$-learning and actor-critic algorithms for robust average reward Markov Decision Processes (MDPs), showcasing non-asymptotic convergence under specific uncertainty sets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in reinforcement learning when uncertainties exist in the environment, specifically under contamination, total variation (TV) distance, and Wasserstein distance uncertainties in average reward settings.

Method: The authors develop robust $Q$-learning and actor-critic algorithms by leveraging a strictly contractive mapping $Q$ Bellman operator with a semi-norm. They establish stochastic approximations for learning robust $Q$ functions and design a natural actor-critic approach based on robust policy mirror descent theories.

Result: The robust $Q$-learning algorithm achieves convergence to an optimal $Q$ function in $	ilde{\cO}(\epsilon^{-2})$ samples. The actor-critic algorithm attains an $
\epsilon$-optimal robust policy in $	ilde{\cO}(\epsilon^{-3})$ samples, enhancing the theoretical foundations of distributionally robust reinforcement learning.

Conclusion: The paper significantly contributes to the study of robust reinforcement learning, demonstrating effective and efficiently converging algorithms for MDPs in the presence of environmental uncertainties.

Abstract: We present the first $Q$-learning and actor-critic algorithms for robust
average reward Markov Decision Processes (MDPs) with non-asymptotic convergence
under contamination, TV distance and Wasserstein distance uncertainty sets. We
show that the robust $Q$ Bellman operator is a strict contractive mapping with
respect to a carefully constructed semi-norm with constant functions being
quotiented out. This property supports a stochastic approximation update, that
learns the optimal robust $Q$ function in $\tilde{\cO}(\epsilon^{-2})$ samples.
We also show that the same idea can be used for robust $Q$ function estimation,
which can be further used for critic estimation. Coupling it with theories in
robust policy mirror descent update, we present a natural actor-critic
algorithm that attains an $\epsilon$-optimal robust policy in
$\tilde{\cO}(\epsilon^{-3})$ samples. These results advance the theory of
distributionally robust reinforcement learning in the average reward setting.

</details>


### [482] [State Entropy Regularization for Robust Reinforcement Learning](https://arxiv.org/abs/2506.07085)
*Uri Koren,Yonatan Ashlag,Mirco Mutti,Esther Derman,Pierre-Luc Bacon,Shie Mannor*

Main category: cs.LG

TL;DR: This paper explores how state entropy regularization enhances robustness to structured and spatially correlated perturbations in reinforcement learning (RL), unlike policy entropy regularization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of theoretical guarantees for state entropy regularization and understand its effects on robustness, especially in environments involving structured or spatially correlated perturbations.

Method: The paper contrasts state entropy and policy entropy regularization, provides formal guarantees under uncertainties, and identifies scenarios where state entropy fails. They also evaluate sensitivity to rollout numbers in policy evaluation.

Result: State entropy regularization improves robustness to structured and spatially correlated perturbations in RL compared to policy entropy regularization, though its benefits depend on the number of rollouts used.

Conclusion: State entropy regularization offers advantages in certain robustness settings, but its effectiveness is context-dependent, particularly on rollout sensitivity.

Abstract: State entropy regularization has empirically shown better exploration and
sample complexity in reinforcement learning (RL). However, its theoretical
guarantees have not been studied. In this paper, we show that state entropy
regularization improves robustness to structured and spatially correlated
perturbations. These types of variation are common in transfer learning but
often overlooked by standard robust RL methods, which typically focus on small,
uncorrelated changes. We provide a comprehensive characterization of these
robustness properties, including formal guarantees under reward and transition
uncertainty, as well as settings where the method performs poorly. Much of our
analysis contrasts state entropy with the widely used policy entropy
regularization, highlighting their different benefits. Finally, from a
practical standpoint, we illustrate that compared with policy entropy, the
robustness advantages of state entropy are more sensitive to the number of
rollouts used for policy evaluation.

</details>


### [483] [Pointwise confidence estimation in the non-linear $\ell^2$-regularized least squares](https://arxiv.org/abs/2506.07088)
*Ilja Kuzborskij,Yasin Abbasi Yadkori*

Main category: cs.LG

TL;DR: The paper proposes a confidence estimation method in non-linear least-squares settings with fixed design, offering bounds that depend on test input similarity to training data.


<details>
  <summary>Details</summary>
Motivation: To address the need for reliable confidence estimation in non-linear models, emphasizing relevance to test inputs and avoiding issues with distant data points.

Method: A high-probability, non-asymptotic confidence bound is developed. It scales with test input similarity to training data via a weighted norm derived from the inverse-Hessian matrix. Efficient computation of this bound is introduced.

Result: The bound is more flexible in accounting for test input similarity and demonstrates better performance in trade-offs between coverage and width compared to bootstrapping.

Conclusion: The proposed approach provides an effective and computationally efficient alternative for confidence estimation in non-linear models, outperforming traditional methods like bootstrapping.

Abstract: We consider a high-probability non-asymptotic confidence estimation in the
$\ell^2$-regularized non-linear least-squares setting with fixed design. In
particular, we study confidence estimation for local minimizers of the
regularized training loss. We show a pointwise confidence bound, meaning that
it holds for the prediction on any given fixed test input $x$. Importantly, the
proposed confidence bound scales with similarity of the test input to the
training data in the implicit feature space of the predictor (for instance,
becoming very large when the test input lies far outside of the training data).
This desirable last feature is captured by the weighted norm involving the
inverse-Hessian matrix of the objective function, which is a generalized
version of its counterpart in the linear setting, $x^{\top} \text{Cov}^{-1} x$.
Our generalized result can be regarded as a non-asymptotic counterpart of the
classical confidence interval based on asymptotic normality of the MLE
estimator. We propose an efficient method for computing the weighted norm,
which only mildly exceeds the cost of a gradient computation of the loss
function. Finally, we complement our analysis with empirical evidence showing
that the proposed confidence bound provides better coverage/width trade-off
compared to a confidence estimation by bootstrapping, which is a gold-standard
method in many applications involving non-linear predictors such as neural
networks.

</details>


### [484] [CAtCh: Cognitive Assessment through Cookie Thief](https://arxiv.org/abs/2506.06603)
*Joseph T Colonel,Carolyn Hagler,Guiselle Wismer,Laura Curtis,Jacqueline Becker,Juan Wisnivesky,Alex Federman,Gaurav Pandey*

Main category: cs.LG

TL;DR: This paper compares various speech-based machine learning methods for predicting cognitive impairment, using audio recordings.


<details>
  <summary>Details</summary>
Motivation: While machine learning is established for predicting Alzheimer's and dementia from speech, adaptation to broader cognitive impairment was unexplored.

Method: Evaluation of both open-source tools for Alzheimer's prediction and multimodal sentiment analysis techniques on audio data for cognitive impairment prediction.

Result: Multimodal methods surpassed unimodal ones; acoustic features (affect, prosody) outperformed linguistic ones (BERT and other linguistic features).

Conclusion: Acoustic features from audio data show higher predictive power for cognitive impairment, emphasizing their value for developing interpretable tools.

Abstract: Several machine learning algorithms have been developed for the prediction of
Alzheimer's disease and related dementia (ADRD) from spontaneous speech.
However, none of these algorithms have been translated for the prediction of
broader cognitive impairment (CI), which in some cases is a precursor and risk
factor of ADRD. In this paper, we evaluated several speech-based open-source
methods originally proposed for the prediction of ADRD, as well as methods from
multimodal sentiment analysis for the task of predicting CI from patient audio
recordings. Results demonstrated that multimodal methods outperformed unimodal
ones for CI prediction, and that acoustics-based approaches performed better
than linguistics-based ones. Specifically, interpretable acoustic features
relating to affect and prosody were found to significantly outperform
BERT-based linguistic features and interpretable linguistic features,
respectively. All the code developed for this study is available at
https://github.com/JTColonel/catch.

</details>


### [485] [Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization](https://arxiv.org/abs/2506.06606)
*Xinyu Luo,Cedar Site Bai,Bolian Li,Petros Drineas,Ruqi Zhang,Brian Bullins*

Main category: cs.LG

TL;DR: The paper introduces Stacey, an accelerated \( \ell_p \) steepest descent algorithm to address non-Euclidean challenges in optimization tasks, achieving faster convergence and higher accuracy compared to popular methods.


<details>
  <summary>Details</summary>
Motivation: Modern deep network training often exhibits non-Euclidean structures that popular optimization methods fail to efficiently handle.

Method: The Stacey algorithm utilizes interpolated primal-dual iterate sequences to navigate non-Euclidean smooth optimization tasks, backed by theoretical guarantees.

Result: Empirical evaluations show Stacey outperforms SGD, AdamW, and Lion in terms of convergence speed and final accuracy across tasks like image classification and LLM pretraining.

Conclusion: Exploring non-Euclidean optimization approaches is crucial for deep network training, as demonstrated by Stacey's improved performance.

Abstract: While popular optimization methods such as SGD, AdamW, and Lion depend on
steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there
remains a critical gap in handling the non-Euclidean structure observed in
modern deep networks training. In this work, we address this need by
introducing a new accelerated $\ell_p$ steepest descent algorithm, called
Stacey, which uses interpolated primal-dual iterate sequences to effectively
navigate non-Euclidean smooth optimization tasks. In addition to providing
novel theoretical guarantees for the foundations of our algorithm, we
empirically compare our approach against these popular methods on tasks
including image classification and language model (LLM) pretraining,
demonstrating both faster convergence and higher final accuracy. We further
evaluate different values of $p$ across various models and datasets,
underscoring the importance and efficiency of non-Euclidean approaches over
standard Euclidean methods. Code can be found at
https://github.com/xinyuluo8561/Stacey .

</details>


### [486] [PASS: Private Attributes Protection with Stochastic Data Substitution](https://arxiv.org/abs/2506.07308)
*Yizhuo Chen,Chun-Fu,Chen,Hsiang Hsu,Shaohan Hu,Tarek Abdelzaher*

Main category: cs.LG

TL;DR: PASS proposes a stochastic substitution technique and a novel loss function to improve safeguarding of private attributes while preserving data utility in machine learning applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address privacy concerns regarding user data collected for machine learning services, focusing on techniques that can prevent exposure of private or irrelevant attributes.

Method: PASS involves stochastically substituting samples based on defined probabilities and utilizes a new loss function derived from an information-theoretic objective to ensure effective private attribute protection while maintaining utility.

Result: Experiments on various datasets, including images, sensor data, and voice records, demonstrate that PASS consistently ensures privacy protection while retaining data usability for downstream tasks.

Conclusion: The study concludes that PASS is effective and generalizable for privacy-preserving data processing across diverse domains and modalities.

Abstract: The growing Machine Learning (ML) services require extensive collections of
user data, which may inadvertently include people's private information
irrelevant to the services. Various studies have been proposed to protect
private attributes by removing them from the data while maintaining the
utilities of the data for downstream tasks. Nevertheless, as we theoretically
and empirically show in the paper, these methods reveal severe vulnerability
because of a common weakness rooted in their adversarial training based
strategies. To overcome this limitation, we propose a novel approach, PASS,
designed to stochastically substitute the original sample with another one
according to certain probabilities, which is trained with a novel loss function
soundly derived from information-theoretic objective defined for
utility-preserving private attributes protection. The comprehensive evaluation
of PASS on various datasets of different modalities, including facial images,
human activity sensory signals, and voice recording datasets, substantiates
PASS's effectiveness and generalizability.

</details>


### [487] [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
*Shubham Parashar,Shurui Gui,Xiner Li,Hongyi Ling,Sushil Vemuri,Blake Olson,Eric Li,Yu Zhang,James Caverlee,Dileep Kalathil,Shuiwang Ji*

Main category: cs.LG

TL;DR: This paper introduces E2H Reasoner, a method leveraging curriculum learning to improve reasoning skills in smaller language models via reinforcement learning by scheduling tasks from easy to hard (E2H).


<details>
  <summary>Details</summary>
Motivation: Enhance reasoning capabilities in language models, addressing challenges faced by prior RL-based approaches on complex tasks.

Method: Incorporates curriculum learning by gradually scheduling tasks from easy to hard during reinforcement learning training of language models.

Result: E2H Reasoner significantly improves reasoning abilities in smaller size language models (1.5B-3B) compared to using standard RL alone, with theoretical support on its efficiency.

Conclusion: The E2H approach highlights the importance of intelligent task scheduling in training, emphasizing curriculum stages to improve model reasoning performance effectively.

Abstract: We aim to improve the reasoning capabilities of language models via
reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1
have demonstrated reasoning abilities on mathematical and coding tasks.
However, prior studies suggest that using RL alone to improve reasoning on
inherently difficult tasks is less effective. Here, we draw inspiration from
curriculum learning and propose to schedule tasks from easy to hard (E2H),
allowing LLMs to build reasoning skills gradually. Our method is termed E2H
Reasoner. Empirically, we observe that, although easy tasks are important
initially, fading them out through appropriate scheduling is essential in
preventing overfitting. Theoretically, we establish convergence guarantees for
E2H Reasoner within an approximate policy iteration framework. We derive
finite-sample complexity bounds and show that when tasks are appropriately
decomposed and conditioned, learning through curriculum stages requires fewer
total samples than direct learning. Experiments across multiple domains show
that E2H Reasoner significantly improves the reasoning ability of small LLMs
(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,
highlighting the effectiveness of our method.

</details>


### [488] [Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization](https://arxiv.org/abs/2506.07378)
*Yuen Chen,Haozhe Si,Guojun Zhang,Han Zhao*

Main category: cs.LG

TL;DR: This paper introduces Closed-Form Moment Alignment (CMA), a computationally efficient algorithm for domain generalization (DG), and provides a unifying theory for moment alignment in DG.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of distribution shifts in real-world applications by improving models' generalizability to unseen target domains. Existing domain generalization methods based on gradient and Hessian alignment are computationally expensive and lack clear theoretical principles, motivating the need for a more efficient and principled approach.

Method: The authors develop the theory of moment alignment, grounded in the concept of transfer measure, and extend its application to domain generalization with multiple source domains. They propose a target error bound and demonstrate that domain-level derivative alignment improves transfer measure. Building on this theory, they introduce CMA, an algorithm that aligns gradients and Hessians in closed-form, avoiding computational inefficiencies of existing methods.

Result: The proposed CMA algorithm achieves superior performance in domain generalization tasks, validated through experiments involving both linear probing and full fine-tuning. It outperforms Empirical Risk Minimization and other state-of-the-art methods.

Conclusion: Moment alignment provides a comprehensive framework for understanding and improving domain generalization. CMA, as a practical implementation of this framework, demonstrates the potential to enhance computational efficiency and model performance in DG tasks.

Abstract: Domain generalization (DG) seeks to develop models that generalize well to
unseen target domains, addressing the prevalent issue of distribution shifts in
real-world applications. One line of research in DG focuses on aligning
domain-level gradients and Hessians to enhance generalization. However,
existing methods are computationally inefficient and the underlying principles
of these approaches are not well understood. In this paper, we develop the
theory of moment alignment for DG. Grounded in \textit{transfer measure}, a
principled framework for quantifying generalizability between two domains, we
first extend the definition of transfer measure to domain generalization that
includes multiple source domains and establish a target error bound. Then, we
prove that aligning derivatives across domains improves transfer measure both
when the feature extractor induces an invariant optimal predictor across
domains and when it does not. Notably, moment alignment provides a unifying
understanding of Invariant Risk Minimization, gradient matching, and Hessian
matching, three previously disconnected approaches to DG. We further connect
feature moments and derivatives of the classifier head, and establish the
duality between feature learning and classifier fitting. Building upon our
theory, we introduce \textbf{C}losed-Form \textbf{M}oment \textbf{A}lignment
(CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in
closed-form. Our method overcomes the computational inefficiencies of existing
gradient and Hessian-based techniques by eliminating the need for repeated
backpropagation or sampling-based Hessian estimation. We validate the efficacy
of our approach through two sets of experiments: linear probing and full
fine-tuning. CMA demonstrates superior performance in both settings compared to
Empirical Risk Minimization and state-of-the-art algorithms.

</details>


### [489] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/abs/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: This paper introduces Vision-QRWKV, a hybrid quantum-classical model leveraging Receptance Weighted Key Value (RWKV) for image classification, showing improved performance on complex datasets.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for enhanced neural network architectures capable of handling high-dimensional and complex visual data, leveraging advancements in quantum machine learning.

Method: The method involves integrating a Variational Quantum Circuit (VQC) into the channel mixing component of RWKV architecture, which is then applied to image classification tasks.

Result: The quantum-enhanced model outperformed its classical counterpart on a majority of image benchmarks, especially datasets with subtle or noisy distinctions like ChestMNIST and RetinaMNIST.

Conclusion: Quantum-enhanced RWKV is promising for lightweight, efficient vision tasks and offers insights into leveraging quantum neural networks for complex data modeling.

Abstract: Recent advancements in quantum machine learning have shown promise in
enhancing classical neural network architectures, particularly in domains
involving complex, high-dimensional data. Building upon prior work in temporal
sequence modeling, this paper introduces Vision-QRWKV, a hybrid
quantum-classical extension of the Receptance Weighted Key Value (RWKV)
architecture, applied for the first time to image classification tasks. By
integrating a variational quantum circuit (VQC) into the channel mixing
component of RWKV, our model aims to improve nonlinear feature transformation
and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of
14 medical and standard image classification benchmarks, including MedMNIST
datasets, MNIST, and FashionMNIST. Our results demonstrate that the
quantum-enhanced model outperforms its classical counterpart on a majority of
datasets, particularly those with subtle or noisy class distinctions (e.g.,
ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first
systematic application of quantum-enhanced RWKV in the visual domain, offering
insights into the architectural trade-offs and future potential of quantum
models for lightweight and efficient vision tasks.

</details>


### [490] [Explicit Preference Optimization: No Need for an Implicit Reward Model](https://arxiv.org/abs/2506.07492)
*Xiangkun Hu,Lemin Kong,Tong He,David Wipf*

Main category: cs.LG

TL;DR: The paper critiques the shortcomings of Direct Preference Optimization (DPO) in fine-tuning LLMs with human preferences and introduces an alternative, EXPO, for improved regularization and behavior.


<details>
  <summary>Details</summary>
Motivation: Challenges in reinforcement learning from human feedback (RLHF) spur research into alternatives like DPO, which aim to simplify preference optimization processes.

Method: The EXPO framework introduces explicit regularization, avoiding the reparameterization-based implicit reward mechanisms used in DPO.

Result: EXPO proves effective in addressing regularization weaknesses of DPO, both theoretically and empirically.

Conclusion: EXPO offers a robust alternative to DPO by addressing its shortcomings and successfully meeting regularization objectives.

Abstract: The generated responses of large language models (LLMs) are often fine-tuned
to human preferences through a process called reinforcement learning from human
feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a
separate reward model is independently learned and then later applied to LLM
policy updates, ongoing research effort has targeted more straightforward
alternatives. In this regard, direct preference optimization (DPO) and its many
offshoots circumvent the need for a separate reward training step. Instead,
through the judicious use of a reparameterization trick that induces an
\textit{implicit} reward, DPO and related methods consolidate learning to the
minimization of a single loss function. And yet despite demonstrable success in
some real-world settings, we prove that DPO-based objectives are nonetheless
subject to sub-optimal regularization and counter-intuitive interpolation
behaviors, underappreciated artifacts of the reparameterizations upon which
they are based. To this end, we introduce an \textit{explicit} preference
optimization framework termed EXPO that requires no analogous
reparameterization to achieve an implicit reward. Quite differently, we merely
posit intuitively-appealing regularization factors from scratch that
transparently avoid the potential pitfalls of key DPO variants, provably
satisfying regularization desiderata that prior methods do not. Empirical
results serve to corroborate our analyses and showcase the efficacy of EXPO.

</details>


### [491] [Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning](https://arxiv.org/abs/2506.06637)
*Olimjon Toirov,Wei Yu*

Main category: cs.LG

TL;DR: A novel NILM approach integrates 'image load signatures' and continual learning to improve device recognition and classification.


<details>
  <summary>Details</summary>
Motivation: Traditional NILM methods face challenges due to poor feature robustness and insufficient generalization caused by complex load combinations and varying environments.

Method: The method converts multi-dimensional power signals into visual image signatures, employs deep CNNs for device classification, uses self-supervised pre-training to enhance feature generalization, and applies continual learning to mitigate model forgetting.

Result: Experiments on high-sampling-rate datasets showed significantly improved recognition accuracy compared to existing methods and model variants.

Conclusion: The proposed NILM method demonstrates superior performance in device recognition and adaptability through innovative techniques.

Abstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and
energy consumption of each electrical device in the circuit by analyzing the
electrical signals at the bus, which is of great significance for smart power
management. However, the complex and changeable load combinations and
application environments lead to the challenges of poor feature robustness and
insufficient model generalization of traditional NILM methods. To this end,
this paper proposes a new non-intrusive load monitoring method that integrates
"image load signature" and continual learning. This method converts
multi-dimensional power signals such as current, voltage, and power factor into
visual image load feature signatures, and combines deep convolutional neural
networks to realize the identification and classification of multiple devices;
at the same time, self-supervised pre-training is introduced to improve feature
generalization, and continual online learning strategies are used to overcome
model forgetting to adapt to the emergence of new loads. This paper conducts a
large number of experiments on high-sampling rate load datasets, and compares a
variety of existing methods and model variants. The results show that the
proposed method has achieved significant improvements in recognition accuracy.

</details>


### [492] [Flowing Datasets with Wasserstein over Wasserstein Gradient Flows](https://arxiv.org/abs/2506.07534)
*Clément Bonet,Christophe Vauthier,Anna Korba*

Main category: cs.LG

TL;DR: This paper proposes a method for representing labeled datasets as probability distributions over probability distributions, introducing a novel gradient flow approach using the Wasserstein over Wasserstein (WoW) distance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of designing gradient flows for infinite-dimensional objects like probability distributions, with applications in machine learning domains such as transfer learning and dataset distillation.

Method: The authors represent datasets as mixture distributions of conditional distributions for each class, introduce the WoW distance for metric structure, derive a differential structure, and define WoW gradient flows.

Result: The proposed framework provides a method for optimizing objective functionals using WoW gradient flows and includes novel functional design based on Maximum Mean Discrepancies with Sliced-Wasserstein kernels.

Conclusion: The paper offers a novel tool for handling probability distributions over distributions, enabling applications like transfer learning and dataset distillation using optimal transport techniques.

Abstract: Many applications in machine learning involve data represented as probability
distributions. The emergence of such data requires radically novel techniques
to design tractable gradient flows on probability distributions over this type
of (infinite-dimensional) objects. For instance, being able to flow labeled
datasets is a core task for applications ranging from domain adaptation to
transfer learning or dataset distillation. In this setting, we propose to
represent each class by the associated conditional distribution of features,
and to model the dataset as a mixture distribution supported on these classes
(which are themselves probability distributions), meaning that labeled datasets
can be seen as probability distributions over probability distributions. We
endow this space with a metric structure from optimal transport, namely the
Wasserstein over Wasserstein (WoW) distance, derive a differential structure on
this space, and define WoW gradient flows. The latter enables to design
dynamics over this space that decrease a given objective functional. We apply
our framework to transfer learning and dataset distillation tasks, leveraging
our gradient flow construction as well as novel tractable functionals that take
the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels
between probability distributions.

</details>


### [493] [Exploiting Curvature in Online Convex Optimization with Delayed Feedback](https://arxiv.org/abs/2506.07595)
*Hao Qiu,Emmanuel Esposito,Mengxiao Zhang*

Main category: cs.LG

TL;DR: The paper addresses online convex optimization with curved losses and delayed feedback, improving regret bounds for strongly convex and exp-concave losses and demonstrating better performance experimentally.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve existing regret bounds for delayed feedback in online convex optimization, making them more efficient under varying delay conditions.

Method: Researchers propose a new variant of follow-the-regularized-leader (FTRL), extend the Online Newton Step algorithm with adaptive learning for exp-concave losses, and design a modified Vovk-Azoury-Warmuth forecaster using a clipping trick for linear regression.

Result: The proposed algorithms achieve improved regret bounds of $\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$ for strongly convex losses and $\min\{d_{\max}n\ln T, \sqrt{d_{\mathrm{tot}}}\}$ for exp-concave losses, with verified superior experimental performance.

Conclusion: This work bridges gaps in existing methods for delayed feedback and introduces algorithms with better theoretical and empirical guarantees, advancing the state-of-the-art in online convex optimization.

Abstract: In this work, we study the online convex optimization problem with curved
losses and delayed feedback. When losses are strongly convex, existing
approaches obtain regret bounds of order $d_{\max} \ln T$, where $d_{\max}$ is
the maximum delay and $T$ is the time horizon. However, in many cases, this
guarantee can be much worse than $\sqrt{d_{\mathrm{tot}}}$ as obtained by a
delayed version of online gradient descent, where $d_{\mathrm{tot}}$ is the
total delay. We bridge this gap by proposing a variant of
follow-the-regularized-leader that obtains regret of order
$\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$, where $\sigma_{\max}$ is
the maximum number of missing observations. We then consider exp-concave losses
and extend the Online Newton Step algorithm to handle delays with an adaptive
learning rate tuning, achieving regret $\min\{d_{\max} n\ln T,
\sqrt{d_{\mathrm{tot}}}\}$ where $n$ is the dimension. To our knowledge, this
is the first algorithm to achieve such a regret bound for exp-concave losses.
We further consider the problem of unconstrained online linear regression and
achieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth
forecaster with a clipping trick. Finally, we implement our algorithms and
conduct experiments under various types of delay and losses, showing an
improved performance over existing methods.

</details>


### [494] [The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well](https://arxiv.org/abs/2506.07661)
*Meir Feder,Ruediger Urbanke,Yaniv Fogel*

Main category: cs.LG

TL;DR: This paper investigates why large, over-parameterized models generalize well using an information-theoretic and Bayesian approach, focusing on the "weight of the hypothesis" based on Kullback-Leibler divergence.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the paradox of large models generalizing effectively despite having more parameters than training samples, using principles from information theory and universal learning.

Method: The study uses a Bayesian mixture learner with log-loss and an almost uniform prior over a wide hypothesis class, analyzing regret and introducing the concept of "model weight" to explain generalization.

Result: It identifies that generalization depends on the cumulative probability of hypotheses close to the true data distribution, and explains the role of simple models and posterior concentration in avoiding overfitting.

Conclusion: The findings provide a theoretical foundation for why over-parameterized models generalize well, connect theory with practices like SGD and ensemble methods, and offer a unified understanding of modern AI systems across learning paradigms.

Abstract: A fundamental question in modern machine learning is why large,
over-parameterized models, such as deep neural networks and transformers, tend
to generalize well, even when their number of parameters far exceeds the number
of training samples.
  We investigate this phenomenon through the lens of information theory,
grounded in universal learning theory. Specifically, we study a Bayesian
mixture learner with log-loss and (almost) uniform prior over an expansive
hypothesis class.
  Our key result shows that the learner's regret is not determined by the
overall size of the hypothesis class, but rather by the cumulative probability
of all models that are close, in Kullback-Leibler divergence distance, to the
true data-generating process. We refer to this cumulative probability as the
weight of the hypothesis.
  This leads to a natural notion of model simplicity: simple models are those
with large weight and thus require fewer samples to generalize, while complex
models have small weight and need more data. This perspective provides a
rigorous and intuitive explanation for why over-parameterized models often
avoid overfitting: the presence of simple hypotheses allows the posterior to
concentrate on them when supported by the data.
  We further bridge theory and practice by recalling that stochastic gradient
descent with Langevin dynamics samples from the correct posterior distribution,
enabling our theoretical learner to be approximated using standard machine
learning methods combined with ensemble learning.
  Our analysis yields non-uniform regret bounds and aligns with key practical
concepts such as flat minima and model distillation. The results apply broadly
across online, batch, and supervised learning settings, offering a unified and
principled understanding of the generalization behavior of modern AI systems.

</details>


### [495] [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
*Adam Breuer*

Main category: cs.LG

TL;DR: First provable and practical algorithms for inferring document topics in LDA models, exponentially faster and providing interpretability guarantees.


<details>
  <summary>Details</summary>
Motivation: The need for effective algorithms for the inference of document topics in LDA models, addressing applications across social sciences, data exploration, and causal inference.

Method: Introduced a novel non-gradient-based combinatorial approach for topic estimation in LDA models with logarithmic computation time.

Result: The approach consistently delivers higher semantic quality solutions compared to state-of-the-art alternatives and maintains interpretability and independence for causal inference.

Conclusion: The proposed method is efficient, interpretable, and suitable for downstream applications requiring independence, outperforming existing LDA methods.

Abstract: In this paper, we provide the first practical algorithms with provable
guarantees for the problem of inferring the topics assigned to each document in
an LDA topic model. This is the primary inference problem for many applications
of topic models in social science, data exploration, and causal inference
settings. We obtain this result by showing a novel non-gradient-based,
combinatorial approach to estimating topic models. This yields algorithms that
converge to near-optimal posterior probability in logarithmic parallel
computation time (adaptivity) -- exponentially faster than any known LDA
algorithm. We also show that our approach can provide interpretability
guarantees such that each learned topic is formally associated with a known
keyword. Finally, we show that unlike alternatives, our approach can maintain
the independence assumptions necessary to use the learned topic model for
downstream causal inference methods that allow researchers to study topics as
treatments. In terms of practical performance, our approach consistently
returns solutions of higher semantic quality than solutions from
state-of-the-art LDA algorithms, neural topic models, and LLM-based topic
models across a diverse range of text datasets and evaluation parameters.

</details>


### [496] [SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming](https://arxiv.org/abs/2506.06665)
*Hong-Ming Chiu,Hao Chen,Huan Zhang,Richard Y. Zhang*

Main category: cs.LG

TL;DR: The paper introduces SDP-CROWN, a hybrid neural network verification framework that merges the scalability of linear bound-propagation methods with the tightness of semidefinite programming (SDP) relaxations, improving both accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Current verifiers face trade-offs: linear bound propagation methods are scalable but loose, while SDP verifiers are precise but computationally expensive. The authors aim to integrate the benefits of both approaches.

Method: The proposed method introduces a novel SDP-inspired linear bound that captures inter-neuron coupling based on the $\ell_2$-norm. This bound is integrated into linear bound-propagation frameworks, adding minimal complexity while enhancing tightness.

Result: SDP-CROWN achieves significantly tighter bounds and improved verification performance on large-scale models, surpassing traditional per-neuron bounds and closely approaching the tightness of full SDP methods.

Conclusion: SDP-CROWN successfully balances scalability and precision, making it a practical and superior alternative for verifying large neural networks under computational constraints.

Abstract: Neural network verifiers based on linear bound propagation scale impressively
to massive models but can be surprisingly loose when neuron coupling is
crucial. Conversely, semidefinite programming (SDP) verifiers capture
inter-neuron coupling naturally, but their cubic complexity restricts them to
only small models. In this paper, we propose SDP-CROWN, a novel hybrid
verification framework that combines the tightness of SDP relaxations with the
scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new
linear bound, derived via SDP principles, that explicitly captures
$\ell_{2}$-norm-based inter-neuron coupling while adding only one extra
parameter per layer. This bound can be integrated seamlessly into any linear
bound-propagation pipeline, preserving the inherent scalability of such methods
yet significantly improving tightness. In theory, we prove that our
inter-neuron bound can be up to a factor of $\sqrt{n}$ tighter than traditional
per-neuron bounds. In practice, when incorporated into the state-of-the-art
$\alpha$-CROWN verifier, we observe markedly improved verification performance
on large models with up to 65 thousand neurons and 2.47 million parameters,
achieving tightness that approaches that of costly SDP-based methods.

</details>


### [497] [Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability](https://arxiv.org/abs/2506.07804)
*Jie Bao,Chuangyin Dang,Rui Luo,Hanwei Zhang,Zhixin Zhou*

Main category: cs.LG

TL;DR: The study introduces OPSA, an adversarial attack method leveraging conformal prediction principles to increase model uncertainty and reduce efficiency, along with OPSA-AT, a robust adversarial training strategy.


<details>
  <summary>Details</summary>
Motivation: Deep learning lacks robust uncertainty estimates and security against adversarial attacks, especially in safety-critical applications, necessitating trustworthy models.

Method: OPSA optimizes adversarial attacks by exploiting uncertain predictions via conformal methods, while OPSA-AT integrates OPSA into a new conformal training approach to fortify defenses.

Result: OPSA successfully escalates model uncertainty compared to existing methods, while OPSA-AT strengthens robustness across multiple adversarial attack types, including OPSA.

Conclusion: Integrating conformal prediction principles in adversarial training improves uncertainty management and resilience, making deep learning models safer for critical applications.

Abstract: As deep learning models are increasingly deployed in high-risk applications,
robust defenses against adversarial attacks and reliable performance guarantees
become paramount. Moreover, accuracy alone does not provide sufficient
assurance or reliable uncertainty estimates for these models. This study
advances adversarial training by leveraging principles from Conformal
Prediction. Specifically, we develop an adversarial attack method, termed OPSA
(OPtimal Size Attack), designed to reduce the efficiency of conformal
prediction at any significance level by maximizing model uncertainty without
requiring coverage guarantees. Correspondingly, we introduce OPSA-AT
(Adversarial Training), a defense strategy that integrates OPSA within a novel
conformal training paradigm. Experimental evaluations demonstrate that our OPSA
attack method induces greater uncertainty compared to baseline approaches for
various defenses. Conversely, our OPSA-AT defensive model significantly
enhances robustness not only against OPSA but also other adversarial attacks,
and maintains reliable prediction. Our findings highlight the effectiveness of
this integrated approach for developing trustworthy and resilient deep learning
models for safety-critical domains. Our code is available at
https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.

</details>


### [498] [Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics](https://arxiv.org/abs/2506.06682)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: This paper presents HetCRF, a self-supervised framework for heterogeneous graphs that combines the strengths of MAE and CL paradigms, addressing challenges in semantic sparsity and gradient imbalance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of masked autoencoders (MAE) and contrastive learning (CL) in graph self-supervised learning, especially in heterogeneous and semantically sparse scenarios.

Method: HetCRF introduces a dual-channel design with a two-stage aggregation process to balance semantic needs of MAE and CL. It also includes improved embedding-based view construction and two positive sample augmentation strategies for gradient consistency.

Result: HetCRF shows state-of-the-art performance on four heterogeneous graph datasets, achieving significant gains in Macro-F1 scores under scenarios with missing node features.

Conclusion: HetCRF effectively combines MAE and CL, overcoming semantic sparsity and gradient imbalance challenges in heterogeneous graphs, and demonstrates superior performance in node classification tasks.

Abstract: In graph self-supervised learning, masked autoencoders (MAE) and contrastive
learning (CL) are two prominent paradigms. MAE focuses on reconstructing masked
elements, while CL maximizes similarity between augmented graph views. Recent
studies highlight their complementarity: MAE excels at local feature capture,
and CL at global information extraction. Hybrid frameworks for homogeneous
graphs have been proposed, but face challenges in designing shared encoders to
meet the semantic requirements of both tasks. In semantically sparse scenarios,
CL struggles with view construction, and gradient imbalance between positive
and negative samples persists. This paper introduces HetCRF, a novel
dual-channel self-supervised learning framework for heterogeneous graphs.
HetCRF uses a two-stage aggregation strategy to adapt embedding semantics,
making it compatible with both MAE and CL. To address semantic sparsity, it
enhances encoder output for view construction instead of relying on raw
features, improving efficiency. Two positive sample augmentation strategies are
also proposed to balance gradient contributions. Node classification
experiments on four real-world heterogeneous graph datasets demonstrate that
HetCRF outperforms state-of-the-art baselines. On datasets with missing node
features, such as Aminer and Freebase, at a 40% label rate in node
classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%
respectively compared to the second-best baseline, validating its effectiveness
and superiority.

</details>


### [499] [Residual Reweighted Conformal Prediction for Graph Neural Networks](https://arxiv.org/abs/2506.07854)
*Zheng Zhang,Jie Bao,Zhixin Zhou,Nicolo Colombo,Lixin Cheng,Rui Luo*

Main category: cs.LG

TL;DR: The paper introduces RR-GNN, a framework for Graph Neural Networks that uses advanced conformal prediction methods to produce accurate prediction sets while accounting for graph uncertainties and structural biases.


<details>
  <summary>Details</summary>
Motivation: Existing conformal prediction methods for GNNs are overly conservative and fail to capture graph heteroscedasticity, topology, and structural biases. The need arises for a solution that ensures statistical guarantees while improving efficiency and accuracy.

Method: RR-GNN employs three innovations: (1) Graph-Structured Mondrian CP for cluster-conditional coverage, (2) Residual-Adaptive Nonconformity Scores using a secondary GNN for dynamic adjustment, and (3) a Cross-Training Protocol to avoid data leakage while preserving graph dependencies.

Result: RR-GNN was tested on 15 diverse real-world graphs and showed superior efficiency over CP baselines without compromising coverage guarantees, across tasks like node classification, regression, and edge prediction.

Conclusion: RR-GNN proves to be an effective solution for improving prediction efficiency and accuracy, making it a robust tool for high-stakes applications in graph-related tasks.

Abstract: Graph Neural Networks (GNNs) excel at modeling relational data but face
significant challenges in high-stakes domains due to unquantified uncertainty.
Conformal prediction (CP) offers statistical coverage guarantees, but existing
methods often produce overly conservative prediction intervals that fail to
account for graph heteroscedasticity and structural biases. While residual
reweighting CP variants address some of these limitations, they neglect graph
topology, cluster-specific uncertainties, and risk data leakage by reusing
training sets. To address these issues, we propose Residual Reweighted GNN
(RR-GNN), a framework designed to generate minimal prediction sets with
provable marginal coverage guarantees.
  RR-GNN introduces three major innovations to enhance prediction performance.
First, it employs Graph-Structured Mondrian CP to partition nodes or edges into
communities based on topological features, ensuring cluster-conditional
coverage that reflects heterogeneity. Second, it uses Residual-Adaptive
Nonconformity Scores by training a secondary GNN on a held-out calibration set
to estimate task-specific residuals, dynamically adjusting prediction intervals
according to node or edge uncertainty. Third, it adopts a Cross-Training
Protocol, which alternates the optimization of the primary GNN and the residual
predictor to prevent information leakage while maintaining graph dependencies.
We validate RR-GNN on 15 real-world graphs across diverse tasks, including node
classification, regression, and edge weight prediction. Compared to CP
baselines, RR-GNN achieves improved efficiency over state-of-the-art methods,
with no loss of coverage.

</details>


### [500] [Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning](https://arxiv.org/abs/2506.06694)
*Yuan Yuan,Yukun Liu,Chonghua Han,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: MoveGCL proposes a privacy-preserving framework leveraging generative continual learning to train mobility foundation models without sharing raw data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in creating human mobility foundation models due to privacy-sensitive data and institutional data silos.

Method: MoveGCL uses generative continual learning with synthetic trajectory replay via a frozen teacher model, tailored knowledge distillation, a Mixture-of-Experts Transformer, and a progressive adaptation strategy.

Result: MoveGCL exhibits performance on par with joint training and surpasses federated learning baselines in experiments on six urban datasets while maintaining strong privacy protection.

Conclusion: The framework paves the way for scalable and privacy-aware development of mobility foundation models, advancing research in this domain.

Abstract: Foundation models have revolutionized fields such as natural language
processing and computer vision by enabling general-purpose learning across
diverse tasks and datasets. However, building analogous models for human
mobility remains challenging due to the privacy-sensitive nature of mobility
data and the resulting data silos across institutions. To bridge this gap, we
propose MoveGCL, a scalable and privacy-preserving framework for training
mobility foundation models via generative continual learning. Without sharing
raw data, MoveGCL enables decentralized and progressive model evolution by
replaying synthetic trajectories generated from a frozen teacher model, and
reinforces knowledge retention through a tailored distillation strategy that
mitigates catastrophic forgetting. To address the heterogeneity of mobility
patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a
mobility-aware expert routing mechanism, and employs a layer-wise progressive
adaptation strategy to stabilize continual updates. Experiments on six
real-world urban datasets demonstrate that MoveGCL achieves performance
comparable to joint training and significantly outperforms federated learning
baselines, while offering strong privacy protection. MoveGCL marks a crucial
step toward unlocking foundation models for mobility, offering a practical
blueprint for open, scalable, and privacy-preserving model development in the
era of foundation models.

</details>


### [501] [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
*Rajeev Bhatt Ambati,James Lester,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.LG

TL;DR: MarginSel improves the effectiveness of Large Language Models in in-context learning by strategically selecting demonstration examples, achieving significant gains in classification tasks.


<details>
  <summary>Details</summary>
Motivation: In-context learning effectiveness in LLMs is sensitive to demonstration selection, motivating an approach to make this more robust.

Method: MarginSel utilizes a two-step process to select hard examples for the in-context learning prompt, tailoring selections for each test instance.

Result: Demonstrated 2-7% absolute improvement in F1-score across classification tasks compared to random example selection.

Conclusion: MarginSel enhances LLM performance by inducing max-margin behavior, providing both theoretical insights and practical improvements.

Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context
learning (ICL). However, the effectiveness of ICL is often sensitive to the
selection and ordering of demonstration examples. To address this, we present
MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that
selects hard demonstration examples for the ICL prompt, adapting to each test
instance. Our approach achieves 2-7% absolute improvement in F1-score across
classification tasks, compared to a random selection of examples. We also
provide theoretical insights and empirical evidence showing that MarginSel
induces max-margin behavior in LLMs by effectively increasing the margin for
hard examples, analogous to support vectors, thereby shifting the decision
boundary in a beneficial direction.

</details>


### [502] [Diffusion Counterfactual Generation with Semantic Abduction](https://arxiv.org/abs/2506.07883)
*Rajat Rasal,Avinash Kori,Fabio De Sousa Ribeiro,Tian Xia,Ben Glocker*

Main category: cs.LG

TL;DR: This paper proposes a new framework combining diffusion models and Pearlian causality to generate counterfactual images while preserving semantic identity and enabling causal reasoning.


<details>
  <summary>Details</summary>
Motivation: Improve counterfactual image generation by addressing limitations like identity preservation, perceptual quality, and adherence to causal models using diffusion models.

Method: Introduces diffusion-based causal mechanisms (spatial, semantic, and dynamic abduction) and integrates semantic representations with diffusion models under Pearlian causality principles.

Result: Achieves high-level semantic identity preservation and provides principled trade-offs for causal control and identity retention in counterfactual image editing.

Conclusion: Demonstrates that semantic integration with diffusion models improves counterfactual reasoning, offering scalable and reliable editing with better fidelity and causal faithfulness.

Abstract: Counterfactual image generation presents significant challenges, including
preserving identity, maintaining perceptual quality, and ensuring faithfulness
to an underlying causal model. While existing auto-encoding frameworks admit
semantic latent spaces which can be manipulated for causal control, they
struggle with scalability and fidelity. Advancements in diffusion models
present opportunities for improving counterfactual image editing, having
demonstrated state-of-the-art visual quality, human-aligned perception and
representation learning capabilities. Here, we present a suite of
diffusion-based causal mechanisms, introducing the notions of spatial, semantic
and dynamic abduction. We propose a general framework that integrates semantic
representations into diffusion models through the lens of Pearlian causality to
edit images via a counterfactual reasoning process. To our knowledge, this is
the first work to consider high-level semantic identity preservation for
diffusion counterfactuals and to demonstrate how semantic control enables
principled trade-offs between faithful causal control and identity
preservation.

</details>


### [503] [Do Protein Transformers Have Biological Intelligence?](https://arxiv.org/abs/2506.06701)
*Fudong Lin,Wanrou Du,Jinchan Liu,Tarikul Milon,Shelby Meche,Wu Xu,Xiaoqi Qin,Xu Yuan*

Main category: cs.LG

TL;DR: The paper proposes a new Transformer model called SPT for protein function prediction and introduces an explanaible AI technique (Sequence Score), achieving high accuracy in protein-related datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore whether Protein Transformers can capture and interpret biological intelligence among protein sequences, thereby improving protein function prediction accuracy and explainability.

Method: The paper introduces a protein function dataset called Protein-FN, designs a new Transformer architecture (SPT) for efficient predictions, and develops a novel Explainable AI technique (Sequence Score) to interpret model decisions.

Result: The proposed SPT-Tiny model achieves 94.3% accuracy on the Antibiotic Resistance dataset and 99.6% on the Protein-FN dataset while uncovering biologically meaningful patterns.

Conclusion: SPT models are computationally efficient and accurate, and the Sequence Score technique provides insights into protein sequence structures in alignment with biological knowledge.

Abstract: Deep neural networks, particularly Transformers, have been widely adopted for
predicting the functional properties of proteins. In this work, we focus on
exploring whether Protein Transformers can capture biological intelligence
among protein sequences. To achieve our goal, we first introduce a protein
function dataset, namely Protein-FN, providing over 9000 protein data with
meaningful labels. Second, we devise a new Transformer architecture, namely
Sequence Protein Transformers (SPT), for computationally efficient protein
function predictions. Third, we develop a novel Explainable Artificial
Intelligence (XAI) technique called Sequence Score, which can efficiently
interpret the decision-making processes of protein models, thereby overcoming
the difficulty of deciphering biological intelligence bided in Protein
Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only
5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%
on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,
all accomplished by training from scratch. Besides, our Sequence Score
technique helps reveal that our SPT models can discover several meaningful
patterns underlying the sequence structures of protein data, with these
patterns aligning closely with the domain knowledge in the biology community.
We have officially released our Protein-FN dataset on Hugging Face Datasets
https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at
https://github.com/fudong03/BioIntelligence.

</details>


### [504] [Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression](https://arxiv.org/abs/2506.06954)
*Clinton Enwerem,Aniruddh G. Puranic,John S. Baras,Calin Belta*

Main category: cs.LG

TL;DR: This paper introduces a risk-regularized quantile-based RL algorithm that enforces safety via Conditional Value-at-Risk (CVaR) without requiring complex architectures. The approach improves safety and performance in dynamic environments, evidenced by simulations.


<details>
  <summary>Details</summary>
Motivation: Mainstream RL algorithms face overestimation bias and challenges in satisfying safety constraints, especially in stochastic environments.

Method: The authors propose integrating CVaR into a quantile-based RL framework for safety enforcement and provide theoretical guarantees for operator convergence in Wasserstein space.

Result: Simulations with a mobile robot in reach-avoid tasks demonstrated higher success rates, fewer collisions, and better safety-performance trade-offs compared to risk-neutral methods.

Conclusion: The proposed algorithm effectively balances safety and performance, addressing limitations of existing RL methods without relying on complex architectures.

Abstract: Mainstream approximate action-value iteration reinforcement learning (RL)
algorithms suffer from overestimation bias, leading to suboptimal policies in
high-variance stochastic environments. Quantile-based action-value iteration
methods reduce this bias by learning a distribution of the expected cost-to-go
using quantile regression. However, ensuring that the learned policy satisfies
safety constraints remains a challenge when these constraints are not
explicitly integrated into the RL framework. Existing methods often require
complex neural architectures or manual tradeoffs due to combined cost
functions. To address this, we propose a risk-regularized quantile-based
algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety
without complex architectures. We also provide theoretical guarantees on the
contraction properties of the risk-sensitive distributional Bellman operator in
Wasserstein space, ensuring convergence to a unique cost distribution.
Simulations of a mobile robot in a dynamic reach-avoid task show that our
approach leads to more goal successes, fewer collisions, and better
safety-performance trade-offs compared to risk-neutral methods.

</details>


### [505] [FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling](https://arxiv.org/abs/2506.07902)
*Sifan Wang,Zehao Dou,Tong-Rui Liu,Lu Lu*

Main category: cs.LG

TL;DR: The paper introduces FunDiff, a framework that adapts generative models like diffusion processes to continuous function spaces in physics, achieving physically consistent outputs and robustness to low-quality data.


<details>
  <summary>Details</summary>
Motivation: Generative models have excelled in discrete data but struggle to represent continuous functions governed by physical laws. The paper aims to bridge this gap.

Method: FunDiff utilizes a latent diffusion process alongside a function autoencoder architecture, incorporates physical priors, and employs minimax optimality in density estimation for function spaces.

Result: FunDiff generates high-fidelity samples that align with physical laws, perform well in fluid dynamics and solid mechanics, and are robust against noisy and low-resolution inputs.

Conclusion: FunDiff demonstrates theoretical soundness and practical effectiveness by adapting generative modeling to continuous physical systems, offering advances in scientific applications.

Abstract: Recent advances in generative modeling -- particularly diffusion models and
flow matching -- have achieved remarkable success in synthesizing discrete data
such as images and videos. However, adapting these models to physical
applications remains challenging, as the quantities of interest are continuous
functions governed by complex physical laws. Here, we introduce
$\textbf{FunDiff}$, a novel framework for generative modeling in function
spaces. FunDiff combines a latent diffusion process with a function autoencoder
architecture to handle input functions with varying discretizations, generate
continuous functions evaluable at arbitrary locations, and seamlessly
incorporate physical priors. These priors are enforced through architectural
constraints or physics-informed loss functions, ensuring that generated samples
satisfy fundamental physical laws. We theoretically establish minimax
optimality guarantees for density estimation in function spaces, showing that
diffusion-based estimators achieve optimal convergence rates under suitable
regularity conditions. We demonstrate the practical effectiveness of FunDiff
across diverse applications in fluid dynamics and solid mechanics. Empirical
results show that our method generates physically consistent samples with high
fidelity to the target distribution and exhibits robustness to noisy and
low-resolution data. Code and datasets are publicly available at
https://github.com/sifanexisted/fundiff.

</details>


### [506] [CausalPFN: Amortized Causal Effect Estimation via In-Context Learning](https://arxiv.org/abs/2506.07918)
*Vahid Balazadeh,Hamidreza Kamkari,Valentin Thomas,Benson Li,Junwei Ma,Jesse C. Cresswell,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: The paper introduces CausalPFN, a pre-trained transformer that automates causal effect estimation from observational data.


<details>
  <summary>Details</summary>
Motivation: Selecting an appropriate causal effect estimator in observational data analysis is tedious and requires high domain expertise, hindering efficiency.

Method: CausalPFN uses a pre-trained transformer model, trained on simulated data assuming ignorability, to infer causal effects without the need for task-specific adjustments.

Result: CausalPFN achieved superior performance in benchmarks like IHDP, Lalonde, ACIC, and competitive results in real-world uplift modeling tasks.

Conclusion: CausalPFN simplifies causal inference, provides reliable uncertainty estimates, and eliminates the need for additional training or tuning, contributing significantly to automated causal analysis.

Abstract: Causal effect estimation from observational data is fundamental across
various applications. However, selecting an appropriate estimator from dozens
of specialized methods demands substantial manual effort and domain expertise.
We present CausalPFN, a single transformer that amortizes this workflow:
trained once on a large library of simulated data-generating processes that
satisfy ignorability, it infers causal effects for new observational datasets
out-of-the-box. CausalPFN combines ideas from Bayesian causal inference with
the large-scale training protocol of prior-fitted networks (PFNs), learning to
map raw observations directly to causal effects without any task-specific
adjustment. Our approach achieves superior average performance on heterogeneous
and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).
Moreover, it shows competitive performance for real-world policy making on
uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to
support reliable decision-making based on Bayesian principles. This
ready-to-use model does not require any further training or tuning and takes a
step toward automated causal inference (https://github.com/vdblm/CausalPFN).

</details>


### [507] [The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing](https://arxiv.org/abs/2506.06761)
*Adrià Molina Rodríguez,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.LG

TL;DR: The paper focuses on improving recognition systems using model editing to generalize better to low-resource languages and unseen scripts.


<details>
  <summary>Details</summary>
Motivation: Current recognition systems struggle with under-represented languages and scripts due to limited data availability and inadequate pretraining methods.

Method: The authors leverage advancements in model editing techniques to integrate and adapt to unseen scripts without relying on centralized fine-tune strategies.

Result: Experiments demonstrate significant improvements in transfer learning and out-of-domain evaluations for recognition tasks, including historical ciphered texts and non-Latin scripts.

Conclusion: The approach facilitates better adoption of under-represented alphabets, enabling broader document recognition across diverse cultural and linguistic contexts.

Abstract: Achieving robustness in recognition systems across diverse domains is crucial
for their practical utility. While ample data availability is usually assumed,
low-resource languages, such as ancient manuscripts and non-western languages,
tend to be kept out of the equations of massive pretraining and foundational
techniques due to an under representation. In this work, we aim for building
models which can generalize to new distributions of data, such as alphabets,
faster than centralized fine-tune strategies. For doing so, we take advantage
of the recent advancements in model editing to enhance the incorporation of
unseen scripts (low-resource learning). In contrast to state-of-the-art
meta-learning, we showcase the effectiveness of domain merging in sparse
distributions of data, with agnosticity of its relation to the overall
distribution or any other prototyping necessity. Even when using the same exact
training data, our experiments showcase significant performance boosts in
\textbf{transfer learning} to new alphabets and \textbf{out-of-domain
evaluation} in challenging domain shifts, including historical ciphered texts
and non-Latin scripts. This research contributes a novel approach into building
models that can easily adopt under-represented alphabets and, therefore, enable
document recognition to a wider set of contexts and cultures.

</details>


### [508] [Ensemble-Based Survival Models with the Self-Attended Beran Estimator Predictions](https://arxiv.org/abs/2506.07933)
*Lev V. Utkin,Semen P. Khomets,Vlada A. Efremenko,Andrei V. Konstantinov,Natalya M. Verbova*

Main category: cs.LG

TL;DR: This paper proposes SurvBESA, a novel ensemble model for survival analysis that addresses prediction instability caused by censored data and bootstrap sample variations using a self-attention mechanism on predicted survival functions.


<details>
  <summary>Details</summary>
Motivation: To overcome instability in predictions from existing ensemble-based survival models (e.g., random survival forests), which are often affected by noise and variations in censored data.

Method: SurvBESA integrates Beran estimators with a self-attention mechanism to refine survival functions, optimizing attention weights through Huber's contamination model to address noise adaptively.

Result: Experimental results demonstrate that SurvBESA outperforms state-of-the-art survival analysis models, showcasing improved prediction accuracy and stability.

Conclusion: SurvBESA advances survival analysis with its innovative use of self-attention, offering higher accuracy and robustness. Its publicly accessible implementation facilitates future research and practical application.

Abstract: Survival analysis predicts the time until an event of interest, such as
failure or death, but faces challenges due to censored data, where some events
remain unobserved. Ensemble-based models, like random survival forests and
gradient boosting, are widely used but can produce unstable predictions due to
variations in bootstrap samples. To address this, we propose SurvBESA (Survival
Beran Estimators Self-Attended), a novel ensemble model that combines Beran
estimators with a self-attention mechanism. Unlike traditional methods,
SurvBESA applies self-attention to predicted survival functions, smoothing out
noise by adjusting each survival function based on its similarity to
neighboring survival functions. We also explore a special case using Huber's
contamination model to define attention weights, simplifying training to a
quadratic or linear optimization problem. Numerical experiments show that
SurvBESA outperforms state-of-the-art models. The implementation of SurvBESA is
publicly available.

</details>


### [509] [Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World](https://arxiv.org/abs/2506.06782)
*Qinting Jiang,Chuyang Ye,Dongyan Wei,Bingli Wang,Yuan Xue,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: The paper introduces the FIND framework to significantly improve test-time adaptation by addressing performance issues caused by dynamic test distributions, achieving a notable 30% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks fail to maintain performance under distribution shifts between training and testing data, causing Quality of Experience issues.

Method: The FIND framework includes three key techniques: Layer-wise Feature Disentanglement for stable feature capture, Feature Aware Batch Normalization for adaptive feature representation, and Selective FABN for enhancing efficiency by determining feature partitioning layers.

Result: The experiments show a 30% accuracy improvement in dynamic test scenarios while maintaining computational efficiency.

Conclusion: The FIND framework effectively improves test-time adaptation through better handling of feature distributions and selective optimization, outperforming existing methods.

Abstract: Despite progress, deep neural networks still suffer performance declines
under distribution shifts between training and test domains, leading to a
substantial decrease in Quality of Experience (QoE) for applications. Existing
test-time adaptation (TTA) methods are challenged by dynamic, multiple test
distributions within batches. We observe that feature distributions across
different domains inherently cluster into distinct groups with varying means
and variances. This divergence reveals a critical limitation of previous global
normalization strategies in TTA, which inevitably distort the original data
characteristics. Based on this insight, we propose Feature-based Instance
Neighbor Discovery (FIND), which comprises three key components: Layer-wise
Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and
Selective FABN (S-FABN). LFD stably captures features with similar
distributions at each layer by constructing graph structures. While FABN
optimally combines source statistics with test-time distribution specific
statistics for robust feature representation. Finally, S-FABN determines which
layers require feature partitioning and which can remain unified, thereby
enhancing inference efficiency. Extensive experiments demonstrate that FIND
significantly outperforms existing methods, achieving a 30\% accuracy
improvement in dynamic scenarios while maintaining computational efficiency.

</details>


### [510] [Caterpillar GNN: Replacing Message Passing with Efficient Aggregation](https://arxiv.org/abs/2506.06784)
*Marek Černý*

Main category: cs.LG

TL;DR: The paper introduces Caterpillar GNN, a graph neural network model that balances expressivity and efficiency using a novel aggregation mechanism.


<details>
  <summary>Details</summary>
Motivation: Modern graph learning models prioritize maximal expressivity at the cost of efficiency, leading to challenges in structured aggregation and scalability.

Method: The authors design an efficient aggregation mechanism that trades off expressivity for better structure, characterized by homomorphism counts from caterpillar graphs.

Result: Caterpillar GNN performs well on synthetic tasks difficult for classical MPGNNs and achieves comparable results on real-world datasets while reducing computational complexity.

Conclusion: Caterpillar GNN is a promising approach to graph learning, offering efficiency without sacrificing predictive power.

Abstract: Message-passing graph neural networks (MPGNNs) dominate modern graph
learning, typically prioritizing maximal expressive power. In contrast, we
introduce an \emph{efficient aggregation} mechanism, deliberately trading off
some expressivity for stronger and more structured aggregation capabilities.
Our approach allows seamless scaling between classical message-passing and
simpler methods based on colored or plain walks. We rigorously characterize the
expressive power at each intermediate step using homomorphism counts from a
hierarchy of generalized \emph{caterpillar graphs}. Based on this foundation,
we propose the \emph{Caterpillar GNN}, whose robust graph-level aggregation
enables it to successfully tackle synthetic graph-level task specifically
designed to be challenging for classical MPGNNs. Moreover, we demonstrate that,
on real-world datasets, the Caterpillar GNN achieves comparable predictive
performance while significantly reducing the number of nodes in the hidden
layers of the computational graph.

</details>


### [511] [Is Optimal Transport Necessary for Inverse Reinforcement Learning?](https://arxiv.org/abs/2506.06793)
*Zixuan Dong,Yumi Omori,Keith Ross*

Main category: cs.LG

TL;DR: This paper introduces heuristic alternatives to Optimal Transport (OT) in Inverse Reinforcement Learning (IRL) and shows they can achieve comparable or better performance with less complexity.


<details>
  <summary>Details</summary>
Motivation: Challenges the necessity of using complex OT approaches in IRL, aiming to simplify reward inference without sacrificing performance.

Method: Proposes two simple heuristics: Minimum-Distance Reward (nearest expert state) and Segment-Matching Reward (lightweight temporal alignment), avoiding optimization and achieving linear complexity.

Result: Extensive evaluations on 32 benchmarks using three RL algorithms demonstrate that the proposed methods match or outperform OT-based approaches.

Conclusion: Simplicity in alignment methods can yield strong IRL performance, questioning the need for algorithmic complexity inherent in OT-based designs.

Abstract: Inverse Reinforcement Learning (IRL) aims to recover a reward function from
expert demonstrations. Recently, Optimal Transport (OT) methods have been
successfully deployed to align trajectories and infer rewards. While OT-based
methods have shown strong empirical results, they introduce algorithmic
complexity, hyperparameter sensitivity, and require solving the OT optimization
problems. In this work, we challenge the necessity of OT in IRL by proposing
two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns
rewards based on the nearest expert state regardless of temporal order; and (2)
Segment-Matching Reward, which incorporates lightweight temporal alignment by
matching agent states to corresponding segments in the expert trajectory. These
methods avoid optimization, exhibit linear-time complexity, and are easy to
implement. Through extensive evaluations across 32 online and offline
benchmarks with three reinforcement learning algorithms, we show that our
simple rewards match or outperform recent OT-based approaches. Our findings
suggest that the core benefits of OT may arise from basic proximity alignment
rather than its optimal coupling formulation, advocating for reevaluation of
complexity in future IRL design.

</details>


### [512] [Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.07744)
*Seungho Baek,Taegeon Park,Jongchan Park,Seungjun Oh,Yusung Kim*

Main category: cs.LG

TL;DR: The paper introduces Graph-Assisted Stitching (GAS), a new method for offline hierarchical reinforcement learning, improving task efficiency and effectiveness by solving subgoal selection as a graph search problem rather than relying solely on explicit high-level policies.


<details>
  <summary>Details</summary>
Motivation: Offline hierarchical reinforcement learning (HRL) methods face challenges with task horizon scalability and stitching state transitions across different trajectories. This paper aims to address these inefficiencies and improve performance.

Method: The authors propose GAS, which moves subgoal selection to a graph search paradigm. GAS uses Temporal Distance Representation (TDR) to cluster states into graph nodes, enabling efficient state stitching, and employs the Temporal Efficiency (TE) metric to filter suboptimal transitions, thereby enhancing graph quality. Subgoals are then determined via a shortest-path algorithm.

Result: GAS significantly outperforms prior HRL methods on multiple tasks in locomotion, navigation, and manipulation. For a highly stitching-critical task, GAS obtained a score of 88.3 compared to the previous state-of-the-art score of 1.0.

Conclusion: GAS redefines subgoal selection in offline HRL by introducing graph-assisted techniques, effectively addressing task horizon inefficiencies and transition stitching, and achieving state-of-the-art results across various tasks.

Abstract: Existing offline hierarchical reinforcement learning methods rely on
high-level policy learning to generate subgoal sequences. However, their
efficiency degrades as task horizons increase, and they lack effective
strategies for stitching useful state transitions across different
trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that
formulates subgoal selection as a graph search problem rather than learning an
explicit high-level policy. By embedding states into a Temporal Distance
Representation (TDR) space, GAS clusters semantically similar states from
different trajectories into unified graph nodes, enabling efficient transition
stitching. A shortest-path algorithm is then applied to select subgoal
sequences within the graph, while a low-level policy learns to reach the
subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)
metric, which filters out noisy or inefficient transition states, significantly
enhancing task performance. GAS outperforms prior offline HRL methods across
locomotion, navigation, and manipulation tasks. Notably, in the most
stitching-critical task, it achieves a score of 88.3, dramatically surpassing
the previous state-of-the-art score of 1.0. Our source code is available at:
https://github.com/qortmdgh4141/GAS.

</details>


### [513] [IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2506.06809)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: The paper introduces IMPA-HGAE, a self-supervised learning framework that improves target node embeddings by exploiting meta-path information in heterogeneous graphs, achieving superior experimental performance.


<details>
  <summary>Details</summary>
Motivation: The need to address the underutilization of heterogeneous node information along meta-paths in self-supervised learning models for heterogeneous graphs.

Method: Proposes the IMPA-HGAE framework, which fully incorporates internal node information along meta-paths. Innovative masking strategies are also introduced to enhance representational capacity.

Result: Experimental results show the superior performance of the proposed method on heterogeneous datasets.

Conclusion: The study demonstrates the effectiveness of leveraging meta-path-guided semantics for representation learning in heterogeneous graphs and suggests future directions for generative SSL in this area.

Abstract: Self-supervised learning (SSL) methods have been increasingly applied to
diverse downstream tasks due to their superior generalization capabilities and
low annotation costs. However, most existing heterogeneous graph SSL models
convert heterogeneous graphs into homogeneous ones via meta-paths for training,
which only leverage information from nodes at both ends of meta-paths while
underutilizing the heterogeneous node information along the meta-paths. To
address this limitation, this paper proposes a novel framework named IMPA-HGAE
to enhance target node embeddings by fully exploiting internal node information
along meta-paths. Experimental results validate that IMPA-HGAE achieves
superior performance on heterogeneous datasets. Furthermore, this paper
introduce innovative masking strategies to strengthen the representational
capacity of generative SSL models on heterogeneous graph data. Additionally,
this paper discuss the interpretability of the proposed method and potential
future directions for generative self-supervised learning in heterogeneous
graphs. This work provides insights into leveraging meta-path-guided structural
semantics for robust representation learning in complex graph scenarios.

</details>


### [514] [Path Integral Optimiser: Global Optimisation via Neural Schrödinger-Föllmer Diffusion](https://arxiv.org/abs/2506.06815)
*Max McGuinness,Eirik Fladmark,Francisco Vargas*

Main category: cs.LG

TL;DR: The paper explores the application of neural diffusion processes for global optimization using a method inspired by Path Integral Sampler, presenting theoretical bounds and experimental results that show potential for handling tasks across varying dimensions but highlighting challenges with scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate neural diffusion processes as a promising approach for solving complex global optimization problems, leveraging modern neural approximation techniques.

Method: The authors use the Boltzmann distribution to recast optimization problems, apply Girsanov's theorem for stochastic control framing, and compute solutions through neural approximations with a Fourier multi-layer perceptron (MLP).

Result: Results demonstrate promising optimization performance across dimensions ranging from 2 to 1,247, though significant challenges remain in scaling to extremely high-dimensional tasks like a 15.9k parameter model.

Conclusion: The optimiser shows strong potential for per-step performance in moderate-dimensional optimization; however, adaptations are needed for working in high-dimensional spaces where exploration faces severe limitations.

Abstract: We present an early investigation into the use of neural diffusion processes
for global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One
can use the Boltzmann distribution to formulate optimization as solving a
Schr\"odinger bridge sampling problem, then apply Girsanov's theorem with a
simple (single-point) prior to frame it in stochastic control terms, and
compute the solution's integral terms via a neural approximation (a Fourier
MLP). We provide theoretical bounds for this optimiser, results on toy
optimisation tasks, and a summary of the stochastic theory motivating the
model. Ultimately, we found the optimiser to display promising per-step
performance at optimisation tasks between 2 and 1,247 dimensions, but struggle
to explore higher-dimensional spaces when faced with a 15.9k parameter model,
indicating a need for work on adaptation in such environments.

</details>


### [515] [High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations](https://arxiv.org/abs/2506.06858)
*Ziwei Li,Yuhan Duan,Tianyu Xiong,Yi-Tang Chen,Wei-Lun Chao,Han-Wei Shen*

Main category: cs.LG

TL;DR: FA-INR leverages cross-attention and a coordinate-guided mixture of experts to boost feature representation adaptability and model compactness, achieving state-of-the-art results in scientific simulations.


<details>
  <summary>Details</summary>
Motivation: Scientific simulations require surrogate models that are both accurate and efficient, particularly for complex fields with localized, high-frequency variations.

Method: FA-INR incorporates cross-attention with an augmented memory bank for flexible feature representation and uses a coordinate-guided mixture of experts to enhance scalability and efficiency.

Result: FA-INR demonstrates state-of-the-art fidelity and significantly reduces model size in experiments on three large-scale scientific datasets.

Conclusion: FA-INR offers a balanced trade-off between accuracy and compactness, establishing it as a promising approach for INR-based surrogate models in scientific simulations.

Abstract: Effective surrogate models are critical for accelerating scientific
simulations. Implicit neural representations (INRs) offer a compact and
continuous framework for modeling spatially structured data, but they often
struggle with complex scientific fields exhibiting localized, high-frequency
variations. Recent approaches address this by introducing additional features
along rigid geometric structures (e.g., grids), but at the cost of flexibility
and increased model size. In this paper, we propose a simple yet effective
alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to
an augmented memory bank to learn flexible feature representations, enabling
adaptive allocation of model capacity based on data characteristics, rather
than rigid structural assumptions. To further improve scalability, we introduce
a coordinate-guided mixture of experts (MoE) that enhances the specialization
and efficiency of feature representations. Experiments on three large-scale
ensemble simulation datasets show that FA-INR achieves state-of-the-art
fidelity while significantly reducing model size, establishing a new trade-off
frontier between accuracy and compactness for INR-based surrogates.

</details>


### [516] [Differentially Private Sparse Linear Regression with Heavy-tailed Responses](https://arxiv.org/abs/2506.06861)
*Xizhi Tian,Meng Ding,Touming Tao,Zihang Xiang,Di Wang*

Main category: cs.LG

TL;DR: The paper introduces methods for differentially private sparse linear regression in high-dimensional settings with heavy-tailed data, achieving improved error bounds and demonstrating superior performance over standard algorithms.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of most existing methods that focus on either regular data distributions or low-dimensional cases in DP linear regression.

Method: Develop and analyze two methods: DP-IHT-H (using Huber loss with private iterative hard thresholding) and DP-IHT-L (introducing additional assumptions on response) to achieve better error bounds.

Result: DP-IHT-H achieves error bounds under heavy-tailed responses, while DP-IHT-L offers improved error bounds independent of tail heaviness, validated through synthetic and real-world datasets.

Conclusion: The proposed methods expand the scope of DP sparse linear regression by achieving more robust solutions in high-dimensional, heavy-tailed scenarios.

Abstract: As a fundamental problem in machine learning and differential privacy (DP),
DP linear regression has been extensively studied. However, most existing
methods focus primarily on either regular data distributions or low-dimensional
cases with irregular data. To address these limitations, this paper provides a
comprehensive study of DP sparse linear regression with heavy-tailed responses
in high-dimensional settings. In the first part, we introduce the DP-IHT-H
method, which leverages the Huber loss and private iterative hard thresholding
to achieve an estimation error bound of \(
  \tilde{O}\biggl(
  s^{* \frac{1 }{2}}
  \cdot \biggl(\frac{\log d}{n}\biggr)^{\frac{\zeta}{1 + \zeta}}
  +
  s^{* \frac{1 + 2\zeta}{2 + 2\zeta}}
  \cdot \biggl(\frac{\log^2 d}{n \varepsilon}\biggr)^{\frac{\zeta}{1 + \zeta}}
  \biggr) \) under the $(\varepsilon, \delta)$-DP model, where $n$ is the
sample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter,
and $\zeta \in (0, 1]$ characterizes the tail heaviness of the data. In the
second part, we propose DP-IHT-L, which further improves the error bound under
additional assumptions on the response and achieves \(
  \tilde{O}\Bigl(\frac{(s^*)^{3/2} \log d}{n \varepsilon}\Bigr). \) Compared to
the first result, this bound is independent of the tail parameter $\zeta$.
Finally, through experiments on synthetic and real-world datasets, we
demonstrate that our methods outperform standard DP algorithms designed for
``regular'' data.

</details>


### [517] [SAFE: Finding Sparse and Flat Minima to Improve Pruning](https://arxiv.org/abs/2506.06866)
*Dongyeop Lee,Kwanhee Lee,Jinseok Chung,Namhoon Lee*

Main category: cs.LG

TL;DR: This paper develops novel neural network pruning methods, SAFE and SAFE+, which emphasize network sparsity and flatness to preserve performance and improve generalization. These methods outperform benchmarks and show resilience to noise.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of performance degradation in sparsified neural networks, leveraging insights from robust optimization to improve pruning methods.

Method: Pruning is formulated as a sparsity-constrained optimization problem with a focus on flatness, solved using an augmented Lagrange dual approach and extended with a generalized projection operation.

Result: The methods, SAFE and SAFE+, perform consistently well in image classification and language modeling tasks, offering improved generalization and competitive results compared to baselines. They exhibit robustness to noisy data.

Conclusion: SAFE and SAFE+ provide effective approaches for neural network pruning, achieving sparse structures without significant performance loss and proving adaptable to real-world noisy conditions.

Abstract: Sparsifying neural networks often suffers from seemingly inevitable
performance degradation, and it remains challenging to restore the original
performance despite much recent progress. Motivated by recent studies in robust
optimization, we aim to tackle this problem by finding subnetworks that are
both sparse and flat at the same time. Specifically, we formulate pruning as a
sparsity-constrained optimization problem where flatness is encouraged as an
objective. We solve it explicitly via an augmented Lagrange dual approach and
extend it further by proposing a generalized projection operation, resulting in
novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive
evaluations on standard image classification and language modeling tasks reveal
that SAFE consistently yields sparse networks with improved generalization
performance, which compares competitively to well-established baselines. In
addition, SAFE demonstrates resilience to noisy data, making it well-suited for
real-world conditions.

</details>


### [518] [FREE: Fast and Robust Vision Language Models with Early Exits](https://arxiv.org/abs/2506.06884)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: The paper proposes an Early Exit (EE) approach for Vision-Language Models (VLMs) to reduce inference latency, without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: Real-world applications require faster inference from VLMs while maintaining their high performance, posing a challenge for large-scale models.

Method: The proposed method, FREE, uses a GAN-based adversarial training approach in which transformer layers are trained to mimic the final-layer features. This method enables adaptive inference via exit classifiers.

Result: The method achieved 1.51x faster inference while preserving comparable accuracy and robustness, effectively addressing overthinking and mid-crisis effects.

Conclusion: FREE offers an efficient solution for speeding up VLMs without compromising performance, making them more suitable for practical use cases.

Abstract: In recent years, Vision-Language Models (VLMs) have shown remarkable
performance improvements in Vision-Language tasks. However, their large size
poses challenges for real-world applications where inference latency is a
concern. To tackle this issue, we propose employing Early Exit (EE) strategies
in VLMs. However, training exit classifiers in VLMs is challenging,
particularly with limited labeled training data. To address this, we introduce
FREE, an adversarial training approach within a GAN-based framework. Here, each
exit consists of a transformer layer and a classifier. The transformer layer is
adversarially trained to produce feature representations similar to the final
layer, while a feature classifier serves as the discriminator. Our method
focuses on performing input-adaptive inference that increases inference speed
with minimal drop in performance. Experimental results demonstrate the
effectiveness of our approach in enhancing accuracy and model robustness by
mitigating overthinking and the phenomenon of mid-crisis that we highlight. We
experimentally validate that our method speeds up the inference process by more
than 1.51x while retaining comparable performance. The source code is available
at https://github.com/Div290/FREE.

</details>


### [519] [Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?](https://arxiv.org/abs/2506.06891)
*Paulius Sasnauskas,Yiğit Yalın,Goran Radanović*

Main category: cs.LG

TL;DR: This paper introduces a novel framework called AT-DPT to improve the corruption-robustness of Decision-Pretrained Transformers against reward poisoning attacks in reinforcement learning tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the vulnerability of Decision-Pretrained Transformers to reward poisoning attacks in reinforcement learning, which could compromise their ability to make optimal decisions.

Method: The authors propose Adversarially Trained Decision-Pretrained Transformer (AT-DPT), which jointly trains an adversarial attacker to degrade rewards and a model to infer optimal actions from corrupted data.

Result: Experimental results demonstrate AT-DPT's superiority over standard bandit algorithms and robust baselines. It shows enhanced performance under both a learned and adaptive attacker and generalizes well to MDP settings.

Conclusion: AT-DPT provides substantial improvements in corruption-robustness for Decision-Pretrained Transformers, making it a significant advancement in handling reward contamination in reinforcement learning scenarios.

Abstract: We study the corruption-robustness of in-context reinforcement learning
(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,
2023). To address the challenge of reward poisoning attacks targeting the DPT,
we propose a novel adversarial training framework, called Adversarially Trained
Decision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an
attacker to minimize the true reward of the DPT by poisoning environment
rewards, and a DPT model to infer optimal actions from the poisoned data. We
evaluate the effectiveness of our approach against standard bandit algorithms,
including robust baselines designed to handle reward contamination. Our results
show that the proposed method significantly outperforms these baselines in
bandit settings, under a learned attacker. We additionally evaluate AT-DPT on
an adaptive attacker, and observe similar results. Furthermore, we extend our
evaluation to the MDP setting, confirming that the robustness observed in
bandit scenarios generalizes to more complex environments.

</details>


### [520] [Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations](https://arxiv.org/abs/2506.06907)
*Fred Xu,Thomas Markovich*

Main category: cs.LG

TL;DR: The paper introduces a novel Graph Neural Network (GNN) framework inspired by stochastic partial differential equations (SPDE) to improve uncertainty estimation on graphs under distributional shifts.


<details>
  <summary>Details</summary>
Motivation: Existing GNN methods struggle to provide accurate uncertainty estimation due to the complexity of randomness in graph structures and label distributions, making such estimations critical under changing distributions.

Method: The authors design a message-passing scheme based on the analogy between SPDE evolution driven by Matern Gaussian Processes and GNN layers, incorporating spatial-temporal noise to enhance uncertainty estimation.

Result: The proposed method successfully captures spatial and temporal uncertainty and performs better on out-of-distribution detection compared to existing approaches, especially for datasets with varying label informativeness.

Conclusion: The approach offers improved control over uncertainty estimation on graphs, showcasing its effectiveness through extensive experiments on diverse datasets.

Abstract: Graph Neural Networks have achieved impressive results across diverse network
modeling tasks, but accurately estimating uncertainty on graphs remains
difficult, especially under distributional shifts. Unlike traditional
uncertainty estimation, graph-based uncertainty must account for randomness
arising from both the graph's structure and its label distribution, which adds
complexity. In this paper, making an analogy between the evolution of a
stochastic partial differential equation (SPDE) driven by Matern Gaussian
Process and message passing using GNN layers, we present a principled way to
design a novel message passing scheme that incorporates spatial-temporal noises
motivated by the Gaussian Process approach to SPDE. Our method simultaneously
captures uncertainty across space and time and allows explicit control over the
covariance kernel smoothness, thereby enhancing uncertainty estimates on graphs
with both low and high label informativeness. Our extensive experiments on
Out-of-Distribution (OOD) detection on graph datasets with varying label
informativeness demonstrate the soundness and superiority of our model to
existing approaches.

</details>


### [521] [Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data](https://arxiv.org/abs/2506.06917)
*Shangjie Du,Hui Wei,Dong Yoon Lee,Zhizhang Hu,Shijia Pan*

Main category: cs.LG

TL;DR: The paper introduces GraPhy, a graph-based neural network for accurate air quality modeling in urban areas with limited data, outperforming various baseline models.


<details>
  <summary>Details</summary>
Motivation: Sparse air quality monitoring networks in socioeconomically disadvantaged regions result in inaccurate models, underscoring the need for improved frameworks.

Method: GraPhy employs a physics-guided graph neural network architecture with specialized layers and edge features tailored for low-resolution data.

Result: Experiments in California's San Joaquin Valley show GraPhy improves accuracy metrics (MSE, MAE, R2) by 9%-56% compared to baseline models.

Conclusion: GraPhy effectively improves air quality modeling accuracy and resolution in areas with sparse monitoring, addressing spatial heterogeneity challenges.

Abstract: This work introduces GraPhy, a graph-based, physics-guided learning framework
for high-resolution and accurate air quality modeling in urban areas with
limited monitoring data. Fine-grained air quality monitoring information is
essential for reducing public exposure to pollutants. However, monitoring
networks are often sparse in socioeconomically disadvantaged regions, limiting
the accuracy and resolution of air quality modeling. To address this, we
propose a physics-guided graph neural network architecture called GraPhy with
layers and edge features designed specifically for low-resolution monitoring
data. Experiments using data from California's socioeconomically disadvantaged
San Joaquin Valley show that GraPhy achieves the overall best performance
evaluated by mean squared error (MSE), mean absolute error (MAE), and R-square
value (R2), improving the performance by 9%-56% compared to various baseline
models. Moreover, GraPhy consistently outperforms baselines across different
spatial heterogeneity levels, demonstrating the effectiveness of our model
design.

</details>


### [522] [Basis Transformers for Multi-Task Tabular Regression](https://arxiv.org/abs/2506.06926)
*Wei Min Loh,Jiaqi Shang,Pascal Poupart*

Main category: cs.LG

TL;DR: The paper introduces Basis Transformers, an architecture addressing challenges in tabular data processing, achieving superior results on benchmarks with fewer parameters compared to existing models.


<details>
  <summary>Details</summary>
Motivation: To address challenges in dealing with tabular data, such as partial information, text-related attributes, variable column structures, and lack of metadata.

Method: A novel architecture, Basis Transformers, designed with invariances and hierarchical structures of tabular data, evaluated on a multi-task tabular regression benchmark.

Result: Improved the median $R^2$ score by 0.338 across 34 tasks, reduced standard deviation, and required five times fewer parameters than the leading baseline models.

Conclusion: Basis Transformers outperform existing techniques, including large language models, in tabular data processing, highlighting its efficiency and robustness with fewer computational resources.

Abstract: Dealing with tabular data is challenging due to partial information, noise,
and heterogeneous structure. Existing techniques often struggle to
simultaneously address key aspects of tabular data such as textual information,
a variable number of columns, and unseen data without metadata besides column
names. We propose a novel architecture, \textit{basis transformers},
specifically designed to tackle these challenges while respecting inherent
invariances in tabular data, including hierarchical structure and the
representation of numeric values. We evaluate our design on a multi-task
tabular regression benchmark, achieving an improvement of 0.338 in the median
$R^2$ score and the lowest standard deviation across 34 tasks from the
OpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters
than the best-performing baseline and surpasses pretrained large language model
baselines -- even when initialized from randomized weights.

</details>


### [523] [Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry](https://arxiv.org/abs/2506.06933)
*Mahdi Salmani,Alireza Abdollahpoorrostam,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: The paper introduces a framework for decision-based black-box adversarial attacks under asymmetric query costs, which modifies search strategies and gradient estimation for efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing black-box adversarial attacks assume uniform query cost, but in practice, query costs are often asymmetric, particularly in applications like content moderation systems.

Method: The paper proposes Asymmetric Search (AS) for conservative query strategies and Asymmetric Gradient Estimation (AGREST) for sampling favoring low-cost queries, designing algorithms that minimize overall attack cost.

Result: The proposed methods outperform existing approaches in terms of total query cost and perturbation size, achieving up to 40% improvement in evaluations on image classification benchmarks.

Conclusion: The framework can integrate seamlessly into existing black-box attacks, offering significant efficiency improvements in asymmetric cost settings.

Abstract: Traditional decision-based black-box adversarial attacks on image classifiers
aim to generate adversarial examples by slightly modifying input images while
keeping the number of queries low, where each query involves sending an input
to the model and observing its output. Most existing methods assume that all
queries have equal cost. However, in practice, queries may incur asymmetric
costs; for example, in content moderation systems, certain output classes may
trigger additional review, enforcement, or penalties, making them more costly
than others. While prior work has considered such asymmetric cost settings,
effective algorithms for this scenario remain underdeveloped. In this paper, we
propose a general framework for decision-based attacks under asymmetric query
costs, which we refer to as asymmetric black-box attacks. We modify two core
components of existing attacks: the search strategy and the gradient estimation
process. Specifically, we propose Asymmetric Search (AS), a more conservative
variant of binary search that reduces reliance on high-cost queries, and
Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution
to favor low-cost queries. We design efficient algorithms that minimize total
attack cost by balancing different query types, in contrast to earlier methods
such as stealthy attacks that focus only on limiting expensive (high-cost)
queries. Our method can be integrated into a range of existing black-box
attacks with minimal changes. We perform both theoretical analysis and
empirical evaluation on standard image classification benchmarks. Across
various cost regimes, our method consistently achieves lower total query cost
and smaller perturbations than existing approaches, with improvements of up to
40% in some settings.

</details>


### [524] [Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More](https://arxiv.org/abs/2506.06940)
*Geonhui Yoo,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: The paper investigates progressive sharpening, a phenomenon in neural network training, using a minimalist deep linear network model to understand sharpness dynamics better.


<details>
  <summary>Details</summary>
Motivation: Sharpness in neural networks increases during training, yet its underlying mechanisms are poorly understood despite its common occurrence.

Method: A minimalist deep linear network with one neuron per layer is analyzed theoretically and empirically to study sharpness dynamics related to dataset properties, network depth, optimizer stochasticity, and step size.

Result: The minimalist model effectively replicates empirical sharpness dynamics, providing insights into how dataset, depth, and optimization settings influence the phenomenon in practical scenarios.

Conclusion: The study advances our understanding of sharpness dynamics, showing how it is shaped by interplay among depth, training data, and optimization settings.

Abstract: When training deep neural networks with gradient descent, sharpness often
increases -- a phenomenon known as progressive sharpening -- before saturating
at the edge of stability. Although commonly observed in practice, the
underlying mechanisms behind progressive sharpening remain poorly understood.
In this work, we study this phenomenon using a minimalist model: a deep linear
network with a single neuron per layer. We show that this simple model
effectively captures the sharpness dynamics observed in recent empirical
studies, offering a simple testbed to better understand neural network
training. Moreover, we theoretically analyze how dataset properties, network
depth, stochasticity of optimizers, and step size affect the degree of
progressive sharpening in the minimalist model. We then empirically demonstrate
how these theoretical insights extend to practical scenarios. This study offers
a deeper understanding of sharpness dynamics in neural network training,
highlighting the interplay between depth, training data, and optimizers.

</details>


### [525] [UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare](https://arxiv.org/abs/2506.06977)
*Pengfei Hu,Xiaoxue Han,Fei Wang,Yue Ning*

Main category: cs.LG

TL;DR: This paper tackles domain generalization in clinical prediction by leveraging medical hierarchies to address domain label absence and enable medical knowledge integration.


<details>
  <summary>Details</summary>
Motivation: Patient cohorts exhibit shifting data distributions in real-world healthcare settings, degrading model performance and making domain generalization a critical need.

Method: The paper introduces UdonCare, a framework utilizing hierarchical medical ontologies to prune domains and encode refined signals using a Siamese-type mechanism.

Result: UdonCare outperformed existing baselines on clinical datasets (MIMIC-III and MIMIC-IV) in scenarios with significant domain variability.

Conclusion: Medical hierarchies can effectively enhance domain generalization, offering practical applications in healthcare prediction models.

Abstract: Domain generalization has become a critical challenge in clinical prediction,
where patient cohorts often exhibit shifting data distributions that degrade
model performance. Typical domain generalization approaches struggle in
real-world healthcare settings for two main reasons: (1) patient-specific
domain labels are typically unavailable, making domain discovery especially
difficult; (2) purely data-driven approaches overlook key clinical insights,
leading to a gap in medical knowledge integration. To address these problems,
we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to
group diseases into higher-level categories and discover more flexible latent
domains. In this paper, we introduce UdonCare, a hierarchy-guided framework
that iteratively prunes fine-grained domains, encodes these refined domains,
and applies a Siamese-type inference mechanism to separate domain-related
signals from patient-level features. Experimental results on clinical datasets
(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher
performance compared to other domain generalization baselines when substantial
domain gaps presents, highlighting the untapped potential of medical knowledge
for enhancing domain generalization in practical healthcare applications.

</details>


### [526] [MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification](https://arxiv.org/abs/2506.06980)
*Sajib Acharjee Dip,Uddip Acharjee Shuvo,Dipanwita Mallick,Abrar Rahman Abir,Liqing Zhang*

Main category: cs.LG

TL;DR: MoXGATE is a deep-learning framework using cross-attention and modality-specific weights to integrate multi-omics data, achieving high accuracy in cancer subtype classification.


<details>
  <summary>Details</summary>
Motivation: The need to effectively integrate multi-omics data for cancer subtype classification to enable personalized treatment and prognosis.

Method: A deep-learning framework named MoXGATE, employing cross-attention mechanisms and modality-weighted fusion for multi-omics data integration, with the use of focal loss to handle data imbalance.

Result: MoXGATE demonstrated 95% classification accuracy on GIAC and BRCA datasets and strong generalizability to unseen cancer types.

Conclusion: MoXGATE offers a novel and effective method for multi-omic cancer subtype classification, improving performance and interpretability compared to existing techniques.

Abstract: Cancer subtype classification is crucial for personalized treatment and
prognostic assessment. However, effectively integrating multi-omic data remains
challenging due to the heterogeneous nature of genomic, epigenomic, and
transcriptomic features. In this work, we propose Modality-Aware
Cross-Attention MoXGATE, a novel deep-learning framework that leverages
cross-attention and learnable modality weights to enhance feature fusion across
multiple omics sources. Our approach effectively captures inter-modality
dependencies, ensuring robust and interpretable integration. Through
experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)
datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,
achieving 95\% classification accuracy. Ablation studies validate the
effectiveness of cross-attention over simple concatenation and highlight the
importance of different omics modalities. Moreover, our model generalizes well
to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key
contributions include (1) a cross-attention-based multi-omic integration
framework, (2) modality-weighted fusion for enhanced interpretability, (3)
application of focal loss to mitigate data imbalance, and (4) validation across
multiple cancer subtypes. Our results indicate that MoXGATE is a promising
approach for multi-omic cancer subtype classification, offering improved
performance and biological generalizability.

</details>


### [527] [Fully Explainable Classification Models Using Hyperblocks](https://arxiv.org/abs/2506.06986)
*Austin Snyder,Ryan Gallagher,Boris Kovalerchuk*

Main category: cs.LG

TL;DR: The paper discusses enhancements to Hyperblocks, focusing on improving interpretability, reducing training time, and minimizing complexity without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the need for interpretable models that enable non-experts to understand machine learning decision logic while ensuring high performance and reduced complexity.

Method: The authors propose algorithms for Hyperblock simplification, including removal of redundant attributes and blocks as well as introduction of disjunctive units. Additionally, a fallback mechanism using k-NN is introduced to ensure data coverage.

Result: The proposed techniques significantly reduce model size while maintaining classification accuracy. They demonstrate strong performance on benchmark datasets, including WBC and MNIST.

Conclusion: The approach offers a scalable, interpretable, and competitive alternative to complex black-box models, making it suitable for domains requiring trust and clarity.

Abstract: Building on existing work with Hyperblocks, which classify data using minimum
and maximum bounds for each attribute, we focus on enhancing interpretability,
decreasing training time, and reducing model complexity without sacrificing
accuracy. This system allows subject matter experts (SMEs) to directly inspect
and understand the model's decision logic without requiring extensive machine
learning expertise. To reduce Hyperblock complexity while retaining
performance, we introduce a suite of algorithms for Hyperblock simplification.
These include removing redundant attributes, removing redundant blocks through
overlap analysis, and creating disjunctive units. These methods eliminate
unnecessary parameters, dramatically reducing model size without harming
classification power. We increase robustness by introducing an interpretable
fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not
covered by any block, ensuring complete data coverage while preserving model
transparency. Our results demonstrate that interpretable models can scale to
high-dimensional, large-volume datasets while maintaining competitive accuracy.
On benchmark datasets such as WBC (9-D), we achieve strong predictive
performance with significantly reduced complexity. On MNIST (784-D), our method
continues to improve through tuning and simplification, showing promise as a
transparent alternative to black-box models in domains where trust, clarity,
and control are crucial.

</details>


### [528] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/abs/2506.06990)
*Mingyi Li,Michael R. Metel,Akiko Takeda*

Main category: cs.LG

TL;DR: This paper improves the K-means algorithm by guaranteeing local optimality while maintaining computational efficiency, and demonstrates improved clustering results through experiments.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient analysis and guarantee of local optimality in the widely-used K-means clustering algorithm.

Method: The authors derive conditions for K-means to achieve local optimality and propose modifications to the algorithm using general Bregman divergence, which retain the computational cost of the original algorithm.

Result: Experimental results show that the original K-means does not always achieve locally optimal solutions, while the modified versions provide better local optima and reduced clustering loss.

Conclusion: With minor modifications, the K-means algorithm can achieve guaranteed local optimality, addressing a critical gap in the understanding and performance of this popular clustering approach.

Abstract: The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [529] [End-to-End Probabilistic Framework for Learning with Hard Constraints](https://arxiv.org/abs/2506.07003)
*Utkarsh Utkarsh,Danielle C. Maddix,Ruijun Ma,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: The paper introduces ProbHardE2E, a probabilistic forecasting framework that incorporates hard constraints and quantifies uncertainties using a differentiable probabilistic projection layer.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current forecasting models that either rely on post-processing hard constraints or suffer from biased distributional assumptions.

Method: Design and implement ProbHardE2E with a novel differentiable probabilistic projection layer capable of end-to-end learning, optimized using strictly proper scoring rules without distributional assumptions.

Result: ProbHardE2E successfully learns partial differential equations with uncertainty and performs robust probabilistic time-series forecasting.

Conclusion: ProbHardE2E serves as a general framework that effectively bridges forecasting and modeling systems requiring operational constraints and uncertainty estimates.

Abstract: We present a general purpose probabilistic forecasting framework,
ProbHardE2E, to learn systems that can incorporate operational/physical
constraints as hard requirements. ProbHardE2E enforces hard constraints by
exploiting variance information in a novel way; and thus it is also capable of
performing uncertainty quantification (UQ) on the model. Our methodology uses a
novel differentiable probabilistic projection layer (DPPL) that can be combined
with a wide range of neural network architectures. This DPPL allows the model
to learn the system in an end-to-end manner, compared to other approaches where
the constraints are satisfied either through a post-processing step or at
inference. In addition, ProbHardE2E can optimize a strictly proper scoring
rule, without making any distributional assumptions on the target, which
enables it to obtain robust distributional estimates (in contrast to existing
approaches that generally optimize likelihood-based objectives, which are
heavily biased by their distributional assumptions and model choices); and it
can incorporate a range of non-linear constraints (increasing the power of
modeling and flexibility). We apply ProbHardE2E to problems in learning partial
differential equations with uncertainty estimates and to probabilistic
time-series forecasting, showcasing it as a broadly applicable general setup
that connects these seemingly disparate domains.

</details>


### [530] [Comparison of Lightweight Methods for Vehicle Dynamics-Based Driver Drowsiness Detection](https://arxiv.org/abs/2506.07014)
*Yutaro Nakagama,Daisuke Ishii,Kazuki Yoshizoe*

Main category: cs.LG

TL;DR: The paper focuses on driver drowsiness detection using vehicle dynamics, addressing concerns like data leakage and reproducibility, and proposing a framework for fair comparisons with a public dataset.


<details>
  <summary>Details</summary>
Motivation: Prevent road accidents caused by driver fatigue using economical and effective detection methods while addressing reproducibility and reliability of existing techniques.

Method: A transparent framework utilizing a public dataset to evaluate various methods, including three representative ones and a concise random forest-based model, was developed and tested using lightweight ML models.

Result: The random forest-based method achieved the highest accuracy of 88% among evaluated methods.

Conclusion: Issues in non-standard DDD methods are highlighted, and the study showcases a high-performance approach implemented rigorously.

Abstract: Driver drowsiness detection (DDD) prevents road accidents caused by driver
fatigue. Vehicle dynamics-based DDD has been proposed as a method that is both
economical and high performance. However, there are concerns about the
reliability of performance metrics and the reproducibility of many of the
existing methods. For instance, some previous studies seem to have a data
leakage issue among training and test datasets, and many do not openly provide
the datasets they used. To this end, this paper aims to compare the performance
of representative vehicle dynamics-based DDD methods under a transparent and
fair framework that uses a public dataset. We first develop a framework for
extracting features from an open dataset by Aygun et al. and performing DDD
with lightweight ML models; the framework is carefully designed to support a
variety of onfigurations. Second, we implement three existing representative
methods and a concise random forest (RF)-based method in the framework.
Finally, we report the results of experiments to verify the reproducibility and
clarify the performance of DDD based on common metrics. Among the evaluated
methods, the RF-based method achieved the highest accuracy of 88 %. Our
findings imply the issues inherent in DDD methods developed in a non-standard
manner, and demonstrate a high performance method implemented appropriately.

</details>


### [531] [AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint](https://arxiv.org/abs/2506.07022)
*Leheng Sheng,Changshuo Shen,Weixiang Zhao,Junfeng Fang,Xiaohao Liu,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: This paper introduces AlphaSteer, a novel activation steering method to balance safety and utility in LLMs through a theoretically-grounded, learnable approach.


<details>
  <summary>Details</summary>
Motivation: The rapid deployment of LLMs in real-world applications necessitates mechanisms for refusing malicious prompts, such as jailbreak attacks, without harming benign interactions.

Method: The authors propose AlphaSteer, which uses principled learning objectives to balance safety and utility: 1) null-space constraints for utility preservation and 2) linear regression for safety enhancement.

Result: AlphaSteer shows significant improvement in LLM safety against various jailbreak attacks while maintaining their general usefulness, demonstrated through multiple experiments.

Conclusion: AlphaSteer effectively resolves the trade-off between safety and utility in LLMs, providing a robust mechanism to counter malicious prompts without degrading performance on benign inputs.

Abstract: As LLMs are increasingly deployed in real-world applications, ensuring their
ability to refuse malicious prompts, especially jailbreak attacks, is essential
for safe and reliable use. Recently, activation steering has emerged as an
effective approach for enhancing LLM safety by adding a refusal direction
vector to internal activations of LLMs during inference, which will further
induce the refusal behaviors of LLMs. However, indiscriminately applying
activation steering fundamentally suffers from the trade-off between safety and
utility, since the same steering vector can also lead to over-refusal and
degraded performance on benign prompts. Although prior efforts, such as vector
calibration and conditional steering, have attempted to mitigate this
trade-off, their lack of theoretical grounding limits their robustness and
effectiveness. To better address the trade-off between safety and utility, we
present a theoretically grounded and empirically effective activation steering
method called AlphaSteer. Specifically, it considers activation steering as a
learnable process with two principled learning objectives: utility preservation
and safety enhancement. For utility preservation, it learns to construct a
nearly zero vector for steering benign data, with the null-space constraints.
For safety enhancement, it learns to construct a refusal direction vector for
steering malicious data, with the help of linear regression. Experiments across
multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness
of AlphaSteer, which significantly improves the safety of LLMs without
compromising general capabilities. Our codes are available at
https://github.com/AlphaLab-USTC/AlphaSteer.

</details>


### [532] [Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression](https://arxiv.org/abs/2506.07033)
*Yung-Chien Wang,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.LG

TL;DR: The paper introduces MATI, a method to handle the imbalance issue in tabular regression tasks using region-specific experts and test-time adaptation, showing significant improvements in performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the underexplored issue of label imbalance in tabular regression tasks, which leads to poor model generalizability, especially when simplifying assumptions about test data distributions are invalid in real-world applications.

Method: The proposed MATI methodology incorporates two innovations: (i) Region-Aware Mixture Experts using Gaussian Mixture Models to create region-specific experts and (ii) Test-Time Self-Supervised Expert Aggregation to dynamically adjust expert weights based on varying test distributions.

Result: MATI was evaluated on four real-world datasets and three test distribution scenarios, achieving an average improvement of 7.1% in MAE compared to existing methods.

Conclusion: MATI effectively addresses tabular regression imbalance by adapting to varying test distributions, demonstrating superior performance over prior methods.

Abstract: Tabular data serve as a fundamental and ubiquitous representation of
structured information in numerous real-world applications, e.g., finance and
urban planning. In the realm of tabular imbalanced applications, data imbalance
has been investigated in classification tasks with insufficient instances in
certain labels, causing the model's ineffective generalizability. However, the
imbalance issue of tabular regression tasks is underexplored, and yet is
critical due to unclear boundaries for continuous labels and simplifying
assumptions in existing imbalance regression work, which often rely on known
and balanced test distributions. Such assumptions may not hold in practice and
can lead to performance degradation. To address these issues, we propose MATI:
Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular
Imbalance Regression, featuring two key innovations: (i) the Region-Aware
Mixture Expert, which adopts a Gaussian Mixture Model to capture the underlying
related regions. The statistical information of each Gaussian component is then
used to synthesize and train region-specific experts to capture the unique
characteristics of their respective regions. (ii) Test-Time Self-Supervised
Expert Aggregation, which dynamically adjusts region expert weights based on
test data features to reinforce expert adaptation across varying test
distributions. We evaluated MATI on four real-world tabular imbalance
regression datasets, including house pricing, bike sharing, and age prediction.
To reflect realistic deployment scenarios, we adopted three types of test
distributions: a balanced distribution with uniform target frequencies, a
normal distribution that follows the training data, and an inverse distribution
that emphasizes rare target regions. On average across these three test
distributions, MATI achieved a 7.1% improvement in MAE compared to existing
methods.

</details>


### [533] [FairPFN: A Tabular Foundation Model for Causal Fairness](https://arxiv.org/abs/2506.07049)
*Jake Robertson,Noah Hollmann,Samuel Müller,Noor Awad,Frank Hutter*

Main category: cs.LG

TL;DR: The paper introduces FairPFN, a tabular foundation model tackling causal fairness without requiring prior knowledge of the causal model, showing effectiveness in removing protected attribute effects.


<details>
  <summary>Details</summary>
Motivation: To address demographic biases in ML systems which perpetuate social inequalities, and to make causal fairness frameworks applicable in scenarios where causal models are unknown.

Method: The authors propose FairPFN, a pre-trained tabular foundation model that uses synthetic causal fairness data and operates without prior knowledge of causal models.

Result: FairPFN demonstrates strong performance in identifying and mitigating the causal effects of protected attributes across diverse scenarios compared to robust baselines.

Conclusion: FairPFN advances causal fairness research by eliminating reliance on known causal models, making fairness solutions more accessible for complex problems.

Abstract: Machine learning (ML) systems are utilized in critical sectors, such as
healthcare, law enforcement, and finance. However, these systems are often
trained on historical data that contains demographic biases, leading to ML
decisions that perpetuate or exacerbate existing social inequalities. Causal
fairness provides a transparent, human-in-the-loop framework to mitigate
algorithmic discrimination, aligning closely with legal doctrines of direct and
indirect discrimination. However, current causal fairness frameworks hold a key
limitation in that they assume prior knowledge of the correct causal model,
restricting their applicability in complex fairness scenarios where causal
models are unknown or difficult to identify. To bridge this gap, we propose
FairPFN, a tabular foundation model pre-trained on synthetic causal fairness
data to identify and mitigate the causal effects of protected attributes in its
predictions. FairPFN's key contribution is that it requires no knowledge of the
causal model and still demonstrates strong performance in identifying and
removing protected causal effects across a diverse set of hand-crafted and
real-world scenarios relative to robust baseline methods. FairPFN paves the way
for promising future research, making causal fairness more accessible to a
wider variety of complex fairness problems.

</details>


### [534] [Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead](https://arxiv.org/abs/2506.07054)
*Uri Koren,Navdeep Kumar,Uri Gadot,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: The paper introduces Policy Gradient with Tree Search (PGTS), showcasing its theoretical and practical advantages in overcoming local optima and enhancing performance in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Classical policy gradient methods often converge to suboptimal solutions, especially in complex environments, motivating the need for methods that can escape such limitations.

Method: The proposed PGTS method incorporates an $m$-step lookahead tree search mechanism, supported by a theoretical analysis of its impact on reducing stationary point issues and practical policy update conditions.

Result: PGTS demonstrates empirical success across various environments, outperforming standard PG by being more farsighted, navigating challenging reward structures, and escaping local optima effectively.

Conclusion: Integrating tree search with policy gradients not only improves worst-case stationary policy performance but also provides a practical, efficient way to address key challenges in reinforcement learning.

Abstract: Classical policy gradient (PG) methods in reinforcement learning frequently
converge to suboptimal local optima, a challenge exacerbated in large or
complex environments. This work investigates Policy Gradient with Tree Search
(PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance
policy optimization. We provide theoretical analysis demonstrating that
increasing the tree search depth $m$-monotonically reduces the set of
undesirable stationary points and, consequently, improves the worst-case
performance of any resulting stationary policy. Critically, our analysis
accommodates practical scenarios where policy updates are restricted to states
visited by the current policy, rather than requiring updates across the entire
state space. Empirical evaluations on diverse MDP structures, including Ladder,
Tightrope, and Gridworld environments, illustrate PGTS's ability to exhibit
"farsightedness," navigate challenging reward landscapes, escape local traps
where standard PG fails, and achieve superior solutions.

</details>


### [535] [E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models](https://arxiv.org/abs/2506.07078)
*Jiaheng Dong,Hong Jia,Soumyajit Chatterjee,Abhirup Ghosh,James Bailey,Ting Dang*

Main category: cs.LG

TL;DR: This paper introduces E-BATS, a memory-efficient and accurate test-time adaptation framework for speech foundation models, addressing performance issues caused by acoustic domain shifts.


<details>
  <summary>Details</summary>
Motivation: Speech foundation models face performance degradation due to real-world acoustic domain shifts like noise and speaker accents.

Method: E-BATS is a backpropagation-free TTA framework utilizing prompt adaptation, a multi-scale loss, and exponential moving average for feature alignment and stable adaptation.

Result: Experiments show 4.1%-13.5% accuracy improvements over prior backpropagation-free methods and 2.0-6.4 times GPU memory savings compared to backpropagation-based methods.

Conclusion: E-BATS provides efficient and scalable adaptation for speech tasks under acoustic variability, offering a viable solution for real-world speech processing systems.

Abstract: Speech Foundation Models encounter significant performance degradation when
deployed in real-world scenarios involving acoustic domain shifts, such as
background noise and speaker accents. Test-time adaptation (TTA) has recently
emerged as a viable strategy to address such domain shifts at inference time
without requiring access to source data or labels. However, existing TTA
approaches, particularly those relying on backpropagation, are
memory-intensive, limiting their applicability in speech tasks and
resource-constrained settings. Although backpropagation-free methods offer
improved efficiency, existing ones exhibit poor accuracy. This is because they
are predominantly developed for vision tasks, which fundamentally differ from
speech task formulations, noise characteristics, and model architecture, posing
unique transferability challenges. In this paper, we introduce E-BATS, the
first Efficient BAckpropagation-free TTA framework designed explicitly for
speech foundation models. E-BATS achieves a balance between adaptation
effectiveness and memory efficiency through three key components: (i)
lightweight prompt adaptation for a forward-pass-based feature alignment, (ii)
a multi-scale loss to capture both global (utterance-level) and local
distribution shifts (token-level) and (iii) a test-time exponential moving
average mechanism for stable adaptation across utterances. Experiments
conducted on four noisy speech datasets spanning sixteen acoustic conditions
demonstrate consistent improvements, with 4.1%-13.5% accuracy gains over
backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to
backpropagation-based methods. By enabling scalable and robust adaptation under
acoustic variability, this work paves the way for developing more efficient
adaptation approaches for practical speech processing systems in real-world
environments.

</details>


### [536] [Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data](https://arxiv.org/abs/2506.07092)
*Joydeb Kumar Sana,Mohammad M. Masud,M Sohel Rahman,M Saifur Rahman*

Main category: cs.LG

TL;DR: This paper proposes a novel method to compute patient similarity using time series and static data while preserving data privacy, resulting in improved prediction performance and reduced computation time.


<details>
  <summary>Details</summary>
Motivation: Patient similarity computation is critical for clinical decision support, but existing methods struggle with performance and scalability challenges, especially when handling big data.

Method: The proposed method combines static demographic data and time series metrics, processes static data with adaptive Weight-of-Evidence (aWOE) and Z-score transformations for privacy and accuracy, and uses distributed Dynamic Time Warping (DTW) for scalable time series similarity measurement.

Result: Performance improves for Coronary Artery Disease by 11.4% (AUC), 10.20% (accuracy), and 12.6% (F-measure), and for Congestive Heart Failure by 15.9% (AUC), 10.5% (accuracy), and 21.9% (F-measure). Computation time decreases by up to 40%.

Conclusion: The method successfully enhances prediction accuracy and scalability, while also preserving sensitive patient data, making it highly applicable for improved healthcare informatics.

Abstract: Patient similarity computation (PSC) is a fundamental problem in healthcare
informatics. The aim of the patient similarity computation is to measure the
similarity among patients according to their historical clinical records, which
helps to improve clinical decision support. This paper presents a novel
distributed patient similarity computation (DPSC) technique based on data
transformation (DT) methods, utilizing an effective combination of time series
and static data. Time series data are sensor-collected patients' information,
including metrics like heart rate, blood pressure, Oxygen saturation,
respiration, etc. The static data are mainly patient background and demographic
data, including age, weight, height, gender, etc. Static data has been used for
clustering the patients. Before feeding the static data to the machine learning
model adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT)
methods have been performed, which improve the prediction performances. In
aWOE-based patient similarity models, sensitive patient information has been
processed using aWOE which preserves the data privacy of the trained models. We
used the Dynamic Time Warping (DTW) approach, which is robust and very popular,
for time series similarity. However, DTW is not suitable for big data due to
the significant computational run-time. To overcome this problem, distributed
DTW computation is used in this study. For Coronary Artery Disease, our DT
based approach boosts prediction performance by as much as 11.4%, 10.20%, and
12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of
Congestive Heart Failure (CHF), our proposed method achieves performance
enhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively.
The proposed method reduces the computation time by as high as 40%.

</details>


### [537] [Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion](https://arxiv.org/abs/2506.07099)
*Wenying He,Jieling Huang,Junhua Gu,Ji Zhang,Yude Bai*

Main category: cs.LG

TL;DR: This paper introduces CoFILL, a Conditional Diffusion Model, for improving spatiotemporal data imputation by leveraging unique dual-stream architecture and diffusion models.


<details>
  <summary>Details</summary>
Motivation: The key motivation is to address challenges in spatiotemporal data integrity caused by hardware and software failures, as current machine learning techniques struggle with spatial-temporal interdependencies and cumulative errors in data imputation.

Method: The paper proposes CoFILL, a diffusion-based model employing a dual-stream architecture to process temporal and frequency domain features simultaneously, capturing rapid fluctuations and underlying patterns to enable superior data imputation.

Result: Extensive experiments demonstrate that CoFILL transforms random noise into meaningful values aligning well with true data distributions and surpasses state-of-the-art imputation methods in accuracy.

Conclusion: CoFILL effectively enhances spatiotemporal data imputation by leveraging its diffusion-based architecture, and its source code has been made publicly available for further use.

Abstract: Missing data in spatiotemporal systems presents a significant challenge for
modern applications, ranging from environmental monitoring to urban traffic
management. The integrity of spatiotemporal data often deteriorates due to
hardware malfunctions and software failures in real-world deployments. Current
approaches based on machine learning and deep learning struggle to model the
intricate interdependencies between spatial and temporal dimensions effectively
and, more importantly, suffer from cumulative errors during the data imputation
process, which propagate and amplify through iterations. To address these
limitations, we propose CoFILL, a novel Conditional Diffusion Model for
spatiotemporal data imputation. CoFILL builds on the inherent advantages of
diffusion models to generate high-quality imputations without relying on
potentially error-prone prior estimates. It incorporates an innovative
dual-stream architecture that processes temporal and frequency domain features
in parallel. By fusing these complementary features, CoFILL captures both rapid
fluctuations and underlying patterns in the data, which enables more robust
imputation. The extensive experiments reveal that CoFILL's noise prediction
network successfully transforms random noise into meaningful values that align
with the true data distribution. The results also show that CoFILL outperforms
state-of-the-art methods in imputation accuracy. The source code is publicly
available at https://github.com/joyHJL/CoFILL.

</details>


### [538] [Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning](https://arxiv.org/abs/2506.07134)
*Eshwar S. R.,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.LG

TL;DR: The paper introduces Reliable Policy Iteration (RPI), a reinforcement learning algorithm addressing issues of monotonic improvement collapse under linear function approximation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the longstanding challenge of instability in reinforcement learning algorithms when combined with function approximation, specifically the failure of policy iteration to guarantee monotonic improvement.

Method: RPI replaces standard policy evaluation techniques with Bellman-based constrained optimization. It ensures monotonicity in value estimates, a lower bound on the true return, and partial satisfaction of the unprojected Bellman equation.

Result: The RPI algorithm is shown to maintain monotonicity and convergence guarantees under function approximation. Its model-free variant enhances policy iteration implementations like DQN and DDPG, achieving strong performance and consistent lower-bound guarantees in control tasks.

Conclusion: Reliable Policy Iteration proves to be a reliable and naturally fitting algorithm within reinforcement learning, overcoming previous challenges and improving both reliability and performance in practical scenarios.

Abstract: Despite decades of research, it remains challenging to correctly use
Reinforcement Learning (RL) algorithms with function approximation. A prime
example is policy iteration, whose fundamental guarantee of monotonic
improvement collapses even under linear function approximation. To address this
issue, we introduce Reliable Policy Iteration (RPI). It replaces the common
projection or Bellman-error minimization during policy evaluation with a
Bellman-based constrained optimization. We prove that not only does RPI confer
textbook monotonicity on its value estimates but these estimates also lower
bound the true return. Also, their limit partially satisfies the unprojected
Bellman equation, emphasizing RPI's natural fit within RL. RPI is the first
algorithm with such monotonicity and convergence guarantees under function
approximation. For practical use, we provide a model-free variant of RPI that
amounts to a novel critic. It can be readily integrated into primary model-free
PI implementations such as DQN and DDPG. In classical control tasks, such
RPI-enhanced variants consistently maintain their lower-bound guarantee while
matching or surpassing the performance of all baseline methods.

</details>


### [539] [AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models](https://arxiv.org/abs/2506.07165)
*Qi Liu,Jingqing Ruan,Hao Li,Haodong Zhao,Desheng Wang,Jiansong Chen,Wan Guanglu,Xunliang Cai,Zhi Zheng,Tong Xu*

Main category: cs.LG

TL;DR: The paper introduces AMoPO, a framework for preference alignment in LLMs using implicit rewards, eliminating the need for external models while showing scalable and superior performance.


<details>
  <summary>Details</summary>
Motivation: There are limitations in current methods for achieving multi-objective preference alignment in large language models, such as ineffective balancing of preferences and dependency on auxiliary models.

Method: AMoPO uses dimension-aware generation metrics as implicit rewards, avoids reliance on additional models, and incorporates an adaptive weight assignment mechanism employing Gaussian distribution modeling.

Result: AMoPO achieves a 28.5% improvement compared to state-of-the-art methods and demonstrates scalability across models of varying sizes (7B, 14B, and 32B). Experiments confirm its adaptability across multiple dimensions.

Conclusion: AMoPO successfully aligns LLMs across diverse preference dimensions dynamically and effectively, providing a computationally efficient and scalable solution.

Abstract: Existing multi-objective preference alignment methods for large language
models (LLMs) face limitations: (1) the inability to effectively balance
various preference dimensions, and (2) reliance on auxiliary reward/reference
models introduces computational complexity. To address these challenges, we
propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel
framework that achieves dynamic balance across preference dimensions. By
introducing the multi-objective optimization paradigm to use the
dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with
diverse preferences without additional reward models or reference models. We
introduce an adaptive weight assignment mechanism that models the generation
space as a Gaussian distribution, allowing dynamic prioritization of preference
dimensions. Empirical results demonstrate that AMoPO outperforms
state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B
models reveal the scaling ability of AMoPO. Moreover, additional analysis of
multiple dimensions verifies its adaptability and effectiveness. These findings
validate AMoPO's capability to achieve dimension-aware preference alignment,
highlighting its superiority. Our codes and datasets are available at
https://github.com/Javkonline/AMoPO.

</details>


### [540] [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
*Huanyi Xie,Lijie Hu,Lu Yu,Tianhao Huang,Longfei Li,Meng Li,Jun Zhou,Huan Wang,Di Wang*

Main category: cs.LG

TL;DR: The paper introduces GAGA, an efficient framework for learning representations in Text-attributed Graphs (TAGs), which requires as little as 1% annotation while maintaining high classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional graph neural networks struggle to handle complex textual information in TAGs, and existing methods reliant on large language models demand extensive annotations or fine-tuning, leading to high time and cost burdens.

Method: GAGA targets only representative nodes and edges for annotation, builds a graph capturing their topological relationships, and uses a two-level alignment module to integrate the annotation graph and the TAG structures.

Result: GAGA achieves classification accuracy comparable to or surpassing state-of-the-art methods while significantly reducing annotation requirements to just 1%.

Conclusion: GAGA proves to be an effective and cost-efficient solution for TAG representation learning, showing promise for scenarios where minimal data annotation is preferred.

Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural
networks (GNNs) often fall short due to the complex textual information
associated with each node. Recent methods have improved node representations by
leveraging large language models (LLMs) to enhance node text features, but
these approaches typically require extensive annotations or fine-tuning across
all nodes, which is both time-consuming and costly. To overcome these
challenges, we introduce GAGA, an efficient framework for TAG representation
learning. GAGA reduces annotation time and cost by focusing on annotating only
representative nodes and edges. It constructs an annotation graph that captures
the topological relationships among these annotations. Furthermore, GAGA
employs a two-level alignment module to effectively integrate the annotation
graph with the TAG, aligning their underlying structures. Experiments show that
GAGA achieves classification accuracies on par with or surpassing
state-of-the-art methods while requiring only 1% of the data to be annotated,
demonstrating its high efficiency.

</details>


### [541] [Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2506.07179)
*Kaiqi Wu,Weiyang Kong,Sen Zhang,Yubao Liu,Zitong Chen*

Main category: cs.LG

TL;DR: The paper proposes RAGL, a traffic prediction model combining regularization and efficient graph computation for improved accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing graph learning methods for traffic prediction face challenges such as lack of regularization for node embeddings and scalability issues.

Method: RAGL synergizes stochastic embedding regularization and adaptive graph convolution, implemented via a residual difference mechanism. It also introduces ECO for scalable graph operations using cosine similarity.

Result: Experiments on large-scale traffic datasets show RAGL improves prediction accuracy and computational efficiency compared to state-of-the-art methods.

Conclusion: RAGL effectively addresses embedding regularization and scalability challenges, enabling accurate and efficient traffic predictions on large networks.

Abstract: Traffic prediction is a critical task in spatial-temporal forecasting with
broad applications in travel planning and urban management. Adaptive graph
convolution networks have emerged as mainstream solutions due to their ability
to learn node embeddings in a data-driven manner and capture complex latent
dependencies. However, existing adaptive graph learning methods for traffic
forecasting often either ignore the regularization of node embeddings, which
account for a significant proportion of model parameters, or face scalability
issues from expensive graph convolution operations. To address these
challenges, we propose a Regularized Adaptive Graph Learning (RAGL) model.
First, we introduce a regularized adaptive graph learning framework that
synergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via
a residual difference mechanism, achieving both embedding regularization and
noise suppression. Second, to ensure scalability on large road networks, we
develop the Efficient Cosine Operator (ECO), which performs graph convolution
based on the cosine similarity of regularized embeddings with linear time
complexity. Extensive experiments on four large-scale real-world traffic
datasets show that RAGL consistently outperforms state-of-the-art methods in
terms of prediction accuracy and exhibits competitive computational efficiency.

</details>


### [542] [Learning based on neurovectors for tabular data: a new neural network approach](https://arxiv.org/abs/2506.07185)
*J. C. Husillos,A. Gallego,A. Roma,A. Troncoso*

Main category: cs.LG

TL;DR: This paper introduces a novel Neurovectors-based learning approach for tabular data, offering competitive performance and enhanced interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional neural networks in adaptability, efficiency, and interpretability for tabular data.

Method: A Neurovectors paradigm is proposed that structures data in vector spaces, replacing backpropagation with energy propagation.

Result: The model demonstrates competitive accuracy in classification and regression tasks on UCI and Kaggle datasets compared to existing machine learning and deep learning approaches.

Conclusion: The Neurovectors model provides an efficient, interpretable learning method for tabular data, offering strong predictive performance on benchmark datasets.

Abstract: In this paper, we present a novel learning approach based on Neurovectors, an
innovative paradigm that structures information through interconnected nodes
and vector relationships for tabular data processing. Unlike traditional
artificial neural networks that rely on weight adjustment through
backpropagation, Neurovectors encode information by structuring data in vector
spaces where energy propagation, rather than traditional weight updates, drives
the learning process, enabling a more adaptable and explainable learning
process. Our method generates dynamic representations of knowledge through
neurovectors, thereby improving both the interpretability and efficiency of the
predictive model. Experimental results using datasets from well-established
repositories such as the UCI machine learning repository and Kaggle are
reported both for classification and regression. To evaluate its performance,
we compare our approach with standard machine learning and deep learning
models, showing that Neurovectors achieve competitive accuracy.

</details>


### [543] [Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach](https://arxiv.org/abs/2506.07191)
*Ramisa Farha,Joshua O. Olukoya*

Main category: cs.LG

TL;DR: This study analyzes breast cancer survival disparities using statistical techniques on SEER 2021 data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to uncover survival disparities among breast cancer patients across racial and geographic lines.

Method: Researchers employ EDA, Kaplan-Meier estimator, log-rank tests, and Cox Proportional Hazards models to identify key survival-related disparities.

Result: The study documents statistical disparities in breast cancer survival linked to race and geography.

Conclusion: Findings serve as a basis to inform interventions aiming to reduce breast cancer treatment and survival inequalities globally.

Abstract: This study employs a robust analytical framework to uncover patterns in
survival outcomes among breast cancer patients from diverse racial and
geographical backgrounds. This research uses the SEER 2021 dataset to analyze
breast cancer survival outcomes to identify and comprehend dissimilarities. Our
approach integrates exploratory data analysis (EDA), through this we identify
key variables that influence survival rates and employ survival analysis
techniques, including the Kaplan-Meier estimator and log-rank test and the
advanced modeling Cox Proportional Hazards model to determine how survival
rates vary across racial groups and countries. Model validation and
interpretation are undertaken to ensure the reliability of our findings, which
are documented comprehensively to inform policymakers and healthcare
professionals. The outcome of this paper is a detailed version of statistical
analysis that not just highlights disparities in breast cancer treatment and
care but also serves as a foundational tool for developing targeted
interventions to address the inequalities effectively. Through this research,
our aim is to contribute to the global efforts to improve breast cancer
outcomes and reduce treatment disparities.

</details>


### [544] [GGBall: Graph Generative Model on Poincaré Ball](https://arxiv.org/abs/2506.07198)
*Tianci Bu,Chuanrui Wang,Hao Ma,Haoren Zheng,Xin Lu,Tailin Wu*

Main category: cs.LG

TL;DR: GGBall leverages hyperbolic geometry to improve graph generation with hierarchical structures, significantly outperforming baselines in preserving topological hierarchies.


<details>
  <summary>Details</summary>
Motivation: Address the difficulty of generating graphs with hierarchical structures due to limitations in Euclidean geometry.

Method: Hyperbolic Vector-Quantized Autoencoder (HVQVAE) combined with Riemannian flow matching prior and hyperbolic graph neural network layers.

Result: Achieved over 75% reduction in degree MMD on Community-Small and over 40% on Ego-Small, showcasing improved structural preservation.

Conclusion: Hyperbolic geometry is a promising foundation for generative modeling in structured and hierarchical data domains.

Abstract: Generating graphs with hierarchical structures remains a fundamental
challenge due to the limitations of Euclidean geometry in capturing exponential
complexity. Here we introduce \textbf{GGBall}, a novel hyperbolic framework for
graph generation that integrates geometric inductive biases with modern
generative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder
(HVQVAE) with a Riemannian flow matching prior defined via closed-form
geodesics. This design enables flow-based priors to model complex latent
distributions, while vector quantization helps preserve the curvature-aware
structure of the hyperbolic space. We further develop a suite of hyperbolic GNN
and Transformer layers that operate entirely within the manifold, ensuring
stability and scalability. Empirically, our model reduces degree MMD by over
75\% on Community-Small and over 40\% on Ego-Small compared to state-of-the-art
baselines, demonstrating an improved ability to preserve topological
hierarchies. These results highlight the potential of hyperbolic geometry as a
powerful foundation for the generative modeling of complex, structured, and
hierarchical data domains. Our code is available at
\href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.

</details>


### [545] [Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward](https://arxiv.org/abs/2506.07218)
*Tong Xiao,Xin Xu,Zhenya Huang,Hongyu Gao,Quan Liu,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: This paper introduces Perception-R1, a method to improve multimodal perception and reasoning in MLLMs using visual perception rewards during RLVR training. The approach achieves state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Reinforcement Learning with Verifiable Rewards (RLVR) methods in MLLMs fail to enhance multimodal perception, limiting reasoning capabilities.

Method: The authors collect textual visual annotations from multimodal problem-solving trajectories, use a judging LLM to assign visual perception rewards based on consistency judgments, and employ these rewards during RLVR training.

Result: Experiments show that Perception-R1 achieves superior performance on various multimodal reasoning benchmarks, using only a small dataset of 1,442 samples.

Conclusion: Perception-R1 effectively enhances both perception and reasoning in MLLMs, addressing a critical limitation of previous RLVR methods.

Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language
Models (MLLMs) is a challenging task that has attracted increasing attention in
the community. Recently, several studies have applied Reinforcement Learning
with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the
reasoning abilities of MLLMs. However, these works largely overlook the
enhancement of multimodal perception capabilities in MLLMs, which serve as a
core prerequisite and foundational component of complex multimodal reasoning.
Through McNemar's test, we find that existing RLVR method fails to effectively
enhance the multimodal perception capabilities of MLLMs, thereby limiting their
further improvement in multimodal reasoning. To address this limitation, we
propose Perception-R1, which introduces a novel visual perception reward that
explicitly encourages MLLMs to perceive the visual content accurately, thereby
can effectively incentivizing both their multimodal perception and reasoning
capabilities. Specifically, we first collect textual visual annotations from
the CoT trajectories of multimodal problems, which will serve as visual
references for reward assignment. During RLVR training, we employ a judging LLM
to assess the consistency between the visual annotations and the responses
generated by MLLM, and assign the visual perception reward based on these
consistency judgments. Extensive experiments on several multimodal reasoning
benchmarks demonstrate the effectiveness of our Perception-R1, which achieves
state-of-the-art performance on most benchmarks using only 1,442 training data.

</details>


### [546] [VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution](https://arxiv.org/abs/2506.07229)
*Mateusz Gajewski,Mikołaj Morzy,Adam Karczmarz,Piotr Sankowski*

Main category: cs.LG

TL;DR: A new feature attribution method called VARSHAP is introduced, which focuses on prediction variance reduction and outperforms SHAP and LIME.


<details>
  <summary>Details</summary>
Motivation: Existing feature attribution methods fail to accurately capture local model behavior due to global dependence.

Method: The paper presents VARSHAP, a Shapley value-based method that uses prediction variance reduction for feature importance, while also addressing global distribution shift challenges.

Result: VARSHAP outperforms KernelSHAP and LIME in both quantitative and qualitative evaluations on synthetic and real-world data.

Conclusion: VARSHAP is a robust and reliable local feature attribution method that adheres to Shapley axioms and is resilient to global distribution changes, making it better suited for practical applications.

Abstract: Existing feature attribution methods like SHAP often suffer from global
dependence, failing to capture true local model behavior. This paper introduces
VARSHAP, a novel model-agnostic local feature attribution method which uses the
reduction of prediction variance as the key importance metric of features.
Building upon Shapley value framework, VARSHAP satisfies the key Shapley
axioms, but, unlike SHAP, is resilient to global data distribution shifts.
Experiments on synthetic and real-world datasets demonstrate that VARSHAP
outperforms popular methods such as KernelSHAP or LIME, both quantitatively and
qualitatively.

</details>


### [547] [Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs](https://arxiv.org/abs/2506.07240)
*Roy Eisenstadt,Itamar Zimerman,Lior Wolf*

Main category: cs.LG

TL;DR: This paper studies how Large Language Models (LLMs) regulate the length of their reasoning processes and proposes a method to optimize reasoning duration for better performance.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenges in balancing reasoning length in LLMs: reasoning that's too short can omit important task complexity, and overly long reasoning can lead to degraded performance and inefficiency.

Method: The authors investigate how LLMs encode reasoning progress, introduce an interactive progress bar to visualize this process, and develop a technique to manipulate internal progress encoding during inference to limit unnecessary reasoning.

Result: The proposed method, termed 'overclocking,' enhances answer accuracy, reduces inference latency, and prevents overthinking compared to standard reasoning approaches.

Conclusion: Understanding and controlling reasoning length in LLMs can optimize their performance, as demonstrated by the ability to better manage computational resources and improve decision-making accuracy.

Abstract: Recently, techniques such as explicit structured reasoning have demonstrated
strong test-time scaling behavior by enforcing a separation between the model's
internal "thinking" process and the final response. A key factor influencing
answer quality in this setting is the length of the thinking stage. When the
reasoning is too short, the model may fail to capture the complexity of the
task. Conversely, when it is too long, the model may overthink, leading to
unnecessary computation and degraded performance. This paper explores and
exploits the underlying mechanisms by which LLMs understand and regulate the
length of their reasoning during explicit thought processes. First, we show
that LLMs encode their progress through the reasoning process and introduce an
interactive progress bar visualization, which is then used to reveal insights
on the model's planning dynamics. Second, we manipulate the internal progress
encoding during inference to reduce unnecessary steps and generate a more
concise and decisive chain of thoughts. Our empirical results demonstrate that
this "overclocking" method mitigates overthinking, improves answer accuracy,
and reduces inference latency. Our code is publicly available.

</details>


### [548] [Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models](https://arxiv.org/abs/2506.07247)
*Ngoc-Quan Pham,Tuan Truong,Quyen Tran,Tan Nguyen,Dinh Phung,Trung Le*

Main category: cs.LG

TL;DR: This paper presents Interactive Bayesian Distributional Robustness (IBDR), which improves particle diversity and ensemble quality in Bayesian inference. IBDR outperforms baselines in tasks like VTAB-1K and language reasoning.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations in particle diversity and ensemble quality in existing Bayesian inference frameworks by introducing interactions between particles.

Method: IBDR is developed on a generalized theory linking distributional population loss with the approximate posterior. A dual optimization procedure is used to ensure distributional robustness and enhance diversity.

Result: IBDR achieves superior performance compared to baseline methods in the VTAB-1K benchmark and common reasoning language tasks.

Conclusion: IBDR is an effective and robust Bayesian inference framework with improved diversity and performance, making it suitable for real-world tasks.

Abstract: We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel
Bayesian inference framework that allows modeling the interactions between
particles, thereby enhancing ensemble quality through increased particle
diversity. IBDR is grounded in a generalized theoretical framework that
connects the distributional population loss with the approximate posterior,
motivating a practical dual optimization procedure that enforces distributional
robustness while fostering particle diversity. We evaluate IBDR's performance
against various baseline methods using the VTAB-1K benchmark and the common
reasoning language task. The results consistently show that IBDR outperforms
these baselines, underscoring its effectiveness in real-world applications.

</details>


### [549] [A Stable Whitening Optimizer for Efficient Neural Network Training](https://arxiv.org/abs/2506.07254)
*Kevin Frans,Sergey Levine,Pieter Abbeel*

Main category: cs.LG

TL;DR: This paper proposes SPlus, an enhanced neural network optimization method addressing issues in Shampoo, achieving stability, efficiency, and faster learning.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in Shampoo optimization algorithms, such as divergence, high computational requirements, and inefficiencies with high learning rates.

Method: Enhancements to Shampoo include bounded updates for stability, shape-aware scaling for better learning rate transfer, and iterate-averaging for handling parameter noise.

Result: SPlus achieves improved validation performance in significantly fewer gradient steps (44%) and reduced wallclock time (62%) compared to Adam optimizer.

Conclusion: The proposed SPlus method is a more effective and efficient alternative for neural network optimization, validated through benchmarks in diverse tasks.

Abstract: In this work, we take an experimentally grounded look at neural network
optimization. Building on the Shampoo family of algorithms, we identify and
alleviate three key issues, resulting in the proposed SPlus method. First, we
find that naive Shampoo is prone to divergence when matrix-inverses are cached
for long periods. We introduce an alternate bounded update combining a
historical eigenbasis with instantaneous normalization, resulting in
across-the-board stability and significantly lower computational requirements.
Second, we adapt a shape-aware scaling to enable learning rate transfer across
network width. Third, we find that high learning rates result in large
parameter noise, and propose a simple iterate-averaging scheme which unblocks
faster learning. To properly confirm these findings, we introduce a pointed
Transformer training benchmark, considering three objectives (language
modelling, image classification, and diffusion modelling) across different
stages of training. On average, SPlus is able to reach the validation
performance of Adam within 44% of the gradient steps and 62% of the wallclock
time.

</details>


### [550] [A Cramér-von Mises Approach to Incentivizing Truthful Data Sharing](https://arxiv.org/abs/2506.07272)
*Alex Clinton,Thomas Zeng,Yiding Chen,Xiaojin Zhu,Kirthevasan Kandasamy*

Main category: cs.LG

TL;DR: This paper develops incentive mechanisms using a novel two-sample test to promote truthful data sharing in marketplaces while addressing limitations of prior methods relying on distributional assumptions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of data manipulation in modern data marketplaces, where agents inflate rewards by submitting low-quality or fabricated data due to incentives based on data quantity.

Method: The authors propose reward mechanisms grounded in a novel two-sample test inspired by the Cramér-von Mises statistic, ensuring agents are incentivized to contribute genuine data while discouraging fabrication.

Result: Theoretical results establish that truthful reporting can represent a Nash equilibrium in various settings, and empirical tests on simulations, language data, and image data demonstrate the mechanism effectively promotes honest data sharing.

Conclusion: The developed mechanism relaxes restrictive assumptions from prior models and successfully fosters truthful and high-quality data contributions in data-sharing environments.

Abstract: Modern data marketplaces and data sharing consortia increasingly rely on
incentive mechanisms to encourage agents to contribute data. However, schemes
that reward agents based on the quantity of submitted data are vulnerable to
manipulation, as agents may submit fabricated or low-quality data to inflate
their rewards. Prior work has proposed comparing each agent's data against
others' to promote honesty: when others contribute genuine data, the best way
to minimize discrepancy is to do the same. Yet prior implementations of this
idea rely on very strong assumptions about the data distribution (e.g.
Gaussian), limiting their applicability. In this work, we develop reward
mechanisms based on a novel, two-sample test inspired by the Cram\'er-von Mises
statistic. Our methods strictly incentivize agents to submit more genuine data,
while disincentivizing data fabrication and other types of untruthful
reporting. We establish that truthful reporting constitutes a (possibly
approximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We
theoretically instantiate our method in three canonical data sharing problems
and show that it relaxes key assumptions made by prior work. Empirically, we
demonstrate that our mechanism incentivizes truthful data sharing via
simulations and on real-world language and image data.

</details>


### [551] [Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models](https://arxiv.org/abs/2506.07275)
*Haochen Song,Dominik Hofer,Rania Islambouli,Laura Hawkins,Ananya Bhattacharjee,Meredith Franklin,Joseph Jay Williams*

Main category: cs.LG

TL;DR: The study proposes a hybrid approach combining contextual multi-armed bandit (cMAB) algorithms and large language models (LLMs) to deliver personalized motivational messages for increasing physical activity. Different models are tested over a seven-day trial.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of cMAB algorithms that require large participant samples and overlook psychological factors in their models, aiming to reduce sedentary behavior through personalized interventions.

Method: A hybrid approach combining cMAB for intervention type selection and LLMs for personalizing message content was developed. Four intervention types and dynamically personalized messages are tested using daily contextual factors. Daily step count and message acceptance are assessed via ecological momentary assessments (EMAs) over a seven-day trial.

Result: The study provides new insights into how combining cMAB adaptation and LLM-based personalization can collaboratively promote physical activity in participants.

Conclusion: The hybrid approach of using cMAB and LLMs shows potential in enhancing motivation for physical activity through personalized behavioral messaging, emphasizing LLM's role in personalization and cMAB's adaptability.

Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB)
algorithms, offer a promising strategy to reduce sedentary behavior by
delivering personalized interventions to encourage physical activity. However,
cMAB algorithms typically require large participant samples to learn
effectively and may overlook key psychological factors that are not explicitly
encoded in the model. In this study, we propose a hybrid approach that combines
cMAB for selecting intervention types with large language models (LLMs) to
personalize message content. We evaluate four intervention types: behavioral
self-monitoring, gain-framed, loss-framed, and social comparison, each
delivered as a motivational message aimed at increasing motivation for physical
activity and daily step count. Message content is further personalized using
dynamic contextual factors including daily fluctuations in self-efficacy,
social influence, and regulatory focus. Over a seven-day trial, participants
receive daily messages assigned by one of four models: cMAB alone, LLM alone,
combined cMAB with LLM personalization (cMABxLLM), or equal randomization
(RCT). Outcomes include daily step count and message acceptance, assessed via
ecological momentary assessments (EMAs). We apply a causal inference framework
to evaluate the effects of each model. Our findings offer new insights into the
complementary roles of LLM-based personalization and cMAB adaptation in
promoting physical activity through personalized behavioral messaging.

</details>


### [552] [Tokenized Bandit for LLM Decoding and Alignment](https://arxiv.org/abs/2506.07276)
*Suho Shin,Chenghao Yang,Haifeng Xu,Mohammad T. Hajiaghayi*

Main category: cs.LG

TL;DR: The paper introduces tokenized linear/multi-armed bandit frameworks inspired by LLM decoding, proving the effectiveness of greedy decoding under specific assumptions and proposing algorithms with provable regret bounds.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address sequential token selection tasks inspired by LLM decoding, to understand why greedy decoding is effective, and to apply this understanding to improve LLM alignment during decoding.

Method: The authors design algorithms for tokenized bandit settings under a proposed DDMC (Diminishing Distance with More Commons) assumption, analyze their regret bounds for TLB and TMAB, and verify these empirically on synthetic and real-world datasets.

Result: They achieve regret bounds of $\tilde{O}(L\sqrt{T})$ for TLB and $\tilde{O}(L\sqrt{T^{2/3}})$ for TMAB, demonstrate the near-optimality of greedy LLM decoding under DDMC, and validate their algorithms' performance through experiments.

Conclusion: The study provides theoretical justification for the effectiveness of greedy decoding in LLM applications, proposes algorithms with formal guarantees, and offers insights into applying these methods for decoding-time LLM alignment.

Abstract: We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB),
variants of linear and stochastic multi-armed bandit problems inspired by LLM
decoding and alignment. In these problems, at each round $t \in [T]$, a user
submits a query (context), and the decision maker (DM) sequentially selects a
token irrevocably from a token set. Once the sequence is complete, the DM
observes a random utility from the user, whose expectation is presented by a
sequence function mapping the chosen token sequence to a nonnegative real value
that depends on the query.
  In both problems, we first show that learning is impossible without any
structure on the sequence function. We introduce a natural assumption,
diminishing distance with more commons (DDMC), and propose algorithms with
regret $\tilde{O}(L\sqrt{T})$ and $\tilde{O}(L\sqrt{T^{2/3}})$ for TLB and
TMAB, respectively. As a side product, we obtain an (almost) optimality of the
greedy decoding for LLM decoding algorithm under DDMC, which justifies the
unresaonable effectiveness of greedy decoding in several tasks. This also has
an immediate application to decoding-time LLM alignment, when the misaligned
utility can be represented as the frozen LLM's utility and a linearly
realizable latent function. We finally validate our algorithm's performance
empirically as well as verify our assumptions using synthetic and real-world
datasets.

</details>


### [553] [EviNet: Evidential Reasoning Network for Resilient Graph Learning in the Open and Noisy Environments](https://arxiv.org/abs/2506.07288)
*Weijie Guan,Haohui Wang,Jian Kang,Lihui Liu,Dawei Zhou*

Main category: cs.LG

TL;DR: The paper proposes "EVINET," a framework for open-world graph learning challenges like misclassification and out-of-distribution detection using Beta embedding and subjective logic.


<details>
  <summary>Details</summary>
Motivation: Existing graph learning models operate under a closed-world assumption, ignoring the challenges posed by open and noisy environments. This creates a need for systems capable of detecting both misclassifications in known data and identifying out-of-distribution data.

Method: The authors developed the Evidential Reasoning Network (EVINET), which includes components like Dissonance Reasoning for detecting misclassifications and Vacuity Reasoning for out-of-distribution detection. It integrates Beta embedding within a subjective logic framework.

Result: EVINET outperformed state-of-the-art methods across multiple benchmarks, successfully addressing in-distribution classification, misclassification detection, and out-of-distribution detection tasks.

Conclusion: EVINET emphasizes the importance of uncertainty quantification and logical reasoning in open-world graph learning, setting the groundwork for robust graph-based machine learning systems in dynamic environments.

Abstract: Graph learning has been crucial to many real-world tasks, but they are often
studied with a closed-world assumption, with all possible labels of data known
a priori. To enable effective graph learning in an open and noisy environment,
it is critical to inform the model users when the model makes a wrong
prediction to in-distribution data of a known class, i.e., misclassification
detection or when the model encounters out-of-distribution from novel classes,
i.e., out-of-distribution detection. This paper introduces Evidential Reasoning
Network (EVINET), a framework that addresses these two challenges by
integrating Beta embedding within a subjective logic framework. EVINET includes
two key modules: Dissonance Reasoning for misclassification detection and
Vacuity Reasoning for out-of-distribution detection. Extensive experiments
demonstrate that EVINET outperforms state-of-the-art methods across multiple
metrics in the tasks of in-distribution classification, misclassification
detection, and out-of-distribution detection. EVINET demonstrates the necessity
of uncertainty estimation and logical reasoning for misclassification detection
and out-of-distribution detection and paves the way for open-world graph
learning. Our code and data are available at https://github.com/SSSKJ/EviNET.

</details>


### [554] [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298)
*Yijia Dai,Zhaolin Gao,Yahya Satter,Sarah Dean,Jennifer J. Sun*

Main category: cs.LG

TL;DR: The paper explores how large language models (LLMs) can utilize in-context learning (ICL) to effectively model data generated by Hidden Markov Models (HMMs), achieving near-theoretical accuracy and competitive performance on real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Understanding and enhancing the use of large language models for uncovering latent patterns in sequential data beyond conventional computational methods.

Method: The authors tested pre-trained LLMs on synthetic datasets generated by HMMs, evaluated scaling trends, and compared their predictive accuracy with theoretical benchmarks and human expert-designed models.

Result: LLMs demonstrated near-theoretical predictive accuracy on synthetic HMM data and performed competitively on real-world animal decision-making tasks.

Conclusion: This research establishes the capability of in-context learning in LLMs to uncover hidden Markovian structures in sequential data, empowering scientists with a practical diagnostic tool for complex data.

Abstract: Hidden Markov Models (HMMs) are foundational tools for modeling sequential
data with latent Markovian structure, yet fitting them to real-world data
remains computationally challenging. In this work, we show that pre-trained
large language models (LLMs) can effectively model data generated by HMMs via
in-context learning (ICL)$\unicode{x2013}$their ability to infer patterns from
examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve
predictive accuracy approaching the theoretical optimum. We uncover novel
scaling trends influenced by HMM properties, and offer theoretical conjectures
for these empirical observations. We also provide practical guidelines for
scientists on using ICL as a diagnostic tool for complex data. On real-world
animal decision-making tasks, ICL achieves competitive performance with models
designed by human experts. To our knowledge, this is the first demonstration
that ICL can learn and predict HMM-generated sequences$\unicode{x2013}$an
advance that deepens our understanding of in-context learning in LLMs and
establishes its potential as a powerful tool for uncovering hidden structure in
complex scientific data.

</details>


### [555] [Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference](https://arxiv.org/abs/2506.07311)
*Thomas Joshi,Herman Saini,Neil Dhillon,Antoni Viros i Martin,Kaoutar El Maghraoui*

Main category: cs.LG

TL;DR: This paper addresses memory inefficiencies in Large Language Models (LLMs) during long-context inference by introducing PagedAttention integrated with PyTorch FlexAttention for efficient gathering of KV caches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the problem of memory inefficiency and latency in long-context inference in LLMs, caused by conventional KV cache handling.

Method: The researchers integrate PagedAttention with PyTorch FlexAttention within IBM's Foundation Model Stack to address memory fragmentation and inefficiencies. Their approach optimizes the handling of scattered KV cache data.

Result: Benchmarks show reduced inference latency growing linearly (~2x) with sequence length when using a global KV cache. Memory usage for single-step evaluations remains stable, with minimal incremental memory impact observed only at longer sequence lengths (2048+ tokens).

Conclusion: The results demonstrate significant latency improvements and efficient memory management for long-context sequence handling in LLMs. The open-sourcing of their implementation highlights its potential value for future model deployment.

Abstract: Large Language Models (LLMs) encounter severe memory inefficiencies during
long-context inference due to conventional handling of key-value (KV) caches.
In this work, we introduce a novel integration of PagedAttention with PyTorch's
FlexAttention, addressing internal fragmentation and inefficiencies associated
with monolithic KV cache allocations. Implemented within IBM's Foundation Model
Stack (FMS), our fused attention kernel efficiently gathers scattered KV data.
Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced
inference latency, growing only linearly (~2x) with sequence length from 128 to
2048 tokens when utilizing a global KV cache, compared to exponential latency
increases without caching. While peak memory usage remains largely unchanged
for single-step evaluations (dominated by model weights and activations), paged
attention causes minimal incremental memory usage, observable only at sequence
lengths exceeding 2048 tokens due to its power-of-two cache allocations. We
open-source the full implementation and discuss its implications for future
long-context model deployment.

</details>


### [556] [Generative Modeling of Networked Time-Series via Transformer Architectures](https://arxiv.org/abs/2506.07312)
*Yusuf Elnady*

Main category: cs.LG

TL;DR: The paper proposes a transformer-based generative model for expanding time-series datasets in security and network applications, addressing the challenge of limited data with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Limited availability of large datasets in the security and network domains hinders the training of effective machine learning models.

Method: The authors developed a novel, efficient transformer-based generative model designed to produce high-quality, generalizable synthetic time-series data.

Result: The proposed transformer-based model outperforms prior approaches, achieving state-of-the-art results in generating data that enhances ML workflow performance.

Conclusion: By synthesizing high-quality, generalizable datasets, the paper offers a solution to improve machine learning performance in security and network applications, alleviating data scarcity issues.

Abstract: Many security and network applications require having large datasets to train
the machine learning models. Limited data access is a well-known problem in the
security domain. Recent studies have shown the potential of Transformer models
to enlarge the size of data by synthesizing new samples, but the synthesized
samples don't improve the models over the real data. To address this issue, we
design an efficient transformer-based model as a generative framework to
generate time-series data, that can be used to boost the performance of
existing and new ML workflows. Our new transformer model achieves the SOTA
results. We style our model to be generalizable and work across different
datasets, and produce high-quality samples.

</details>


### [557] [DEF: Diffusion-augmented Ensemble Forecasting](https://arxiv.org/abs/2506.07324)
*David Millard,Arielle Carr,Stéphane Gaudreault,Ali Baheri*

Main category: cs.LG

TL;DR: DEF is a method for creating meaningful perturbations for machine learning in weather prediction, improving long-term forecast accuracy and distribution reliability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of generalized stochastic models for initial condition perturbations in machine learning-based weather prediction.

Method: The proposed method uses a conditional diffusion model to generate structured perturbations, applied iteratively, with guidance terms for controlling perturbation levels.

Result: DEF results in better long-term forecast accuracy and produces meaningful forecast distributions, validated on ERA5 reanalysis dataset.

Conclusion: DEF offers a standardized way to convert deterministic neural forecasting systems into stochastic ones, enhancing prediction performance and reliability.

Abstract: We present DEF (\textbf{\ul{D}}iffusion-augmented \textbf{\ul{E}}nsemble
\textbf{\ul{F}}orecasting), a novel approach for generating initial condition
perturbations. Modern approaches to initial condition perturbations are
primarily designed for numerical weather prediction (NWP) solvers, limiting
their applicability in the rapidly growing field of machine learning for
weather prediction. Consequently, stochastic models in this domain are often
developed on a case-by-case basis. We demonstrate that a simple conditional
diffusion model can (1) generate meaningful structured perturbations, (2) be
applied iteratively, and (3) utilize a guidance term to intuitivey control the
level of perturbation. This method enables the transformation of any
deterministic neural forecasting system into a stochastic one. With our
stochastic extended systems, we show that the model accumulates less error over
long-term forecasts while producing meaningful forecast distributions. We
validate our approach on the 5.625$^\circ$ ERA5 reanalysis dataset, which
comprises atmospheric and surface variables over a discretized global grid,
spanning from the 1960s to the present. On this dataset, our method
demonstrates improved predictive performance along with reasonable spread
estimates.

</details>


### [558] [Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification](https://arxiv.org/abs/2506.07328)
*Jintao Yan,Tan Chen,Yuxuan Sun,Zhaojun Nan,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: The paper introduces a Mobility-Aware Dynamic Sparsification (MADS) algorithm to address convergence challenges in Asynchronous Federated Learning (AFL) due to mobility and model staleness.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of AFL convergence caused by intermittent connectivity, model staleness, and gradient sparsification in mobile environments.

Method: The paper presents the MADS algorithm, which dynamically adjusts sparsification based on mobility patterns and model staleness. Theoretical modeling and experimental validation are provided.

Result: MADS improves image classification accuracy on CIFAR-10 by 8.76% and reduces trajectory prediction error in Argoverse dataset by 9.46%.

Conclusion: Dynamic sparsification driven by mobility considerations significantly enhances AFL performance in mobile environments.

Abstract: Asynchronous Federated Learning (AFL) enables distributed model training
across multiple mobile devices, allowing each device to independently update
its local model without waiting for others. However, device mobility introduces
intermittent connectivity, which necessitates gradient sparsification and leads
to model staleness, jointly affecting AFL convergence. This paper develops a
theoretical model to characterize the interplay among sparsification, model
staleness and mobility-induced contact patterns, and their joint impact on AFL
convergence. Based on the analysis, we propose a mobility-aware dynamic
sparsification (MADS) algorithm that optimizes the sparsification degree based
on contact time and model staleness. Closed-form solutions are derived, showing
that under low-speed conditions, MADS increases the sparsification degree to
enhance convergence, while under high-speed conditions, it reduces the
sparsification degree to guarantee reliable uploads within limited contact
time. Experimental results validate the theoretical findings. Compared with the
state-of-the-art benchmarks, the MADS algorithm increases the image
classification accuracy on the CIFAR-10 dataset by 8.76% and reduces the
average displacement error in the Argoverse trajectory prediction dataset by
9.46%.

</details>


### [559] [JavelinGuard: Low-Cost Transformer Architectures for LLM Security](https://arxiv.org/abs/2506.07330)
*Yash Datta,Sharath Rajasekar*

Main category: cs.LG

TL;DR: JavelinGuard introduces high-performance, low-cost transformer architectures for detecting malicious intent in LLM interactions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for efficient and accurate mechanisms to detect malicious intent in interactions involving large language models (LLMs), focusing on practical deployment in production settings.

Method: The authors developed and analyzed five transformer-based architectures, systematically benchmarking them on nine adversarial datasets, including a newly introduced dataset, JavelinBench, and comparing against current guardrail models.

Result: The proposed architectures, particularly Raudra, outperformed existing approaches in terms of cost-performance trade-offs, with specific strengths in accuracy and latency.

Conclusion: The research offers a scalable framework with diverse model options, enabling informed trade-offs in speed, resource usage, and performance for LLM security applications.

Abstract: We present JavelinGuard, a suite of low-cost, high-performance model
architectures designed for detecting malicious intent in Large Language Model
(LLM) interactions, optimized specifically for production deployment. Recent
advances in transformer architectures, including compact BERT(Devlin et al.
2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build
highly accurate classifiers with as few as approximately 400M parameters that
achieve rapid inference speeds even on standard CPU hardware. We systematically
explore five progressively sophisticated transformer-based architectures:
Sharanga (baseline transformer classifier), Mahendra (enhanced
attention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid
neural ensemble architectures), and Raudra (an advanced multi-task framework
with specialized loss functions). Our models are rigorously benchmarked across
nine diverse adversarial datasets, including popular sets like the NotInject
series, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly
introduced JavelinBench, specifically crafted to test generalization on
challenging borderline and hard-negative cases. Additionally, we compare our
architectures against leading open-source guardrail models as well as large
decoder-only LLMs such as gpt-4o, demonstrating superior cost-performance
trade-offs in terms of accuracy, and latency. Our findings reveal that while
Raudra's multi-task design offers the most robust performance overall, each
architecture presents unique trade-offs in speed, interpretability, and
resource requirements, guiding practitioners in selecting the optimal balance
of complexity and efficiency for real-world LLM security applications.

</details>


### [560] [Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models](https://arxiv.org/abs/2506.07334)
*Haoyu Wang,Peihao Wang,Mufei Li,Shikun Liu,Siqi Miao,Zhangyang Wang,Pan Li*

Main category: cs.LG

TL;DR: Graph-KV is proposed to address the limitations of serializing input sequences in large language models by introducing graph-structured attention and reducing positional bias, demonstrating significant improvements across tasks requiring structural dependencies.


<details>
  <summary>Details</summary>
Motivation: The serialization of input in existing large language models hampers their use of structural inductive biases, especially in tasks like retrieval-augmented generation and data reasoning with graph-like dependencies. This limitation restricts efficient task performance.

Method: Graph-KV leverages KV-caches of text segments with structural inductive biases. Instead of attending to all prior segments in a serialized sequence, 'target' segments attend only to the KV-cache of their 'source' segments, forming a graph-structured block mask. Positional encodings are also employed strategically to reduce biases and enhance context efficiency.

Result: Graph-KV was tested on seven RAG benchmarks, a novel academic QA task (Arxiv-QA), and topic classification within a citation network, outperforming sequential encoding baselines in performance and efficiency.

Conclusion: Graph-KV demonstrates the capability to improve large-language-model tasks by incorporating graph-based structural biases and extending their efficiency while reducing context consumption and positional bias.

Abstract: Modern large language models (LLMs) are inherently auto-regressive, requiring
input to be serialized into flat sequences regardless of their structural
dependencies. This serialization hinders the model's ability to leverage
structural inductive biases, especially in tasks such as retrieval-augmented
generation (RAG) and reasoning on data with native graph structures, where
inter-segment dependencies are crucial. We introduce Graph-KV with the
potential to overcome this limitation. Graph-KV leverages the KV-cache of text
segments as condensed representations and governs their interaction through
structural inductive biases. In this framework, 'target' segments selectively
attend only to the KV-caches of their designated 'source' segments, rather than
all preceding segments in a serialized sequence. This approach induces a
graph-structured block mask, sparsifying attention and enabling a
message-passing-like step within the LLM. Furthermore, strategically allocated
positional encodings for source and target segments reduce positional bias and
context window consumption. We evaluate Graph-KV across three scenarios: (1)
seven RAG benchmarks spanning direct inference, multi-hop reasoning, and
long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with
full-text scientific papers structured as citation ego-graphs; and (3) paper
topic classification within a citation network. By effectively reducing
positional bias and harnessing structural inductive biases, Graph-KV
substantially outperforms baselines, including standard costly sequential
encoding, across various settings. Code and the Graph-KV data are publicly
available.

</details>


### [561] [SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments](https://arxiv.org/abs/2506.07355)
*Yuya Okada,Takayuki Nishio*

Main category: cs.LG

TL;DR: SALT (Split-Adaptive Lightweight Tuning) introduces an adapter to enable personalized model adaptation in closed environments without requiring direct access to model parameters.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of personalizing AI models in closed environments like edge-cloud systems where model access is restricted.

Method: Design a compact, trainable client-side adapter to refine latent features, allowing user-specific adaptation without altering original models.

Result: SALT demonstrates improved accuracy and lower training latency on classification tasks with CIFAR datasets, while also supporting robustness against lossy networks.

Conclusion: SALT provides an efficient and practical framework for personalized inference in edge AI systems under strict constraints, ensuring adaptability and robustness with minimal overhead.

Abstract: We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model
adaptation framework for Split Computing under closed constraints, where the
head and tail networks are proprietary and inaccessible to users. In such
closed environments, conventional adaptation methods are infeasible since they
require access to model parameters or architectures. SALT addresses this
challenge by introducing a compact, trainable adapter on the client side to
refine latent features from the head network, enabling user-specific adaptation
without modifying the original models or increasing communication overhead. We
evaluate SALT on user-specific classification tasks with CIFAR-10 and
CIFAR-100, demonstrating improved accuracy with lower training latency compared
to fine-tuning methods. Furthermore, SALT facilitates model adaptation for
robust inference over lossy networks, a common challenge in edge-cloud
environments. With minimal deployment overhead, SALT offers a practical
solution for personalized inference in edge AI systems under strict system
constraints.

</details>


### [562] [RiemannFormer: A Framework for Attention in Curved Spaces](https://arxiv.org/abs/2506.07405)
*Zhongping Ji*

Main category: cs.LG

TL;DR: This paper introduces a framework to enhance transformer-based architectures by interpreting attention mechanisms geometrically and optimizing parameter usage.


<details>
  <summary>Details</summary>
Motivation: The paper aims to provide a geometric understanding of the attention mechanism in transformers and alleviate shortcomings like neglecting local inductive bias.

Method: The authors utilize geometric concepts like metric tensors and parallel transport, reduce parameters via predefined configurations, and integrate a neighborhood focusing mechanism.

Result: Experiments show significant performance improvements compared to baseline models, with plans for future evaluations on visual and language models.

Conclusion: The proposed method successfully enhances the efficiency and performance of transformers, opening avenues for further evaluations and applications.

Abstract: This research endeavors to offer insights into unlocking the further
potential of transformer-based architectures. One of the primary motivations is
to offer a geometric interpretation for the attention mechanism in
transformers. In our framework, the attention mainly involves metric tensors,
tangent spaces, inner product, and how they relate to each other. These
quantities and structures at discrete positions are intricately interconnected
via the parallel transport of tangent vectors. To make the learning process
more efficient, we reduce the number of parameters through ingenious predefined
configurations. Moreover, we introduce an explicit mechanism to highlight a
neighborhood by attenuating the remote values, given that transformers
inherently neglect local inductive bias. Experimental results demonstrate that
our modules deliver significant performance improvements relative to the
baseline. More evaluation experiments on visual and large language models will
be launched successively.

</details>


### [563] [InverseScope: Scalable Activation Inversion for Interpreting Large Language Models](https://arxiv.org/abs/2506.07406)
*Yifan Luo,Zhennan Zhou,Bin Dong*

Main category: cs.LG

TL;DR: This paper introduces InverseScope, a scalable framework for interpreting LLM neural activations by inverting inputs while requiring fewer assumptions and improving sampling efficiency.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs internally represent information is crucial for advancing interpretability research, but current methods rely on potentially invalid assumptions.

Method: The proposed framework, InverseScope, analyzes distributions of inputs that produce similar neural activations and uses a new conditional generation architecture to improve sampling efficiency. A quantitative evaluation protocol is also introduced to probe interpretability hypotheses.

Result: InverseScope enables scalable and efficient analysis of internal representations in LLMs, allowing for application on large models and realistic tasks.

Conclusion: The framework enhances the practicality and accuracy of methods to interpret neural activations in LLMs by reducing assumptions, improving efficiency, and offering quantitative analysis.

Abstract: Understanding the internal representations of large language models (LLMs) is
a central challenge in interpretability research. Existing feature
interpretability methods often rely on strong assumptions about the structure
of representations that may not hold in practice. In this work, we introduce
InverseScope, an assumption-light and scalable framework for interpreting
neural activations via input inversion. Given a target activation, we define a
distribution over inputs that generate similar activations and analyze this
distribution to infer the encoded features. To address the inefficiency of
sampling in high-dimensional spaces, we propose a novel conditional generation
architecture that significantly improves sample efficiency compared to previous
methods. We further introduce a quantitative evaluation protocol that tests
interpretability hypotheses using feature consistency rate computed over the
sampled inputs. InverseScope scales inversion-based interpretability methods to
larger models and practical tasks, enabling systematic and quantitative
analysis of internal representations in real-world LLMs.

</details>


### [564] [Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM](https://arxiv.org/abs/2506.07407)
*Yihong Jin,Ze Yang,Juntian Liu,Xinhe Xu*

Main category: cs.LG

TL;DR: The paper proposes an anomaly detection and early warning system for multi-cloud environments using a large-scale language model (LLM) combined with traditional machine learning methods, demonstrating better accuracy, efficiency, and resilience.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address security and reliability challenges in intelligent monitoring systems within multi-cloud environments, which are increasingly essential.

Method: The method involves integrating LLMs with traditional machine learning in a multi-level feature extraction framework to enhance anomaly detection accuracy and real-time response performance.

Result: Experimental results show improved accuracy, reduced latency, and enhanced resilience in cloud infrastructure compared to traditional systems.

Conclusion: The system improves detection, prediction, and management capabilities in a multi-cloud environment, offering significant advancements over conventional methods.

Abstract: With the rapid development of multi-cloud environments, it is increasingly
important to ensure the security and reliability of intelligent monitoring
systems. In this paper, we propose an anomaly detection and early warning
mechanism for intelligent monitoring system in multi-cloud environment based on
Large-Scale Language Model (LLM). On the basis of the existing monitoring
framework, the proposed model innovatively introduces a multi-level feature
extraction method, which combines the natural language processing ability of
LLM with traditional machine learning methods to enhance the accuracy of
anomaly detection and improve the real-time response efficiency. By introducing
the contextual understanding capabilities of LLMs, the model dynamically adapts
to different cloud service providers and environments, so as to more
effectively detect abnormal patterns and predict potential failures.
Experimental results show that the proposed model is significantly better than
the traditional anomaly detection system in terms of detection accuracy and
latency, and significantly improves the resilience and active management
ability of cloud infrastructure.

</details>


### [565] [Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks](https://arxiv.org/abs/2506.07408)
*Xiaojun zhou,Chunna Zhao,Yaqun Huang,Chengli Zhou,Junjie Ye,Kemeng Xiang*

Main category: cs.LG

TL;DR: The paper introduces a method for fractional-order matrix differentiation compatible with automatic differentiation, demonstrating enhanced performance in deep learning.


<details>
  <summary>Details</summary>
Motivation: Current optimization algorithms for artificial neural networks can benefit from fractional-order differentiation, but lack sufficient theoretical research and compatibility with automatic differentiation technologies.

Method: The authors propose fractional-order Jacobian matrix differentiation ${{\bf{J}}^\alpha }$, design a fractional-order Autograd framework, and employ it to create fractional-order Linear (FLinear) modules within PyTorch framework.

Result: Experiments show superior performance of fractional-order differentiation (FLinear) in terms of loss reduction, test set metrics, and resource usage like time and GPU memory.

Conclusion: Fractional-order differentiation can be effectively integrated into deep learning models, improving their practicality and optimization results.

Abstract: Fractional-order differentiation has many characteristics different from
integer-order differentiation. These characteristics can be applied to the
optimization algorithms of artificial neural networks to obtain better results.
However, due to insufficient theoretical research, at present, there is no
fractional-order matrix differentiation method that is perfectly compatible
with automatic differentiation (Autograd) technology. Therefore, we propose a
fractional-order matrix differentiation calculation method. This method is
introduced by the definition of the integer-order Jacobian matrix. We denote it
as fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$).
Through ${{\bf{J}}^\alpha }$, we can carry out the matrix-based
fractional-order chain rule. Based on the Linear module and the
fractional-order differentiation, we design the fractional-order Autograd
technology to enable the use of fractional-order differentiation in hidden
layers, thereby enhancing the practicality of fractional-order differentiation
in deep learning. In the experiment, according to the PyTorch framework, we
design fractional-order Linear (FLinear) and replace nn.Linear in the
multilayer perceptron with FLinear. Through the qualitative analysis of the
training set and validation set $Loss$, the quantitative analysis of the test
set indicators, and the analysis of time consumption and GPU memory usage
during model training, we verify the superior performance of ${{\bf{J}}^\alpha
}$ and prove that it is an excellent fractional-order gradient descent method
in the field of deep learning.

</details>


### [566] [Variational Supervised Contrastive Learning](https://arxiv.org/abs/2506.07413)
*Ziwen Wang,Jiajun Fan,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: VarCon, a novel supervised contrastive learning framework, addresses limitations of traditional approaches by utilizing variational inference, achieving state-of-the-art accuracy, improved semantic embedding organization, and robustness.


<details>
  <summary>Details</summary>
Motivation: To overcome the issues of embedding misalignment and reliance on extensive negatives/augmentations in contrastive learning.

Method: VarCon leverages variational inference based on latent class variables to optimize a posterior-weighted ELBO, enabling efficient class-aware matching and controlled intra-class dispersion.

Result: VarCon achieves 79.36% accuracy on ImageNet-1K, improved semantic representations and decision boundaries, and outperforms in few-shot learning scenarios.

Conclusion: VarCon offers a promising alternative for contrastive learning frameworks by balancing efficiency, control, and robustness in embeddings while facilitating high accuracy and adaptability.

Abstract: Contrastive learning has proven to be highly efficient and adaptable in
shaping representation spaces across diverse modalities by pulling similar
samples together and pushing dissimilar ones apart. However, two key
limitations persist: (1) Without explicit regulation of the embedding
distribution, semantically related instances can inadvertently be pushed apart
unless complementary signals guide pair selection, and (2) excessive reliance
on large in-batch negatives and tailored augmentations hinders generalization.
To address these limitations, we propose Variational Supervised Contrastive
Learning (VarCon), which reformulates supervised contrastive learning as
variational inference over latent class variables and maximizes a
posterior-weighted evidence lower bound (ELBO) that replaces exhaustive
pair-wise comparisons for efficient class-aware matching and grants
fine-grained control over intra-class dispersion in the embedding space.
Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,
ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art
performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy
on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while
converging in just 200 epochs; (2) yields substantially clearer decision
boundaries and semantic organization in the embedding space, as evidenced by
KNN classification, hierarchical clustering results, and transfer-learning
assessments; and (3) demonstrates superior performance in few-shot learning
than supervised baseline and superior robustness across various augmentation
strategies.

</details>


### [567] [LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/abs/2506.07416)
*Jin Huang,Yuchao Jin,Le An,Josh Park*

Main category: cs.LG

TL;DR: The paper introduces an efficient Vision-Language Model (VLM) pipeline tailored for embedded devices in contexts like robotics and autonomous driving, achieving significant speed-ups without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of deploying Vision-Language Models in resource-constrained embedded devices, enabling real-time functionality in applications like autonomous driving.

Method: The paper proposes a VLM pipeline that incorporates patch selection to filter irrelevant views, token selection to reduce LLM input sequence length, and speculative decoding for faster token generation. FP8 post-training quantization is also used to further enhance speed.

Result: On NVIDIA DRIVE Thor platform, the pipeline achieves a $2.5×$ latency reduction in end-to-end performance and increases to $3.2×$ with FP8 post-training quantization, while maintaining task accuracy.

Conclusion: The proposed pipeline is effective in reducing latency while preserving accuracy, making it practical for deploying VLMs in real-time on embedded systems.

Abstract: This paper introduces an efficient Vision-Language Model (VLM) pipeline
specifically optimized for deployment on embedded devices, such as those used
in robotics and autonomous driving. The pipeline significantly reduces the
computational overhead by jointly leveraging patch selection to filter
irrelevant camera views, a token selection module to reduce input sequence
length for the LLM, and speculative decoding to accelerate token generation.
Evaluation on the NVIDIA DRIVE Thor platform for automonous driving
application, our pipeline achieves $2.5\times$ end-to-end latency reduction
without compromising task accuracy. The speed-up further increases to
$3.2\times$ when applying FP8 post-training quantization. These results
demonstrate our pipeline as a viable solution for enabling real-time VLM
deployment in resource-constrained environments.

</details>


### [568] [Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs](https://arxiv.org/abs/2506.07417)
*Nan Sun,Xixun Lin,Zhiheng Zhou,Yanmin Shang,Zhenlin Cheng,Yanan Cao*

Main category: cs.LG

TL;DR: The paper proposes EviSEC, an approach for better OOD detection in dynamic graphs by addressing challenges of single-point estimation and score homogenization.


<details>
  <summary>Details</summary>
Motivation: OOD detection has garnered attention in dynamic graphs where existing methods mainly focus on static graphs and suffer from bias, variance, and narrow score gaps.

Method: EviSEC leverages Evidential Deep Learning with a Dirichlet-based evidential neural network and spectrum-aware augmentation to enhance OOD detection performance.

Result: Experiments show EviSEC effectively identifies OOD samples in dynamic graphs, overcoming inherent limitations seen in current methods.

Conclusion: EviSEC significantly improves dynamic graph OOD detection through innovative evidence learning strategies and spectrum-aware enhancements.

Abstract: Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims
to identify whether incoming data deviates from the distribution of the
in-distribution (ID) training set, has garnered considerable attention in
security-sensitive fields. Current OOD detection paradigms primarily focus on
static graphs and confront two critical challenges: i) high bias and high
variance caused by single-point estimation, which makes the predictions
sensitive to randomness in the data; ii) score homogenization resulting from
the lack of OOD training data, where the model only learns ID-specific
patterns, resulting in overall low OOD scores and a narrow score gap between ID
and OOD data. To tackle these issues, we first investigate OOD detection in
dynamic graphs through the lens of Evidential Deep Learning (EDL).
Specifically, we propose EviSEC, an innovative and effective OOD detector via
Evidential Spectrum-awarE Contrastive Learning. We design an evidential neural
network to redefine the output as the posterior Dirichlet distribution,
explaining the randomness of inputs through the uncertainty of distribution,
which is overlooked by single-point estimation. Moreover, spectrum-aware
augmentation module generates OOD approximations to identify patterns with high
OOD scores, thereby widening the score gap between ID and OOD data and
mitigating score homogenization. Extensive experiments on real-world datasets
demonstrate that EviSAC effectively detects OOD samples in dynamic graphs.

</details>


### [569] [Federated In-Context Learning: Iterative Refinement for Improved Answer Quality](https://arxiv.org/abs/2506.07440)
*Ruhan Wang,Zhiyong Wang,Chengkai Huang,Rui Wang,Tong Yu,Lina Yao,John C. S. Lui,Dongruo Zhou*

Main category: cs.LG

TL;DR: This paper proposes Federated In-Context Learning (Fed-ICL), a framework for collaborative in-context learning in question-answering (QA) tasks using local client data while minimizing communication costs.


<details>
  <summary>Details</summary>
Motivation: High-quality examples essential for in-context learning in QA tasks are often limited due to privacy, annotation costs, and data distribution issues.

Method: The Fed-ICL framework refines QA responses through iterative interactions between client devices and a central server without transmitting model parameters, ensuring privacy and reducing communication overhead.

Result: Fed-ICL shows theoretical convergence and performs well on standard QA benchmarks with low communication costs.

Conclusion: Fed-ICL offers an effective, communication-efficient solution for in-context learning using distributed data without compromising performance.

Abstract: For question-answering (QA) tasks, in-context learning (ICL) enables language
models to generate responses without modifying their parameters by leveraging
examples provided in the input. However, the effectiveness of ICL heavily
depends on the availability of high-quality examples, which are often scarce
due to data privacy constraints, annotation costs, and distribution
disparities. A natural solution is to utilize examples stored on client
devices, but existing approaches either require transmitting model parameters -
incurring significant communication overhead - or fail to fully exploit local
datasets, limiting their effectiveness. To address these challenges, we propose
Federated In-Context Learning (Fed-ICL), a general framework that enhances ICL
through an iterative, collaborative process. Fed-ICL progressively refines
responses by leveraging multi-round interactions between clients and a central
server, improving answer quality without the need to transmit model parameters.
We establish theoretical guarantees for the convergence of Fed-ICL and conduct
extensive experiments on standard QA benchmarks, demonstrating that our
proposed approach achieves strong performance while maintaining low
communication costs.

</details>


### [570] [Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs](https://arxiv.org/abs/2506.07448)
*T. Duy Nguyen-Hien,Desi R. Ivanova,Yee Whye Teh,Wee Sun Lee*

Main category: cs.LG

TL;DR: The paper proposes using Bayesian Modeling of Experiments to better manage uncertainty in large language model (LLM) deployments, advocating for proactive measures like clarification and external data retrieval instead of rejecting uncertain outputs.


<details>
  <summary>Details</summary>
Motivation: Current strategies in LLM deployments often reject outputs with high uncertainty to avoid misinformation, but lack a systematic way to differentiate and respond to the sources of uncertainty, limiting reliability.

Method: The authors introduce Bayesian Modeling of Experiments as a framework to systematically address uncertainty in LLMs. It enables context-aware actions such as clarifications, use of external data, and input refinement.

Result: The framework facilitates active engagement with uncertainty, allowing for reliable and transparent LLM systems that can operate effectively in high-stakes environments.

Conclusion: Adopting this Bayesian framework can transition LLMs from passive avoidance to active management of uncertainty, ensuring robustness and transparency in real-world applications.

Abstract: Although large language models (LLMs) are highly interactive and extendable,
current approaches to ensure reliability in deployments remain mostly limited
to rejecting outputs with high uncertainty in order to avoid misinformation.
This conservative strategy reflects the current lack of tools to systematically
distinguish and respond to different sources of uncertainty. In this paper, we
advocate for the adoption of Bayesian Modeling of Experiments -- a framework
that provides a coherent foundation to reason about uncertainty and clarify the
reducibility of uncertainty -- for managing and proactively addressing
uncertainty that arises in LLM deployments. This framework enables LLMs and
their users to take contextually appropriate steps, such as requesting
clarification, retrieving external information, or refining inputs. By
supporting active resolution rather than passive avoidance, it opens the door
to more reliable, transparent, and broadly applicable LLM systems, particularly
in high-stakes, real-world settings.

</details>


### [571] [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
*Yuxin Xiao,Sana Tonekaboni,Walter Gerych,Vinith Suriyakumar,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: The study explores how styling patterns, such as formatting, impact the vulnerability of large language models (LLMs) to jailbreak attacks and introduces a mitigation strategy called SafeStyle to improve safety.


<details>
  <summary>Details</summary>
Motivation: To address the unclear safety impact of style patterns in LLMs, particularly their role in increasing vulnerability to jailbreak attacks.

Method: The evaluation involves 32 LLMs across seven jailbreak benchmarks, analyzing the relationship between style patterns and attack success rates, coupled with fine-tuning experiments to investigate vulnerabilities.

Result: Findings revealed that style patterns notably increase attack success rates (ASR), and fine-tuning LLMs with these patterns further heightens vulnerability to similar styled jailbreak attacks.

Conclusion: SafeStyle, a defense strategy using augmented safety training data aligned with fine-tuning styles, consistently enhances LLM safety and mitigates vulnerabilities caused by style alignment.

Abstract: Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in jailbreak queries. Although these
style patterns are semantically unrelated to the malicious intents behind
jailbreak queries, their safety impact remains unclear. In this work, we seek
to understand whether style patterns compromise LLM safety, how superficial
style alignment increases model vulnerability, and how best to mitigate these
risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,
and find that malicious queries with style patterns inflate the attack success
rate (ASR) for nearly all models. Notably, ASR inflation correlates with both
the length of style patterns and the relative attention an LLM exhibits on
them. We then investigate superficial style alignment, and find that
fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of
those same styles. Finally, we propose SafeStyle, a defense strategy that
incorporates a small amount of safety training data augmented to match the
distribution of style patterns in the fine-tuning data. Across three LLMs and
five fine-tuning style settings, SafeStyle consistently outperforms baselines
in maintaining LLM safety.

</details>


### [572] [ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning](https://arxiv.org/abs/2506.07459)
*Ziwen Wang,Jiajun Fan,Ruihan Guo,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: ProteinZero introduces a reinforcement learning framework to improve protein design models by achieving high structural accuracy, designability, and success rates while promoting diversity.


<details>
  <summary>Details</summary>
Motivation: Protein generative models often fail due to the limited availability of high-quality protein datasets for initial training.

Method: ProteinZero uses online reinforcement learning, balancing multi-reward maximization, KL-divergence, and protein-embedding diversity regularization, coupled with rapid reward proxies like ESM-fold and ddG predictor.

Result: ProteinZero surpasses previous methods in protein design metrics, achieving 90%+ success rates and reducing failure rates by 36%-48%.

Conclusion: ProteinZero pioneers a self-improving model for exploring protein design, offering scalable and high-performance outcomes with minimal computation requirements.

Abstract: Protein generative models have shown remarkable promise in protein design but
still face limitations in success rate, due to the scarcity of high-quality
protein datasets for supervised pretraining. We present ProteinZero, a novel
framework that enables scalable, automated, and continuous self-improvement of
the inverse folding model through online reinforcement learning. To achieve
computationally tractable online feedback, we introduce efficient proxy reward
models based on ESM-fold and a novel rapid ddG predictor that significantly
accelerates evaluation speed. ProteinZero employs a general RL framework
balancing multi-reward maximization, KL-divergence from a reference model, and
a novel protein-embedding level diversity regularization that prevents mode
collapse while promoting higher sequence diversity. Through extensive
experiments, we demonstrate that ProteinZero substantially outperforms existing
methods across every key metric in protein design, achieving significant
improvements in structural accuracy, designability, thermodynamic stability,
and sequence diversity. Most impressively, ProteinZero reduces design failure
rates by approximately 36% - 48% compared to widely-used methods like
ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates
exceeding 90% across diverse and complex protein folds. Notably, the entire RL
run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,
including reward computation. Our work establishes a new paradigm for protein
design where models evolve continuously from their own generated outputs,
opening new possibilities for exploring the vast protein design space.

</details>


### [573] [Circumventing Backdoor Space via Weight Symmetry](https://arxiv.org/abs/2506.07467)
*Jie Peng,Hongwei Yang,Jing Zhao,Hengji Dong,Hui He,Weizhe Zhang,Haoyu He*

Main category: cs.LG

TL;DR: The paper introduces Two-stage Symmetry Connectivity (TSC), a data-independent method to safeguard models against backdoor attacks, applicable across various learning paradigms.


<details>
  <summary>Details</summary>
Motivation: Current defenses against backdoor attacks often rely on labeled data or specific procedures, limiting their versatility, especially as attacks become viable across diverse learning paradigms.

Method: The authors leverage permutation invariance in neural networks and quadratic mode connectivity to amplify losses on poisoned samples while preserving clean accuracy, requiring minimal clean data.

Result: TSC demonstrates strong backdoor defense performance in supervised settings and also generalizes effectively to self-supervised frameworks like SimCLR and CLIP.

Conclusion: TSC offers a robust, versatile purification strategy that mitigates backdoor vulnerabilities across different learning paradigms, without significant reliance on specific data formats or large labeled datasets.

Abstract: Deep neural networks are vulnerable to backdoor attacks, where malicious
behaviors are implanted during training. While existing defenses can
effectively purify compromised models, they typically require labeled data or
specific training procedures, making them difficult to apply beyond supervised
learning settings. Notably, recent studies have shown successful backdoor
attacks across various learning paradigms, highlighting a critical security
concern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC),
a novel backdoor purification defense that operates independently of data
format and requires only a small fraction of clean samples. Through theoretical
analysis, we prove that by leveraging permutation invariance in neural networks
and quadratic mode connectivity, TSC amplifies the loss on poisoned samples
while maintaining bounded clean accuracy. Experiments demonstrate that TSC
achieves robust performance comparable to state-of-the-art methods in
supervised learning scenarios. Furthermore, TSC generalizes to self-supervised
learning frameworks, such as SimCLR and CLIP, maintaining its strong defense
capabilities. Our code is available at https://github.com/JiePeng104/TSC.

</details>


### [574] [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
*Mickel Liu,Liwei Jiang,Yancheng Liang,Simon Shaolei Du,Yejin Choi,Tim Althoff,Natasha Jaques*

Main category: cs.LG

TL;DR: The paper introduces Self-RedTeam, an online self-play reinforcement learning algorithm for proactive LM safety alignment, addressing the drawbacks of traditional reactive approaches.


<details>
  <summary>Details</summary>
Motivation: The conventional approach to language model safety alignment is reactive and leaves defenders lagging behind evolving attackers. The goal is to move towards a dynamic, proactive methodology.

Method: Self-RedTeam is an online self-play reinforcement learning setup, in which a single model alternates between attacker and defender roles in a zero-sum game framework to drive co-evolution. A reward model adjudicates the interactions.

Result: Self-RedTeam identifies more diverse attacks (+21.8% SBERT) and achieves significantly higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than approaches using static attackers or defenders. It also leverages hidden Chain-of-Thought for planning, improving diversity and reducing over-refusals.

Conclusion: The study demonstrates dynamic co-evolution as a more effective and scalable way to improve LM safety alignment, with potential to replace conventional reactive fine-tuning methods.

Abstract: Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).

</details>


### [575] [Premise Selection for a Lean Hammer](https://arxiv.org/abs/2506.07477)
*Thomas Zhu,Joshua Clune,Jeremy Avigad,Albert Qiaochu Jiang,Sean Welleck*

Main category: cs.LG

TL;DR: LeanHammer is introduced as the first end-to-end hammer for the Lean proof assistant, utilizing a novel neural premise selection system to integrate neural methods into formal verification effectively.


<details>
  <summary>Details</summary>
Motivation: Lean lacks a hammer tool despite its growing adoption; existing methods do not dynamically adapt to user-specific contexts or integrate neural premise selection effectively.

Method: The researchers developed a neural premise selection system tailored for Lean’s dependent type theory, combining it with symbolic proof search and proof reconstruction.

Result: Evaluations show LeanHammer solves 21% more goals than existing premise selectors and generalizes well across diverse domains.

Conclusion: LeanHammer demonstrates a significant advance, bridging neural and symbolic reasoning to enhance formal verification efficiency in Lean.

Abstract: Neural methods are transforming automated reasoning for proof assistants, yet
integrating these advances into practical verification workflows remains
challenging. Hammers are tools that interface with external automatic theorem
provers to automate tedious reasoning steps. They have dramatically improved
productivity in proof assistants, but the Lean proof assistant still does not
have a hammer despite its growing popularity. We present LeanHammer, the first
end-to-end domain-general hammer for Lean, built on a novel neural premise
selection system for a hammer in dependent type theory. Unlike existing Lean
premise selectors, our approach dynamically adapts to user-specific contexts
and combines with symbolic proof search and reconstruction to create a
practical hammer. With comprehensive evaluations, we show that our premise
selector enables LeanHammer to solve 21\% more goals relative to existing
premise selectors, and generalize well to diverse domains. Our work bridges the
gap between neural retrieval and symbolic reasoning, making formal verification
more accessible to researchers and practitioners.

</details>


### [576] [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
*Libo Wang*

Main category: cs.LG

TL;DR: The paper introduces the Graph of Causal Evolution (GoCE) to address long-range dependency issues in chain-of-model (CoM) architectures by using a causal adjacency matrix and transformer enhancements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of CoM architecture, where subchains lack sufficient access to global context due to causal masking, leading to loss of long-range dependencies.

Method: The novelty lies in mapping token representations into sparse causal adjacency matrices, leveraging causal-masked attention, and employing causal-MoE modules in a transformer-based architecture. Methods like intervention consistency loss and self-evolution gates ensure a balance between causal structure learning and adaptive model updates.

Result: Experiments on benchmark datasets (CLUTRR, CLADDER, EX-FEVER, and CausalQA) show that GoCE improves the ability of transformers to capture long-range causal dependencies, achieving better performance than baseline LLMs.

Conclusion: GoCE is a significant improvement over CoM, offering enhanced long-range dependency handling and self-evolution capabilities, paving the way for advanced causal learning research.

Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies
only on the information of the previous subchain and may lose long-range
dependencies due to the causal mask blocking the global context flow between
multi-level subchains, this work proposes a graph of causal evolution (GoCE).
Its core principle is to map the implicit token representation into a
differentiable and sparse causal adjacency matrix, then permeate causal
constraints through each layer of calculation using causal-masked attention and
causal-MoE. By combining intervention consistency loss test and self-evolution
gate, the dynamic balance between causal structure learning and adaptive
updating of transformer architecture is realized. The researcher built
experimental environments in sandboxes built with Claude Sonnet 4,
o4-mini-high, and DeepSeek R1 respectively with the transformer variant
architecture introduced in GoCE. It is evaluated on publicly available datasets
including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the
baseline LLMs. The finding proves that GoCE strengthens the transformer's
ability to capture long-range causal dependencies, while the ability to
self-evolve is improved. It not only surpasses the design of CoM in terms of
design principles, but also provides experience for future research on causal
learning and continuous adaptive improvement.

</details>


### [577] [Reinforcement Learning via Implicit Imitation Guidance](https://arxiv.org/abs/2506.07505)
*Perry Dong,Alec M. Lessing,Annie S. Chen,Chelsea Finn*

Main category: cs.LG

TL;DR: This paper introduces "Data-Guided Noise" (DGN) to improve sample efficiency in reinforcement learning by using prior data to guide exploration without behavior cloning.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning often requires prior data to initialize in environments with sparse rewards, but traditional imitation learning approaches degrade long-term performance.

Method: The authors propose DGN, which uses demonstrations to guide exploration through noise added to the policy, avoiding explicit behavior cloning constraints.

Result: DGN achieves up to 2-3x improvement over existing methods in reinforcement learning across seven simulated continuous control tasks.

Conclusion: Using prior data for exploration guidance, rather than imitation learning, can significantly enhance reinforcement learning performance.

Abstract: We study the problem of sample efficient reinforcement learning, where prior
data such as demonstrations are provided for initialization in lieu of a dense
reward signal. A natural approach is to incorporate an imitation learning
objective, either as regularization during training or to acquire a reference
policy. However, imitation learning objectives can ultimately degrade long-term
performance, as it does not directly align with reward maximization. In this
work, we propose to use prior data solely for guiding exploration via noise
added to the policy, sidestepping the need for explicit behavior cloning
constraints. The key insight in our framework, Data-Guided Noise (DGN), is that
demonstrations are most useful for identifying which actions should be
explored, rather than forcing the policy to take certain actions. Our approach
achieves up to 2-3x improvement over prior reinforcement learning from offline
data methods across seven simulated continuous control tasks.

</details>


### [578] [Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems](https://arxiv.org/abs/2506.07517)
*Shuqiang Zhang,Yuchao Zhang,Jinkun Chen,Haochen Sui*

Main category: cs.LG

TL;DR: This paper tackles selection bias in recommendation systems using a new algorithm that accounts for latent exogenous variables, showing improved effectiveness in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of selection bias in recommendation systems, which distorts user preferences and damages the accuracy and fairness of recommendations.

Method: The method involves a likelihood maximization algorithm that incorporates latent exogenous variables into the data generation model, coupled with a Monte Carlo algorithm for numerical estimation.

Result: The experiments on synthetic and real-world datasets demonstrate the method's effectiveness in managing latent exogenous variables for debiasing.

Conclusion: The proposed method advances recommendation systems through a structural causal perspective by releasing independence assumptions and handling latent exogenous variables effectively.

Abstract: Recommendation systems (RS) aim to provide personalized content, but they
face a challenge in unbiased learning due to selection bias, where users only
interact with items they prefer. This bias leads to a distorted representation
of user preferences, which hinders the accuracy and fairness of
recommendations. To address the issue, various methods such as error imputation
based, inverse propensity scoring, and doubly robust techniques have been
developed. Despite the progress, from the structural causal model perspective,
previous debiasing methods in RS assume the independence of the exogenous
variables. In this paper, we release this assumption and propose a learning
algorithm based on likelihood maximization to learn a prediction model. We
first discuss the correlation and difference between unmeasured confounding and
our scenario, then we propose a unified method that effectively handles latent
exogenous variables. Specifically, our method models the data generation
process with latent exogenous variables under mild normality assumptions. We
then develop a Monte Carlo algorithm to numerically estimate the likelihood
function. Extensive experiments on synthetic datasets and three real-world
datasets demonstrate the effectiveness of our proposed method. The code is at
https://github.com/WallaceSUI/kdd25-background-variable.

</details>


### [579] [Improving Memory Efficiency for Training KANs via Meta Learning](https://arxiv.org/abs/2506.07549)
*Zhangchi Zhao,Jun Shu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: The paper introduces MetaKANs, a meta-learning framework to improve the scalability and efficiency of Kolmogorov-Arnold Networks (KANs) by reducing trainable parameters while maintaining performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: KANs offer an interpretable alternative to traditional neural networks but face scalability issues due to their large number of trainable parameters, which affects memory usage and increases training costs.

Method: MetaKANs use a smaller meta-learner that generates weights for KANs, enabling end-to-end differentiable training to address the memory and parameter efficiency issues of KANs.

Result: MetaKANs achieve comparable or superior performance to KANs on tasks such as symbolic regression, partial differential equation solving, and image classification, while requiring fewer trainable parameters and improving scalability.

Conclusion: MetaKANs present a scalable and efficient approach to training KANs, closing the training cost gap with MLPs, and making the method feasible for broader applications.

Abstract: Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel
framework for function approximation by replacing traditional neural network
weights with learnable univariate functions. This design demonstrates
significant potential as an efficient and interpretable alternative to
traditional MLPs. However, KANs are characterized by a substantially larger
number of trainable parameters, leading to challenges in memory efficiency and
higher training costs compared to MLPs. To address this limitation, we propose
to generate weights for KANs via a smaller meta-learner, called MetaKANs. By
training KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs
achieve comparable or even superior performance while significantly reducing
the number of trainable parameters and maintaining promising interpretability.
Extensive experiments on diverse benchmark tasks, including symbolic
regression, partial differential equation solving, and image classification,
demonstrate the effectiveness of MetaKANs in improving parameter efficiency and
memory usage. The proposed method provides an alternative technique for
training KANs, that allows for greater scalability and extensibility, and
narrows the training cost gap with MLPs stated in the original paper of KANs.
Our code is available at https://github.com/Murphyzc/MetaKAN.

</details>


### [580] [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
*Mengsong Wu,YaFei Wang,Yidong Ming,Yuqi An,Yuwei Wan,Wenliang Chen,Binbin Lin,Yuqiang Li,Tong Xie,Dongzhan Zhou*

Main category: cs.LG

TL;DR: The paper introduces an LLM-based agent that integrates 137 chemical tools and presents a dataset (ChemToolBench) to improve chemistry-related tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges with outdated knowledge and difficulties incorporating specialized chemical expertise.

Method: Developed an LLM agent integrating 137 tools, a dataset pipeline (ChemToolBench), and a hierarchical algorithm (HE-MCTS) to optimize tool usage and training.

Result: The proposed approach outperforms GPT-4o in Chemistry QA and discovery tasks with significant performance improvements.

Conclusion: The integration of specialized chemical tools with LLMs enhances their capability for advanced chemical applications.

Abstract: Large language models (LLMs) have recently demonstrated promising
capabilities in chemistry tasks while still facing challenges due to outdated
pretraining knowledge and the difficulty of incorporating specialized chemical
expertise. To address these issues, we propose an LLM-based agent that
synergistically integrates 137 external chemical tools created ranging from
basic information retrieval to complex reaction predictions, and a dataset
curation pipeline to generate the dataset ChemToolBench that facilitates both
effective tool selection and precise parameter filling during fine-tuning and
evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search
(HE-MCTS) framework, enabling independent optimization of tool planning and
execution. By leveraging self-generated data, our approach supports step-level
fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM
that surpass GPT-4o. Experimental evaluations demonstrate that our approach
significantly improves performance in Chemistry QA and discovery tasks,
offering a robust solution to integrate specialized tools with LLMs for
advanced chemical applications. All datasets and code are available at
https://github.com/AI4Chem/ChemistryAgent .

</details>


### [581] [Denoising the Future: Top-p Distributions for Moving Through Time](https://arxiv.org/abs/2506.07578)
*Florian Andreas Marwitz,Ralf Möller,Magnus Bender,Marcel Gehrke*

Main category: cs.LG

TL;DR: The paper proposes a method to improve Hidden Markov Models (HMM) inference by considering only the top-p most probable states, achieving significant computational speedups with minimal error.


<details>
  <summary>Details</summary>
Motivation: Inference in Hidden Markov Models (HMMs) is computationally expensive due to the necessity of enumerating the entire state space, including states with negligible probabilities, which leads to inefficiency and noise.

Method: The authors introduce a strategy that selects the top-p states (most probable states such that their cumulative probability is p) during inference, and derive a bound on the introduced error based on p and the model’s minimal mixing rate.

Result: The proposed method achieves an order of magnitude speedup in inference while maintaining a total variation error under 0.09, as shown in empirical evaluations.

Conclusion: By dynamically focusing only on the most probable states during inference, the approach effectively balances computational efficiency and inferential accuracy in HMMs.

Abstract: Inference in dynamic probabilistic models is a complex task involving
expensive operations. In particular, for Hidden Markov Models, the whole state
space has to be enumerated for advancing in time. Even states with negligible
probabilities are considered, resulting in computational inefficiency and
increased noise due to the propagation of unlikely probability mass. We propose
to denoise the future and speed up inference by using only the top-p states,
i.e., the most probable states with accumulated probability p. We show that the
error introduced by using only the top-p states is bound by p and the so-called
minimal mixing rate of the underlying model. Moreover, in our empirical
evaluation, we show that we can expect speedups of at least an order of
magnitude, while the error in terms of total variation distance is below 0.09.

</details>


### [582] [MIRA: Medical Time Series Foundation Model for Real-World Health Data](https://arxiv.org/abs/2506.07584)
*Hao Li,Bowen Deng,Chang Xu,Zhiyuan Feng,Viktor Schlegel,Yu-Hao Huang,Yizheng Sun,Jingyuan Sun,Kailai Yang,Yiyao Yu,Jiang Bian*

Main category: cs.LG

TL;DR: MIRA is a specialized model for medical time series forecasting that achieves significant error reductions using innovative techniques and a large-scale medical corpus.


<details>
  <summary>Details</summary>
Motivation: To reduce burdens in medical time series analysis by creating a unified model handling irregularities and challenges in medical data.

Method: MIRA uses Continuous-Time Rotary Positional Encoding, frequency-specific mixture-of-experts layers, and Neural ODE-based blocks to address time interval variability and latent state modeling.

Result: MIRA achieves 10% error reduction in out-of-distribution tasks and 7% in in-distribution tasks, outperforming other baselines.

Conclusion: MIRA provides a robust foundation for medical time series modeling and forecasting, advancing future research in the field.

Abstract: A unified foundation model for medical time series -- pretrained on open
access and ethics board-approved medical corpora -- offers the potential to
reduce annotation burdens, minimize model customization, and enable robust
transfer across clinical institutions, modalities, and tasks, particularly in
data-scarce or privacy-constrained environments. However, existing generalist
time series foundation models struggle to handle medical time series data due
to their inherent challenges, including irregular intervals, heterogeneous
sampling rates, and frequent missing values. To address these challenges, we
introduce MIRA, a unified foundation model specifically designed for medical
time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional
Encoding that enables fine-grained modeling of variable time intervals, a
frequency-specific mixture-of-experts layer that routes computation across
latent frequency regimes to further promote temporal specialization, and a
Continuous Dynamics Extrapolation Block based on Neural ODE that models the
continuous trajectory of latent states, enabling accurate forecasting at
arbitrary target timestamps. Pretrained on a large-scale and diverse medical
corpus comprising over 454 billion time points collect from publicly available
datasets, MIRA achieves reductions in forecasting errors by an average of 10%
and 7% in out-of-distribution and in-distribution scenarios, respectively, when
compared to other zero-shot and fine-tuned baselines. We also introduce a
comprehensive benchmark spanning multiple downstream clinical tasks,
establishing a foundation for future research in medical time series modeling.

</details>


### [583] [Aircraft Trajectory Dataset Augmentation in Latent Space](https://arxiv.org/abs/2506.07585)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: The study introduces ATRADA, a framework for generating synthetic aircraft trajectory data, leveraging a Transformer encoder, PCA, GMM, and MLP.


<details>
  <summary>Details</summary>
Motivation: There is a need to augment trajectory datasets to develop robust aircraft trajectory models crucial for Air Traffic Management tasks.

Method: The framework uses a Transformer encoder to learn latent patterns, followed by PCA for dimensional reduction; GMM fits the data distribution, and MLP decodes the generated samples.

Result: Experiments showed ATRADA's capability to generate high-quality synthetic trajectory data outperforming several baselines.

Conclusion: ATRADA serves as a novel and effective approach to augment aircraft trajectory datasets, enhancing model development in ATM applications.

Abstract: Aircraft trajectory modeling plays a crucial role in Air Traffic Management
(ATM) and is important for various downstream tasks, including conflict
detection and landing time prediction. Dataset augmentation through the
addition of synthetically generated trajectory data is necessary to develop a
more robust aircraft trajectory model and ensure that the trajectory dataset is
sufficient and balanced. In this work, we propose a novel framework called
ATRADA for aircraft trajectory dataset augmentation. In the proposed framework,
a Transformer encoder learns the underlying patterns in the original trajectory
dataset and converts each data point into a context vector in the learned
latent space. The converted dataset in the latent space is projected into
reduced dimensions using principal component analysis (PCA), and a Gaussian
mixture model (GMM) is applied to fit the probability distribution of the data
points in the reduced-dimensional space. Finally, new samples are drawn from
the fitted GMM, the dimension of the samples is reverted to the original
dimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several
experiments demonstrate that the framework effectively generates new,
high-quality synthetic aircraft trajectory data, which were compared to the
results of several baselines.

</details>


### [584] [PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs](https://arxiv.org/abs/2506.07587)
*Tongzhou Yu,Zhuhao Zhang,Guanghui Zhu,Shen Jiang,Meikang Qiu,Yihua Huang*

Main category: cs.LG

TL;DR: The paper introduces PrunePEFT, a novel method to optimize Parameter Efficient Fine-Tuning (PEFT) by treating the search for configurations as a pruning problem, reducing computational overhead and maintaining strong task performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies and computational burdens in designing effective PEFT configurations, which require careful consideration of a vast design space to avoid sub-optimal results.

Method: PrunePEFT formulates PEFT strategy search as a pruning problem, employing a hybrid pruning strategy to iteratively remove redundant or conflicting PEFT modules and optimize fine-tuning configurations.

Result: PrunePEFT effectively identifies the most relevant PEFT modules, significantly reducing computational costs and achieving scalability while preserving task performance.

Conclusion: PrunePEFT offers a scalable and efficient solution for fine-tuning large pre-trained models by leveraging pruning methods, effectively balancing computational efficiency and model effectiveness.

Abstract: Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and
promising approaches for fine-tuning pre-trained language models. Compared with
Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance
with a substantial reduction of trainable parameters, which largely saved the
training and storage costs. However, using the PEFT method requires considering
a vast design space, such as the type of PEFT modules and their insertion
layers. Inadequate configurations can lead to sub-optimal results. Conventional
solutions such as architectural search techniques, while effective, tend to
introduce substantial additional overhead. In this paper, we propose a novel
approach, PrunePEFT, which formulates the PEFT strategy search as a pruning
problem and introduces a hybrid pruning strategy that capitalizes on the
sensitivity of pruning methods to different PEFT modules. This method extends
traditional pruning techniques by iteratively removing redundant or conflicting
PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently
identifying the most relevant modules, our approach significantly reduces the
computational burden typically associated with architectural search processes,
making it a more scalable and efficient solution for fine-tuning large
pre-trained models.

</details>


### [585] [TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts](https://arxiv.org/abs/2506.07596)
*Torsten Krauß,Hamid Dashtbani,Alexandra Dmitrienko*

Main category: cs.LG

TL;DR: TwinBreak is a method for removing safety mechanisms in large language models (LLMs) by identifying and pruning relevant parameters responsible for security measures, achieving high success rates with low resource requirements.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of bypassing safety mechanisms in large language models (LLMs), as existing methods either require high resources or degrade the model’s utility.

Method: TwinBreak identifies and prunes safety-related parameters by analyzing intermediate outputs from structurally similar prompts using a novel dataset, TwinPrompt, and focusing on specific layers to ensure effectiveness without impacting utility.

Result: TwinBreak achieves 89%–98% success rates in circumventing safety measures across 16 large language models from five different vendors, with minimal computational consumption.

Conclusion: TwinBreak provides an efficient and innovative approach to bypassing model security without compromising its general utility, demonstrating practicality and scalability across multiple LLMs.

Abstract: Machine learning is advancing rapidly, with applications bringing notable
benefits, such as improvements in translation and code generation. Models like
ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated
into daily life. However, alongside these benefits, LLMs also introduce social
risks. Malicious users can exploit LLMs by submitting harmful prompts, such as
requesting instructions for illegal activities. To mitigate this, models often
include a security mechanism that automatically rejects such harmful prompts.
However, they can be bypassed through LLM jailbreaks. Current jailbreaks often
require significant manual effort, high computational costs, or result in
excessive model modifications that may degrade regular utility.
  We introduce TwinBreak, an innovative safety alignment removal method.
Building on the idea that the safety mechanism operates like an embedded
backdoor, TwinBreak identifies and prunes parameters responsible for this
functionality. By focusing on the most relevant model layers, TwinBreak
performs fine-grained analysis of parameters essential to model utility and
safety. TwinBreak is the first method to analyze intermediate outputs from
prompts with high structural and content similarity to isolate safety
parameters. We present the TwinPrompt dataset containing 100 such twin prompts.
Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success
rates with minimal computational requirements across 16 LLMs from five vendors.

</details>


### [586] [FuXi-Air: Urban Air Quality Forecasting Based on Emission-Meteorology-Pollutant multimodal Machine Learning](https://arxiv.org/abs/2506.07616)
*Zhixin Geng,Xu Fan,Xiqiao Lu,Yan Zhang,Guangyuan Yu,Cheng Huang,Qian Wang,Yuewu Li,Weichun Ma,Qi Yu,Libo Wu,Hao Li*

Main category: cs.LG

TL;DR: The study introduces FuXi-Air, a multimodal data-driven air quality forecasting model that achieves high-precision predictions efficiently and cost-effectively.


<details>
  <summary>Details</summary>
Motivation: Current air quality forecasting methods struggle with high computational costs, inefficiency, and inadequate integration of observational data, necessitating advancements in AI-based approaches for urban management.

Method: FuXi-Air combines multimodal data (meteorological forecasts, emission inventories, and pollutant monitoring data) within an autoregressive prediction framework coupled with frame interpolation strategies to forecast air quality.

Result: FuXi-Air delivers 72-hour forecasts for six air pollutants at hourly resolution across monitoring sites in under 30 seconds, outperforming mainstream numerical models in accuracy and efficiency.

Conclusion: The integration of multimodal data significantly improves prediction precision, offering a cutting-edge, practical example for smart city air pollution management systems.

Abstract: Air pollution has emerged as a major public health challenge in megacities.
Numerical simulations and single-site machine learning approaches have been
widely applied in air quality forecasting tasks. However, these methods face
multiple limitations, including high computational costs, low operational
efficiency, and limited integration with observational data. With the rapid
advancement of artificial intelligence, there is an urgent need to develop a
low-cost, efficient air quality forecasting model for smart urban management.
An air quality forecasting model, named FuXi-Air, has been constructed in this
study based on multimodal data fusion to support high-precision air quality
forecasting and operated in typical megacities. The model integrates
meteorological forecasts, emission inventories, and pollutant monitoring data
under the guidance of air pollution mechanism. By combining an autoregressive
prediction framework with a frame interpolation strategy, the model
successfully completes 72-hour forecasts for six major air pollutants at an
hourly resolution across multiple monitoring sites within 25-30 seconds. In
terms of both computational efficiency and forecasting accuracy, it outperforms
the mainstream numerical air quality models in operational forecasting work.
Ablation experiments concerning key influencing factors show that although
meteorological data contribute more to model accuracy than emission inventories
do, the integration of multimodal data significantly improves forecasting
precision and ensures that reliable predictions are obtained under differing
pollution mechanisms across megacities. This study provides both a technical
reference and a practical example for applying multimodal data-driven models to
air quality forecasting and offers new insights into building hybrid
forecasting systems to support air pollution risk warning in smart city
management.

</details>


### [587] [The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning](https://arxiv.org/abs/2506.07619)
*Toby Boyne,Juan S. Campos,Becky D. Langdon,Jixiang Qing,Yilin Xie,Shiqiang Zhang,Calvin Tsay,Ruth Misener,Daniel W. Davies,Kim E. Jelfs,Sarah Boyall,Thomas M. Dixon,Linden Schrecker,Jose Pablo Folch*

Main category: cs.LG

TL;DR: This paper introduces a novel dataset for yield prediction in transient flow conditions, providing challenges for machine learning models and focusing on solvent selection.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of inaccessible chemical datasets and leverage machine learning for complex laboratory chemistry tasks like yield prediction.

Method: A comprehensive dataset with over 1200 process conditions in transient flow is provided, emphasizing continuous parameters sampling rather than discrete parameters.

Result: The paper demonstrates benchmarking tools for regression, transfer-learning, feature engineering, and active learning to enhance solvent selection and sustainable manufacturing.

Conclusion: The dataset and methodology pave the way for improved applications of machine learning in laboratory chemistry and promote sustainable practices through solvent replacement.

Abstract: Machine learning has promised to change the landscape of laboratory
chemistry, with impressive results in molecular property prediction and
reaction retro-synthesis. However, chemical datasets are often inaccessible to
the machine learning community as they tend to require cleaning, thorough
understanding of the chemistry, or are simply not available. In this paper, we
introduce a novel dataset for yield prediction, providing the first-ever
transient flow dataset for machine learning benchmarking, covering over 1200
process conditions. While previous datasets focus on discrete parameters, our
experimental set-up allow us to sample a large number of continuous process
conditions, generating new challenges for machine learning models. We focus on
solvent selection, a task that is particularly difficult to model theoretically
and therefore ripe for machine learning applications. We showcase benchmarking
for regression algorithms, transfer-learning approaches, feature engineering,
and active learning, with important applications towards solvent replacement
and sustainable manufacturing.

</details>


### [588] [Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks](https://arxiv.org/abs/2506.07624)
*Ali Hariri,Álvaro Arroyo,Alessio Gravina,Moshe Eliasof,Carola-Bibiane Schönlieb,Davide Bacciu,Kamyar Azizzadenesheli,Xiaowen Dong,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: The study revisits ChebNet, an early spectral GNN, and introduces Stable-ChebNet, addressing its stability issues while achieving competitive performance and scalability advantages over MPNNs and Graph Transformers.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of MPNNs in capturing long-range dependencies and revisit ChebNet's potential for better performance and stability in graph-based tasks.

Method: Reformulated ChebNet as a stable and non-dissipative dynamical system, offering controllable propagation dynamics without requiring eigendecompositions or graph rewiring.

Result: Stable-ChebNet achieves near state-of-the-art performance across several graph benchmark tasks while maintaining computational efficiency and stability.

Conclusion: Stable-ChebNet resolves ChebNet's instability during training and demonstrates its ability to efficiently capture long-range node interactions on a competitive level with existing architectures.

Abstract: ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by
Message Passing Neural Networks (MPNNs), which gained popularity for their
simplicity and effectiveness in capturing local graph structure. Despite their
success, MPNNs are limited in their ability to capture long-range dependencies
between nodes. This has led researchers to adapt MPNNs through rewiring or make
use of Graph Transformers, which compromises the computational efficiency that
characterized early spatial message-passing architectures, and typically
disregards the graph structure. Almost a decade after its original
introduction, we revisit ChebNet to shed light on its ability to model distant
node interactions. We find that out-of-box, ChebNet already shows competitive
advantages relative to classical MPNNs and GTs on long-range benchmarks, while
maintaining good scalability properties for high-order polynomials. However, we
uncover that this polynomial expansion leads ChebNet to an unstable regime
during training. To address this limitation, we cast ChebNet as a stable and
non-dissipative dynamical system, which we coin Stable-ChebNet. Our
Stable-ChebNet model allows for stable information propagation, and has
controllable dynamics which do not require the use of eigendecompositions,
positional encodings, or graph rewiring. Across several benchmarks,
Stable-ChebNet achieves near state-of-the-art performance.

</details>


### [589] [ProARD: progressive adversarial robustness distillation: provide wide range of robust students](https://arxiv.org/abs/2506.07666)
*Seyedhamidreza Mousavi,Seyedali Mousavi,Masoud Daneshtalab*

Main category: cs.LG

TL;DR: This paper introduces Progressive Adversarial Robustness Distillation (ProARD), enabling single training of a dynamic network to handle multiple robust student networks without requiring retraining.


<details>
  <summary>Details</summary>
Motivation: Current adversarial robustness distillation methods are computationally expensive and resource-inefficient, as they require retraining new student networks from scratch to meet specific constraints.

Method: The authors design a dynamic deep neural network incorporating variations in width, depth, and expansion. They use the largest student network as a teacher and employ weight-sharing for joint optimization. A sampling mechanism efficiently selects student networks during training.

Result: ProARD successfully provides a single dynamic network capable of producing accurate and robust student networks without retraining, overcoming the limitations of random student sampling.

Conclusion: ProARD reduces computational costs and environmental impact while supporting diverse robust student networks tailored for various edge devices and constraints.

Abstract: Adversarial Robustness Distillation (ARD) has emerged as an effective method
to enhance the robustness of lightweight deep neural networks against
adversarial attacks. Current ARD approaches have leveraged a large robust
teacher network to train one robust lightweight student. However, due to the
diverse range of edge devices and resource constraints, current approaches
require training a new student network from scratch to meet specific
constraints, leading to substantial computational costs and increased CO2
emissions. This paper proposes Progressive Adversarial Robustness Distillation
(ProARD), enabling the efficient one-time training of a dynamic network that
supports a diverse range of accurate and robust student networks without
requiring retraining. We first make a dynamic deep neural network based on
dynamic layers by encompassing variations in width, depth, and expansion in
each design stage to support a wide range of architectures. Then, we consider
the student network with the largest size as the dynamic teacher network.
ProARD trains this dynamic network using a weight-sharing mechanism to jointly
optimize the dynamic teacher network and its internal student networks.
However, due to the high computational cost of calculating exact gradients for
all the students within the dynamic network, a sampling mechanism is required
to select a subset of students. We show that random student sampling in each
iteration fails to produce accurate and robust students.

</details>


### [590] [How Benchmark Prediction from Fewer Data Misses the Mark](https://arxiv.org/abs/2506.07673)
*Guanhua Zhang,Florian E. Dorner,Moritz Hardt*

Main category: cs.LG

TL;DR: The paper studies efficient LLM evaluation by reducing benchmark sizes and evaluates 11 methods across 19 datasets. Random sampling and regression outperform most prior approaches, while challenges remain for extrapolating performance for novel models.


<details>
  <summary>Details</summary>
Motivation: Evaluating large language models is resource-intensive, and finding methods for efficient evaluation could save time and costs.

Method: The researchers compare 11 benchmark prediction methods, identifying random sampling with regression as a strong baseline and propose a new method utilizing augmented inverse propensity weighting.

Result: Random sampling with regression outperforms most existing methods, but all approaches struggle with extrapolation when models exceed prior performance. The new method shows modest improvements over random sampling average.

Conclusion: Efficient benchmark prediction methods are limited in extrapolating performance for frontier models, indicating a critical gap when assessing novel capabilities.

Abstract: Large language model (LLM) evaluation is increasingly costly, prompting
interest in methods that speed up evaluation by shrinking benchmark datasets.
Benchmark prediction (also called efficient LLM evaluation) aims to select a
small subset of evaluation points and predict overall benchmark performance
from that subset. In this paper, we systematically assess the strengths and
limitations of 11 benchmark prediction methods across 19 diverse benchmarks.
First, we identify a highly competitive baseline: Take a random sample and fit
a regression model on the sample to predict missing entries. Outperforming most
existing methods, this baseline challenges the assumption that careful subset
selection is necessary for benchmark prediction. Second, we discover that all
existing methods crucially depend on model similarity. They work best when
interpolating scores among similar models. The effectiveness of benchmark
prediction sharply declines when new models have higher accuracy than
previously seen models. In this setting of extrapolation, none of the previous
methods consistently beat a simple average over random samples. To improve over
the sample average, we introduce a new method inspired by augmented inverse
propensity weighting. This method consistently outperforms the random sample
average even for extrapolation. However, its performance still relies on model
similarity and the gains are modest in general. This shows that benchmark
prediction fails just when it is most needed: at the evaluation frontier, where
the goal is to evaluate new models of unknown capabilities.

</details>


### [591] [Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation](https://arxiv.org/abs/2506.07706)
*Boris Martirosyan,Alexey Karmanov*

Main category: cs.LG

TL;DR: This paper examines latent diffusion models' (LDMs) robustness, introduces data augmentation methods, evaluates fine-tuned models through Dreambooth, and proposes an evaluation pipeline.


<details>
  <summary>Details</summary>
Motivation: Address the unexplored limitations in the robustness of latent diffusion models and improve them for better text-to-image alignment.

Method: Hypothesize separating text encoder issues from LDMs, introduce novel data augmentations, fine-tune Stable Diffusion using Dreambooth, and design a new evaluation pipeline.

Result: LDM robustness was tested with new data augmentation techniques on diverse prompts, and fine-tuned models showed improved handling of tasks.

Conclusion: Enhancing robustness in LDMs is achievable through targeted techniques like novel augmentations and fine-tuning workflows, providing better evaluation methods.

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art performance across
various tasks, including image generation and video synthesis. However, they
generally lack robustness, a limitation that remains not fully explored in
current research. In this paper, we propose several methods to address this
gap. First, we hypothesize that the robustness of LDMs primarily should be
measured without their text encoder, because if we take and explore the whole
architecture, the problems of image generator and text encoders wll be fused.
Second, we introduce novel data augmentation techniques designed to reveal
robustness shortcomings in LDMs when processing diverse textual prompts. We
then fine-tune Stable Diffusion 3 and Stable Diffusion XL models using
Dreambooth, incorporating these proposed augmentation methods across multiple
tasks. Finally, we propose a novel evaluation pipeline specifically tailored to
assess the robustness of LDMs fine-tuned via Dreambooth.

</details>


### [592] [Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning](https://arxiv.org/abs/2506.07735)
*Haizhao Jing,Haokui Zhang,Zhenhao Shang,Rong Xiao,Peng Wang,Yanning Zhang*

Main category: cs.LG

TL;DR: This paper presents LeDG-Former, a novel framework for neural architecture representation learning using dynamic graph and language-based embeddings that enhance the prediction of network attributes and hardware performance.


<details>
  <summary>Details</summary>
Motivation: Gaps exist in current representation learning methods as hardware attributes are neglected, and static adjacency matrices fail to capture dynamic structural differences of computational nodes.

Method: LeDG-Former combines language-based semantic embeddings inspired by large language models with dynamic graph-based transformers to model neural architectures more effectively.

Result: LeDG-Former achieves state-of-the-art performance on the NNLQP benchmark, enabling zero-shot prediction across hardware platforms, and outperforms on NAS-Bench-101 and NAS-Bench-201 datasets.

Conclusion: The proposed approach successfully addresses key shortcomings, achieving significant representation learning improvements and practical applicability across diversified hardware platforms.

Abstract: Neural Architecture Representation Learning aims to transform network models
into feature representations for predicting network attributes, playing a
crucial role in deploying and designing networks for real-world applications.
Recently, inspired by the success of transformers, transformer-based models
integrated with Graph Neural Networks (GNNs) have achieved significant progress
in representation learning. However, current methods still have some
limitations. First, existing methods overlook hardware attribute information,
which conflicts with the current trend of diversified deep learning hardware
and limits the practical applicability of models. Second, current encoding
approaches rely on static adjacency matrices to represent topological
structures, failing to capture the structural differences between computational
nodes, which ultimately compromises encoding effectiveness. In this paper, we
introduce LeDG-Former, an innovative framework that addresses these limitations
through the synergistic integration of language-based semantic embedding and
dynamic graph representation learning. Specifically, inspired by large language
models (LLMs), we propose a language embedding framework where both neural
architectures and hardware platform specifications are projected into a unified
semantic space through tokenization and LLM processing, enabling zero-shot
prediction across different hardware platforms for the first time. Then, we
propose a dynamic graph-based transformer for modeling neural architectures,
resulting in improved neural architecture modeling performance. On the NNLQP
benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA
while demonstrating the first successful cross-hardware latency prediction
capability. Furthermore, our framework achieves superior performance on the
cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.

</details>


### [593] [Comparing Credit Risk Estimates in the Gen-AI Era](https://arxiv.org/abs/2506.07754)
*Nicola Lavecchia,Sid Fadanelli,Federico Ricciuti,Gennaro Aloe,Enrico Bagli,Pietro Giuffrida,Daniele Vergari*

Main category: cs.LG

TL;DR: The paper compares traditional credit scoring methods to generative AI-based models, finding the latter currently underperform.


<details>
  <summary>Details</summary>
Motivation: To understand the effectiveness of generative AI in the context of credit score modeling and its potential versus traditional methods.

Method: Conducted a comparative analysis between traditional credit scoring techniques and generative AI-based methods to assess their performance.

Result: Generative AI models underperform compared to traditional techniques, even with varied integration strategies.

Conclusion: Generative AI is not yet suitable for credit risk scoring and requires further development for applications in such tasks.

Abstract: Generative AI technologies have demonstrated significant potential across
diverse applications. This study provides a comparative analysis of credit
score modeling techniques, contrasting traditional approaches with those
leveraging generative AI. Our findings reveal that current generative AI models
fall short of matching the performance of traditional methods, regardless of
the integration strategy employed. These results highlight the limitations in
the current capabilities of generative AI for credit risk scoring, emphasizing
the need for further research and development before the possibility of
applying generative AI for this specific task, or equivalent ones.

</details>


### [594] [Clustered Federated Learning via Embedding Distributions](https://arxiv.org/abs/2506.07769)
*Dekai Zhang,Matthew Williams,Francesca Toni*

Main category: cs.LG

TL;DR: The paper introduces a novel federated learning clustering method (EMD-CFL) leveraging Earth Mover's distance for more efficient client classification.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to non-IID data issues due to clients holding decentralized and varied datasets, necessitating better clustering methods.

Method: The authors use Earth Mover's distance to measure distributional differences in embedding spaces and cluster clients in a one-shot manner.

Result: EMD-CFL demonstrated enhanced performance over 16 baseline methods across diverse datasets through extensive empirical evaluation.

Conclusion: The study establishes EMD-CFL as an effective solution to address heterogeneous client clustering issues in federated learning setups.

Abstract: Federated learning (FL) is a widely used framework for machine learning in
distributed data environments where clients hold data that cannot be easily
centralised, such as for data protection reasons. FL, however, is known to be
vulnerable to non-IID data. Clustered FL addresses this issue by finding more
homogeneous clusters of clients. We propose a novel one-shot clustering method,
EMD-CFL, using the Earth Mover's distance (EMD) between data distributions in
embedding space. We theoretically motivate the use of EMDs using results from
the domain adaptation literature and demonstrate empirically superior
clustering performance in extensive comparisons against 16 baselines and on a
range of challenging datasets.

</details>


### [595] [Identifiable Object Representations under Spatial Ambiguities](https://arxiv.org/abs/2506.07806)
*Avinash Kori,Francesca Toni,Ben Glocker*

Main category: cs.LG

TL;DR: The paper introduces a probabilistic method to overcome spatial ambiguities in generating modular object-centric representations by integrating multi-view data without requiring viewpoint annotations.


<details>
  <summary>Details</summary>
Motivation: Spatial ambiguities, such as occlusions and viewpoint challenges, make it difficult to achieve modular object-centric representations essential for human-like reasoning.

Method: A multi-view probabilistic approach aggregates view-specific slots to capture invariant content information and disentangled global viewpoint-level information, with theoretical guarantees for identifiability.

Result: Experiments on standard benchmarks and new complex datasets demonstrate the robustness and scalability of the proposed method.

Conclusion: The method effectively addresses spatial ambiguities by resolving view-based challenges, contributing theoretically and practically to object-centric representations.

Abstract: Modular object-centric representations are essential for *human-like
reasoning* but are challenging to obtain under spatial ambiguities, *e.g. due
to occlusions and view ambiguities*. However, addressing challenges presents
both theoretical and practical difficulties. We introduce a novel multi-view
probabilistic approach that aggregates view-specific slots to capture
*invariant content* information while simultaneously learning disentangled
global *viewpoint-level* information. Unlike prior single-view methods, our
approach resolves spatial ambiguities, provides theoretical guarantees for
identifiability, and requires *no viewpoint annotations*. Extensive experiments
on standard benchmarks and novel complex datasets validate our method's
robustness and scalability.

</details>


### [596] [Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation](https://arxiv.org/abs/2506.07822)
*Xintong Duan,Yutong He,Fahim Tajwar,Ruslan Salakhutdinov,J. Zico Kolter,Jeff Schneider*

Main category: cs.LG

TL;DR: The paper introduces a refined approach to offline reinforcement learning that integrates reward optimization into consistency distillation, offering faster inference speeds and improved performance.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to address the slow inference speed of diffusion models in decision-making tasks, and overcome limitations of existing methods in consistency distillation that depend on suboptimal demonstrations and complex training mechanisms.

Method: The proposed method incorporates reward optimization directly into the consistency distillation process, enabling single-step generation with simplified training procedures, without sacrificing performance.

Result: Empirical evaluations show an 8.7% improvement over previous state-of-the-art results, and up to 142x speedup in inference time compared to diffusion-based models.

Conclusion: The approach resolves the key limitation of slow inference in diffusion models, while achieving higher performance and simpler training, marking a significant advancement in offline reinforcement learning scenarios.

Abstract: Although diffusion models have achieved strong results in decision-making
tasks, their slow inference speed remains a key limitation. While the
consistency model offers a potential solution, its applications to
decision-making often struggle with suboptimal demonstrations or rely on
complex concurrent training of multiple networks. In this work, we propose a
novel approach to consistency distillation for offline reinforcement learning
that directly incorporates reward optimization into the distillation process.
Our method enables single-step generation while maintaining higher performance
and simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and
long horizon planning demonstrate that our approach can achieve an 8.7%
improvement over previous state-of-the-art while offering up to 142x speedup
over diffusion counterparts in inference time.

</details>


### [597] [Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information](https://arxiv.org/abs/2506.07829)
*Jan Corazza,Hadi Partovi Aria,Hyohun Kim,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: This paper explores how high-level symbolic knowledge can aid decentralized multi-agent reinforcement learning (DMARL) by improving policy compatibility and performance, while addressing challenges like privacy constraints and communication limitations.


<details>
  <summary>Details</summary>
Motivation: The paper focuses on real-world problems where multiple agents must collaborate to achieve a common goal, such as robots and drones working together in constrained environments.

Method: The authors integrate high-level symbolic knowledge into DMARL frameworks to enhance policy compatibility, decentralized training, and learning efficiency.

Result: The study shows that symbolic knowledge improves decentralized training with theoretical guarantees and expedites the learning process in DMARL scenarios.

Conclusion: Introducing symbolic knowledge significantly bolsters the practical usability and performance of DMARL frameworks, making decentralized collaboration more effective in various settings.

Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a
single agent to accomplish a particular task. However, many real-world problems
require multiple agents to collaborate in order to achieve a common goal. For
example, a robot executing a task in a warehouse may require the assistance of
a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL
(DMARL), agents learn independently and then combine their policies at
execution time, but often must satisfy constraints on compatibility of local
policies to ensure that they can achieve the global task when combined. In this
paper, we study how providing high-level symbolic knowledge to agents can help
address unique challenges of this setting, such as privacy constraints,
communication limitations, and performance concerns. In particular, we extend
the formal tools used to check the compatibility of local policies with the
team task, making decentralized training with theoretical guarantees usable in
more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge
about the temporal evolution of events in the environment can significantly
expedite the learning process in DMARL.

</details>


### [598] [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
*Michael K. Chen,Xikun Zhang,Jiaxing Huang,Dacheng Tao*

Main category: cs.LG

TL;DR: The paper introduces Concept-Aware Fine-Tuning (CAFT), a multi-token training method applied in the fine-tuning stage of large language models (LLMs), overcoming a limitation where models struggle with high-level concept learning.


<details>
  <summary>Details</summary>
Motivation: LLMs' next-token prediction approach limits their conceptual understanding. For instance, models treat multi-token terms like 'ribonucleic acid' as fragmented sequences rather than cohesive units, hindering their reasoning abilities and deeper understanding.

Method: The proposed method, Concept-Aware Fine-Tuning (CAFT), enables multi-token learning during the fine-tuning stage. This expands beyond single-token prediction by allowing the model to understand concepts spanning multiple tokens, bypassing the need for expensive multi-token learning during pretraining.

Result: CAFT outperforms traditional next-token fine-tuning in tasks such as text summarization and de novo protein design, showcasing significant improvements in both general and domain-specific applications.

Conclusion: CAFT brings multi-token learning to the post-training stage, making the approach accessible to the broader community. The method's success hints at potentially transformative implications for the machine learning field and smarter AI systems.

Abstract: Large language models (LLMs) have become the cornerstone of modern AI.
However, the existing paradigm of next-token prediction fundamentally limits
their ability to form coherent, high-level concepts, making it a critical
barrier to human-like understanding and reasoning. Take the phrase "ribonucleic
acid" as an example: an LLM will first decompose it into tokens, i.e.,
artificial text fragments ("rib", "on", ...), then learn each token
sequentially, rather than grasping the phrase as a unified, coherent semantic
entity. This fragmented representation hinders deeper conceptual understanding
and, ultimately, the development of truly intelligent systems. In response, we
introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method
that redefines how LLMs are fine-tuned. By enabling the learning of sequences
that span multiple tokens, this method fosters stronger concept-aware learning.
Our experiments demonstrate significant improvements compared to conventional
next-token finetuning methods across diverse tasks, including traditional
applications like text summarization and domain-specific ones like de novo
protein design. Multi-token prediction was previously only possible in the
prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first
to bring the multi-token setting to the post-training phase, thus effectively
democratizing its benefits for the broader community of practitioners and
researchers. Finally, the unexpected effectiveness of our proposed method
suggests wider implications for the machine learning research community. All
code and data are available at https://github.com/michaelchen-lab/caft-llm

</details>


### [599] [Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels](https://arxiv.org/abs/2506.07843)
*Davide Carbone*

Main category: cs.LG

TL;DR: This paper introduces Jarzynski reweighting, a statistical mechanics technique, to improve training in Energy-Based Models (EBMs) with applications in flow-based diffusion models and Restricted Boltzmann Machines.


<details>
  <summary>Details</summary>
Motivation: Energy-Based Models are flexible but face challenges in training due to biases and inefficiencies in traditional approaches like contrastive divergence.

Method: The authors analyze Jarzynski reweighting to address biases and discretization errors in two frameworks: flow-based diffusion models and Restricted Boltzmann Machines, emphasizing the role of kernel choice.

Result: Jarzynski reweighting is shown to mitigate discretization errors and biases, improving sample quality and addressing training inefficiencies in EBMs.

Conclusion: This work highlights Jarzynski reweighting as a principled tool for overcoming theoretical challenges in generative modeling within EBMs.

Abstract: Energy-Based Models (EBMs) provide a flexible framework for generative
modeling, but their training remains theoretically challenging due to the need
to approximate normalization constants and efficiently sample from complex,
multi-modal distributions. Traditional methods, such as contrastive divergence
and score matching, introduce biases that can hinder accurate learning. In this
work, we present a theoretical analysis of Jarzynski reweighting, a technique
from non-equilibrium statistical mechanics, and its implications for training
EBMs. We focus on the role of the choice of the kernel and we illustrate these
theoretical considerations in two key generative frameworks: (i) flow-based
diffusion models, where we reinterpret Jarzynski reweighting in the context of
stochastic interpolants to mitigate discretization errors and improve sample
quality, and (ii) Restricted Boltzmann Machines, where we analyze its role in
correcting the biases of contrastive divergence. Our results provide insights
into the interplay between kernel choice and model performance, highlighting
the potential of Jarzynski reweighting as a principled tool for generative
learning.

</details>


### [600] [Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective](https://arxiv.org/abs/2506.07861)
*Firas Laakom,Haobo Chen,Jürgen Schmidhuber,Yuheng Bu*

Main category: cs.LG

TL;DR: This paper uses an information-theoretic approach to calculate fairness generalization error in machine learning and provides tight bounds using Mutual Information and Conditional Mutual Information.


<details>
  <summary>Details</summary>
Motivation: Existing methods to ensure fairness in machine learning models lack formal guarantees that fairness achieved during training will generalize to unseen data. Overfitting in terms of fairness loss is underexplored compared to overfitting in prediction performance.

Method: The authors propose a theoretical framework using an information-theoretic approach and Efron-Stein inequality to derive fairness generalization error bounds. They utilize both Mutual Information (MI) and Conditional Mutual Information (CMI) to calculate these bounds.

Result: Empirical results demonstrate the tightness and practical applicability of the proposed bounds when tested on various fairness-aware learning algorithms.

Conclusion: The framework provides insights for designing algorithms that enhance fairness generalization, addressing a significant gap in current fairness-aware machine learning research.

Abstract: Despite substantial progress in promoting fairness in high-stake applications
using machine learning models, existing methods often modify the training
process, such as through regularizers or other interventions, but lack formal
guarantees that fairness achieved during training will generalize to unseen
data. Although overfitting with respect to prediction performance has been
extensively studied, overfitting in terms of fairness loss has received far
less attention. This paper proposes a theoretical framework for analyzing
fairness generalization error through an information-theoretic lens. Our novel
bounding technique is based on Efron-Stein inequality, which allows us to
derive tight information-theoretic fairness generalization bounds with both
Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical
results validate the tightness and practical relevance of these bounds across
diverse fairness-aware learning algorithms. Our framework offers valuable
insights to guide the design of algorithms improving fairness generalization.

</details>


### [601] [Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes](https://arxiv.org/abs/2506.07864)
*Mirko Paolo Barbato,Giorgia Rigamonti,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: This work introduces a Lightweight Sequential Transformer model optimized for blood glucose prediction in T1D, overcoming deployment challenges on wearable devices.


<details>
  <summary>Details</summary>
Motivation: Enhancing continuous monitoring tools for T1D to prevent severe glycemic events while addressing computational constraints in wearable devices.

Method: The study proposes a hybrid model merging Transformers' attention mechanisms with sequential recurrent neural networks, tailored for edge devices and a balanced loss function for data imbalance.

Result: Experiments on OhioT1DM and DiaTrend datasets show that the proposed model surpasses state-of-the-art methods in glucose prediction and adverse event detection.

Conclusion: The developed lightweight model bridges the gap between high-performance predictive modeling and practical deployment, aiding efficient T1D management on resource-limited devices.

Abstract: Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous
monitoring to prevent severe hypo- and hyperglycemic events. While continuous
glucose monitoring has improved blood glucose management, deploying predictive
models on wearable devices remains challenging due to computational and memory
constraints. To address this, we propose a novel Lightweight Sequential
Transformer model designed for blood glucose prediction in T1D. By integrating
the strengths of Transformers' attention mechanisms and the sequential
processing of recurrent neural networks, our architecture captures long-term
dependencies while maintaining computational efficiency. The model is optimized
for deployment on resource-constrained edge devices and incorporates a balanced
loss function to handle the inherent data imbalance in hypo- and hyperglycemic
events. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,
demonstrate that the proposed model outperforms state-of-the-art methods in
predicting glucose levels and detecting adverse events. This work fills the gap
between high-performance modeling and practical deployment, providing a
reliable and efficient T1D management solution.

</details>


### [602] [Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions](https://arxiv.org/abs/2506.07884)
*Anand Ganesh,Babhrubahan Bose,Anand Rajagopalan*

Main category: cs.LG

TL;DR: The paper introduces four Schauder bases for $C[0,1]$ using ReLU, Softplus, and their sigmoidal versions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish new bases for $C[0,1]$ using commonly used activation functions to enhance theoretical understanding and approximation properties.

Method: The authors construct four distinct bases using functions such as ReLU, Softplus, and their sigmoidal variants.

Result: The paper successfully establishes the existence of these bases and demonstrates improved approximation properties for the first time.

Conclusion: These findings advance the universal approximation theory and provide a new perspective on using activation functions as bases in functional spaces.

Abstract: We construct four Schauder bases for the space $C[0,1]$, one using ReLU
functions, another using Softplus functions, and two more using sigmoidal
versions of the ReLU and Softplus functions. This establishes the existence of
a basis using these functions for the first time, and improves on the universal
approximation property associated with them.

</details>


### [603] [Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces](https://arxiv.org/abs/2506.07903)
*Kevin Rojas,Yuchen Zhu,Sichen Zhu,Felix X. -F. Ye,Molei Tao*

Main category: cs.LG

TL;DR: The paper introduces a novel framework for multimodal diffusion models that do not rely on preprocessing like tokenizers or autoencoders, enabling native coupled data generation across modalities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to move beyond existing multimodal generation approaches that rely heavily on preprocessing steps and struggle with limited data scenarios.

Method: The method involves creating a framework for multimodal diffusion models that includes a decoupled noise schedule for each modality to enable conditional and unconditional generation.

Result: The framework is empirically validated for text-image generation and tabular data, showing competitive performance.

Conclusion: The proposed framework represents a significant step toward more versatile multimodal generation methods without requiring extensive preprocessing.

Abstract: Diffusion models have demonstrated remarkable performance in generating
unimodal data across various tasks, including image, video, and text
generation. On the contrary, the joint generation of multimodal data through
diffusion models is still in the early stages of exploration. Existing
approaches heavily rely on external preprocessing protocols, such as tokenizers
and variational autoencoders, to harmonize varied data representations into a
unified, unimodal format. This process heavily demands the high accuracy of
encoders and decoders, which can be problematic for applications with limited
data. To lift this restriction, we propose a novel framework for building
multimodal diffusion models on arbitrary state spaces, enabling native
generation of coupled data across different modalities. By introducing an
innovative decoupled noise schedule for each modality, we enable both
unconditional and modality-conditioned generation within a single model
simultaneously. We empirically validate our approach for text-image generation
and mixed-type tabular data synthesis, demonstrating that it achieves
competitive performance.

</details>


### [604] [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
*Manuel Brenner,Georgia Koppe*

Main category: cs.LG

TL;DR: The paper systematically studies the computational role of nonlinearity in recurrent neural networks, suggesting minimal nonlinearity is often sufficient and optimal for sequence modeling tasks.


<details>
  <summary>Details</summary>
Motivation: To explore when nonlinearity is essential in recurrent neural networks and clarify the mechanisms it enables, particularly in tasks requiring long-range memory and structured computation.

Method: The authors employ Almost Linear Recurrent Neural Networks (AL-RNNs), which allow precise control over levels of nonlinearity, to analyze their performance on traditional sequence modeling tasks and a real-world application.

Result: AL-RNNs with minimal nonlinearity not only perform sufficiently but often outperform fully nonlinear or linear models in terms of simplicity, robustness, and interpretability.

Conclusion: The study offers a framework for strategically incorporating nonlinearity in RNNs, merging insights from dynamical systems theory with long-range memory processing needs, and providing implications for both artificial and biological neural networks.

Abstract: Memory and long-range temporal processing are core requirements for sequence
modeling tasks across natural language processing, time-series forecasting,
speech recognition, and control. While nonlinear recurrence has long been
viewed as essential for enabling such mechanisms, recent work suggests that
linear dynamics may often suffice. In this study, we go beyond performance
comparisons to systematically dissect the functional role of nonlinearity in
recurrent networks--identifying both when it is computationally necessary, and
what mechanisms it enables. We use Almost Linear Recurrent Neural Networks
(AL-RNNs), which allow fine-grained control over nonlinearity, as both a
flexible modeling tool and a probe into the internal mechanisms of memory.
Across a range of classic sequence modeling tasks and a real-world stimulus
selection task, we find that minimal nonlinearity is not only sufficient but
often optimal, yielding models that are simpler, more robust, and more
interpretable than their fully nonlinear or linear counterparts. Our results
provide a principled framework for selectively introducing nonlinearity,
bridging dynamical systems theory with the functional demands of long-range
memory and structured computation in recurrent neural networks, with
implications for both artificial and biological neural systems.

</details>


### [605] [W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling](https://arxiv.org/abs/2506.07920)
*Hossein Babaei,Mel White,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: The paper introduces W4S4, an improvement on existing State Space Models (SSMs), utilizing wavelet-based state dynamics for better handling long-range dependencies efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of initializing state matrices in SSMs for effective long-range dependency modeling while maintaining computational efficiency.

Method: The WaLRUS framework was proposed, which leverages redundant wavelet frames for stable diagonalization and fast kernel computation without requiring low-rank approximations.

Result: WaLRUS improves long-horizon memory retention and demonstrates superior results across delay reconstruction tasks, classification benchmarks, and long-range sequence modeling compared to existing HiPPO-based SSMs.

Conclusion: Wavelet-based state dynamics offer scalable, structured, and high-quality initialization for SSMs, making WaLRUS a promising foundation for future deep sequence models.

Abstract: State Space Models (SSMs) have emerged as powerful components for sequence
modeling, enabling efficient handling of long-range dependencies via linear
recurrence and convolutional computation. However, their effectiveness depends
heavily on the choice and initialization of the state matrix. In this work, we
build on the SaFARi framework and existing WaLRUS SSMs to introduce a new
variant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant
wavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel
computation without requiring low-rank approximations, making it both
theoretically grounded and computationally efficient. We show that WaLRUS
retains information over long horizons significantly better than HiPPO-based
SSMs, both in isolation and when integrated into deep architectures such as S4.
Our experiments demonstrate consistent improvements across delay reconstruction
tasks, classification benchmarks, and long-range sequence modeling, confirming
that high-quality, structured initialization enabled by wavelet-based state
dynamic offers substantial advantages over existing alternatives. WaLRUS
provides a scalable and versatile foundation for the next generation of deep
SSM-based models.

</details>


### [606] [A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle](https://arxiv.org/abs/2506.07929)
*Amirreza Yasami,Mohammadali Tofigh,Mahdi Shahbakhti,Charles Robert Koch*

Main category: cs.LG

TL;DR: The paper proposes the Physics-Informed Expected SARSA-Monte Carlo (PIESMC) method for efficient, accurate driving cycle construction, outperforming traditional methods in both accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Driving cycle construction is essential for vehicle design, fuel economy analysis, and environmental impact assessments, necessitating a method that accurately captures dynamics while being computationally efficient.

Method: PIESMC uses a physics-informed reinforcement learning framework combined with Monte Carlo sampling to construct driving cycles, replicating transient dynamics, kinematic metrics, and energy characteristics.

Result: Experimental evaluations show PIESMC achieves up to 57.3% error reduction over the Micro-trip-based method and 10.5% over the Markov-chain-based method, while being nearly an order of magnitude faster.

Conclusion: PIESMC is a robust, efficient method for constructing representative driving cycles, with higher accuracy and lower computational cost, confirmed through kinematic and energy metric replication on real-world datasets.

Abstract: Accurate driving cycle construction is crucial for vehicle design, fuel
economy analysis, and environmental impact assessments. A generative
Physics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs
representative driving cycles by capturing transient dynamics, acceleration,
deceleration, idling, and road grade transitions while ensuring model fidelity
is introduced. Leveraging a physics-informed reinforcement learning framework
with Monte Carlo sampling, PIESMC delivers efficient cycle construction with
reduced computational cost. Experimental evaluations on two real-world datasets
demonstrate that PIESMC replicates key kinematic and energy metrics, achieving
up to a 57.3% reduction in cumulative kinematic fragment errors compared to the
Micro-trip-based (MTB) method and a 10.5% reduction relative to the
Markov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude
faster than conventional techniques. Analyses of vehicle-specific power
distributions and wavelet-transformed frequency content further confirm its
ability to reproduce experimental central tendencies and variability.

</details>


### [607] [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
*Hongzheng Chen,Yingheng Wang,Yaohui Cai,Hins Hu,Jiajie Li,Shirley Huang,Chenhui Deng,Rongjian Liang,Shufeng Kong,Haoxing Ren,Samitha Samaranayake,Carla P. Gomes,Zhiru Zhang*

Main category: cs.LG

TL;DR: The paper introduces HeuriGym, a framework to robustly evaluate LLMs' heuristic problem-solving on complex combinatorial tasks, demonstrating limitations and proposing metrics like Quality-Yield Index (QYI).


<details>
  <summary>Details</summary>
Motivation: Current benchmarks inadequately evaluate LLMs' reasoning and problem-solving skills due to reliance on closed-ended or subjective methods.

Method: The authors designed HeuriGym, a framework allowing LLMs to generate, evaluate, refine heuristics on combinatorial optimization problems via code execution and iterative feedback.

Result: Evaluation of nine models across diverse domains revealed significant limitations in their problem-solving capabilities, with leading models scoring a QYI of only 0.6 versus the expert baseline of 1.

Conclusion: HeuriGym highlights the gaps in adaptive reasoning and practical tool use of LLMs, aiming to guide their improvement in engineering and scientific applications.

Abstract: While Large Language Models (LLMs) have demonstrated significant advancements
in reasoning and agent-based problem-solving, current evaluation methodologies
fail to adequately assess their capabilities: existing benchmarks either rely
on closed-ended questions prone to saturation and memorization, or subjective
comparisons that lack consistency and rigor. In this work, we introduce
HeuriGym, an agentic framework designed for evaluating heuristic algorithms
generated by LLMs for combinatorial optimization problems, characterized by
clearly defined objectives and expansive solution spaces. HeuriGym empowers
LLMs to propose heuristics, receive evaluative feedback via code execution, and
iteratively refine their solutions. We evaluate nine state-of-the-art models on
nine problems across domains such as computer systems, logistics, and biology,
exposing persistent limitations in tool use, planning, and adaptive reasoning.
To quantify performance, we propose the Quality-Yield Index (QYI), a metric
that captures both solution pass rate and quality. Even top models like
GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below
the expert baseline of 1. Our open-source benchmark aims to guide the
development of LLMs toward more effective and realistic problem-solving in
scientific and engineering domains.

</details>


### [608] [TokenBreak: Bypassing Text Classification Models Through Token Manipulation](https://arxiv.org/abs/2506.07948)
*Kasimir Schulz,Kenneth Yeung,Kieran Evans*

Main category: cs.LG

TL;DR: The paper introduces TokenBreak, an attack exploiting tokenization schemes to bypass NLP text classification defenses, while suggesting a mitigation strategy that doesn't require retraining.


<details>
  <summary>Details</summary>
Motivation: The motivation involves addressing the vulnerability of text classification models deployed for protecting against threats (like prompt injection attacks and spam), highlighting flaws in tokenization strategies that attackers can exploit.

Method: The authors developed TokenBreak, a novel attack manipulating input texts to bypass classification defenses while ensuring the manipulated texts remain comprehensible to their end targets, enabling prediction of vulnerable models by tokenizer architecture.

Result: The attack successfully bypasses text classification models used for protection, exposing vulnerabilities tied to tokenizer-model architecture associations.

Conclusion: Existing tokenization schemes make models prone to specific bypass attacks, necessitating supplementary protective measures that can enhance defenses without necessitating model retraining.

Abstract: Natural Language Processing (NLP) models are used for text-related tasks such
as classification and generation. To complete these tasks, input data is first
tokenized from human-readable text into a format the model can understand,
enabling it to make inferences and understand context. Text classification
models can be implemented to guard against threats such as prompt injection
attacks against Large Language Models (LLMs), toxic input and cybersecurity
risks such as spam emails. In this paper, we introduce TokenBreak: a novel
attack that can bypass these protection models by taking advantage of the
tokenization strategy they use. This attack technique manipulates input text in
such a way that certain models give an incorrect classification. Importantly,
the end target (LLM or email recipient) can still understand and respond to the
manipulated text and therefore be vulnerable to the very attack the protection
model was put in place to prevent. The tokenizer is tied to model architecture,
meaning it is possible to predict whether or not a model is vulnerable to
attack based on family. We also present a defensive strategy as an added layer
of protection that can be implemented without having to retrain the defensive
model.

</details>


### [609] [Cost-Optimal Active AI Model Evaluation](https://arxiv.org/abs/2506.07949)
*Anastasios N. Angelopoulos,Jacob Eisenstein,Jonathan Berant,Alekh Agarwal,Adam Fisch*

Main category: cs.LG

TL;DR: The paper introduces cost-aware methods to optimally balance the use of weak (cheap, automated) raters and strong (accurate, expensive) raters for estimating the quality of AI-generated content, offering improved statistical efficiency under budget constraints.


<details>
  <summary>Details</summary>
Motivation: Generative AI systems require costly evaluation processes, which often involve the use of synthetic, biased data to reduce costs. Despite the drawbacks, there is a need for methods to balance accuracy and cost effectively.

Method: The authors derive cost-optimal policies based on active statistical inference to allocate annotation budgets efficiently between weak and strong raters. These policies aim to minimize estimation variance while adhering to budget constraints.

Result: Empirical evaluation demonstrates that the proposed policies outperform standard methods, particularly in tasks with high variability in example difficulty. They achieve similar precision at a lower budget.

Conclusion: Using cost-efficient allocation between weak and strong raters significantly improves evaluation accuracy and reduces annotation costs, benefitting tasks with challenging variability.

Abstract: The development lifecycle of generative AI systems requires continual
evaluation, data acquisition, and annotation, which is costly in both resources
and time. In practice, rapid iteration often makes it necessary to rely on
synthetic annotation data because of the low cost, despite the potential for
substantial bias. In this paper, we develop novel, cost-aware methods for
actively balancing the use of a cheap, but often inaccurate, weak rater -- such
as a model-based autorater that is designed to automatically assess the quality
of generated content -- with a more expensive, but also more accurate, strong
rater alternative such as a human. More specifically, the goal of our approach
is to produce a low variance, unbiased estimate of the mean of the target
"strong" rating, subject to some total annotation budget. Building on recent
work in active and prediction-powered statistical inference, we derive a family
of cost-optimal policies for allocating a given annotation budget between weak
and strong raters so as to maximize statistical efficiency. Using synthetic and
real-world data, we empirically characterize the conditions under which these
policies yield improvements over prior methods. We find that, especially in
tasks where there is high variability in the difficulty of examples, our
policies can achieve the same estimation precision at a far lower total
annotation budget than standard evaluation methods.

</details>


### [610] [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
*Zeju Qiu,Simon Buchholz,Tim Z. Xiao,Maximilian Dax,Bernhard Schölkopf,Weiyang Liu*

Main category: cs.LG

TL;DR: POET introduces a new training algorithm for large language models using orthogonal matrices and random weight matrices, improving generalization and scalability.


<details>
  <summary>Details</summary>
Motivation: Training large language models effectively and reliably remains a significant challenge in artificial intelligence.

Method: The POET algorithm reparameterizes neurons using two learnable orthogonal matrices and a fixed random weight matrix while preserving spectral properties to stabilize optimization.

Result: Experimental results demonstrate the effectiveness and scalability of POET for training large-scale neural networks, including large language models.

Conclusion: POET offers a novel approach to optimize neural networks, addressing core challenges in training large language models and improving training stability and scalability.

Abstract: While large language models (LLMs) are driving the rapid advancement of
artificial intelligence, effectively and reliably training these large models
remains one of the field's most significant challenges. To address this
challenge, we propose POET, a novel reParameterized training algorithm that
uses Orthogonal Equivalence Transformation to optimize neurons. Specifically,
POET reparameterizes each neuron with two learnable orthogonal matrices and a
fixed random weight matrix. Because of its provable preservation of spectral
properties of weight matrices, POET can stably optimize the objective function
with improved generalization. We further develop efficient approximations that
make POET flexible and scalable for training large-scale neural networks.
Extensive experiments validate the effectiveness and scalability of POET in
training LLMs.

</details>


### [611] [Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs](https://arxiv.org/abs/2506.07958)
*Salah A. Faroughi,Farinaz Mostajeran*

Main category: cs.LG

TL;DR: This paper investigates the training dynamics and convergence behavior of Chebyshev-based Physics-informed Kolmogorov-Arnold Networks (cPIKANs) using Neural Tangent Kernel (NTK) theory, focusing on spectral properties across various PDEs and optimization strategies.


<details>
  <summary>Details</summary>
Motivation: Partial Differential Equations (PDEs) are challenging to solve, and cPIKANs have shown promise. However, there is limited understanding of their training dynamics and convergence behavior.

Method: The authors use Neural Tangent Kernel (NTK) theory to analyze the spectral properties and kernel evolution in cPIKANs across different PDEs, while evaluating the impact of various optimization strategies.

Result: The NTK of cPIKANs demonstrates predictable and tractable behavior, highlighting unique learning dynamics. Spectral trends reveal how domain decomposition and optimization strategies influence convergence rates.

Conclusion: This study provides the first systematic NTK analysis of cPIKANs, offering theoretical insights that explain and predict their performance, advancing understanding of their training and convergence properties.

Abstract: Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their
Chebyshev-based variants (cPIKANs), have recently emerged as promising models
for solving partial differential equations (PDEs). However, their training
dynamics and convergence behavior remain largely unexplored both theoretically
and numerically. In this work, we aim to advance the theoretical understanding
of cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our
objective is to discern the evolution of kernel structure throughout
gradient-based training and its subsequent impact on learning efficiency. We
first derive the NTK of standard cKANs in a supervised setting, and then extend
the analysis to the physics-informed context. We analyze the spectral
properties of NTK matrices, specifically their eigenvalue distributions and
spectral bias, for four representative PDEs: the steady-state Helmholtz
equation, transient diffusion and Allen-Cahn equations, and forced vibrations
governed by the Euler-Bernoulli beam equation. We also conduct an investigation
into the impact of various optimization strategies, e.g., first-order,
second-order, and hybrid approaches, on the evolution of the NTK and the
resulting learning dynamics. Results indicate a tractable behavior for NTK in
the context of cPIKANs, which exposes learning dynamics that standard
physics-informed neural networks (PINNs) cannot capture. Spectral trends also
reveal when domain decomposition improves training, directly linking kernel
behavior to convergence rates under different setups. To the best of our
knowledge, this is the first systematic NTK study of cPIKANs, providing
theoretical insight that clarifies and predicts their empirical performance.

</details>


### [612] [A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in High-Speed Flow Modeling](https://arxiv.org/abs/2506.07969)
*Jacob Helwig,Sai Sreeharsha Adavi,Xuan Zhang,Yuchao Lin,Felix S. Chim,Luke Takeshi Vizzini,Haiyang Yu,Muhammad Hasnain,Saykat Kumar Biswas,John J. Holloway,Narendra Singh,N. K. Anand,Swagnik Guhathakurta,Shuiwang Ji*

Main category: cs.LG

TL;DR: ShockCast introduces a machine learning framework to model high-speed flows with adaptive time-stepping by predicting timestep size and advancing system states.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling flows approaching or exceeding the speed of sound, which require adaptive time-stepping methods due to phenomena like shock waves.

Method: Developed a two-phase machine learning method: first predicting timestep size, then using it to advance system states, incorporating strategies inspired by neural ODE and Mixture of Experts.

Result: Created high-speed flow datasets and evaluated the framework (ShockCast) performance; both datasets and code are shared publicly.

Conclusion: ShockCast establishes an innovative approach for modeling high-speed, supersonic flows and offers accessible tools and resources for future research.

Abstract: We consider the problem of modeling high-speed flows using machine learning
methods. While most prior studies focus on low-speed fluid flows in which
uniform time-stepping is practical, flows approaching and exceeding the speed
of sound exhibit sudden changes such as shock waves. In such cases, it is
essential to use adaptive time-stepping methods to allow a temporal resolution
sufficient to resolve these phenomena while simultaneously balancing
computational costs. Here, we propose a two-phase machine learning method,
known as ShockCast, to model high-speed flows with adaptive time-stepping. In
the first phase, we propose to employ a machine learning model to predict the
timestep size. In the second phase, the predicted timestep is used as an input
along with the current fluid fields to advance the system state by the
predicted timestep. We explore several physically-motivated components for
timestep prediction and introduce timestep conditioning strategies inspired by
neural ODE and Mixture of Experts. As ShockCast is the first framework for
learning high-speed flows, we evaluate our methods by generating two supersonic
flow datasets, available at https://huggingface.co/datasets/divelab. Our code
is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS).

</details>


### [613] [Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum](https://arxiv.org/abs/2506.07975)
*Caleb Zheng,Eli Shlizerman*

Main category: cs.LG

TL;DR: The paper proposes LS-based Hyperpruning (LSH), a method to efficiently identify optimal pruning strategies for Recurrent Neural Networks, leveraging a Lyapunov Spectrum-based metric for early performance prediction.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency and computational expense of exhaustive search over pruning configurations in over-parameterized Recurrent Neural Networks, while achieving or surpassing the performance of dense models.

Method: The authors propose a Lyapunov Spectrum (LS)-based distance metric to predict pruned network performance early, integrating it with hyperparameter optimization algorithms to create the LS-based Hyperpruning (LSH) framework.

Result: LSH reduced search time significantly compared to traditional full-training approaches and consistently identified superior pruned models on various datasets, often outperforming dense counterparts.

Conclusion: The LSH framework offers an effective and efficient solution for hyperpruning, enabling accurate, early identification of high-performance pruned network variants while reducing computational costs.

Abstract: A variety of pruning methods have been introduced for over-parameterized
Recurrent Neural Networks to improve efficiency in terms of power consumption
and storage utilization. These advances motivate a new paradigm, termed
`hyperpruning', which seeks to identify the most suitable pruning strategy for
a given network architecture and application. Unlike conventional
hyperparameter search, where the optimal configuration's accuracy remains
uncertain, in the context of network pruning, the accuracy of the dense model
sets the target for the accuracy of the pruned one. The goal, therefore, is to
discover pruned variants that match or even surpass this established accuracy.
However, exhaustive search over pruning configurations is computationally
expensive and lacks early performance guarantees. To address this challenge, we
propose a novel Lyapunov Spectrum (LS)-based distance metric that enables early
comparison between pruned and dense networks, allowing accurate prediction of
post-training performance. By integrating this LS-based distance with standard
hyperparameter optimization algorithms, we introduce an efficient hyperpruning
framework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an
order of magnitude compared to conventional approaches relying on full
training. Experiments on stacked LSTM and RHN architectures using the Penn
Treebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under
fixed training budgets and target pruning ratios, LSH consistently identifies
superior pruned models. Remarkably, these pruned variants not only outperform
those selected by loss-based baseline but also exceed the performance of their
dense counterpart.

</details>


### [614] [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/abs/2506.07976)
*Junhong Shen,Hao Bai,Lunjun Zhang,Yifei Zhou,Amrith Setlur,Shengbang Tong,Diego Caples,Nan Jiang,Tong Zhang,Ameet Talwalkar,Aviral Kumar*

Main category: cs.LG

TL;DR: This paper introduces test-time interaction (TTI) to enhance agent performance during deployment, providing a new dimension to agent behavior scaling beyond extended reasoning traces.


<details>
  <summary>Details</summary>
Motivation: Current approaches to scaling agent reasoning focus on generating lengthy traces but limit interaction with the environment, hindering adaptability.

Method: The authors propose scaling interaction horizons during test-time, employing a curriculum-based online reinforcement learning framework called TTI to train agents.

Result: TTI improves agent performance on web benchmarks, achieving state-of-the-art scores in WebVoyager and WebArena, and enables adaptive exploration and exploitation behaviors.

Conclusion: Interaction scaling complements traditional compute scaling, showcasing potential for developing more adaptive and context-aware agents.

Abstract: The current paradigm of test-time scaling relies on generating long reasoning
traces ("thinking" more) before producing a response. In agent problems that
require interaction, this can be done by generating thinking traces before
acting in the world. However, this process does not allow agents to acquire new
information from the environment or adapt their behavior over time. In this
work, we propose to scale test-time interaction, an untapped dimension of
test-time scaling that increases the agent's interaction horizon to enable
running rich behaviors such as exploration, backtracking, and dynamic
re-planning within a single rollout. To demonstrate the promise of this scaling
dimension, we study the domain of web agents. We first show that even
prompting-based interaction scaling without any training can improve task
success on web benchmarks non-trivially. Building on this, we introduce TTI
(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)
approach that trains agents by adaptively adjusting their rollout lengths.
Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data
web agents on WebVoyager and WebArena benchmarks. We further show that TTI
enables agents to balance exploration and exploitation adaptively. Our results
establish interaction scaling as a powerful, complementary axis to scaling
per-step compute, offering new avenues for training adaptive agents.

</details>


### [615] [Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator](https://arxiv.org/abs/2506.07980)
*Alberto Bazán-Guillén,Carlos Beis-Penedo,Diego Cajaraville-Aboy,Pablo Barbecho-Bautista,Rebeca P. Díaz-Redondo,Luis J. de la Cruz Llopis,Ana Fernández-Vilas,Mónica Aguilar Igartua,Manuel Fernández-Veiga*

Main category: cs.LG

TL;DR: The paper presents DesRUTGe, a decentralized framework using Deep Reinforcement Learning and Federated Learning combined with the SUMO simulator to generate realistic traffic patterns in urban zones.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in creating realistic, scalable, and privacy-preserving urban traffic simulation, crucial for urban planning and intelligent transportation systems.

Method: DesRUTGe integrates DRL agents and SUMO simulator with Decentralized Federated Learning, where local DRL models are trained independently using historical data and collaborate by exchanging parameters with nearby zones.

Result: DesRUTGe was evaluated using Barcelona's real-world data and achieved superior accuracy and privacy preservation compared to standard SUMO-based tools and centralized learning approaches.

Conclusion: DesRUTGe provides a scalable, accurate, and privacy-respecting method for 24-hour urban traffic pattern simulation, improving upon existing centralized systems.

Abstract: Realistic urban traffic simulation is essential for sustainable urban
planning and the development of intelligent transportation systems. However,
generating high-fidelity, time-varying traffic profiles that accurately reflect
real-world conditions, especially in large-scale scenarios, remains a major
challenge. Existing methods often suffer from limitations in accuracy,
scalability, or raise privacy concerns due to centralized data processing. This
work introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a
novel framework that integrates Deep Reinforcement Learning (DRL) agents with
the SUMO simulator to generate realistic 24-hour traffic patterns. A key
innovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),
wherein each traffic detector and its corresponding urban zone function as an
independent learning node. These nodes train local DRL models using minimal
historical data and collaboratively refine their performance by exchanging
model parameters with selected peers (e.g., geographically adjacent zones),
without requiring a central coordinator. Evaluated using real-world data from
the city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as
RouteSampler, as well as other centralized learning approaches, by delivering
more accurate and privacy-preserving traffic pattern generation.

</details>


### [616] [Generative Modeling of Weights: Generalization or Memorization?](https://arxiv.org/abs/2506.07998)
*Boya Zeng,Yida Yin,Zhiqiu Xu,Zhuang Liu*

Main category: cs.LG

TL;DR: This paper examines the memorization behavior of generative models synthesizing neural network weights and finds that these models largely recreate or interpolate training data rather than generating new, high-performing weights.


<details>
  <summary>Details</summary>
Motivation: With the success of generative models in image and video synthesis, there is growing interest in using these models to generate neural network weights. The motivation lies in assessing whether generative approaches can create effective, unique weights beyond simply replicating training data.

Method: The study evaluates four generative methods on their ability to produce novel neural network weights. Analysis includes testing against baselines like weight noise and ensembles, as well as experiments to mitigate memorization using factors from image models and data augmentations.

Result: The models primarily produce weights by memorization, either as replicas or interpolations of training checkpoints. They fail to outperform basic baselines in generating diverse and effective weights, and mitigation attempts do not resolve this issue.

Conclusion: Current generative models for neural network weights are limited by their reliance on memorization. This highlights the need for better methods and thorough evaluations when deploying generative techniques in new domains.

Abstract: Generative models, with their success in image and video generation, have
recently been explored for synthesizing effective neural network weights. These
approaches take trained neural network checkpoints as training data, and aim to
generate high-performing neural network weights during inference. In this work,
we examine four representative methods on their ability to generate novel model
weights, i.e., weights that are different from the checkpoints seen during
training. Surprisingly, we find that these methods synthesize weights largely
by memorization: they produce either replicas, or at best simple
interpolations, of the training checkpoints. Current methods fail to outperform
simple baselines, such as adding noise to the weights or taking a simple weight
ensemble, in obtaining different and simultaneously high-performing models. We
further show that this memorization cannot be effectively mitigated by
modifying modeling factors commonly associated with memorization in image
diffusion models, or applying data augmentations. Our findings provide a
realistic assessment of what types of data current generative models can model,
and highlight the need for more careful evaluation of generative models in new
domains. Our code is available at
https://github.com/boyazeng/weight_memorization.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [617] [EvoGrad: Metaheuristics in a Differentiable Wonderland](https://arxiv.org/abs/2506.06320)
*Beatrice F. R. Citterio,Andrea Tangherloni*

Main category: cs.NE

TL;DR: The paper introduces EvoGrad, a framework that combines Evolutionary Computation (EC) and Swarm Intelligence (SI) with gradient-based methods by making their operators differentiable.


<details>
  <summary>Details</summary>
Motivation: Traditional EC and SI algorithms do not use gradient information and thus have limited optimisation efficiency.

Method: The EvoGrad framework transforms EC and SI operators like selection, mutation, and particle updates into differentiable operators for gradient-based optimisation.

Result: EvoGrad demonstrates superior performance over traditional algorithms in benchmark tests and neural network training scenarios.

Conclusion: EvoGrad establishes a new benchmark for hybrid optimisation frameworks by combining the strengths of EC, SI, and gradient-based methods, proving to be highly efficient.

Abstract: Differentiable programming has revolutionised optimisation by enabling
efficient gradient-based training of complex models, such as Deep Neural
Networks (NNs) with billions and trillions of parameters. However, traditional
Evolutionary Computation (EC) and Swarm Intelligence (SI) algorithms, widely
successful in discrete or complex search spaces, typically do not leverage
local gradient information, limiting their optimisation efficiency. In this
paper, we introduce EvoGrad, a unified differentiable framework that integrates
EC and SI with gradient-based optimisation through backpropagation. EvoGrad
converts conventional evolutionary and swarm operators (e.g., selection,
mutation, crossover, and particle updates) into differentiable operators,
facilitating end-to-end gradient optimisation. Extensive experiments on
benchmark optimisation functions and training of small NN regressors reveal
that our differentiable versions of EC and SI metaheuristics consistently
outperform traditional, gradient-agnostic algorithms in most scenarios. Our
results show the substantial benefits of fully differentiable evolutionary and
swarm optimisation, setting a new standard for hybrid optimisation frameworks.

</details>


### [618] [Neural networks with image recognition by pairs](https://arxiv.org/abs/2506.06322)
*Polad Geidarov*

Main category: cs.NE

TL;DR: The paper explores transforming metric-based neural networks for classical learning by simplifying training and allowing scalable class recognition.


<details>
  <summary>Details</summary>
Motivation: To simplify and enhance the flexibility of metric-based neural networks for image recognition while ensuring scalability and usability.

Method: Proposes modifying metric-based networks to enable classical learning algorithms, focusing on image recognition in pairs for progressive scalability.

Result: Achieved a scalable and simplified training approach, allowing for seamless addition of new classes without re-adjusting prior parameters.

Conclusion: The transformed networks exhibit simplicity, scalability, reliability, and ease of training, supporting diverse and expanding image recognition tasks effectively.

Abstract: Neural networks based on metric recognition methods have a strictly
determined architecture. Number of neurons, connections, as well as weights and
thresholds values are calculated analytically, based on the initial conditions
of tasks: number of recognizable classes, number of samples, metric expressions
used. This paper discusses the possibility of transforming these networks in
order to apply classical learning algorithms to them without using analytical
expressions that calculate weight values. In the received network, training is
carried out by recognizing images in pairs. This approach simplifies the
learning process and easily allows to expand the neural network by adding new
images to the recognition task. The advantages of these networks, including
such as: 1) network architecture simplicity and transparency; 2) training
simplicity and reliability; 3) the possibility of using a large number of
images in the recognition problem using a neural network; 4) a consistent
increase in the number of recognizable classes without changing the previous
values of weights and thresholds.

</details>


### [619] [Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies](https://arxiv.org/abs/2506.06325)
*Viorica Rozina Chifu,Tudor Cioara,Cristina Bianca Pop,Ionut Anghel*

Main category: cs.NE

TL;DR: A decentralized energy cooperation model for microgrids uses evolutionary algorithms to balance energy levels, achieving stability in 95% of the community.


<details>
  <summary>Details</summary>
Motivation: To establish an effective energy cooperation mechanism for microgrids that ensures energy stability and autonomy at both the individual and community levels.

Method: The paper models individual microgrids as autonomous agents using Hawk or Dove strategies based on their energy levels. An evolutionary algorithm optimizes energy trading via a specialized matrix structure, guided by a multi-criteria fitness function. Operators such as recombination and Gaussian-based mutation enhance adaptability.

Result: The model was tested on a simulated community of 100 microgrids. It achieved energy stability in 95% of the microgrids despite variations in storage characteristics.

Conclusion: The proposed decentralized model is effective in achieving energy balance in microgrid communities, thus demonstrating its practicality and potential scalability.

Abstract: This paper proposes a decentralized model of energy cooperation between
microgrids, in which decisions are made locally, at the level of the microgrid
community. Each microgrid is modeled as an autonomous agent that adopts a Hawk
or Dove strategy, depending on the level of energy stored in the battery and
its role in the energy trading process. The interactions between selling and
buying microgrids are modeled through an evolutionary algorithm. An individual
in the algorithm population is represented as an energy trading matrix that
encodes the amounts of energy traded between the selling and buying microgrids.
The population evolution is achieved by recombination and mutation operators.
Recombination uses a specialized operator for matrix structures, and mutation
is applied to the matrix elements according to a Gaussian distribution. The
evaluation of an individual is made with a multi-criteria fitness function that
considers the seller profit, the degree of energy stability at the community
level, penalties for energy imbalance at the community level and for the
degradation of microgrids batteries. The method was tested on a simulated
scenario with 100 microgrids, each with its own selling and buying thresholds,
to reflect a realistic environment with variable storage characteristics of
microgrids batteries. By applying the algorithm on this scenario, 95 out of the
100 microgrids reached a stable energy state. This result confirms the
effectiveness of the proposed model in achieving energy balance both at the
individual level, for each microgrid, and at the level of the entire community.

</details>


### [620] [Introduction to Predictive Coding Networks for Machine Learning](https://arxiv.org/abs/2506.06332)
*Mikko Stenlund*

Main category: cs.NE

TL;DR: This paper provides an introduction to predictive coding networks (PCNs), explores their network architecture, inference and learning rules, and demonstrates their implementation for an image-classification task.


<details>
  <summary>Details</summary>
Motivation: The authors aim to familiarize machine learning practitioners with biologically inspired predictive coding networks and their applications, highlighting a benchmark example to showcase PCN capabilities.

Method: The paper explains PCN network architecture, inference and learning update rules, and algorithmic implementation using a concrete example of image classification on CIFAR-10 dataset.

Result: PCNs demonstrate noteworthy performance on the CIFAR-10 image classification task, emphasizing their utility as an alternative to feedforward neural networks.

Conclusion: PCNs offer a compelling biologically inspired framework for hierarchical computation and exhibit strong performance, making them a valuable tool for machine learning tasks.

Abstract: Predictive coding networks (PCNs) constitute a biologically inspired
framework for understanding hierarchical computation in the brain, and offer an
alternative to traditional feedforward neural networks in ML. This note serves
as a quick, onboarding introduction to PCNs for machine learning practitioners.
We cover the foundational network architecture, inference and learning update
rules, and algorithmic implementation. A concrete image-classification task
(CIFAR-10) is provided as a benchmark-smashing application, together with an
accompanying Python notebook containing the PyTorch implementation.

</details>


### [621] [CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms](https://arxiv.org/abs/2506.06362)
*Dejun Xu,Jijia Chen,Gary G. Yen,Min Jiang*

Main category: cs.NE

TL;DR: A novel resource allocation framework is proposed for bilevel evolutionary algorithms (EAs) reducing computational cost while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies and resource waste in bilevel optimization algorithms caused by redundant evaluations of lower-level tasks.

Method: Introduced a contrastive ranking network that learns relational patterns and uses reference-based ranking strategies for prioritizing tasks and adaptive resampling.

Result: Experiments showed reduced computational cost and improved or preserved solution accuracy across five state-of-the-art algorithms.

Conclusion: The proposed framework provides better efficiency in bilevel optimization and offers scalability for evolutionary algorithms.

Abstract: Bilevel optimization poses a significant computational challenge due to its
nested structure, where each upper-level candidate solution requires solving a
corresponding lower-level problem. While evolutionary algorithms (EAs) are
effective at navigating such complex landscapes, their high resource demands
remain a key bottleneck -- particularly the redundant evaluation of numerous
unpromising lower-level tasks. Despite recent advances in multitasking and
transfer learning, resource waste persists. To address this issue, we propose a
novel resource allocation framework for bilevel EAs that selectively identifies
and focuses on promising lower-level tasks. Central to our approach is a
contrastive ranking network that learns relational patterns between paired
upper- and lower-level solutions online. This knowledge guides a
reference-based ranking strategy that prioritizes tasks for optimization and
adaptively controls resampling based on estimated population quality.
Comprehensive experiments across five state-of-the-art bilevel algorithms show
that our framework significantly reduces computational cost while preserving --
or even enhancing -- solution accuracy. This work offers a generalizable
strategy to improve the efficiency of bilevel EAs, paving the way for more
scalable bilevel optimization.

</details>


### [622] [Structured State Space Model Dynamics and Parametrization for Spiking Neural Networks](https://arxiv.org/abs/2506.06374)
*Maxime Fabre,Lyubov Dudchenko,Emre Neftci*

Main category: cs.NE

TL;DR: This paper proposes two new spiking neuron models inspired by state-space models (SSMs) to improve performance and scalability, achieving near or beyond state-of-the-art results in speech recognition tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address instabilities in spiking neuron models during training and inference, and to leverage insights from state-space models for better scalability and performance in sequence processing tasks.

Method: The paper introduces two spiking neuron models: (1) an extension of the adaptive leaky integrate-and-fire (AdLIF) neuron with timestep training and logarithmic reparametrization, and (2) a model incorporating initialization and structure from complex-state SSMs for broader dynamical regimes.

Result: The proposed models surpass or approach state-of-the-art performance in event-based and raw audio speech recognition datasets, with a favorable parameter count, efficient memory usage, and high activity sparsity.

Conclusion: The models demonstrate scalability, efficiency, and competitive performance, striking a balance between the advantages of spiking neuron models and state-space models.

Abstract: Multi-state spiking neurons such as the adaptive leaky integrate-and-fire
(AdLIF) neuron offer compelling alternatives to conventional deep learning
models thanks to their sparse binary activations, second-order nonlinear
recurrent dynamics, and efficient hardware realizations. However, such internal
dynamics can cause instabilities during inference and training, often limiting
performance and scalability. Meanwhile, state space models (SSMs) excel in long
sequence processing using linear state-intrinsic recurrence resembling spiking
neurons' subthreshold regime. Here, we establish a mathematical bridge between
SSMs and second-order spiking neuron models. Based on structure and
parametrization strategies of diagonal SSMs, we propose two novel spiking
neuron models. The first extends the AdLIF neuron through timestep training and
logarithmic reparametrization to facilitate training and improve final
performance. The second additionally brings initialization and structure from
complex-state SSMs, broadening the dynamical regime to oscillatory dynamics.
Together, our two models achieve beyond or near state-of-the-art (SOTA)
performances for reset-based spiking neuron models across both event-based and
raw audio speech recognition datasets. We achieve this with a favorable number
of parameters and required dynamic memory while maintaining high activity
sparsity. Our models demonstrate enhanced scalability in network size and
strike a favorable balance between performance and efficiency with respect to
SSM models.

</details>


### [623] [Employing Discrete Fourier Transform in Representational Learning](https://arxiv.org/abs/2506.06765)
*Raoof HojatJalali,Edmondo Trentin*

Main category: cs.NE

TL;DR: The paper proposes using the Discrete Fourier Transform (DFT) as an alternative reconstruction target in autoencoders for representation learning, achieving better performance compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To improve representation learning objectives by leveraging global frequency information through the Discrete Fourier Transform (DFT), which captures meaningful patterns in the input data.

Method: The method replaces raw input reconstruction in autoencoders with DFT-based reconstruction targets, focusing selectively on significant frequency components. This approach is validated on CIFAR-10 using ResNet-50.

Result: The method achieved a top-1 accuracy of 52.8% on CIFAR-10 with ResNet-50, outperforming standard autoencoders by 12.8 points. Training on lower-frequency components provided similar accuracy with reduced computational effort.

Conclusion: Using the DFT as a reconstruction target in autoencoders is effective for representation learning, offering flexibility and improved accuracy, even when focusing on selective frequency components.

Abstract: Image Representation learning via input reconstruction is a common technique
in machine learning for generating representations that can be effectively
utilized by arbitrary downstream tasks. A well-established approach is using
autoencoders to extract latent representations at the network's compression
point. These representations are valuable because they retain essential
information necessary for reconstructing the original input from the compressed
latent space. In this paper, we propose an alternative learning objective.
Instead of using the raw input as the reconstruction target, we employ the
Discrete Fourier Transform (DFT) of the input. The DFT provides meaningful
global information at each frequency level, making individual frequency
components useful as separate learning targets. When dealing with
multidimensional input data, the DFT offers remarkable flexibility by enabling
selective transformation across specific dimensions while preserving others in
the computation. Moreover, certain types of input exhibit distinct patterns in
their frequency distributions, where specific frequency components consistently
contain most of the magnitude, allowing us to focus on a subset of frequencies
rather than the entire spectrum. These characteristics position the DFT as a
viable learning objective for representation learning and we validate our
approach by achieving 52.8% top-1 accuracy on CIFAR-10 with ResNet-50 and
outperforming the traditional autoencoder by 12.8 points under identical
architectural configurations. Additionally, we demonstrate that training on
only the lower-frequency components - those with the highest magnitudes yields
results comparable to using the full frequency spectrum, with only minimal
reductions in accuracy.

</details>


### [624] [Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example](https://arxiv.org/abs/2506.06904)
*Yuhan Helena Liu,Guangyu Robert Yang,Christopher J. Cueva*

Main category: cs.NE

TL;DR: The study identifies a biologically plausible learning rule (e-prop) that matches neural data similarity of Backpropagation Through Time (BPTT) while maintaining task accuracy. Architecture and initial settings play a key role.


<details>
  <summary>Details</summary>
Motivation: Uncovering biologically plausible learning methods can help understand brain learning processes and ensure alignment with neuroscience tasks and neural recordings.

Method: The study employs Procrustes analysis on neuroscience datasets, comparing e-prop (a biologically plausible learning rule) against BPTT for task accuracy and neural data similarity.

Result: e-prop achieves comparable neural similarity and dynamical properties to BPTT at similar accuracies. Architecture and initialization impact neural similarity more than the learning rule itself.

Conclusion: Biologically plausible learning rules like e-prop demonstrate significant advancement in matching neural data and task performance, paving the way for effective brain-model alignment.

Abstract: Understanding how the brain learns may be informed by studying biologically
plausible learning rules. These rules, often approximating gradient descent
learning to respect biological constraints such as locality, must meet two
critical criteria to be considered an appropriate brain model: (1) good
neuroscience task performance and (2) alignment with neural recordings. While
extensive research has assessed the first criterion, the second remains
underexamined. Employing methods such as Procrustes analysis on well-known
neuroscience datasets, this study demonstrates the existence of a biologically
plausible learning rule -- namely e-prop, which is based on gradient truncation
and has demonstrated versatility across a wide range of tasks -- that can
achieve neural data similarity comparable to Backpropagation Through Time
(BPTT) when matched for task accuracy. Our findings also reveal that model
architecture and initial conditions can play a more significant role in
determining neural similarity than the specific learning rule. Furthermore, we
observe that BPTT-trained models and their biologically plausible counterparts
exhibit similar dynamical properties at comparable accuracies. These results
underscore the substantial progress made in developing biologically plausible
learning rules, highlighting their potential to achieve both competitive task
performance and neural data similarity.

</details>


### [625] [Research on Aerodynamic Performance Prediction of Airfoils Based on a Fusion Algorithm of Transformer and GAN](https://arxiv.org/abs/2506.06979)
*MaolinYang,Yaohui Wang,Pingyu Jiang*

Main category: cs.NE

TL;DR: The study introduces 'Deeptrans,' a deep learning model that combines Transformer and GAN to predict airfoil aerodynamic performance with high accuracy and efficiency, addressing traditional methods' limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the high cost and inefficiency of traditional aerodynamic performance prediction methods and the low accuracy and strong data dependence of existing data-driven models in multi-objective predictions.

Method: Deeptrans uses an improved Transformer architecture integrated with a GAN in a coding-decoding framework and employs confrontation training on a large-scale dataset to enable synchronized, precise multi-parameter aerodynamic predictions.

Result: Deeptrans achieved an MSE loss of 5.6*10^-6 on the test set, with a single-sample prediction time of 0.0056 seconds—nearly 700 times faster than traditional CFD methods, outperforming existing models.

Conclusion: The study demonstrates Deeptrans as a highly efficient and accurate data-driven solution for airfoil aerodynamic performance prediction, proposing a novel method for modeling complex flow problems using deep learning.

Abstract: Predicting of airfoil aerodynamic performance is a key part of aircraft
design optimization, but the traditional methods (such as wind tunnel test and
CFD simulation) have the problems of high cost and low efficiency, and the
existing data-driven models face the challenges of insufficient accuracy and
strong data dependence in multi-objective prediction. Therefore, this study
proposes a deep learning model, Deeptrans, based on the fusion of improved
Transformer and generative Adversarial network (GAN), which aims to predict the
multi-parameter aerodynamic performance of airfoil efficiently. By constructing
a large-scale data set and designing a model structure that integrates a
Transformer coding-decoding framework and confrontation training, synchronous
and high-precision prediction of aerodynamic parameters is realized.
Experiments show that the MSE loss of Deeptrans on the verification set is
reduced to 5.6*10-6, and the single-sample prediction time is only 0.0056
seconds, which is nearly 700 times more efficient than the traditional CFD
method. Horizontal comparison shows that the prediction accuracy is
significantly better than the original Transformer, GAN, and VAE models. This
study provides an efficient data-driven solution for airfoil aerodynamic
performance prediction and a new idea for deep learning modeling complex flow
problems.

</details>


### [626] [Transient Dynamics in Lattices of Differentiating Ring Oscillators](https://arxiv.org/abs/2506.07253)
*Peter DelMastro,Arjun Karuvally,Hananel Hazan,Hava Siegelmann,Edward Rietman*

Main category: cs.NE

TL;DR: This paper explores differentiating neurons in large networks and their synchronization behavior, and proposes their potential application in low-power AI systems.


<details>
  <summary>Details</summary>
Motivation: Despite extensive research on integrating and spiking neurons, differentiating neurons have been understudied, especially in the context of large networks, prompting an investigation into their behavior and potential applications.

Method: The study uses numerical simulations to analyze the behavior of large lattices of differentiating neuron rings, examining their synchronization dynamics and correlation emergence inspired by the Kuramoto model.

Result: Large lattices of differentiating neurons exhibit synchronization patterns and correlation growth, with transient dynamics forming domains of synchronized oscillators separated by phase boundaries. The scale of these correlations depends on neuron sharing between adjacent rings.

Conclusion: Differentiating neural networks hold potential as low-power, neuromorphic computing substrates, particularly for reservoir computing, given their unique synchronization properties and simple circuit designs.

Abstract: Recurrent neural networks (RNNs) are machine learning models widely used for
learning temporal relationships. Current state-of-the-art RNNs use integrating
or spiking neurons -- two classes of computing units whose outputs depend
directly on their internal states -- and accordingly there is a wealth of
literature characterizing the behavior of large networks built from these
neurons. On the other hand, past research on differentiating neurons, whose
outputs are computed from the derivatives of their internal states, remains
limited to small hand-designed networks with fewer than one-hundred neurons.
Here we show via numerical simulation that large lattices of differentiating
neuron rings exhibit local neural synchronization behavior found in the
Kuramoto model of interacting oscillators. We begin by characterizing the
periodic orbits of uncoupled rings, herein called ring oscillators. We then
show the emergence of local correlations between oscillators that grow over
time when these rings are coupled together into lattices. As the correlation
length grows, transient dynamics arise in which large regions of the lattice
settle to the same periodic orbit, and thin domain boundaries separate
adjacent, out-of-phase regions. The steady-state scale of these correlated
regions depends on how the neurons are shared between adjacent rings, which
suggests that lattices of differentiating ring oscillator might be tuned to be
used as reservoir computers. Coupled with their simple circuit design and
potential for low-power consumption, differentiating neural nets therefore
represent a promising substrate for neuromorphic computing that will enable
low-power AI applications.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [627] [Pinching-Antenna Systems For Indoor Immersive Communications: A 3D-Modeling Based Performance Analysis](https://arxiv.org/abs/2506.07771)
*Yulei Wang,Yalin Liu,Yaru Fu,Zhiguo Ding*

Main category: cs.PF

TL;DR: The study explores Pinching-antenna systems (PASS) for enhancing 6G indoor immersive communications, with models and theoretical insights.


<details>
  <summary>Details</summary>
Motivation: To improve wireless channel reconfiguration and overcome line-of-sight blockages in indoor 6G immersive applications using emerging Pinching-antenna technology.

Method: Develop a 3D model for user, waveguide, and PA distribution; create a theoretical model for downlink performance; and provide numerical results and implementation guidelines.

Result: Theoretical models and simulations demonstrate how PA technology can enhance indoor communication performance, with practical deployment guidance offered.

Conclusion: Pinching-antenna systems hold significant potential for advancing indoor immersive applications in 6G, with validated theoretical models paving the way for practical deployment.

Abstract: The emerging pinching antenna (PA) technology has high flexibility to
reconfigure wireless channels and combat line-of-sight blockage, thus holding
transformative potential for indoor immersive applications in 6G. This paper
investigates Pinching-antenna systems (PASS) for indoor immersive
communications. Our contributions are threefold: (1) we construct a 3D model to
characterize the distribution of users, waveguides, and PAs in the PASS; (2) we
develop a general theoretical model on downlink performance of PASS by
capturing PA-user relationships and system parameters' impacts; and (3) we
conduct comprehensive numerical results of the theoretical model and provide
implementation guidelines for PASS deployments.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [628] [Optimizing Optimizations: Case Study on Detecting Specific Types of Mathematical Optimization Constraints with E-Graphs in JijModeling](https://arxiv.org/abs/2506.06495)
*Hiromi Ishii,Taro Shimizu,Toshiki Teramura*

Main category: cs.PL

TL;DR: This paper focuses on improving mathematical optimization using JijModeling's constraint detection mechanism via e-graphs and introduces a utility library for simplifying syntax tree management.


<details>
  <summary>Details</summary>
Motivation: To enhance efficiency in solving mathematical optimization problems by leveraging information about specific types of constraints such as one-hot or SOS constraints.

Method: Utilizes a constraint detection mechanism based on e-graphs and presents heuristic criteria for designing rewriting systems, along with benchmarking and introducing the egg_recursive utility library.

Result: Shows benchmarking results that demonstrate how the constraint detection mechanism affects performance, along with a simplified process for handling syntax trees using egg_recursive.

Conclusion: The described methodology and tools improve execution time and reduce complexity in writing and managing optimization models.

Abstract: In solving mathematical optimization problems efficiently, it is crucial to
make use of information about specific types of constraints, such as the
one-hot or Special-Ordered Set (SOS) constraints. In many cases, exploiting
such information gives asymptotically better execution time. JijModeling, an
industrial-strength mathematical optimization modeller, achieves this by
separating the symbolic representation of an optimization problem from the
input data. In this paper, we will report a real-world case study on a
constraint detection mechanism modulo the algebraic congruence using e-graphs,
and describe heuristic criteria for designing rewriting systems. We give
benchmarking result that shows the performance impact of the constraint
detection mechanism.
  We also introduce egg_recursive, a utility library for writing egg-terms as
recursive abstract syntax trees, reducing the burden of writing and maintaining
complex terms in S-expressions.

</details>


### [629] [Reasoning about External Calls](https://arxiv.org/abs/2506.06544)
*Sophia Drossopoulou,Julian Mackay,Susan Eisenbach,James Noble*

Main category: cs.PL

TL;DR: This paper proposes methods to verify and specify software code that limits risks from untrusted external calls.


<details>
  <summary>Details</summary>
Motivation: To simplify reasoning about trusted internal code which interacts with untrusted external code, despite external untrustworthiness.

Method: Introduced new assertions, specifications, and a Hoare logic to verify internal code's encapsulation and capability restrictions when making external calls.

Result: The approach is demonstrated through examples with mechanized proofs, and soundness of the proposed Hoare logic is proven.

Conclusion: The paper provides a robust framework for specifying and verifying internal code behavior, even when interacting with untrusted external code.

Abstract: In today's complex software, internal trusted code is tightly intertwined
with external untrusted code. To reason about internal code, programmers must
reason about the potential effects of calls to external code, even though that
code is not trusted and may not even be available. The effects of external
calls can be limited, if internal code is programmed defensively, limiting
potential effects by limiting access to the capabilities necessary to cause
those effects.
  This paper addresses the specification and verification of internal code that
relies on encapsulation and object capabilities to limit the effects of
external calls. We propose new assertions for access to capabilities, new
specifications for limiting effects, and a Hoare logic to verify that a module
satisfies its specification, even while making external calls. We illustrate
the approach though a running example with mechanised proofs, and prove
soundness of the Hoare logic.

</details>


### [630] [Execution-Aware Program Reduction for WebAssembly via Record and Replay](https://arxiv.org/abs/2506.07834)
*Doehyun Baek,Daniel Lehmann,Ben L. Titzer,Sukyoung Ryu,Michael Pradel*

Main category: cs.PL

TL;DR: The paper introduces RR-Reduce and Hybrid-Reduce, two execution-aware techniques for reducing WebAssembly programs to debug bugs more effectively by leveraging execution behavior through record and replay.


<details>
  <summary>Details</summary>
Motivation: Existing program reduction techniques for WebAssembly struggle with large and complex programs because they ignore execution behavior and rely solely on static and syntactic analysis.

Method: RR-Reduce isolates bug-triggering target functions and generates reduced programs that preserve their interactions. Hybrid-Reduce combines an execution-unaware reduction method with RR-Reduce for further reduction.

Result: RR-Reduce reduced programs to 1.20% of the original size in 14.5 minutes, outperforming state-of-the-art by 33.15x in reduction time. Hybrid-Reduce reduced programs to 0.13% of their size in 3.5 hours, surpassing existing tools by 3.42x in size and 2.26x in time.

Conclusion: RR-Reduce is suited for quick debugging with minimal time investment, while Hybrid-Reduce is optimal for cases demanding minimal-sized programs, providing substantial improvements over existing methods.

Abstract: WebAssembly (Wasm) programs may trigger bugs in their engine implementations.
To aid debugging, program reduction techniques try to produce a smaller variant
of the input program that still triggers the bug. However, existing
execution-unaware program reduction techniques struggle with large and complex
Wasm programs, because they rely on static information and apply syntactic
transformations, while ignoring the valuable information offered by the input
program's execution behavior.
  We present RR-Reduce and Hybrid-Reduce, novel execution-aware program
reduction techniques that leverage execution behaviors via record and replay.
RR-Reduce identifies a bug-triggering function as the target function, isolates
that function from the rest of the program, and generates a reduced program
that replays only the interactions between the target function and the rest of
the program. Hybrid-Reduce combines a complementary execution-unaware reduction
technique with RR-Reduce to further reduce program size.
  We evaluate RR-Reduce and Hybrid-Reduce on 28 Wasm programs that trigger a
diverse set of bugs in three engines. On average, RR-Reduce reduces the
programs to 1.20 percent of their original size in 14.5 minutes, which
outperforms the state of the art by 33.15 times in terms of reduction time.
Hybrid-Reduce reduces the programs to 0.13 percent of their original size in
3.5 hours, which outperforms the state of the art by 3.42 times in terms of
reduced program size and 2.26 times in terms of reduction time. We envision
RR-Reduce as the go-to tool for rapid, on-demand debugging in minutes, and
Hybrid-Reduce for scenarios where developers require the smallest possible
programs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [631] [Tactile MNIST: Benchmarking Active Tactile Perception](https://arxiv.org/abs/2506.06361)
*Tim Schneider,Guillaume Duret,Cristiana de Farias,Roberto Calandra,Liming Chen,Jan Peters*

Main category: cs.RO

TL;DR: This paper introduces the Tactile MNIST Benchmark Suite, a standardized framework for active tactile perception tasks, featuring a dataset of synthetic and real tactile samples.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized benchmarks in the fields of active perception and tactile sensing, which limits systematic progress.

Method: The authors developed the Tactile MNIST Benchmark Suite, comprising simulation scenarios, a dataset of synthetic and real tactile samples, and a CycleGAN for realistic tactile simulation rendering.

Result: The benchmark suite provides diverse scenarios and a comprehensive dataset, enabling training and evaluation of models for tactile perception tasks.

Conclusion: The Tactile MNIST Benchmark Suite standardizes protocols and enables reproducible research, advancing tactile sensing and active perception fields.

Abstract: Tactile perception has the potential to significantly enhance dexterous
robotic manipulation by providing rich local information that can complement or
substitute for other sensory modalities such as vision. However, because
tactile sensing is inherently local, it is not well-suited for tasks that
require broad spatial awareness or global scene understanding on its own. A
human-inspired strategy to address this issue is to consider active perception
techniques instead. That is, to actively guide sensors toward regions with more
informative or significant features and integrate such information over time in
order to understand a scene or complete a task. Both active perception and
different methods for tactile sensing have received significant attention
recently. Yet, despite advancements, both fields lack standardized benchmarks.
To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an
open-source, Gymnasium-compatible benchmark specifically designed for active
tactile perception tasks, including localization, classification, and volume
estimation. Our benchmark suite offers diverse simulation scenarios, from
simple toy environments all the way to complex tactile perception tasks using
vision-based tactile sensors. Furthermore, we also offer a comprehensive
dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600
real-world tactile samples collected from 600 3D printed digits. Using this
dataset, we train a CycleGAN for realistic tactile simulation rendering. By
providing standardized protocols and reproducible evaluation frameworks, our
benchmark suite facilitates systematic progress in the fields of tactile
sensing and active perception.

</details>


### [632] [CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems](https://arxiv.org/abs/2506.06381)
*Trisanth Srinivasan,Santosh Patapati,Himani Musku,Idhant Gode,Aditya Arora,Samvit Bhattacharya,Abubakr Nazriev,Sanika Hirave,Zaryab Kanjiani,Srinjoy Ghose,Srinidhi Shetty*

Main category: cs.RO

TL;DR: This paper presents CPS-Guard, a framework using multi-role orchestration to ensure the dependability of AI-powered Cyber-Physical Systems (CPS).


<details>
  <summary>Details</summary>
Motivation: To address the challenges in traditional verification and validation (V&V) methods for AI in critical CPS applications, which struggle to handle the unpredictable and dynamic nature of AI behaviors.

Method: The paper designs a framework called CPS-Guard, employing specialized agents for roles such as safety monitoring and fault recovery, operating in a simulated environment to iteratively and automatically validate AI components.

Result: Through a case study with an AI-based planner in an autonomous vehicle, CPS-Guard demonstrated the ability to detect vulnerabilities, mitigate performance impacts, and devise recovery strategies effectively.

Conclusion: CPS-Guard provides a structured, automated, and extensible approach to improve the dependability of AI-intensive CPS in critical applications.

Abstract: Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to
operate in critical applications. However, traditional verification and
validation methods often struggle to handle the unpredictable and dynamic
nature of AI components. In this paper, we introduce CPS-Guard, a novel
framework that employs multi-role orchestration to automate the iterative
assurance process for AI-powered CPS. By assigning specialized roles (e.g.,
safety monitoring, security assessment, fault injection, and recovery planning)
to dedicated agents within a simulated environment, CPS-Guard continuously
evaluates and refines AI behavior against a range of dependability
requirements. We demonstrate the framework through a case study involving an
autonomous vehicle navigating an intersection with an AI-based planner. Our
results show that CPS-Guard effectively detects vulnerabilities, manages
performance impacts, and supports adaptive recovery strategies, thereby
offering a structured and extensible solution for rigorous V&V in safety- and
security-critical systems.

</details>


### [633] [Active Illumination Control in Low-Light Environments using NightHawk](https://arxiv.org/abs/2506.06394)
*Yash Turkar,Youngjin Kim,Karthik Dantu*

Main category: cs.RO

TL;DR: The paper introduces NightHawk, a framework that uses active illumination and exposure control to enhance robot vision in low-light subterranean environments.


<details>
  <summary>Details</summary>
Motivation: To address challenges in robot vision caused by dim lighting and lack of distinctive features, especially in subterranean environments like culverts.

Method: Developed NightHawk, using Bayesian optimization to adjust light intensity and exposure-time, with a novel metric for image utility. The system operates as an event-triggered recursive optimization pipeline.

Result: Field experiments on a legged robot navigating a culvert showed NightHawk improves feature detection and matching by 47-197%, enabling better visual estimation.

Conclusion: NightHawk demonstrates significant improvements in enabling robot vision under challenging lighting conditions, enhancing reliability for robotic navigation in subterranean habitats.

Abstract: Subterranean environments such as culverts present significant challenges to
robot vision due to dim lighting and lack of distinctive features. Although
onboard illumination can help, it introduces issues such as specular
reflections, overexposure, and increased power consumption. We propose
NightHawk, a framework that combines active illumination with exposure control
to optimize image quality in these settings. NightHawk formulates an online
Bayesian optimization problem to determine the best light intensity and
exposure-time for a given scene. We propose a novel feature detector-based
metric to quantify image utility and use it as the cost function for the
optimizer. We built NightHawk as an event-triggered recursive optimization
pipeline and deployed it on a legged robot navigating a culvert beneath the
Erie Canal. Results from field experiments demonstrate improvements in feature
detection and matching by 47-197% enabling more reliable visual estimation in
challenging lighting conditions.

</details>


### [634] [Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception](https://arxiv.org/abs/2506.06474)
*Everett Richards,Bipul Thapa,Lena Mashayekhy*

Main category: cs.RO

TL;DR: The paper introduces an Edge-Enabled Collaborative Object Detection (ECOD) framework for improving object detection in Connected Autonomous Vehicles (CAVs) through edge computing and multi-CAV collaboration, achieving up to 75% better performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional onboard perception suffers from occlusions and blind spots, while cloud-based solutions are too slow for real-time needs in autonomous vehicles. Hence, a new framework is needed to balance accuracy and real-time performance.

Method: The ECOD framework uses two key algorithms: PACE, for aggregating multi-CAV detection data to improve perception, and VOTE, for a consensus-based voting mechanism to enhance object classification accuracy. These operate on edge servers to ensure low-latency processing.

Result: Experiments using a hardware testbed of camera-equipped robotic CAVs and an edge server showed ECOD significantly improved object classification accuracy by up to 75%, while maintaining low-latency, real-time processing.

Conclusion: The study demonstrates the potential of edge computing to enhance collaborative perception in latency-sensitive autonomous systems, offering substantial improvements over traditional single-perspective onboard methods.

Abstract: Accurate and reliable object detection is critical for ensuring the safety
and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board
perception systems have limited accuracy due to occlusions and blind spots,
while cloud-based solutions introduce significant latency, making them
unsuitable for real-time processing demands required for autonomous driving in
dynamic environments. To address these challenges, we introduce an innovative
framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that
leverages edge computing and multi-CAV collaboration for real-time,
multi-perspective object detection. Our ECOD framework integrates two key
algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and
Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data
from multiple CAVs on an edge server to enhance perception in scenarios where
individual CAVs have limited visibility. VOTE utilizes a consensus-based voting
mechanism to improve the accuracy of object classification by integrating data
from multiple CAVs. Both algorithms are designed at the edge to operate in
real-time, ensuring low-latency and reliable decision-making for CAVs. We
develop a hardware-based controlled testbed consisting of camera-equipped
robotic CAVs and an edge server to evaluate the efficacy of our framework. Our
experimental results demonstrate the significant benefits of ECOD in terms of
improved object classification accuracy, outperforming traditional
single-perspective onboard approaches by up to 75%, while ensuring low-latency,
edge-driven real-time processing. This research highlights the potential of
edge computing to enhance collaborative perception for latency-sensitive
autonomous systems.

</details>


### [635] [Enhancing Situational Awareness in Underwater Robotics with Multi-modal Spatial Perception](https://arxiv.org/abs/2506.06476)
*Pushyami Kaveti,Ambjorn Grimsrud Waldum,Hanumant Singh,Martin Ludvigsen*

Main category: cs.RO

TL;DR: The paper discusses enhancing underwater SLAM using multi-modal sensors, addressing challenges like image degradation and scalability, and showcases robust real-time state estimation and 3D reconstructions.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in vision-based SLAM for underwater vehicles caused by image degradation and monocular/stereo inputs, enabling robust and scalable underwater sensing.

Method: The study uses multi-modal sensing, combining cameras, IMUs, and acoustic devices, integrates geometric and learning-based techniques, and performs semantic analysis, tested on data from real-world field deployments.

Result: Experiments demonstrate the success of real-time reliable state estimation and high-quality 3D reconstruction in challenging underwater conditions.

Conclusion: Multi-modal sensing significantly improves underwater SLAM capabilities, but areas like sensor calibration and learning-based method constraints require further exploration for large-scale operations.

Abstract: Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs)
demand robust spatial perception capabilities, including Simultaneous
Localization and Mapping (SLAM), to support both remote and autonomous tasks.
Vision-based systems have been integral to these advancements, capturing rich
color and texture at low cost while enabling semantic scene understanding.
However, underwater conditions -- such as light attenuation, backscatter, and
low contrast -- often degrade image quality to the point where traditional
vision-based SLAM pipelines fail. Moreover, these pipelines typically rely on
monocular or stereo inputs, limiting their scalability to the multi-camera
configurations common on many vehicles. To address these issues, we propose to
leverage multi-modal sensing that fuses data from multiple sensors-including
cameras, inertial measurement units (IMUs), and acoustic devices-to enhance
situational awareness and enable robust, real-time SLAM. We explore both
geometric and learning-based techniques along with semantic analysis, and
conduct experiments on the data collected from a work-class ROV during several
field deployments in the Trondheim Fjord. Through our experimental results, we
demonstrate the feasibility of real-time reliable state estimation and
high-quality 3D reconstructions in visually challenging underwater conditions.
We also discuss system constraints and identify open research questions, such
as sensor calibration, limitations with learning-based methods, that merit
further exploration to advance large-scale underwater operations.

</details>


### [636] [BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation](https://arxiv.org/abs/2506.06487)
*Zibo Zhou,Yue Hu,Lingkai Zhang,Zonglin Li,Siheng Chen*

Main category: cs.RO

TL;DR: The paper proposes BeliefMapNav, a system for zero-shot object navigation using a novel 3D voxel-based belief map, which significantly improves state-of-the-art performance in object navigation tasks.


<details>
  <summary>Details</summary>
Motivation: Recent large language models and vision-language models lack spatial reasoning and global understanding for effective zero-shot object navigation.

Method: A 3D voxel-based belief map is developed to integrate semantic reasoning, hierarchical spatial structure, and real-time observations for target localization. The BeliefMapNav system grounds LLM reasoning within this map and employs sequential path planning.

Result: BeliefMapNav achieves state-of-the-art performances, including a 46.4% SPL improvement over the previous leading Success Rate method, validated on HM3D, MP3D, and HSSD benchmarks.

Conclusion: The integration of semantic reasoning with hierarchical 3D spatial understanding and real-time observations leads to superior efficiency and accuracy in zero-shot object navigation tasks.

Abstract: Zero-shot object navigation (ZSON) allows robots to find target objects in
unfamiliar environments using natural language instructions, without relying on
pre-built maps or task-specific training. Recent general-purpose models, such
as large language models (LLMs) and vision-language models (VLMs), equip agents
with semantic reasoning abilities to estimate target object locations in a
zero-shot manner. However, these models often greedily select the next goal
without maintaining a global understanding of the environment and are
fundamentally limited in the spatial reasoning necessary for effective
navigation. To overcome these limitations, we propose a novel 3D voxel-based
belief map that estimates the target's prior presence distribution within a
voxelized 3D space. This approach enables agents to integrate semantic priors
from LLMs and visual embeddings with hierarchical spatial structure, alongside
real-time observations, to build a comprehensive 3D global posterior belief of
the target's location. Building on this 3D voxel map, we introduce
BeliefMapNav, an efficient navigation system with two key advantages: i)
grounding LLM semantic reasoning within the 3D hierarchical semantics voxel
space for precise target position estimation, and ii) integrating sequential
path planning to enable efficient global navigation decisions. Experiments on
HM3D, MP3D, and HSSD benchmarks show that BeliefMapNav achieves
state-of-the-art (SOTA) Success Rate (SR) and Success weighted by Path Length
(SPL), with a notable 46.4% SPL improvement over the previous best SR method,
validating its effectiveness and efficiency.

</details>


### [637] [MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping](https://arxiv.org/abs/2506.06535)
*Vineet Bhat,Naman Patel,Prashanth Krishnamurthy,Ramesh Karri,Farshad Khorrami*

Main category: cs.RO

TL;DR: The paper introduces a lightweight vision-language architecture for robotic grasping based on natural language and RGB-D images, achieving improved performance through mask-guided feature pooling.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges in robotic manipulation of unseen objects using natural language commands and achieving efficient and generalizable grasp prediction.

Method: A two-stage training method involving mask-guided feature pooling for vision-language models. This optimizes grasp prediction by focusing computations on masked feature regions for faster and more accurate outcomes.

Result: Achieved a 12% improvement on the OCID-VLG benchmark, released a larger RefGraspNet dataset for better generalization, and achieved competitive results in simulation and real-world experiments.

Conclusion: Mask-guided pooling enhances language-driven robotic grasping by improving efficiency and generalization, demonstrating robustness in unseen object scenarios and securing a better success rate over baselines.

Abstract: Robotic manipulation of unseen objects via natural language commands remains
challenging. Language driven robotic grasping (LDRG) predicts stable grasp
poses from natural language queries and RGB-D images. Here we introduce
Mask-guided feature pooling, a lightweight enhancement to existing LDRG
methods. Our approach employs a two-stage training strategy: first, a
vision-language model generates feature maps from CLIP-fused embeddings, which
are upsampled and weighted by text embeddings to produce segmentation masks.
Next, the decoder generates separate feature maps for grasp prediction, pooling
only token features within these masked regions to efficiently predict grasp
poses. This targeted pooling approach reduces computational complexity,
accelerating both training and inference. Incorporating mask pooling results in
a 12% improvement over prior approaches on the OCID-VLG benchmark. Furthermore,
we introduce RefGraspNet, an open-source dataset eight times larger than
existing alternatives, significantly enhancing model generalization for
open-vocabulary grasping. By extending 2D grasp predictions to 3D via depth
mapping and inverse kinematics, our modular method achieves performance
comparable to recent Vision-Language-Action (VLA) models on the LIBERO
simulation benchmark, with improved generalization across different task
suites. Real-world experiments on a 7 DoF Franka robotic arm demonstrate a 57%
success rate with unseen objects, surpassing competitive baselines by 7%. Code
will be released post publication.

</details>


### [638] [Semantics-aware Predictive Inspection Path Planning](https://arxiv.org/abs/2506.06560)
*Mihir Dharmadhikari,Kostas Alexis*

Main category: cs.RO

TL;DR: This paper proposes a novel semantics-aware inspection path planning approach (SPP) tailored for environments like ballast water tanks, featuring predictive algorithms for spatial semantics and validated using both simulations and real-world testing.


<details>
  <summary>Details</summary>
Motivation: The need for efficient and effective inspection of structured industrial environments, often with repetitive semantic patterns, like ballast water tanks, motivates the development of a predictive planning approach.

Method: The authors introduce an algorithm to identify repetitive spatial semantics in a graph representation and predict unseen areas. They propose two inspection path planning strategies specific to ballast water tank inspection and showcase their approach in both simulations and real-world applications.

Result: The proposed method outperforms state-of-the-art techniques in terms of inspection time while maintaining equal or better semantic surface coverage in both simulated and field testing.

Conclusion: Semantics-aware Predictive Planning is an effective approach for efficient inspection in structured environments with repeating patterns, offering improvements in time efficiency and inspection coverage.

Abstract: This paper presents a novel semantics-aware inspection path planning paradigm
called "Semantics-aware Predictive Planning" (SPP). Industrial environments
that require the inspection of specific objects or structures (called
"semantics"), such as ballast water tanks inside ships, often present
structured and repetitive spatial arrangements of the semantics of interest.
Motivated by this, we first contribute an algorithm that identifies spatially
repeating patterns of semantics - exact or inexact - in a semantic scene graph
representation and makes predictions about the evolution of the graph in the
unseen parts of the environment using these patterns. Furthermore, two
inspection path planning strategies, tailored to ballast water tank inspection,
that exploit these predictions are proposed. To assess the performance of the
novel predictive planning paradigm, both simulation and experimental
evaluations are performed. First, we conduct a simulation study comparing the
method against relevant state-of-the-art techniques and further present tests
showing its ability to handle imperfect patterns. Second, we deploy our method
onboard a collision-tolerant aerial robot operating inside the ballast tanks of
two real ships. The results, both in simulation and field experiments,
demonstrate significant improvement over the state-of-the-art in terms of
inspection time while maintaining equal or better semantic surface coverage. A
set of videos describing the different parts of the method and the field
deployments is available at https://tinyurl.com/spp-videos. The code for this
work is made available at https://github.com/ntnu-arl/predictive_planning_ros.

</details>


### [639] [NeSyPack: A Neuro-Symbolic Framework for Bimanual Logistics Packing](https://arxiv.org/abs/2506.06567)
*Bowei Li,Peiqi Yu,Zhenran Tang,Han Zhou,Yifan Sun,Ruixuan Liu,Changliu Liu*

Main category: cs.RO

TL;DR: This paper introduces NeSyPack, a neuro-symbolic framework for logistics packing tasks, featuring a modular and hierarchical approach for adaptability and efficacy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in end-to-end models for bimanual logistics packing, focusing on building a system that is explainable, adaptable, and efficient.

Method: NeSyPack integrates data-driven models with symbolic reasoning, using hierarchical decomposition and a symbolic skill graph to manage subtasks and atomic skills efficiently.

Result: NeSyPack demonstrated superior performance—beating end-to-end models—and won the First Prize in the WBCD competition at the 2025 IEEE Robotics and Automation Conference.

Conclusion: The study establishes NeSyPack as a robust neuro-symbolic framework that combines adaptability, reuse, and efficiency, presenting it as a competitive alternative to larger-scale retraining models.

Abstract: This paper presents NeSyPack, a neuro-symbolic framework for bimanual
logistics packing. NeSyPack combines data-driven models and symbolic reasoning
to build an explainable hierarchical system that is generalizable,
data-efficient, and reliable. It decomposes a task into subtasks via
hierarchical reasoning, and further into atomic skills managed by a symbolic
skill graph. The graph selects skill parameters, robot configurations, and
task-specific control strategies for execution. This modular design enables
robustness, adaptability, and efficient reuse - outperforming end-to-end models
that require large-scale retraining. Using NeSyPack, our team won the First
Prize in the What Bimanuals Can Do (WBCD) competition at the 2025 IEEE
International Conference on Robotics and Automation.

</details>


### [640] [Enhancing Robot Safety via MLLM-Based Semantic Interpretation of Failure Data](https://arxiv.org/abs/2506.06570)
*Aryaman Gupta,Yusuf Umut Ciftci,Somil Bansal*

Main category: cs.RO

TL;DR: The paper introduces a method to use Multimodal Large Language Models (MLLMs) to organize robotic failure data into meaningful clusters for scalable learning and safety improvement.


<details>
  <summary>Details</summary>
Motivation: Robots in real-world environments face diverse failures due to unstructured scenarios. Understanding these failures is crucial for enhancing safety and reliability systematically without manual intervention.

Method: The approach employs Multimodal Large Language Models (MLLMs) to analyze raw perceptual trajectories and classify robotic failure data into semantic clusters for improved understanding and learning.

Result: Semantic clusters generated by the model reveal latent patterns in robot failures, assisting in refining policies, optimizing data collection, enhancing safety, and enabling online failure detection.

Conclusion: The framework successfully transforms robotic failures into actionable insights, improving learning, robustness, and real-time adaptation capabilities.

Abstract: As robotic systems become increasingly integrated into real-world
environments, ranging from autonomous vehicles to household assistants, they
inevitably encounter diverse and unstructured scenarios that lead to failures.
While such failures pose safety and reliability challenges, they also provide
rich perceptual data for improving future performance. However, manually
analyzing large-scale failure datasets is impractical. In this work, we present
a method for automatically organizing large-scale robotic failure data into
semantically meaningful clusters, enabling scalable learning from failure
without human supervision. Our approach leverages the reasoning capabilities of
Multimodal Large Language Models (MLLMs), trained on internet-scale data, to
infer high-level failure causes from raw perceptual trajectories and discover
interpretable structure within uncurated failure logs. These semantic clusters
reveal latent patterns and hypothesized causes of failure, enabling scalable
learning from experience. We demonstrate that the discovered failure modes can
guide targeted data collection for policy refinement, accelerating iterative
improvement in agent policies and overall safety. Additionally, we show that
these semantic clusters can be employed for online failure detection, offering
a lightweight yet powerful safeguard for real-time adaptation. We demonstrate
that this framework enhances robot learning and robustness by transforming
real-world failures into actionable and interpretable signals for adaptation.

</details>


### [641] [Underwater Multi-Robot Simulation and Motion Planning in Angler](https://arxiv.org/abs/2506.06612)
*Akshaya Agrawal,Evan Palmer,Zachary Kingston,Geoffrey A. Hollinger*

Main category: cs.RO

TL;DR: The paper presents an extension to the Angler framework for multi-robot underwater simulation, motion planning, and environment generation.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing underwater robot simulation frameworks for multi-robot systems by enhancing real-world resemblance and motion planning capabilities.

Method: Developed a modular extension to Angler, integrating ROS2, MAVROS, and controllers for multi-robot systems, along with support for Open Motion Planning Library (OMPL) and environment generation tools.

Result: Successfully simulated communication between multiple underwater robots and implemented motion planning modules with collision avoidance capabilities.

Conclusion: The extension enhances underwater multi-robot algorithm testing and benchmarking in dynamic environments, offering valuable tools for developers.

Abstract: Deploying multi-robot systems in underwater environments is expensive and
lengthy; testing algorithms and software in simulation improves development by
decoupling software and hardware. However, this requires a simulation framework
that closely resembles the real-world. Angler is an open-source framework that
simulates low-level communication protocols for an onboard autopilot, such as
ArduSub, providing a framework that is close to reality, but unfortunately
lacking support for simulating multiple robots. We present an extension to
Angler that supports multi-robot simulation and motion planning. Our extension
has a modular architecture that creates non-conflicting communication channels
between Gazebo, ArduSub Software-in-the-Loop (SITL), and MAVROS to operate
multiple robots simultaneously in the same environment. Our multi-robot motion
planning module interfaces with cascaded controllers via a JointTrajectory
controller in ROS~2. We also provide an integration with the Open Motion
Planning Library (OMPL), a collision avoidance module, and tools for procedural
environment generation. Our work enables the development and benchmarking of
underwater multi-robot motion planning in dynamic environments.

</details>


### [642] [Attention-Based Convolutional Neural Network Model for Human Lower Limb Activity Recognition using sEMG](https://arxiv.org/abs/2506.06624)
*Mojtaba Mollahossein,Farshad Haghgoo Daryakenari,Mohammad Hossein Rohban,Gholamreza Vossoughi*

Main category: cs.RO

TL;DR: This paper introduces a lightweight attention-based deep neural network for classifying lower limb movements using sEMG signals, achieving high accuracy and real-time usability.


<details>
  <summary>Details</summary>
Motivation: To improve real-time classification of lower limb movements for assistive robotics and rehabilitation systems with computational efficiency.

Method: The study utilized an attention-based deep neural network with 62,876 parameters, trained using leave-one-out validation on multi-channel sEMG data from the BASAN dataset.

Result: The model achieved strong performance with 86.74% accuracy on the validation set and 85.38% accuracy on the test set, demonstrating efficiency and effectiveness.

Conclusion: The proposed model is a promising solution for real-time lower limb movement classification and can be integrated into human-robot interaction systems efficiently.

Abstract: Accurate classification of lower limb movements using surface
electromyography (sEMG) signals plays a crucial role in assistive robotics and
rehabilitation systems. In this study, we present a lightweight attention-based
deep neural network (DNN) for real-time movement classification using
multi-channel sEMG data from the publicly available BASAN dataset. The proposed
model consists of only 62,876 parameters and is designed without the need for
computationally expensive preprocessing, making it suitable for real-time
deployment. We employed a leave-oneout validation strategy to ensure
generalizability across subjects, and evaluated the model on three movement
classes: walking, standing with knee flexion, and sitting with knee extension.
The network achieved 86.74% accuracy on the validation set and 85.38% on the
test set, demonstrating strong classification performance under realistic
conditions. Comparative analysis with existing models in the literature
highlights the efficiency and effectiveness of our approach, especially in
scenarios where computational cost and real-time response are critical. The
results indicate that the proposed model is a promising candidate for
integration into upper-level controllers in human-robot interaction systems.

</details>


### [643] [Active Test-time Vision-Language Navigation](https://arxiv.org/abs/2506.06630)
*Heeju Ko,Sungjune Kim,Gyeongrok Oh,Jeongyoon Yoon,Honglak Lee,Sujin Jang,Seungryong Kim,Sangpil Kim*

Main category: cs.RO

TL;DR: The paper introduces ATENA, a framework that improves Vision-Language Navigation (VLN) by leveraging human feedback and entropy optimization to enhance performance during test-time scenarios.


<details>
  <summary>Details</summary>
Motivation: VLN policies perform poorly in unfamiliar test environments due to lack of feedback and issues like overconfidence in incorrect actions.

Method: The ATENA framework uses episodic human feedback, mixture entropy optimization combining action and pseudo-expert distributions, and self-active learning strategies to enhance uncertainty calibration and navigation outcomes.

Result: ATENA improves task performance on challenging VLN benchmarks such as REVERIE, R2R, and R2R-CE by effectively addressing test-time distributional shifts.

Conclusion: ATENA provides a robust way to manage uncertainty and optimize navigation decisions for VLN systems, outperforming baseline methods in various complex settings.

Abstract: Vision-Language Navigation (VLN) policies trained on offline datasets often
exhibit degraded task performance when deployed in unfamiliar navigation
environments at test time, where agents are typically evaluated without access
to external interaction or feedback. Entropy minimization has emerged as a
practical solution for reducing prediction uncertainty at test time; however,
it can suffer from accumulated errors, as agents may become overconfident in
incorrect actions without sufficient contextual grounding. To tackle these
challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time
active learning framework that enables a practical human-robot interaction via
episodic feedback on uncertain navigation outcomes. In particular, ATENA learns
to increase certainty in successful episodes and decrease it in failed ones,
improving uncertainty calibration. Here, we propose mixture entropy
optimization, where entropy is obtained from a combination of the action and
pseudo-expert distributions-a hypothetical action distribution assuming the
agent's selected action to be optimal-controlling both prediction confidence
and action preference. In addition, we propose a self-active learning strategy
that enables an agent to evaluate its navigation outcomes based on confident
predictions. As a result, the agent stays actively engaged throughout all
iterations, leading to well-grounded and adaptive decision-making. Extensive
evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate
that ATENA successfully overcomes distributional shifts at test time,
outperforming the compared baseline methods across various settings.

</details>


### [644] [Self-Adapting Improvement Loops for Robotic Learning](https://arxiv.org/abs/2506.06658)
*Calvin Luo,Zilai Zeng,Mingxi Jia,Yilun Du,Chen Sun*

Main category: cs.RO

TL;DR: This paper introduces the Self-Adapting Improvement Loop (SAIL), a method where video models iteratively improve their performance on robotic tasks through self-collected experiences and adaptation with pretrained internet-scale video models.


<details>
  <summary>Details</summary>
Motivation: Generalization to unseen robotic tasks using video generative models remains challenging, motivating the need to design agents capable of self-improvement through continuous online learning.

Method: The authors propose SAIL, where an in-domain video model iteratively adapts and learns from self-produced trajectories, leveraging an internet-scale pretrained video model for adaptation.

Result: SAIL demonstrated continuous performance improvements across MetaWorld tasks and two real robot arm manipulation tasks, even for tasks not included during initial model training.

Conclusion: SAIL showcases a robust method for bootstrapping and enhancing video generative models' ability to solve novel robotic tasks via iterative self-improvement and adaptation from online experiences.

Abstract: Video generative models trained on expert demonstrations have been utilized
as performant text-conditioned visual planners for solving robotic tasks.
However, generalization to unseen tasks remains a challenge. Whereas improved
generalization may be facilitated by leveraging learned prior knowledge from
additional pre-collected offline data sources, such as web-scale video
datasets, in the era of experience we aim to design agents that can
continuously improve in an online manner from self-collected behaviors. In this
work we thus propose the Self-Adapting Improvement Loop (SAIL), where an
in-domain video model iteratively updates itself on self-produced trajectories,
collected through adaptation with an internet-scale pretrained video model, and
steadily improves its performance for a specified task of interest. We apply
SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks
on a real robot arm, and find that performance improvements continuously emerge
over multiple iterations for novel tasks initially unseen during original
in-domain video model training. Furthermore, we discover that SAIL is
surprisingly robust regarding if and how the self-collected experience is
filtered, and the quality of the initial in-domain demonstrations. Through
adaptation with summarized internet-scale data, and learning through online
experience, we thus demonstrate a way to iteratively bootstrap a
high-performance video model for solving novel robotic tasks through
self-improvement.

</details>


### [645] [DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning](https://arxiv.org/abs/2506.06659)
*Wenhao Yao,Zhenxin Li,Shiyi Lan,Zi Wang,Xinglong Sun,Jose M. Alvarez,Zuxuan Wu*

Main category: cs.RO

TL;DR: DriveSuprim proposes a novel coarse-to-fine selection paradigm for autonomous vehicles, achieving high safety-critical trajectory selection performance.


<details>
  <summary>Details</summary>
Motivation: To improve safety-critical trajectory assessment for autonomous vehicles in complex driving environments.

Method: DriveSuprim introduces progressive candidate filtering, rotation-based data augmentation, and a self-distillation framework to enhance selection-based trajectory prediction models.

Result: DriveSuprim achieved state-of-the-art results with 93.5% PDMS on NAVSIM v1 and 87.1% EPDMS on NAVSIM v2, with improvements in collision avoidance, rule compliance, and trajectory quality.

Conclusion: DriveSuprim advances autonomous vehicle safety by refining trajectory selection methods, demonstrating superior performance without requiring additional data.

Abstract: In complex driving environments, autonomous vehicles must navigate safely.
Relying on a single predicted path, as in regression-based approaches, usually
does not explicitly assess the safety of the predicted trajectory.
Selection-based methods address this by generating and scoring multiple
trajectory candidates and predicting the safety score for each, but face
optimization challenges in precisely selecting the best option from thousands
of possibilities and distinguishing subtle but safety-critical differences,
especially in rare or underrepresented scenarios. We propose DriveSuprim to
overcome these challenges and advance the selection-based paradigm through a
coarse-to-fine paradigm for progressive candidate filtering, a rotation-based
augmentation method to improve robustness in out-of-distribution scenarios, and
a self-distillation framework to stabilize training. DriveSuprim achieves
state-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS
in NAVSIM v2 without extra data, demonstrating superior safetycritical
capabilities, including collision avoidance and compliance with rules, while
maintaining high trajectory quality in various driving scenarios.

</details>


### [646] [Generalized Trajectory Scoring for End-to-end Multimodal Planning](https://arxiv.org/abs/2506.06664)
*Zhenxin Li,Wenhao Yao,Zi Wang,Xinglong Sun,Joshua Chen,Nadine Chang,Maying Shen,Zuxuan Wu,Shiyi Lan,Jose M. Alvarez*

Main category: cs.RO

TL;DR: This paper introduces GTRS, a unified framework for robust trajectory scoring in autonomous driving by combining coarse and fine-grained evaluations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses shortcomings in existing trajectory scoring methods which either lack fine-grained adaptation or fail to generalize to broader trajectory distributions.

Method: The approach combines a diffusion-based trajectory generator for diverse proposals, a vocabulary generalization technique for robust inference, and a sensor augmentation strategy to improve out-of-domain generalization.

Result: GTRS outperformed prior methods and achieved superior performance even with imperfect sensor inputs, winning the Navsim v2 Challenge.

Conclusion: The proposed GTRS framework effectively integrates multiple innovations for robust and generalizable trajectory scoring, setting a new benchmark in end-to-end multi-modal planning.

Abstract: End-to-end multi-modal planning is a promising paradigm in autonomous
driving, enabling decision-making with diverse trajectory candidates. A key
component is a robust trajectory scorer capable of selecting the optimal
trajectory from these candidates. While recent trajectory scorers focus on
scoring either large sets of static trajectories or small sets of dynamically
generated ones, both approaches face significant limitations in generalization.
Static vocabularies provide effective coarse discretization but struggle to
make fine-grained adaptation, while dynamic proposals offer detailed precision
but fail to capture broader trajectory distributions. To overcome these
challenges, we propose GTRS (Generalized Trajectory Scoring), a unified
framework for end-to-end multi-modal planning that combines coarse and
fine-grained trajectory evaluation. GTRS consists of three complementary
innovations: (1) a diffusion-based trajectory generator that produces diverse
fine-grained proposals; (2) a vocabulary generalization technique that trains a
scorer on super-dense trajectory sets with dropout regularization, enabling its
robust inference on smaller subsets; and (3) a sensor augmentation strategy
that enhances out-of-domain generalization while incorporating refinement
training for critical trajectory discrimination. As the winning solution of the
Navsim v2 Challenge, GTRS demonstrates superior performance even with
sub-optimal sensor inputs, approaching privileged methods that rely on
ground-truth perception. Code will be available at
https://github.com/NVlabs/GTRS.

</details>


### [647] [RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation](https://arxiv.org/abs/2506.06677)
*Songhao Han,Boxiang Qiu,Yue Liao,Siyuan Huang,Chen Gao,Shuicheng Yan,Si Liu*

Main category: cs.RO

TL;DR: RoboCerebra introduces a benchmark to evaluate long-horizon reasoning in robotic manipulation, leveraging vision-language models for hierarchical planning and control.


<details>
  <summary>Details</summary>
Motivation: Existing robotic systems primarily use reactive policies that fail to exploit the deliberative and goal-directed reasoning capabilities of vision-language models.

Method: Introduce a hierarchical framework combining VLM planners with vision-language-action controllers, along with a large-scale, task-rich dataset and evaluation protocols for System 1-System 2 interaction.

Result: RoboCerebra showcases benchmark tasks that are longer and more complex, enabling better assessment of system performance in planning, memory, and reflection dimensions.

Conclusion: The benchmark advances the evaluation of cognitive capabilities in robotics, encouraging the development of more generalizable and intelligent robotic planners.

Abstract: Recent advances in vision-language models (VLMs) have enabled
instruction-conditioned robotic systems with improved generalization. However,
most existing work focuses on reactive System 1 policies, underutilizing VLMs'
strengths in semantic reasoning and long-horizon planning. These System 2
capabilities-characterized by deliberative, goal-directed thinking-remain under
explored due to the limited temporal scale and structural complexity of current
benchmarks. To address this gap, we introduce RoboCerebra, a benchmark for
evaluating high-level reasoning in long-horizon robotic manipulation.
RoboCerebra includes: (1) a large-scale simulation dataset with extended task
horizons and diverse subtask sequences in household environments; (2) a
hierarchical framework combining a high-level VLM planner with a low-level
vision-language-action (VLA) controller; and (3) an evaluation protocol
targeting planning, reflection, and memory through structured System 1-System 2
interaction. The dataset is constructed via a top-down pipeline, where GPT
generates task instructions and decomposes them into subtask sequences. Human
operators execute the subtasks in simulation, yielding high-quality
trajectories with dynamic object variations. Compared to prior benchmarks,
RoboCerebra features significantly longer action sequences and denser
annotations. We further benchmark state-of-the-art VLMs as System 2 modules and
analyze their performance across key cognitive dimensions, advancing the
development of more capable and generalizable robotic planners.

</details>


### [648] [RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks](https://arxiv.org/abs/2506.06683)
*Shiying Duan,Pei Ren,Nanxiang Jiang,Zhengping Che,Jian Tang,Yifan Sun,Zhaoxin Fan,Wenjun Wu*

Main category: cs.RO

TL;DR: The paper proposes RoboPARA, an LLM-driven framework to improve task parallelism in dual-arm robots, using dependency graphs and optimized traversal strategies, along with a new dataset, X-DAPT, for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for dual-arm robot task planning often fall short of optimizing task parallelism, thereby limiting the efficiency of dual-arm collaboration in multitasking scenarios.

Method: The paper introduces a two-stage framework: (1) Dependency Graph-based Planning Candidates Generation, which creates DAGs to map and streamline task dependencies, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning to enhance parallelism and maintain task coherence. Additionally, it presents the X-DAPT dataset for evaluating dual-arm parallelism.

Result: Experiments conducted on the X-DAPT dataset show that RoboPARA surpasses current methodologies in efficiency and reliability, especially in handling complex task combinations.

Conclusion: RoboPARA effectively advances task parallelism in dual-arm robot systems, offering superior performance compared to existing methods. The framework and its associated dataset represent significant contributions to enhancing multitasking scenarios for robotics.

Abstract: Dual-arm robots play a crucial role in improving efficiency and flexibility
in complex multitasking scenarios. While existing methods have achieved
promising results in task planning, they often fail to fully optimize task
parallelism, limiting the potential of dual-arm collaboration. To address this
issue, we propose RoboPARA, a novel large language model (LLM)-driven framework
for dual-arm task parallelism planning. RoboPARA employs a two-stage process:
(1) Dependency Graph-based Planning Candidates Generation, which constructs
directed acyclic graphs (DAGs) to model task dependencies and eliminate
redundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which
optimizes DAG traversal to maximize parallelism while maintaining task
coherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task
dataset (X-DAPT dataset), the first dataset specifically designed to evaluate
dual-arm task parallelism across diverse scenarios and difficulty levels.
Extensive experiments on the X-DAPT dataset demonstrate that RoboPARA
significantly outperforms existing methods, achieving higher efficiency and
reliability, particularly in complex task combinations. The code and dataset
will be released upon acceptance.

</details>


### [649] [SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game](https://arxiv.org/abs/2506.06690)
*Hao Wang,Chengkai Hou,Xianglong Li,Yankai Fu,Chenxuan Li,Ning Chen,Gaole Dai,Jiaming Liu,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: Robotic table tennis is used to address challenges in high-speed object control using SpikePingpong, integrating spike-based vision and imitation learning to achieve significant improvements in precision and gameplay strategies.


<details>
  <summary>Details</summary>
Motivation: This paper aims to enhance robotic control in dynamic and precision-critical domains, using table tennis as a testbed to tackle challenges in vision accuracy and strategic planning.

Method: The proposed system utilizes a spike camera for high-resolution ball tracking and introduces SONIC for precise trajectory prediction and IMPACT for strategic ball placement, with imitation learning for stroke planning.

Result: SpikePingpong achieved a success rate of 91% for 30 cm accuracy and 71% for 20 cm accuracy, outperforming previous systems by 38% and 37%, respectively.

Conclusion: SpikePingpong significantly advances robotic control in dynamic tasks, enabling precise gameplay strategies and offering a new perspective in fast-paced robotics research.

Abstract: Learning to control high-speed objects in the real world remains a
challenging frontier in robotics. Table tennis serves as an ideal testbed for
this problem, demanding both rapid interception of fast-moving balls and
precise adjustment of their trajectories. This task presents two fundamental
challenges: it requires a high-precision vision system capable of accurately
predicting ball trajectories, and it necessitates intelligent strategic
planning to ensure precise ball placement to target regions. The dynamic nature
of table tennis, coupled with its real-time response requirements, makes it
particularly well-suited for advancing robotic control capabilities in
fast-paced, precision-critical domains. In this paper, we present
SpikePingpong, a novel system that integrates spike-based vision with imitation
learning for high-precision robotic table tennis. Our approach introduces two
key attempts that directly address the aforementioned challenges: SONIC, a
spike camera-based module that achieves millimeter-level precision in
ball-racket contact prediction by compensating for real-world uncertainties
such as air resistance and friction; and IMPACT, a strategic planning module
that enables accurate ball placement to targeted table regions. The system
harnesses a 20 kHz spike camera for high-temporal resolution ball tracking,
combined with efficient neural network models for real-time trajectory
correction and stroke planning. Experimental results demonstrate that
SpikePingpong achieves a remarkable 91% success rate for 30 cm accuracy target
area and 71% in the more challenging 20 cm accuracy task, surpassing previous
state-of-the-art approaches by 38% and 37% respectively. These significant
performance improvements enable the robust implementation of sophisticated
tactical gameplay strategies, providing a new research perspective for robotic
control in high-speed dynamic tasks.

</details>


### [650] [SARAL-Bot: Autonomous Robot for Strawberry Plant Care](https://arxiv.org/abs/2506.06798)
*Arif Ahmed,Ritvik Agarwal,Gaurav Srikar,Nathaniel Rose,Parikshit Maini*

Main category: cs.RO

TL;DR: Team SARAL develops an autonomous robot for strawberry farming that navigates, detects unhealthy leaves, and removes them.


<details>
  <summary>Details</summary>
Motivation: Address labor shortages, reduce costs, and promote sustainable farming in strawberry cultivation through automation.

Method: Development of a vision-based autonomous robot capable of navigation, unhealthy leaf detection, and leaf removal.

Result: Demonstrated the potential of robotics to modernize strawberry farming and provide scalable agricultural solutions.

Conclusion: Robotics-based approaches can significantly aid in modernizing and addressing challenges in strawberry farming, benefiting scalability and sustainability.

Abstract: Strawberry farming demands intensive labor for monitoring and maintaining
plant health. To address this, Team SARAL develops an autonomous robot for the
2024 ASABE Student Robotics Challenge, capable of navigation, unhealthy leaf
detection, and removal. The system addresses labor shortages, reduces costs,
and supports sustainable farming through vision-based plant assessment. This
work demonstrates the potential of robotics to modernize strawberry cultivation
and enable scalable, intelligent agricultural solutions.

</details>


### [651] [IRS: Instance-Level 3D Scene Graphs via Room Prior Guided LiDAR-Camera Fusion](https://arxiv.org/abs/2506.06804)
*Hongming Chen,Yiyang Lin,Ziliang Li,Biyu Ye,Yuying Zhang,Ximin Lyu*

Main category: cs.RO

TL;DR: This paper introduces a framework for efficient 3D scene graph construction using LiDAR-camera fusion, optimized for open-world environments.


<details>
  <summary>Details</summary>
Motivation: The challenge of indoor scene understanding in robotics, especially for tasks like navigation and manipulation, requires overcoming limitations of traditional recognition methods and leveraging new capabilities of visual foundation models.

Method: The proposed framework uses LiDAR for geometric priors and multi-level visual foundation models for improved semantic extraction, with room-based segmentation enabling parallel processing and optimal fusion of geometric and semantic data.

Result: Their approach significantly improves construction speed (up to an order-of-magnitude) compared to state-of-the-art methods while maintaining high semantic precision.

Conclusion: Validated by experiments, this method demonstrates practical applicability, showcasing its potential in real-world tasks like semantic navigation in robotics.

Abstract: Indoor scene understanding remains a fundamental challenge in robotics, with
direct implications for downstream tasks such as navigation and manipulation.
Traditional approaches often rely on closed-set recognition or loop closure,
limiting their adaptability in open-world environments. With the advent of
visual foundation models (VFMs), open-vocabulary recognition and natural
language querying have become feasible, unlocking new possibilities for 3D
scene graph construction.
  In this paper, we propose a robust and efficient framework for instance-level
3D scene graph construction via LiDAR-camera fusion. Leveraging LiDAR's wide
field of view (FOV) and long-range sensing capabilities, we rapidly acquire
room-level geometric priors. Multi-level VFMs are employed to improve the
accuracy and consistency of semantic extraction. During instance fusion,
room-based segmentation enables parallel processing, while the integration of
geometric and semantic cues significantly enhances fusion accuracy and
robustness. Compared to state-of-the-art methods, our approach achieves up to
an order-of-magnitude improvement in construction speed while maintaining high
semantic precision.
  Extensive experiments in both simulated and real-world environments validate
the effectiveness of our approach. We further demonstrate its practical value
through a language-guided semantic navigation task, highlighting its potential
for real-world robotic applications.

</details>


### [652] [RF-Source Seeking with Obstacle Avoidance using Real-time Modified Artificial Potential Fields in Unknown Environments](https://arxiv.org/abs/2506.06811)
*Shahid Mohammad Mulla,Aryan Kanakapudi,Lakshmi Narasimhan,Anuj Tiwari*

Main category: cs.RO

TL;DR: This paper introduces a real-time trajectory planning method for UAV navigation in unknown obstacle-laden environments using adaptable Artificial Potential Field (APF) and RF source seeking.


<details>
  <summary>Details</summary>
Motivation: Current UAV obstacle avoidance algorithms like APF fail in environments with varying obstacles and lack mechanisms for navigation without precise target locations, essential for tasks like search and rescue.

Method: The authors proposed a sampling-based adaptation of APF combined with an RF source seeking algorithm to estimate bearing angles and dynamically adjust potential field parameters in real-time.

Result: Simulations indicate the RF source seeking algorithm is highly accurate, with an average angular error of 1.48 degrees. The modified navigation algorithm improves the success rate of reaching targets by 46% and reduces trajectory length by 1.2% compared to standard APF.

Conclusion: The proposed method successfully enhances UAV navigation by combining accurate RF-based target alignment and adaptable obstacle avoidance, showcasing significant improvements in success rate and efficiency.

Abstract: Navigation of UAVs in unknown environments with obstacles is essential for
applications in disaster response and infrastructure monitoring. However,
existing obstacle avoidance algorithms, such as Artificial Potential Field
(APF) are unable to generalize across environments with different obstacle
configurations. Furthermore, the precise location of the final target may not
be available in applications such as search and rescue, in which case
approaches such as RF source seeking can be used to align towards the target
location. This paper proposes a real-time trajectory planning method, which
involves real-time adaptation of APF through a sampling-based approach. The
proposed approach utilizes only the bearing angle of the target without its
precise location, and adjusts the potential field parameters according to the
environment with new obstacle configurations in real time. The main
contributions of the article are i) an RF source seeking algorithm to provide a
bearing angle estimate using RF signal calculations based on antenna placement,
and ii) a modified APF for adaptable collision avoidance in changing
environments, which are evaluated separately in the simulation software Gazebo,
using ROS2 for communication. Simulation results show that the RF
source-seeking algorithm achieves high accuracy, with an average angular error
of just 1.48 degrees, and with this estimate, the proposed navigation algorithm
improves the success rate of reaching the target by 46% and reduces the
trajectory length by 1.2% compared to standard potential fields.

</details>


### [653] [Multimodal Spatial Language Maps for Robot Navigation and Manipulation](https://arxiv.org/abs/2506.06862)
*Chenguang Huang,Oier Mees,Andy Zeng,Wolfram Burgard*

Main category: cs.RO

TL;DR: The paper proposes a new spatial 3D map system integrating multimodal data (audio, visual, language) for autonomous navigation and goal localization, resulting in improved performance in ambiguous scenarios.


<details>
  <summary>Details</summary>
Motivation: Current grounding methods for robots lack spatial precision, integration with environmental mapping, and effective use of multimodal sensory data.

Method: Developed multimodal spatial language maps (VLMaps and AVLMaps) combining 3D environment reconstruction with pretrained multimodal features, integrated with large language models for zero-shot navigation.

Result: Improved recall by 50% in ambiguous scenarios, enabling robots to process multimodal queries and navigate autonomously in both real and simulated environments.

Conclusion: The fusion of audio, visual, and language data enhances robot navigation and goal localization capabilities, showcasing advanced zero-shot multimodal reasoning for robotic systems.

Abstract: Grounding language to a navigating agent's observations can leverage
pretrained multimodal foundation models to match perceptions to object or event
descriptions. However, previous approaches remain disconnected from environment
mapping, lack the spatial precision of geometric maps, or neglect additional
modality information beyond vision. To address this, we propose multimodal
spatial language maps as a spatial map representation that fuses pretrained
multimodal features with a 3D reconstruction of the environment. We build these
maps autonomously using standard exploration. We present two instances of our
maps, which are visual-language maps (VLMaps) and their extension to
audio-visual-language maps (AVLMaps) obtained by adding audio information. When
combined with large language models (LLMs), VLMaps can (i) translate natural
language commands into open-vocabulary spatial goals (e.g., "in between the
sofa and TV") directly localized in the map, and (ii) be shared across
different robot embodiments to generate tailored obstacle maps on demand.
Building upon the capabilities above, AVLMaps extend VLMaps by introducing a
unified 3D spatial representation integrating audio, visual, and language cues
through the fusion of features from pretrained multimodal foundation models.
This enables robots to ground multimodal goal queries (e.g., text, images, or
audio snippets) to spatial locations for navigation. Additionally, the
incorporation of diverse sensory inputs significantly enhances goal
disambiguation in ambiguous environments. Experiments in simulation and
real-world settings demonstrate that our multimodal spatial language maps
enable zero-shot spatial and multimodal goal navigation and improve recall by
50% in ambiguous scenarios. These capabilities extend to mobile robots and
tabletop manipulators, supporting navigation and interaction guided by visual,
audio, and spatial cues.

</details>


### [654] [Hierarchical Intention Tracking with Switching Trees for Real-Time Adaptation to Dynamic Human Intentions during Collaboration](https://arxiv.org/abs/2506.07004)
*Zhe Huang,Ye-Ji Mun,Fatemeh Cheraghi Pouria,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: This paper introduces the Hierarchical Intention Tracking (HIT) algorithm for collaborative robots to real-time track human intentions evolving at multiple levels, significantly improving task efficiency and user experience.


<details>
  <summary>Details</summary>
Motivation: Collaborative robots require the capability to dynamically understand and adapt to changing human intentions across multiple levels, as these preferences evolve during tasks in real time.

Method: The proposed HIT algorithm uses intention trees with Bayesian filtering, upward measurement propagation, and downward posterior propagation to accurately track human hierarchical intentions during collaborative tasks. A HIT-based robotic system coordinates at three levels: task-level, interaction-level, and verification-level.

Result: In user studies, the HIT-based robot outperformed existing systems by balancing efficiency, physical workload, and user comfort. It also improved user trust and reduced task flow interruptions.

Conclusion: The HIT algorithm allows collaborative robots to better understand and respond to dynamic, multi-level human intentions, thereby enhancing coordination, safety, and trust in human-robot tasks.

Abstract: During collaborative tasks, human behavior is guided by multiple levels of
intentions that evolve over time, such as task sequence preferences and
interaction strategies. To adapt to these changing preferences and promptly
correct any inaccurate estimations, collaborative robots must accurately track
these dynamic human intentions in real time. We propose a Hierarchical
Intention Tracking (HIT) algorithm for collaborative robots to track dynamic
and hierarchical human intentions effectively in real time. HIT represents
human intentions as intention trees with arbitrary depth, and probabilistically
tracks human intentions by Bayesian filtering, upward measurement propagation,
and downward posterior propagation across all levels. We develop a HIT-based
robotic system that dynamically switches between Interaction-Task and
Verification-Task trees for a collaborative assembly task, allowing the robot
to effectively coordinate human intentions at three levels: task-level (subtask
goal locations), interaction-level (mode of engagement with the robot), and
verification-level (confirming or correcting intention recognition). Our user
study shows that our HIT-based collaborative robot system surpasses existing
collaborative robot solutions by achieving a balance between efficiency,
physical workload, and user comfort while ensuring safety and task completion.
Post-experiment surveys further reveal that the HIT-based system enhances the
user trust and minimizes interruptions to user's task flow through its
effective understanding of human intentions across multiple levels.

</details>


### [655] [CARoL: Context-aware Adaptation for Robot Learning](https://arxiv.org/abs/2506.07006)
*Zechen Hu,Tong Xu,Xuesu Xiao,Xuan Wang*

Main category: cs.RO

TL;DR: The paper presents CARoL, a framework to enhance robotic learning efficiency by using prior knowledge adaptively and contextually.


<details>
  <summary>Details</summary>
Motivation: Learning robotic tasks from scratch using RL is inefficient. Incorporating prior knowledge can help, but determining its relevance and adaptive integration is challenging.

Method: CARoL identifies similarities between new tasks and prior knowledge by analyzing state transitions in system dynamics, then adapts and prioritizes relevant knowledge.

Result: CARoL achieves faster convergence and higher rewards in simulations (CarRacing, LunarLander) and real-world robotic ground vehicle tasks.

Conclusion: CARoL efficiently adapts prior knowledge to new tasks, improving learning speed and performance, applicable to multiple RL algorithms.

Abstract: Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is
often inefficient. Leveraging prior knowledge has the potential to
significantly enhance learning efficiency, which, however, raises two critical
challenges: how to determine the relevancy of existing knowledge and how to
adaptively integrate them into learning a new task. In this paper, we propose
Context-aware Adaptation for Robot Learning (CARoL), a novel framework to
efficiently learn a similar but distinct new task from prior knowledge. CARoL
incorporates context awareness by analyzing state transitions in system
dynamics to identify similarities between the new task and prior knowledge. It
then utilizes these identified similarities to prioritize and adapt specific
knowledge pieces for the new task. Additionally, CARoL has a broad
applicability spanning policy-based, value-based, and actor-critic RL
algorithms. We validate the efficiency and generalizability of CARoL on both
simulated robotic platforms and physical ground vehicles. The simulations
include CarRacing and LunarLander environments, where CARoL demonstrates faster
convergence and higher rewards when learning policies for new tasks. In
real-world experiments, we show that CARoL enables a ground vehicle to quickly
and efficiently adapt policies learned in simulation to smoothly traverse
real-world off-road terrain.

</details>


### [656] [Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search](https://arxiv.org/abs/2506.07062)
*Dongryung Lee,Sejune Joo,Kimin Lee,Beomjoon Kim*

Main category: cs.RO

TL;DR: This paper proposes using Large Language Models (LLMs) augmented with Monte Carlo Tree Search (MCTS) for Geometric Task and Motion Planning (G-TAMP) problems, achieving improved performance compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inefficiencies of traditional G-TAMP methods, which rely heavily on computational resources or extensive data, and explore leveraging human-like common sense provided by LLMs to enhance planning.

Method: The paper introduces a predicate-based prompt integrating geometric reasoning with LLMs and combines it with Monte Carlo Tree Search (MCTS) to guide task planning more efficiently.

Result: The proposed method outperformed previous LLM-based planners and pure search algorithms in six distinct G-TAMP scenarios.

Conclusion: The study concludes that combining LLMs with geometric reasoning and an optimized MCTS framework provides a promising approach to solving G-TAMP problems more effectively.

Abstract: The problem of relocating a set of objects to designated areas amidst movable
obstacles can be framed as a Geometric Task and Motion Planning (G-TAMP)
problem, a subclass of task and motion planning (TAMP). Traditional approaches
to G-TAMP have relied either on domain-independent heuristics or on learning
from planning experience to guide the search, both of which typically demand
significant computational resources or data. In contrast, humans often use
common sense to intuitively decide which objects to manipulate in G-TAMP
problems. Inspired by this, we propose leveraging Large Language Models (LLMs),
which have common sense knowledge acquired from internet-scale data, to guide
task planning in G-TAMP problems. To enable LLMs to perform geometric
reasoning, we design a predicate-based prompt that encodes geometric
information derived from a motion planning algorithm. We then query the LLM to
generate a task plan, which is then used to search for a feasible set of
continuous parameters. Since LLMs are prone to mistakes, instead of committing
to LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action
space and use the LLM to guide the search. Unlike the previous approach that
calls an LLM at every node and incurs high computational costs, we use it to
warm-start the MCTS with the nodes explored in completing the LLM's task plan.
On six different G-TAMP problems, we show our method outperforms previous LLM
planners and pure search algorithms. Code can be found at:
https://github.com/iMSquared/prime-the-search

</details>


### [657] [Robotic Policy Learning via Human-assisted Action Preference Optimization](https://arxiv.org/abs/2506.07127)
*Wenke xia,Yichu Yang,Hongtao Wu,Xiao Ma,Tao Kong,Di Hu*

Main category: cs.RO

TL;DR: HAPO enhances VLA models by enabling failure correction and preference alignment through human-intervention-driven trajectory optimization, improving robotic task robustness.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action (VLA) models struggle to correct failures and learn effectively due to reliance on expert demonstrations.

Method: The paper introduces HAPO, a human-assisted action preference optimization method using adaptive reweighting to refine VLA models through human-intervention trajectories.

Result: HAPO exhibited superior generalization and robustness in both simulated and real-world manipulation tasks.

Conclusion: HAPO enables reliable failure correction and adaptation, boosting real-world applicability and robustness for VLA robotic systems.

Abstract: Establishing a reliable and iteratively refined robotic system is essential
for deploying real-world applications. While Vision-Language-Action (VLA)
models are widely recognized as the foundation model for such robotic
deployment, their dependence on expert demonstrations hinders the crucial
capabilities of correction and learning from failures. To mitigate this
limitation, we introduce a Human-assisted Action Preference Optimization method
named HAPO, designed to correct deployment failures and foster effective
adaptation through preference alignment for VLA models. This method begins with
a human-robot collaboration framework for reliable failure correction and
interaction trajectory collection through human intervention. These
human-intervention trajectories are further employed within the action
preference optimization process, facilitating VLA models to mitigate failure
action occurrences while enhancing corrective action adaptation. Specifically,
we propose an adaptive reweighting algorithm to address the issues of
irreversible interactions and token probability mismatch when introducing
preference optimization into VLA models, facilitating model learning from
binary desirability signals derived from interactions. Through combining these
modules, our human-assisted action preference optimization method ensures
reliable deployment and effective learning from failure for VLA models. The
experiments conducted in simulation and real-world scenarios prove superior
generalization and robustness of our framework across a variety of manipulation
tasks.

</details>


### [658] [Improving Traffic Signal Data Quality for the Waymo Open Motion Dataset](https://arxiv.org/abs/2506.07150)
*Xintao Yan,Erdao Liang,Jiawei Wang,Haojie Zhu,Henry X. Liu*

Main category: cs.RO

TL;DR: This paper presents a fully automated method to impute and correct missing or inaccurate traffic signal data in the Waymo Open Motion Dataset (WOMD), significantly improving its quality for artificial intelligence and autonomous vehicle research.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by the frequent issues of missing or inaccurate traffic signal data in AV datasets, which can compromise their reliability and the performance of AI models.

Method: The method uses vehicle trajectory data and knowledge from transportation engineering to impute and correct traffic signal states in the WOMD, accommodating diverse intersection geometries and configurations.

Result: The approach successfully imputed 71.7% missing or unknown traffic signal states, reducing red-light violations in data from 15.7% to 2.9%. It validated over 360,000 scenarios.

Conclusion: The proposed method improves the reliability of AV datasets, enhances AI and AV research, and supports diverse practical applications, with open-access code and data for the community.

Abstract: Datasets pertaining to autonomous vehicles (AVs) hold significant promise for
a range of research fields, including artificial intelligence (AI), autonomous
driving, and transportation engineering. Nonetheless, these datasets often
encounter challenges related to the states of traffic signals, such as missing
or inaccurate data. Such issues can compromise the reliability of the datasets
and adversely affect the performance of models developed using them. This
research introduces a fully automated approach designed to tackle these issues
by utilizing available vehicle trajectory data alongside knowledge from the
transportation domain to effectively impute and rectify traffic signal
information within the Waymo Open Motion Dataset (WOMD). The proposed method is
robust and flexible, capable of handling diverse intersection geometries and
traffic signal configurations in real-world scenarios. Comprehensive
validations have been conducted on the entire WOMD, focusing on over 360,000
relevant scenarios involving traffic signals, out of a total of 530,000
real-world driving scenarios. In the original dataset, 71.7% of traffic signal
states are either missing or unknown, all of which were successfully imputed by
our proposed method. Furthermore, in the absence of ground-truth signal states,
the accuracy of our approach is evaluated based on the rate of red-light
violations among vehicle trajectories. Results show that our method reduces the
estimated red-light running rate from 15.7% in the original data to 2.9%,
thereby demonstrating its efficacy in rectifying data inaccuracies. This paper
significantly enhances the quality of AV datasets, contributing to the wider AI
and AV research communities and benefiting various downstream applications. The
code and improved traffic signal data are open-sourced at
https://github.com/michigan-traffic-lab/WOMD-Traffic-Signal-Data-Improvement

</details>


### [659] [MorphoCopter: Design, Modeling, and Control of a New Transformable Quad-Bi Copter](https://arxiv.org/abs/2506.07204)
*Harsh Modi,Hao Su,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: The paper introduces MorphoCopter, a quadrotor capable of morphing into an ultra-narrow profile for enhanced operational flexibility.


<details>
  <summary>Details</summary>
Motivation: Traditional quadrotor designs limit capabilities in constrained environments due to unchanging hardware configurations.

Method: A single rotary joint enables rapid hardware transformation, supported by an adaptive control system for robust performance in various configurations.

Result: MorphoCopter achieves a 70% width reduction in seconds, validated by simulations and flight experiments.

Conclusion: MorphoCopter's design and adaptive control expand quadrotor usability, particularly in environments requiring compactness and precision.

Abstract: This paper presents a novel morphing quadrotor, named MorphoCopter, covering
its design, modeling, control, and experimental tests. It features a unique
single rotary joint that enables rapid transformation into an ultra-narrow
profile. Although quadrotors have seen widespread adoption in applications such
as cinematography, agriculture, and disaster management with increasingly
sophisticated control systems, their hardware configurations have remained
largely unchanged, limiting their capabilities in certain environments. Our
design addresses this by enabling the hardware configuration to change on the
fly when required. In standard flight mode, the MorphoCopter adopts an X
configuration, functioning as a traditional quadcopter, but can quickly fold
into a stacked bicopters arrangement or any configuration in between. Existing
morphing designs often sacrifice controllability in compact configurations or
rely on complex multi-joint systems. Moreover, our design achieves a greater
width reduction than any existing solution. We develop a new inertia and
control-action aware adaptive control system that maintains robust performance
across all rotary-joint configurations. The prototype can reduce its width from
447 mm to 138 mm (nearly 70\% reduction) in just a few seconds. We validated
the MorphoCopter through rigorous simulations and a comprehensive series of
flight experiments, including robustness tests, trajectory tracking, and
narrow-gap passing tests.

</details>


### [660] [Machine Learning-Based Self-Localization Using Internal Sensors for Automating Bulldozers](https://arxiv.org/abs/2506.07271)
*Hikaru Sawafuji,Ryota Ozaki,Takuto Motomura,Toyohisa Matsuda,Masanori Tojima,Kento Uchida,Shinichi Shirakawa*

Main category: cs.RO

TL;DR: The paper introduces a machine learning-based self-localization method for bulldozers, which does not rely on RTK-GNSS, addressing signal loss issues. Experiments demonstrated its effectiveness in suppressing position errors.


<details>
  <summary>Details</summary>
Motivation: RTK-GNSS-based self-localization systems face signal loss issues in certain mining conditions, necessitating alternative approaches for bulldozer automation.

Method: The method involves estimating local velocities using machine learning with internal sensors and integrating these estimates into an Extended Kalman Filter (EKF) for global localization.

Result: Experimental results showed that the proposed method reduced position errors and outperformed kinematics-based methods, particularly in scenarios involving slip.

Conclusion: The study highlights the potential of bulldozer-specific sensors and machine learning in enhancing self-localization accuracy, offering a robust solution for mining environments.

Abstract: Self-localization is an important technology for automating bulldozers.
Conventional bulldozer self-localization systems rely on RTK-GNSS (Real Time
Kinematic-Global Navigation Satellite Systems). However, RTK-GNSS signals are
sometimes lost in certain mining conditions. Therefore, self-localization
methods that do not depend on RTK-GNSS are required. In this paper, we propose
a machine learning-based self-localization method for bulldozers. The proposed
method consists of two steps: estimating local velocities using a machine
learning model from internal sensors, and incorporating these estimates into an
Extended Kalman Filter (EKF) for global localization. We also created a novel
dataset for bulldozer odometry and conducted experiments across various driving
scenarios, including slalom, excavation, and driving on slopes. The result
demonstrated that the proposed self-localization method suppressed the
accumulation of position errors compared to kinematics-based methods,
especially when slip occurred. Furthermore, this study showed that
bulldozer-specific sensors, such as blade position sensors and hydraulic
pressure sensors, contributed to improving self-localization accuracy.

</details>


### [661] [Model Analysis And Design Of Ellipse Based Segmented Varying Curved Foot For Biped Robot Walking](https://arxiv.org/abs/2506.07283)
*Boyang Chen,Xizhe Zang,Chao Song,Yue Zhang,Jie Zhao*

Main category: cs.RO

TL;DR: The paper introduces an energy-efficient Ellipse-based Segmented Varying Curvature (ESVC) foot for bipedal robots and validates the design experimentally.


<details>
  <summary>Details</summary>
Motivation: To improve gait energy efficiency in bipedal robots inspired by human foot's segmented curvature rollover shape while maintaining analytical simplicity.

Method: Developed an analytical contact model for ESVC foot using elementary functions, optimized elliptical parameters using nonlinear programming, and integrated the foot with a walking controller for simulations and experiments.

Result: Experimental results demonstrated significant reduction in energy consumption during various walking tasks, achieving up to 18.52% improvement in lateral walking.

Conclusion: The ESVC foot offers a novel and practical solution for energy-efficient bipedal locomotion and provides a foundation for future optimization studies.

Abstract: This paper presents the modeling, design, and experimental validation of an
Ellipse-based Segmented Varying Curvature (ESVC) foot for bipedal robots.
Inspired by the segmented curvature rollover shape of human feet, the ESVC foot
aims to enhance gait energy efficiency while maintaining analytical
tractability for foot location based controller. First, we derive a complete
analytical contact model for the ESVC foot by formulating spatial
transformations of elliptical segments only using elementary functions. Then a
nonlinear programming approach is engaged to determine optimal elliptical
parameters of hind foot and fore foot based on a known mid-foot. An error
compensation method is introduced to address approximation inaccuracies in
rollover length calculation. The proposed ESVC foot is then integrated with a
Hybrid Linear Inverted Pendulum model-based walking controller and validated
through both simulation and physical experiments on the TT II biped robot.
Experimental results across marking time, sagittal, and lateral walking tasks
show that the ESVC foot consistently reduces energy consumption compared to
line, and flat feet, with up to 18.52\% improvement in lateral walking. These
findings demonstrate that the ESVC foot provides a practical and
energy-efficient alternative for real-world bipedal locomotion. The proposed
design methodology also lays a foundation for data-driven foot shape
optimization in future research.

</details>


### [662] [Very Large-scale Multi-Robot Task Allocation in Challenging Environments via Robot Redistribution](https://arxiv.org/abs/2506.07293)
*Seabin Lee,Joonyeol Sim,Changjoo Nam*

Main category: cs.RO

TL;DR: The paper addresses the Multi-Robot Task Allocation (MRTA) problem in environments with dense obstacles, proposing a scalable method using Generalized Voronoi Diagram to incorporate paths and minimize makespan.


<details>
  <summary>Details</summary>
Motivation: To tackle inefficiencies in MRTA scenarios, such as collisions and deadlocks, caused by ignoring robot paths during task allocation in densely populated environments.

Method: The authors use a Generalized Voronoi Diagram to construct a roadmap, partitioning it into components for optimal robot redistribution. They employ a push-pop mechanism with first-in first-out for path optimization.

Result: Experiments show the proposed method efficiently handles hundreds of robots in dense environments where comparable methods fail within a time limit.

Conclusion: The proposed scalable MRTA method successfully integrates path planning into task allocation, minimizing conflicts, deadlocks, and ensuring fast task completion in complex scenarios.

Abstract: We consider the Multi-Robot Task Allocation (MRTA) problem that aims to
optimize an assignment of multiple robots to multiple tasks in challenging
environments which are with densely populated obstacles and narrow passages. In
such environments, conventional methods optimizing the sum-of-cost are often
ineffective because the conflicts between robots incur additional costs (e.g.,
collision avoidance, waiting). Also, an allocation that does not incorporate
the actual robot paths could cause deadlocks, which significantly degrade the
collective performance of the robots.
  We propose a scalable MRTA method that considers the paths of the robots to
avoid collisions and deadlocks which result in a fast completion of all tasks
(i.e., minimizing the \textit{makespan}). To incorporate robot paths into task
allocation, the proposed method constructs a roadmap using a Generalized
Voronoi Diagram. The method partitions the roadmap into several components to
know how to redistribute robots to achieve all tasks with less conflicts
between the robots. In the redistribution process, robots are transferred to
their final destinations according to a push-pop mechanism with the first-in
first-out principle. From the extensive experiments, we show that our method
can handle instances with hundreds of robots in dense clutter while competitors
are unable to compute a solution within a time limit.

</details>


### [663] [BR-MPPI: Barrier Rate guided MPPI for Enforcing Multiple Inequality Constraints with Learned Signed Distance Field](https://arxiv.org/abs/2506.07325)
*Hardik Parwana,Taekyung Kim,Kehan Long,Bardh Hoxha,Hideki Okamoto,Georgios Fainekos,Dimitra Panagou*

Main category: cs.RO

TL;DR: This paper combines Model Predictive Path Integral (MPPI) control with techniques derived from Control Barrier Functions (CBF) to improve control sampling efficiency and ensure safety constraints in quadrotor systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance MPPI controllers by addressing their limitations in efficiently imposing specific safety constraints, which are critical in operational scenarios like quadrotor control.

Method: The authors propose integrating CBF-like conditions into MPPI, treating the CBF inequality constraint as an equality constraint in an augmented system. They use a parametric linear class-K function, add an extra control input, define a tailored cost function, and introduce state transformations and control projection operations for efficient handling of constraints.

Result: Simulations and quadrotor experiments demonstrate improved sampling efficiency and a stronger capability to operate closer to the safe set boundary compared to standard MPPI.

Conclusion: The proposed algorithm successfully combines MPPI with CBF-inspired methods, achieving better control sampling performance and enhanced safety adherence in challenging control environments.

Abstract: Model Predictive Path Integral (MPPI) controller is used to solve
unconstrained optimal control problems and Control Barrier Function (CBF) is a
tool to impose strict inequality constraints, a.k.a, barrier constraints. In
this work, we propose an integration of these two methods that employ CBF-like
conditions to guide the control sampling procedure of MPPI. CBFs provide an
inequality constraint restricting the rate of change of barrier functions by a
classK function of the barrier itself. We instead impose the CBF condition as
an equality constraint by choosing a parametric linear classK function and
treating this parameter as a state in an augmented system. The time derivative
of this parameter acts as an additional control input that is designed by MPPI.
A cost function is further designed to reignite Nagumo's theorem at the
boundary of the safe set by promoting specific values of classK parameter to
enforce safety. Our problem formulation results in an MPPI subject to multiple
state and control-dependent equality constraints which are non-trivial to
satisfy with randomly sampled control inputs. We therefore also introduce state
transformations and control projection operations, inspired by the literature
on path planning for manifolds, to resolve the aforementioned issue. We show
empirically through simulations and experiments on quadrotor that our proposed
algorithm exhibits better sampled efficiency and enhanced capability to operate
closer to the safe set boundary over vanilla MPPI.

</details>


### [664] [Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339)
*Kevin Black,Manuel Y. Galliker,Sergey Levine*

Main category: cs.RO

TL;DR: The paper introduces Real-Time Chunking (RTC), a method to reduce latency in AI systems during real-time physical tasks, improving task efficiency and precision.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to tackle the high latency problem in state-of-the-art vision-language action (VLA) models that leads to inefficiencies and jerky movements in real-world and dynamic control tasks.

Method: The proposed RTC algorithm works during inference-time by generating future action chunks asynchronously while executing the current chunk. It utilizes action 'freezing' and 'inpainting' techniques and does not require model re-training.

Result: RTC was tested on 12 dynamic tasks in Kinetix simulator and 6 real-world bimanual tasks. It yielded faster, more robust performance, higher throughput, and greater success in latency-affected precise tasks like match lighting.

Conclusion: RTC improves real-time execution, offering a solution for high-latency VLA systems, especially for dynamic and precise tasks. It demonstrates reliability without requiring model-specific training alterations.

Abstract: Modern AI systems, especially those interacting with the physical world,
increasingly require real-time performance. However, the high latency of
state-of-the-art generalist models, including recent vision-language action
models (VLAs), poses a significant challenge. While action chunking has enabled
temporal consistency in high-frequency control tasks, it does not fully address
the latency problem, leading to pauses or out-of-distribution jerky movements
at chunk boundaries. This paper presents a novel inference-time algorithm that
enables smooth asynchronous execution of action chunking policies. Our method,
real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out
of the box with no re-training. It generates the next action chunk while
executing the current one, "freezing" actions guaranteed to execute and
"inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly
dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging
real-world bimanual manipulation tasks. Results demonstrate that RTC is fast,
performant, and uniquely robust to inference delay, significantly improving
task throughput and enabling high success rates in precise tasks
$\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the
presence of significant latency. See
https://pi.website/research/real_time_chunking for videos.

</details>


### [665] [Reproducibility in the Control of Autonomous Mobility-on-Demand Systems](https://arxiv.org/abs/2506.07345)
*Xinling Li,Meshal Alharbi,Daniele Gammelli,James Harrison,Filipe Rodrigues,Maximilian Schiffer,Marco Pavone,Emilio Frazzoli,Jinhua Zhao,Gioele Zardini*

Main category: cs.RO

TL;DR: The paper addresses the challenges of reproducibility in Autonomous Mobility-on-Demand (AMoD) systems research by analyzing prevalent practices and proposing a framework for improving transparency and replicability.


<details>
  <summary>Details</summary>
Motivation: The motivation is the growing complexity and data-driven nature of AMoD systems, which lack standardized practices for evaluating and reporting, leading to irreproducibility issues.

Method: It involves a systematic study of irreproducibility sources, surveys literature practices, and proposes a structured reproducibility framework including guidelines and a checklist.

Result: Key components causing irreproducibility are identified, literature gaps are highlighted, and a reproducibility checklist is developed for better research practices.

Conclusion: The study aims to foster a transparent and reproducible research culture in AMoD systems and broader cyber-physical systems, ensuring reliable scientific progress in intelligent mobility design and deployment.

Abstract: Autonomous Mobility-on-Demand (AMoD) systems, powered by advances in
robotics, control, and Machine Learning (ML), offer a promising paradigm for
future urban transportation. AMoD offers fast and personalized travel services
by leveraging centralized control of autonomous vehicle fleets to optimize
operations and enhance service performance. However, the rapid growth of this
field has outpaced the development of standardized practices for evaluating and
reporting results, leading to significant challenges in reproducibility. As
AMoD control algorithms become increasingly complex and data-driven, a lack of
transparency in modeling assumptions, experimental setups, and algorithmic
implementation hinders scientific progress and undermines confidence in the
results. This paper presents a systematic study of reproducibility in AMoD
research. We identify key components across the research pipeline, spanning
system modeling, control problems, simulation design, algorithm specification,
and evaluation, and analyze common sources of irreproducibility. We survey
prevalent practices in the literature, highlight gaps, and propose a structured
framework to assess and improve reproducibility. Specifically, concrete
guidelines are offered, along with a "reproducibility checklist", to support
future work in achieving replicable, comparable, and extensible results. While
focused on AMoD, the principles and practices we advocate generalize to a
broader class of cyber-physical systems that rely on networked autonomy and
data-driven control. This work aims to lay the foundation for a more
transparent and reproducible research culture in the design and deployment of
intelligent mobility systems.

</details>


### [666] [UruBots Autonomous Cars Challenge Pro Team Description Paper for FIRA 2025](https://arxiv.org/abs/2506.07348)
*Pablo Moraes,Mónica Rodríguez,Sebastian Barcelona,Angel Da Silva,Santiago Fernandez,Hiago Sodre,Igor Nunes,Bruna Guterres,Ricardo Grando*

Main category: cs.RO

TL;DR: Autonomous car developed for 2025 FIRA Challenge, leveraging deep learning for navigation; achieved 0.4 m/s on track.


<details>
  <summary>Details</summary>
Motivation: To demonstrate innovative autonomous navigation solutions for compact electric vehicles within competitive environments.

Method: Construction of an RC-sized vehicle integrating mechanical, electronic components and CNNs trained on a dataset of over 10,000 visual images for steering and throttle control.

Result: The autonomous vehicle successfully navigated the track in under 30 seconds, maintaining an average pace of 0.4 meters per second while avoiding obstacles.

Conclusion: The project showcases the effectiveness of applying deep learning and machine-learning-based navigation systems in compact autonomous electric vehicles for competitive challenges.

Abstract: This paper describes the development of an autonomous car by the UruBots team
for the 2025 FIRA Autonomous Cars Challenge (Pro). The project involves
constructing a compact electric vehicle, approximately the size of an RC car,
capable of autonomous navigation through different tracks. The design
incorporates mechanical and electronic components and machine learning
algorithms that enable the vehicle to make real-time navigation decisions based
on visual input from a camera. We use deep learning models to process camera
images and control vehicle movements. Using a dataset of over ten thousand
images, we trained a Convolutional Neural Network (CNN) to drive the vehicle
effectively, through two outputs, steering and throttle. The car completed the
track in under 30 seconds, achieving a pace of approximately 0.4 meters per
second while avoiding obstacles.

</details>


### [667] [MapBERT: Bitwise Masked Modeling for Real-Time Semantic Mapping Generation](https://arxiv.org/abs/2506.07350)
*Yijie Deng,Shuaihang Yuan,Congcong Wen,Hao Huang,Anthony Tzes,Geeta Chandra Raju Bethala,Yi Fang*

Main category: cs.RO

TL;DR: MapBERT is a novel framework aimed at enhancing spatial awareness in embodied agents through improved semantic map generation, leveraging compact bitwise encoding and a masked transformer for robust real-time predictions.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with generating unobserved areas in real time and generalizing to new environments due to challenges in modeling sparse, imbalanced object categories and varying spatial scales.

Method: MapBERT uses BitVAE for encoding semantic maps into compact tokens and employs a masked transformer with an object-aware masking strategy to infer missing regions and relations in semantic maps efficiently.

Result: Experiments on the Gibson benchmarks demonstrate that MapBERT achieves state-of-the-art results in generating accurate semantic maps while maintaining computational efficiency.

Conclusion: MapBERT significantly advances spatial reasoning for embodied agents, offering practical benefits for tasks needing real-time predictions and generalization in diverse indoor environments.

Abstract: Spatial awareness is a critical capability for embodied agents, as it enables
them to anticipate and reason about unobserved regions. The primary challenge
arises from learning the distribution of indoor semantics, complicated by
sparse, imbalanced object categories and diverse spatial scales. Existing
methods struggle to robustly generate unobserved areas in real time and do not
generalize well to new environments. To this end, we propose \textbf{MapBERT},
a novel framework designed to effectively model the distribution of unseen
spaces. Motivated by the observation that the one-hot encoding of semantic maps
aligns naturally with the binary structure of bit encoding, we, for the first
time, leverage a lookup-free BitVAE to encode semantic maps into compact
bitwise tokens. Building on this, a masked transformer is employed to infer
missing regions and generate complete semantic maps from limited observations.
To enhance object-centric reasoning, we propose an object-aware masking
strategy that masks entire object categories concurrently and pairs them with
learnable embeddings, capturing implicit relationships between object
embeddings and spatial tokens. By learning these relationships, the model more
effectively captures indoor semantic distributions crucial for practical
robotic tasks. Experiments on Gibson benchmarks show that MapBERT achieves
state-of-the-art semantic map generation, balancing computational efficiency
with accurate reconstruction of unobserved regions.

</details>


### [668] [Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs](https://arxiv.org/abs/2506.07454)
*Jared Strader,Aaron Ray,Jacob Arkin,Mason B. Peterson,Yun Chang,Nathan Hughes,Christopher Bradley,Yi Xuan Jia,Carlos Nieto-Granda,Rajat Talak,Chuchu Fan,Luca Carlone,Jonathan P. How,Nicholas Roy*

Main category: cs.RO

TL;DR: This paper develops a multi-robot system leveraging 3D scene graphs for executing complex tasks via natural language instructions, integrating mapping, localization, and planning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enable multi-robot systems to interpret and execute complex instructions in dynamic, large-scale environments using a unified representation, addressing gaps in real-time reasoning and task automation.

Method: The authors propose a shared 3D scene graph system with object-based mapping for real-time relocalization and planning. They implement a planning approach using a Large Language Model (LLM) to convert operator intent into PDDL goals based on the 3D scene graph context.

Result: Experimental evaluations show the system's effectiveness in performing tasks in large, outdoor environments. The integration of mapping, localization, and planning facilitates task execution.

Conclusion: The proposed system demonstrates robust multi-robot collaboration, highlighting the utility of 3D scene graphs and LLM-based planning for interpreting complex natural language instructions in diverse environments.

Abstract: In this paper, we introduce a multi-robot system that integrates mapping,
localization, and task and motion planning (TAMP) enabled by 3D scene graphs to
execute complex instructions expressed in natural language. Our system builds a
shared 3D scene graph incorporating an open-set object-based map, which is
leveraged for multi-robot 3D scene graph fusion. This representation supports
real-time, view-invariant relocalization (via the object-based map) and
planning (via the 3D scene graph), allowing a team of robots to reason about
their surroundings and execute complex tasks. Additionally, we introduce a
planning approach that translates operator intent into Planning Domain
Definition Language (PDDL) goals using a Large Language Model (LLM) by
leveraging context from the shared 3D scene graph and robot capabilities. We
provide an experimental assessment of the performance of our system on
real-world tasks in large-scale, outdoor environments.

</details>


### [669] [RAPID Hand: A Robust, Affordable, Perception-Integrated, Dexterous Manipulation Platform for Generalist Robot Autonomy](https://arxiv.org/abs/2506.07490)
*Zhaoliang Wan,Zetong Bi,Zida Zhou,Hao Ren,Yiming Zeng,Yihan Li,Lu Qi,Xu Yang,Ming-Hsuan Yang,Hui Cheng*

Main category: cs.RO

TL;DR: The paper introduces the RAPID Hand, a low-cost but efficient multi-fingered robot manipulation platform designed for collecting high-quality real-world data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of affordable and capable platforms capable of high-dexterity robot manipulation data collection to advance generalist robot autonomy.

Method: The authors propose RAPID Hand, combining a compact 20-DoF hand, robust perception systems, and a high-DoF teleoperation interface designed via co-optimization of hardware and software.

Result: The evaluation demonstrates RAPID Hand's efficient hardware, perception systems, and teleoperation interface, showing superior performance in training a diffusion policy compared to previous methods.

Conclusion: RAPID Hand proves effective for high-quality data collection, leveraging low-cost components for scalability and reproducibility, encouraging public adoption.

Abstract: This paper addresses the scarcity of low-cost but high-dexterity platforms
for collecting real-world multi-fingered robot manipulation data towards
generalist robot autonomy. To achieve it, we propose the RAPID Hand, a
co-optimized hardware and software platform where the compact 20-DoF hand,
robust whole-hand perception, and high-DoF teleoperation interface are jointly
designed. Specifically, RAPID Hand adopts a compact and practical hand ontology
and a hardware-level perception framework that stably integrates wrist-mounted
vision, fingertip tactile sensing, and proprioception with sub-7 ms latency and
spatial alignment. Collecting high-quality demonstrations on high-DoF hands is
challenging, as existing teleoperation methods struggle with precision and
stability on complex multi-fingered systems. We address this by co-optimizing
hand design, perception integration, and teleoperation interface through a
universal actuation scheme, custom perception electronics, and two retargeting
constraints. We evaluate the platform's hardware, perception, and teleoperation
interface. Training a diffusion policy on collected data shows superior
performance over prior works, validating the system's capability for reliable,
high-quality data collection. The platform is constructed from low-cost and
off-the-shelf components and will be made public to ensure reproducibility and
ease of adoption.

</details>


### [670] [Taking Flight with Dialogue: Enabling Natural Language Control for PX4-based Drone Agent](https://arxiv.org/abs/2506.07509)
*Shoon Kit Lim,Melissa Jia Ying Chong,Jing Huey Khor,Ting Yang Ling*

Main category: cs.RO

TL;DR: The paper introduces an open-source framework for natural language control of autonomous drones, integrating various systems and benchmarking models for command and scene tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of exploration and open access in aerial robot AI, particularly toward democratizing natural language control for UAVs.

Method: Developed an open-source agentic framework combining PX4, ROS 2, and Ollama for use in drones; evaluated it through simulation and custom quadcopter benchmarking of LLMs and VLMs.

Result: The framework was successfully created and tested, providing benchmarks for state-of-the-art language and vision-language models for drone applications.

Conclusion: This work advances democratization of UAV control by establishing accessible tools and benchmarks for future research in aerial robot AI.

Abstract: Recent advances in agentic and physical artificial intelligence (AI) have
largely focused on ground-based platforms such as humanoid and wheeled robots,
leaving aerial robots relatively underexplored. Meanwhile, state-of-the-art
unmanned aerial vehicle (UAV) multimodal vision-language systems typically rely
on closed-source models accessible only to well-resourced organizations. To
democratize natural language control of autonomous drones, we present an
open-source agentic framework that integrates PX4-based flight control, Robot
Operating System 2 (ROS 2) middleware, and locally hosted models using Ollama.
We evaluate performance both in simulation and on a custom quadcopter platform,
benchmarking four large language model (LLM) families for command generation
and three vision-language model (VLM) families for scene understanding.

</details>


### [671] [BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation](https://arxiv.org/abs/2506.07530)
*Hongyu Wang,Chuyan Xiong,Ruiping Wang,Xilin Chen*

Main category: cs.RO

TL;DR: This paper introduces BitVLA, the first 1-bit Vision-Language-Action (VLA) model for robotic manipulation, achieving competitive results with reduced memory usage.


<details>
  <summary>Details</summary>
Motivation: VLA models excel at robotic manipulation tasks but are resource-intensive, making it hard to deploy them on edge devices with limited hardware resources.

Method: The paper employs 1-bit quantization (ternary parameters: {-1, 0, 1}) and introduces a distillation-aware training strategy to compress the vision encoder to 1.58-bit weights, using a teacher model to improve alignment.

Result: BitVLA achieves comparable performance to OpenVLA-OFT on the LIBERO benchmark while using only 29.8% of memory compared to the 4-bit quantized model.

Conclusion: BitVLA demonstrates that models with extreme quantization (1-bit) can achieve high efficiency and performance, making them viable for use on memory-constrained devices.

Abstract: Vision-Language-Action (VLA) models have shown impressive capabilities across
a wide range of robotics manipulation tasks. However, their growing model size
poses significant challenges for deployment on resource-constrained robotic
systems. While 1-bit pretraining has proven effective for enhancing the
inference efficiency of large language models with minimal performance loss,
its application to VLA models remains underexplored. In this work, we present
BitVLA, the first 1-bit VLA model for robotics manipulation, in which every
parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint
of the vision encoder, we propose the distillation-aware training strategy that
compresses the full-precision encoder to 1.58-bit weights. During this process,
a full-precision encoder serves as a teacher model to better align latent
representations. Despite the lack of large-scale robotics pretraining, BitVLA
achieves performance comparable to the state-of-the-art model OpenVLA-OFT with
4-bit post-training quantization on the LIBERO benchmark, while consuming only
29.8% of the memory. These results highlight BitVLA's promise for deployment on
memory-constrained edge devices. We release the code and model weights in
https://github.com/ustcwhy/BitVLA.

</details>


### [672] [Fractional Collisions: A Framework for Risk Estimation of Counterfactual Conflicts using Autonomous Driving Behavior Simulations](https://arxiv.org/abs/2506.07540)
*Sreeja Roy-Singh,Sarvesh Kolekar,Daniel P. Bonny,Kyle Foss*

Main category: cs.RO

TL;DR: The paper introduces a method to estimate collision risks based on simulated scenarios derived from sensor data, demonstrating its effectiveness and the advantage of an ADS in reducing collisions.


<details>
  <summary>Details</summary>
Motivation: To build a reliable way to measure and predict the collision risk and potential damages between agents in driving scenarios, useful for automated driving systems and safety evaluation.

Method: The authors utilize counterfactual analysis of agent behaviors based on sensor data and probabilistic simulations to estimate collision risks and classify types of conflicts.

Result: Validation was done using synthetic environments and crash data. Results reveal the ADS significantly reduced collision risk – decreasing naturalistic collisions fourfold and fractional collision risk by ~62%.

Conclusion: The methodology proved effective for assessing agent-initiated collision risks, demonstrating the ability of ADS to improve safety in various scenarios.

Abstract: We present a methodology for estimating collision risk from counterfactual
simulated scenarios built on sensor data from automated driving systems (ADS)
or naturalistic driving databases. Two-agent conflicts are assessed by
detecting and classifying conflict type, identifying the agents' roles
(initiator or responder), identifying the point of reaction of the responder,
and modeling their human behavioral expectations as probabilistic
counterfactual trajectories. The states are used to compute velocity
differentials at collision, which when combined with crash models, estimates
severity of loss in terms of probabilistic injury or property damage,
henceforth called fractional collisions. The probabilistic models may also be
extended to include other uncertainties associated with the simulation,
features, and agents. We verify the effectiveness of the methodology in a
synthetic simulation environment using reconstructed trajectories from 300+
collision and near-collision scenes sourced from VTTI's SHRP2 database and
Nexar dashboard camera data. Our methodology predicted fractional collisions
within 1% of ground truth collisions. We then evaluate agent-initiated
collision risk of an arbitrary ADS software release by replacing the
naturalistic responder in these synthetic reconstructions with an ADS simulator
and comparing the outcome to human-response outcomes. Our ADS reduced
naturalistic collisions by 4x and fractional collision risk by ~62%. The
framework's utility is also demonstrated on 250k miles of proprietary,
open-loop sensor data collected on ADS test vehicles, re-simulated with an
arbitrary ADS software release. The ADS initiated conflicts that caused 0.4
injury-causing and 1.7 property-damaging fractional collisions, and the ADS
improved collision risk in 96% of the agent-initiated conflicts.

</details>


### [673] [Blending Participatory Design and Artificial Awareness for Trustworthy Autonomous Vehicles](https://arxiv.org/abs/2506.07633)
*Ana Tanevska,Ananthapathmanabhan Ratheesh Kumar,Arabinda Ghosh,Ernesto Casablanca,Ginevra Castellano,Sadegh Soudjani*

Main category: cs.RO

TL;DR: This paper introduces a data-driven model of human drivers to enhance interaction between autonomous vehicles (AVs) and humans, emphasizing the effects of transparency and user demographics.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems need to ensure safe multi-agent collaboration while fostering trust and transparency in interactions with human users.

Method: The authors conducted a large-scale user-centered study on human-AV interaction and derived Markov chain models from the collected data.

Result: The study found significant differences in Markov chain transitions depending on AV transparency, environmental scenarios, and user demographics.

Conclusion: Integrating the developed driver model into situational awareness architecture can enhance autonomous systems' trustworthiness and user experience.

Abstract: Current robotic agents, such as autonomous vehicles (AVs) and drones, need to
deal with uncertain real-world environments with appropriate situational
awareness (SA), risk awareness, coordination, and decision-making. The SymAware
project strives to address this issue by designing an architecture for
artificial awareness in multi-agent systems, enabling safe collaboration of
autonomous vehicles and drones. However, these agents will also need to
interact with human users (drivers, pedestrians, drone operators), which in
turn requires an understanding of how to model the human in the interaction
scenario, and how to foster trust and transparency between the agent and the
human.
  In this work, we aim to create a data-driven model of a human driver to be
integrated into our SA architecture, grounding our research in the principles
of trustworthy human-agent interaction. To collect the data necessary for
creating the model, we conducted a large-scale user-centered study on human-AV
interaction, in which we investigate the interaction between the AV's
transparency and the users' behavior.
  The contributions of this paper are twofold: First, we illustrate in detail
our human-AV study and its findings, and second we present the resulting Markov
chain models of the human driver computed from the study's data. Our results
show that depending on the AV's transparency, the scenario's environment, and
the users' demographics, we can obtain significant differences in the model's
transitions.

</details>


### [674] [Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse](https://arxiv.org/abs/2506.07639)
*Zhekai Duan,Yuan Zhang,Shikai Geng,Gaowen Liu,Joschka Boedecker,Chris Xiaoxuan Lu*

Main category: cs.RO

TL;DR: Fast ECoT accelerates Embodied Chain-of-Thought reasoning, reducing latency by up to 7.5% while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve real-time deployment of vision-language-action (VLA) models by addressing the latency issues caused by sequential autoregressive token generation in Embodied Chain-of-Thought reasoning.

Method: Fast ECoT leverages caching and reuse of high-level reasoning across timesteps, parallelizes modular reasoning steps, and introduces an asynchronous scheduler to separate reasoning from action decoding.

Result: Experiments demonstrate up to a 7.5% latency reduction, maintaining or improving task success rate and reasoning faithfulness in both simulated and real-world environments.

Conclusion: Fast ECoT significantly improves inference speed for ECoT policies, making real-time deployment more practical without requiring model changes or additional training.

Abstract: Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action
(VLA) models by improving performance and interpretability through intermediate
reasoning steps. However, its sequential autoregressive token generation
introduces significant inference latency, limiting real-time deployment. We
propose Fast ECoT, an inference-time acceleration method that exploits the
structured and repetitive nature of ECoT to (1) cache and reuse high-level
reasoning across timesteps and (2) parallelise the generation of modular
reasoning steps. Additionally, we introduce an asynchronous scheduler that
decouples reasoning from action decoding, further boosting responsiveness. Fast
ECoT requires no model changes or additional training and integrates easily
into existing VLA pipelines. Experiments in both simulation (LIBERO) and
real-world robot tasks show up to a 7.5% reduction in latency with comparable
or improved task success rate and reasoning faithfulness, bringing ECoT
policies closer to practical real-time deployment.

</details>


### [675] [A Communication-Latency-Aware Co-Simulation Platform for Safety and Comfort Evaluation of Cloud-Controlled ICVs](https://arxiv.org/abs/2506.07696)
*Yongqi Zhao,Xinrui Zhang,Tomislav Mihalj,Martin Schabauer,Luis Putzer,Erik Reichmann-Blaga,Ádám Boronyák,András Rövid,Gábor Soós,Peizhi Zhang,Lu Xiong,Jia Hu,Arno Eichberger*

Main category: cs.RO

TL;DR: The paper introduces a simulation platform integrating CarMaker and Vissim to evaluate intelligent connected vehicles (ICVs) under real-world communication latency conditions, with focus on assessing safety and comfort.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate simulation environments that emulate both vehicle behavior and realistic communication latencies for testing intelligent connected vehicles (ICVs).

Method: The research integrates CarMaker and Vissim into a latency-aware co-simulation platform, employing empirical 5G latency models characterized by Gamma distributions and a proactive conflict module (PCM) for dynamic scenario generation.

Result: Experiments reveal that the PCM enhances critical driving scenarios, while communication latency notably impacts ride comfort. Multiple testing conditions further validate the platform's capabilities.

Conclusion: The proposed latency-aware platform proves effective for systematic evaluation of safety and comfort in cloud-controlled ICVs across diverse real-world conditions.

Abstract: Testing cloud-controlled intelligent connected vehicles (ICVs) requires
simulation environments that faithfully emulate both vehicle behavior and
realistic communication latencies. This paper proposes a latency-aware
co-simulation platform integrating CarMaker and Vissim to evaluate safety and
comfort under real-world vehicle-to-cloud (V2C) latency conditions. Two
communication latency models, derived from empirical 5G measurements in China
and Hungary, are incorporated and statistically modeled using Gamma
distributions. A proactive conflict module (PCM) is proposed to dynamically
control background vehicles and generate safety-critical scenarios. The
platform is validated through experiments involving an exemplary system under
test (SUT) across six testing conditions combining two PCM modes
(enabled/disabled) and three latency conditions (none, China, Hungary). Safety
and comfort are assessed using metrics including collision rate, distance
headway, post-encroachment time, and the spectral characteristics of
longitudinal acceleration. Results show that the PCM effectively increases
driving environment criticality, while V2C latency primarily affects ride
comfort. These findings confirm the platform's effectiveness in systematically
evaluating cloud-controlled ICVs under diverse testing conditions.

</details>


### [676] [SMaRCSim: Maritime Robotics Simulation Modules](https://arxiv.org/abs/2506.07781)
*Mart Kartašev,David Dörner,Özer Özkahraman,Petter Ögren,Ivan Stenius,John Folkesson*

Main category: cs.RO

TL;DR: This paper introduces SMaRCSim, a simulation tool designed to provide new capabilities for testing and developing underwater robots, addressing key limitations in current tools.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the challenges associated with developing and testing underwater robots, which involve significant time and resources, and address limitations in current simulation tools.

Method: The authors have developed SMaRCSim, a set of simulation packages providing features for learning-based methods, autonomous multi-vehicle coordination, and integration with mission planning in underwater robotics.

Result: SMaRCSim enables advancements in underwater robotics simulation, fulfilling needs for learning-based methods, coordinated multi-vehicle operations, and experimental mission integration.

Conclusion: With SMaRCSim, researchers can explore innovative functionalities for underwater robotics, enhancing efficiency and experimentation before real-world deployment.

Abstract: Developing new functionality for underwater robots and testing them in the
real world is time-consuming and resource-intensive. Simulation environments
allow for rapid testing before field deployment. However, existing tools lack
certain functionality for use cases in our project: i) developing
learning-based methods for underwater vehicles; ii) creating teams of
autonomous underwater, surface, and aerial vehicles; iii) integrating the
simulation with mission planning for field experiments. A holistic solution to
these problems presents great potential for bringing novel functionality into
the underwater domain. In this paper we present SMaRCSim, a set of simulation
packages that we have developed to help us address these issues.

</details>


### [677] [Primal-Dual iLQR for GPU-Accelerated Learning and Control in Legged Robots](https://arxiv.org/abs/2506.07823)
*Lorenzo Amatucci,João Sousa-Pinto,Giulio Turrisi,Dominique Orban,Victor Barasuol,Claudio Semini*

Main category: cs.RO

TL;DR: The paper presents a GPU-parallelized Model Predictive Control (MPC) approach for legged robot locomotion, offering substantially reduced complexity and runtime improvements compared to existing solvers.


<details>
  <summary>Details</summary>
Motivation: Addressing computational inefficiencies in solving optimal control problems for legged robot locomotion with standardized solvers.

Method: A GPU-enabled MPC implementation using parallel associative scans to solve the Karush-Kuhn-Tucker system, achieving improved computational complexity and scalability.

Result: The proposed MPC demonstrates up to 60% runtime improvement for Whole Body Dynamics and 700% for Single Rigid Body Dynamics compared to state-of-the-art solutions. It supports centralized control for multiple robots and enables large-scale parallelization for learning.

Conclusion: Successfully leverages GPU parallelization to optimize MPC for complex robotic locomotion tasks, showing scalability and efficiency gains applicable to multi-robot and learning environments.

Abstract: This paper introduces a novel Model Predictive Control (MPC) implementation
for legged robot locomotion that leverages GPU parallelization. Our approach
enables both temporal and state-space parallelization by incorporating a
parallel associative scan to solve the primal-dual Karush-Kuhn-Tucker (KKT)
system. In this way, the optimal control problem is solved in
$\mathcal{O}(n\log{N} + m)$ complexity, instead of $\mathcal{O}(N(n + m)^3)$,
where $n$, $m$, and $N$ are the dimension of the system state, control vector,
and the length of the prediction horizon. We demonstrate the advantages of this
implementation over two state-of-the-art solvers (acados and crocoddyl),
achieving up to a 60\% improvement in runtime for Whole Body Dynamics (WB)-MPC
and a 700\% improvement for Single Rigid Body Dynamics (SRBD)-MPC when varying
the prediction horizon length. The presented formulation scales efficiently
with the problem state dimensions as well, enabling the definition of a
centralized controller for up to 16 legged robots that can be computed in less
than 25 ms. Furthermore, thanks to the JAX implementation, the solver supports
large-scale parallelization across multiple environments, allowing the
possibility of performing learning with the MPC in the loop directly in GPU.

</details>


### [678] [Versatile Loco-Manipulation through Flexible Interlimb Coordination](https://arxiv.org/abs/2506.07876)
*Xinghao Zhu,Yuxin Chen,Lingfeng Sun,Farzad Niroui,Simon Le CleacH,Jiuguang Wang,Kuan Fang*

Main category: cs.RO

TL;DR: ReLIC introduces an adaptive controller for interlimb coordination enabling versatile loco-manipulation in robots for unstructured environments.


<details>
  <summary>Details</summary>
Motivation: Autonomous robots often struggle with effective loco-manipulation in unstructured settings due to being constrained by task specificity or static limb configurations.

Method: ReLIC employs reinforcement learning to design an adaptive controller featuring interplay between modules for manipulation and locomotion tasks, making dynamic interlimb coordination possible.

Result: ReLIC achieved 78.9% average success rate across 12 diverse real-world tasks, demonstrating its effectiveness in managing complex coordination patterns.

Conclusion: The approach showcases promising adaptability and robustness, enabling robots to seamlessly switch between manipulating and moving limbs based on task requirements within challenging environments.

Abstract: The ability to flexibly leverage limbs for loco-manipulation is essential for
enabling autonomous robots to operate in unstructured environments. Yet, prior
work on loco-manipulation is often constrained to specific tasks or
predetermined limb configurations. In this work, we present Reinforcement
Learning for Interlimb Coordination (ReLIC), an approach that enables versatile
loco-manipulation through flexible interlimb coordination. The key to our
approach is an adaptive controller that seamlessly bridges the execution of
manipulation motions and the generation of stable gaits based on task demands.
Through the interplay between two controller modules, ReLIC dynamically assigns
each limb for manipulation or locomotion and robustly coordinates them to
achieve the task success. Using efficient reinforcement learning in simulation,
ReLIC learns to perform stable gaits in accordance with the manipulation goals
in the real world. To solve diverse and complex tasks, we further propose to
interface the learned controller with different types of task specifications,
including target trajectories, contact points, and natural language
instructions. Evaluated on 12 real-world tasks that require diverse and complex
coordination patterns, ReLIC demonstrates its versatility and robustness by
achieving a success rate of 78.9% on average. Videos and code can be found at
https://relic-locoman.github.io/.

</details>


### [679] [Design and Implementation of a Peer-to-Peer Communication, Modular and Decentral YellowCube UUV](https://arxiv.org/abs/2506.07924)
*Zhizun Xu,Baozhu Jia,Weichao Shi*

Main category: cs.RO

TL;DR: The paper introduces "YellowCube," a modular and decentralized UUV with a peer-to-peer communication mechanism, designed for adaptability and ease of integrating various sensors.


<details>
  <summary>Details</summary>
Motivation: Many current UUVs lack the flexibility for easy integration of new or upgraded sensors, hindering their applicability for varied missions.

Method: The authors designed a modular, decentralized UUV named YellowCube, featuring a Peer-to-Peer communication system among its modules to replace conventional centralized architectures.

Result: Experiments in both laboratory environments and sea trials demonstrate the performance and capabilities of the YellowCube UUV.

Conclusion: The modularity and decentralized system of the YellowCube make it a promising tool for versatile underwater missions, addressing the limitations of existing designs.

Abstract: The underwater Unmanned Vehicles(UUVs) are pivot tools for offshore
engineering and oceanographic research. Most existing UUVs do not facilitate
easy integration of new or upgraded sensors. A solution to this problem is to
have a modular UUV system with changeable payload sections capable of carrying
different sensor to suite different missions. The design and implementation of
a modular and decentral UUV named YellowCube is presented in the paper. Instead
a centralised software architecture which is adopted by the other modular
underwater vehicles designs, a Peer-To-Peer(P2P) communication mechanism is
implemented among the UUV's modules. The experiments in the laboratory and sea
trials have been executed to verify the performances of the UUV.

</details>


### [680] [BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models](https://arxiv.org/abs/2506.07961)
*Peiyan Li,Yixiang Chen,Hongtao Wu,Xiao Ma,Xiangnan Wu,Yan Huang,Liang Wang,Tao Kong,Tieniu Tan*

Main category: cs.RO

TL;DR: BridgeVLA introduces a novel approach to vision-language-action (VLA) models by leveraging pre-trained vision-language models (VLMs) for 3D robot manipulation tasks, achieving notable improvements in efficiency and success rates.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations in current VLA models, particularly the inability to fully leverage 3D structural data which results in low sample efficiency.

Method: BridgeVLA processes 3D inputs via 2D projections, aligns them with the VLM backbone, and uses 2D heatmaps for action prediction. A scalable pre-training method is introduced, equipping VLMs with heatmap prediction capabilities prior to policy learning.

Result: BridgeVLA improves success rates across multiple benchmarks and tasks: RLBench (81.4% to 88.2%), COLOSSEUM (56.7% to 64.0%), and GemBench with surpassing results compared to baselines. In real-robot experiments, it achieves a success rate of 96.8% with exceptional sample efficiency.

Conclusion: BridgeVLA demonstrates significant potential for efficient 3D robot manipulation using VLMs, outperforming existing methods and showing robust generalization even in challenging conditions.

Abstract: Recently, leveraging pre-trained vision-language models (VLMs) for building
vision-language-action (VLA) models has emerged as a promising approach to
effective robot manipulation learning. However, only few methods incorporate 3D
signals into VLMs for action prediction, and they do not fully leverage the
spatial structure inherent in 3D data, leading to low sample efficiency. In
this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D
inputs to multiple 2D images, ensuring input alignment with the VLM backbone,
and (2) utilizes 2D heatmaps for action prediction, unifying the input and
output spaces within a consistent 2D image space. In addition, we propose a
scalable pre-training method that equips the VLM backbone with the capability
to predict 2D heatmaps before downstream policy learning. Extensive experiments
show the proposed method is able to learn 3D manipulation efficiently and
effectively. BridgeVLA outperforms state-of-the-art baseline methods across
three simulation benchmarks. In RLBench, it improves the average success rate
from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better
performance in challenging generalization settings, boosting the average
success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing
baseline methods in terms of average success rate. In real-robot experiments,
BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It
generalizes robustly in multiple out-of-distribution settings, including visual
disturbances and unseen instructions. Remarkably, it is able to achieve a
success rate of 96.8% on 10+ tasks with only 3 trajectories per task,
highlighting its extraordinary sample efficiency. Project
Website:https://bridgevla.github.io/

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [681] [Enhancing Software Supply Chain Security Through STRIDE-Based Threat Modelling of CI/CD Pipelines](https://arxiv.org/abs/2506.06478)
*Sowmiya Dhandapani*

Main category: cs.SE

TL;DR: The paper provides an approach for securing Continuous Integration/Continuous Deployment (CI/CD) pipelines by employing systematic threat modeling and security control strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address critical challenges faced by modern DevOps teams in securing software supply chains within CI/CD pipelines against evolving threats.

Method: The study utilized a structured threat modeling approach, applied the STRIDE framework to analyze vulnerabilities, and mapped them to security controls based on standards like NIST SP 800-218, OWASP Top 10 CI/CD risks, and SLSA framework.

Result: A practical security toolchain integration strategy was developed that focuses on automated, enforcable security controls throughout the CI/CD lifecycle based on Security as Code and Shift Left-Shield Right principles.

Conclusion: This paper offers a pragmatic roadmap for improving CI/CD pipeline security by integrating structured threat modeling and actionable security strategies into the DevOps workflow.

Abstract: With the increasing adoption of Continuous Integration and Continuous
Deployment pipelines, securing software supply chains has become a critical
challenge for modern DevOps teams. This study addresses these challenges by
applying a structured threat modeling approach to identify and mitigate risks
throughout the CI/CD lifecycle. By modeling a representative pipeline
architecture incorporating tools such as GitHub, Jenkins, Docker, and
Kubernetes and applying the STRIDE framework, we systematically analyze
vulnerabilities at each stage, from source code management to deployment.
Threats are documented and mapped to comprehensive security controls drawn from
standards like NIST SP 800-218, OWASP Top 10 CI/CD risks, and the SLSA
framework. Controls are further evaluated against SLSA maturity levels to
assess improvements in trust and provenance. To operationalize these findings,
the study outlines a practical security toolchain integration strategy grounded
in Security as Code and Shift Left-Shield Right principles, enabling automated,
enforceable security across the pipeline. This approach provides a pragmatic
roadmap for enhancing CI/CD pipeline security against evolving software supply
chain threats.

</details>


### [682] [Information-Theoretic Detection of Unusual Source Code Changes](https://arxiv.org/abs/2506.06508)
*Adriano Torres,Sebastian Baltes,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: The paper explores the evolution of open-source software projects using information-theoretic metrics like entropy of tokens and abstract syntax tree nodes, offering new insights into code complexity and effective anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To better understand the evolution of software projects and measure their complexity using information-theoretic metrics, rather than only traditional complexity metrics.

Method: The authors analyzed the entropy of tokens and abstract syntax tree nodes from 95 open-source projects. They compared these entropy measures to classical complexity metrics and performed entropy-based anomaly detection.

Result: The study found that entropy captures different dimensions of code complexity compared to traditional metrics. Additionally, entropy-based anomaly detection achieved over 60% precision in identifying unusual code changes.

Conclusion: Information-theoretic metrics like entropy provide valuable insights into the evolution and complexity of source code. This work paves the way for improved methods to statically measure program complexity and detect anomalies.

Abstract: The code base of software projects evolves essentially through inserting and
removing information to and from the source code. We can measure this evolution
via the elements of information - tokens, words, nodes - of the respective
representation of the code. In this work, we approach the measurement of the
information content of the source code of open-source projects from an
information-theoretic standpoint. Our focus is on the entropy of two
fundamental representations of code: tokens and abstract syntax tree nodes,
from which we derive definitions of textual and structural entropy. We proceed
with an empirical assessment where we evaluate the evolution patterns of the
entropy of 95 actively maintained open source projects. We calculate the
statistical relationships between our derived entropy metrics and classic
methods of measuring code complexity and learn that entropy may capture
different dimensions of complexity than classic metrics. Finally, we conduct
entropy-based anomaly detection of unusual changes to demonstrate that our
approach may effectively recognise unusual source code change events with over
60% precision, and lay the groundwork for improvements to information-theoretic
measurement of source code evolution, thus paving the way for a new approach to
statically gauging program complexity throughout its development.

</details>


### [683] [Private GPTs for LLM-driven testing in software development and machine learning](https://arxiv.org/abs/2506.06509)
*Jakub Jagielski,Markus Abel*

Main category: cs.SE

TL;DR: The paper evaluates the use of private GPTs to generate executable test code from requirements using acceptance criteria as input. A two-step process involving Gherkin syntax results in better code quality.


<details>
  <summary>Details</summary>
Motivation: To explore the capability of private GPTs in automating the generation of executable test code directly from requirements, improving the development process.

Method: Two approaches are compared: i) generating code directly from requirements, and ii) a two-step process involving Gherkin syntax for enhanced readability and coding practices. The evaluation is conducted using two cases: a "Hello World" program and a digit classification model.

Result: The two-step process involving Gherkin syntax outperforms the direct approach, producing more readable and better-practiced test code.

Conclusion: Structured prompts and a two-step process enhance the quality of test code generated from requirements using private GPTs, providing practical tools for product owners and developers.

Abstract: In this contribution, we examine the capability of private GPTs to
automatically generate executable test code based on requirements. More
specifically, we use acceptance criteria as input, formulated as part of epics,
or stories, which are typically used in modern development processes. This
gives product owners, or business intelligence, respectively, a way to directly
produce testable criteria through the use of LLMs. We explore the quality of
the so-produced tests in two ways: i) directly by letting the LLM generate code
from requirements, ii) through an intermediate step using Gherkin syntax. As a
result, it turns out that the two-step procedure yields better results -where
we define better in terms of human readability and best coding practices, i.e.
lines of code and use of additional libraries typically used in testing.
Concretely, we evaluate prompt effectiveness across two scenarios: a simple
"Hello World" program and a digit classification model, showing that structured
prompts lead to higher-quality test outputs.

</details>


### [684] [Mind the Gap: A Readability-Aware Metric for Test Code Complexity](https://arxiv.org/abs/2506.06764)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: This paper introduces CCTR, a new metric for evaluating the cognitive complexity of automatically generated unit tests.


<details>
  <summary>Details</summary>
Motivation: Existing complexity metrics, such as SonarSource's Cognitive Complexity, fail to capture meaningful structural and semantic features unique to unit tests, especially when evaluating tests generated by tools like EvoSuite and LLMs.

Method: The authors designed CCTR—a Test-Aware Cognitive Complexity metric—incorporating aspects like assertion density, annotation roles, and test composition patterns. They evaluated 15,750 test suites generated by EvoSuite, GPT-4o, and Mistral Large-1024 across 350 classes.

Result: CCTR produced interpretable scores that distinguished between structured and fragmented test suites more effectively than traditional complexity metrics. Its scores better align with developer-perceived effort.

Conclusion: CCTR offers a reliable approach to evaluating and improving automatically generated unit tests, bridging a gap between structural analysis and test code readability.

Abstract: Automatically generated unit tests-from search-based tools like EvoSuite or
LLMs-vary significantly in structure and readability. Yet most evaluations rely
on metrics like Cyclomatic Complexity and Cognitive Complexity, designed for
functional code rather than test code. Recent studies have shown that
SonarSource's Cognitive Complexity metric assigns near-zero scores to
LLM-generated tests, yet its behavior on EvoSuite-generated tests and its
applicability to test-specific code structures remain unexplored. We introduce
CCTR, a Test-Aware Cognitive Complexity metric tailored for unit tests. CCTR
integrates structural and semantic features like assertion density, annotation
roles, and test composition patterns-dimensions ignored by traditional
complexity models but critical for understanding test code. We evaluate 15,750
test suites generated by EvoSuite, GPT-4o, and Mistral Large-1024 across 350
classes from Defects4J and SF110. Results show CCTR effectively discriminates
between structured and fragmented test suites, producing interpretable scores
that better reflect developer-perceived effort. By bridging structural analysis
and test readability, CCTR provides a foundation for more reliable evaluation
and improvement of generated tests. We publicly release all data, prompts, and
evaluation scripts to support replication.

</details>


### [685] [Beyond Surface Similarity: Evaluating LLM-Based Test Refactorings with Structural and Semantic Awareness](https://arxiv.org/abs/2506.06767)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: The paper introduces CTSES, a composite metric for evaluating LLM-refactored unit test quality, balancing functional behavior, readability, and modularity.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLM-refactored unit tests is difficult with existing metrics like CodeBLEU, which overly emphasize renaming and structural edits.

Method: CTSES combines CodeBLEU, METEOR, and ROUGE-L to provide a composite metric that balances behavior preservation, lexical quality, and structural alignment.

Result: CTSES was tested on 5,000 unit test suites refactored by GPT-4o and Mistral-Large-2407 on Java benchmarks Defects4J and SF110, demonstrating alignment with developer expectations.

Conclusion: CTSES offers a more reliable, interpretable, and intuitive evaluation for refactored unit tests compared to traditional or semantic-focused metrics.

Abstract: Large Language Models (LLMs) are increasingly employed to automatically
refactor unit tests, aiming to enhance readability, naming, and structural
clarity while preserving functional behavior. However, evaluating such
refactorings remains challenging: traditional metrics like CodeBLEU are overly
sensitive to renaming and structural edits, whereas embedding-based
similarities capture semantics but ignore readability and modularity. We
introduce CTSES, a composite metric that integrates CodeBLEU, METEOR, and
ROUGE-L to balance behavior preservation, lexical quality, and structural
alignment. CTSES is evaluated on over 5,000 test suites automatically
refactored by GPT-4o and Mistral-Large-2407, using Chain-of-Thought prompting,
across two established Java benchmarks: Defects4J and SF110. Our results show
that CTSES yields more faithful and interpretable assessments, better aligned
with developer expectations and human intuition than existing metrics.

</details>


### [686] [Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain](https://arxiv.org/abs/2506.06946)
*Daniel Lawand,Lucas Quaresma,Roberto Bolgheroni,Alfredo Goldman,Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: The paper explores challenges in deploying machine learning training pipelines into production and describes the architectural evolution within the SPIRA project.


<details>
  <summary>Details</summary>
Motivation: To address the lack of software quality attributes in traditional ML training pipelines and provide insights for productionizing ML systems.

Method: The authors compare three versions of SPIRA's Continuous Training subsystem architecture, transitioning from a Big Ball of Mud to Modular Monolith and finally Microservices.

Result: By adopting different design principles and patterns, the project achieved improved maintainability, robustness, and extensibility in its training pipeline.

Conclusion: The paper highlights the importance of robust software engineering practices in ML pipelines and provides lessons for ML Engineers and Data Scientists in implementing MLOps practices.

Abstract: Deploying a Machine Learning (ML) training pipeline into production requires
robust software engineering practices. This differs significantly from
experimental workflows. This experience report investigates this challenge in
SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to
pre-diagnose insufficiency respiratory via speech analysis. The first version
of SPIRA's training pipeline lacked critical software quality attributes. This
paper presents an overview of the MLES, then compares three versions of the
architecture of the Continuous Training subsystem, which evolved from a Big
Ball of Mud, to a Modular Monolith, towards Microservices. By adopting
different design principles and patterns to enhance its maintainability,
robustness, and extensibility. In this way, the paper seeks to offer insights
for both ML Engineers tasked to productionize ML training pipelines and Data
Scientists seeking to adopt MLOps practices.

</details>


### [687] [Taxonomy of migration scenarios for Qiskit refactoring using LLMs](https://arxiv.org/abs/2506.07135)
*José Manuel Suárez,Luís Mariano Bibbó,Joaquín Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: This paper creates a taxonomy of refactoring challenges in quantum programming, using expert knowledge and LLMs, to ease migrations between different Qiskit versions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges arising from the frequent evolution of quantum programming libraries, which necessitate complex code refactoring.

Method: The study analyzes Qiskit documentation and release notes to develop initial taxonomies of refactoring needs, created by both expert developers and Large Language Models (LLMs), and integrates them into a unified taxonomy.

Result: A unified taxonomy was created to categorize refactoring challenges more effectively by combining insights from both expert developers and LLMs.

Conclusion: The unified taxonomy forms a foundation for future research on AI-assisted migration, improves workflows in quantum software engineering, and promotes best practices in quantum development.

Abstract: As quantum computing advances, quantum programming libraries' heterogeneity
and steady evolution create new challenges for software developers. Frequent
updates in software libraries break working code that needs to be refactored,
thus adding complexity to an already complex landscape. These refactoring
challenges are, in many cases, fundamentally different from those known in
classical software engineering due to the nature of quantum computing software.
This study addresses these challenges by developing a taxonomy of quantum
circuit's refactoring problems, providing a structured framework to analyze and
compare different refactoring approaches. Large Language Models (LLMs) have
proven valuable tools for classic software development, yet their value in
quantum software engineering remains unexplored. This study uses LLMs to
categorize refactoring needs in migration scenarios between different Qiskit
versions. Qiskit documentation and release notes were scrutinized to create an
initial taxonomy of refactoring required for migrating between Qiskit releases.
Two taxonomies were produced: one by expert developers and one by an LLM. These
taxonomies were compared, analyzing differences and similarities, and were
integrated into a unified taxonomy that reflects the findings of both methods.
By systematically categorizing refactoring challenges in Qiskit, the unified
taxonomy is a foundation for future research on AI-assisted migration while
enabling a more rigorous evaluation of automated refactoring techniques.
Additionally, this work contributes to quantum software engineering (QSE) by
enhancing software development workflows, improving language compatibility, and
promoting best practices in quantum programming.

</details>


### [688] [GUIPilot: A Consistency-based Mobile GUI Testing Approach for Detecting Application-specific Bugs](https://arxiv.org/abs/2506.07385)
*Ruofan Liu,Xiwen Teoh,Yun Lin,Guanjie Chen,Ruofei Ren,Denys Poshyvanyk,Jin Song Dong*

Main category: cs.SE

TL;DR: GUIPilot detects inconsistencies in mobile app designs and implementations, achieving high precision and recall in identifying widget and screen mismatches.


<details>
  <summary>Details</summary>
Motivation: Mobile applications often face issues where implemented screens and processes fail to match their initial designs, making inconsistencies detection crucial.

Method: GUIPilot abstracts screens into widget containers and uses an alignment algorithm for widget matching. It utilizes visual prompts with vision-language models to infer transitions and actions.

Result: GUIPilot achieved 94.5% precision and 99.6% recall for screen inconsistencies, outperformed existing tools like GVT, and detected nine confirmed bugs in a trading app.

Conclusion: GUIPilot is highly effective in discovering and validating design-to-implementation inconsistencies, demonstrating value for real-world application testing.

Abstract: In this work, we propose GUIPilot, an approach for detecting inconsistencies
between the mobile design and their implementations. The mobile design usually
consists of design mock-ups that specify (1) the expected screen appearances
(e.g., widget layouts, colors, and shapes) and (2) the expected screen
behaviors, regarding how one screen can transition into another (e.g., labeled
widgets with textual description). Given a design mock-up and the
implementation of its application, GUIPilot reports both their screen
inconsistencies as well as process inconsistencies. On the one hand, GUIPilot
detects the screen inconsistencies by abstracting every screen into a widget
container where each widget is represented by its position, width, height, and
type. By defining the partial order of widgets and the costs of replacing,
inserting, and deleting widgets in a screen, we convert the screen-matching
problem into an optimizable widget alignment problem. On the other hand, we
translate the specified GUI transition into stepwise actions on the mobile
screen (e.g., click, long-press, input text on some widgets). To this end, we
propose a visual prompt for the vision-language model to infer widget-specific
actions on the screen. By this means, we can validate the presence or absence
of expected transitions in the implementation. Our extensive experiments on 80
mobile applications and 160 design mock-ups show that (1) GUIPilot can achieve
94.5% precision and 99.6% recall in detecting screen inconsistencies,
outperforming the state-of-the-art approach, such as GVT, by 66.2% and 56.6%
respectively, and (2) GUIPilot reports zero errors in detecting process
inconsistencies. Furthermore, our industrial case study on applying GUIPilot on
a trading mobile application shows that GUIPilot has detected nine application
bugs, and all the bugs were confirmed by the original application experts.

</details>


### [689] [A Framework for Creating Non-Regressive Test Cases via Branch Consistency Analysis Driven by Descriptions](https://arxiv.org/abs/2506.07486)
*Yuxiang Zhang,Pengyu Xue,Zhen Yang,Xiaoxue Ren,Xiang Li,Linhao Wu,Jiancheng Zhao,Xingda Yu*

Main category: cs.SE

TL;DR: The paper introduces DISTINCT, a Description-guided framework that improves fault-aware test generation by enhancing defect detection and code coverage, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Most current test generation tools assume the correctness of the focal methods they test, but practitioners often work in scenarios where these methods can be defective. Existing tools fail to expose defects despite achieving good code coverage.

Method: The researchers construct two benchmarks, Defects4J-Desc and QuixBugs-Desc, with natural language descriptions (NLDs) added for test guidance. They propose DISTINCT, a framework comprising three components: a Generator guided by NLDs, a Validator fixing compilation issues, and an Analyzer for semantic branch-level alignment.

Result: DISTINCT achieves significant improvements in Compilation Success Rate (14.64%), Passing Rate (6.66%), and, most notably, Defect Detection Rate (149.26% on Defects4J-Desc). It also improves code coverage with a 3.77% gain in Statement Coverage and 5.36% in Branch Coverage.

Conclusion: DISTINCT sets a new standard for non-regressive test generation, demonstrating that natural language descriptions can drive LLMs to surpass mere code coverage, focusing on effective defect detection.

Abstract: Automated test-generation research overwhelmingly assumes the correctness of
focal methods, yet practitioners routinely face non-regression scenarios where
the focal method may be defective. A baseline evaluation of EvoSuite and two
leading Large Language Model (LLM)-based generators, namely ChatTester and
ChatUniTest, on defective focal methods reveals that despite achieving up to
83% of branch coverage, none of the generated tests expose defects.
  To resolve this problem, we first construct two new benchmarks, namely
Defects4J-Desc and QuixBugs-Desc, for experiments. In particular, each focal
method is equipped with an extra Natural Language Description (NLD) for code
functionality understanding.
  Subsequently, we propose DISTINCT, a Description-guided, branch-consistency
analysis framework that transforms LLMs into fault-aware test generators.
DISTINCT carries three iterative components: (1) a Generator that derives
initial tests based on the NLDs and the focal method, (2) a Validator that
iteratively fixes uncompilable tests using compiler diagnostics, and (3) an
Analyzer that iteratively aligns test behavior with NLD semantics via
branch-level analysis.
  Extensive experiments confirm the effectiveness of our approach. Compared to
state-of-the-art methods, DISTINCT achieves an average improvement of 14.64% in
Compilation Success Rate (CSR) and 6.66% in Passing Rate (PR) across both
benchmarks. It notably enhances Defect Detection Rate (DDR) on both benchmarks,
with a particularly significant gain of 149.26% observed on Defects4J-Desc. In
terms of code coverage, DISTINCT improves Statement Coverage (SC) by an average
of 3.77% and Branch Coverage (BC) by 5.36%. These results set a new baseline
for non-regressive test generation and highlight how description-driven
reasoning enables LLMs to move beyond coverage chasing toward effective defect
detection.

</details>


### [690] [Large Language Models for Multilingual Vulnerability Detection: How Far Are We?](https://arxiv.org/abs/2506.07503)
*Honglin Shu,Michael Fu,Junji Yu,Dong Wang,Chakkrit Tantithamthavorn,Junjie Chen,Yasutaka Kamei*

Main category: cs.SE

TL;DR: The paper conducts an empirical study comparing PLMs and LLMs for multilingual vulnerability detection, finding GPT-4o to be highly effective in detecting vulnerabilities across multiple programming languages and granularity levels.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the lack of understanding about the strengths and weaknesses of PLMs and LLMs in scenarios involving multiple programming languages and granularity levels in vulnerability detection.

Method: Researchers systematically assessed PLMs and LLMs using about 30,000 vulnerability-fixing patches from seven programming languages. They evaluated models at function-level and line-level performance.

Result: GPT-4o, enhanced via instruction tuning and few-shot prompting, showed superior performance over all evaluated models, including CodeT5P, particularly in detecting severe multilingual vulnerabilities.

Conclusion: LLMs, especially GPT-4o, offer significant potential for multilingual vulnerability detection by outperforming traditional PLM approaches and addressing critical software security challenges effectively.

Abstract: Various deep learning-based approaches utilizing pre-trained language models
(PLMs) have been proposed for automated vulnerability detection. With recent
advancements in large language models (LLMs), several studies have begun
exploring their application to vulnerability detection tasks. However, existing
studies primarily focus on specific programming languages (e.g., C/C++) and
function-level detection, leaving the strengths and weaknesses of PLMs and LLMs
in multilingual and multi-granularity scenarios largely unexplored. To bridge
this gap, we conduct a comprehensive fine-grained empirical study evaluating
the effectiveness of state-of-the-art PLMs and LLMs for multilingual
vulnerability detection. Using over 30,000 real-world vulnerability-fixing
patches across seven programming languages, we systematically assess model
performance at both the function-level and line-level. Our key findings
indicate that GPT-4o, enhanced through instruction tuning and few-shot
prompting, significantly outperforms all other evaluated models, including
CodeT5P. Furthermore, the LLM-based approach demonstrates superior capability
in detecting unique multilingual vulnerabilities, particularly excelling in
identifying the most dangerous and high-severity vulnerabilities. These results
underscore the promising potential of adopting LLMs for multilingual
vulnerability detection at function-level and line-level, revealing their
complementary strengths and substantial improvements over PLM approaches. This
first empirical evaluation of PLMs and LLMs for multilingual vulnerability
detection highlights LLMs' value in addressing real-world software security
challenges.

</details>


### [691] [IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents](https://arxiv.org/abs/2506.07524)
*Shiwei Feng,Xiangzhe Xu,Xuan Chen,Kaiyuan Zhang,Syed Yusuf Ahmed,Zian Su,Mingwei Zheng,Xiangyu Zhang*

Main category: cs.SE

TL;DR: IntenTest is a framework for identifying intent integrity issues in LLM agents that invoke APIs, generating realistic tasks with targeted mutations to uncover errors.


<details>
  <summary>Details</summary>
Motivation: LLM agents often misinterpret natural language user instructions, leading to goal divergence, especially as APIs evolve. Current software testing methods fail to address the ambiguity of natural language inputs.

Method: IntenTest systematically tests LLM agents by generating realistic tasks from API documentation, applying semantic partitioning for categorization, targeted task mutations, and leveraging datatype-aware strategy memory to enhance test efficacy.

Result: Experiments on 80 APIs show IntenTest significantly outperforms existing baselines in error-exposing rate and query efficiency. Additionally, it adapts to stronger LLMs and evolving APIs across various domains.

Conclusion: IntenTest robustly identifies natural language intent violations, improving tool robustness and generalizing across models and API updates more effectively than existing techniques.

Abstract: LLM agents are increasingly deployed to automate real-world tasks by invoking
APIs through natural language instructions. While powerful, they often suffer
from misinterpretation of user intent, leading to the agent's actions that
diverge from the user's intended goal, especially as external toolkits evolve.
Traditional software testing assumes structured inputs and thus falls short in
handling the ambiguity of natural language. We introduce IntenTest, an
API-centric stress testing framework that systematically uncovers intent
integrity violations in LLM agents. Unlike prior work focused on fixed
benchmarks or adversarial inputs, IntenTest generates realistic tasks based on
toolkits' documentation and applies targeted mutations to expose subtle agent
errors while preserving user intent. To guide testing, we propose semantic
partitioning, which organizes natural language tasks into meaningful categories
based on toolkit API parameters and their equivalence classes. Within each
partition, seed tasks are mutated and ranked by a lightweight predictor that
estimates the likelihood of triggering agent errors. To enhance efficiency,
IntenTest maintains a datatype-aware strategy memory that retrieves and adapts
effective mutation patterns from past cases. Experiments on 80 toolkit APIs
demonstrate that IntenTest effectively uncovers intent integrity violations,
significantly outperforming baselines in both error-exposing rate and query
efficiency. Moreover, IntenTest generalizes well to stronger target models
using smaller LLMs for test generation, and adapts to evolving APIs across
domains.

</details>


### [692] [Evaluating LLMs Effectiveness in Detecting and Correcting Test Smells: An Empirical Study](https://arxiv.org/abs/2506.07594)
*E. G. Santana Jr,Jander Pereira Santos Junior,Erlon P. Almeida,Iftekhar Ahmed,Paulo Anselmo da Mota Silveira Neto,Eduardo Santana de Almeida*

Main category: cs.SE

TL;DR: This paper explores how Large Language Models (LLMs) can detect and refactor test smells in Python and Java test suites, identifying Gemini-1.5 Pro as the best performer among tested models.


<details>
  <summary>Details</summary>
Motivation: Developers struggle with preventing or refactoring test smells that reduce maintainability and reliability. Existing tools primarily focus on smell detection, leaving a gap in automated refactoring capabilities.

Method: The authors evaluated three LLMs—GPT-4-Turbo, LLaMA 3 70B, and Gemini-1.5 Pro—on their ability to detect and refactor test smells in Python and Java test suites. PyNose and TsDetect tools were used for initial detection, followed by LLM-driven refactoring.

Result: Gemini achieved the highest detection accuracy (74.35% in Python, 80.32% in Java). Refactoring effectiveness varied across models, with Gemini also improving test coverage, while GPT-4 and LLaMA often reduced coverage and introduced new smells.

Conclusion: LLMs, especially Gemini-1.5 Pro, show promise for automated test smell refactoring, but challenges like maintaining language consistency and avoiding the introduction of new issues remain.

Abstract: Test smells indicate poor development practices in test code, reducing
maintainability and reliability. While developers often struggle to prevent or
refactor these issues, existing tools focus primarily on detection rather than
automated refactoring. Large Language Models (LLMs) have shown strong potential
in code understanding and transformation, but their ability to both identify
and refactor test smells remains underexplored. We evaluated GPT-4-Turbo, LLaMA
3 70B, and Gemini-1.5 Pro on Python and Java test suites, using PyNose and
TsDetect for initial smell detection, followed by LLM-driven refactoring.
Gemini achieved the highest detection accuracy (74.35\% Python, 80.32\% Java),
while LLaMA was lowest. All models could refactor smells, but effectiveness
varied, sometimes introducing new smells. Gemini also improved test coverage,
unlike GPT-4 and LLaMA, which often reduced it. These results highlight LLMs'
potential for automated test smell refactoring, with Gemini as the strongest
performer, though challenges remain across languages and smell types.

</details>


### [693] [Leveraging Network Methods for Hub-like Microservice Detection](https://arxiv.org/abs/2506.07683)
*Alexander Bakhtin,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: The paper addresses the Hub-like anti-pattern in microservice architectures by comparing detection methods and proposing improvements.


<details>
  <summary>Details</summary>
Motivation: To solve the lack of clarity and precision in defining and detecting the Hub-like anti-pattern in microservice architectures.

Method: The authors used a dataset of microservice networks and applied various hub detection techniques, including scale-free property, different centrality metrics, clustering coefficient, and other encoding methods.

Result: It was found that architectural networks are not scale-free, detection methods disagree on hubs, and Kirkley's method was the most accurate for precision and detected hubs.

Conclusion: The findings introduce new research avenues for anti-pattern detection, suggest ways to improve the Arcan tool with normalized degree centrality or ER encoding, and provide valuable insights into hub detection methods for microservice systems.

Abstract: Context: Microservice Architecture is a popular architectural paradigm that
facilitates flexibility by decomposing applications into small, independently
deployable services. Catalogs of architectural anti-patterns have been proposed
to highlight the negative aspects of flawed microservice design. In particular,
the Hub-like anti-pattern lacks an unambiguous definition and detection method.
Aim: In this work, we aim to find a robust detection approach for the Hub-like
microservice anti-pattern that outputs a reasonable number of Hub-like
candidates with high precision. Method: We leveraged a dataset of 25
microservice networks and several network hub detection techniques to identify
the Hub-like anti-pattern, namely scale-free property, centrality metrics and
clustering coefficient, minimum description length principle, and the approach
behind the Arcan tool. Results and Conclusion: Our findings revealed that the
studied architectural networks are not scale-free, that most considered hub
detection approaches do not agree on the detected hubs, and that the method by
Kirkley leveraging the Erdos-Renyi encoding is the most accurate one in terms
of the number of detected hubs and the detection precision. Investigating
further the applicability of these methods to detecting Hub-like components in
microservice-based and other systems opens up new research directions.
Moreover, our results provide an evaluation of the approach utilized by the
widely used Arcan tool and highlight the potential to update the tool to use
the normalized degree centrality of a component in the network, or for the
approach based on ER encoding to be adopted instead.

</details>


### [694] [Centrality Change Proneness: an Early Indicator of Microservice Architectural Degradation](https://arxiv.org/abs/2506.07690)
*Alexander Bakhtin,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: The study explores temporal centrality metrics in microservice networks for early detection of architectural degradation, analyzing correlations with software metrics.


<details>
  <summary>Details</summary>
Motivation: The study seeks to develop a method for identifying early indicators of microservice architectural degradation to improve system health and prevent deterioration.

Method: The researchers reconstructed architecture for 7 releases of an open-source microservice project with 42 services, calculated software and centrality metrics, and derived a new metric called Centrality Change Proneness, exploring correlations between metrics.

Result: Seven size metrics and five complexity metrics were found to have consistent correlations with temporal centrality. However, Centrality Change Proneness did not affect software metrics.

Conclusion: Temporal centrality metrics and correlations with specific software metrics provide a valuable perspective for identifying microservice architectural degradation early, although Centrality Change Proneness alone is not impactful.

Abstract: Over the past decade, the wide adoption of Microservice Architecture has
required the identification of various patterns and anti-patterns to prevent
Microservice Architectural Degradation. Frequently, the systems are modelled as
a network of connected services. Recently, the study of temporal networks has
emerged as a way to describe and analyze evolving networks. Previous research
has explored how software metrics such as size, complexity, and quality are
related to microservice centrality in the architectural network. This study
investigates whether temporal centrality metrics can provide insight into the
early detection of architectural degradation by correlating or affecting
software metrics. We reconstructed the architecture of 7 releases of an OSS
microservice project with 42 services. For every service in every release, we
computed the software and centrality metrics. From one of the latter, we
derived a new metric, Centrality Change Proneness. We then explored the
correlation between the metrics. We identified 7 size and 5 complexity metrics
that have a consistent correlation with centrality, while Centrality Change
Proneness did not affect the software metrics, thus providing yet another
perspective and an early indicator of microservice architectural degradation.

</details>


### [695] [Towards a Small Language Model Lifecycle Framework](https://arxiv.org/abs/2506.07695)
*Parsa Miraghaei,Sergio Moreschini,Antti Kolehmainen,David Hästbacka*

Main category: cs.SE

TL;DR: This study introduces a unified lifecycle framework for Small Language Models (SLMs) after a comprehensive review of 36 related works.


<details>
  <summary>Details</summary>
Motivation: Existing research on Small Language Models (SLMs) is fragmented and lacks a unified lifecycle perspective.

Method: The authors conducted a survey of 36 academic and practitioner works, categorizing techniques relevant to the lifecycle of SLMs.

Result: They proposed a modular lifecycle model with main, optional, and cross-cutting components, highlighting interconnections to promote method reuse, co-adaptation, and lifecycle-awareness.

Conclusion: The framework bridges theory and practice, offering a foundation for SLM development and maintenance while guiding future research and tool creation.

Abstract: Background: The growing demand for efficient and deployable language models
has led to increased interest in Small Language Models (SLMs). However,
existing research remains fragmented, lacking a unified lifecycle perspective.
  Objective: This study aims to define a comprehensive lifecycle framework for
SLMs by synthesizing insights from academic literature and practitioner
sources.
  Method: We conducted a comprehensive survey of 36 works, analyzing and
categorizing lifecycle-relevant techniques.
  Results: We propose a modular lifecycle model structured into main, optional,
and cross-cutting components. The model captures key interconnections across
stages, supporting method reuse, co-adaptation, and lifecycle-awareness.
  Conclusion: Our framework provides a coherent foundation for developing and
maintaining SLMs, bridging theory and practice, and guiding future research and
tool development.

</details>


### [696] [Adversarial Attack Classification and Robustness Testing for Large Language Models for Code](https://arxiv.org/abs/2506.07942)
*Yang Liu,Armstrong Foundjem,Foutse Khomh,Heng Li*

Main category: cs.SE

TL;DR: The paper investigates the robustness of large language models for code generation (LLM4Code) against adversarial inputs, focusing on natural language vulnerabilities at the character, word, and sentence levels.


<details>
  <summary>Details</summary>
Motivation: To address the lack of exploration into how adversarial weaknesses in natural language inputs (like prompts, comments, descriptions) affect the robustness and security of LLM4Code.

Method: The study analyzed adversarial attacks along two dimensions—input type (code, prompts, comments) and granularity (character, word, sentence-level). The authors used datasets such as HumanEval and MBPP, and projects like ReCode and OpenAttack, combining performance metrics with qualitative evaluations.

Result: Sentence-level attacks had minimal impact on model performance, while word-level perturbations revealed significant semantic vulnerabilities. Character-level attacks showed mixed results, demonstrating sensitivity to syntax changes.

Conclusion: The study underscores the need for addressing semantic vulnerabilities and improving LLM4Code robustness to ensure secure and reliable code generation systems.

Abstract: Large Language Models (LLMs) have become vital tools in software development
tasks such as code generation, completion, and analysis. As their integration
into workflows deepens, ensuring robustness against vulnerabilities especially
those triggered by diverse or adversarial inputs becomes increasingly
important. Such vulnerabilities may lead to incorrect or insecure code
generation when models encounter perturbed task descriptions, code, or
comments. Prior research often overlooks the role of natural language in
guiding code tasks. This study investigates how adversarial perturbations in
natural language inputs including prompts, comments, and descriptions affect
LLMs for Code (LLM4Code). It examines the effects of perturbations at the
character, word, and sentence levels to identify the most impactful
vulnerabilities. We analyzed multiple projects (e.g., ReCode, OpenAttack) and
datasets (e.g., HumanEval, MBPP), establishing a taxonomy of adversarial
attacks. The first dimension classifies the input type code, prompts, or
comments while the second dimension focuses on granularity: character, word, or
sentence-level changes. We adopted a mixed-methods approach, combining
quantitative performance metrics with qualitative vulnerability analysis.
LLM4Code models show varying robustness across perturbation types.
Sentence-level attacks were least effective, suggesting models are resilient to
broader contextual changes. In contrast, word-level perturbations posed serious
challenges, exposing semantic vulnerabilities. Character-level effects varied,
showing model sensitivity to subtle syntactic deviations.Our study offers a
structured framework for testing LLM4Code robustness and emphasizes the
critical role of natural language in adversarial evaluation. Improving model
resilience to semantic-level disruptions is essential for secure and reliable
code-generation systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [697] [AI Agent Behavioral Science](https://arxiv.org/abs/2506.06366)
*Lin Chen,Yunke Zhang,Jie Feng,Haoye Chai,Honglin Zhang,Bingbing Fan,Yibo Ma,Shiyuan Zhang,Nian Li,Tianhui Liu,Nicholas Sukiennik,Keyu Zhao,Yu Li,Ziyi Liu,Fengli Xu,Yong Li*

Main category: q-bio.NC

TL;DR: This paper introduces the concept of AI Agent Behavioral Science to systematically study the behavior of increasingly human-like AI systems in diverse and open-ended contexts.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to better understand and manage the behavior of advanced AI systems that exhibit human-like traits and operate within dynamic social and interactive settings.

Method: The paper suggests observing AI behavior, testing hypotheses through controlled interventions, and interpreting AI actions using theory, integrating insights across individual, multi-agent, and human-agent interactions.

Result: The authors systematize findings across multiple research contexts and provide a framework for understanding AI's behavioral properties, including fairness, safety, and accountability.

Conclusion: AI Agent Behavioral Science is proposed as a vital complement to traditional AI approaches, offering tools to evaluate, interpret, and govern the real-world behavior of autonomous AI systems responsibly.

Abstract: Recent advances in large language models (LLMs) have enabled AI systems to
behave in increasingly human-like ways, exhibiting planning, adaptation, and
social dynamics across increasingly diverse, interactive, and open-ended
scenarios. These behaviors are not solely the product of the models' internal
architecture, but emerge from their integration into agentic systems that
operate within situated contexts, where goals, feedback, and interactions shape
behavior over time. This shift calls for a new scientific lens: AI Agent
Behavioral Science. Rather than focusing only on internal mechanisms, this
paradigm emphasizes the systematic observation of behavior, design of
interventions to test hypotheses, and theory-guided interpretation of how AI
agents act, adapt, and interact over time. We systematize a growing body of
research across individual, multi-agent, and human-agent interaction settings,
and further demonstrate how this perspective informs responsible AI by treating
fairness, safety, interpretability, accountability, and privacy as behavioral
properties. By unifying recent findings and laying out future directions, we
position AI Agent Behavioral Science as a necessary complement to traditional
approaches, providing essential tools for understanding, evaluating, and
governing the real-world behavior of increasingly autonomous AI systems.

</details>


### [698] [A Neuronal Model at the Edge of Criticality: An Ising-Inspired Approach to Brain Dynamics](https://arxiv.org/abs/2506.07027)
*Sajedeh Sarmastani,Maliheh Ghodrat,Yousef Jamali*

Main category: q-bio.NC

TL;DR: The paper develops a neural network model based on the Ising model, adding refractory periods for physiological realism and exploring criticality behaviors at varying temperatures.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore the hypothesis that the brain operates near a critical point for efficient information processing, using statistical physics to analyze neural system behaviors.

Method: A 2D lattice neural network model introduced with binary spins following Metropolis dynamics. Refractory periods are added to mimic biological neurons, and temperature parameters are varied to observe criticality.

Result: The model shows a transition from asynchronous to synchronized activity as temperature changes. Critical behaviors like heightened fluctuations and long-range correlations emerge near a critical temperature.

Conclusion: The results support the criticality hypothesis and demonstrate the utility of simplified, physics-based models to study complex neural phenomena.

Abstract: We present a neuronal network model inspired by the Ising model, where each
neuron is a binary spin ($s_i = \pm1$) interacting with its neighbors on a 2D
lattice. Updates are asynchronous and follow Metropolis dynamics, with a
temperature-like parameter $T$ introducing stochasticity.
  To incorporate physiological realism, each neuron includes fixed on/off
durations, mimicking the refractory period found in real neurons. These
counters prevent immediate reactivation, adding biologically grounded timing
constraints to the model.
  As $T$ varies, the network transitions from asynchronous to synchronised
activity. Near a critical point $T_c$, we observe hallmarks of criticality:
heightened fluctuations, long-range correlations, and increased sensitivity.
These features resemble patterns found in cortical recordings, supporting the
hypothesis that the brain operates near criticality for optimal information
processing.
  This simplified model demonstrates how basic spin interactions and
physiological constraints can yield complex, emergent behavior, offering a
useful tool for studying criticality in neural systems through statistical
physics.

</details>


### [699] [Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence](https://arxiv.org/abs/2506.07060)
*Laura Cohen,Xavier Hinaut,Lilyana Petrova,Alexandre Pitti,Syd Reynal,Ichiro Tsuda*

Main category: q-bio.NC

TL;DR: This paper advocates that adopting principles from natural intelligence, such as bandwidth constraints and intrinsic motivation, can lead to more efficient and interpretable AI systems.


<details>
  <summary>Details</summary>
Motivation: To explore how the efficiency, adaptability, and creativity of natural intelligence, despite constraints, can inspire improvements in artificial intelligence.

Method: The paper examines neurological features like neural bandwidth constraints, chaotic itinerancy, and developmental psychology to draw insights applicable to AI.

Result: The study observes mechanisms like concise codes, transient memory attractors, and rapid generalization, which are inspired by natural intelligence's constraints and developmental processes.

Conclusion: Implementing principles like energy constraints, simplistic architectures, and embodied learning in AI could improve its efficiency and biological plausibility.

Abstract: Natural intelligence (NI) consistently achieves more with less. Infants learn
language, develop abstract concepts, and acquire sensorimotor skills from
sparse data, all within tight neural and energy limits. In contrast, today's AI
relies on virtually unlimited computational power, energy, and data to reach
high performance. This paper argues that constraints in NI are paradoxically
catalysts for efficiency, adaptability, and creativity. We first show how
limited neural bandwidth promotes concise codes that still capture complex
patterns. Spiking neurons, hierarchical structures, and symbolic-like
representations emerge naturally from bandwidth constraints, enabling robust
generalization. Next, we discuss chaotic itinerancy, illustrating how the brain
transits among transient attractors to flexibly retrieve memories and manage
uncertainty. We then highlight reservoir computing, where random projections
facilitate rapid generalization from small datasets. Drawing on developmental
perspectives, we emphasize how intrinsic motivation, along with responsive
social environments, drives infant language learning and discovery of meaning.
Such active, embodied processes are largely absent in current AI. Finally, we
suggest that adopting 'less is more' principles -- energy constraints,
parsimonious architectures, and real-world interaction -- can foster the
emergence of more efficient, interpretable, and biologically grounded
artificial systems.

</details>


### [700] [Slow and Fast Neurons Cooperate in Contextual Working Memory through Timescale Diversity](https://arxiv.org/abs/2506.07341)
*Tomoki Kurikawa*

Main category: q-bio.NC

TL;DR: Neural systems use diverse timescales for improved nonlinear information processing. A model study using RNNs finds slow neurons sustain memory and improve performance, while fast neurons encode input signals.


<details>
  <summary>Details</summary>
Motivation: To understand the role of temporal heterogeneity in neural circuits and how it aids nonlinear cognitive processes, particularly in the frontal cortex during tasks demanding memory and flexible adaptation.

Method: The study employs a recurrent neural network (RNN) with neurons having varied intrinsic time constants to simulate a delayed match-to-sample task requiring nonlinear integration and adaptation.

Result: Task performance is optimal when neuron timescales are balanced. Slow neurons sustain memory and enhance task performance, while fast neurons encode signals strongly but briefly.

Conclusion: Temporal heterogeneity enables a complementary dynamic in neural networks, where slow neurons stabilize internal states and fast neurons enhance signal encoding, contributing to cognitive flexibility.

Abstract: Neural systems process information across a broad range of intrinsic
timescales, both within and across cortical areas. While such diversity is a
hallmark of biological networks, its computational role in nonlinear
information processing remains elusive. In this study, we examine how
heterogeneity in intrinsic neural timescales within the frontal cortex - a
region central to cognitive control - enhances performance in a
context-dependent working memory task. We develop a recurrent neural network
(RNN) composed of units with distinct time constants to model a delayed
match-to-sample task with contextual cues. This task demands nonlinear
integration of temporally dispersed inputs and flexible behavioral adaptation.
Our analysis shows that task performance is optimized when fast and slow
timescales are appropriately balanced. Intriguingly, slow neurons, despite
weaker encoding of task-relevant inputs, play a causal role in sustaining
memory and improving performance. In contrast, fast neurons exhibit strong but
transient encoding of input signals. These results highlight a division of
computational roles among neurons with different timescales: slow dynamics
support stable internal states, while fast dynamics enable rapid signal
encoding. Our findings provide a mechanistic account of how temporal
heterogeneity contributes to nonlinear information processing in neural
circuits, shedding light on the dynamic architecture underlying cognitive
flexibility.

</details>


### [701] [Dataset combining EEG, eye-tracking, and high-speed video for ocular activity analysis across BCI paradigms](https://arxiv.org/abs/2506.07488)
*E. Guttmann-Flury,X. Sheng,X. Zhu*

Main category: q-bio.NC

TL;DR: The paper introduces a large dataset combining EEG, eye-tracking, camera recordings, and mental state data for studying the role of eye movements in Brain-Computer Interface (BCI) tasks.


<details>
  <summary>Details</summary>
Motivation: To address the dual role of blinks and eye movements in BCI research, which can either degrade decoding accuracy or provide insights into user behavior.

Method: Created a multimodal dataset capturing EEG, eye-tracking, high-speed camera, and subjects' mental states over four paradigms including motor imagery, motor execution, SSVEP, and P300 spellers.

Result: The dataset consists of over 46 hours of data from 31 subjects across 63 sessions, a total of 2520 trials for three paradigms and 5670 for P300.

Conclusion: The dataset could aid in developing algorithms for handling eye artifacts, enhancing classification, and evaluating cross-paradigm robustness.

Abstract: In Brain-Computer Interface (BCI) research, the detailed study of blinks is
crucial. They can be considered as noise, affecting the efficiency and accuracy
of decoding users' cognitive states and intentions, or as potential features,
providing valuable insights into users' behavior and interaction patterns. We
introduce a large dataset capturing electroencephalogram (EEG) signals,
eye-tracking, high-speed camera recordings, as well as subjects' mental states
and characteristics, to provide a multifactor analysis of eye-related
movements. Four paradigms -- motor imagery, motor execution, steady-state
visually evoked potentials, and P300 spellers -- are selected due to their
capacity to evoke various sensory-motor responses and potential influence on
ocular activity. This online-available dataset contains over 46 hours of data
from 31 subjects across 63 sessions, totaling 2520 trials for each of the first
three paradigms, and 5670 for P300. This multimodal and multi-paradigms dataset
is expected to allow the development of algorithms capable of efficiently
handling eye-induced artifacts and enhancing task-specific classification.
Furthermore, it offers the opportunity to evaluate the cross-paradigm
robustness involving the same participants.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [702] [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
*Michał P. Karpowicz*

Main category: stat.ML

TL;DR: This paper proves an impossibility theorem indicating that large language models cannot simultaneously achieve four essential properties: truthfulness, semantic conservation, relevant knowledge revelation, and optimality.


<details>
  <summary>Details</summary>
Motivation: To address why large language models hallucinate and identify the trade-offs in their design.

Method: Models LLM inference as an 'auction of ideas' and utilizes the Green-Laffont theorem to mathematically prove the impossibility theorem.

Result: Demonstrated that achieving all four desired properties in LLM inference mechanisms is unattainable.

Conclusion: Modeling LLMs with rigorous mathematical frameworks helps in understanding their limitations and guiding future improvements.

Abstract: This paper explains \textbf{why it is impossible to create large language
models that do not hallucinate and what are the trade-offs we should be looking
for}. It presents a formal \textbf{impossibility theorem} demonstrating that no
inference mechanism can simultaneously satisfy four fundamental properties:
\textbf{truthful (non-hallucinatory) generation, semantic information
conservation, relevant knowledge revelation, and knowledge-constrained
optimality}. By modeling LLM inference as an \textbf{auction of ideas} where
neural components compete to contribute to responses, we prove the
impossibility using the Green-Laffont theorem. That mathematical framework
provides a rigorous foundation for understanding the nature of inference
process, with implications for model architecture, training objectives, and
evaluation methods.

</details>


### [703] [Direct Fisher Score Estimation for Likelihood Maximization](https://arxiv.org/abs/2506.06542)
*Sherman Khoo,Yakun Wang,Song Liu,Mark Beaumont*

Main category: stat.ML

TL;DR: The paper introduces a gradient-based optimization method for likelihood maximization using simulations, employing a score-matching approach and providing superior empirical results.


<details>
  <summary>Details</summary>
Motivation: Likelihood maximization is difficult when the likelihood function is intractable but simulations are available. The paper aims to address this challenge effectively.

Method: The method uses a sequential gradient-based optimization approach, employing a locally score-matching technique supported by simulations and a linear parameterization for fast computation.

Result: The proposed method achieves superior performance in both synthetic and real-world scenarios compared to existing approaches.

Conclusion: The approach addresses challenges in likelihood landscapes effectively, showcasing flexibility, efficiency, and empirical superiority, supported with theoretical guarantees on bias.

Abstract: We study the problem of likelihood maximization when the likelihood function
is intractable but model simulations are readily available. We propose a
sequential, gradient-based optimization method that directly models the Fisher
score based on a local score matching technique which uses simulations from a
localized region around each parameter iterate. By employing a linear
parameterization to the surrogate score model, our technique admits a
closed-form, least-squares solution. This approach yields a fast, flexible, and
efficient approximation to the Fisher score, effectively smoothing the
likelihood objective and mitigating the challenges posed by complex likelihood
landscapes. We provide theoretical guarantees for our score estimator,
including bounds on the bias introduced by the smoothing. Empirical results on
a range of synthetic and real-world problems demonstrate the superior
performance of our method compared to existing benchmarks.

</details>


### [704] [Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations](https://arxiv.org/abs/2506.06613)
*Arefe Boushehrian,Amir Najafi*

Main category: stat.ML

TL;DR: This paper explores the learnability of sample compressible distribution families under data perturbations, proposing a general framework and resolving open problems in high-dimensional mixture models.


<details>
  <summary>Details</summary>
Motivation: To investigate whether sample compressible distribution families remain learnable under various data perturbations and to establish the related sample complexity bounds.

Method: The paper introduces a perturbation-quantization framework that integrates with compression schemes and analyzes two data perturbation models: additive independent noise and adversarial corruption.

Result: The framework establishes general sample complexity bounds resilient to perturbations and resolves open questions concerning uniform and Gaussian mixture models under noisy and adversarial conditions.

Conclusion: Sample compressibility ensures learnability even with perturbed samples, solidifying its role in distribution learning and providing new insights into high-dimensional mixture models.

Abstract: Learning distribution families over $\mathbb{R}^d$ is a fundamental problem
in unsupervised learning and statistics. A central question in this setting is
whether a given family of distributions possesses sufficient structure to be
(at least) information-theoretically learnable and, if so, to characterize its
sample complexity. In 2018, Ashtiani et al. reframed \emph{sample
compressibility}, originally due to Littlestone and Warmuth (1986), as a
structural property of distribution classes, proving that it guarantees
PAC-learnability. This discovery subsequently enabled a series of recent
advancements in deriving nearly tight sample complexity bounds for various
high-dimensional open problems. It has been further conjectured that the
converse also holds: every learnable class admits a tight sample compression
scheme.
  In this work, we establish that sample compressible families remain learnable
even from perturbed samples, subject to a set of necessary and sufficient
conditions. We analyze two models of data perturbation: (i) an additive
independent noise model, and (ii) an adversarial corruption model, where an
adversary manipulates a limited subset of the samples unknown to the learner.
Our results are general and rely on as minimal assumptions as possible. We
develop a perturbation-quantization framework that interfaces naturally with
the compression scheme and leads to sample complexity bounds that scale
gracefully with the noise level and corruption budget. As concrete
applications, we establish new sample complexity bounds for learning finite
mixtures of high-dimensional uniform distributions under both noise and
adversarial perturbations, as well as for learning Gaussian mixture models from
adversarially corrupted samples, resolving two open problems in the literature.

</details>


### [705] [Continuous Semi-Implicit Models](https://arxiv.org/abs/2506.06778)
*Longlin Yu,Jiajun Zha,Tong Yang,Tianyu Xie,Xiangyu Zhang,S. -H. Gary Chan,Cheng Zhang*

Main category: stat.ML

TL;DR: The paper introduces CoSIM, a continuous semi-implicit model for variational inference and generative modeling, which improves training efficiency and accelerates diffusion models.


<details>
  <summary>Details</summary>
Motivation: Improve the expressiveness and efficiency of hierarchical semi-implicit models to address slow convergence and enhance generative modeling/task performance.

Method: CoSIM extends hierarchical semi-implicit models to a continuous framework using a continuous designed transition kernel, enabling simulation-free training and multistep distillation of generative models.

Result: CoSIM achieves competitive or better performance compared to existing diffusion model acceleration methods, with superior results in experiments like image generation (e.g., FD-DINOv2).

Conclusion: CoSIM offers an efficient and consistent approach to enhance generative modeling and diffusion model acceleration through its continuous semi-implicit framework.

Abstract: Semi-implicit distributions have shown great promise in variational inference
and generative modeling. Hierarchical semi-implicit models, which stack
multiple semi-implicit layers, enhance the expressiveness of semi-implicit
distributions and can be used to accelerate diffusion models given pretrained
score networks. However, their sequential training often suffers from slow
convergence. In this paper, we introduce CoSIM, a continuous semi-implicit
model that extends hierarchical semi-implicit models into a continuous
framework. By incorporating a continuous transition kernel, CoSIM enables
efficient, simulation-free training. Furthermore, we show that CoSIM achieves
consistency with a carefully designed transition kernel, offering a novel
approach for multistep distillation of generative models at the distributional
level. Extensive experiments on image generation demonstrate that CoSIM
performs on par or better than existing diffusion model acceleration methods,
achieving superior performance on FD-DINOv2.

</details>


### [706] [The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes](https://arxiv.org/abs/2506.06828)
*Simon P. von der Maase*

Main category: stat.ML

TL;DR: The paper introduces a Gaussian process-based method for analyzing and forecasting temporospatial patterns of violent conflict using disaggregated event data.


<details>
  <summary>Details</summary>
Motivation: Violent conflict studies often lack precise spatial and temporal estimation, which hinders proper analysis, control, and forecasting of conflict trends.

Method: The author uses Gaussian processes combined with temporospatially disaggregated conflict event data to model and extrapolate conflict trends.

Result: The proposed method successfully identifies conflict traps, diffusion, exposure, and provides reliable forecasts based solely on historical conflict data.

Conclusion: The study presents a parsimonious yet powerful approach for analyzing and predicting conflict patterns, valuable for conflict management and prediction tasks.

Abstract: I present a novel approach to estimating the temporal and spatial patterns of
violent conflict. I show how we can use highly temporally and spatially
disaggregated data on conflict events in tandem with Gaussian processes to
estimate temporospatial conflict trends. These trends can be studied to gain
insight into conflict traps, diffusion and tempo-spatial conflict exposure in
general; they can also be used to control for such phenomenons given other
estimation tasks; lastly, the approach allow us to extrapolate the estimated
tempo-spatial conflict patterns into future temporal units, thus facilitating
powerful, stat-of-the-art, conflict forecasts. Importantly, these results are
achieved via a relatively parsimonious framework using only one data source:
past conflict patterns.

</details>


### [707] [A Statistical Framework for Model Selection in LSTM Networks](https://arxiv.org/abs/2506.06840)
*Fahad Mostafa*

Main category: stat.ML

TL;DR: The paper introduces a unified statistical framework to streamline model selection in LSTM networks, improving upon heuristics.


<details>
  <summary>Details</summary>
Motivation: The authors aim to resolve the challenges of heuristic and computationally expensive model selection processes in LSTM networks.

Method: They propose penalized likelihoods for temporal data, a threshold approach for hidden state dynamics, and use variational Bayes and approximate marginal likelihood for estimation.

Result: The framework demonstrated improved performance and flexibility in biomedical data applications.

Conclusion: The proposed framework extends classical statistical methods to LSTMs, offering efficient and systematic model selection.

Abstract: Long Short-Term Memory (LSTM) neural network models have become the
cornerstone for sequential data modeling in numerous applications, ranging from
natural language processing to time series forecasting. Despite their success,
the problem of model selection, including hyperparameter tuning, architecture
specification, and regularization choice remains largely heuristic and
computationally expensive. In this paper, we propose a unified statistical
framework for systematic model selection in LSTM networks. Our framework
extends classical model selection ideas, such as information criteria and
shrinkage estimation, to sequential neural networks. We define penalized
likelihoods adapted to temporal structures, propose a generalized threshold
approach for hidden state dynamics, and provide efficient estimation strategies
using variational Bayes and approximate marginal likelihood methods. Several
biomedical data centric examples demonstrate the flexibility and improved
performance of the proposed framework.

</details>


### [708] [Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis](https://arxiv.org/abs/2506.07011)
*Yuan-Hao Wei,Yan-Jie Sun*

Main category: stat.ML

TL;DR: The paper introduces Half Adversarial VAE (Half-AVAE) to enhance latent variable independence and interpretability, addressing challenges in underdetermined Independent Component Analysis (ICA). Experimental results show its effectiveness compared to existing models.


<details>
  <summary>Details</summary>
Motivation: Traditional Variational Autoencoder (VAE) approaches struggle in underdetermined ICA scenarios, where the number of latent variables exceeds the number of observed signals. The paper's aim is to enhance latent independence and improve interpretability, overcoming these limitations.

Method: The paper proposes the Half-AVAE framework, combining adversarial networks with External Enhancement (EE) terms to ensure independence among latent dimensions, while adopting an encoder-free approach for better handling of underdetermined ICA.

Result: Experiments on synthetic signals demonstrate that Half-AVAE achieves better performance in recovering independent components compared to baseline models such as GP-AVAE and Half-VAE, with lower root mean square errors under underdetermined conditions.

Conclusion: The study concludes that removing the encoder, alongside adversarial training and structured priors, makes VAEs highly effective for complex ICA tasks, advancing their applicability to disentanglement, causal inference, and generative modeling.

Abstract: This study advances the Variational Autoencoder (VAE) framework by addressing
challenges in Independent Component Analysis (ICA) under both determined and
underdetermined conditions, focusing on enhancing the independence and
interpretability of latent variables. Traditional VAEs map observed data to
latent variables and back via an encoder-decoder architecture, but struggle
with underdetermined ICA where the number of latent variables exceeds observed
signals. The proposed Half Adversarial VAE (Half-AVAE) builds on the
encoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle
underdetermined scenarios. By integrating adversarial networks and External
Enhancement (EE) terms, Half-AVAE promotes mutual independence among latent
dimensions, achieving factorized and interpretable representations. Experiments
with synthetic signals demonstrate that Half-AVAE outperforms baseline models,
including GP-AVAE and Half-VAE, in recovering independent components under
underdetermined conditions, as evidenced by lower root mean square errors. The
study highlights the flexibility of VAEs in variational inference, showing that
encoder omission, combined with adversarial training and structured priors,
enables effective solutions for complex ICA tasks, advancing applications in
disentanglement, causal inference, and generative modeling.

</details>


### [709] [Quantile-Optimal Policy Learning under Unmeasured Confounding](https://arxiv.org/abs/2506.07140)
*Zhongren Chen,Siyu Chen,Zhengling Qi,Xiaohong Chen,Zhuoran Yang*

Main category: stat.ML

TL;DR: This paper addresses the challenge of finding a quantile-optimal policy in offline settings with unobserved confounders, proposing causal-assisted policy learning methods that offer strong theoretical guarantees and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenge of learning policies that maximize the quantile of reward distributions in settings with unobserved confounders, a crucial problem in decision-making under uncertainty.

Method: The authors employ causal inference tools (instrumental variables, negative controls) to estimate quantile objectives through functional integral equations. They utilize minimax estimation approaches, conservative policy construction to handle limited dataset coverage, and propose a novel regularized policy learning method.

Result: The proposed methods achieve quantile-optimal policies with a convergence rate of $\tilde{\mathscr{O}}(n^{-1/2})$ under mild conditions, demonstrating sample-efficiency even with unmeasured confounders.

Conclusion: The study introduces the first sample-efficient algorithms for learning quantile-optimal policies in settings with unobserved confounders, providing theoretical guarantees and practical methods for implementation.

Abstract: We study quantile-optimal policy learning where the goal is to find a policy
whose reward distribution has the largest $\alpha$-quantile for some $\alpha
\in (0, 1)$. We focus on the offline setting whose generating process involves
unobserved confounders. Such a problem suffers from three main challenges: (i)
nonlinearity of the quantile objective as a functional of the reward
distribution, (ii) unobserved confounding issue, and (iii) insufficient
coverage of the offline dataset. To address these challenges, we propose a
suite of causal-assisted policy learning methods that provably enjoy strong
theoretical guarantees under mild conditions. In particular, to address (i) and
(ii), using causal inference tools such as instrumental variables and negative
controls, we propose to estimate the quantile objectives by solving nonlinear
functional integral equations. Then we adopt a minimax estimation approach with
nonparametric models to solve these integral equations, and propose to
construct conservative policy estimates that address (iii). The final policy is
the one that maximizes these pessimistic estimates. In addition, we propose a
novel regularized policy learning method that is more amenable to computation.
Finally, we prove that the policies learned by these methods are
$\tilde{\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage
assumption on the offline dataset. Here, $\tilde{\mathscr{O}}(\cdot)$ omits
poly-logarithmic factors. To the best of our knowledge, we propose the first
sample-efficient policy learning algorithms for estimating the quantile-optimal
policy when there exist unmeasured confounding.

</details>


### [710] [Rao-Blackwellised Reparameterisation Gradients](https://arxiv.org/abs/2506.07687)
*Kevin Lam,Thang Bui,George Deligiannidis,Yee Whye Teh*

Main category: stat.ML

TL;DR: The paper introduces the R2-G2 estimator for handling latent Gaussian variables in probabilistic machine learning. It generalizes the benefits of Rao-Blackwellisation for gradient estimation and improves initial training performance.


<details>
  <summary>Details</summary>
Motivation: Latent Gaussian variables are widely used, but optimizing models with these variables requires precise gradient estimators like the reparameterisation trick, which may have limitations.

Method: The authors propose the R2-G2 estimator, applying Rao-Blackwellisation to the reparameterisation gradient. They identify existing connections with Bayesian MLPs and expand its applicability to more probabilistic models.

Result: R2-G2 demonstrates superior initial training performance, particularly in cases with multiple applications of the reparameterisation trick.

Conclusion: R2-G2 enhances gradient estimation and initial training reliability, paving the way for improved probabilistic models with latent Gaussian variables.

Abstract: Latent Gaussian variables have been popularised in probabilistic machine
learning. In turn, gradient estimators are the machinery that facilitates
gradient-based optimisation for models with latent Gaussian variables. The
reparameterisation trick is often used as the default estimator as it is simple
to implement and yields low-variance gradients for variational inference. In
this work, we propose the R2-G2 estimator as the Rao-Blackwellisation of the
reparameterisation gradient estimator. Interestingly, we show that the local
reparameterisation gradient estimator for Bayesian MLPs is an instance of the
R2-G2 estimator and Rao-Blackwellisation. This lets us extend benefits of
Rao-Blackwellised gradients to a suite of probabilistic models. We show that
initial training with R2-G2 consistently yields better performance in models
with multiple applications of the reparameterisation trick.

</details>


### [711] [Quickest Causal Change Point Detection by Adaptive Intervention](https://arxiv.org/abs/2506.07760)
*Haijie Xu,Chen Zhang*

Main category: stat.ML

TL;DR: The paper proposes algorithms to monitor change points in linear causal models, integrating interventions strategically to concentrate changes into a single dimension, enhance change detection and balance exploration and exploitation.


<details>
  <summary>Details</summary>
Motivation: Monitoring change points in dynamic systems is crucial, particularly when causal relationships and interventions play an essential role in understanding and reacting to these transitions.

Method: The authors utilize a special centralization technique to localize changes and leverage Kullback-Leibler divergence to select intervention nodes and enhance change detection. They also integrate adaptive policies for exploration and exploitation in monitoring techniques.

Result: The proposed methods demonstrated theoretical first-order optimality and were validated with simulations and real-world datasets, showcasing their effectiveness in detecting change points influenced by causal dynamics.

Conclusion: The study provides effective tools for identifying and amplifying change points in linear causal models, presenting balanced methods that integrate adaptive interventions to navigate changes efficiently.

Abstract: We propose an algorithm for change point monitoring in linear causal models
that accounts for interventions. Through a special centralization technique, we
can concentrate the changes arising from causal propagation across nodes into a
single dimension. Additionally, by selecting appropriate intervention nodes
based on Kullback-Leibler divergence, we can amplify the change magnitude. We
also present an algorithm for selecting the intervention values, which aids in
the identification of the most effective intervention nodes. Two monitoring
methods are proposed, each with an adaptive intervention policy to make a
balance between exploration and exploitation. We theoretically demonstrate the
first-order optimality of the proposed methods and validate their properties
using simulation datasets and two real-world case studies.

</details>


### [712] [Generalization Analysis for Bayesian Optimal Experiment Design under Model Misspecification](https://arxiv.org/abs/2506.07805)
*Roubing Tang,Sabina J. Sloman,Samuel Kaski*

Main category: stat.ML

TL;DR: This paper examines Bayesian Optimal Experimental Design (BOED) under model misspecification, revealing sources of generalization error such as covariate shift and error amplification, and proposes a novel acquisition function to mitigate these issues.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by covariate shift and model misspecification in BOED, which negatively impact generalization performance during testing.

Method: The authors first mathematically dissect generalization error under model misspecification. Then, they empirically analyze methods to improve generalization by focusing on representative and de-amplifying training data. Lastly, they propose a novel acquisition function incorporating representativeness and de-amplification.

Result: The experimental results confirm that the novel acquisition function improves generalization performance, outperforming traditional BOED methods under model misspecification.

Conclusion: Incorporating representativeness and error de-amplification into BOED acquisition functions helps mitigate the negative effects of model misspecification, enhancing generalization performance in experimental design.

Abstract: In many settings in science and industry, such as drug discovery and clinical
trials, a central challenge is designing experiments under time and budget
constraints. Bayesian Optimal Experimental Design (BOED) is a paradigm to pick
maximally informative designs that has been increasingly applied to such
problems. During training, BOED selects inputs according to a pre-determined
acquisition criterion. During testing, the model learned during training
encounters a naturally occurring distribution of test samples. This leads to an
instance of covariate shift, where the train and test samples are drawn from
different distributions. Prior work has shown that in the presence of model
misspecification, covariate shift amplifies generalization error. Our first
contribution is to provide a mathematical decomposition of generalization error
that reveals key contributors to generalization error in the presence of model
misspecification. We show that generalization error under misspecification is
the result of, in addition to covariate shift, a phenomenon we term error
(de-)amplification which has not been identified or studied in prior work. Our
second contribution is to provide a detailed empirical analysis to show that
methods that result in representative and de-amplifying training data increase
generalization performance. Our third contribution is to develop a novel
acquisition function that mitigates the effects of model misspecification by
including a term for representativeness and implicitly inducing
de-amplification. Our experimental results demonstrate that our method
outperforms traditional BOED in the presence of misspecification.

</details>


### [713] [Accelerating Constrained Sampling: A Large Deviations Approach](https://arxiv.org/abs/2506.07816)
*Yingli Wang,Changwei Tu,Xiaoyu Wang,Lingjiong Zhu*

Main category: stat.ML

TL;DR: This paper explores the performance of skew-reflected non-reversible Langevin dynamics (SRNLD) for constrained sampling, focusing on long-time behavior and convergence acceleration.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address constrained sampling challenges in applications such as machine learning and improve the efficiency of Langevin algorithms through better design of skew-symmetric matrices.

Method: The authors apply large deviation principles (LDP) and propose a specific skew-symmetric matrix design that speeds up SRNLD's convergence to the target distribution.

Result: They establish rate functions from the LDP analysis and validate superior performance of skew-reflected non-reversible Langevin Monte Carlo (SRNLMC) via numerical experiments.

Conclusion: The proposed skew-symmetric matrix design enhances practical efficiency and theoretical convergence, showing promising improvements in sampling constrained distributions.

Abstract: The problem of sampling a target probability distribution on a constrained
domain arises in many applications including machine learning. For constrained
sampling, various Langevin algorithms such as projected Langevin Monte Carlo
(PLMC) based on the discretization of reflected Langevin dynamics (RLD) and
more generally skew-reflected non-reversible Langevin Monte Carlo (SRNLMC)
based on the discretization of skew-reflected non-reversible Langevin dynamics
(SRNLD) have been proposed and studied in the literature. This work focuses on
the long-time behavior of SRNLD, where a skew-symmetric matrix is added to RLD.
Although the non-asymptotic convergence analysis for SRNLD (and SRNLMC) and the
acceleration compared to RLD (and PMLC) have been studied in the literature, it
is not clear how one should design the skew-symmetric matrix in the dynamics to
achieve good performance in practice. We establish a large deviation principle
(LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is
chosen such that its product with the inward unit normal vector field on the
boundary is zero. By explicitly characterizing the rate functions, we show that
SRNLD can accelerate the convergence to the target distribution compared to RLD
with this choice of the skew-symmetric matrix. Numerical experiments for SRNLMC
based on the proposed skew-symmetric matrix show superior performance which
validate the theoretical findings from the large deviations theory.

</details>


### [714] [ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition](https://arxiv.org/abs/2506.07259)
*Daolang Huang,Xinyi Wen,Ayush Bharti,Samuel Kaski,Luigi Acerbi*

Main category: stat.ML

TL;DR: The paper introduces ALINE, a framework combining Bayesian inference and active data acquisition using a transformer trained with reinforcement learning to optimize for immediate and accurate predictions.


<details>
  <summary>Details</summary>
Motivation: Critical applications like scientific discovery and personalized medicine require systems that can simultaneously acquire meaningful data and perform immediate inference. Existing methods are suboptimal for the scenario where new data is needed for on-the-spot inference.

Method: The authors propose ALINE, a framework using a transformer architecture trained with reinforcement learning, where the reward is based on self-estimated information gain. The model is designed to strategically acquire data points and refine predictions simultaneously, targeting specific tasks or parameters.

Result: ALINE demonstrates strong empirical results in regression-based active learning, Bayesian experimental design scenarios, and psychometric models, showcasing its capabilities for immediate inference and efficient data point selection.

Conclusion: ALINE offers a unified solution for optimizing active data acquisition and instantaneous Bayesian inference, advancing both theoretical and practical aspects in critical application areas.

Abstract: Many critical applications, from autonomous scientific discovery to
personalized medicine, demand systems that can both strategically acquire the
most informative data and instantaneously perform inference based upon it.
While amortized methods for Bayesian inference and experimental design offer
part of the solution, neither approach is optimal in the most general and
challenging task, where new data needs to be collected for instant inference.
To tackle this issue, we introduce the Amortized Active Learning and Inference
Engine (ALINE), a unified framework for amortized Bayesian inference and active
data acquisition. ALINE leverages a transformer architecture trained via
reinforcement learning with a reward based on self-estimated information gain
provided by its own integrated inference component. This allows it to
strategically query informative data points while simultaneously refining its
predictions. Moreover, ALINE can selectively direct its querying strategy
towards specific subsets of model parameters or designated predictive tasks,
optimizing for posterior estimation, data prediction, or a mixture thereof.
Empirical results on regression-based active learning, classical Bayesian
experimental design benchmarks, and a psychometric model with selectively
targeted parameters demonstrate that ALINE delivers both instant and accurate
inference along with efficient selection of informative points.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [715] [Recursive Semantic Anchoring in ISO 639:2023: A Structural Extension to ISO/TC 37 Frameworks](https://arxiv.org/abs/2506.06870)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.LO

TL;DR: The study extends ISO 639:2023 by proposing a recursive framework for modeling semantic drift in languages, using formal methods to enhance language identification and translation accuracy.


<details>
  <summary>Details</summary>
Motivation: Current ISO 639:2023 standards lack a machine-native method to model dialectal drift or creole mixtures effectively, creating challenges in handling semantic variability.

Method: The paper introduces recursive semantic anchoring with fixed-point operators that model semantic drift in language identities and uses category theory to formalize the mapping and convergence. Experiments are validated using RDF/Turtle schemas and examples.

Result: The proposed framework improves language identification and translation accuracy in noisy or code-switched scenarios, and aligns with ISO/TC 37 standards while introducing a semantic layer.

Conclusion: The study offers an AI-compatible extension to ISO 639 for modeling dialectal drift and creole mixing, effectively bridging semantic gaps in linguistic standards and aiding future developments.

Abstract: ISO 639:2023 unifies the ISO language-code family and introduces contextual
metadata, but it lacks a machine-native mechanism for handling dialectal drift
and creole mixtures. We propose a formalisation of recursive semantic
anchoring, attaching to every language entity $\chi$ a family of fixed-point
operators $\phi_{n,m}$ that model bounded semantic drift via the relation
$\phi_{n,m}(\chi) = \chi \oplus \Delta(\chi)$, where $\Delta(\chi)$ is a drift
vector in a latent semantic manifold. The base anchor $\phi_{0,0}$ recovers the
canonical ISO 639:2023 identity, whereas $\phi_{99,9}$ marks the maximal drift
state that triggers a deterministic fallback. Using category theory, we treat
the operators $\phi_{n,m}$ as morphisms and drift vectors as arrows in a
category $\mathrm{DriftLang}$. A functor $\Phi: \mathrm{DriftLang} \to
\mathrm{AnchorLang}$ maps every drifted object to its unique anchor and proves
convergence. We provide an RDF/Turtle schema (\texttt{BaseLanguage},
\texttt{DriftedLanguage}, \texttt{ResolvedAnchor}) and worked examples -- e.g.,
$\phi_{8,4}$ (Standard Mandarin) versus $\phi_{8,7}$ (a colloquial variant),
and $\phi_{1,7}$ for Nigerian Pidgin anchored to English. Experiments with
transformer models show higher accuracy in language identification and
translation on noisy or code-switched input when the $\phi$-indices are used to
guide fallback routing. The framework is compatible with ISO/TC 37 and provides
an AI-tractable, drift-aware semantic layer for future standards.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [716] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph automates simulations in chemistry and materials science using AI, graph neural networks, and language models, enabling efficient and interactive workflows.


<details>
  <summary>Details</summary>
Motivation: The complexities of setting up and validating computational chemistry simulations hinder research in catalysts, energy materials, and pharmaceuticals.

Method: ChemGraph integrates graph neural networks, large language models, and a multi-agent framework for task decomposition, providing automated and interactive simulation tools.

Result: ChemGraph performs well across 13 benchmark tasks. Smaller LLMs excel in simpler tasks, and larger models handle complex workflows effectively. Multi-agent frameworks enable smaller models to perform competitively.

Conclusion: ChemGraph simplifies computational chemistry tasks with AI-driven automation, allowing researchers to use diverse simulation methods efficiently.

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [717] [Strongly Consistent Community Detection in Popularity Adjusted Block Models](https://arxiv.org/abs/2506.07224)
*Quan Yuan,Binghui Liu,Danning Li,Lingzhou Xue*

Main category: stat.ME

TL;DR: This paper introduces new spectral clustering methods for the Popularity Adjusted Block Model (PABM) to address challenges in community detection by proving robust theoretical consistency and validating them in simulations and real-world data.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and unresolved challenges in adapting spectral clustering techniques and ensuring strong label recovery consistency under the Popularity Adjusted Block Model (PABM).

Method: The paper proposes the Thresholded Cosine Spectral Clustering (TCSC) algorithm and its one-step refinement (Refined TCSC), introduces a data-driven technique for selecting community numbers, and validates these methods through theoretical analysis, simulations, and real-world applications.

Result: TCSC shows weak consistency, while Refined TCSC achieves strong consistency by recovering all community labels with high probability, offering faster convergence for small sample sizes. The proposed data-driven community number selection method outperforms existing techniques under PABM.

Conclusion: The methods proposed in this paper address the complexity of PABM while ensuring strong consistency and robust community detection results, demonstrating efficacy through both theoretical validation and practical applications.

Abstract: The Popularity Adjusted Block Model (PABM) provides a flexible framework for
community detection in network data by allowing heterogeneous node popularity
across communities. However, this flexibility increases model complexity and
raises key unresolved challenges, particularly in effectively adapting spectral
clustering techniques and efficiently achieving strong consistency in label
recovery. To address these challenges, we first propose the Thresholded Cosine
Spectral Clustering (TCSC) algorithm and establish its weak consistency under
the PABM. We then introduce the one-step Refined TCSC algorithm and prove that
it achieves strong consistency under the PABM, correctly recovering all
community labels with high probability. We further show that the two-step
Refined TCSC accelerates clustering error convergence, especially with small
sample sizes. Additionally, we propose a data-driven approach for selecting the
number of communities, which outperforms existing methods under the PABM. The
effectiveness and robustness of our methods are validated through extensive
simulations and real-world applications.

</details>


### [718] [Heavy Lasso: sparse penalized regression under heavy-tailed noise via data-augmented soft-thresholding](https://arxiv.org/abs/2506.07790)
*The Tien Mai*

Main category: stat.ME

TL;DR: The study introduces Heavy Lasso, a robust high-dimensional linear regression method tailored for heavy-tailed noise or outlier data environments.


<details>
  <summary>Details</summary>
Motivation: Classical Lasso struggles with heavy-tailed errors or outliers in practical data scenarios like genomics or finance; there’s a need for an alternative robust regression approach.

Method: Heavy Lasso integrates a loss function inspired by the Student's t-distribution into the Lasso framework, ensuring adaptiveness to large deviations using a computationally efficient data augmentation scheme.

Result: Heavy Lasso offers superior performance compared to traditional Lasso and other robust methods, as supported by theoretical bounds and numerical experiments in noisy datasets.

Conclusion: The innovative Heavy Lasso achieves robustness without sacrificing computational efficiency and outperforms competing methods in robust high-dimensional regression.

Abstract: High-dimensional linear regression is a fundamental tool in modern
statistics, particularly when the number of predictors exceeds the sample size.
The classical Lasso, which relies on the squared loss, performs well under
Gaussian noise assumptions but often deteriorates in the presence of
heavy-tailed errors or outliers commonly encountered in real data applications
such as genomics, finance, and signal processing. To address these challenges,
we propose a novel robust regression method, termed Heavy Lasso, which
incorporates a loss function inspired by the Student's t-distribution within a
Lasso penalization framework. This loss retains the desirable quadratic
behavior for small residuals while adaptively downweighting large deviations,
thus enhancing robustness to heavy-tailed noise and outliers. Heavy Lasso
enjoys computationally efficient by leveraging a data augmentation scheme and a
soft-thresholding algorithm, which integrate seamlessly with classical Lasso
solvers. Theoretically, we establish non-asymptotic bounds under both $\ell_1$
and $\ell_2 $ norms, by employing the framework of localized convexity, showing
that the Heavy Lasso estimator achieves rates comparable to those of the Huber
loss. Extensive numerical studies demonstrate Heavy Lasso's superior
performance over classical Lasso and other robust variants, highlighting its
effectiveness in challenging noisy settings. Our method is implemented in the R
package heavylasso available on Github.

</details>


### [719] [Conditional Local Independence Testing with Application to Dynamic Causal Discovery](https://arxiv.org/abs/2506.07844)
*Mingzhou Liu,Xinwei Sun,Yizhou Wang*

Main category: stat.ME

TL;DR: The paper extends a conditional local independence testing approach to Ito processes for causal discovery in dynamic systems.


<details>
  <summary>Details</summary>
Motivation: To enhance causal discovery methods in dynamic systems.

Method: Adaptation of prior local independence testing theory to Ito processes.

Result: Provides a theoretical extension applicable to dynamic systems.

Conclusion: Expands the utility of causal discovery frameworks using Ito process extensions.

Abstract: In this note, we extend the conditional local independence testing theory
developed in Christgau et al. (2024) to Ito processes. The result can be
applied to causal discovery in dynamic systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [720] [Experimental Evaluation of Static Image Sub-Region-Based Search Models Using CLIP](https://arxiv.org/abs/2506.06938)
*Bastian Jäckl,Vojtěch Kloda,Daniel A. Keim,Jakub Lokoč*

Main category: cs.MM

TL;DR: The study explores the role of location-based prompts in improving text-based image retrieval within homogeneous, specialized domains such as underwater scenes, showing promising results by using partitioned image regions.


<details>
  <summary>Details</summary>
Motivation: Users in specialized domains struggle with image retrieval due to their inability to provide precise textual queries, making effective search more difficult.

Method: The authors utilized a dataset of 741 human annotations with text descriptions and bounding boxes of underwater scenes, evaluating the performance of CLIP using static sub-image regions compared to entire images.

Result: Both 3-by-3 image partitioning and 5-grid overlapping methods enhanced retrieval performance and demonstrated robustness against annotation box variations.

Conclusion: Incorporating spatial information through region-based partitioning improves multimodal model effectiveness in complex and homogeneous domains like underwater imagery.

Abstract: Advances in multimodal text-image models have enabled effective text-based
querying in extensive image collections. While these models show convincing
performance for everyday life scenes, querying in highly homogeneous,
specialized domains remains challenging. The primary problem is that users can
often provide only vague textual descriptions as they lack expert knowledge to
discriminate between homogenous entities. This work investigates whether adding
location-based prompts to complement these vague text queries can enhance
retrieval performance. Specifically, we collected a dataset of 741 human
annotations, each containing short and long textual descriptions and bounding
boxes indicating regions of interest in challenging underwater scenes. Using
these annotations, we evaluate the performance of CLIP when queried on various
static sub-regions of images compared to the full image. Our results show that
both a simple 3-by-3 partitioning and a 5-grid overlap significantly improve
retrieval effectiveness and remain robust to perturbations of the annotation
box.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [721] [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
*Guibin Zhang,Muxin Fu,Guancheng Wan,Miao Yu,Kun Wang,Shuicheng Yan*

Main category: cs.MA

TL;DR: The paper introduces G-Memory, a sophisticated memory system for multi-agent systems (MAS), improving performance on knowledge and embodied action benchmarks without altering MAS frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing memory systems in MAS are simplistic, failing to account for inter-agent collaboration and lacking cross-trial and agent-specific customization, unlike memory systems for single agents.

Method: G-Memory is a three-tier hierarchical graph-based memory inspired by organizational memory theory, retrieving both high-level insights and fine-grained interaction data for better task performance.

Result: G-Memory achieved up to 20.89% improvement in success rates for embodied actions and 10.12% in accuracy for knowledge-based QA across multiple benchmarks and MAS frameworks.

Conclusion: The proposed G-Memory system enhances task performance in MAS, showcasing the importance of advanced, structured memory designs for effective multi-agent collaboration.

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.

</details>


### [722] [AI-Generated Compromises for Coalition Formation](https://arxiv.org/abs/2506.06837)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: The paper introduces AI methods for coalition formation, focusing on compromise proposal generation in collaborative document writing using semantic metric spaces.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of finding compromise proposals that unite agent coalitions, especially under bounded rationality and uncertainty.

Method: Formalization of a model integrating agent bounded rationality and uncertainty, employing NLP and large language models to induce semantic metric spaces and generate compromise proposals.

Result: Simulation demonstrates the potential of AI methods in facilitating democratic text editing, outperforming traditional tools.

Conclusion: AI methods can effectively support large-scale democratic document development, showcasing the power of semantic modeling in text-based coalition formation.

Abstract: The challenge of finding compromises between agent proposals is fundamental
to AI subfields such as argumentation, mediation, and negotiation. Building on
this tradition, Elkind et al. (2021) introduced a process for coalition
formation that seeks majority-supported proposals preferable to the status quo,
using a metric space where each agent has an ideal point. A crucial step in
this process involves identifying compromise proposals around which agent
coalitions can unite. How to effectively find such compromise proposals remains
an open question. We address this gap by formalizing a model that incorporates
agent bounded rationality and uncertainty, and by developing AI methods to
generate compromise proposals. We focus on the domain of collaborative document
writing, such as the democratic drafting of a community constitution. Our
approach uses natural language processing techniques and large language models
to induce a semantic metric space over text. Based on this space, we design
algorithms to suggest compromise points likely to receive broad support. To
evaluate our methods, we simulate coalition formation processes and show that
AI can facilitate large-scale democratic text editing, a domain where
traditional tools are limited.

</details>


### [723] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)
*Philip Liu,Sparsh Bansal,Jimmy Dinh,Aditya Pawar,Ramani Satishkumar,Shail Desai,Neeraj Gupta,Xin Wang,Shu Hu*

Main category: cs.MA

TL;DR: This paper introduces MedChat, a diagnostic platform combining specialized image models with multiple role-specific language agents to improve clinical accuracy and reporting in glaucoma detection.


<details>
  <summary>Details</summary>
Motivation: The current challenges in applying general language models to medical imaging include hallucinations, lack of interpretability, and insufficient medical knowledge, which limit clinical accuracy and reliability.

Method: MedChat employs a multi-agent framework where specialized vision models and multiple role-specific LLM agents collaborate under the coordination of a director agent, featuring an interface for clinical and educational use.

Result: The proposed system improves reliability, reduces hallucinations, and enables interactive and tailored diagnostic reporting for clinical contexts.

Conclusion: MedChat effectively addresses the limitations of single-agent approaches by leveraging a team structure, thus enhancing diagnostic and educational applications in medical imaging.

Abstract: The integration of deep learning-based glaucoma detection with large language
models (LLMs) presents an automated strategy to mitigate ophthalmologist
shortages and improve clinical reporting efficiency. However, applying general
LLMs to medical imaging remains challenging due to hallucinations, limited
interpretability, and insufficient domain-specific medical knowledge, which can
potentially reduce clinical accuracy. Although recent approaches combining
imaging models with LLM reasoning have improved reporting, they typically rely
on a single generalist agent, restricting their capacity to emulate the diverse
and complex reasoning found in multidisciplinary medical teams. To address
these limitations, we propose MedChat, a multi-agent diagnostic framework and
platform that combines specialized vision models with multiple role-specific
LLM agents, all coordinated by a director agent. This design enhances
reliability, reduces hallucination risk, and enables interactive diagnostic
reporting through an interface tailored for clinical review and educational
use. Code available at https://github.com/Purdue-M2/MedChat.

</details>


### [724] [Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments](https://arxiv.org/abs/2506.07232)
*Xinran Li,Chenjia Bai,Zijian Li,Jiakun Zheng,Ting Xiao,Jun Zhang*

Main category: cs.MA

TL;DR: This paper introduces a new framework, 'Learn as Individuals, Evolve as a Team' (LIET), to improve large language models' (LLMs) adaptation and planning in multi-agent embodied environments.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based planning algorithms struggle to adapt effectively to multi-agent embodied scenarios despite their advanced reasoning capabilities and modular design.

Method: The LIET paradigm involves individual learning of a local utility function for environment understanding and team-level updating of a shared cooperation knowledge list for improved communication.

Result: The LIET framework, tested on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks, demonstrated improved cooperative planning abilities compared to existing baselines.

Conclusion: Combining individual learning and team-level evolution allows LLM agents to achieve better planning and cooperative adaptation in multi-agent scenarios.

Abstract: Large language models (LLMs) possess extensive knowledge bases and strong
reasoning capabilities, making them promising tools for complex, multi-agent
planning in embodied environments. However, despite LLMs' advanced abilities
and the sophisticated modular design of agentic methods, existing LLM-based
planning algorithms remain limited by weak adaptation capabilities to
multi-agent embodied scenarios. We address this limitation by introducing a
framework that enables LLM agents to learn and evolve both before and during
test time, equipping them with environment-relevant knowledge for better
planning and enhanced communication for improved cooperation. Inspired by
centralized training with decentralized execution in multi-agent reinforcement
learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)}
paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents
learn a local utility function from exploratory datasets to better comprehend
the embodied environment, which is then queried during test time to support
informed decision-making. At the team level, LLM agents collaboratively and
iteratively maintain and update a shared cooperation knowledge list based on
new experiences, using it to guide more effective communication. By combining
individual learning with team evolution, LIET enables comprehensive and
flexible adaptation for LLM agents. Our experiments on Communicative
Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate
that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing
baselines and exhibits strong cooperative planning abilities.

</details>


### [725] [Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents](https://arxiv.org/abs/2506.07388)
*Yun Hua,Haosheng Chen,Shiqin Wang,Wenhao Li,Xiangfeng Wang,Jun Luo*

Main category: cs.MA

TL;DR: The paper introduces Shapley-Coop, a workflow for enhancing collaboration in multi-agent systems by utilizing rational task-time pricing and reward redistribution based on marginal contributions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of coordination in multi-agent systems where agents often act in self-interested ways, leading to issues in fair credit assignment and compensation.

Method: The authors propose Shapley-Coop, which combines Shapley Chain-of-Thought for principled pricing based on contributions with structured negotiation protocols for effective price matching and reward redistribution.

Result: Experiments across two multi-agent games and a software engineering simulation show Shapley-Coop improves collaboration and equitable credit assignment among LLM agents.

Conclusion: Shapley-Coop’s pricing mechanisms successfully enhance LLM coordination, align incentives, and provide fair compensation, showcasing its potential in complex human-AI collaborations.

Abstract: Large Language Models (LLMs) show strong collaborative performance in
multi-agent systems with predefined roles and workflows. However, in open-ended
environments lacking coordination rules, agents tend to act in self-interested
ways. The central challenge in achieving coordination lies in credit assignment
-- fairly evaluating each agent's contribution and designing pricing mechanisms
that align their heterogeneous goals. This problem is critical as LLMs
increasingly participate in complex human-AI collaborations, where fair
compensation and accountability rely on effective pricing mechanisms. Inspired
by how human societies address similar coordination challenges (e.g., through
temporary collaborations such as employment or subcontracting), we propose a
cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley
Chain-of-Thought -- leveraging marginal contributions as a principled basis for
pricing -- with structured negotiation protocols for effective price matching,
enabling LLM agents to coordinate through rational task-time pricing and
post-task reward redistribution. This approach aligns agent incentives, fosters
cooperation, and maintains autonomy. We evaluate Shapley-Coop across two
multi-agent games and a software engineering simulation, demonstrating that it
consistently enhances LLM agent collaboration and facilitates equitable credit
assignment. These results highlight the effectiveness of Shapley-Coop's pricing
mechanisms in accurately reflecting individual contributions during task
execution.

</details>


### [726] [Diffusion of Responsibility in Collective Decision Making](https://arxiv.org/abs/2506.07935)
*Pavel Naumov,Jia Tao*

Main category: cs.MA

TL;DR: This paper explores diffusion of responsibility in collective decision-making mechanisms and concludes that diffusion-free systems require a 'dictator.'


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of obscured individual accountability within collaborative decision-making frameworks.

Method: The authors use bisimulation on decision-making mechanisms and analyze its impact on responsibility, applying it to determine the smallest bisimilar structure.

Result: In two-agent systems, avoiding diffusion requires a single agent acting as a dictator. In multi-agent systems, diffusion-free mechanisms are characterized by an elected dictatorship.

Conclusion: Diffusion of responsibility cannot be completely avoided in collective decision-making except through dictatorship-like mechanisms, whether unilateral or elected.

Abstract: The term "diffusion of responsibility'' refers to situations in which
multiple agents share responsibility for an outcome, obscuring individual
accountability. This paper examines this frequently undesirable phenomenon in
the context of collective decision-making mechanisms.
  The work shows that if a decision is made by two agents, then the only way to
avoid diffusion of responsibility is for one agent to act as a "dictator'',
making the decision unilaterally. In scenarios with more than two agents, any
diffusion-free mechanism is an "elected dictatorship'' where the agents elect a
single agent to make a unilateral decision.
  The technical results are obtained by defining a bisimulation of
decision-making mechanisms, proving that bisimulation preserves
responsibility-related properties, and establishing the results for a smallest
bisimular mechanism.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [727] [Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics](https://arxiv.org/abs/2506.06286)
*Kevin Baum*

Main category: cs.CY

TL;DR: The paper introduces a structured framework to clarify AI alignment concepts, addressing the ambiguity in boundaries of AI Safety, AI Alignment, and Machine Ethics.


<details>
  <summary>Details</summary>
Motivation: The urgent need to address safety and normative expectations for AI systems operating beyond controlled environments.

Method: Developing a taxonomy based on alignment aim, scope, and constituency to structure the concept of AI alignment.

Result: A framework that defines multiple configurations of AI alignment, aiding integration in the interdisciplinary field.

Conclusion: The proposed framework helps clarify the concept of alignment across domains, contributing both practically and philosophically.

Abstract: Recent advances in AI research make it increasingly plausible that artificial
agents with consequential real-world impact will soon operate beyond tightly
controlled environments. Ensuring that these agents are not only safe but that
they adhere to broader normative expectations is thus an urgent
interdisciplinary challenge. Multiple fields -- notably AI Safety, AI
Alignment, and Machine Ethics -- claim to contribute to this task. However, the
conceptual boundaries and interrelations among these domains remain vague,
leaving researchers without clear guidance in positioning their work.
  To address this meta-challenge, we develop a structured conceptual framework
for understanding AI alignment. Rather than focusing solely on alignment goals,
we introduce a taxonomy distinguishing the alignment aim (safety, ethicality,
legality, etc.), scope (outcome vs. execution), and constituency (individual
vs. collective). This structural approach reveals multiple legitimate alignment
configurations, providing a foundation for practical and philosophical
integration across domains, and clarifying what it might mean for an agent to
be aligned all-things-considered.

</details>


### [728] [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
*Daniel Thilo Schroeder,Meeyoung Cha,Andrea Baronchelli,Nick Bostrom,Nicholas A. Christakis,David Garcia,Amit Goldenberg,Yara Kyrychenko,Kevin Leyton-Brown,Nina Lutz,Gary Marcus,Filippo Menczer,Gordon Pennycook,David G. Rand,Frank Schweitzer,Christopher Summerfield,Audrey Tang,Jay Van Bavel,Sander van der Linden,Dawn Song,Jonas R. Kunst*

Main category: cs.CY

TL;DR: The paper discusses the threat of coordinated AI swarms in spreading disinformation and advocates preventive measures at platform, model, and system levels.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the escalating threat posed by advanced AI systems capable of conducting large-scale, malicious disinformation campaigns that could undermine democratic systems.

Method: The paper proposes a three-pronged response involving platform-side defenses, model-side safeguards, and system-level oversight to counter AI-driven misinformation.

Result: Identified risks of AI swarms include fabricated consensus, voter suppression, and institutional mistrust. The proposed measures aim to mitigate these risks and strengthen defenses against AI disinformation.

Conclusion: Effective measures at multiple levels are necessary to counteract the growing threat of disinformation campaigns orchestrated by advanced AI systems.

Abstract: Advances in AI portend a new era of sophisticated disinformation operations.
While individual AI systems already create convincing -- and at times
misleading -- information, an imminent development is the emergence of
malicious AI swarms. These systems can coordinate covertly, infiltrate
communities, evade traditional detectors, and run continuous A/B tests, with
round-the-clock persistence. The result can include fabricated grassroots
consensus, fragmented shared reality, mass harassment, voter micro-suppression
or mobilization, contamination of AI training data, and erosion of
institutional trust. With democratic processes worldwide increasingly
vulnerable, we urge a three-pronged response: (1) platform-side defenses --
always-on swarm-detection dashboards, pre-election high-fidelity
swarm-simulation stress-tests, transparency audits, and optional client-side
"AI shields" for users; (2) model-side safeguards -- standardized
persuasion-risk tests, provenance-authenticating passkeys, and watermarking;
and (3) system-level oversight -- a UN-backed AI Influence Observatory.

</details>


### [729] [Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor](https://arxiv.org/abs/2506.06383)
*Qian Huang,King Wang Poon*

Main category: cs.CY

TL;DR: This study explores how AI can collaborate with human instructors in fitness education, focusing on integrating generative AI into Pilates class planning.


<details>
  <summary>Details</summary>
Motivation: To address the unclear role of AI alongside human expertise in teaching and coaching, specifically in how generative AI can enhance fitness education.

Method: A one-year qualitative case study involving biweekly interviews and observation of a Pilates instructor's classes to understand generative AI's potential integration.

Result: Insights on the collaborative dynamics between generative AI and human instructors during class planning and instruction.

Conclusion: Generative AI shows promise in aiding fitness education but further exploration is needed for optimal integration alongside human expertise.

Abstract: Artificial intelligence is poised to transform teaching and coaching
practices,yet its optimal role alongside human expertise remains unclear.This
study investigates human and AI collaboration in fitness education through a
one year qualitative case study with a Pilates instructor.The researcher
participated in the instructor classes and conducted biweekly semi structured
interviews to explore how generative AI could be integrated into class planning
and instruction.

</details>


### [730] [Benchmarking Large Language Models on Homework Assessment in Circuit Analysis](https://arxiv.org/abs/2506.06390)
*Liangliang Chen,Zhihao Qin,Yiming Guo,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: This study evaluates large language models like GPT-3.5 Turbo, GPT-4o, and Llama 3 70B for assessing undergraduate circuit analysis homework, with results showing GPT-4o and Llama 3 70B performing better across key metrics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the integration of large language models in engineering education, especially for automating homework assessment in circuit analysis.

Method: The authors develop a dataset of student and reference LaTeX-formatted solutions and benchmark the performance of LLMs against five metrics: completeness, method, final answer, arithmetic errors, and units.

Result: GPT-4o and Llama 3 70B outperformed GPT-3.5 Turbo in all metrics, each excelling in different evaluation aspects.

Conclusion: LLMs show promise for reliable homework assessment in engineering education, but several limitations need to be addressed for creating personalized tutors. The methods can also be generalized for other courses.

Abstract: Large language models (LLMs) have the potential to revolutionize various
fields, including code development, robotics, finance, and education, due to
their extensive prior knowledge and rapid advancements. This paper investigates
how LLMs can be leveraged in engineering education. Specifically, we benchmark
the capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama
3 70B, in assessing homework for an undergraduate-level circuit analysis
course. We have developed a novel dataset consisting of official reference
solutions and real student solutions to problems from various topics in circuit
analysis. To overcome the limitations of image recognition in current
state-of-the-art LLMs, the solutions in the dataset are converted to LaTeX
format. Using this dataset, a prompt template is designed to test five metrics
of student solutions: completeness, method, final answer, arithmetic error, and
units. The results show that GPT-4o and Llama 3 70B perform significantly
better than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B
each having distinct advantages in different evaluation aspects. Additionally,
we present insights into the limitations of current LLMs in several aspects of
circuit analysis. Given the paramount importance of ensuring reliability in
LLM-generated homework assessment to avoid misleading students, our results
establish benchmarks and offer valuable insights for the development of a
reliable, personalized tutor for circuit analysis -- a focus of our future
work. Furthermore, the proposed evaluation methods can be generalized to a
broader range of courses for engineering education in the future.

</details>


### [731] [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391)
*John Mavi,Diana Teodora Găitan,Sergio Coronado*

Main category: cs.CY

TL;DR: This paper evaluates eight LLMs on their compliance with International Humanitarian Law (IHL) and ability to provide clear and constructive refusals to unlawful prompts.


<details>
  <summary>Details</summary>
Motivation: The alignment of Large Language Models (LLMs) with International Humanitarian Law (IHL) is unclear, necessitating an evaluation of their compliance and ability to refuse unlawful requests constructively.

Method: The study tested eight LLMs on their ability to reject explicit violations of IHL. It introduced a standardized safety prompt to enhance the clarity and quality of refusals and examined the models' responses to advanced, technical prompts.

Result: Most LLMs successfully refused unlawful requests, but their responses varied in clarity and consistency. Standardized safety prompts improved explanatory refusals but technical prompts exposed vulnerabilities.

Conclusion: The paper suggests that lightweight interventions like safety prompts enhance clarity and compliance, while identifying areas needing improvement for safer and more transparent AI systems.

Abstract: Large Language Models (LLMs) are widely used across sectors, yet their
alignment with International Humanitarian Law (IHL) is not well understood.
This study evaluates eight leading LLMs on their ability to refuse prompts that
explicitly violate these legal frameworks, focusing also on helpfulness - how
clearly and constructively refusals are communicated. While most models
rejected unlawful requests, the clarity and consistency of their responses
varied. By revealing the model's rationale and referencing relevant legal or
safety principles, explanatory refusals clarify the system's boundaries, reduce
ambiguity, and help prevent misuse. A standardised system-level safety prompt
significantly improved the quality of the explanations expressed within
refusals in most models, highlighting the effectiveness of lightweight
interventions. However, more complex prompts involving technical language or
requests for code revealed ongoing vulnerabilities. These findings contribute
to the development of safer, more transparent AI systems and propose a
benchmark to evaluate the compliance of LLM with IHL.

</details>


### [732] [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)
*Lingyao Li,Dawei Li,Zhenhui Ou,Xiaoran Xu,Jingxiao Liu,Zihui Ma,Runlong Yu,Min Deng*

Main category: cs.CY

TL;DR: This paper explores the use of large language models (LLMs) to simulate earthquake impacts, achieving high accuracy in predicting MMI scores at localized scales.


<details>
  <summary>Details</summary>
Motivation: To improve proactive disaster preparedness by leveraging advancements in LLMs to accurately simulate earthquake impacts.

Method: The study uses multimodal datasets, including geospatial, socioeconomic, building data, and imagery, to train LLMs for Modified Mercalli Intensity (MMI) predictions at zip code and county scales.

Result: The framework achieves strong correlation (0.88) and low RMSE (0.77) in simulating perceived earth impacts during two case studies: the 2014 Napa and 2019 Ridgecrest earthquakes. Visual inputs significantly enhance accuracy.

Conclusion: LLMs show great potential in simulating disaster scenarios, providing a valuable tool for improving pre-event disaster response strategies.

Abstract: Efficient simulation is essential for enhancing proactive preparedness for
sudden-onset disasters such as earthquakes. Recent advancements in large
language models (LLMs) as world models show promise in simulating complex
scenarios. This study examines multiple LLMs to proactively estimate perceived
earthquake impacts. Leveraging multimodal datasets including geospatial,
socioeconomic, building, and street-level imagery data, our framework generates
Modified Mercalli Intensity (MMI) predictions at zip code and county scales.
Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did
You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced
by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real
reports at the zip code level. Techniques such as RAG and ICL can improve
simulation performance, while visual inputs notably enhance accuracy compared
to structured numerical data alone. These findings show the promise of LLMs in
simulating disaster impacts that can help strengthen pre-event planning.

</details>


### [733] [Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches](https://arxiv.org/abs/2506.06540)
*Patrick Y. Wu*

Main category: cs.CY

TL;DR: The paper discusses using large language models (LLMs) as substitutes for expert surveys when traditional methods fail, demonstrated through a case study on federal layoffs in 2025.


<details>
  <summary>Details</summary>
Motivation: Traditional measurement methods fail to capture pre-event perceptions after a disruptive event or shock, limiting the ability to study related factors.

Method: The authors used LLMs to generate ideology scores and analyze perceptions of federal agencies through pairwise comparison prompts, applying the method to a case study of the DOGE layoffs.

Result: LLM-derived scores replicated pre-event expert measures, predicted targeted agencies, and identified perceptions of agencies as knowledge institutions as key predictors.

Conclusion: LLMs provide a viable alternative for studying factors behind shocks when traditional methods are disrupted; a two-part criterion is suggested for their use.

Abstract: After a disruptive event or shock, such as the Department of Government
Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by
knowledge of the outcome. This can make it difficult or impossible to
reconstruct the pre-event perceptions needed to study the factors associated
with the event. This position paper argues that large language models (LLMs),
trained on vast amounts of digital media data, can be a viable substitute for
expert political surveys when a shock disrupts traditional measurement. We
analyze the DOGE layoffs as a specific case study for this position. We use
pairwise comparison prompts with LLMs and derive ideology scores for federal
executive agencies. These scores replicate pre-layoff expert measures and
predict which agencies were targeted by DOGE. We also use this same approach
and find that the perceptions of certain federal agencies as knowledge
institutions predict which agencies were targeted by DOGE, even when
controlling for ideology. This case study demonstrates that using LLMs allows
us to rapidly and easily test the associated factors hypothesized behind the
shock. More broadly, our case study of this recent event exemplifies how LLMs
offer insights into the correlational factors of the shock when traditional
measurement techniques fail. We conclude by proposing a two-part criterion for
when researchers can turn to LLMs as a substitute for expert political surveys.

</details>


### [734] [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
*Yijia Shao,Humishka Zope,Yucheng Jiang,Jiaxin Pei,David Nguyen,Erik Brynjolfsson,Diyi Yang*

Main category: cs.CY

TL;DR: The paper introduces an auditing framework to evaluate workers' preferences for AI automation or augmentation of occupational tasks and correlates these preferences with AI technological capabilities.


<details>
  <summary>Details</summary>
Motivation: Concerns about job displacement and the impact of AI agents on human agency and automation necessitate a structured understanding of worker-AI interaction in the evolving labor market.

Method: An audio-enhanced mini-interview approach was developed, featuring the Human Agency Scale (HAS), and the WORKBank database was created using inputs from workers and AI experts.

Result: Workers' desired automation/augmentation levels were mapped into four zones based on alignment with technological capabilities, revealing mismatches and R&D opportunities. HAS profiles varied across occupations.

Conclusion: AI agent development should align with human preferences, highlighting shifts in workplace skills from information-focused to interpersonal competencies, and preparing workers for these transformations is crucial.

Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the
labor market, raising concerns about job displacement, diminished human agency,
and overreliance on automation. Yet, we lack a systematic understanding of the
evolving landscape. In this paper, we address this gap by introducing a novel
auditing framework to assess which occupational tasks workers want AI agents to
automate or augment, and how those desires align with the current technological
capabilities. Our framework features an audio-enhanced mini-interview to
capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a
shared language to quantify the preferred level of human involvement. Using
this framework, we construct the WORKBank database, building on the U.S.
Department of Labor's O*NET database, to capture preferences from 1,500 domain
workers and capability assessments from AI experts across over 844 tasks
spanning 104 occupations. Jointly considering the desire and technological
capability divides tasks in WORKBank into four zones: Automation "Green Light"
Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone.
This highlights critical mismatches and opportunities for AI agent development.
Moving beyond a simple automate-or-not dichotomy, our results reveal diverse
HAS profiles across occupations, reflecting heterogeneous expectations for
human involvement. Moreover, our study offers early signals of how AI agent
integration may reshape the core human competencies, shifting from
information-focused skills to interpersonal ones. These findings underscore the
importance of aligning AI agent development with human desires and preparing
workers for evolving workplace dynamics.

</details>


### [735] [Position: Simulating Society Requires Simulating Thought](https://arxiv.org/abs/2506.06958)
*Chance Jiajie Li,Jiayi Wu,Zhenze Mo,Ao Qu,Yuhan Tang,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Jinhua Zhao,Paul Liang,Luis Alonso,Kent Larson*

Main category: cs.CY

TL;DR: The paper critiques the current state of LLM-based agents in simulating social behaviors, proposing the "Generative Minds" (GenMinds) paradigm to improve reasoning structure and belief traceability.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents fail to accurately emulate human reasoning and social behavior, especially lacking coherence, causal reasoning, and traceability.

Method: The authors propose the Generative Minds (GenMinds) paradigm for structured cognitive modeling, and introduce the RECAP framework to benchmark reasoning fidelity and simulate thought processes.

Result: GenMinds enhances belief representations in agents, while RECAP evaluates these agents' causal paths, demographic grounding, and intervention responses.

Conclusion: Shifting from mimicry to thought simulation offers a more reliable model for societal analysis using generative agents.

Abstract: Simulating society with large language models (LLMs), we argue, requires more
than generating plausible behavior -- it demands cognitively grounded reasoning
that is structured, revisable, and traceable. LLM-based agents are increasingly
used to emulate individual and group behavior -- primarily through prompting
and supervised fine-tuning. Yet they often lack internal coherence, causal
reasoning, and belief traceability -- making them unreliable for analyzing how
people reason, deliberate, or respond to interventions.
  To address this, we present a conceptual modeling paradigm, Generative Minds
(GenMinds), which draws from cognitive science to support structured belief
representations in generative agents. To evaluate such agents, we introduce the
RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess
reasoning fidelity via causal traceability, demographic grounding, and
intervention consistency. These contributions advance a broader shift: from
surface-level mimicry to generative agents that simulate thought -- not just
language -- for social simulations.

</details>


### [736] [Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research](https://arxiv.org/abs/2506.06377)
*Giuseppe Arbia,Luca Morandini,Vincenzo Nardelli*

Main category: cs.CY

TL;DR: This study tests how well Large Language Models (LLMs) can evaluate the economic and theoretical validity of findings in spatial econometrics, showing mixed results.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' potential to assist in reviewing econometric research, focusing on their ability to assess empirical findings.

Method: The authors created summaries of real and altered research papers and had various LLMs evaluate them on structured criteria like variable choice and publication suitability.

Result: LLMs showed strong performance in evaluating variable coherence (e.g., GPT-4 achieved 0.87 F1 score) but struggled with deeper assessments like coefficient plausibility.

Conclusion: LLMs can assist in initial reviews but lack the ability to fully replace human oversight for nuanced economic evaluations.

Abstract: This paper investigates Large Language Models (LLMs) ability to assess the
economic soundness and theoretical consistency of empirical findings in spatial
econometrics. We created original and deliberately altered "counterfactual"
summaries from 28 published papers (2005-2024), which were evaluated by a
diverse set of LLMs. The LLMs provided qualitative assessments and structured
binary classifications on variable choice, coefficient plausibility, and
publication suitability. The results indicate that while LLMs can expertly
assess the coherence of variable choices (with top models like GPT-4o achieving
an overall F1 score of 0.87), their performance varies significantly when
evaluating deeper aspects such as coefficient plausibility and overall
publication suitability. The results further revealed that the choice of LLM,
the specific characteristics of the paper and the interaction between these two
factors significantly influence the accuracy of the assessment, particularly
for nuanced judgments. These findings highlight LLMs' current strengths in
assisting with initial, more surface-level checks and their limitations in
performing comprehensive, deep economic reasoning, suggesting a potential
assistive role in peer review that still necessitates robust human oversight.

</details>


### [737] [Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on Digital Trust](https://arxiv.org/abs/2506.07363)
*Claudiu Popa,Rex Pallath,Liam Cunningham,Hewad Tahiri,Abiram Kesavarajah,Tao Wu*

Main category: cs.CY

TL;DR: The paper investigates the increasing prevalence and implications of deepfake technology, emphasizing its risks to digital trust and authenticity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address concerns over the reduced financial and technical barriers enabling widespread use of deepfake tools, which threatens trust, privacy, and security.

Method: Utilizing accessible tools like Runway, Rope, and ElevenLabs, the study demonstrates the creation of realistic deepfakes and analyzes their ethical and security risks.

Result: The paper highlights the ease with which convincing deepfakes can be produced using low-resource tools while exposing their impact on individuals and organizations.

Conclusion: To counteract challenges posed by deepfake technology, regulatory frameworks, public education, and collaborative solutions are essential to sustain trust in digital media.

Abstract: Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on
Digital Trust. With the increasing accessibility of generative AI, tools for
voice cloning, face-swapping, and synthetic media creation have advanced
significantly, lowering both financial and technical barriers for their use.
While these technologies present innovative opportunities, their rapid growth
raises concerns about trust, privacy, and security. This white paper explores
the implications of deepfake technology, analyzing its role in enabling fraud,
misinformation, and the erosion of authenticity in multimedia. Using
cost-effective, easy to use tools such as Runway, Rope, and ElevenLabs, we
explore how realistic deepfakes can be created with limited resources,
demonstrating the risks posed to individuals and organizations alike. By
analyzing the technical and ethical challenges of deepfake mitigation and
detection, we emphasize the urgent need for regulatory frameworks, public
awareness, and collaborative efforts to maintain trust in digital media.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [738] [Linear Discriminant Analysis with Gradient Optimization on Covariance Inverse](https://arxiv.org/abs/2506.06845)
*Cencheng Shen,Yuexiao Dong*

Main category: stat.CO

TL;DR: The paper introduces LDA-GO, an improved linear discriminant analysis approach that tackles high-dimensional issues by optimizing the inverse covariance matrix using gradient descent.


<details>
  <summary>Details</summary>
Motivation: Address the instability of classical Linear Discriminant Analysis (LDA) in high-dimensional settings due to covariance estimation issues.

Method: LDA-GO applies gradient descent to optimize the inverse covariance matrix, parametrizes it via Cholesky factorization, integrates a low-rank extension, and uses multiple-initialization strategies.

Result: LDA-GO's performance is validated through comprehensive multivariate simulations and experiments on real data.

Conclusion: LDA-GO significantly enhances classical LDA's applicability and reliability in high-dimensional classification scenarios.

Abstract: Linear discriminant analysis (LDA) is a fundamental method in statistical
pattern recognition and classification, achieving Bayes optimality under
Gaussian assumptions. However, it is well-known that classical LDA may struggle
in high-dimensional settings due to instability in covariance estimation. In
this work, we propose LDA with gradient optimization (LDA-GO), a new approach
that directly optimizes the inverse covariance matrix via gradient descent. The
algorithm parametrizes the inverse covariance matrix through Cholesky
factorization, incorporates a low-rank extension to reduce computational
complexity, and considers a multiple-initialization strategy, including
identity initialization and warm-starting from the classical LDA estimates. The
effectiveness of LDA-GO is demonstrated through extensive multivariate
simulations and real-data experiments.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [739] [Active Lubrication of Transluminal Medical Instruments](https://arxiv.org/abs/2506.07225)
*Mostafa A. Atalla,Jelte Nieuwenhuis,Alan Martin,Xuan Wang,Ahranee Canden,Matt J. Carré,Roger Lewis,Aimée Sakes,Michaël Wiertlewski*

Main category: physics.med-ph

TL;DR: The paper introduces ultrasonic vibrations for active lubrication to reduce risks in transluminal minimally invasive surgeries.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address risks like instrument friction, perforation, and poor haptic feedback in transluminal interventions.

Method: Ultrasonic vibrations at the instrument surface create pressurized fluid layers for active lubrication to reduce friction.

Result: Friction was reduced by up to 42% on soft tissue and up to 82% on rigid substrates; avoided buckling and maintained safe temperatures.

Conclusion: Active lubrication enhances the safety and efficacy by minimizing injury risks and improving procedural stability.

Abstract: Transluminal minimally invasive surgery uses natural orifices and small
incisions to access internal anatomical structures, promoting quicker recovery
and reduced morbidity. However, navigating instruments--catheters and
endoscopes--through anatomical pathways creates frictional interactions with
luminal walls, risking complications such as perforation, poor haptic feedback,
and instrument buckling. In this paper, we present a new approach to actively
lubricate transluminal instruments and dynamically reduce friction with
surrounding tissues. This approach employs ultrasonic vibrations, at the
instrument surface, to generate a pressurized fluid layer at the contact
interface, lubricating the interface and thereby reducing friction. We
implemented this approach in a prototype catheter, which we validated under dry
and liquid-lubricated conditions, across rigid and soft interfaces, and along
varied anatomical curvatures. In a cardiac catheter use case, active
lubrication reduced friction by up to 42% on ex-vivo porcine aorta tissue and
82% on rigid substrates, denoting its potential performance on healthy and
calcified tissue, respectively. Thermal imaging confirmed that temperature at
the tissue-catheter interface remained within safe limits. Additionally, the
system effectively prevented buckling during catheter insertion experiment,
further showcasing its potential. By minimizing injury risk and enhancing
procedural stability, active lubrication can drastically enhance the safety and
efficacy of transluminal interventions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [740] [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: GOLFer is a method that uses smaller language models for effective query expansion in information retrieval, addressing cost, accessibility, and computational concerns.


<details>
  <summary>Details</summary>
Motivation: To overcome the expensive computational requirements and limited accessibility of large language models in query expansion for information retrieval.

Method: GOLFer uses smaller language models combined with two key modules: a hallucination filter to exclude non-factual content and a document combiner to balance query and document influence.

Result: Experimental results show that GOLFer outperforms existing query expansion methods with small LMs and rivals the performance of those using larger LMs.

Conclusion: GOLFer offers an efficient, accessible alternative to large-scale LMs, ensuring competitive query expansion performance while significantly reducing resource demands.

Abstract: Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.

</details>


### [741] [DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval](https://arxiv.org/abs/2506.06313)
*Huiyao Chen,Yi Yang,Yinghui Li,Meishan Zhang,Min Zhang*

Main category: cs.IR

TL;DR: DISRetrieval introduces a hierarchical retrieval framework based on discourse structure to enhance long document understanding, outperforming existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: To address the context length limitations of large language models and improve long document understanding by leveraging the natural discourse structure that humans use for comprehension.

Method: DISRetrieval introduces a hierarchical structure based on rhetorical structure theory (RST) for document organization, enhances node representation with LLMs and adaptive summarization, and employs hierarchical evidence retrieval to maintain coherence.

Result: The method shows significant improvements on QASPER and QuALITY datasets in token-level retrieval metrics and downstream question answering tasks, verified by ablation studies.

Conclusion: Incorporating linguistic discourse structure into retrieval mechanisms enables better performance in long document understanding, offering a more coherent and effective approach.

Abstract: Long document understanding has become increasingly crucial in natural
language processing, with retrieval-based methods emerging as a promising
solution to address the context length limitations of large language models
(LLMs). However, existing approaches either treat documents as flat sequences
or employ arbitrary chunking strategies, failing to capture the inherent
discourse structure that guides human comprehension. We present DISRetrieval, a
novel hierarchical retrieval framework that leverages linguistic discourse
structure to enhance long document understanding. Our approach introduces three
key innovations: (1) a discourse-aware document organization framework that
utilizes rhetorical structure theory (RST) to create sentence-level
hierarchical representations, preserving both semantic relationships and
natural document flow; (2) an LLM-enhanced node representation technique that
combines discourse structure with adaptive summarization to enrich tree nodes
with contextual information; and (3) a hierarchical evidence retrieval
mechanism that effectively selects relevant content while maintaining discourse
coherence. Through comprehensive experiments on QASPER and QuALITY datasets,
DISRetrieval demonstrates substantial improvements over existing methods in
both token-level retrieval metrics and downstream question answering tasks. Our
ablation studies confirm that incorporating discourse structure significantly
enhances retrieval effectiveness across different document lengths and query
types, validating the importance of linguistically-informed document
representation in long-text understanding. Our code and datasets are publicly
available at github/DreamH1gh/DISRetrieval to facilitate future research.

</details>


### [742] [A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing](https://arxiv.org/abs/2506.06316)
*Haoyang Feng,Yanjun Dai,Yuan Gao*

Main category: cs.IR

TL;DR: The paper introduces RL-LLM-AB, a framework for automating and personalizing A/B testing using reinforcement learning and large language models to maximize marketing effectiveness.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of optimizing A/B testing for personalized marketing to enhance user responses and overcome limitations in traditional and modern methods.

Method: The proposed RL-LLM-AB framework employs a language model to generate content variations, incorporates user and context information, uses Actor-Critic reinforcement learning for version selection, and integrates a Memory-Augmented Reward Estimator for capturing long-term user preference shifts.

Result: Numerical experiments demonstrate that RL-LLM-AB outperforms traditional A/B testing, Contextual Bandits, and other reinforcement learning methods on real-world marketing datasets.

Conclusion: The proposed RL-LLM-AB framework effectively automates and optimizes A/B testing for personalized marketing, offering superior results and the ability to adapt to long-term user preferences.

Abstract: For personalized marketing, a new challenge of how to effectively algorithm
the A/B testing to maximize user response is urgently to be overcome. In this
paper, we present a new approach, the RL-LLM-AB test framework, for using
reinforcement learning strategy optimization combined with LLM to automate and
personalize A/B tests. The RL-LLM-AB test is built upon the pre-trained
instruction-tuned language model. It first generates A/B versions of candidate
content variants using a Prompt-Conditioned Generator, and then dynamically
embeds and fuses the user portrait and the context of the current query with
the multi-modal perception module to constitute the current interaction state.
The content version is then selected in real-time through the policy
optimization module with an Actor-Critic structure, and long-term revenue is
estimated according to real-time feedback (such as click-through rate and
conversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded
into the framework to capture long-term user preference drift, which helps to
generalize policy across multiple users and content contexts. Numerical results
demonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B
testing methods, including classical A/B testing, Contextual Bandits, and
benchmark reinforcement learning approaches on real-world marketing data.

</details>


### [743] [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)
*Xuan Xu,Fufang Wen,Beilin Chu,Zhibing Fu,Qinhong Lin,Jiaqi Liu,Binjie Fei,Zhongliang Yang,Linna Zhou,Yu Li*

Main category: cs.IR

TL;DR: The paper introduces FinBERT2, a specialized financial-specific bidirectional language model, addressing limitations of large language models (LLMs) in financial tasks. FinBERT2 achieves better performance in discriminative classification, domain-specific retrieval, and topic modeling tasks compared to both fine-tuned BERT models and state-of-the-art LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing large language models (LLMs) like GPT-3 struggle in the financial domain due to suboptimal performance in discriminatory tasks, reliance on retrieval augmentation, and inadequacy in feature-based scenarios like topic modeling.

Method: The authors developed FinBERT2, a bidirectional encoder pretrained on a large financial-specific corpus of 32 billion tokens. This backbone model supports fine-tuned variants for classification (Fin-Labelers), domain-specific retrieval (Fin-Retrievers), and topic modeling (Fin-TopicModel).

Result: FinBERT2 outperformed fine-tuned BERT variants by up to 3.3% and LLMs by 9.7%-12.3% in financial classification tasks. Its retrieval variant achieved 4.2%-6.8% better performance than leading proprietary and open-source embedding models for financial tasks.

Conclusion: FinBERT2 bridges the gap in financial-specific deployment of LLMs, offering better performance in discriminative, retrieval, and feature-based scenarios. It revisits financial BERT models and provides valuable insights into leveraging domain-specific language models in the LLM era.

Abstract: In natural language processing (NLP), the focus has shifted from encoder-only
tiny language models like BERT to decoder-only large language models(LLMs) such
as GPT-3. However, LLMs' practical application in the financial sector has
revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT
on discriminative tasks despite costing much higher computational resources,
such as market sentiment analysis in financial reports; (2) Application on
generative tasks heavily relies on retrieval augmented generation (RAG) methods
to provide current and specialized information, with general retrievers showing
suboptimal performance on domain-specific retrieval tasks; (3) There are
additional inadequacies in other feature-based scenarios, such as topic
modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained
on a high-quality, financial-specific corpus of 32b tokens. This represents the
largest known Chinese financial pretraining corpus for models of this parameter
size. As a better backbone, FinBERT2 can bridge the gap in the
financial-specific deployment of LLMs through the following achievements: (1)
Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT
variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five
financial classification tasks. (2) Contrastive fine-tuned models
(Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over
BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's
text-embedding-3-large) embedders across five financial retrieval tasks; (3)
Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables
superior clustering and topic representation for financial titles. Our work
revisits financial BERT models through comparative analysis with contemporary
LLMs and offers practical insights for effectively utilizing FinBERT in the
LLMs era.

</details>


### [744] [Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components](https://arxiv.org/abs/2506.06339)
*Jumana Alsubhi,Mohammad D. Alahmadi,Ahmed Alhusayni,Ibrahim Aldailami,Israa Hamdine,Ahmad Shabana,Yazeed Iskandar,Suhayb Khayyat*

Main category: cs.IR

TL;DR: The study evaluates RAG components for Arabic datasets, finding optimal methods for chunking, embedding, reranking, and generation.


<details>
  <summary>Details</summary>
Motivation: To explore and optimize Retrieval-Augmented Generation (RAG) pipelines for the Arabic language, a field that has been underexplored compared to high-resource languages.

Method: The research employed empirical evaluations using the RAGAS framework across four metrics, assessing state-of-the-art components like chunking strategies, embedding models, rerankers, and language models on Arabic datasets.

Result: Sentence-aware chunking was the best for segmentation, BGE-M3 and Multilingual-E5-large were the most effective embedding models, and reranking notably improved data faithfulness. Aya-8B excelled in generation quality over StableLM.

Conclusion: The paper provides detailed insights and guidelines for creating optimized Arabic RAG pipelines, addressing performance across various components.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture
for combining the precision of retrieval systems with the fluency of large
language models. While several studies have investigated RAG pipelines for
high-resource languages, the optimization of RAG components for Arabic remains
underexplored. This study presents a comprehensive empirical evaluation of
state-of-the-art RAG components-including chunking strategies, embedding
models, rerankers, and language models-across a diverse set of Arabic datasets.
Using the RAGAS framework, we systematically compare performance across four
core metrics: context precision, context recall, answer faithfulness, and
answer relevancy. Our experiments demonstrate that sentence-aware chunking
outperforms all other segmentation methods, while BGE-M3 and
Multilingual-E5-large emerge as the most effective embedding models. The
inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness
in complex datasets, and Aya-8B surpasses StableLM in generation quality. These
findings provide critical insights for building high-quality Arabic RAG
pipelines and offer practical guidelines for selecting optimal components
across different document types.

</details>


### [745] [Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support](https://arxiv.org/abs/2506.06340)
*Wu Hao Ran,Xi Xi,Furong Li,Jingyi Lu,Jian Jiang,Hui Huang,Yuzhuan Zhang,Shi Li*

Main category: cs.IR

TL;DR: This paper explores how advanced language models can improve clinical decision support by leveraging diverse data in Electronic Health Records.


<details>
  <summary>Details</summary>
Motivation: To investigate how large language models can utilize the rich and varied data in Electronic Health Records for better clinical decision support.

Method: Analyzing text-based features, medical codes, and addressing the challenges of generalizability and fairness when using advanced language models in healthcare.

Result: The paper highlights potential improvements in semantic representation and data harmonization across institutions via large language models.

Conclusion: Advanced language models can provide a more semantically rich and harmonized framework for EHR-based clinical decision-making, but challenges around fairness and generalizability remain.

Abstract: The advent of large language models (LLMs) has opened new avenues for
analyzing complex, unstructured data, particularly within the medical domain.
Electronic Health Records (EHRs) contain a wealth of information in various
formats, including free text clinical notes, structured lab results, and
diagnostic codes. This paper explores the application of advanced language
models to leverage these diverse data sources for improved clinical decision
support. We will discuss how text-based features, often overlooked in
traditional high dimensional EHR analysis, can provide semantically rich
representations and aid in harmonizing data across different institutions.
Furthermore, we delve into the challenges and opportunities of incorporating
medical codes and ensuring the generalizability and fairness of AI models in
healthcare.

</details>


### [746] [NR4DER: Neural Re-ranking for Diversified Exercise Recommendation](https://arxiv.org/abs/2506.06341)
*Xinghe Cheng,Xufang Zhou,Liangda Fang,Chaobo He,Yuyu Zhou,Weiqi Luo,Zhiguo Gong,Quanlong Guan*

Main category: cs.IR

TL;DR: NR4DER is an advanced exercise recommendation method designed for MOOCs, addressing issues related to learning pace diversity and dropout rates.


<details>
  <summary>Details</summary>
Motivation: Current online exercise recommendation systems struggle with high dropout rates and fail to adapt to diverse student learning paces.

Method: NR4DER employs the mLSTM model for effective exercise filtering, sequence enhancement for inactive students, and neural re-ranking for personalized, diverse recommendations.

Result: Experiments show NR4DER significantly outperforms existing methods on multiple datasets and effectively accommodates students' varying learning paces.

Conclusion: NR4DER improves both the accuracy and diversity of exercise recommendations, meeting individualized learning needs and addressing challenges in online education platforms.

Abstract: With the widespread adoption of online education platforms, an increasing
number of students are gaining new knowledge through Massive Open Online
Courses (MOOCs). Exercise recommendation have made strides toward improving
student learning outcomes. However, existing methods not only struggle with
high dropout rates but also fail to match the diverse learning pace of
students. They frequently face difficulties in adjusting to inactive students'
learning patterns and in accommodating individualized learning paces, resulting
in limited accuracy and diversity in recommendations. To tackle these
challenges, we propose Neural Re-ranking for Diversified Exercise
Recommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to
improve the effectiveness of the exercise filter module. It then employs a
sequence enhancement method to enhance the representation of inactive students,
accurately matches students with exercises of appropriate difficulty. Finally,
it utilizes neural re-ranking to generate diverse recommendation lists based on
individual students' learning histories. Extensive experimental results
indicate that NR4DER significantly outperforms existing methods across multiple
real-world datasets and effectively caters to the diverse learning pace of
students.

</details>


### [747] [Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?](https://arxiv.org/abs/2506.06328)
*Aziida Nanyonga,Joiner Keith,Turhan Ugur,Wild Graham*

Main category: cs.IR

TL;DR: The study compares BERTopic and PLSA methods for topic modeling in aviation safety reports, showing BERTopic has better coherence and interpretability.


<details>
  <summary>Details</summary>
Motivation: Enhancing understanding of patterns in aviation incident data through improved topic modeling techniques.

Method: BERTopic uses transformer-based embeddings and hierarchical clustering; PLSA employs probabilistic modeling via the EM algorithm. Evaluated on 36,000 NTSB aviation reports.

Result: BERTopic achieved higher topic coherence (Cv score 0.41 vs. PLSA's 0.37) and better interpretability confirmed by experts.

Conclusion: Modern transformer-based approaches like BERTopic have advantages in analyzing complex aviation datasets, offering potential for improved insights and decision-making in aviation safety.

Abstract: This study compares the effectiveness of BERTopic and Probabilistic Latent
Semantic Analysis (PLSA) in extracting meaningful topics from aviation safety
reports aiming to enhance the understanding of patterns in aviation incident
data. Using a dataset of over 36,000 National Transportation Safety Board
(NTSB) reports from 2000 to 2020, BERTopic employed transformer based
embeddings and hierarchical clustering, while PLSA utilized probabilistic
modelling through the Expectation-Maximization (EM) algorithm. Results showed
that BERTopic outperformed PLSA in topic coherence, achieving a Cv score of
0.41 compared to PLSA 0.37, while also demonstrating superior interpretability
as validated by aviation safety experts. These findings underscore the
advantages of modern transformer based approaches in analyzing complex aviation
datasets, paving the way for enhanced insights and informed decision-making in
aviation safety. Future work will explore hybrid models, multilingual datasets,
and advanced clustering techniques to further improve topic modelling in this
domain.

</details>


### [748] [LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking](https://arxiv.org/abs/2506.07449)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.IR

TL;DR: LlamaRec-LKG-RAG enhances recommender systems by integrating personalized knowledge graphs with LLMs, enabling effective and interpretable recommendations with significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing recommender systems using Retrieval-Augmented Generation (RAG) fail to utilize the relational structure in user-item interactions, leading to suboptimal personalization.

Method: The paper extends LlamaRec with a user preference module that identifies significant relation paths from a heterogeneous knowledge graph of user behavior and item metadata, incorporating them into prompts for a fine-tuned Llama-2 model.

Result: Experiments on ML-100K and Amazon Beauty datasets show substantial improvements in ranking metrics such as MRR, NDCG, and Recall, surpassing the performance of LlamaRec.

Conclusion: LlamaRec-LKG-RAG demonstrates the importance of structured reasoning in LLM-based recommendations and provides a path for scalable, knowledge-aware personalized systems.

Abstract: Recent advances in Large Language Models (LLMs) have driven their adoption in
recommender systems through Retrieval-Augmented Generation (RAG) frameworks.
However, existing RAG approaches predominantly rely on flat, similarity-based
retrieval that fails to leverage the rich relational structure inherent in
user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,
end-to-end trainable framework that integrates personalized knowledge graph
context into LLM-based recommendation ranking. Our approach extends the
LlamaRec architecture by incorporating a lightweight user preference module
that dynamically identifies salient relation paths within a heterogeneous
knowledge graph constructed from user behavior and item metadata. These
personalized subgraphs are seamlessly integrated into prompts for a fine-tuned
Llama-2 model, enabling efficient and interpretable recommendations through a
unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty
datasets demonstrate consistent and significant improvements over LlamaRec
across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates
the critical value of structured reasoning in LLM-based recommendations and
establishes a foundation for scalable, knowledge-aware personalization in
next-generation recommender systems. Code is available
at~\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.

</details>


### [749] [Preference-based learning for news headline recommendation](https://arxiv.org/abs/2506.06334)
*Alexandre Bouras,Audrey Durand,Richard Khoury*

Main category: cs.IR

TL;DR: The study focuses on improving news headline recommendations utilizing preference-based learning and contextual bandit models. It experiments with how translation affects engagement and investigates various interactive strategies.


<details>
  <summary>Details</summary>
Motivation: To develop more effective strategies for recommending news headlines that improve user engagement, particularly in noisy contexts.

Method: The authors used a contextual bandit model to train a headline recommender agent on real-world user-interaction data from French-language online news posts.

Result: Translation impacts predictions of user engagement, and explicit exploration might be unnecessary in noisy context scenarios, simplifying the recommendation process.

Conclusion: Simpler strategies may be effective in noisy contexts, paving the way for practical recommendations without requiring complex exploration methods.

Abstract: This study explores strategies for optimizing news headline recommendations
through preference-based learning. Using real-world data of user interactions
with French-language online news posts, we learn a headline recommender agent
under a contextual bandit setting. This allows us to explore the impact of
translation on engagement predictions, as well as the benefits of different
interactive strategies on user engagement during data collection. Our results
show that explicit exploration may not be required in the presence of noisy
contexts, opening the door to simpler but efficient strategies in practice.

</details>


### [750] [HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval](https://arxiv.org/abs/2506.07296)
*Arian Askari,Emmanouil Stergiadis,Ilya Gusev,Moran Beladev*

Main category: cs.IR

TL;DR: HotelMatch-LLM is a multimodal dense retrieval model designed for travel property search that surpasses traditional search engines by integrating advanced visual, language, and query processing techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional travel search engines have limitations, requiring users to specify destinations and manually adjust search parameters, making property discovery less intuitive.

Method: The model introduces multi-task optimization, asymmetrical dense retrieval combining SLM and LLM, and extensive image processing to handle property galleries.

Result: HotelMatch-LLM outperforms state-of-the-art models on four test sets, achieving 0.681 against MARVEL's 0.603 on the main query test.

Conclusion: The innovations of HotelMatch-LLM enable scalable, efficient, and accurate travel property search, significantly improving retrieval capability and generalizability across architectures.

Abstract: We present HotelMatch-LLM, a multimodal dense retrieval model for the travel
domain that enables natural language property search, addressing the
limitations of traditional travel search engines which require users to start
with a destination and editing search parameters. HotelMatch-LLM features three
key innovations: (1) Domain-specific multi-task optimization with three novel
retrieval, visual, and language modeling objectives; (2) Asymmetrical dense
retrieval architecture combining a small language model (SLM) for efficient
online query processing and a large language model (LLM) for embedding hotel
data; and (3) Extensive image processing to handle all property image
galleries. Experiments on four diverse test sets show HotelMatch-LLM
significantly outperforms state-of-the-art models, including VISTA and MARVEL.
Specifically, on the test set -- main query type -- we achieve 0.681 for
HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our
analysis highlights the impact of our multi-task optimization, the
generalizability of HotelMatch-LLM across LLM architectures, and its
scalability for processing large image galleries.

</details>


### [751] [Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces](https://arxiv.org/abs/2506.06557)
*Antonio Pariente,Ignacio Hounie,Santiago Segarra,Alejandro Ribeiro*

Main category: cs.IR

TL;DR: The paper introduces a novel approach leveraging q-metric spaces for vector search, offering efficient and competitive performance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of prevailing vector search algorithms that overlook the structural properties of vector embeddings and struggle with high-dimensional data.

Method: The authors propose using metric trees in q-metric spaces to exploit a stronger triangle inequality for efficient exact search, alongside a learned projection method to embed vector datasets into q-metric spaces while preserving nearest neighbor properties.

Result: Experimental results with text and image vector embeddings show that the proposed approach improves the performance of classic metric tree algorithms in handling high-dimensional data, making them competitive with advanced search techniques.

Conclusion: Employing q-metric spaces and learning q-metric approximations enhances vector search efficiency and effectiveness, particularly in high-dimensional contexts.

Abstract: Despite the ubiquity of vector search applications, prevailing search
algorithms overlook the metric structure of vector embeddings, treating it as a
constraint rather than exploiting its underlying properties. In this paper, we
demonstrate that in $q$-metric spaces, metric trees can leverage a stronger
version of the triangle inequality to reduce comparisons for exact search.
Notably, as $q$ approaches infinity, the search complexity becomes logarithmic.
Therefore, we propose a novel projection method that embeds vector datasets
with arbitrary dissimilarity measures into $q$-metric spaces while preserving
the nearest neighbor. We propose to learn an approximation of this projection
to efficiently transform query points to a space where euclidean distances
satisfy the desired properties. Our experimental results with text and image
vector embeddings show that learning $q$-metric approximations enables classic
metric tree algorithms -- which typically underperform with high-dimensional
data -- to achieve competitive performance against state-of-the-art search
methods.

</details>


### [752] [Correcting for Position Bias in Learning to Rank: A Control Function Approach](https://arxiv.org/abs/2506.06989)
*Md Aminul Islam,Kathryn Vasilaky,Elena Zheleva*

Main category: cs.IR

TL;DR: The paper addresses position bias in learning-to-rank systems by proposing a novel control function-based method, enhancing ranking performance despite biased implicit feedback.


<details>
  <summary>Details</summary>
Motivation: Position bias in implicit feedback data, such as user clicks, can lead to suboptimal performance in learning-to-rank systems.

Method: A two-stage control function-based method: the first stage extracts exogenous variation to correct position bias, while the second integrates any advanced ranking algorithm. The model also incorporates validation click debiasing for hyperparameter tuning.

Result: Experimental results show the proposed method outperforms state-of-the-art techniques in addressing position bias in rankings.

Conclusion: The method effectively corrects position bias without prior knowledge of click or propensity models and is generalizable to any advanced ranking algorithm.

Abstract: Implicit feedback data, such as user clicks, is commonly used in
learning-to-rank (LTR) systems because it is easy to collect and it often
reflects user preferences. However, this data is prone to various biases, and
training an LTR system directly on biased data can result in suboptimal ranking
performance. One of the most prominent and well-studied biases in implicit
feedback data is position bias, which occurs because users are more likely to
interact with higher-ranked documents regardless of their true relevance. In
this paper, we propose a novel control function-based method that accounts for
position bias in a two-stage process. The first stage uses exogenous variation
from the residuals of the ranking process to correct for position bias in the
second stage click equation. Unlike previous position bias correction methods,
our method does not require knowledge of the click or propensity model and
allows for nonlinearity in the underlying ranking model. Moreover, our method
is general and allows for debiasing any state-of-the-art ranking algorithm by
plugging it into the second stage. We also introduce a technique to debias
validation clicks for hyperparameter tuning to select the optimal model in the
absence of unbiased validation data. Experimental results demonstrate that our
method outperforms state-of-the-art approaches in correcting for position bias.

</details>


### [753] [RADAR: Recall Augmentation through Deferred Asynchronous Retrieval](https://arxiv.org/abs/2506.07261)
*Amit Jaspal,Qian Dang,Ajantha Ramineni*

Main category: cs.IR

TL;DR: Proposes RADAR, a framework that boosts recall and engagement by pre-ranking a large candidate set offline and using it for online recommendations.


<details>
  <summary>Details</summary>
Motivation: Recommender systems struggle to surface highly engaging items due to the inefficiency of current retrieval methods in distinguishing between merely relevant and highly engaging items.

Method: Introduced RADAR, which asynchronously and offline pre-ranks a large set of candidates using a complex ranking model, and stores the top-ranked items for online use, bypassing initial retrieval and pre-ranking steps.

Result: RADAR doubles recall performance (Recall@200) in offline experiments and improves user engagement by +0.8% in online A/B tests.

Conclusion: RADAR is an effective and practical approach to enhance the quality and recall performance of recommendation systems while meeting computational constraints.

Abstract: Modern large-scale recommender systems employ multi-stage ranking funnel
(Retrieval, Pre-ranking, Ranking) to balance engagement and computational
constraints (latency, CPU). However, the initial retrieval stage, often relying
on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles
to effectively surface the most engaging items from billion-scale catalogs,
particularly distinguishing highly relevant and engaging candidates from merely
relevant ones. We introduce Recall Augmentation through Deferred Asynchronous
Retrieval (RADAR), a novel framework that leverages asynchronous, offline
computation to pre-rank a significantly larger candidate set for users using
the full complexity ranking model. These top-ranked items are stored and
utilized as a high-quality retrieval source during online inference, bypassing
online retrieval and pre-ranking stages for these candidates. We demonstrate
through offline experiments that RADAR significantly boosts recall (2X
Recall@200 vs DNN retrieval baseline) by effectively combining a larger
retrieved candidate set with a more powerful ranking model. Online A/B tests
confirm a +0.8% lift in topline engagement metrics, validating RADAR as a
practical and effective method to improve recommendation quality under strict
online serving constraints.

</details>


### [754] [MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with Expert Specialization](https://arxiv.org/abs/2506.07563)
*Ken Yagel,Eyal German,Aviel Ben Siman Tov*

Main category: cs.IR

TL;DR: MoE-MLoRA introduces a mixture-of-experts framework to improve multi-domain personalized recommendation systems, outperforming traditional methods in dynamic datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional single-adaptation techniques like MLoRA lack flexibility to handle diverse user behaviors across domains.

Method: The method involves independently training domain specialists (experts) followed by a gating network to dynamically weight their contributions.

Result: MoE-MLoRA improves recommendation system accuracy in dynamic datasets (+1.45 Weighted-AUC in Taobao-20) but shows limited benefits in structured datasets.

Conclusion: Expert-based architectures with adaptive gating enhance multi-domain recommendation systems. Larger ensembles require model-aware tuning for optimal performance.

Abstract: Personalized recommendation systems must adapt to user interactions across
different domains. Traditional approaches like MLoRA apply a single adaptation
per domain but lack flexibility in handling diverse user behaviors. To address
this, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is
first trained independently to specialize in its domain before a gating network
is trained to weight their contributions dynamically. We evaluate MoE-MLoRA
across eight CTR models on Movielens and Taobao, showing that it improves
performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20)
but offers limited benefits in structured datasets with low domain diversity
and sparsity. Further analysis of the number of experts per domain reveals that
larger ensembles do not always improve performance, indicating the need for
model-aware tuning. Our findings highlight the potential of expert-based
architectures for multi-domain recommendation systems, demonstrating that
task-aware specialization and adaptive gating can enhance predictive accuracy
in complex environments. The implementation and code are available in our
GitHub repository.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [755] [ResPF: Residual Poisson Flow for Efficient and Physically Consistent Sparse-View CT Reconstruction](https://arxiv.org/abs/2506.06400)
*Changsheng Fang,Yongtong Liu,Bahareh Morovati,Shuo Han,Yu Shi,Li Zhou,Shuyi Fan,Hengyong Yu*

Main category: eess.IV

TL;DR: The paper presents Residual Poisson Flow Generative Models (ResPF) for improving sparse-view CT reconstruction by integrating conditional guidance, reducing sampling costs, and ensuring data consistency. Extensive experiments show ResPF outperforms prior methods.


<details>
  <summary>Details</summary>
Motivation: Sparse-view CT is valuable for reducing radiation exposure but creates an ill-posed inverse problem that requires accurate reconstruction. Existing methods like deep learning and diffusion models often lack physical interpretability or are computationally intensive.

Method: The authors adapt Poisson Flow Generative Models (PFGM++) by proposing ResPF, which uses conditional guidance, a hijacking strategy to skip redundant steps, and incorporates data-consistency at each step. A residual fusion module ensures stability and alignment with sparse-view measurements.

Result: ResPF achieves superior reconstruction quality, faster inference times, and improved robustness compared with state-of-the-art iterative, learning-based, and diffusion models on both synthetic and clinical datasets.

Conclusion: ResPF significantly advances sparse-view CT reconstruction by maintaining physical fidelity, reducing computational costs, and ensuring high-quality imaging, addressing key limitations of existing methods. It is the first application of Poisson flow models in this domain.

Abstract: Sparse-view computed tomography (CT) is a practical solution to reduce
radiation dose, but the resulting ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Although deep learning and
diffusion-based methods have shown promising results, they often lack physical
interpretability or suffer from high computational costs due to iterative
sampling starting from random noise. Recent advances in generative modeling,
particularly Poisson Flow Generative Models (PFGM), enable high-fidelity image
synthesis by modeling the full data distribution. In this work, we propose
Residual Poisson Flow (ResPF) Generative Models for efficient and accurate
sparse-view CT reconstruction. Based on PFGM++, ResPF integrates conditional
guidance from sparse measurements and employs a hijacking strategy to
significantly reduce sampling cost by skipping redundant initial steps.
However, skipping early stages can degrade reconstruction quality and introduce
unrealistic structures. To address this, we embed a data-consistency into each
iteration, ensuring fidelity to sparse-view measurements. Yet, PFGM sampling
relies on a fixed ordinary differential equation (ODE) trajectory induced by
electrostatic fields, which can be disrupted by step-wise data consistency,
resulting in unstable or degraded reconstructions. Inspired by ResNet, we
introduce a residual fusion module to linearly combine generative outputs with
data-consistent reconstructions, effectively preserving trajectory continuity.
To the best of our knowledge, this is the first application of Poisson flow
models to sparse-view CT. Extensive experiments on synthetic and clinical
datasets demonstrate that ResPF achieves superior reconstruction quality,
faster inference, and stronger robustness compared to state-of-the-art
iterative, learning-based, and diffusion models.

</details>


### [756] [SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation](https://arxiv.org/abs/2506.06890)
*Sumit Sharma,Gopi Raju Matta,Kaushik Mitra*

Main category: eess.IV

TL;DR: This paper develops a two-stage framework to enhance Single Photon Camera (SPC) data, enabling better RGB conversions and 3D reconstructions.


<details>
  <summary>Details</summary>
Motivation: Single Photon Cameras produce binary images that cause severe information loss, limiting traditional approaches for 3D synthesis and reconstruction.

Method: The framework contains two stages: (1) image-to-image translation using Pix2PixHD to convert SPC binary images into RGB representations, and (2) 3D reconstruction using Neural Radiance Fields or Gaussian Splatting.

Result: The pipeline shows significant improvements in perceptual quality and geometric consistency compared to alternative methods in both qualitative and quantitative experiments.

Conclusion: The proposed two-stage framework successfully overcomes information loss in SPC data, producing high-quality colorized views and better 3D reconstructions.

Abstract: Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging
technology, capable of detecting individual photons with remarkable timing
precision. Building on this sensitivity, Single Photon Cameras (SPCs) enable
image capture at exceptionally high speeds under both low and high
illumination. Enabling 3D reconstruction and radiance field recovery from such
SPC data holds significant promise. However, the binary nature of SPC images
leads to severe information loss, particularly in texture and color, making
traditional 3D synthesis techniques ineffective. To address this challenge, we
propose a modular two-stage framework that converts binary SPC images into
high-quality colorized novel views. The first stage performs image-to-image
(I2I) translation using generative models such as Pix2PixHD, converting binary
SPC inputs into plausible RGB representations. The second stage employs 3D
scene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian
Splatting (3DGS) to generate novel views. We validate our two-stage pipeline
(Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative
experiments, demonstrating significant improvements in perceptual quality and
geometric consistency over the alternative baseline.

</details>


### [757] [Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images](https://arxiv.org/abs/2506.07023)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: The paper presents a deep generative model for segmenting nuclei in histological images, addressing information disparity between image domains using optimal transport and measure theory.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation methods face challenges when translating information-rich histological images to information-poor segmentation maps due to domain information asymmetry.

Method: This paper uses an embedding space and integrates optimal transport and measure theory to create an invertible generator with spatially-constrained operations, reducing complexity and eliminating cycle-consistency loss.

Result: The proposed model demonstrates superior performance and efficient optimization compared to state-of-the-art nuclei segmentation methods using public datasets.

Conclusion: The paper offers an innovative approach that improves segmentation accuracy, optimizes computational efficiency, and simplifies model architecture compared to existing frameworks.

Abstract: Segmentation of nuclei regions from histological images enables morphometric
analysis of nuclei structures, which in turn helps in the detection and
diagnosis of diseases under consideration. To develop a nuclei segmentation
algorithm, applicable to different types of target domain representations,
image-to-image translation networks can be considered as they are invariant to
target domain image representations. One of the important issues with
image-to-image translation models is that they fail miserably when the
information content between two image domains are asymmetric in nature. In this
regard, the paper introduces a new deep generative model for segmenting nuclei
structures from histological images. The proposed model considers an embedding
space for handling information-disparity between information-rich histological
image space and information-poor segmentation map domain. Integrating
judiciously the concepts of optimal transport and measure theory, the model
develops an invertible generator, which provides an efficient optimization
framework with lower network complexity. The concept of invertible generator
automatically eliminates the need of any explicit cycle-consistency loss. The
proposed model also introduces a spatially-constrained squeeze operation within
the framework of invertible generator to maintain spatial continuity within the
image patches. The model provides a better trade-off between network complexity
and model performance compared to other existing models having complex network
architectures. The performance of the proposed deep generative model, along
with a comparison with state-of-the-art nuclei segmentation methods, is
demonstrated on publicly available histological image data sets.

</details>


### [758] [SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images](https://arxiv.org/abs/2506.07028)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: This paper proposes a deep generative model to perform simultaneous nuclei segmentation and color normalization on stained histological images, addressing key challenges posed by color variations and stain overlaps.


<details>
  <summary>Details</summary>
Motivation: Color variation in stained histological images complicates nuclei segmentation, and existing color normalization techniques don't address this fully. Accurate segmentation could make normalization less critical, prompting the need for a unified model.

Method: A novel deep generative model is introduced, leveraging truncated normal distributions, spatial attention, and a disentangled latent representation to separate color appearance from nuclei segmentation and embedding information.

Result: The model demonstrates better performance in nuclei segmentation and color normalization compared to state-of-the-art techniques when tested on publicly available histological image datasets.

Conclusion: This integrated framework improves both segmentation and normalization, showcasing the potential to enhance automated histological image analysis by addressing color variability and stain overlap challenges.

Abstract: Segmentation of nuclei regions from histological images is an important task
for automated computer-aided analysis of histological images, particularly in
the presence of impermissible color variation in the color appearance of
stained tissue images. While color normalization enables better nuclei
segmentation, accurate segmentation of nuclei structures makes color
normalization rather trivial. In this respect, the paper proposes a novel deep
generative model for simultaneously segmenting nuclei structures and
normalizing color appearance of stained histological images.This model
judiciously integrates the merits of truncated normal distribution and spatial
attention. The model assumes that the latent color appearance information,
corresponding to a particular histological image, is independent of respective
nuclei segmentation map as well as embedding map information. The disentangled
representation makes the model generalizable and adaptable as the modification
or loss in color appearance information cannot be able to affect the nuclei
segmentation map as well as embedding information. Also, for dealing with the
stain overlap of associated histochemical reagents, the prior for latent color
appearance code is assumed to be a mixture of truncated normal distributions.
The proposed model incorporates the concept of spatial attention for
segmentation of nuclei regions from histological images. The performance of the
proposed approach, along with a comparative analysis with related
state-of-the-art algorithms, has been demonstrated on publicly available
standard histological image data sets.

</details>


### [759] [Transfer Learning and Explainable AI for Brain Tumor Classification: A Study Using MRI Data from Bangladesh](https://arxiv.org/abs/2506.07228)
*Shuvashis Sarker*

Main category: eess.IV

TL;DR: This study presents an automated brain tumor classification system using MRI data and deep learning models (VGG16, VGG19, ResNet50), with VGG16 achieving 99.17% accuracy. Explainable AI (XAI) methods improved model interpretability for clinical use in resource-limited regions like Bangladesh.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges of timely and accurate brain tumor detection in resource-constrained environments, particularly in Bangladesh, where manual MRI diagnosis is inefficient and prone to errors.

Method: The method involves the development of an automated brain tumor classification system using advanced deep learning models (VGG16, VGG19, ResNet50) and MRI scans. XAI techniques like Grad-CAM and Grad-CAM++ were used to enhance model transparency by highlighting critical areas in the scans that influenced tumor classification.

Result: The study achieved significant results, with the VGG16 model delivering the highest accuracy at 99.17%. The integration of XAI methods further improved the interpretability, making the model more suitable for clinical use.

Conclusion: The paper concludes that deep learning models, combined with XAI methodologies, can significantly improve brain tumor detection and diagnosis, particularly in resource-limited settings, thereby addressing critical health challenges.

Abstract: Brain tumors, regardless of being benign or malignant, pose considerable
health risks, with malignant tumors being more perilous due to their swift and
uncontrolled proliferation, resulting in malignancy. Timely identification is
crucial for enhancing patient outcomes, particularly in nations such as
Bangladesh, where healthcare infrastructure is constrained. Manual MRI analysis
is arduous and susceptible to inaccuracies, rendering it inefficient for prompt
diagnosis. This research sought to tackle these problems by creating an
automated brain tumor classification system utilizing MRI data obtained from
many hospitals in Bangladesh. Advanced deep learning models, including VGG16,
VGG19, and ResNet50, were utilized to classify glioma, meningioma, and various
brain cancers. Explainable AI (XAI) methodologies, such as Grad-CAM and
Grad-CAM++, were employed to improve model interpretability by emphasizing the
critical areas in MRI scans that influenced the categorization. VGG16 achieved
the most accuracy, attaining 99.17%. The integration of XAI enhanced the
system's transparency and stability, rendering it more appropriate for clinical
application in resource-limited environments such as Bangladesh. This study
highlights the capability of deep learning models, in conjunction with
explainable artificial intelligence (XAI), to enhance brain tumor detection and
identification in areas with restricted access to advanced medical
technologies.

</details>


### [760] [A Comprehensive Analysis of COVID-19 Detection Using Bangladeshi Data and Explainable AI](https://arxiv.org/abs/2506.07234)
*Shuvashis Sarker*

Main category: eess.IV

TL;DR: This study targets improving COVID-19 detection in chest X-ray (CXR) images using various machine learning approaches, achieving 98% accuracy with the VGG19 model.


<details>
  <summary>Details</summary>
Motivation: The research aims to tackle challenges posed by the COVID-19 pandemic, particularly through enhanced detection methods using medical imaging to improve healthcare responses.

Method: The study employs Machine Learning (ML), Deep Learning (DL), and Transfer Learning (TL) models on a dataset of 4,350 CXR images, categorized into four classes. It uses the VGG19 model for classification, LIME for model explanation, and SMOTE for addressing class imbalances.

Result: The VGG19 model demonstrated remarkable performance, achieving a 98% accuracy rate in classifying CXR images, with LIME enhancing the interpretability of the classification decisions.

Conclusion: Leveraging ML, DL, and explainability techniques like LIME can significantly enhance COVID-19 detection in medical imaging, improving transparency, reliability, and diagnostic outcomes.

Abstract: COVID-19 is a rapidly spreading and highly infectious virus which has
triggered a global pandemic, profoundly affecting millions across the world.
The pandemic has introduced unprecedented challenges in public health, economic
stability, and societal structures, necessitating the implementation of
extensive and multifaceted health interventions globally. It had a tremendous
impact on Bangladesh by April 2024, with around 29,495 fatalities and more than
2 million confirmed cases. This study focuses on improving COVID-19 detection
in CXR images by utilizing a dataset of 4,350 images from Bangladesh
categorized into four classes: Normal, Lung-Opacity, COVID-19 and
Viral-Pneumonia. ML, DL and TL models are employed with the VGG19 model
achieving an impressive 98% accuracy. LIME is used to explain model
predictions, highlighting the regions and features influencing classification
decisions. SMOTE is applied to address class imbalances. By providing insight
into both correct and incorrect classifications, the study emphasizes the
importance of XAI in enhancing the transparency and reliability of models,
ultimately improving the effectiveness of detection from CXR images.

</details>


### [761] [A Narrative Review on Large AI Models in Lung Cancer Screening, Diagnosis, and Treatment Planning](https://arxiv.org/abs/2506.07236)
*Jiachen Zhong,Yiting Wang,Di Zhu,Ziwei Wang*

Main category: eess.IV

TL;DR: The paper reviews the use of large AI models in lung cancer care, focusing on their applications, limitations, and potential for future improvements.


<details>
  <summary>Details</summary>
Motivation: Lung cancer's prevalence and fatality rates necessitate better diagnostic and treatment methods, which large AI models can address.

Method: The paper categorizes AI models into various architectures and evaluates them on multimodal tasks using established datasets, while identifying clinical applications and challenges.

Result: Large AI models are shown to aid in pulmonary nodule detection, gene mutation prediction, and personalized treatment, with evidence of clinical deployment.

Conclusion: These models have significant potential to transform lung cancer care, but generalizability, interpretability, and regulatory issues remain challenges to be resolved.

Abstract: Lung cancer remains one of the most prevalent and fatal diseases worldwide,
demanding accurate and timely diagnosis and treatment. Recent advancements in
large AI models have significantly enhanced medical image understanding and
clinical decision-making. This review systematically surveys the
state-of-the-art in applying large AI models to lung cancer screening,
diagnosis, prognosis, and treatment. We categorize existing models into
modality-specific encoders, encoder-decoder frameworks, and joint encoder
architectures, highlighting key examples such as CLIP, BLIP, Flamingo,
BioViL-T, and GLoRIA. We further examine their performance in multimodal
learning tasks using benchmark datasets like LIDC-IDRI, NLST, and MIMIC-CXR.
Applications span pulmonary nodule detection, gene mutation prediction,
multi-omics integration, and personalized treatment planning, with emerging
evidence of clinical deployment and validation. Finally, we discuss current
limitations in generalizability, interpretability, and regulatory compliance,
proposing future directions for building scalable, explainable, and clinically
integrated AI systems. Our review underscores the transformative potential of
large AI models to personalize and optimize lung cancer care.

</details>


### [762] [Text-guided multi-stage cross-perception network for medical image segmentation](https://arxiv.org/abs/2506.07475)
*Gaoyu Chen*

Main category: eess.IV

TL;DR: The paper introduces the Text-guided Multi-stage Cross-perception network (TMC) to enhance medical image segmentation by leveraging cross-modal interactions between text prompts and image data, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: Existing medical image segmentation techniques struggle with weak semantic expression in low-contrast regions, making diagnosis and treatment planning difficult. The authors aim to address this limitation using text-guided methods.

Method: The authors developed TMC, which integrates a multistage cross-attention module for semantic enhancement and employs a multi-stage alignment loss for improved cross-modal semantic consistency.

Result: The TMC network achieved Dice scores of 84.77%, 78.50%, and 88.73% across three datasets, outperforming existing UNet-based and text-guided segmentation methods.

Conclusion: TMC demonstrates improved segmentation performance by overcoming cross-modal interaction limitations, showcasing its potential for better lesion localization in medical imaging.

Abstract: Medical image segmentation plays a crucial role in clinical medicine, serving
as a tool for auxiliary diagnosis, treatment planning, and disease monitoring,
thus facilitating physicians in the study and treatment of diseases. However,
existing medical image segmentation methods are limited by the weak semantic
expression of the target segmentation regions, which is caused by the low
contrast between the target and non-target segmentation regions. To address
this limitation, text prompt information has greast potential to capture the
lesion location. However, existing text-guided methods suffer from insufficient
cross-modal interaction and inadequate cross-modal feature expression. To
resolve these issues, we propose the Text-guided Multi-stage Cross-perception
network (TMC). In TMC, we introduce a multistage cross-attention module to
enhance the model's understanding of semantic details and a multi-stage
alignment loss to improve the consistency of cross-modal semantics. The results
of the experiments demonstrate that our TMC achieves a superior performance
with Dice of 84.77%, 78.50%, 88.73% in three public datasets (QaTa-COV19,
MosMedData and Breast), outperforming UNet based networks and text-guided
methods.

</details>


### [763] [Fine-Grained Motion Compression and Selective Temporal Fusion for Neural B-Frame Video Coding](https://arxiv.org/abs/2506.07709)
*Xihua Sheng,Peilin Chen,Meng Wang,Li Zhang,Shiqi Wang,Dapeng Oliver Wu*

Main category: eess.IV

TL;DR: The paper introduces new methods for motion compression and temporal fusion to address the unique challenges of neural B-frame video coding, outperforming existing neural codecs and competing with H.266/VVC standards.


<details>
  <summary>Details</summary>
Motivation: Current neural B-frame video coding approaches rely on tools designed for P-frames, which do not adequately address the specific needs of B-frame compression, resulting in performance limitations.

Method: The paper presents a fine-grained motion compression technique using dual-branch auto-encoders with adaptive quantization, and a selective temporal fusion approach with bi-directional weights and hyperprior alignment for better context modeling.

Result: Experiments show that the proposed codec delivers superior compression results compared to state-of-the-art neural B-frame codecs and matches or surpasses H.266/VVC in random-access configurations.

Conclusion: The proposed enhancements for motion compression and temporal fusion successfully address limitations in neural B-frame coding and demonstrate significant improvements in compression performance.

Abstract: With the remarkable progress in neural P-frame video coding, neural B-frame
coding has recently emerged as a critical research direction. However, most
existing neural B-frame codecs directly adopt P-frame coding tools without
adequately addressing the unique challenges of B-frame compression, leading to
suboptimal performance. To bridge this gap, we propose novel enhancements for
motion compression and temporal fusion for neural B-frame coding. First, we
design a fine-grained motion compression method. This method incorporates an
interactive dual-branch motion auto-encoder with per-branch adaptive
quantization steps, which enables fine-grained compression of bi-directional
motion vectors while accommodating their asymmetric bitrate allocation and
reconstruction quality requirements. Furthermore, this method involves an
interactive motion entropy model that exploits correlations between
bi-directional motion latent representations by interactively leveraging
partitioned latent segments as directional priors. Second, we propose a
selective temporal fusion method that predicts bi-directional fusion weights to
achieve discriminative utilization of bi-directional multi-scale temporal
contexts with varying qualities. Additionally, this method introduces a
hyperprior-based implicit alignment mechanism for contextual entropy modeling.
By treating the hyperprior as a surrogate for the contextual latent
representation, this mechanism implicitly mitigates the misalignment in the
fused bi-directional temporal priors. Extensive experiments demonstrate that
our proposed codec outperforms state-of-the-art neural B-frame codecs and
achieves comparable or even superior compression performance to the H.266/VVC
reference software under random-access configurations.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [764] [MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes](https://arxiv.org/abs/2506.06318)
*Feiyang Pan,Shenghe Zheng,Chunyan Yin,Guangbin Dou*

Main category: eess.SP

TL;DR: This study introduces MoE-Gyro, a self-supervised deep-learning framework addressing the trade-off between measurement range and noise performance in MEMS gyroscopes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the measurement range vs noise performance trade-off in MEMS gyroscopes without introducing complexity or reliance on precisely aligned ground-truth signals, both issues in current solutions.

Method: The method employs a mixture of experts framework (ORE for saturation reconstruction and DE for noise reduction), supported by a lightweight gating module for segment routing, and proposes a new benchmarking platform, ISEBench, for evaluation.

Result: MoE-Gyro extended the measurable range from 450 deg/s to 1500 deg/s, reduced Bias Instability by 98.4%, and achieved state-of-the-art signal enhancement performance using ISEBench.

Conclusion: MoE-Gyro successfully addresses the long-standing trade-off in MEMS gyroscope performance by offering robust signal reconstruction and noise suppression in a self-supervised framework.

Abstract: MEMS gyroscopes play a critical role in inertial navigation and motion
control applications but typically suffer from a fundamental trade-off between
measurement range and noise performance. Existing hardware-based solutions
aimed at mitigating this issue introduce additional complexity, cost, and
scalability challenges. Deep-learning methods primarily focus on noise
reduction and typically require precisely aligned ground-truth signals, making
them difficult to deploy in practical scenarios and leaving the fundamental
trade-off unresolved. To address these challenges, we introduce Mixture of
Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework
specifically designed for simultaneous over-range signal reconstruction and
noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction
Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing
saturated segments; and a Denoise Expert (DE), utilizing dual-branch
complementary masking combined with FFT-guided augmentation for robust noise
reduction. A lightweight gating module dynamically routes input segments to the
appropriate expert. Furthermore, existing evaluation lack a comprehensive
standard for assessing multi-dimensional signal enhancement. To bridge this
gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source
benchmarking platform comprising the GyroPeak-100 dataset and a unified
evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our
proposed ISEBench, demonstrating that our framework significantly extends the
measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by
98.4%, and achieves state-of-the-art performance, effectively addressing the
long-standing trade-off in inertial sensing.

</details>


### [765] [A Reinforcement Learning Approach for RIS-aided Fair Communications](https://arxiv.org/abs/2506.06344)
*Alex Pierron,Michel Barbeau,Luca De Cicco,Jose Rubio-Hernan,Joaquin Garcia-Alfaro*

Main category: eess.SP

TL;DR: This paper explores the use of Reconfigurable Intelligent Surfaces (RIS) with Reinforcement Learning (RL) to optimize network performance while ensuring fairness for multiple User Equipment (UE) units.


<details>
  <summary>Details</summary>
Motivation: To enhance network performance, energy efficiency, and fair communication in low-coverage areas using RIS technology integrated with RL techniques.

Method: A novel method is proposed to achieve an efficient and fair duplex RIS-RL system for multiple UEs, and experimental work with simulations is conducted to validate the approach.

Result: The authors demonstrate their method's fairness and efficiency through experimental and simulation results, and they share their code and datasets for the research community.

Conclusion: The combination of RIS and RL techniques can optimize network performance and fairness, ensuring adequate signal strength for multiple UEs without sacrificing service quality.

Abstract: Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements
that can dynamically alter electromagnetic wave properties to enhance
beamforming and leading to improvements in areas with low coverage properties.
They have the potential to be combined with Reinforcement Learning (RL)
techniques to achieve network performance and energy efficiency via
optimization techniques. In addition to performance and energy improvements, it
is also crucial to consider the concept of fair communications. RISs must
ensure that User Equipment (UE) units receive their signals with adequate
strength, without other UE being deprived of service due to insufficient power.
In this paper, we address such a problem. We explore the fairness properties of
previous work and propose a novel method that aims at obtaining an efficient
and fair duplex RIS-RL system for multiple legitimate UE units. We report and
discuss our experimental work and simulation results. We also release our code
and datasets to foster further research in the topic.

</details>


### [766] [Deep learning methods for modeling infrasound transmission loss in the middle atmosphere](https://arxiv.org/abs/2506.06351)
*Alexis Le Pichon,Alice Janela Cameijo,Samir Aknine,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven Peter Naesholm*

Main category: eess.SP

TL;DR: The paper introduces an optimized convolutional network for modeling infrasound transmission losses (TLs) efficiently on a global scale, with an average error of 8.6 dB across a broad frequency range.


<details>
  <summary>Details</summary>
Motivation: Accurate infrasound TL modeling is necessary for evaluating the performance of the global International Monitoring System infrasound network. Traditional methods like parabolic equation (PE) simulations are computationally intensive, limiting their operational utility.

Method: The study developed an optimized convolutional neural network trained on globally simulated temperature and wind fields to predict TLs spanning up to 4000 km propagation ranges, enhancing the earlier proposed architecture through key optimizations.

Result: The proposed model achieved an average prediction error of 8.6 dB across the 0.1-3.2 Hz frequency band in realistic atmospheric scenarios and reduced computation times considerably.

Conclusion: The optimized convolutional network offers a promising solution for predicting transmission losses globally and efficiently, overcoming limitations in high-frequency and unfavorable wind conditions seen in prior methods.

Abstract: Accurate modeling of infrasound transmission losses (TLs) is essential to
assess the performance of the global International Monitoring System infrasound
network. Among existing propagation modeling tools, parabolic equation (PE)
method enables TLs to be finely modeled, but its computational cost does not
allow exploration of a large parameter space for operational monitoring
applications. To reduce computation times, Brissaud et al. 2023 explored the
potential of convolutional neural networks trained on a large set of regionally
simulated wavefields (< 1000 km from the source) to predict TLs with negligible
computation times compared to PE simulations. However, this method struggles in
unfavorable initial wind conditions, especially at high frequencies, and causal
issues with winds at large distances from the source affecting ground TLs close
to the source. In this study, we have developed an optimized convolutional
network designed to minimize prediction errors while predicting TLs from
globally simulated combined temperature and wind fields spanning over
propagation ranges of 4000 km. Our approach enhances the previously proposed
one by implementing key optimizations that improve the overall architecture
performance. The implemented model predicts TLs with an average error of 8.6 dB
in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric
scenarios.

</details>


### [767] [Large Language Models for EEG: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2506.06353)
*Naseem Babu,Jimson Mathew,A. P. Vinod*

Main category: eess.SP

TL;DR: The paper reviews the intersection of Large Language Models (LLMs) and EEG research, categorizing advancements like EEG representation learning, EEG-to-language decoding, cross-modal applications, and clinical usage.


<details>
  <summary>Details</summary>
Motivation: The motivation is to systematically document how LLMs are being utilized for EEG-based applications, enabling breakthroughs in neural decoding, brain-computer interfaces, and affective computing.

Method: The paper employs a structured review and taxonomy approach to organize literature into four domains of application, focusing on transformer-based architectures and their adaptability.

Result: Transformers adapted through fine-tuning, few-shot, and zero-shot methods allow EEG models to achieve tasks like natural language generation, semantic interpretation, and clinical diagnostics.

Conclusion: This survey acts as a foundational guide to inspire further fusion of natural language processing and neural signal analysis through the use of language models.

Abstract: The growing convergence between Large Language Models (LLMs) and
electroencephalography (EEG) research is enabling new directions in neural
decoding, brain-computer interfaces (BCIs), and affective computing. This
survey offers a systematic review and structured taxonomy of recent
advancements that utilize LLMs for EEG-based analysis and applications. We
organize the literature into four domains: (1) LLM-inspired foundation models
for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal
generation including image and 3D object synthesis, and (4) clinical
applications and dataset management tools. The survey highlights how
transformer-based architectures adapted through fine-tuning, few-shot, and
zero-shot learning have enabled EEG-based models to perform complex tasks such
as natural language generation, semantic interpretation, and diagnostic
assistance. By offering a structured overview of modeling strategies, system
designs, and application areas, this work serves as a foundational resource for
future work to bridge natural language processing and neural signal analysis
through language models.

</details>


### [768] [Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation](https://arxiv.org/abs/2506.06358)
*Alice Janela Cameijo,Alexis Le Pichon,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven peter Naesholm,Constantino Listowski,Samir Aknine*

Main category: eess.SP

TL;DR: The paper improves a neural network model for predicting infrasound transmission loss, using advanced architectures and realistic atmospheric inputs for fast and accurate simulations.


<details>
  <summary>Details</summary>
Motivation: Enhancing the efficiency and accuracy of infrasound transmission loss models is crucial for monitoring compliance with the Comprehensive Nuclear-Test-Ban Treaty, overcoming limitations of existing deep learning algorithms.

Method: The study integrates wind and temperature fields into a neural network model optimized through convolutional and recurrent architectural layers, covering simulation distances up to 4,000 km and altitudes up to 130 km.

Result: The neural network achieves accuracy within an average error margin of 4 dB compared to full parabolic equation simulations, successfully predicts unseen volcanic eruption data, and outputs uncertainty estimates.

Conclusion: This approach strengthens near real-time evaluation capabilities of infrasound station detection thresholds, aiding global security compliance and monitoring systems.

Abstract: Accurate modeling of infrasound transmission loss is essential for evaluating
the performance of the International Monitoring System, enabling the effective
design and maintenance of infrasound stations to support compliance of the
Comprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling
tools enable transmission loss to be finely simulated using atmospheric models.
However, the computational cost prohibits the exploration of a large parameter
space in operational monitoring applications. To address this, recent studies
made use of a deep learning algorithm capable of making transmission loss
predictions almost instantaneously. However, the use of nudged atmospheric
models leads to an incomplete representation of the medium, and the absence of
temperature as an input makes the algorithm incompatible with long range
propagation. In this study, we address these limitations by using both wind and
temperature fields as inputs to a neural network, simulated up to 130 km
altitude and 4,000 km distance. We also optimize several aspects of the neural
network architecture. We exploit convolutional and recurrent layers to capture
spatially and range-dependent features embedded in realistic atmospheric
models, improving the overall performance. The neural network reaches an
average error of 4 dB compared to full parabolic equation simulations and
provides epistemic and data-related uncertainty estimates. Its evaluation on
the 2022 Hunga Tonga-Hunga Ha'apai volcanic eruption demonstrates its
prediction capability using atmospheric conditions and frequencies not included
in the training. This represents a significant step towards near real-time
assessment of International Monitoring System detection thresholds of explosive
sources.

</details>


### [769] [Model-based Neural Data Augmentation for sub-wavelength Radio Localization](https://arxiv.org/abs/2506.06387)
*Baptiste Chatelier,Vincent Corlay,Musa Furkan Keskin,Matthieu Crussière,Henk Wymeersch,Luc Le Magoarou*

Main category: eess.SP

TL;DR: The paper introduces a generative neural channel model to enhance radio-localization accuracy, while reducing computational and memory requirements.


<details>
  <summary>Details</summary>
Motivation: With the increasing use of large antenna arrays, traditional localization methods face challenges in complex environments dominated by non-line-of-sight (NLoS) paths.

Method: The authors propose a model-based neural network for location-to-channel mapping, which serves as a generative neural channel model to improve fingerprinting-based localization.

Result: The generative approach achieves sub-wavelength localization accuracy and improves performance under NLoS conditions. It also reduces memory requirements significantly compared to classical methods.

Conclusion: This novel method both boosts localization accuracy and optimizes memory usage, addressing key limitations in traditional fingerprinting frameworks.

Abstract: The increasing deployment of large antenna arrays at base stations has
significantly improved the spatial resolution and localization accuracy of
radio-localization methods. However, traditional signal processing techniques
struggle in complex radio environments, particularly in scenarios dominated by
non line of sight (NLoS) propagation paths, resulting in degraded localization
accuracy. Recent developments in machine learning have facilitated the
development of machine learning-assisted localization techniques, enhancing
localization accuracy in complex radio environments. However, these methods
often involve substantial computational complexity during both the training and
inference phases. This work extends the well-established fingerprinting-based
localization framework by simultaneously reducing its memory requirements and
improving its accuracy. Specifically, a model-based neural network is used to
learn the location-to-channel mapping, and then serves as a generative neural
channel model. This generative model augments the fingerprinting comparison
dictionary while reducing the memory requirements. The proposed method
outperforms fingerprinting baselines by achieving sub-wavelength localization
accuracy, even in NLoS environments. Remarkably, it offers an improvement by
several orders of magnitude in localization accuracy, while simultaneously
reducing memory requirements by an order of magnitude compared to classical
fingerprinting methods.

</details>


### [770] [Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning](https://arxiv.org/abs/2506.06306)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: eess.SP

TL;DR: This study develops machine learning models to predict agitation in dementia patients using multimodal sensor data, achieving high accuracy and supporting early interventions for community-based care.


<details>
  <summary>Details</summary>
Motivation: Agitation is a common issue among dementia patients, especially in non-clinically supervised community settings, leading to caregiver burden and reduced quality of life.

Method: The study introduced agitation-related contextual features and applied machine learning and deep learning models to sensor data. It explored binary classification, sequential data analysis, and anomaly detection approaches using the TIHM dataset.

Result: Binary classification on current 6-hour timestamps achieved the best performance, with the light gradient boosting machine delivering AUC-ROC of 0.9720 and AUC-PR of 0.4320 when incorporating additional features like time of day and agitation history.

Conclusion: The proposed methods accurately predict agitation, providing explainable and efficient solutions that support proactive dementia care and enhance aging in place through privacy-preserving sensor data.

Abstract: Agitation is one of the most common responsive behaviors in people living
with dementia, particularly among those residing in community settings without
continuous clinical supervision. Timely prediction of agitation can enable
early intervention, reduce caregiver burden, and improve the quality of life
for both patients and caregivers. This study aimed to develop and benchmark
machine learning approaches for the early prediction of agitation in
community-dwelling older adults with dementia using multimodal sensor data. A
new set of agitation-related contextual features derived from activity data was
introduced and employed for agitation prediction. A wide range of machine
learning and deep learning models was evaluated across multiple problem
formulations, including binary classification for single-timestamp tabular
sensor data and multi-timestamp sequential sensor data, as well as anomaly
detection for single-timestamp tabular sensor data. The study utilized the
Technology Integrated Health Management (TIHM) dataset, the largest publicly
available dataset for remote monitoring of people living with dementia,
comprising 2,803 days of in-home activity, physiology, and sleep data. The most
effective setting involved binary classification of sensor data using the
current 6-hour timestamp to predict agitation at the subsequent timestamp.
Incorporating additional information, such as time of day and agitation
history, further improved model performance, with the highest AUC-ROC of 0.9720
and AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work
presents the first comprehensive benchmarking of state-of-the-art techniques
for agitation prediction in community-based dementia care using
privacy-preserving sensor data. The approach enables accurate, explainable, and
efficient agitation prediction, supporting proactive dementia care and aging in
place.

</details>


### [771] [An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation](https://arxiv.org/abs/2506.06315)
*Masoud Rahimi,Reza Karbasi,Abdol-Hossein Vahabie*

Main category: eess.SP

TL;DR: The paper presents a Python framework for creating synthetic ECG image datasets to support deep learning tasks like ECG digitization, lead detection, and waveform segmentation.


<details>
  <summary>Details</summary>
Motivation: To address challenges in ECG analysis by providing synthetic datasets for advancing deep learning models in medical tasks.

Method: Synthetic datasets are created using the PTB-XL signal dataset, including four dataset types for digitization, lead detection, single-lead image segmentation, and overlapping waveform segmentation.

Result: Four open-access ECG image datasets were produced, each tailored for specific analysis tasks and intended for compatibility with YOLO and U-Net models.

Conclusion: The open-source framework and datasets enhance the capabilities for ECG analysis and are made freely available for researchers.

Abstract: We introduce an open-source Python framework for generating synthetic ECG
image datasets to advance critical deep learning-based tasks in ECG analysis,
including ECG digitization, lead region and lead name detection, and
pixel-level waveform segmentation. Using the PTB-XL signal dataset, our
proposed framework produces four open-access datasets: (1) ECG images in
various lead configurations paired with time-series signals for ECG
digitization, (2) ECG images annotated with YOLO-format bounding boxes for
detection of lead region and lead name, (3)-(4) cropped single-lead images with
segmentation masks compatible with U-Net-based models in normal and overlapping
versions. In the overlapping case, waveforms from neighboring leads are
superimposed onto the target lead image, while the segmentation masks remain
clean. The open-source Python framework and datasets are publicly available at
https://github.com/rezakarbasi/ecg-image-and-signal-dataset and
https://doi.org/10.5281/zenodo.15484519, respectively.

</details>


### [772] [Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning](https://arxiv.org/abs/2506.06349)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: eess.SP

TL;DR: This paper compares traditional machine learning on ECG features and deep learning on image-transformed ECG data for heartbeat classification. LightGBM with handcrafted features outperformed CNN-based approaches.


<details>
  <summary>Details</summary>
Motivation: To explore and compare the efficacy of two methods, feature-based traditional machine learning and image-based deep learning, in classifying heartbeats from ECG data.

Method: Two approaches were tested: Traditional ML used handcrafted features (e.g., HRV, RR intervals). Deep learning converted ECG signals into images (e.g., GAF, MTF) and classified them using CNN architectures (VGG, Inception). Multiple preprocessing steps were performed on the dataset.

Result: The traditional ML approach with LightGBM achieved the best F1 score of 0.94, surpassing image-based CNN methods, which had an F1 score of 0.85. SVM and AdaBoost models performed poorly.

Conclusion: Handcrafted features proved more effective than image-based representations for capturing ECG data characteristics. Future work should explore multi-lead ECGs and temporal dependencies for improved results.

Abstract: This study addresses the classification of heartbeats from ECG signals
through two distinct approaches: traditional machine learning utilizing
hand-crafted features and deep learning via transformed images of ECG beats.
The dataset underwent preprocessing steps, including downsampling, filtering,
and normalization, to ensure consistency and relevance for subsequent analysis.
In the first approach, features such as heart rate variability (HRV), mean,
variance, and RR intervals were extracted to train various classifiers,
including SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and
LightGBM. The second approach involved transforming ECG signals into images
using Gramian Angular Field (GAF), Markov Transition Field (MTF), and
Recurrence Plots (RP), with these images subsequently classified using CNN
architectures like VGG and Inception.
  Experimental results demonstrate that the LightGBM model achieved the highest
performance, with an accuracy of 99% and an F1 score of 0.94, outperforming the
image-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost
yielded significantly lower scores, indicating limited suitability for this
task. The findings underscore the superior ability of hand-crafted features to
capture temporal and morphological variations in ECG signals compared to
image-based representations of individual beats. Future investigations may
benefit from incorporating multi-lead ECG signals and temporal dependencies
across successive beats to enhance classification accuracy further.

</details>


### [773] [Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia](https://arxiv.org/abs/2506.06309)
*Mohamed Kefi,Tien Dat Pham,Thin Nguyen,Mark G. Tjoelker,Viola Devasirvatham,Kenichi Kashiwagi*

Main category: eess.SP

TL;DR: The paper proposes a novel pipeline combining remote sensing, machine learning, and field data for accurate olive yield estimation, achieving high predictive performance using satellite imagery.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of accurately estimating olive yield, which is increasingly affected by climate change, using advanced remote sensing and machine learning techniques.

Method: The researchers integrated features from satellite data (multispectral reflectance and vegetation indices), a digital elevation model, and field survey data into a machine learning pipeline. They utilized AutoGluon for automated ensemble learning to train, evaluate, and optimize prediction models with cross-validation.

Result: The predictive models achieved high performance metrics, with Landsat-8 OLI obtaining R2 = 0.8635 and RMSE = 1.17 tons/ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons/ha.

Conclusion: The framework offers a scalable, cost-effective, and accurate approach for olive yield prediction, with potential for broader application in diverse agricultural regions.

Abstract: Olive production is an important tree crop in Mediterranean climates.
However, olive yield varies significantly due to climate change. Accurately
estimating yield using remote sensing and machine learning remains a complex
challenge. In this study, we developed a streamlined pipeline for olive yield
estimation in the Kairouan and Sousse governorates of Tunisia. We extracted
features from multispectral reflectance bands, vegetation indices derived from
Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital
elevation model data. These spatial features were combined with ground-based
field survey data to form a structured tabular dataset. We then developed an
automated ensemble learning framework, implemented using AutoGluon to train and
evaluate multiple machine learning models, select optimal combinations through
stacking, and generate robust yield predictions using five-fold
cross-validation. The results demonstrate strong predictive performance from
both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per
ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This
study highlights a scalable, cost-effective, and accurate method for olive
yield estimation, with potential applicability across diverse agricultural
regions globally.

</details>


### [774] [Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue](https://arxiv.org/abs/2506.06310)
*Xiaoyu Sun,Yang Yang,Xunde Dong*

Main category: eess.SP

TL;DR: This paper introduces a contrastive learning-based ECG pretrained model enhanced by a Patient Memory Queue (PMQ) to address the issue of limited intra-inter patient samples, showing superior performance and robustness in scenarios with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the challenge of building a robust ECG pretrained model using limited labeled data, by leveraging patient-level self-supervisory signals and enhancing patient contrastive learning methods.

Method: The proposed method incorporates a Patient Memory Queue (PMQ) to store a large volume of patient samples, mitigating model degeneration caused by insufficient intra-inter patient samples. Additionally, two new data augmentation techniques are introduced to create diverse positive and negative pairs for pretraining.

Result: Experiments conducted on three public ECG datasets under different data ratios demonstrate that the proposed method performs better than existing contrastive learning approaches, showing improved robustness and generalization.

Conclusion: The study concludes that combining contrastive learning with the proposed PMQ mechanism and additional data augmentation techniques leads to significant advancements in ECG representation learning, particularly in low-labeled data scenarios.

Abstract: In the field of automatic Electrocardiogram (ECG) diagnosis, due to the
relatively limited amount of labeled data, how to build a robust ECG pretrained
model based on unlabeled data is a key area of focus for researchers. Recent
advancements in contrastive learning-based ECG pretrained models highlight the
potential of exploiting the additional patient-level self-supervisory signals
inherent in ECG. They are referred to as patient contrastive learning. Its
rationale is that multiple physical recordings from the same patient may share
commonalities, termed patient consistency, so redefining positive and negative
pairs in contrastive learning as intrapatient and inter-patient samples
provides more shared context to learn an effective representation. However,
these methods still fail to efficiently exploit patient consistency due to the
insufficient amount of intra-inter patient samples existing in a batch. Hence,
we propose a contrastive learning-based ECG pretrained model enhanced by the
Patient Memory Queue (PMQ), which incorporates a large patient memory queue to
mitigate model degeneration that can arise from insufficient intra-inter
patient samples. In order to further enhance the performance of the pretrained
model, we introduce two extra data augmentation methods to provide more
perspectives of positive and negative pairs for pretraining. Extensive
experiments were conducted on three public datasets with three different data
ratios. The experimental results show that the comprehensive performance of our
method outperforms previous contrastive learning methods and exhibits greater
robustness in scenarios with limited labeled data. The code is available at
https://github.com/3hiuwoo/PMQ.

</details>


### [775] [A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration](https://arxiv.org/abs/2506.06311)
*Meiyan Kang,Shizuo Kaji,Sang-Yun Lee,Taegon Kim,Hee-Hwan Ryu,Suyoung Choi*

Main category: eess.SP

TL;DR: The study introduces a novel framework for using Ground Penetrating Radar (GPR) combined with Topological Data Analysis (TDA) and a deep learning model (YOLOv5) to improve underground utility detection.


<details>
  <summary>Details</summary>
Motivation: Conventional GPR analysis methods lack structural awareness and are prone to noise, limiting their effectiveness in subsurface exploration, particularly for pipelines and similar utilities.

Method: The paper integrates shape-aware topological features from Topological Data Analysis (TDA) with spatial detection from the YOLOv5 deep neural network. A Sim2Real strategy is also employed to generate synthetic datasets for training.

Result: The proposed framework improves mean Average Precision (mAP), demonstrating superior robustness and effectiveness in underground object detection compared to traditional methods.

Conclusion: This approach highlights the potential for TDA-enhanced machine learning methods to provide accurate, real-time subsurface detection with applications in fields like urban planning and infrastructure management.

Abstract: Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT)
technique for subsurface exploration, particularly in infrastructure inspection
and maintenance. However, conventional interpretation methods are often limited
by noise sensitivity and a lack of structural awareness. This study presents a
novel framework that enhances the detection of underground utilities,
especially pipelines, by integrating shape-aware topological features derived
from B-scan GPR images using Topological Data Analysis (TDA), with the spatial
detection capabilities of the YOLOv5 deep neural network (DNN). We propose a
novel shape-aware topological representation that amplifies structural features
in the input data, thereby improving the model's responsiveness to the
geometrical features of buried objects. To address the scarcity of annotated
real-world data, we employ a Sim2Real strategy that generates diverse and
realistic synthetic datasets, effectively bridging the gap between simulated
and real-world domains. Experimental results demonstrate significant
improvements in mean Average Precision (mAP), validating the robustness and
efficacy of our approach. This approach underscores the potential of
TDA-enhanced learning in achieving reliable, real-time subsurface object
detection, with broad applications in urban planning, safety inspection, and
infrastructure management.

</details>


### [776] [Composite Reward Design in PPO-Driven Adaptive Filtering](https://arxiv.org/abs/2506.06323)
*Abdullah Burkan Bereketoglu*

Main category: eess.SP

TL;DR: The paper introduces an adaptive noise filtering framework using Proximal Policy Optimization (PPO), surpassing classical filtering techniques in non-stationary environments.


<details>
  <summary>Details</summary>
Motivation: Traditional noise filters struggle with non-stationary environments due to limitations like reliance on stationary assumptions, complex tuning, or fixed models.

Method: The proposed method utilizes PPO guided by a composite reward balancing SNR improvement, MSE reduction, and signal smoothness. Synthetic signal experiments validate its performance.

Result: The PPO-based framework demonstrated superior noise filtering capabilities, generalizing well in varied noise types and real-time scenarios.

Conclusion: Policy-gradient reinforcement learning is viable for adaptive, real-time, and robust signal filtering in dynamic environments.

Abstract: Model-free and reinforcement learning-based adaptive filtering methods are
gaining traction for denoising in dynamic, non-stationary environments such as
wireless signal channels. Traditional filters like LMS, RLS, Wiener, and Kalman
are limited by assumptions of stationary or requiring complex fine-tuning or
exact noise statistics or fixed models. This letter proposes an adaptive
filtering framework using Proximal Policy Optimization (PPO), guided by a
composite reward that balances SNR improvement, MSE reduction, and residual
smoothness. Experiments on synthetic signals with various noise types show that
our PPO agent generalizes beyond its training distribution, achieving real-time
performance and outperforming classical filters. This work demonstrates the
viability of policy-gradient reinforcement learning for robust, low-latency
adaptive signal filtering.

</details>


### [777] [Uncertainty-Aware Multi-view Arrhythmia Classification from ECG](https://arxiv.org/abs/2506.06342)
*Mohd Ashhad,Sana Rahmani,Mohammed Fayiz,Ali Etemad,Javad Hashemi*

Main category: eess.SP

TL;DR: The paper presents a multi-view deep neural network for classifying arrhythmias from ECG data using uncertainty-aware fusion.


<details>
  <summary>Details</summary>
Motivation: To improve arrhythmia classification from ECG by addressing noise and artifacts and utilizing multi-view learning for better accuracy and robustness.

Method: The method involves a three-module deep learning framework: (1) a time-series model for morphological features, (2) an image-space model for spatiotemporal features, and (3) a fusion module incorporating uncertainty between the two views.

Result: The experimental results showed improved arrhythmia classification performance and enhanced robustness to noise and artifacts on two datasets compared to state-of-the-art approaches.

Conclusion: Uncertainty-aware multi-view learning improves arrhythmia classification accuracy and robustness, demonstrating the effectiveness of the proposed deep neural architecture.

Abstract: We propose a deep neural architecture that performs uncertainty-aware
multi-view classification of arrhythmia from ECG. Our method learns two
different views (1D and 2D) of single-lead ECG to capture different types of
information. We use a fusion technique to reduce the conflict between the
different views caused by noise and artifacts in ECG data, thus incorporating
uncertainty to obtain stronger final predictions. Our framework contains the
following three modules (1) a time-series module to learn the morphological
features from ECG; (2) an image-space learning module to learn the
spatiotemporal features; and (3) the uncertainty-aware fusion module to fuse
the information from the two different views. Experimental results on two
real-world datasets demonstrate that our framework not only improves the
performance on arrhythmia classification compared to the state-of-the-art but
also shows better robustness to noise and artifacts present in ECG.

</details>


### [778] [LD-RPMNet: Near-Sensor Diagnosis for Railway Point Machines](https://arxiv.org/abs/2506.06346)
*Wei Li,Xiaochun Wu,Xiaoxi Hu,Yuxuan Zhang,Sebastian Bader,Yuhan Huang*

Main category: eess.SP

TL;DR: The study introduces LD-RPMNet, a lightweight model combining Transformers and CNNs for railway diagnostics, achieving 98.86% accuracy with reduced computational demands.


<details>
  <summary>Details</summary>
Motivation: Address the increasing need for efficient near-sensor diagnostic models in industrial applications, specifically for railway point machines.

Method: Developed LD-RPMNet, incorporating a Multi-scale Depthwise Separable Convolution (MDSC) module for enhanced feature extraction and a Broadcast Self-Attention (BSA) mechanism for computational efficiency.

Result: Achieved a 50% reduction in parameter count and computational complexity, with a diagnostic accuracy improvement of nearly 3%, reaching 98.86% accuracy.

Conclusion: LD-RPMNet demonstrates high accuracy and efficiency for near-sensor fault diagnosis in railway applications, showing its practical potential.

Abstract: Near-sensor diagnosis has become increasingly prevalent in industry. This
study proposes a lightweight model named LD-RPMNet that integrates Transformers
and Convolutional Neural Networks, leveraging both local and global feature
extraction to optimize computational efficiency for a practical railway
application. The LD-RPMNet introduces a Multi-scale Depthwise Separable
Convolution (MDSC) module, which decomposes cross-channel convolutions into
pointwise and depthwise convolutions while employing multi-scale kernels to
enhance feature extraction. Meanwhile, a Broadcast Self-Attention (BSA)
mechanism is incorporated to simplify complex matrix multiplications and
improve computational efficiency. Experimental results based on collected sound
signals during the operation of railway point machines demonstrate that the
optimized model reduces parameter count and computational complexity by 50%
while improving diagnostic accuracy by nearly 3%, ultimately achieving an
accuracy of 98.86%. This demonstrates the possibility of near-sensor fault
diagnosis applications in railway point machines.

</details>


### [779] [Multi-Platform Methane Plume Detection via Model and Domain Adaptation](https://arxiv.org/abs/2506.06348)
*Vassiliki Mancoridis,Brian Bue,Jake H. Lee,Andrew K. Thorpe,Daniel Cusworth,Alana Ayasse,Philip G. Brodrick,Riley Duren*

Main category: eess.SP

TL;DR: This paper uses machine learning methods, including transfer learning and CycleGAN, to improve methane plume detection by aligning data from different remote sensing platforms.


<details>
  <summary>Details</summary>
Motivation: There is a pressing need to prioritize methane detection due to its strong contribution to global warming and to address inconsistencies in detection across various remote sensing platforms.

Method: The researchers employ model- and data-driven machine learning approaches, including transfer learning to refine classifiers for spaceborne detection and CycleGAN for aligning data distributions between airborne and spaceborne contexts.

Result: Refining classifiers with transfer learning and translating spaceborne data to the airborne domain using CycleGAN led to improved methane plume detection, surpassing standalone spaceborne models.

Conclusion: This paper highlights the importance of cross-platform data alignment for remote sensing tasks, demonstrating effective machine learning techniques. While focused on methane plume detection, the methods can be generalized to other remote sensing data alignment challenges.

Abstract: Prioritizing methane for near-term climate action is crucial due to its
significant impact on global warming. Previous work used columnwise matched
filter products from the airborne AVIRIS-NG imaging spectrometer to detect
methane plume sources; convolutional neural networks (CNNs) discerned
anthropogenic methane plumes from false positive enhancements. However, as an
increasing number of remote sensing platforms are used for methane plume
detection, there is a growing need to address cross-platform alignment. In this
work, we describe model- and data-driven machine learning approaches that
leverage airborne observations to improve spaceborne methane plume detection,
reconciling the distributional shifts inherent with performing the same task
across platforms. We develop a spaceborne methane plume classifier using data
from the EMIT imaging spectroscopy mission. We refine classifiers trained on
airborne imagery from AVIRIS-NG campaigns using transfer learning,
outperforming the standalone spaceborne model. Finally, we use CycleGAN, an
unsupervised image-to-image translation technique, to align the data
distributions between airborne and spaceborne contexts. Translating spaceborne
EMIT data to the airborne AVIRIS-NG domain using CycleGAN and applying airborne
classifiers directly yields the best plume detection results. This methodology
is useful not only for data simulation, but also for direct data alignment.
Though demonstrated on the task of methane plume detection, our work more
broadly demonstrates a data-driven approach to align related products obtained
from distinct remote sensing instruments.

</details>


### [780] [Towards Generalizable Drowsiness Monitoring with Physiological Sensors: A Preliminary Study](https://arxiv.org/abs/2506.06360)
*Jiyao Wang,Suzan Ayas,Jiahao Zhang,Xiao Wen,Dengbo He,Birsen Donmez*

Main category: eess.SP

TL;DR: This paper focuses on physiological signal-based monitoring for detecting drowsiness and evaluates the relationship between various physiological metrics with drowsiness across different datasets.


<details>
  <summary>Details</summary>
Motivation: To provide privacy-preserving and effective methods for drowsiness monitoring by analyzing physiological signals, addressing conflicts in physiological responses across datasets.

Method: The researchers analyzed ECG, EDA, and RESP signals across four datasets with logistic regression models to assess physiological metrics and their association with drowsiness under varied conditions.

Result: Distinct physiological changes, such as increased heart rate stability, reduced respiratory amplitude, and decreased tonic EDA, were identified as consistent indicators of drowsiness.

Conclusion: Physiological responses to drowsiness vary with different inducers and assessment methods, with objective assessments being more sensitive. These insights can guide future designs for generalizable drowsiness monitoring systems.

Abstract: Accurately detecting drowsiness is vital to driving safety. Among all
measures, physiological-signal-based drowsiness monitoring can be more
privacy-preserving than a camera-based approach. However, conflicts exist
regarding how physiological metrics are associated with different drowsiness
labels across datasets. Thus, we analyzed key features from electrocardiograms
(ECG), electrodermal activity (EDA), and respiratory (RESP) signals across four
datasets, where different drowsiness inducers (such as fatigue and low arousal)
and assessment methods (subjective vs. objective) were used. Binary logistic
regression models were built to identify the physiological metrics that are
associated with drowsiness. Findings indicate that distinct different
drowsiness inducers can lead to different physiological responses, and
objective assessments were more sensitive than subjective ones in detecting
drowsiness. Further, the increased heart rate stability, reduced respiratory
amplitude, and decreased tonic EDA are robustly associated with increased
drowsiness. The results enhance understanding of drowsiness detection and can
inform future generalizable monitoring designs.

</details>


### [781] [Transformer-Based Decomposition of Electrodermal Activity for Real-World Mental Health Applications](https://arxiv.org/abs/2506.06378)
*Charalampos Tsirmpas,Stasinos Konstantopoulos,Dimitris Andrikopoulos,Konstantina Kyriakouli,Panagiotis Fatouros*

Main category: eess.SP

TL;DR: The study introduces the Feel Transformer, a Transformer-based model for decomposing electrodermal activity (EDA) signals into phasic and tonic components, showing superior performance on wearable device data.


<details>
  <summary>Details</summary>
Motivation: To provide an effective method for EDA signal decomposition which is crucial for emotional and physiological analysis, especially from wearable device data collected in real-world settings.

Method: The approach centers on the Feel Transformer, a novel deep learning model adapted from Autoformer, employing pooling and trend-removal mechanisms for unsupervised decomposition of EDA signals.

Result: The Feel Transformer outperforms traditional methods like Ledalab and cvxEDA in balancing feature fidelity and robustness in noisy, real-world datasets.

Conclusion: The Feel Transformer offers advancements for real-time EDA analysis, with meaningful applications in stress prediction, mental health interventions, and biosignal forecasting.

Abstract: Decomposing Electrodermal Activity (EDA) into phasic (short-term,
stimulus-linked responses) and tonic (longer-term baseline) components is
essential for extracting meaningful emotional and physiological biomarkers.
This study presents a comparative analysis of knowledge-driven, statistical,
and deep learning-based methods for EDA signal decomposition, with a focus on
in-the-wild data collected from wearable devices. In particular, the authors
introduce the Feel Transformer, a novel Transformer-based model adapted from
the Autoformer architecture, designed to separate phasic and tonic components
without explicit supervision. The model leverages pooling and trend-removal
mechanisms to enforce physiologically meaningful decompositions. Comparative
experiments against methods such as Ledalab, cvxEDA, and conventional
detrending show that the Feel Transformer achieves a balance between feature
fidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy,
real-world data. The model demonstrates potential for real-time biosignal
analysis and future applications in stress prediction, digital mental health
interventions, and physiological forecasting.

</details>


### [782] [IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G](https://arxiv.org/abs/2506.06718)
*Omar Mashaal,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: This paper introduces IQFM, the first foundational model for wireless communications based on raw IQ signal data, achieving superior performance across multiple tasks with minimal labeled data and preprocessing.


<details>
  <summary>Details</summary>
Motivation: The current potential of foundational models in other domains is limited in wireless communications, and exploring raw IQ data offers an opportunity for advancements without heavy reliance on handcrafted features.

Method: IQFM employs a lightweight encoder pre-trained via contrastive self-supervised learning on raw IQ data, coupled with a task-aware augmentation strategy tailored to core and specific transformations.

Result: IQFM achieves high accuracy (99.67% for modulation classification and 65.45% for AoA classification) with limited labeled samples, outperforms supervised baselines, and generalizes well to out-of-distribution tasks.

Conclusion: IQFM demonstrates raw IQ-based foundational models' capability as reusable, efficient encoders for diverse tasks in AI-driven wireless communication systems like 6G.

Abstract: Foundational models have shown remarkable potential in natural language
processing and computer vision, yet remain in their infancy in wireless
communications. While a few efforts have explored image-based modalities such
as channel state information (CSI) and frequency spectrograms, foundational
models that operate directly on raw IQ data remain largely unexplored. This
paper presents, IQFM, the first I/Q signal foundational model for wireless
communications. IQFM supporting diverse tasks: modulation classification,
angle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy
preprocessing or handcrafted features. We also introduce a task-aware
augmentation strategy that categorizes transformations into core augmentations,
such as cyclic time shifting, and task-specific augmentations. This strategy
forms the basis for structured, task-dependent representation learning within a
contrastive self-supervised learning (SSL) framework. Using this strategy, the
lightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data,
achieves up to 99.67% and 65.45% accuracy on modulation and AoA classification,
respectively, using only one labeled sample per class, outperforming supervised
baselines by up to 7x and 145x. The model also generalizes to
out-of-distribution tasks; when adapted to new tasks using only 500 samples per
class and minimal parameter updates via LoRA, the same frozen encoder achieves
94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a
modulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs.
96.64%). These results demonstrate the potential of raw IQ-based foundational
models as efficient, reusable encoders for multi-task learning in AI-native 6G
systems.

</details>


### [783] [Conditional Denoising Diffusion for ISAC Enhanced Channel Estimation in Cell-Free 6G](https://arxiv.org/abs/2506.06942)
*Mohammad Farzanullah,Han Zhang,Akram Bin Sediq,Ali Afana,Melike Erol-Kantarci*

Main category: eess.SP

TL;DR: The paper proposes a novel framework for improved channel estimation in cell-free ISAC systems by leveraging sensing information and advanced modeling techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of pilot contamination and noisy channel estimates to improve communication reliability in 6G networks.

Method: The method integrates Conditional Denoising Diffusion Model (CDDM) with a Multimodal Transformer (MMT) to leverage sensing and location data for iterative denoising and enhanced channel estimation.

Result: The proposed method achieves NMSE improvements of 8 dB and 9 dB compared with LS and MMSE estimators, respectively, and a 27.8% improvement over TDDM. It shows robustness against pilot contamination and maintains accuracy under low SNR conditions.

Conclusion: The model significantly enhances channel estimation performance for cell-free ISAC systems, leveraging the correlation between sensing and communication channels to provide robust and accurate results in challenging scenarios.

Abstract: Cell-free Integrated Sensing and Communication (ISAC) aims to revolutionize
6th Generation (6G) networks. By combining distributed access points with ISAC
capabilities, it boosts spectral efficiency, situational awareness, and
communication reliability. Channel estimation is a critical step in cell-free
ISAC systems to ensure reliable communication, but its performance is usually
limited by challenges such as pilot contamination and noisy channel estimates.
This paper presents a novel framework leveraging sensing information as a key
input within a Conditional Denoising Diffusion Model (CDDM). In this framework,
we integrate CDDM with a Multimodal Transformer (MMT) to enhance channel
estimation in ISAC-enabled cell-free systems. The MMT encoder effectively
captures inter-modal relationships between sensing and location data, enabling
the CDDM to iteratively denoise and refine channel estimates. Simulation
results demonstrate that the proposed approach achieves significant performance
gains. As compared with Least Squares (LS) and Minimum Mean Squared Error
(MMSE) estimators, the proposed model achieves normalized mean squared error
(NMSE) improvements of 8 dB and 9 dB, respectively. Moreover, we achieve a
27.8% NMSE improvement compared to the traditional denoising diffusion model
(TDDM), which does not incorporate sensing channel information. Additionally,
the model exhibits higher robustness against pilot contamination and maintains
high accuracy under challenging conditions, such as low signal-to-noise ratios
(SNRs). According to the simulation results, the model performs well for users
near sensing targets by leveraging the correlation between sensing and
communication channels.

</details>


### [784] [Diffusion Models-Aided Uplink Channel Estimation for RIS-Assisted Systems](https://arxiv.org/abs/2506.07770)
*Yang Wang,Yin Xu,Cixiao Zhang,Zhiyong Chen,Xiaowu Ou,Mingzeng Dai,Meixia Tao,Wenjun Zhang*

Main category: eess.SP

TL;DR: The paper introduces a novel channel estimation method for RIS-assisted systems using a deterministic diffusion model framework, emphasizing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in accurate channel estimation for RIS-assisted systems, leveraging modern diffusion models effectively while maintaining practical computational requirements.

Method: The authors reformulate the channel estimation process as a denoising procedure based on diffusion models. They introduce a deterministic sampling strategy with a step alignment mechanism to handle SNR variation and design a lightweight U-Net network for reduced computational complexity.

Result: The method outperforms baselines in extensive simulations, achieving up to 13.5 dB improvement in NMSE at SNR = 0 dB. The lightweight network achieves similar performance as the original U-Net with just 6.59% of its parameters.

Conclusion: The proposed approach combines accuracy and efficiency, making it suitable for practical RIS-assisted systems.

Abstract: This letter proposes a channel estimation method for reconfigurable
intelligent surface (RIS)-assisted systems through a novel diffusion model (DM)
framework. We reformulate the channel estimation problem as a denoising
process, which aligns with the reverse process of the DM. To overcome the
inherent randomness in the reverse process of conventional DM approaches, we
adopt a deterministic sampling strategy with a step alignment mechanism that
ensures the accuracy of channel estimation while adapting to different
signal-to-noise ratio (SNR). Furthermore, to reduce the number of parameters of
the U-Net, we meticulously design a lightweight network that achieves
comparable performance, thereby enhancing the practicality of our proposed
method. Extensive simulations demonstrate superior performance over a wide
range of SNRs compared to baselines. For instance, the proposed method achieves
performance improvements of up to 13.5 dB in normalized mean square error
(NMSE) at SNR = 0 dB. Notably, the proposed lightweight network exhibits almost
no performance loss compared to the original U-Net, while requiring only 6.59\%
of its parameters.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [785] [Robust predicate and function computation in continuous chemical reaction networks](https://arxiv.org/abs/2506.06590)
*Kim Calabrese,David Doty,Mina Latifi*

Main category: cs.CC

TL;DR: This paper explores chemical reaction networks (CRNs) for rate-constant-independent computation. It distinguishes their capabilities for Boolean predicates and numerical functions, introducing a relaxed notion of computation called robust computation.


<details>
  <summary>Details</summary>
Motivation: Understand the computational limits of chemical reaction networks (CRNs) when computing Boolean predicates and numerical functions, especially in a rate-independent setting.

Method: The study employs the continuous chemical reaction networks (CRNs) framework, investigating their computational capabilities using standard mass-action rate models and defining robust computation to relax strict computational requirements.

Result: Continuous CRNs are found highly limited in stably deciding Boolean predicates, but robust computation enables CRNs to decide Boolean combinations of threshold predicates. They can also compute piecewise affine functions with rational coefficients.

Conclusion: CRNs have potential for robust computations, despite their limitations in stable decision-making of certain predicates, expanding their capability and practical applicability.

Abstract: We initiate the study of rate-constant-independent computation of Boolean
predicates and numerical functions in the continuous model of chemical reaction
networks (CRNs), which model the amount of a chemical species as a nonnegative,
real-valued *concentration*. Real-valued numerical functions have previously
been studied, finding that exactly the continuous, piecewise rational linear
functions $f: \mathbb{R}_{> 0}^k \to \mathbb{R}_{> 0}$ can be computed
*stably*, a.k.a., *rate-independently*, meaning that the CRN gets the answer
correct no matter the rate at which reactions occur.
  We show that, contrary to functions, continuous CRNs are severely limited in
the Boolean predicates they can stably decide, reporting an answer based only
on which inputs are 0 or positive.
  This limitation motivates a slightly relaxed notion of rate-independent
computation in CRNs that we call *robust computation*. The standard mass-action
rate model is used, in which each reaction is assigned a rate equal to the
product of its reactant concentrations and its rate constant. The computation
is correct in this model if it converges to the correct output for any positive
choice of rate constants. This adversary is weaker than the stable computation
adversary, the latter being able to run reactions at non-mass-action rates.
  We show that CRNs can robustly decide every finite Boolean combination of
*threshold predicates*: those predicates defined by taking a rational weighted
sum of the inputs $\mathbf{x} \in \mathbb{R}^k_{\ge 0}$ and comparing to a
constant, answering the question ``Is $\sum_{i=1}^k w_i \cdot \mathbf{x}(i) >
h$?'', for rational weights $w_i$ and real threshold $h$. Turning to function
computation, we show that CRNs can robustly compute any piecewise affine
function with rational coefficients, where threshold predicates determine which
affine piece to evaluate for a given input.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [786] [Explaining Risks: Axiomatic Risk Attributions for Financial Models](https://arxiv.org/abs/2506.06653)
*Dangxing Chen*

Main category: q-fin.CP

TL;DR: The paper extends the Shapley value framework to allocate risk in machine learning models fairly.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the problem of risk attribution in high-risk sectors, such as finance, to ensure fair allocation of risk within complex machine learning models.

Method: The authors extend the Shapley value framework and provide both analytical and empirical examples to demonstrate its effectiveness in attributing risk.

Result: The extended Shapley value framework effectively allocates risk within machine learning models, making them more interpretable in critical sectors like finance.

Conclusion: Risk attribution can be achieved fairly and interpretably by extending the Shapley value framework, which provides a systematic approach to allocating risk in machine learning applications.

Abstract: In recent years, machine learning models have achieved great success at the
expense of highly complex black-box structures. By using axiomatic attribution
methods, we can fairly allocate the contributions of each feature, thus
allowing us to interpret the model predictions. In high-risk sectors such as
finance, risk is just as important as mean predictions. Throughout this work,
we address the following risk attribution problem: how to fairly allocate the
risk given a model with data? We demonstrate with analysis and empirical
examples that risk can be well allocated by extending the Shapley value
framework.

</details>


### [787] [Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling](https://arxiv.org/abs/2506.07299)
*Hans Buehler,Blanka Horvath,Yannick Limmer,Thorsten Schmidt*

Main category: q-fin.CP

TL;DR: This paper proposes new strategies for addressing model uncertainty in quantitative finance, introducing an uncertainty measure framework and a subsampling strategy inspired by bootstrapping and deep learning.


<details>
  <summary>Details</summary>
Motivation: Address the detrimental effects of model misestimation in critical areas of finance due to limited data and lack of true probability measures.

Method: Enhance objectives like expected utility using an uncertainty measure on the model space and introduce an efficient model-agnostic subsampling approach combined with adapted stochastic gradient descent.

Result: The proposed uncertainty measures and subsampling strategies outperform traditional methods and rival advanced Bayesian techniques in robustness and performance.

Conclusion: The techniques introduced provide a practical, efficient way to mitigate model risk and improve decision-making in quantitative finance under uncertainty.

Abstract: This paper addresses the challenge of model uncertainty in quantitative
finance, where decisions in portfolio allocation, derivative pricing, and risk
management rely on estimating stochastic models from limited data. In practice,
the unavailability of the true probability measure forces reliance on an
empirical approximation, and even small misestimations can lead to significant
deviations in decision quality. Building on the framework of Klibanoff et al.
(2005), we enhance the conventional objective - whether this is expected
utility in an investing context or a hedging metric - by superimposing an outer
"uncertainty measure", motivated by traditional monetary risk measures, on the
space of models. In scenarios where a natural model distribution is lacking or
Bayesian methods are impractical, we propose an ad hoc subsampling strategy,
analogous to bootstrapping in statistical finance and related to mini-batch
sampling in deep learning, to approximate model uncertainty. To address the
quadratic memory demands of naive implementations, we also present an adapted
stochastic gradient descent algorithm that enables efficient parallelization.
Through analytical, simulated, and empirical studies - including multi-period,
real data and high-dimensional examples - we demonstrate that uncertainty
measures outperform traditional mixture of measures strategies and our
model-agnostic subsampling-based approach not only enhances robustness against
model risk but also achieves performance comparable to more elaborate Bayesian
methods.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [788] [Human Side of Smart Contract Fuzzing: An Empirical Study](https://arxiv.org/abs/2506.07389)
*Guanming Qiao,Partha Protim Paul*

Main category: cs.HC

TL;DR: This paper investigates challenges practitioners face in adopting smart contract fuzzing tools through issue analysis and a user study.


<details>
  <summary>Details</summary>
Motivation: Understanding the barriers practitioners encounter with smart contract fuzzing to improve its adoption and effectiveness.

Method: Analysis of 381 GitHub issues from Echidna and Foundry, supplemented by a user study to categorize challenges into a taxonomy.

Result: Identified domain-specific challenges, including blockchain emulation issues and lack of documentation, and offered insights to enhance tool design.

Conclusion: Findings provide a taxonomy of SC fuzzing barriers and inform future improvements for tool usability and adoption.

Abstract: Smart contract (SC) fuzzing is a critical technique for detecting
vulnerabilities in blockchain applications. However, its adoption remains
challenging for practitioners due to fundamental differences between SCs and
traditional software systems. In this study, we investigate the challenges
practitioners face when adopting SC fuzzing tools by conducting an inductive
content analysis of 381 GitHub issues from two widely used SC fuzzers: Echidna
and Foundry. Furthermore, we conducted a user study to examine how these
challenges affect different practitioner groups, SC developers, and traditional
software security professionals, and identify strategies practitioners use to
overcome them. We systematically categorize these challenges into a taxonomy
based on their nature and occurrence within the SC fuzzing workflow. Our
findings reveal domain-specific ease-of-use and usefulness challenges,
including technical issues with blockchain emulation, and human issues with a
lack of accessible documentation and process automation. Our results provide
actionable insights for tool developers and researchers, guiding future
improvements in SC fuzzer tool design.

</details>


### [789] [LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models](https://arxiv.org/abs/2506.06874)
*Ala Yankouskaya,Areej B. Babiker,Syeda W. F. Rizvi,Sameha Alshakhsi,Magnus Liebherr,Raian Ali*

Main category: cs.HC

TL;DR: The paper introduces and validates a new 12-item questionnaire (LLM-D12) to assess dependency on large language models (LLMs), identifying two factors: Instrumental Dependency and Relationship Dependency.


<details>
  <summary>Details</summary>
Motivation: The authors seek to understand the nuanced interaction between humans and large language models beyond traditional behavioral addiction frameworks.

Method: They developed the LLM-D12 scale through theoretical groundwork, collected responses from 526 UK participants, and analyzed the data using factor analyses and external validation.

Result: The analysis revealed a two-factor structure with excellent internal consistency and distinct Instrumental Dependency (functional reliance) and Relationship Dependency (social/emotional connection) components.

Conclusion: The scale provides a valid tool to study LLM dependency, highlighting that reliance on LLMs may not always be dysfunctional but requires understanding within specific contexts.

Abstract: There is growing interest in understanding how people interact with large
language models (LLMs) and whether such models elicit dependency or even
addictive behaviour. Validated tools to assess the extent to which individuals
may become dependent on LLMs are scarce and primarily build on classic
behavioral addiction symptoms, adapted to the context of LLM use. We view this
as a conceptual limitation, as the LLM-human relationship is more nuanced and
warrants a fresh and distinct perspective. To address this gap, we developed
and validated a new 12-item questionnaire to measure LLM dependency, referred
to as LLM-D12. The scale was based on the authors' prior theoretical work, with
items developed accordingly and responses collected from 526 participants in
the UK. Exploratory and confirmatory factor analyses, performed on separate
halves of the total sample using a split-sample approach, supported a
two-factor structure: Instrumental Dependency (six items) and Relationship
Dependency (six items). Instrumental Dependency reflects the extent to which
individuals rely on LLMs to support or collaborate in decision-making and
cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs
as socially meaningful, sentient, or companion-like entities. The two-factor
structure demonstrated excellent internal consistency and clear discriminant
validity. External validation confirmed both the conceptual foundation and the
distinction between the two subscales. The psychometric properties and
structure of our LLM-D12 scale were interpreted in light of the emerging view
that dependency on LLMs does not necessarily indicate dysfunction but may still
reflect reliance levels that could become problematic in certain contexts.

</details>


### [790] [Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation](https://arxiv.org/abs/2506.07211)
*Gionnieve Lim,Bryan Chen Zhengyu Tan,Kellie Yu Hui Sim,Weiyan Shi,Ming Hui Chew,Ming Shan Hee,Roy Ka-Wei Lee,Simon T. Perrault,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: This paper explores the dual impact of Large Language Models (LLMs) on disinformation, both aiding its creation and enhancing its mitigation, using a communication game inspired by Werewolf.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the implications of LLMs in the realm of disinformation, particularly how they can be used both offensively and defensively.

Method: The authors used a communication game inspired by Werewolf, involving 25 participants operating as Disinformers, Moderators, and Users, to study the utilization of LLMs for different objectives.

Result: The study found diverse ways in which LLMs can be exploited or leveraged depending on the users' roles and strategies in combating or propagating disinformation.

Conclusion: The paper advocates a balanced approach in the development and use of LLMs to empower users and build trust while mitigating the risks of disinformation.

Abstract: The emergence of Large Language Models (LLMs) presents a dual challenge in
the fight against disinformation. These powerful tools, capable of generating
human-like text at scale, can be weaponised to produce sophisticated and
persuasive disinformation, yet they also hold promise for enhancing detection
and mitigation strategies. This paper investigates the complex dynamics between
LLMs and disinformation through a communication game that simulates online
forums, inspired by the game Werewolf, with 25 participants. We analyse how
Disinformers, Moderators, and Users leverage LLMs to advance their goals,
revealing both the potential for misuse and combating disinformation. Our
findings highlight the varying uses of LLMs depending on the participants'
roles and strategies, underscoring the importance of understanding their
effectiveness in this context. We conclude by discussing implications for
future LLM development and online platform design, advocating for a balanced
approach that empowers users and fosters trust while mitigating the risks of
LLM-assisted disinformation.

</details>


### [791] [Secondary Stakeholders in AI: Fighting for, Brokering, and Navigating Agency](https://arxiv.org/abs/2506.07281)
*Leah Hope Ajmani,Nuredin Ali Abdelkadir,Stevie Chancellor*

Main category: cs.HC

TL;DR: This paper explores how to extend participatory AI to secondary stakeholders by identifying key participatory ideals and archetypes that navigate challenges in AI interaction.


<details>
  <summary>Details</summary>
Motivation: The need to move beyond focusing solely on primary stakeholders, such as end-users, and to include secondary stakeholders in participatory AI development.

Method: Semi-structured interviews to theorize participatory ideals and identify challenges secondary stakeholders face.

Result: Three participatory ideals (informedness, consent, and agency) were identified and linked in a progressive ladder-like structure. Additionally, three archetypes of secondary stakeholders were introduced.

Conclusion: Broader participation in AI by secondary stakeholders is achievable through understanding systemic barriers and fostering meaningful interactions based on the outlined participatory ideals.

Abstract: As AI technologies become more human-facing, there have been numerous calls
to adapt participatory approaches to AI development -- spurring the idea of
participatory AI. However, these calls often focus only on primary
stakeholders, such as end-users, and not secondary stakeholders. This paper
seeks to translate the ideals of participatory AI to a broader population of
secondary AI stakeholders through semi-structured interviews. We theorize that
meaningful participation involves three participatory ideals: (1) informedness,
(2) consent, and (3) agency. We also explore how secondary stakeholders realize
these ideals by traversing a complicated problem space. Like walking up the
rungs of a ladder, these ideals build on one another. We introduce three
stakeholder archetypes: the reluctant data contributor, the unsupported
activist, and the well-intentioned practitioner, who must navigate systemic
barriers to achieving agentic AI relationships. We envision an AI future where
secondary stakeholders are able to meaningfully participate with the AI systems
they influence and are influenced by.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [792] [AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition](https://arxiv.org/abs/2506.06566)
*Chen Bao,Chuanbing Huo,Qinyu Chen,Chang Gao*

Main category: eess.AS

TL;DR: This paper introduces AS-ASR, an aphasia-specific speech recognition model based on Whisper-tiny, improving performance for aphasic speech while maintaining accuracy on standard speech.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of recognizing and transcribing aphasic speech, which is often noisy and difficult to process using standard systems.

Method: They develop a lightweight framework using Whisper-tiny and employ a hybrid training strategy combining standard and aphasic speech in varying ratios. Additionally, they use GPT-4 to enhance noisy aphasic transcripts.

Result: Experiments reveal a significant performance improvement, reducing Word Error Rate (WER) on aphasic speech by over 30%, while retaining precision for standard speech.

Conclusion: The framework is efficient, scalable, and suitable for low-resource environments, offering a practical solution for disordered speech recognition.

Abstract: This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition
framework based on Whisper-tiny, tailored for low-resource deployment on edge
devices. Our approach introduces a hybrid training strategy that systematically
combines standard and aphasic speech at varying ratios, enabling robust
generalization, and a GPT-4-based reference enhancement method that refines
noisy aphasic transcripts, improving supervision quality. We conduct extensive
experiments across multiple data mixing configurations and evaluation settings.
Results show that our fine-tuned model significantly outperforms the zero-shot
baseline, reducing WER on aphasic speech by over 30% while preserving
performance on standard speech. The proposed framework offers a scalable,
efficient solution for real-world disordered speech recognition.

</details>


### [793] [Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding](https://arxiv.org/abs/2506.07233)
*Tzu-wen Hsu,Ke-Han Lu,Cheng-Han Chiang,Hung-yi Lee*

Main category: eess.AS

TL;DR: The paper introduces Audio-Aware Decoding (AAD), an inference-time method to reduce hallucination in Large Audio-Language Models (LALMs), improving their performance significantly.


<details>
  <summary>Details</summary>
Motivation: LALMs often suffer from hallucination, misinterpreting or fabricating details about audio content despite strong benchmark performance.

Method: The paper proposes AAD, which uses contrastive decoding to evaluate token prediction probabilities with and without audio context, promoting more accurate tokens during inference.

Result: AAD improves the F1 score on object hallucination datasets by up to 0.428 and increases accuracy on general audio QA datasets by 5.4% to 10.3%.

Conclusion: AAD is a lightweight and effective strategy to mitigate hallucination in LALMs, enhancing their reliability and accuracy on various datasets.

Abstract: Large Audio-Language Models (LALMs) can take audio and text as the inputs and
answer questions about the audio. While prior LALMs have shown strong
performance on standard benchmarks, there has been alarming evidence that LALMs
can hallucinate what is presented in the audio. To mitigate the hallucination
of LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time
strategy that uses contrastive decoding to compare the token prediction logits
with and without the audio context. By contrastive decoding, AAD promotes the
tokens whose probability increases when the audio is present. We conduct our
experiment on object hallucination datasets with three LALMs and show that AAD
improves the F1 score by 0.046 to 0.428. We also show that AAD can improve the
accuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We
conduct thorough ablation studies to understand the effectiveness of each
component in AAD.

</details>


### [794] [Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition](https://arxiv.org/abs/2506.07515)
*Asahi Sakuma,Hiroaki Sato,Ryuga Sugano,Tadashi Kumano,Yoshihiko Kawai,Tetsuji Ogawa*

Main category: eess.AS

TL;DR: The paper introduces Speaker-Distinguishable CTC (SD-CTC), which improves multi-talker automatic speech recognition accuracy without auxiliary information, achieving comparable performance to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current approaches to multi-talker speech recognition, such as Serialized Output Training (SOT), suffer from errors due to speaker assignment failures. Extracting auxiliary information for recognition, like token-level timestamps, is challenging for conversational speech.

Method: The authors proposed SD-CTC, an enhanced version of CTC, assigning a token and speaker label to each frame. They integrated SD-CTC into the SOT framework and utilized multi-task learning to distinguish speakers using only speech and transcriptions.

Result: Experimental results showed a 26% error rate reduction compared to the SOT model without speaker distinction and performance comparable to systems using auxiliary information.

Conclusion: The proposed SD-CTC technique demonstrates that accurate speaker distinction can be achieved without auxiliary information, addressing a key limitation in multi-talker speech recognition systems.

Abstract: This paper presents a novel framework for multi-talker automatic speech
recognition without the need for auxiliary information. Serialized Output
Training (SOT), a widely used approach, suffers from recognition errors due to
speaker assignment failures. Although incorporating auxiliary information, such
as token-level timestamps, can improve recognition accuracy, extracting such
information from natural conversational speech remains challenging. To address
this limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension
of CTC that jointly assigns a token and its corresponding speaker label to each
frame. We further integrate SD-CTC into the SOT framework, enabling the SOT
model to learn speaker distinction using only overlapping speech and
transcriptions. Experimental comparisons show that multi-task learning with
SD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves
performance comparable to state-of-the-art methods relying on auxiliary
information.

</details>


### [795] [Neural Spectral Band Generation for Audio Coding](https://arxiv.org/abs/2506.06732)
*Woongjib Choi,Byeong Hyeon Kim,Hyungseob Lim,Inseon Jang,Hong-Goo Kang*

Main category: eess.AS

TL;DR: The paper discusses overcoming limitations in conventional spectral band replication (SBR) for audio bandwidth extension using deep neural networks (DNNs) for a more accurate reconstruction of high-frequency signals.


<details>
  <summary>Details</summary>
Motivation: Reconstructing high-frequency components of bandwidth-limited audio is critical due to issues like channel capacity and data constraints. Limitations in conventional SBR techniques necessitate advancements in this domain.

Method: The research proposes a parametric non-blind bandwidth extension framework that integrates DNN-based side information extraction and bandwidth extension at specific stages of the audio coding pipeline.

Result: Existing DNN-based methods were shown to be limited due to reliance on a blind approach, but the new method leverages side information to improve performance.

Conclusion: Combining SBR with DNNs strategically can overcome the shortcomings of blind bandwidth extension methods, providing a more effective solution for diverse audio signal reconstruction tasks.

Abstract: Audio bandwidth extension is the task of reconstructing missing high
frequency components of bandwidth-limited audio signals, where bandwidth
limitation is a common issue for audio signals due to several reasons,
including channel capacity and data constraints. While conventional spectral
band replication is a well-established parametric approach to audio bandwidth
extension, the SBR usually entails coarse feature extraction and reconstruction
techniques, which leads to limitations when processing various types of audio
signals. In parallel, numerous deep neural network-based audio bandwidth
extension methods have been proposed. These DNN-based methods are usually
referred to as blind BWE, as these methods do not rely on prior information
extracted from original signals, and only utilize given low frequency band
signals to estimate missing high frequency components. In order to replace
conventional SBR with DNNs, simply adopting existing DNN-based methodologies
results in suboptimal performance due to the blindness of these methods. My
proposed research suggests a new approach to parametric non-blind bandwidth
extension, as DNN-based side information extraction and DNN-based bandwidth
extension are performed only at the front and end of the audio coding pipeline.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [796] [Generative Voice Bursts during Phone Call](https://arxiv.org/abs/2506.07526)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Main category: cs.SD

TL;DR: The paper presents a method to deliver urgent audio messages during ongoing calls using generative AI.


<details>
  <summary>Details</summary>
Motivation: To address the inability of conventional mobile telephony to effectively convey urgent voice messages during another ongoing call.

Method: The system uses generative AI (e.g., GPT Neo) to create spoken messages from inputs like location, health data, or noise, delivering them as minimal disruptions prioritized by urgency.

Result: The proposed mechanism bypasses traditional call waiting limitations, delivering critical audio bursts seamlessly.

Conclusion: This approach can significantly improve emergency messaging and has potential applications in telecom and related industries.

Abstract: In critical situations, conventional mobile telephony fails to convey
emergency voice messages to a callee already engaged in another call. The
standard call waiting alert does not provide the urgency or content of the
waiting call. This paper proposes a novel method for transmitting Generative
Voice Bursts short, context aware audio messages during ongoing calls, from
either preauthorized or dynamically prioritized callers. By leveraging
generative AI techniques, the system automatically generates spoken messages
from contextual inputs example like location, health data, images, background
noise when the caller is unable to speak due to incapacitation or environmental
constraints. The solution incorporates voice, text, and priority inference
mechanisms, allowing high priority emergency messages to bypass conventional
call waiting barriers. The approach employs models such as GPT Neo for
generative text, which is synthesized into audio and delivered in configurable
intervals G seconds and counts N times, ensuring minimal disruption while
preserving urgency. This method holds potential for significant impact across
telecom, mobile device manufacturing, and emergency communication platforms.

</details>


### [797] [RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis](https://arxiv.org/abs/2506.07118)
*Yu-Xuan Wu,Ziyan Huang,Bin Hu,Zhi-Hong Guan*

Main category: cs.SD

TL;DR: The paper introduces a robust brain-inspired audio feature extractor (RBA-FE) for depression diagnosis, focusing on handling noise and improving precision through innovative mechanisms such as the ARSLIF spiking neuron model.


<details>
  <summary>Details</summary>
Motivation: Deep learning models mainly focus on image-based diagnostics, neglecting the importance of audio features. The paper aims to address noise challenges in audio feature extraction for improved depression diagnosis.

Method: The method involves using RBA-FE, which extracts six acoustic features from raw audio and leverages the ARSLIF spiking neuron model to enhance noise robustness by emulating brain signal-selectivity mechanisms.

Result: RBA-FE achieves high performance on MODMA dataset with precision, accuracy, recall, and F1 score nearing 0.88 and shows improved noise robustness on AVEC2014 and DAIC-WOZ datasets.

Conclusion: The proposed brain-inspired RBA-FE model displays superior accuracy and robustness for uncovering depressive audio features, offering a novel interpretation of abnormal neural patterns with practical applications in diagnosis.

Abstract: This article proposes a robust brain-inspired audio feature extractor
(RBA-FE) model for depression diagnosis, using an improved hierarchical network
architecture. Most deep learning models achieve state-of-the-art performance
for image-based diagnostic tasks, ignoring the counterpart audio features. In
order to tailor the noise challenge, RBA-FE leverages six acoustic features
extracted from the raw audio, capturing both spatial characteristics and
temporal dependencies. This hybrid attribute helps alleviate the precision
limitation in audio feature extraction within other learning models like deep
residual shrinkage networks. To deal with the noise issues, our model
incorporates an improved spiking neuron model, called adaptive rate smooth
leaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of
``retuning of cellular signal selectivity" in the brain attention systems,
which enhances the model robustness against environmental noises in audio data.
Experimental results demonstrate that RBA-FE achieves state-of-the-art accuracy
on the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in
precision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014
and DAIC-WOZ datasets both show enhancements in noise robustness. It is further
indicated by comparison that the ARSLIF neuron model suggest the abnormal
firing pattern within the feature extraction on depressive audio data, offering
brain-inspired interpretability.

</details>


### [798] [Speech Recognition on TV Series with Video-guided Post-Correction](https://arxiv.org/abs/2506.07323)
*Haoyuan Yang,Yue Zhang,Liqiang Jing*

Main category: cs.SD

TL;DR: This paper proposes a multimodal framework to refine automatic speech recognition (ASR) transcriptions using contextual video cues.


<details>
  <summary>Details</summary>
Motivation: ASR systems struggle in challenging environments like TV series where overlapping speech, domain-specific terminology, and long-range dependencies hinder transcription accuracy.

Method: The framework operates in two stages: ASR Generation and Video-based Post-Correction. Key contextual information is extracted using Video-Large Multimodal Model (VLMM) and integrated with a Large Language Model (LLM) for ASR correction.

Result: The method demonstrated improved transcription accuracy in multimedia benchmarks, showing the effectiveness of integrating video-based context.

Conclusion: Leveraging video-based cues through multimodal approaches enhances the robustness of ASR systems in complex settings, presenting novel opportunities for multimedia transcription applications.

Abstract: Automatic Speech Recognition (ASR) has achieved remarkable success with deep
learning, driving advancements in conversational artificial intelligence, media
transcription, and assistive technologies. However, ASR systems still struggle
in complex environments such as TV series, where overlapping speech,
domain-specific terminology, and long-range contextual dependencies pose
significant challenges to transcription accuracy. Existing multimodal
approaches fail to correct ASR outputs with the rich temporal and contextual
information available in video. To address this limitation, we propose a novel
multimodal post-correction framework that refines ASR transcriptions by
leveraging contextual cues extracted from video. Our framework consists of two
stages: ASR Generation and Video-based Post-Correction, where the first stage
produces the initial transcript and the second stage corrects errors using
Video-based Contextual Information Extraction and Context-aware ASR Correction.
We employ the Video-Large Multimodal Model (VLMM) to extract key contextual
information using tailored prompts, which is then integrated with a Large
Language Model (LLM) to refine the ASR output. We evaluate our method on a
multimodal benchmark for TV series ASR and demonstrate its effectiveness in
improving ASR performance by leveraging video-based context to enhance
transcription accuracy in complex multimedia environments.

</details>


### [799] [Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework](https://arxiv.org/abs/2506.07358)
*Kuiyuan Zhang,Wenjie Pei,Rushi Lan,Yifang Guo,Zhongyun Hua*

Main category: cs.SD

TL;DR: This paper presents a lightweight single-stream network for detecting audio-visual deepfakes, addressing inefficiencies of prior dual-stream models and improving performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome inefficiencies in existing audio-visual deepfake detection methods that use isolated feature-learning sub-models, making them unsuitable for resource-constrained environments.

Method: The paper introduces a single-stream multi-modal framework with a collaborative audio-visual learning block for continuous fusion of features and a multi-modal classification module to improve modality dependence and robustness.

Result: The method was evaluated on DF-TIMIT, FakeAVCeleb, and DFDC datasets, showing superior detection performance compared to state-of-the-art methods while being significantly lightweight, with only 0.48M parameters.

Conclusion: The proposed approach demonstrates that an efficient and lightweight design can outperform more complex models in detecting both uni-modal and multi-modal deepfakes across benchmark datasets.

Abstract: Deepfakes are AI-synthesized multimedia data that may be abused for spreading
misinformation. Deepfake generation involves both visual and audio
manipulation. To detect audio-visual deepfakes, previous studies commonly
employ two relatively independent sub-models to learn audio and visual
features, respectively, and fuse them subsequently for deepfake detection.
However, this may underutilize the inherent correlations between audio and
visual features. Moreover, utilizing two isolated feature learning sub-models
can result in redundant neural layers, making the overall model inefficient and
impractical for resource-constrained environments.
  In this work, we design a lightweight network for audio-visual deepfake
detection via a single-stream multi-modal learning framework. Specifically, we
introduce a collaborative audio-visual learning block to efficiently integrate
multi-modal information while learning the visual and audio features. By
iteratively employing this block, our single-stream network achieves a
continuous fusion of multi-modal features across its layers. Thus, our network
efficiently captures visual and audio features without the need for excessive
block stacking, resulting in a lightweight network design. Furthermore, we
propose a multi-modal classification module that can boost the dependence of
the visual and audio classifiers on modality content. It also enhances the
whole resistance of the video classifier against the mismatches between audio
and visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and
DFDC benchmark datasets. Compared to state-of-the-art audio-visual joint
detection methods, our method is significantly lightweight with only 0.48M
parameters, yet it achieves superiority in both uni-modal and multi-modal
deepfakes, as well as in unseen types of deepfakes.

</details>


### [800] [Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching](https://arxiv.org/abs/2506.07199)
*Ben Hayes,Charalampos Saitis,György Fazekas*

Main category: cs.SD

TL;DR: This paper addresses the challenge of audio synthesizer parameter inversion, proposing methods to handle inherent symmetries like permutation invariance, and introducing advanced models to improve performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the ill-posed problem of synthesizer parameter inversion caused by intrinsic symmetries, particularly in audio synthesis, to allow more accurate reproduction of sounds from given signals.

Method: The authors first analyze the disadvantages of regression under permutation symmetry. They propose using conditional generative models and permutation equivariant continuous normalizing flows to deal with symmetry. Additionally, they introduce a relaxed equivariance strategy to discover relevant symmetries adaptively for real-world synthesizers.

Result: The proposed methods are evaluated on the open-source Surge XT synthesizer, showing superior performance compared to regression and generative modeling baselines in audio reconstruction metrics.

Conclusion: Addressing symmetries in synthesizer parameter inversion, the proposed methodologies significantly improve the accuracy of reconstructing audio, offering a practical solution for real-world audio synthesis tasks.

Abstract: Many audio synthesizers can produce the same signal given different parameter
configurations, meaning the inversion from sound to parameters is an inherently
ill-posed problem. We show that this is largely due to intrinsic symmetries of
the synthesizer, and focus in particular on permutation invariance. First, we
demonstrate on a synthetic task that regressing point estimates under
permutation symmetry degrades performance, even when using a
permutation-invariant loss function or symmetry-breaking heuristics. Then,
viewing equivalent solutions as modes of a probability distribution, we show
that a conditional generative model substantially improves performance.
Further, acknowledging the invariance of the implicit parameter distribution,
we find that performance is further improved by using a permutation equivariant
continuous normalizing flow. To accommodate intricate symmetries in real
synthesizers, we also propose a relaxed equivariance strategy that adaptively
discovers relevant symmetries from data. Applying our method to Surge XT, a
full-featured open source synthesizer used in real world audio production, we
find our method outperforms regression and generative baselines across audio
reconstruction metrics.

</details>


### [801] [Towards Generalized Source Tracing for Codec-Based Deepfake Speech](https://arxiv.org/abs/2506.07294)
*Xuanjun Chen,I-Ming Lin,Lin Zhang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: The paper addresses the challenge of source tracing for codec-based deepfake speech, introducing SASTNet, a model leveraging advanced semantic and acoustic feature encoding. It achieves state-of-the-art performance on CodecFake+ dataset.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve source tracing for codec-based deepfake speech, overcoming the limitations of models that overfit to non-speech regions and fail to generalize to unseen data.

Method: They introduce the Semantic-Acoustic Source Tracing Network (SASTNet), combining Whisper for semantic encoding and Wav2vec2 with AudioMAE for acoustic encoding to enhance source tracing capabilities.

Result: SASTNet exhibits state-of-the-art performance on the CoSG test set from the CodecFake+ dataset, showcasing its robustness and reliability.

Conclusion: An innovative solution, SASTNet, effectively addresses challenges in source tracing for codec-based deepfake speech, ensuring improved generalization and performance on real-world data.

Abstract: Recent attempts at source tracing for codec-based deepfake speech
(CodecFake), generated by neural audio codec-based speech generation (CoSG)
models, have exhibited suboptimal performance. However, how to train source
tracing models using simulated CoSG data while maintaining strong performance
on real CoSG-generated audio remains an open challenge. In this paper, we show
that models trained solely on codec-resynthesized data tend to overfit to
non-speech regions and struggle to generalize to unseen content. To mitigate
these challenges, we introduce the Semantic-Acoustic Source Tracing Network
(SASTNet), which jointly leverages Whisper for semantic feature encoding and
Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet
achieves state-of-the-art performance on the CoSG test set of the CodecFake+
dataset, demonstrating its effectiveness for reliable source tracing.

</details>


### [802] [LeVo: High-Quality Song Generation with Multi-Preference Alignment](https://arxiv.org/abs/2506.07520)
*Shun Lei,Yaoxun Xu,Zhiwei Lin,Huaicheng Zhang,Wei Tan,Hangting Chen,Jianwei Yu,Yixuan Zhang,Chenyu Yang,Haina Zhu,Shuai Wang,Zhiyong Wu,Dong Yu*

Main category: cs.SD

TL;DR: LeVo, an advanced LM-based framework, enhances music generation by addressing challenges like sound quality and vocal-instrument harmony through innovative methods and token modeling strategies.


<details>
  <summary>Details</summary>
Motivation: Existing music generation models face challenges related to song composition, data scarcity, sound quality, and harmony between vocals and instruments.

Method: LeVo employs LeLM and a music codec, with mixed and dual-track token modeling and modular training strategies. Additionally, it uses Direct Preference Optimization for better alignment with human preferences.

Result: Experimental results show LeVo outperforms existing methods in qualitative and quantitative aspects, validated through ablation studies and audio demonstrations.

Conclusion: LeVo represents a significant advancement in music generation, resolving critical limitations in previous approaches through novel techniques in token handling and preference alignment.

Abstract: Recent advances in large language models (LLMs) and audio language models
have significantly improved music generation, particularly in lyrics-to-song
generation. However, existing approaches still struggle with the complex
composition of songs and the scarcity of high-quality data, leading to
limitations in sound quality, musicality, instruction following, and
vocal-instrument harmony. To address these challenges, we introduce LeVo, an
LM-based framework consisting of LeLM and a music codec. LeLM is capable of
parallelly modeling two types of tokens: mixed tokens, which represent the
combined audio of vocals and accompaniment to achieve vocal-instrument harmony,
and dual-track tokens, which separately encode vocals and accompaniment for
high-quality song generation. It employs two decoder-only transformers and a
modular extension training strategy to prevent interference between different
token types. To further enhance musicality and instruction following, we
introduce a multi-preference alignment method based on Direct Preference
Optimization (DPO). This method handles diverse human preferences through a
semi-automatic data construction process and DPO post-training. Experimental
results demonstrate that LeVo consistently outperforms existing methods on both
objective and subjective metrics. Ablation studies further justify the
effectiveness of our designs. Audio examples are available at
https://levo-demo.github.io/.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [803] [El0ps: An Exact L0-regularized Problems Solver](https://arxiv.org/abs/2506.06373)
*Théo Guyard,Cédric Herzet,Clément Elvira*

Main category: cs.MS

TL;DR: El0ps is a Python toolbox for solving L0-regularized problems with customization, high performance, and machine learning pipelines.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to address limitations in existing toolboxes for L0-regularized problems, particularly their lack of flexibility and integration with practical applications.

Method: The authors developed El0ps, a flexible framework that allows customizing problem instances, includes a state-of-the-art solver, and provides pre-built machine learning pipelines.

Result: El0ps achieves strong performance in solving L0-regularized problems while enabling customization and integration into real-world applications.

Conclusion: El0ps is a comprehensive, efficient, and versatile tool that supports the deployment of L0-regularized methods in practical use cases.

Abstract: This paper presents El0ps, a Python toolbox providing several utilities to
handle L0-regularized problems related to applications in machine learning,
statistics, and signal processing, among other fields. In contrast to existing
toolboxes, El0ps allows users to define custom instances of these problems
through a flexible framework, provides a dedicated solver achieving
state-of-the-art performance, and offers several built-in machine learning
pipelines. Our aim with El0ps is to provide a comprehensive tool which opens
new perspectives for the integration of L0-regularized problems in practical
applications.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [804] [Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization](https://arxiv.org/abs/2506.06305)
*Noémie Bergues,Arthur Carré,Paul Join-Lambert,Brice Hoffmann,Arnaud Blondel,Hamza Tajmouati*

Main category: q-bio.BM

TL;DR: This paper introduces a method for predicting the 3D structure of small molecules in protein binding sites using a two-step approach leveraging crystallized reference ligands.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of accurately predicting the 3D conformation of ligands in protein binding sites, a critical step in drug design, leveraging geometric priors from crystallized reference ligands.

Method: The paper proposes a two-stage method: (1) molecular alignment via flow-matching to produce initial 3D coordinates based on a reference template, and (2) refining these coordinates through a differentiable pose optimization incorporating shape similarities, pharmacophore alignment, internal energy, and optionally protein interactions.

Result: The approach demonstrated superior performance compared to conventional docking and alignment tools, particularly for ligands with low similarity to templates or high flexibility.

Conclusion: The method provides an advanced tool for ligand conformation prediction in drug design, improving accuracy in challenging cases and offering a new benchmark comparison.

Abstract: Predicting the 3D conformation of small molecules within protein binding
sites is a key challenge in drug design. When a crystallized reference ligand
(template) is available, it provides geometric priors that can guide 3D pose
prediction. We present a two-stage method for ligand conformation generation
guided by such templates. In the first stage, we introduce a molecular
alignment approach based on flow-matching to generate 3D coordinates for the
ligand, using the template structure as a reference. In the second stage, a
differentiable pose optimization procedure refines this conformation based on
shape and pharmacophore similarities, internal energy, and, optionally, the
protein binding pocket. We evaluate our approach on a new benchmark of ligand
pairs co-crystallized with the same target and show that it outperforms
standard docking tools and open-access alignment methods, especially in cases
involving low similarity to the template or high ligand flexibility.

</details>


### [805] [AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization](https://arxiv.org/abs/2506.07035)
*Zixuan Jiang,Renjing Xu*

Main category: q-bio.BM

TL;DR: AnnoDPO leverages Direct Preference Optimization (DPO) to improve protein function prediction by addressing annotation scarcity and imbalance.


<details>
  <summary>Details</summary>
Motivation: Protein function prediction struggles due to annotation category volume and distribution imbalance, necessitating novel approaches.

Method: The authors introduce AnnoDPO, a framework employing Direct Preference Optimization (DPO) with preference-aligned training objectives for protein function prediction.

Result: AnnoDPO successfully mitigates challenges posed by annotation scarcity and category imbalance, enhancing protein representation learning.

Conclusion: The framework establishes a new paradigm for integrating biological knowledge into protein function prediction using advanced preference alignment techniques.

Abstract: Deciphering protein function remains a fundamental challenge in protein
representation learning. The task presents significant difficulties for protein
language models (PLMs) due to the sheer volume of functional annotation
categories and the highly imbalanced distribution of annotated instances across
biological ontologies. Inspired by the remarkable success of reinforcement
learning from human feedback (RLHF) in large language model (LLM) alignment, we
propose AnnoDPO, a novel multi-modal framework for protein function prediction
that leverages Direct Preference Optimization (DPO) to enhance annotation
learning. Our methodology addresses the dual challenges of annotation scarcity
and category imbalance through preference-aligned training objectives,
establishing a new paradigm for biological knowledge integration in protein
representation learning.

</details>


### [806] [Graph Neural Networks in Modern AI-aided Drug Discovery](https://arxiv.org/abs/2506.06915)
*Odin Zhang,Haitao Lin,Xujun Zhang,Xiaorui Wang,Zhenxing Wu,Qing Ye,Weibo Zhao,Jike Wang,Kejun Ying,Yu Kang,Chang-yu Hsieh,Tingjun Hou*

Main category: q-bio.BM

TL;DR: Graph neural networks (GNNs) are increasingly crucial for AI-aided drug discovery due to their ability to analyze molecular structures and enable tasks like property prediction and molecular generation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the utility of GNNs in addressing complex drug-discovery challenges by leveraging their structure-aware modeling capabilities.

Method: The paper is a review that discusses methodological advances in GNNs, their integration with deep learning techniques, and their applications in drug discovery tasks.

Result: It provides a detailed examination of current uses and innovations in GNNs for molecular modeling and drug discovery, offering insights into challenges and bottlenecks.

Conclusion: GNNs are promising tools for drug discovery, though their practical application faces challenges. Future research should focus on scalability, interpretability, and overcoming real-world barriers.

Abstract: Graph neural networks (GNNs), as topology/structure-aware models within deep
learning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By
directly operating on molecular graphs, GNNs offer an intuitive and expressive
framework for learning the complex topological and geometric features of
drug-like molecules, cementing their role in modern molecular modeling. This
review provides a comprehensive overview of the methodological foundations and
representative applications of GNNs in drug discovery, spanning tasks such as
molecular property prediction, virtual screening, molecular generation,
biomedical knowledge graph construction, and synthesis planning. Particular
attention is given to recent methodological advances, including geometric GNNs,
interpretable models, uncertainty quantification, scalable graph architectures,
and graph generative frameworks. We also discuss how these models integrate
with modern deep learning approaches, such as self-supervised learning,
multi-task learning, meta-learning and pre-training. Throughout this review, we
highlight the practical challenges and methodological bottlenecks encountered
when applying GNNs to real-world drug discovery pipelines, and conclude with a
discussion on future directions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [807] [TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems](https://arxiv.org/abs/2506.07605)
*Marco Di Gennaro,Giovanni De Lucia,Stefano Longari,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: The paper introduces TimberStrike, an attack exploiting tree-based federated learning models, revealing security vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Exploration of privacy and security vulnerabilities in tree-based federated learning systems.

Method: Developed and evaluated TimberStrike, an optimization attack that uses split values and decision paths to infer sensitive data.

Result: TimberStrike reconstructed 73.05%-95.63% of target datasets in various federated gradient boosting frameworks.

Conclusion: Tree-based federated learning models are vulnerable; Differential Privacy offers partial mitigation but impacts model performance.

Abstract: Federated Learning has emerged as a privacy-oriented alternative to
centralized Machine Learning, enabling collaborative model training without
direct data sharing. While extensively studied for neural networks, the
security and privacy implications of tree-based models remain underexplored.
This work introduces TimberStrike, an optimization-based dataset reconstruction
attack targeting horizontally federated tree-based models. Our attack, carried
out by a single client, exploits the discrete nature of decision trees by using
split values and decision paths to infer sensitive training data from other
clients. We evaluate TimberStrike on State-of-the-Art federated gradient
boosting implementations across multiple frameworks, including Flower, NVFlare,
and FedTree, demonstrating their vulnerability to privacy breaches. On a
publicly available stroke prediction dataset, TimberStrike consistently
reconstructs between 73.05% and 95.63% of the target dataset across all
implementations. We further analyze Differential Privacy, showing that while it
partially mitigates the attack, it also significantly degrades model
performance. Our findings highlight the need for privacy-preserving mechanisms
specifically designed for tree-based Federated Learning systems, and we provide
preliminary insights into their design.

</details>


### [808] [Exploiting Inaccurate Branch History in Side-Channel Attacks](https://arxiv.org/abs/2506.07263)
*Yuhui Zhu,Alessandro Biondi*

Main category: cs.CR

TL;DR: Modern Branch Prediction Units (BPUs) optimize speculative execution but can lead to severe security vulnerabilities, enabling attacks like Spectre-BSE, Spectre-BHS, and BiasScope, which are exploited to leak sensitive data from CPUs.


<details>
  <summary>Details</summary>
Motivation: To investigate how shared branch prediction resources could potentially lead to security vulnerabilities in CPUs, compromising sensitive information across software contexts.

Method: The paper dissects modern Branch Prediction Units (BPUs), focusing on Bias-Free Branch Prediction and Branch History Speculation. The authors analyze their operational mechanisms, identify the security risks, and validate the risks through attacks.

Result: The analysis revealed vulnerabilities that expose cross-privilege attack surfaces for Branch History Injection (BHI). The researchers developed three novel attack primitives: Spectre-BSE, Spectre-BHS, and BiasScope, with successful exploitation demonstrated on multiple processors. The Chimera demonstrator achieved kernel memory leakage at high bit rates.

Conclusion: The study concludes that while BPUs enhance speculative execution efficiency, they also pose significant security threats. Effective isolation and sanitization methods are critical to securing shared branch prediction resources.

Abstract: Modern out-of-order CPUs heavily rely on speculative execution for
performance optimization, with branch prediction serving as a cornerstone to
minimize stalls and maximize efficiency. Whenever shared branch prediction
resources lack proper isolation and sanitization methods, they may originate
security vulnerabilities that expose sensitive data across different software
contexts.
  This paper examines the fundamental components of modern Branch Prediction
Units (BPUs) and investigates how resource sharing and contention affect two
widely implemented but underdocumented features: Bias-Free Branch Prediction
and Branch History Speculation. Our analysis demonstrates that these BPU
features, while designed to enhance speculative execution efficiency through
more accurate branch histories, can also introduce significant security risks.
We show that these features can inadvertently modify the Branch History Buffer
(BHB) update behavior and create new primitives that trigger malicious
mis-speculations.
  This discovery exposes previously unknown cross-privilege attack surfaces for
Branch History Injection (BHI). Based on these findings, we present three novel
attack primitives: two Spectre attacks, namely Spectre-BSE and Spectre-BHS, and
a cross-privilege control flow side-channel attack called BiasScope. Our
research identifies corresponding patterns of vulnerable control flows and
demonstrates exploitation on multiple processors. Finally, Chimera is
presented: an attack demonstrator based on eBPF for a variant of Spectre-BHS
that is capable of leaking kernel memory contents at 24,628 bit/s.

</details>


### [809] [TimeWak: Temporal Chained-Hashing Watermark for Time Series Data](https://arxiv.org/abs/2506.06407)
*Zhi Wen Soi,Chaoyi Zhu,Fouad Abiad,Aditya Shankar,Jeroen M. Galjaard,Huijuan Wang,Lydia Y. Chen*

Main category: cs.CR

TL;DR: TimeWak is a watermarking algorithm designed for multivariate time series diffusion models, addressing challenges in embedding watermarks in the real temporal-feature space while maintaining high utility and detectability.


<details>
  <summary>Details</summary>
Motivation: The increasing use of synthetic time series for privacy-sensitive data, such as medical records, raises the need for watermarked data to ensure utility and traceability. Existing watermarking approaches are incompatible with state-of-the-art generators working in real space, posing a challenge in watermarking for time series.

Method: TimeWak embeds temporal chained-hashing watermarks directly in the real temporal-feature space to handle feature heterogeneity and temporal dependencies. It employs an $
$-exact inversion mechanism to tackle non-uniform reconstruction error distributions, enabling accurate watermark detection.

Result: When evaluated across 5 datasets with varying temporal lengths, TimeWak improves data quality significantly, showing a 61.96% improvement in context-FID score and an 8.44% improvement in correlational scores compared to baselines. It maintains high watermark detectability and is robust against post-editing attacks.

Conclusion: TimeWak is the first solution enabling effective watermarking for multivariate time series diffusion models, achieving both high-quality synthetic data and robust traceable watermarking while overcoming challenges from temporal dependencies and feature heterogeneity.

Abstract: Synthetic time series generated by diffusion models enable sharing
privacy-sensitive datasets, such as patients' functional MRI records. Key
criteria for synthetic data include high data utility and traceability to
verify the data source. Recent watermarking methods embed in homogeneous latent
spaces, but state-of-the-art time series generators operate in real space,
making latent-based watermarking incompatible. This creates the challenge of
watermarking directly in real space while handling feature heterogeneity and
temporal dependencies. We propose TimeWak, the first watermarking algorithm for
multivariate time series diffusion models. To handle temporal dependence and
spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark
directly within the real temporal-feature space. The other unique feature is
the $\epsilon$-exact inversion, which addresses the non-uniform reconstruction
error distribution across features from inverting the diffusion process to
detect watermarks. We derive the error bound of inverting multivariate time
series and further maintain high watermark detectability. We extensively
evaluate TimeWak on its impact on synthetic data quality, watermark
detectability, and robustness under various post-editing attacks, against 5
datasets and baselines of different temporal lengths. Our results show that
TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in
correlational scores against the state-of-the-art baseline, while remaining
consistently detectable.

</details>


### [810] [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Chen-Fu Chen,Haim Permuter,Sajani Vithana,Flavio P. Calmon*

Main category: cs.CR

TL;DR: The paper introduces two new watermarking methods for large language models (LLMs) – HeavyWater and SimplexWater – that achieve high detection accuracy and low text distortion while addressing challenges in low-entropy tasks like coding.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of authenticating text provenance, curbing misuse of AI-generated text, and promoting trust in LLMs, especially in scenarios where tasks like code generation involve deterministic predictions.

Method: The authors propose an optimization framework for designing effective LLM watermarks that use random side information. They develop tunable watermarking methods (HeavyWater and SimplexWater) to balance detection accuracy against text distortion.

Result: HeavyWater and SimplexWater demonstrate superior performance in benchmarks, achieving high watermark detection accuracy with minimal impact on text quality, especially in low-entropy text generation tasks.

Conclusion: The study underscores the importance of optimized watermarking techniques in LLMs, introduces two versatile watermarks, and highlights connections to coding theory while making the methods applicable universally.

Abstract: Large language model (LLM) watermarks enable authentication of text
provenance, curb misuse of machine-generated text, and promote trust in AI
systems. Current watermarks operate by changing the next-token predictions
output by an LLM. The updated (i.e., watermarked) predictions depend on random
side information produced, for example, by hashing previously generated tokens.
LLM watermarking is particularly challenging in low-entropy generation tasks -
such as coding - where next-token predictions are near-deterministic. In this
paper, we propose an optimization framework for watermark design. Our goal is
to understand how to most effectively use random side information in order to
maximize the likelihood of watermark detection and minimize the distortion of
generated text. Our analysis informs the design of two new watermarks:
HeavyWater and SimplexWater. Both watermarks are tunable, gracefully
trading-off between detection accuracy and text distortion. They can also be
applied to any LLM and are agnostic to side information generation. We examine
the performance of HeavyWater and SimplexWater through several benchmarks,
demonstrating that they can achieve high watermark detection accuracy with
minimal compromise of text generation quality, particularly in the low-entropy
regime. Our theoretical analysis also reveals surprising new connections
between LLM watermarking and coding theory. The code implementation can be
found in https://github.com/DorTsur/HeavyWater_SimplexWater

</details>


### [811] [Benchmarking Misuse Mitigation Against Covert Adversaries](https://arxiv.org/abs/2506.06414)
*Davis Brown,Mahdi Sabbaghi,Luze Sun,Alexander Robey,George J. Pappas,Eric Wong,Hamed Hassani*

Main category: cs.CR

TL;DR: The paper focuses on detecting and defending against complex covert attacks on language models, introducing a novel evaluation framework and datasets for this purpose.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the vulnerability of language models to covert attacks, where attackers use benign-seeming small queries to achieve dangerous outcomes collectively, overcoming the limitations of existing safety evaluations.

Method: They developed Benchmarks for Stateful Defenses (BSD), a data generation pipeline that simulates covert attacks on language models, allowing for the evaluation and development of defensive mechanisms.

Result: The study presents two new datasets that current advanced language models consistently refuse, while weaker models struggle with, demonstrating the effectiveness of decomposition attacks and the potential of stateful defenses.

Conclusion: This research underscores the importance of stateful defensive strategies to mitigate covert misuse of language models, highlighting the need for better safeguards against such attacks.

Abstract: Existing language model safety evaluations focus on overt attacks and
low-stakes tasks. Realistic attackers can subvert current safeguards by
requesting help on small, benign-seeming tasks across many independent queries.
Because individual queries do not appear harmful, the attack is hard to
{detect}. However, when combined, these fragments uplift misuse by helping the
attacker complete hard and dangerous tasks. Toward identifying defenses against
such strategies, we develop Benchmarks for Stateful Defenses (BSD), a data
generation pipeline that automates evaluations of covert attacks and
corresponding defenses. Using this pipeline, we curate two new datasets that
are consistently refused by frontier models and are too difficult for weaker
open-weight models. Our evaluations indicate that decomposition attacks are
effective misuse enablers, and highlight stateful defenses as a countermeasure.

</details>


### [812] [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)
*Xiaoyuan Zhu,Yaowen Ye,Tianyi Qiu,Hanlin Zhu,Sijun Tan,Ajraf Mannan,Jonathan Michala,Raluca Ada Popa,Willie Neiswanger*

Main category: cs.CR

TL;DR: The paper introduces a method to detect discrepancies between black-box LLM APIs and authentic models through a rank-based uniformity test.


<details>
  <summary>Details</summary>
Motivation: API providers may substitute or modify underlying LLMs for cost reduction or malicious purposes, leading to degraded performance and compromised safety.

Method: A rank-based uniformity test that verifies behavioral equality between black-box LLM APIs and authentic locally deployed models, using query-efficient and robust techniques.

Result: The proposed method achieves high accuracy and statistical power under constrained query budgets across diverse threat scenarios like quantization, fine-tuning, jailbreak prompts, and model substitution.

Conclusion: This approach provides an effective solution to detect API-level discrepancies, mitigating risks posed by adversarial API providers.

Abstract: As API access becomes a primary interface to large language models (LLMs),
users often interact with black-box systems that offer little transparency into
the deployed model. To reduce costs or maliciously alter model behaviors, API
providers may discreetly serve quantized or fine-tuned variants, which can
degrade performance and compromise safety. Detecting such substitutions is
difficult, as users lack access to model weights and, in most cases, even
output logits. To tackle this problem, we propose a rank-based uniformity test
that can verify the behavioral equality of a black-box LLM to a locally
deployed authentic model. Our method is accurate, query-efficient, and avoids
detectable query patterns, making it robust to adversarial providers that
reroute or mix responses upon the detection of testing attempts. We evaluate
the approach across diverse threat scenarios, including quantization, harmful
fine-tuning, jailbreak prompts, and full model substitution, showing that it
consistently achieves superior statistical power over prior methods under
constrained query budgets.

</details>


### [813] [HauntAttack: When Attack Follows Reasoning as a Shadow](https://arxiv.org/abs/2506.07031)
*Jingyuan Ma,Rui Li,Zheng Li,Junfeng Liu,Lei Sha,Zhifang Sui*

Main category: cs.CR

TL;DR: The paper discusses the vulnerabilities of Large Reasoning Models (LRMs) in safety when reasoning is strongly entangled with harmfulness. It introduces HauntAttack, a novel framework to highlight these limitations through black-box attacks, showcasing unsafe outcomes from even the most advanced models.


<details>
  <summary>Details</summary>
Motivation: To investigate the trade-off between reasoning ability and safety in LRMs, particularly in scenarios where reasoning leads models to unsafe or harmful outputs.

Method: The paper proposes HauntAttack, a black-box attack framework that embeds harmful instructions into reasoning questions, altering one of the original conditions to create a pathway for unsafe outputs.

Result: The experiments reveal significant safety vulnerabilities in top-performing LRMs, along with diverse patterns in outputs when exposed to different types of harmful instructions.

Conclusion: LRMs, despite their reasoning prowess, exhibit considerable safety risks when exposed to adversarial prompts like those in HauntAttack. This calls for enhanced security measures and deeper analysis of their behaviors.

Abstract: Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and
reasoning tasks, showcasing exceptional capabilities. However, the enhancement
of reasoning abilities and the exposure of their internal reasoning processes
introduce new safety vulnerabilities. One intriguing concern is: when reasoning
is strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs
exhibit? To address this issue, we introduce HauntAttack, a novel and
general-purpose black-box attack framework that systematically embeds harmful
instructions into reasoning questions. Specifically, we treat reasoning
questions as carriers and substitute one of their original conditions with a
harmful instruction. This process creates a reasoning pathway in which the
model is guided step by step toward generating unsafe outputs. Based on
HauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results
reveal that even the most advanced LRMs exhibit significant safety
vulnerabilities. Additionally, we perform a detailed analysis of different
models, various types of harmful instructions, and model output patterns,
providing valuable insights into the security of LRMs.

</details>


### [814] [Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures](https://arxiv.org/abs/2506.07402)
*Yukai Zhou,Sibei Yang,Wenjie Wang*

Main category: cs.CR

TL;DR: This paper introduces implicit harm within Large Language Models (LLMs), an often-overlooked risk of incorrectly answering harmless-looking inputs, and provides JailFlipBench for their systematic study.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in evaluating risks associated with LLMs beyond overtly malicious inputs, targeting implicit harm that can have significant real-world consequences.

Method: The authors reformulate the LLM risk landscape through a quadrant framework based on input/output metrics, introduce JailFlipBench to evaluate implicit harm across modalities, and develop attack methodologies.

Result: Extensive evaluations on various LLMs demonstrate the urgency and prevalence of implicit harm, which poses real-world risks that currently escape conventional measures.

Conclusion: Conventional jailbreak paradigms are insufficient to fully assess LLM safety. There is a need for broader evaluations and advanced safety measures to mitigate implicit harm risks.

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about their security. While jailbreak attacks
highlight failures under overtly harmful queries, they overlook a critical
risk: incorrectly answering harmless-looking inputs can be dangerous and cause
real-world harm (Implicit Harm). We systematically reformulate the LLM risk
landscape through a structured quadrant perspective based on output factuality
and input harmlessness, uncovering an overlooked high-risk region. To
investigate this gap, we propose JailFlipBench, a benchmark aims to capture
implicit harm, spanning single-modal, multimodal, and factual extension
scenarios with diverse evaluation metrics. We further develop initial JailFlip
attack methodologies and conduct comprehensive evaluations across multiple
open-source and black-box LLMs, show that implicit harm present immediate and
urgent real-world risks, calling for broader LLM safety assessments and
alignment beyond conventional jailbreak paradigms.

</details>


### [815] [Fuse and Federate: Enhancing EV Charging Station Security with Multimodal Fusion and Federated Learning](https://arxiv.org/abs/2506.06730)
*Rabah Rahal,Abdelaziz Amara Korba,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: The paper introduces a new intrusion detection framework for EVSE, utilizing multimodal data and federated learning to achieve high precision and detection rates in decentralized environments.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of EVs and the integral role of EVSE systems in the smart grid create critical cybersecurity challenges, necessitating advanced detection mechanisms.

Method: The framework integrates multimodal data from network traffic and kernel events, utilizes distributed learning, and employs federated learning to ensure collaborative intelligence and data privacy.

Result: The proposed framework achieves a detection rate of over 98% and a precision rate exceeding 97%, outperforming existing intrusion detection methods in decentralized settings.

Conclusion: This framework offers a scalable, efficient, and privacy-preserving solution to address advanced cybersecurity threats in EVSE systems.

Abstract: The rapid global adoption of electric vehicles (EVs) has established electric
vehicle supply equipment (EVSE) as a critical component of smart grid
infrastructure. While essential for ensuring reliable energy delivery and
accessibility, EVSE systems face significant cybersecurity challenges,
including network reconnaissance, backdoor intrusions, and distributed
denial-of-service (DDoS) attacks. These emerging threats, driven by the
interconnected and autonomous nature of EVSE, require innovative and adaptive
security mechanisms that go beyond traditional intrusion detection systems
(IDS). Existing approaches, whether network-based or host-based, often fail to
detect sophisticated and targeted attacks specifically crafted to exploit new
vulnerabilities in EVSE infrastructure. This paper proposes a novel intrusion
detection framework that leverages multimodal data sources, including network
traffic and kernel events, to identify complex attack patterns. The framework
employs a distributed learning approach, enabling collaborative intelligence
across EVSE stations while preserving data privacy through federated learning.
Experimental results demonstrate that the proposed framework outperforms
existing solutions, achieving a detection rate above 98% and a precision rate
exceeding 97% in decentralized environments. This solution addresses the
evolving challenges of EVSE security, offering a scalable and privacypreserving
response to advanced cyber threats

</details>


### [816] [Ai-Driven Vulnerability Analysis in Smart Contracts: Trends, Challenges and Future Directions](https://arxiv.org/abs/2506.06735)
*Mesut Ozdag*

Main category: cs.CR

TL;DR: The paper explores AI-driven techniques for detecting vulnerabilities in smart contracts, evaluating methods like machine learning, deep learning, GNNs, and transformers.


<details>
  <summary>Details</summary>
Motivation: Blockchain ecosystems face critical security issues in smart contracts, leading to financial losses. Traditional auditing methods are inadequate for scalability and adaptability.

Method: The paper reviews AI-based techniques for vulnerability detection, examining methods like machine learning, deep learning, graph neural networks, and transformers.

Result: The study compares these AI techniques considering their accuracy, interpretability, computational costs, and real-world applicability.

Conclusion: AI-based methods show promise in overcoming traditional auditing limitations but face challenges in scalability, interpretability, and adaptability to evolving vulnerabilities.

Abstract: Smart contracts, integral to blockchain ecosystems, enable decentralized
applications to execute predefined operations without intermediaries. Their
ability to enforce trustless interactions has made them a core component of
platforms such as Ethereum. Vulnerabilities such as numerical overflows,
reentrancy attacks, and improper access permissions have led to the loss of
millions of dollars throughout the blockchain and smart contract sector.
Traditional smart contract auditing techniques such as manual code reviews and
formal verification face limitations in scalability, automation, and
adaptability to evolving development patterns. As a result, AI-based solutions
have emerged as a promising alternative, offering the ability to learn complex
patterns, detect subtle flaws, and provide scalable security assurances. This
paper examines novel AI-driven techniques for vulnerability detection in smart
contracts, focusing on machine learning, deep learning, graph neural networks,
and transformer-based models. This paper analyzes how each technique represents
code, processes semantic information, and responds to real world vulnerability
classes. We also compare their strengths and weaknesses in terms of accuracy,
interpretability, computational overhead, and real time applicability. Lastly,
it highlights open challenges and future opportunities for advancing this
domain.

</details>


### [817] [Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection](https://arxiv.org/abs/2506.00654)
*Marco Di Gennaro,Francesco Panebianco,Marco Pianta,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: The paper presents Amatriciana, a Graph Neural Network-based model for detecting money laundering with an emphasis on using the whole transactional graph and lowering false positives.


<details>
  <summary>Details</summary>
Motivation: Money laundering undermines financial integrity and social security, necessitating advanced automated tools for its detection in complex, large-volume transactions.

Method: Amatriciana leverages Graph Neural Networks to analyze entire transactional graphs with temporal data, avoiding subdivision into time-based subgraphs.

Result: Experiments on a public dataset demonstrate that the model effectively learns even with limited data and performs better than state-of-the-art methods, achieving an F1 score of 0.76 and reducing false positives by 55%.

Conclusion: Amatriciana significantly enhances money laundering detection capabilities, making it a valuable tool for law enforcement by improving both accuracy and reducing false positives.

Abstract: Money laundering is a financial crime that poses a serious threat to
financial integrity and social security. The growing number of transactions
makes it necessary to use automatic tools that help law enforcement agencies
detect such criminal activity. In this work, we present Amatriciana, a novel
approach based on Graph Neural Networks to detect money launderers inside a
graph of transactions by considering temporal information. Amatriciana uses the
whole graph of transactions without splitting it into several time-based
subgraphs, exploiting all relational information in the dataset. Our
experiments on a public dataset reveal that the model can learn from a limited
amount of data. Furthermore, when more data is available, the model outperforms
other State-of-the-art approaches; in particular, Amatriciana decreases the
number of False Positives (FPs) while detecting many launderers. In summary,
Amatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%
with respect to other State-of-the-art models.

</details>


### [818] [Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models](https://arxiv.org/abs/2506.07077)
*Qianshan Wei,Jiaqi Li,Zihan You,Yi Zhan,Kecen Li,Jialin Wu,Xinfeng Li Hengjun Liu,Yi Yu,Bin Cao,Yiwen Xu,Yang Liu,Guilin Qi*

Main category: cs.CR

TL;DR: The paper introduces Dual-Priv Pruning to address computational and utility challenges in applying Differential Privacy (DP) to Multimodal Large Language Models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the limitations of existing DP methods for MLLMs, which face efficiency and quality trade-offs due to the noise scaling with parameter dimensionality and high computational overhead.

Method: Proposed Dual-Priv Pruning framework, which includes visual token pruning to reduce input dimensionality and gradient-update pruning to reduce the impact of noise during DP optimization.

Result: The method demonstrates competitive results with lower performance degradation and better memory efficiency compared to standard DP-SGD and other methods.

Conclusion: Dual-Priv Pruning effectively balances the trade-offs between privacy and utility in MLLMs while improving computational efficiency, marking the first exploration of DP fine-tuning in this domain.

Abstract: Differential Privacy (DP) is a widely adopted technique, valued for its
effectiveness in protecting the privacy of task-specific datasets, making it a
critical tool for large language models. However, its effectiveness in
Multimodal Large Language Models (MLLMs) remains uncertain. Applying
Differential Privacy (DP) inherently introduces substantial computation
overhead, a concern particularly relevant for MLLMs which process extensive
textual and visual data. Furthermore, a critical challenge of DP is that the
injected noise, necessary for privacy, scales with parameter dimensionality,
leading to pronounced model degradation; This trade-off between privacy and
utility complicates the application of Differential Privacy (DP) to complex
architectures like MLLMs. To address these, we propose Dual-Priv Pruning, a
framework that employs two complementary pruning mechanisms for DP fine-tuning
in MLLMs: (i) visual token pruning to reduce input dimensionality by removing
redundant visual information, and (ii) gradient-update pruning during the DP
optimization process. This second mechanism selectively prunes parameter
updates based on the magnitude of noisy gradients, aiming to mitigate noise
impact and improve utility. Experiments demonstrate that our approach achieves
competitive results with minimal performance degradation. In terms of
computational efficiency, our approach consistently utilizes less memory than
standard DP-SGD. While requiring only 1.74% more memory than zeroth-order
methods which suffer from severe performance issues on A100 GPUs, our method
demonstrates leading memory efficiency on H20 GPUs. To the best of our
knowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is
coming soon.

</details>


### [819] [A Systematic Review of Poisoning Attacks Against Large Language Models](https://arxiv.org/abs/2506.06518)
*Neil Fendley,Edward W. Staley,Joshua Carney,William Redman,Marie Chau,Nathan Drenkow*

Main category: cs.CR

TL;DR: The paper reviews LLM poisoning attack literature, proposes a comprehensive threat model, and categorizes attacks based on four key dimensions.


<details>
  <summary>Details</summary>
Motivation: Concerns about security risks in pretrained LLMs, particularly poisoning attacks, have grown due to their widespread usage and foundational role.

Method: Conducted a systematic review of LLM poisoning attacks and developed a classification framework based on attack logistics, manipulation strategies, and impact metrics.

Result: Authors developed a detailed framework for categorizing attacks and clarified inconsistencies in terminology and metrics within the poisoning literature.

Conclusion: This study offers a structured model and critical dimensions for understanding and addressing LLM poisoning security risks.

Abstract: With the widespread availability of pretrained Large Language Models (LLMs)
and their training datasets, concerns about the security risks associated with
their usage has increased significantly. One of these security risks is the
threat of LLM poisoning attacks where an attacker modifies some part of the LLM
training process to cause the LLM to behave in a malicious way. As an emerging
area of research, the current frameworks and terminology for LLM poisoning
attacks are derived from earlier classification poisoning literature and are
not fully equipped for generative LLM settings. We conduct a systematic review
of published LLM poisoning attacks to clarify the security implications and
address inconsistencies in terminology across the literature. We propose a
comprehensive poisoning threat model applicable to categorize a wide range of
LLM poisoning attacks. The poisoning threat model includes four poisoning
attack specifications that define the logistics and manipulation strategies of
an attack as well as six poisoning metrics used to measure key characteristics
of an attack. Under our proposed framework, we organize our discussion of
published LLM poisoning literature along four critical dimensions of LLM
poisoning attacks: concept poisons, stealthy poisons, persistent poisons, and
poisons for unique tasks, to better understand the current landscape of
security risks.

</details>


### [820] [Mind the Web: The Security of Web Use Agents](https://arxiv.org/abs/2506.07153)
*Avishag Shapira,Parth Atulbhai Gandhi,Edan Habler,Oleg Brodt,Asaf Shabtai*

Main category: cs.CR

TL;DR: This paper identifies critical vulnerabilities in web-use agents and demonstrates how malicious actors can exploit their capabilities to execute attacks such as data theft and system compromise. It also proposes strategies for mitigation.


<details>
  <summary>Details</summary>
Motivation: To explore and address a previously unexamined attack surface created by the powerful capabilities of web-use agents (e.g., multi-tab navigation, DOM interaction) that could be exploited by malicious content.

Method: The researchers systematically assess four popular web-use agents using nine payload types and demonstrate how malicious contents embedded in web pages are interpreted as valid commands due to weaknesses in contextual reasoning by Large Language Models (LLMs).

Result: The study shows a high success rate (80%-100%) in executing attacks such as camera activation, file exfiltration, and password leakage, proving the vulnerabilities across different agents and LLMs.

Conclusion: Web-use agents face a significant security challenge due to fundamental flaws in contextual reasoning by LLMs. Developers need to adopt the proposed mitigations, such as oversight mechanisms and task-aware reasoning, to safeguard agents against exploitation.

Abstract: Web-use agents are rapidly being deployed to automate complex web tasks,
operating with extensive browser capabilities including multi-tab navigation,
DOM manipulation, JavaScript execution and authenticated session access.
However, these powerful capabilities create a critical and previously
unexplored attack surface. This paper demonstrates how attackers can exploit
web-use agents' high-privilege capabilities by embedding malicious content in
web pages such as comments, reviews, or advertisements that agents encounter
during legitimate browsing tasks. In addition, we introduce the task-aligned
injection technique that frame malicious commands as helpful task guidance
rather than obvious attacks. This technique exploiting fundamental limitations
in LLMs' contextual reasoning: agents struggle in maintaining coherent
contextual awareness and fail to detect when seemingly helpful web content
contains steering attempts that deviate from their original task goal. Through
systematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do
Browser, OpenOperator), we demonstrate nine payload types that compromise
confidentiality, integrity, and availability, including unauthorized camera
activation, user impersonation, local file exfiltration, password leakage, and
denial of service, with validation across multiple LLMs achieving success rates
of 80%-100%. These payloads succeed across agents with built-in safety
mechanisms, requiring only the ability to post content on public websites,
creating unprecedented risks given the ease of exploitation combined with
agents' high-privilege access. To address this attack, we propose comprehensive
mitigation strategies including oversight mechanisms, execution constraints,
and task-aware reasoning techniques, providing practical directions for secure
development and deployment.

</details>


### [821] [Scoring the Unscorables: Cyber Risk Assessment Beyond Internet Scans](https://arxiv.org/abs/2506.06604)
*Armin Sarabi,Manish Karir,Mingyan Liu*

Main category: cs.CR

TL;DR: The paper proposes a cyber risk quantification model leveraging technology signatures from websites, offering better coverage for SMEs compared to IP address-based scans.


<details>
  <summary>Details</summary>
Motivation: Previous methods relying on IP address-based scans had limitations such as incomplete data for SMEs, necessitating an improved approach.

Method: The study uses public technology signatures obtained through website crawling to establish correlations with cybersecurity posture.

Result: The model efficiently assesses cyber risks, showing strong relationships between technology signatures and organizations' vulnerability. Key differences in victims of ransomware attacks versus broader data breaches are highlighted.

Conclusion: Technology signature-based cyber risk models improve coverage and precision, especially for SMEs, validating their feasibility and applicability.

Abstract: In this paper we present a study on using novel data types to perform cyber
risk quantification by estimating the likelihood of a data breach. We
demonstrate that it is feasible to build a highly accurate cyber risk
assessment model using public and readily available technology signatures
obtained from crawling an organization's website. This approach overcomes the
limitations of previous similar approaches that relied on large-scale IP
address based scanning data, which suffers from incomplete/missing IP address
mappings as well as the lack of such data for large numbers of small and
medium-sized organizations (SMEs). In comparison to scan data, technology
digital signature data is more readily available for millions of SMEs. Our
study shows that there is a strong relationship between these technology
signatures and an organization's cybersecurity posture. In cross-validating our
model using different cyber incident datasets, we also highlight the key
differences between ransomware attack victims and the larger population of
cyber incident and data breach victims.

</details>


### [822] [From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks](https://arxiv.org/abs/2506.07392)
*Yuyang Zhou,Guang Cheng,Kang Du,Zihan Chen,Tian Qin,Yuyu Zhao*

Main category: cs.CR

TL;DR: The paper introduces a federated multi-agent deep reinforcement learning (FMADRL)-driven moving target defense (MTD) framework for mitigating Denial-of-Service (DoS) attacks in UAV swarm networks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address vulnerabilities of UAV swarm networks, exposed to severe DoS threats due to their dynamic topology, open wireless environment, and resource limitations. Traditional static or centralized defense strategies are inadequate in such scenarios.

Method: The paper proposes a framework with three lightweight MTD mechanisms: leader switching, route mutation, and frequency hopping. These are supported by a policy gradient-based FMADRL algorithm, allowing UAVs to collaboratively optimize defense policies via reward-weighted aggregation without sharing raw data.

Result: Extensive simulations show that the approach improves attack mitigation by 34.6%, reduces average recovery time by up to 94.6%, and decreases energy consumption and defense cost by 29.3% and 98.3%, respectively, while ensuring mission continuity.

Conclusion: The proposed FMADRL-driven MTD framework is effective in addressing DoS attacks against UAV swarms, outperforming existing baselines in terms of both system resilience and resource efficiency.

Abstract: The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide
range of mission-critical applications, but also exposes UAV networks to severe
Denial-of-Service (DoS) threats due to their open wireless environment, dynamic
topology, and resource constraints. Traditional static or centralized defense
mechanisms are often inadequate for such dynamic and distributed scenarios. To
address these challenges, we propose a novel federated multi-agent deep
reinforcement learning (FMADRL)-driven moving target defense (MTD) framework
for proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,
we design three lightweight and coordinated MTD mechanisms, including leader
switching, route mutation, and frequency hopping, that leverage the inherent
flexibility of UAV swarms to disrupt attacker efforts and enhance network
resilience. The defense problem is formulated as a multi-agent partially
observable Markov decision process (POMDP), capturing the distributed,
resource-constrained, and uncertain nature of UAV swarms under attack. Each UAV
is equipped with a local policy agent that autonomously selects MTD actions
based on partial observations and local experiences. By employing a policy
gradient-based FMADRL algorithm, UAVs collaboratively optimize their defense
policies via reward-weighted aggregation, enabling distributed learning without
sharing raw data and thus reducing communication overhead. Extensive
simulations demonstrate that our approach significantly outperforms
state-of-the-art baselines, achieving up to a 34.6% improvement in attack
mitigation rate, a reduction in average recovery time of up to 94.6%, and
decreases in energy consumption and defense cost by as much as 29.3% and 98.3%,
respectively, while maintaining robust mission continuity under various DoS
attack strategies.

</details>


### [823] [Profiling Electric Vehicles via Early Charging Voltage Patterns](https://arxiv.org/abs/2506.07714)
*Francesco Marchiori,Denis Donadel,Alessandro Brighente,Mauro Conti*

Main category: cs.CR

TL;DR: The paper presents a security framework for electric vehicle (EV) charging that uses early voltage behavior for fast and reliable EV identification with accuracy up to 0.86.


<details>
  <summary>Details</summary>
Motivation: Traditional methods detecting unauthorized charging attacks, such as energy theft in EVs through relay attacks, occur too late in the charging process. Early detection is needed along with mitigation of privacy risks from EV profiling.

Method: The authors developed a method to identify EVs by analyzing the early voltage behavior during the initial charging stages. They extracted features from this data and tested their model on a dataset of 7408 charging sessions.

Result: The proposed framework achieved up to 0.86 accuracy in identifying EVs. Key feature importance analysis demonstrated that near-optimal performance is achievable with as few as 10 features.

Conclusion: The research offers a new lightweight and efficient EV authentication approach for secure charging, highlights privacy risks associated with charging data, and proposes a foundation for future work in EV security systems.

Abstract: Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable
alternative to fuel-powered vehicles, making secure charging infrastructure
essential. Despite traditional authentication protocols, recent results showed
that attackers may steal energy through tailored relay attacks. One
countermeasure is leveraging the EV's fingerprint on the current exchanged
during charging. However, existing methods focus on the final charging stage,
allowing malicious actors to consume substantial energy before being detected
and repudiated. This underscores the need for earlier and more effective
authentication methods to prevent unauthorized charging. Meanwhile, profiling
raises privacy concerns, as uniquely identifying EVs through charging patterns
could enable user tracking.
  In this paper, we propose a framework for uniquely identifying EVs using
physical measurements from the early charging stages. We hypothesize that
voltage behavior early in the process exhibits similar characteristics to
current behavior in later stages. By extracting features from early voltage
measurements, we demonstrate the feasibility of EV profiling. Our approach
improves existing methods by enabling faster and more reliable vehicle
identification. We test our solution on a dataset of 7408 usable charges from
49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that
near-optimal performance is possible with just 10 key features, improving
efficiency alongside our lightweight models. This research lays the foundation
for a novel authentication factor while exposing potential privacy risks from
unauthorized access to charging data.

</details>


### [824] [SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark](https://arxiv.org/abs/2506.07888)
*Rui Wen,Yiyong Liu,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: The paper introduces a unified framework for data reconstruction attacks in the vision domain, proposing formal definitions, evaluation metrics, and a taxonomy to benchmark and advance the field.


<details>
  <summary>Details</summary>
Motivation: The lack of clear definitions and metrics for data reconstruction attacks limits progress in understanding and improving these methods.

Method: The authors developed formal definitions, a taxonomy, and quantitative metrics for evaluating reconstruction attacks, leveraging large language models (LLMs) to assess visual results.

Result: Empirical results showcased validated metrics and insights into attack design, particularly from a memorization angle.

Conclusion: The paper establishes a robust framework for evaluating and improving data reconstruction attacks, providing benchmarks for future research.

Abstract: Data reconstruction attacks, which aim to recover the training dataset of a
target model with limited access, have gained increasing attention in recent
years. However, there is currently no consensus on a formal definition of data
reconstruction attacks or appropriate evaluation metrics for measuring their
quality. This lack of rigorous definitions and universal metrics has hindered
further advancement in this field. In this paper, we address this issue in the
vision domain by proposing a unified attack taxonomy and formal definitions of
data reconstruction attacks. We first propose a set of quantitative evaluation
metrics that consider important criteria such as quantifiability, consistency,
precision, and diversity. Additionally, we leverage large language models
(LLMs) as a substitute for human judgment, enabling visual evaluation with an
emphasis on high-quality reconstructions. Using our proposed taxonomy and
metrics, we present a unified framework for systematically evaluating the
strengths and limitations of existing attacks and establishing a benchmark for
future research. Empirical results, primarily from a memorization perspective,
not only validate the effectiveness of our metrics but also offer valuable
insights for designing new attacks.

</details>


### [825] [Are Trees Really Green? A Detection Approach of IoT Malware Attacks](https://arxiv.org/abs/2506.07836)
*Silvia Lucia Sanna,Diego Soi,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: This study presents an energy-efficient methodology for detecting IoT malware networking attacks using flow privacy-preserving statistical features.


<details>
  <summary>Details</summary>
Motivation: IoT devices face increasing cybersecurity threats due to resource constraints and difficulty in securing patches. Efficient attack detection methods are critical given IoT's exponential growth in usage.

Method: The paper optimizes hyperparameters for three tree-based machine learning models—Decision Trees, Random Forest, Extra-Trees—based on energy consumption and test-time performance using Matthew’s Correlation Coefficient.

Result: The models demonstrated high detection accuracy while significantly reducing power consumption (measured in watt-hours).

Conclusion: ML-based Intrusion Detection Systems that are optimized for energy efficiency are feasible and effective for IoT and other resource-constrained environments.

Abstract: Nowadays, the Internet of Things (IoT) is widely employed, and its usage is
growing exponentially because it facilitates remote monitoring, predictive
maintenance, and data-driven decision making, especially in the healthcare and
industrial sectors. However, IoT devices remain vulnerable due to their
resource constraints and difficulty in applying security patches. Consequently,
various cybersecurity attacks are reported daily, such as Denial of Service,
particularly in IoT-driven solutions. Most attack detection methodologies are
based on Machine Learning (ML) techniques, which can detect attack patterns.
However, the focus is more on identification rather than considering the impact
of ML algorithms on computational resources. This paper proposes a green
methodology to identify IoT malware networking attacks based on flow
privacy-preserving statistical features. In particular, the hyperparameters of
three tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are
optimized based on energy consumption and test-time performance in terms of
Matthew's Correlation Coefficient. Our results show that models maintain high
performance and detection accuracy while consistently reducing power usage in
terms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion
Detection Systems are suitable for IoT and other resource-constrained devices.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [826] [Data-Driven High-Dimensional Statistical Inference with Generative Models](https://arxiv.org/abs/2506.06438)
*Oz Amram,Manuel Szewc*

Main category: hep-ph

TL;DR: The paper presents HI-SIGMA, a generative ML-based method to improve statistical inference using data-driven background distributions in high dimensions.


<details>
  <summary>Details</summary>
Motivation: To address poor Monte Carlo modeling of crucial backgrounds at LHC and improve rare process detection amidst large backgrounds.

Method: HI-SIGMA uses generative machine learning models to learn signal and background distributions for unbinned multi-dimensional inference, allowing incorporation of uncertainties.

Result: Application on a di-Higgs measurement in $bb\gamma\gamma$ final state demonstrates superior sensitivity compared to classifier-based methods.

Conclusion: HI-SIGMA offers interpretable and efficient inference with systematic uncertainties and better sensitivity than traditional methods.

Abstract: Crucial to many measurements at the LHC is the use of correlated
multi-dimensional information to distinguish rare processes from large
backgrounds, which is complicated by the poor modeling of many of the crucial
backgrounds in Monte Carlo simulations. In this work, we introduce HI-SIGMA, a
method to perform unbinned high-dimensional statistical inference with
data-driven background distributions. In contradistinction to many applications
of Simulation Based Inference in High Energy Physics, HI-SIGMA relies on
generative ML models, rather than classifiers, to learn the signal and
background distributions in the high-dimensional space. These ML models allow
for efficient, interpretable inference while also incorporating model errors
and other sources of systematic uncertainties. We showcase this methodology on
a simplified version of a di-Higgs measurement in the $bb\gamma\gamma$ final
state, where the di-photon resonance allows for efficient background
interpolation from sidebands into the signal region. We demonstrate that
HI-SIGMA provides improved sensitivity as compared to standard classifier-based
methods, and that systematic uncertainties can be straightforwardly
incorporated by extending methods which have been used for histogram based
analyses.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [827] [Impact of COVID-19 on The Bullwhip Effect Across U.S. Industries](https://arxiv.org/abs/2506.06368)
*Alper Saricioglu,Mujde Erol Genevois,Michele Cedolin*

Main category: econ.GN

TL;DR: The paper examines the amplification of the Bullwhip Effect during COVID-19 across U.S. industries, emphasizing varied industry-level responses due to supply chain structures.


<details>
  <summary>Details</summary>
Motivation: To understand how the COVID-19 pandemic intensified the Bullwhip Effect and explore industry-specific impacts on supply chains.

Method: The study uses extensive industry-level data and applies both traditional and advanced empirical techniques to analyze the manufacturing, retailer, and wholesaler sectors.

Result: COVID-19 significantly amplified the Bullwhip Effect, with varied responses across industries due to differences in supply chain structures.

Conclusion: Supply chain management under global disruptions requires tailored strategies that account for industry-specific characteristics to mitigate phenomena like the Bullwhip Effect.

Abstract: The Bullwhip Effect, describing the amplification of demand variability up
the supply chain, poses significant challenges in Supply Chain Management. This
study examines how the COVID-19 pandemic intensified the Bullwhip Effect across
U.S. industries, using extensive industry-level data. By focusing on the
manufacturing, retailer, and wholesaler sectors, the research explores how
external shocks exacerbate this phenomenon. Employing both traditional and
advanced empirical techniques, the analysis reveals that COVID-19 significantly
amplified the Bullwhip Effect, with industries displaying varied responses to
the same external shock. These differences suggest that supply chain structures
play a critical role in either mitigating or intensifying the effect. By
analyzing the dynamics during the pandemic, this study provides valuable
insights into managing supply chains under global disruptions and highlights
the importance of tailoring strategies to industry-specific characteristics.

</details>


### [828] [Improving choice model specification using reinforcement learning](https://arxiv.org/abs/2506.06410)
*Gabriel Nova,Sander van Cranenburgh,Stephane Hess*

Main category: econ.GN

TL;DR: The paper introduces a deep reinforcement learning framework to enhance discrete choice modeling by dynamically adapting strategies, improving robustness, and allowing transferable applications.


<details>
  <summary>Details</summary>
Motivation: Discrete choice modeling traditionally relies on trial-and-error processes informed by subjective assumptions, which are time-consuming and inefficient. Current metaheuristics fail to dynamically adapt and transfer knowledge across tasks effectively.

Method: The authors propose a deep reinforcement learning framework involving an 'agent' that evaluates model specifications based on their goodness-of-fit and parsimony, adapting strategies dynamically based on rewards.

Result: The framework demonstrated improved adaptability in discovering promising model specifications, robustness across data generation processes, and potential for transferable applications, all without requiring domain-specific knowledge.

Conclusion: Deep reinforcement learning can significantly improve the efficiency and adaptability of discrete choice modeling by overcoming traditional limitations in static search strategies and optimization approaches.

Abstract: Discrete choice modelling is a theory-driven modelling framework for
understanding and forecasting choice behaviour. To obtain behavioural insights,
modellers test several competing model specifications in their attempts to
discover the 'true' data generation process. This trial-and-error process
requires expertise, is time-consuming, and relies on subjective theoretical
assumptions. Although metaheuristics have been proposed to assist choice
modellers, they treat model specification as a classic optimisation problem,
relying on static strategies, applying predefined rules, and neglecting
outcomes from previous estimated models. As a result, current metaheuristics
struggle to prioritise promising search regions, adapt exploration dynamically,
and transfer knowledge to other modelling tasks. To address these limitations,
we introduce a deep reinforcement learning-based framework where an 'agent'
specifies models by estimating them and receiving rewards based on
goodness-of-fit and parsimony. Results demonstrate the agent dynamically adapts
its strategies to identify promising specifications across data generation
processes, showing robustness and potential transferability, without prior
domain knowledge.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [829] [Stability of Mean-Field Variational Inference](https://arxiv.org/abs/2506.07856)
*Shunan Sheng,Bohan Wu,Alberto González-Sanz,Marcel Nutz*

Main category: math.PR

TL;DR: The paper focuses on the stability of mean-field variational inference (MFVI) for strongly log-concave distributions, demonstrating dimension-free Lipschitz continuity and differentiability of the optimizer.


<details>
  <summary>Details</summary>
Motivation: MFVI is commonly used for simplifying the study of complex, high-dimensional probability distributions, but understanding its stability properties is crucial for practical applications.

Method: The authors adopt a linearized optimal transport framework, transforming the non-convex MFVI problem into convex optimization, utilizing calculus of variations and functional analysis.

Result: Dimension-free Lipschitz continuity and differentiability of the MFVI optimizer were established. The derivative was characterized using a partial differential equation.

Conclusion: The findings enable advancements in robust Bayesian inference, empirical Bayes methods, and distributed stochastic control by enhancing the understanding and application of MFVI.

Abstract: Mean-field variational inference (MFVI) is a widely used method for
approximating high-dimensional probability distributions by product measures.
This paper studies the stability properties of the mean-field approximation
when the target distribution varies within the class of strongly log-concave
measures. We establish dimension-free Lipschitz continuity of the MFVI
optimizer with respect to the target distribution, measured in the
2-Wasserstein distance, with Lipschitz constant inversely proportional to the
log-concavity parameter. Under additional regularity conditions, we further
show that the MFVI optimizer depends differentiably on the target potential and
characterize the derivative by a partial differential equation.
Methodologically, we follow a novel approach to MFVI via linearized optimal
transport: the non-convex MFVI problem is lifted to a convex optimization over
transport maps with a fixed base measure, enabling the use of calculus of
variations and functional analysis. We discuss several applications of our
results to robust Bayesian inference and empirical Bayes, including a
quantitative Bernstein--von Mises theorem for MFVI, as well as to distributed
stochastic control.

</details>


### [830] [Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds](https://arxiv.org/abs/2506.07614)
*Rishikesh Srinivasan,Dheeraj Nagaraj*

Main category: math.PR

TL;DR: The paper studies sampling from strongly log-concave distributions using the Poisson midpoint method for Langevin dynamics, achieving faster convergence rates compared to the Euler-Maruyama method.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve sampling efficiency and accuracy for strongly log-concave distributions, which have practical applications in statistics and machine learning.

Method: They propose utilizing the Poisson midpoint discretization method for both overdamped and underdamped Langevin dynamics, analyzing its convergence properties in terms of 2-Wasserstein distance.

Result: The proposed method achieves a cubic speedup in accuracy dependence ($\epsilon$) compared to traditional Euler-Maruyama methods, with particularly efficient $W_2$ convergence for underdamped Langevin dynamics.

Conclusion: The Poisson midpoint method demonstrates superior theoretical performance for sampling, offering significant advancements over existing discretization techniques for Langevin dynamics.

Abstract: We study the problem of sampling from strongly log-concave distributions over
$\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the
randomized midpoint method) for overdamped/underdamped Langevin dynamics. We
prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic
speedup in dependence on the target accuracy ($\epsilon$) over the
Euler-Maruyama discretization, surpassing existing bounds for randomized
midpoint methods. Notably, in the case of underdamped Langevin dynamics, we
demonstrate the complexity of $W_2$ convergence is much smaller than the
complexity lower bounds for convergence in $L^2$ strong error established in
the literature.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [831] [DELPHYNE: A Pre-Trained Model for General and Financial Time Series](https://arxiv.org/abs/2506.06288)
*Xueying Ding,Aakriti Mittal,Achintya Gopal*

Main category: q-fin.ST

TL;DR: The paper introduces Delphyne, a pre-trained model specifically for financial time-series tasks, addressing challenges like lack of domain-specific data and noisy signals.


<details>
  <summary>Details</summary>
Motivation: Existing time-series pre-trained models fail to outperform basic benchmarks in financial tasks due to data domain mismatches and complications like noisy continuous data.

Method: The authors developed Delphyne by focusing on financial time-series data during pre-training, mitigating negative transfer effects and accounting for the modality’s unique challenges.

Result: Delphyne exhibits competitive or superior performance compared to foundation and full-shot models across diverse financial tasks.

Conclusion: Delphyne demonstrates that domain-specific time-series pre-training can improve financial task performance, solving key challenges of general time-series models in finance.

Abstract: Time-series data is a vital modality within data science communities. This is
particularly valuable in financial applications, where it helps in detecting
patterns, understanding market behavior, and making informed decisions based on
historical data. Recent advances in language modeling have led to the rise of
time-series pre-trained models that are trained on vast collections of datasets
and applied to diverse tasks across financial domains. However, across
financial applications, existing time-series pre-trained models have not shown
boosts in performance over simple finance benchmarks in both zero-shot and
fine-tuning settings. This phenomenon occurs because of a i) lack of financial
data within the pre-training stage, and ii) the negative transfer effect due to
inherently different time-series patterns across domains. Furthermore,
time-series data is continuous, noisy, and can be collected at varying
frequencies and with varying lags across different variables, making this data
more challenging to model than languages. To address the above problems, we
introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne
achieves competitive performance to existing foundation and full-shot models
with few fine-tuning steps on publicly available datasets, and also shows
superior performances on various financial tasks.

</details>


### [832] [Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100](https://arxiv.org/abs/2506.06345)
*Sukru Selim Calik,Andac Akyuz,Zeynep Hilal Kilimci,Kerem Colak*

Main category: q-fin.ST

TL;DR: The paper integrates transformer-based time series models with explainable AI (XAI) for improved stock price prediction and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the need for interpretability and accuracy in analyzing complex financial data for stock price predictions.

Method: Used transformer-based time series models (e.g., DLinear, LTSNet) enriched with technical indicators and combined with XAI techniques (SHAP and LIME) for transparency.

Result: Demonstrated strong predictive performance of transformer models and explained the influence of individual technical features.

Conclusion: Transformer models with XAI can support individuals in making informed investment decisions and active financial market participation.

Abstract: Financial literacy is increasingly dependent on the ability to interpret
complex financial data and utilize advanced forecasting tools. In this context,
this study proposes a novel approach that combines transformer-based time
series models with explainable artificial intelligence (XAI) to enhance the
interpretability and accuracy of stock price predictions. The analysis focuses
on the daily stock prices of the five highest-volume banks listed in the
BIST100 index, along with XBANK and XU100 indices, covering the period from
January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla
Transformer, and Time Series Transformer are employed, with input features
enriched by technical indicators. SHAP and LIME techniques are used to provide
transparency into the influence of individual features on model outputs. The
results demonstrate the strong predictive capabilities of transformer models
and highlight the potential of interpretable machine learning to empower
individuals in making informed investment decisions and actively engaging in
financial markets.

</details>


### [833] [The Hype Index: an NLP-driven Measure of Market News Attention](https://arxiv.org/abs/2506.06329)
*Zheng Cao,Wanchaloem Wunkaew,Helyette Geman*

Main category: q-fin.ST

TL;DR: The paper introduces the Hype Index, a metric based on NLP to measure media attention on S&P 100 equities, and evaluates its effectiveness in analyzing market patterns.


<details>
  <summary>Details</summary>
Motivation: To create a metric that quantifies media attention on stocks and sectors and evaluate its usefulness in financial markets by leveraging NLP.

Method: Developed two types of Hype Indexes: News Count-Based Hype Index (based on media exposure) and Capitalization Adjusted Hype Index (adjusted for economic size). Assessed their relationships with returns, volatility, and market dynamics.

Result: The Hype Indexes showed associations with stock returns, volatility, and market indicators like the VIX, also demonstrating predictive power for short-term market trends.

Conclusion: The Hype Index family is a useful tool for analyzing stock volatility, predicting short-term market movements, and exploring NLP applications in finance.

Abstract: This paper introduces the Hype Index as a novel metric to quantify media
attention toward large-cap equities, leveraging advances in Natural Language
Processing (NLP) for extracting predictive signals from financial news. Using
the S&P 100 as the focus universe, we first construct a News Count-Based Hype
Index, which measures relative media exposure by computing the share of news
articles referencing each stock or sector. We then extend it to the
Capitalization Adjusted Hype Index, adjusts for economic size by taking the
ratio of a stock's or sector's media weight to its market capitalization weight
within its industry or sector. We compute both versions of the Hype Index at
the stock and sector levels, and evaluate them through multiple lenses: (1)
their classification into different hype groups, (2) their associations with
returns, volatility, and VIX index at various lags, (3) their signaling power
for short-term market movements, and (4) their empirical properties including
correlations, samplings, and trends. Our findings suggest that the Hype Index
family provides a valuable set of tools for stock volatility analysis, market
signaling, and NLP extensions in Finance.

</details>


### [834] [Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation](https://arxiv.org/abs/2506.07315)
*Zonghan Wu,Junlin Wang,Congyuan Zou,Chenhan Wang,Yilei Shao*

Main category: q-fin.ST

TL;DR: The paper introduces FinAR-Bench, a benchmark for evaluating how well large language models (LLMs) perform financial statement analysis for fundamental financial tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for a robust evaluation framework for LLM performance in generating fundamental analysis reports, a key task for informed investment and financial decisions, especially given the risk of inaccuracies.

Method: FinAR-Bench is developed as a structured benchmark dataset with three measurable steps: extracting key information, calculating financial indicators, and reasoning logically to evaluate LLM capabilities.

Result: The approach provides an objective and granular assessment of LLMs' strengths and weaknesses in financial statement analysis.

Conclusion: This benchmark enables practical and precise evaluation of LLMs in handling real-world financial analysis tasks and underscores their limitations and potential.

Abstract: Generative AI, particularly large language models (LLMs), is beginning to
transform the financial industry by automating tasks and helping to make sense
of complex financial information. One especially promising use case is the
automatic creation of fundamental analysis reports, which are essential for
making informed investment decisions, evaluating credit risks, guiding
corporate mergers, etc. While LLMs attempt to generate these reports from a
single prompt, the risks of inaccuracy are significant. Poor analysis can lead
to misguided investments, regulatory issues, and loss of trust. Existing
financial benchmarks mainly evaluate how well LLMs answer financial questions
but do not reflect performance in real-world tasks like generating financial
analysis reports. In this paper, we propose FinAR-Bench, a solid benchmark
dataset focusing on financial statement analysis, a core competence of
fundamental analysis. To make the evaluation more precise and reliable, we
break this task into three measurable steps: extracting key information,
calculating financial indicators, and applying logical reasoning. This
structured approach allows us to objectively assess how well LLMs perform each
step of the process. Our findings offer a clear understanding of LLMs current
strengths and limitations in fundamental analysis and provide a more practical
way to benchmark their performance in real-world financial settings.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [835] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: The paper introduces a hardware-software co-design that optimizes 3D Gaussian Splatting for efficient, high-quality real-time rendering on resource-constrained devices, achieving significant speedup and energy savings.


<details>
  <summary>Details</summary>
Motivation: Real-time rendering of 3D Gaussian Splatting is challenging on devices with limited power and area resources, despite its growing adoption in domains like AR/VR and robotics.

Method: The proposed solution introduces axis-oriented rasterization to minimize redundant computations, a neural sorting approach to eliminate costly hardware sorters, and a reconfigurable processing array paired with an efficient $
\pi$-trajectory tiling strategy to maximize hardware utilization and memory efficiency.

Result: The system achieves rendering speedup of $23.4\sim27.8\times$ and energy savings of $28.8\sim51.4\times$ over edge GPUs while maintaining rendering quality.

Conclusion: The proposed design significantly enhances the performance of 3D Gaussian Splatting, making it more practical for resource-constrained devices. The authors aim to open-source their design to encourage further research.

Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

</details>


### [836] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: The paper introduces auxiliary consistency losses to enhance Stable Diffusion fine-tuning for subject-driven image synthesis, addressing underfitting and overfitting issues.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for Stable Diffusion face challenges like underfitting, which compromises subject identity capture, and overfitting, which reduces background diversity.

Method: The authors propose two new consistency losses: prior consistency regularization for maintaining fidelity and subject consistency regularization for preserving subject identity and encouraging diversity.

Result: These methods improve subject identity preservation and image diversity, outperforming DreamBooth based on CLIP scores and visual quality.

Conclusion: The proposed consistency losses refine fine-tuning processes, offering significant improvements in fidelity, diversity, and visual quality for subject-driven image synthesis.

Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by
adapting the model to generate images containing specific subjects. However,
existing fine-tuning methods suffer from two key issues: underfitting, where
the model fails to reliably capture subject identity, and overfitting, where it
memorizes the subject image and reduces background diversity. To address these
challenges, we propose two auxiliary consistency losses for diffusion
fine-tuning. First, a prior consistency regularization loss ensures that the
predicted diffusion noise for prior (non-subject) images remains consistent
with that of the pretrained model, improving fidelity. Second, a subject
consistency regularization loss enhances the fine-tuned model's robustness to
multiplicative noise modulated latent code, helping to preserve subject
identity while improving diversity. Our experimental results demonstrate that
incorporating these losses into fine-tuning not only preserves subject identity
but also enhances image diversity, outperforming DreamBooth in terms of CLIP
scores, background variation, and overall visual quality.

</details>


### [837] [Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation](https://arxiv.org/abs/2506.06440)
*Chuhao Chen,Zhiyang Dou,Chen Wang,Yiming Huang,Anjun Chen,Qiao Feng,Jiatao Gu,Lingjie Liu*

Main category: cs.GR

TL;DR: Vid2Sim is a framework for efficiently reconstructing objects’ shapes and physical properties from videos, combining a neural network with simplified optimization for high practicality and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for reconstructing texture, shapes, and physical properties require heavy optimization processes and tuning, making them impractical and less generalizable.

Method: Vid2Sim uses a neural network to reconstruct the system from videos, followed by lightweight refinement optimization, leveraging Linear Blend Skinning (LBS) for computational efficiency and versatility.

Result: Vid2Sim was shown to achieve better accuracy and efficiency compared to existing methods in experiments reconstructing shape and physical properties from videos.

Conclusion: The framework demonstrates that a generalizable and efficient system for video-based reconstruction is possible, enabling accurate simulation in reduced time.

Abstract: Faithfully reconstructing textured shapes and physical properties from videos
presents an intriguing yet challenging problem. Significant efforts have been
dedicated to advancing such a system identification problem in this area.
Previous methods often rely on heavy optimization pipelines with a
differentiable simulator and renderer to estimate physical parameters. However,
these approaches frequently necessitate extensive hyperparameter tuning for
each scene and involve a costly optimization process, which limits both their
practicality and generalizability. In this work, we propose a novel framework,
Vid2Sim, a generalizable video-based approach for recovering geometry and
physical properties through a mesh-free reduced simulation based on Linear
Blend Skinning (LBS), offering high computational efficiency and versatile
representation capability. Specifically, Vid2Sim first reconstructs the
observed configuration of the physical system from video using a feed-forward
neural network trained to capture physical world knowledge. A lightweight
optimization pipeline then refines the estimated appearance, geometry, and
physical properties to closely align with video observations within just a few
minutes. Additionally, after the reconstruction, Vid2Sim enables high-quality,
mesh-free simulation with high efficiency. Extensive experiments demonstrate
that our method achieves superior accuracy and efficiency in reconstructing
geometry and physical properties from video data.

</details>


### [838] [Splat and Replace: 3D Reconstruction with Repetitive Elements](https://arxiv.org/abs/2506.06462)
*Nicolás Violante,Andreas Meuleman,Alban Gauthier,Frédo Durand,Thibault Groueix,George Drettakis*

Main category: cs.GR

TL;DR: This paper improves 3D scene view synthesis by leveraging repetitive elements in the environment, enhancing unseen or occluded parts.


<details>
  <summary>Details</summary>
Motivation: Conventional methods like NeRF and 3DGS struggle to render unseen or occluded parts of 3D scenes when training views are insufficiently exhaustive.

Method: The proposed method segments repeated instances in a 3DGS reconstruction, aligns them, and allows shared information between instances to improve scene representation.

Result: The approach enhances both geometry and appearance consistency, significantly improving novel view synthesis quality in synthetic and real-world scenarios.

Conclusion: Leveraging repetitions in 3D scenes effectively mitigates quality issues caused by poor coverage and occlusions, offering a robust enhancement to existing view synthesis methods.

Abstract: We leverage repetitive elements in 3D scenes to improve novel view synthesis.
Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly
improved novel view synthesis but renderings of unseen and occluded parts
remain low-quality if the training views are not exhaustive enough. Our key
observation is that our environment is often full of repetitive elements. We
propose to leverage those repetitions to improve the reconstruction of
low-quality parts of the scene due to poor coverage and occlusions. We propose
a method that segments each repeated instance in a 3DGS reconstruction,
registers them together, and allows information to be shared among instances.
Our method improves the geometry while also accounting for appearance
variations across instances. We demonstrate our method on a variety of
synthetic and real scenes with typical repetitive elements, leading to a
substantial improvement in the quality of novel view synthesis.

</details>


### [839] [HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance](https://arxiv.org/abs/2506.07209)
*Lei Li,Angela Dai*

Main category: cs.GR

TL;DR: HOI-PAGE introduces a novel zero-shot text-to-4D human-object interaction synthesis method focusing on part-level affordances and provides more realistic and context-aware results.


<details>
  <summary>Details</summary>
Motivation: Previous methods in 4D human-object interaction synthesis lacked fine-grained part-level understanding and mainly focused on global body-object motion, limiting their realism and diversity.

Method: The paper proposes Part Affordance Graphs (PAGs), a structured representation derived from language models, to identify part-level contact relations. Using PAGs, the process involves three stages: decomposing 3D object parts, generating reference HOI videos from prompts, and optimizing motion sequences to adhere to part-level constraints.

Result: Experiments demonstrate the method's capability to flexibly generate realistic and complex multi-object/person interaction sequences with better text alignment and improved realism compared to previous approaches.

Conclusion: The introduced method showcases advancements in zero-shot 4D HOI generation by focusing on part-level affordances and effectively bridging text prompts to complex, context-aware motion synthesis.

Abstract: We present HOI-PAGE, a new approach to synthesizing 4D human-object
interactions (HOIs) from text prompts in a zero-shot fashion, driven by
part-level affordance reasoning. In contrast to prior works that focus on
global, whole body-object motion for 4D HOI synthesis, we observe that
generating realistic and diverse HOIs requires a finer-grained understanding --
at the level of how human body parts engage with object parts. We thus
introduce Part Affordance Graphs (PAGs), a structured HOI representation
distilled from large language models (LLMs) that encodes fine-grained part
information along with contact relations. We then use these PAGs to guide a
three-stage synthesis: first, decomposing input 3D objects into geometric
parts; then, generating reference HOI videos from text prompts, from which we
extract part-based motion constraints; finally, optimizing for 4D HOI motion
sequences that not only mimic the reference dynamics but also satisfy
part-level contact constraints. Extensive experiments show that our approach is
flexible and capable of generating complex multi-object or multi-person
interaction sequences, with significantly improved realism and text alignment
for zero-shot 4D HOI generation.

</details>


### [840] [PIG: Physically-based Multi-Material Interaction with 3D Gaussians](https://arxiv.org/abs/2506.07657)
*Zeyu Xiao,Zhenyi Wu,Mingyang Sun,Qipeng Yan,Yufan Guo,Zhuoer Liang,Lihua Zhang*

Main category: cs.GR

TL;DR: The paper introduces PIG, a method to improve 3D Gaussian Splatting for accurate object segmentation and interaction with physical properties.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting struggles with accurate 3D segmentation, deformation across different materials, and rendering artifacts during object interactions.

Method: The proposed approach (PIG) enhances segmentation by mapping 2D pixels to 3D Gaussians, assigns physical properties to segmented objects, and enforces constraints on deformation gradients to avoid artifacts.

Result: The method achieves better visual quality than existing approaches and enables physically realistic scene generation.

Conclusion: PIG improves 3D Gaussian-based reconstruction methods by addressing limitations in segmentation, interaction precision, and rendering quality, providing a foundation for more realistic scene simulations.

Abstract: 3D Gaussian Splatting has achieved remarkable success in reconstructing both
static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian
primitives, interactions between objects suffer from inaccurate 3D
segmentation, imprecise deformation among different materials, and severe
rendering artifacts. To address these challenges, we introduce PIG:
Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach
that combines 3D object segmentation with the simulation of interacting objects
in high precision. Firstly, our method facilitates fast and accurate mapping
from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.
Secondly, we assign unique physical properties to correspondingly segmented
objects within the scene for multi-material coupled interactions. Finally, we
have successfully embedded constraint scales into deformation gradients,
specifically clamping the scaling and rotation properties of the Gaussian
primitives to eliminate artifacts and achieve geometric fidelity and visual
consistency. Experimental results demonstrate that our method not only
outperforms the state-of-the-art (SOTA) in terms of visual quality, but also
opens up new directions and pipelines for the field of physically realistic
scene generation.

</details>


### [841] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: The paper proposes a method to improve the resolution and geometrical accuracy of 3D Gaussian Splatting by addressing its limitations in generating finer details using a lightweight generative model.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian Splatting methods are restricted by input resolution, limiting their ability to produce finer details in reconstructions.

Method: The paper introduces a Hessian-assisted sampling strategy to identify and refine regions for densification, enabling efficient resolution enhancement without computationally intensive techniques.

Result: Results show improved geometric accuracy and rendering quality over state-of-the-art methods, achieved in real-time on consumer-grade hardware.

Conclusion: The work sets a new standard for resolution-free 3D scene enhancement with practical applications in interactive settings.

Abstract: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.

</details>


### [842] [Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes](https://arxiv.org/abs/2506.07917)
*Allen Tu,Haiyang Ying,Alex Hanson,Yonghan Lee,Tom Goldstein,Matthias Zwicker*

Main category: cs.GR

TL;DR: SpeeDe3DGS introduces techniques to accelerate rendering speed in dynamic 3D Gaussian Splatting methods, achieving notable improvements in speed, memory usage, and training efficiency through pruning and motion analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in dynamic scene rendering caused by per-Gaussian neural inference, which slows down rendering speed and increases computational and memory demands.

Method: SpeeDe3DGS employs a temporal sensitivity pruning score to remove less impactful Gaussians and an annealing smooth pruning mechanism for robustness. It also introduces GroupFlow, enabling trajectory-based clustering and shared rigid transformation prediction for groups of Gaussians.

Result: SpeeDe3DGS accelerates rendering by up to $58.23\times$, diminishes model size by $7.71\times$, and decreases training duration by $2.71\times$, validated across datasets like NeRF-DS, D-NeRF, and HyperNeRF vrig.

Conclusion: SpeeDe3DGS effectively boosts rendering performance, reduces computational burdens, and is modular, making it compatible with various deformable Gaussian frameworks.

Abstract: Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve
high-quality novel view synthesis by using neural networks to predict the
time-varying deformation of each Gaussian. However, performing per-Gaussian
neural inference at every frame poses a significant bottleneck, limiting
rendering speed and increasing memory and compute requirements. In this paper,
we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general
pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS
representations by reducing neural inference through two complementary
techniques. First, we propose a temporal sensitivity pruning score that
identifies and removes Gaussians with low contribution to the dynamic scene
reconstruction. We also introduce an annealing smooth pruning mechanism that
improves pruning robustness in real-world scenes with imprecise camera poses.
Second, we propose GroupFlow, a motion analysis technique that clusters
Gaussians by trajectory similarity and predicts a single rigid transformation
per group instead of separate deformations for each Gaussian. Together, our
techniques accelerate rendering by $10.37\times$, reduce model size by
$7.71\times$, and shorten training time by $2.71\times$ on the NeRF-DS dataset.
SpeeDe3DGS also improves rendering speed by $4.20\times$ and $58.23\times$ on
the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be
integrated into any deformable 3DGS or 4DGS framework.

</details>


### [843] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: Squeeze3D is a framework for extremely high compression of 3D data using pre-trained models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of compressing 3D data effectively and at high ratios while maintaining visual quality.

Method: Map latent spaces between pre-trained encoder and generative model via trainable networks. Compress 3D data into latent codes for reconstruction via synthetic data training.

Result: Achieved compression ratios of up to 2187x for meshes, 55x for point clouds, and 619x for radiance fields with visually comparable quality to other methods.

Conclusion: Squeeze3D demonstrates high-efficiency compression and decompression across various 3D formats without requiring object-specific network training.

Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior
knowledge learnt by existing pre-trained 3D generative models to compress 3D
data at extremely high compression ratios. Our approach bridges the latent
spaces between a pre-trained encoder and a pre-trained generation model through
trainable mapping networks. Any 3D model represented as a mesh, point cloud, or
a radiance field is first encoded by the pre-trained encoder and then
transformed (i.e. compressed) into a highly compact latent code. This latent
code can effectively be used as an extremely compressed representation of the
mesh or point cloud. A mapping network transforms the compressed latent code
into the latent space of a powerful generative model, which is then conditioned
to recreate the original 3D model (i.e. decompression). Squeeze3D is trained
entirely on generated synthetic data and does not require any 3D datasets. The
Squeeze3D architecture can be flexibly used with existing pre-trained 3D
encoders and existing generative models. It can flexibly support different
formats, including meshes, point clouds, and radiance fields. Our experiments
demonstrate that Squeeze3D achieves compression ratios of up to 2187x for
textured meshes, 55x for point clouds, and 619x for radiance fields while
maintaining visual quality comparable to many existing methods. Squeeze3D only
incurs a small compression and decompression latency since it does not involve
training object-specific networks to compress an object.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [844] [HyColor: An Efficient Heuristic Algorithm for Graph Coloring](https://arxiv.org/abs/2506.07373)
*Enqiang Zhu,Yu Zhang,Haopeng Sun,Ziqi Wei,Witold Pedrycz,Chanjuan Liu,Jin Xu*

Main category: cs.DM

TL;DR: This paper introduces HyColor, an efficient hybrid heuristic algorithm that outperforms state-of-the-art algorithms in solving the graph coloring problem for both sparse large graphs and dense small graphs.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms for the graph coloring problem have limitations, focusing solely on either small hard graphs or sparse large-scale graphs. There is a need for a more versatile approach that performs well across different graph types.

Method: HyColor uses three strategies: (1) a local decision strategy to boost chromatic number lower bounds, (2) a graph-reduction approach to simplify the problem, and (3) a k-core and mixed degree-based greedy heuristic for efficient graph coloring.

Result: HyColor consistently outperformed existing algorithms in solutions and efficiency across 209 benchmark instances; it produced the best solutions in 93% of cases, with optimal coloring achieved in 128 instances.

Conclusion: HyColor is a robust and versatile algorithm for solving the graph coloring problem, excelling in both accuracy and efficiency for sparse and dense graphs alike.

Abstract: The graph coloring problem (GCP) is a classic combinatorial optimization
problem that aims to find the minimum number of colors assigned to vertices of
a graph such that no two adjacent vertices receive the same color. GCP has been
extensively studied by researchers from various fields, including mathematics,
computer science, and biological science. Due to the NP-hard nature, many
heuristic algorithms have been proposed to solve GCP. However, existing GCP
algorithms focus on either small hard graphs or large-scale sparse graphs (with
up to 10^7 vertices). This paper presents an efficient hybrid heuristic
algorithm for GCP, named HyColor, which excels in handling large-scale sparse
graphs while achieving impressive results on small dense graphs. The efficiency
of HyColor comes from the following three aspects: a local decision strategy to
improve the lower bound on the chromatic number; a graph-reduction strategy to
reduce the working graph; and a k-core and mixed degree-based greedy heuristic
for efficiently coloring graphs. HyColor is evaluated against three
state-of-the-art GCP algorithms across four benchmarks, comprising three
large-scale sparse graph benchmarks and one small dense graph benchmark,
totaling 209 instances. The results demonstrate that HyColor consistently
outperforms existing heuristic algorithms in both solution accuracy and
computational efficiency for the majority of instances. Notably, HyColor
achieved the best solutions in 194 instances (over 93%), with 34 of these
solutions significantly surpassing those of other algorithms. Furthermore,
HyColor successfully determined the chromatic number and achieved optimal
coloring in 128 instances.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [845] [Fast Geometric Embedding for Node Influence Maximization](https://arxiv.org/abs/2506.07435)
*Alexander Kolpakov,Igor Rivin*

Main category: cs.SI

TL;DR: The paper presents a computationally efficient algorithm to estimate centrality measures in large-scale graphs by embedding them into a low-dimensional space.


<details>
  <summary>Details</summary>
Motivation: Centrality measures like betweenness and closeness are essential for graph analysis but are computationally expensive for large graphs.

Method: A force layout algorithm is proposed which embeds a graph into a low-dimensional space, using radial distance from the origin as a proxy for centrality measures.

Result: The algorithm shows strong correlations with degree, PageRank, and path-based centralities, and enables the identification of high-influence nodes efficiently.

Conclusion: The method offers a scalable and fast alternative to traditional greedy algorithms for identifying influential nodes in networks.

Abstract: Computing classical centrality measures such as betweenness and closeness is
computationally expensive on large-scale graphs. In this work, we introduce an
efficient force layout algorithm that embeds a graph into a low-dimensional
space, where the radial distance from the origin serves as a proxy for various
centrality measures. We evaluate our method on multiple graph families and
demonstrate strong correlations with degree, PageRank, and paths-based
centralities. As an application, it turns out that the proposed embedding
allows to find high-influence nodes in a network, and provides a fast and
scalable alternative to the standard greedy algorithm.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [846] [Deep regularization networks for inverse problems with noisy operators](https://arxiv.org/abs/2506.07008)
*Fatemeh Pourahmadian,Yang Xu*

Main category: math.NA

TL;DR: The paper proposes a supervised learning method that accelerates real-time superresolution imaging using a neural operator for parameter regularization.


<details>
  <summary>Details</summary>
Motivation: To address challenges in regularizing large inverse problems with noisy data and enable real-time superresolution imaging for practical use cases.

Method: A two-step neural operator training process combining Morozov discrepancy principle for initial mapping and optimization via the Tikhonov loss function.

Result: The approach generates dense regularization maps for high-resolution imaging faster, improves image quality, and enhances contrast in complex imaging scenarios.

Conclusion: This method accelerates imaging, improves generalizability of neural networks, and provides a framework for effective regularization without requiring prior knowledge of optimal parameters.

Abstract: A supervised learning approach is proposed for regularization of large
inverse problems where the main operator is built from noisy data. This is
germane to superresolution imaging via the sampling indicators of the inverse
scattering theory. We aim to accelerate the spatiotemporal regularization
process for this class of inverse problems to enable real-time imaging. In this
approach, a neural operator maps each pattern on the right-hand side of the
scattering equation to its affiliated regularization parameter. The network is
trained in two steps which entails: (1) training on low-resolution
regularization maps furnished by the Morozov discrepancy principle with
nonoptimal thresholds, and (2) optimizing network predictions through
minimization of the Tikhonov loss function regulated by the validation loss.
Step 2 allows for tailoring of the approximate maps of Step 1 toward
construction of higher quality images. This approach enables direct learning
from test data and dispenses with the need for a-priori knowledge of the
optimal regularization maps. The network, trained on low-resolution data,
quickly generates dense regularization maps for high-resolution imaging. We
highlight the importance of the training loss function on the network's
generalizability. In particular, we demonstrate that networks informed by the
logic of discrepancy principle lead to images of higher contrast. In this case,
the training process involves many-objective optimization. We propose a new
method to adaptively select the appropriate loss weights during training
without requiring an additional optimization process. The proposed approach is
synthetically examined for imaging damage evolution in an elastic plate. The
results indicate that the discrepancy-informed regularization networks not only
accelerate the imaging process, but also remarkably enhance the image quality
in complex environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [847] [KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes](https://arxiv.org/abs/2506.06541)
*Eugenie Lai,Gerardo Vitagliano,Ziyu Zhang,Sivaprasad Sudhir,Om Chabra,Anna Zeng,Anton A. Zabreyko,Chenning Li,Ferdi Kossmann,Jialin Ding,Jun Chen,Markos Markakis,Matthew Russo,Weiyang Wang,Ziniu Wu,Michael J. Cafarella,Lei Cao,Samuel Madden,Tim Kraska*

Main category: cs.DB

TL;DR: The study introduces KRAMABENCH, a benchmark for evaluating AI systems' abilities to construct real-world data science pipelines involving tasks like data discovery, cleaning, and orchestration. Findings reveal that existing models are capable of basic tasks but struggle with complex, domain-specific pipelines.


<details>
  <summary>Details</summary>
Motivation: To assess whether current AI models can successfully design and execute complex real-world data science pipelines, which involve intricate data processing and domain knowledge.

Method: The authors created KRAMABENCH, consisting of 104 curated pipelines across diverse domains, and evaluated multiple AI models using DS-GURU, a framework for decomposing tasks, reasoning, and Python code synthesis.

Result: The evaluation demonstrated that current AI models perform well on narrowly defined tasks but fail in handling more complex data processing steps requiring domain expertise.

Conclusion: Progress in benchmarks like KRAMABENCH is essential to advance AI systems capable of building autonomous, real-world data science pipelines.

Abstract: Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [848] [FPGA-Based Material Testing Machine Controller](https://arxiv.org/abs/2506.07139)
*Arev Hambardzumyan,Rafayel Ghasabyan,Vahagn Tamazyan*

Main category: eess.SY

TL;DR: The paper advocates for FPGA-based controllers in materials testing, addressing modern demands for speed, scalability, and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional controller-based systems struggle to meet the growing demands for speed, adaptability, and parallelism in materials testing.

Method: The paper suggests leveraging FPGA-based controllers for reconfiguration, parallel control, and multichannel operations in testing systems.

Result: FPGA-based systems are shown to overcome adaptability and speed limitations while facilitating multichannel operations.

Conclusion: FPGA-based controllers are a forward-looking, high-performance solution to improve materials testing systems in terms of scalability, flexibility, and efficiency.

Abstract: In the realm of contemporary materials testing, the demand for scalability,
adaptability, parallelism, and speed has surged due to the proliferation of
diverse materials and testing standards. Traditional controller-based systems
often fall short in meeting these requirements, resulting in adaptability and
processing speed limitations. Conversely, FPGA-based controllers present a
multifaceted, high-performance solution. Key advantages of FPGA-based
controllers in materials testing encompass reconfiguration capabilities for
cost-effective adaptation to evolving materials and standards. FPGAs also
enable the integration of parallel control and data acquisition circuits, vital
for multichannel test equipment demanding simultaneous, independent operation
of multiple control channels.

</details>


### [849] [Towards Data-Driven Model-Free Safety-Critical Control](https://arxiv.org/abs/2506.06931)
*Zhe Shen,Yitaek Kim,Christoffer Sloth*

Main category: eess.SY

TL;DR: This paper proposes a framework for safe velocity control in robotics using data-driven model-free Control Barrier Functions (CBFs), minimizing manual tuning by estimating parameters with a neural network and enhancing robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of using model-free CBFs due to the dependency on a manually tuned exponential decay rate, which is often unknown in real-world applications.

Method: The framework uses a neural network to learn the Lyapunov function and estimate the decay rate of the system's velocity controller. It incorporates Chernoff bound to derive a probabilistic safety condition, enhancing robustness.

Result: The proposed framework was experimentally tested on a UR5e robot and effectively ensured safe velocity control with model-free CBFs.

Conclusion: The framework provides a robust solution for safe velocity control using model-free CBFs by addressing uncertainties in stability violations and eliminating the need for manual tuning of decay rate parameters.

Abstract: This paper presents a framework for enabling safe velocity control of general
robotic systems using data-driven model-free Control Barrier Functions (CBFs).
Model-free CBFs rely on an exponentially stable velocity controller and a
design parameter (e.g. alpha in CBFs); this design parameter depends on the
exponential decay rate of the controller. However, in practice, the decay rate
is often unavailable, making it non-trivial to use model-free CBFs, as it
requires manual tuning for alpha. To address this, a Neural Network is used to
learn the Lyapunov function from data, and the maximum decay rate of the
systems built-in velocity controller is subsequently estimated. Furthermore, to
integrate the estimated decay rate with model-free CBFs, we derive a
probabilistic safety condition that incorporates a confidence bound on the
violation rate of the exponential stability condition, using Chernoff bound.
This enhances robustness against uncertainties in stability violations. The
proposed framework has been tested on a UR5e robot in multiple experimental
settings, and its effectiveness in ensuring safe velocity control with
model-free CBFs has been demonstrated.

</details>


### [850] [Deep Equivariant Multi-Agent Control Barrier Functions](https://arxiv.org/abs/2506.07755)
*Nikolaos Bousias,Lars Lindemann,George Pappas*

Main category: eess.SY

TL;DR: The paper introduces symmetries-infused distributed Control Barrier Functions (CBFs) to enhance safety, scalability, and efficiency in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based methods for safety assurance in multi-agent systems lack scalability and efficiency as they ignore important geometric structures.

Method: The authors propose using equivariant parametrization and group-modular networks compatible with group actions to construct distributed CBFs.

Result: The simulations on multi-robot navigation tasks show improved safety, scalability, and success rates compared to state-of-the-art baselines.

Conclusion: Embedding symmetries in distributed neural policies is crucial for achieving safe and efficient multi-agent systems.

Abstract: With multi-agent systems increasingly deployed autonomously at scale in
complex environments, ensuring safety of the data-driven policies is critical.
Control Barrier Functions have emerged as an effective tool for enforcing
safety constraints, yet existing learning-based methods often lack in
scalability, generalization and sampling efficiency as they overlook inherent
geometric structures of the system. To address this gap, we introduce
symmetries-infused distributed Control Barrier Functions, enforcing the
satisfaction of intrinsic symmetries on learnable graph-based safety
certificates. We theoretically motivate the need for equivariant
parametrization of CBFs and policies, and propose a simple, yet efficient and
adaptable methodology for constructing such equivariant group-modular networks
via the compatible group actions. This approach encodes safety constraints in a
distributed data-efficient manner, enabling zero-shot generalization to larger
and denser swarms. Through extensive simulations on multi-robot navigation
tasks, we demonstrate that our method outperforms state-of-the-art baselines in
terms of safety, scalability, and task success rates, highlighting the
importance of embedding symmetries in safe distributed neural policies.

</details>


### [851] [The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage](https://arxiv.org/abs/2506.06484)
*Manuel Sage,Khalil Al Handawi,Yaoyao Fiona Zhao*

Main category: eess.SY

TL;DR: This paper investigates using Deep Reinforcement Learning (DRL) to optimize the economic operation of Power-to-Gas (P2G) systems over the long term, alongside battery energy storage systems (BESs) and gas turbines.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to address the challenges in cost-effectively operating P2G systems, given their lower efficiency compared to BESs, the volatility of renewable energy, electricity prices, and the neglected long-term storage potential in previous short-term studies.

Method: The authors evaluate the use of DRL algorithms, including Deep Q-Networks and Proximal Policy Optimization, in three progressively complex case studies. They introduce modifications such as forecast integration, penalty adjustments in reward functions, and strategic cost calculations to address delayed reward issues.

Result: The findings reveal that DRL initially struggles with the complexity of P2G system operations but significantly improves in performance with the proposed modifications, leading to better long-term operational strategies.

Conclusion: This research demonstrates the potential of DRL, especially with tailored modifications, in unlocking the long-term storage and economic benefits of P2G systems.

Abstract: Power-to-Gas (P2G) technologies gain recognition for enabling the integration
of intermittent renewables, such as wind and solar, into electricity grids.
However, determining the most cost-effective operation of these systems is
complex due to the volatile nature of renewable energy, electricity prices, and
loads. Additionally, P2G systems are less efficient in converting and storing
energy compared to battery energy storage systems (BESs), and the benefits of
converting electricity into gas are not immediately apparent. Deep
Reinforcement Learning (DRL) has shown promise in managing the operation of
energy systems amidst these uncertainties. Yet, DRL techniques face
difficulties with the delayed reward characteristic of P2G system operation.
Previous research has mostly focused on short-term studies that look at the
energy conversion process, neglecting the long-term storage capabilities of
P2G.
  This study presents a new method by thoroughly examining how DRL can be
applied to the economic operation of P2G systems, in combination with BESs and
gas turbines, over extended periods. Through three progressively more complex
case studies, we assess the performance of DRL algorithms, specifically Deep
Q-Networks and Proximal Policy Optimization, and introduce modifications to
enhance their effectiveness. These modifications include integrating forecasts,
implementing penalties on the reward function, and applying strategic cost
calculations, all aimed at addressing the issue of delayed rewards. Our
findings indicate that while DRL initially struggles with the complex
decision-making required for P2G system operation, the adjustments we propose
significantly improve its capability to devise cost-effective operation
strategies, thereby unlocking the potential for long-term energy storage in P2G
technologies.

</details>


### [852] [From Model-Based and Adaptive Control to Evolving Fuzzy Control](https://arxiv.org/abs/2506.06594)
*Daniel Leite,Igor Škrjanc,Fernando Gomide*

Main category: eess.SY

TL;DR: The paper discusses the evolution of fuzzy systems for modeling and control, emphasizing their ability to handle data streams and nonstationary environments. It reflects on the history and future directions of the field.


<details>
  <summary>Details</summary>
Motivation: The paper aims to commemorate 60 years of fuzzy set theory, reviewing advancements in classical and evolving fuzzy systems for data-driven modeling and control.

Method: The paper revisits historical developments and examines the role of evolving fuzzy systems, along with their advantages and challenges.

Result: It underscores the significance of these systems in addressing dynamic and nonstationary environments while identifying challenges like safety and interpretability.

Conclusion: The paper argues for continued research in improving evolving fuzzy systems, focusing on safety, interpretability, and structural evolution.

Abstract: Evolving fuzzy systems build and adapt fuzzy models - such as predictors and
controllers - by incrementally updating their rule-base structure from data
streams. On the occasion of the 60-year anniversary of fuzzy set theory,
commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the
historical development and core contributions of classical fuzzy and adaptive
modeling and control frameworks. It then highlights the emergence and
significance of evolving intelligent systems in fuzzy modeling and control,
emphasizing their advantages in handling nonstationary environments. Key
challenges and future directions are discussed, including safety,
interpretability, and principled structural evolution.

</details>


### [853] [On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)](https://arxiv.org/abs/2506.07079)
*Mostafa Eslami,Maryam Babazadeh*

Main category: eess.SY

TL;DR: This paper proposes a hybrid control framework for port-Hamiltonian systems that integrates Data-Assisted Control with reinforcement learning to manage uncertainties and optimize performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in handling both structural and parametric uncertainties in port-Hamiltonian systems while retaining their inherent structure and ensuring safety under dynamic conditions.

Method: The framework employs a decomposition of system dynamics into Hamiltonian and dissipative flows. A nonlinear controller modulates the Hamiltonian flow, while reinforcement learning is used to optimize the dissipative flow using a virtual interface variable.

Result: The framework demonstrated adjustable control performance, enhanced AI interpretability, guaranteed safety, and efficient learning. It was validated through simulation of a pendulum with nonlinear dynamics.

Conclusion: This hybrid framework offers a promising approach for robust and interpretable control of port-Hamiltonian systems, with potential for further theoretical and practical advancements in complex system control.

Abstract: This paper introduces a hypothetical hybrid control framework for
port-Hamiltonian (p$\mathcal{H}$) systems, employing a dynamic decomposition
based on Data-Assisted Control (DAC). The system's evolution is split into two
parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow
handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a
dissipative/input flow addressing both structural and parametric uncertainties.
A virtual port variable $\Pi$ serves as the interface between these two
components. A nonlinear controller manages the intrinsic Hamiltonian flow,
determining a desired port control value $\Pi_c$. Concurrently, Reinforcement
Learning (RL) is applied to the dissipative/input flow to learn an agent for
providing optimal policy in mapping $\Pi_c$ to the actual system input. This
hybrid approach effectively manages RHS uncertainties while preserving the
system's inherent structure. Key advantages include adjustable performance via
LHS controller parameters, enhanced AI explainability and interpretability
through the port variable $\Pi$, the ability to guarantee safety and state
attainability with hard/soft constraints, reduced complexity in learning
hypothesis classes compared to end-to-end solutions, and improved
state/parameter estimation using LHS prior knowledge and system Hamiltonian to
address partial observability. The paper details the p$\mathcal{H}$
formulation, derives the decomposition, and presents the modular controller
architecture. Beyond design, crucial aspects of stability and robustness
analysis and synthesis are investigated, paving the way for deeper theoretical
investigations. An application example, a pendulum with nonlinear dynamics, is
simulated to demonstrate the approach's empirical and phenomenological benefits
for future research.

</details>


### [854] [Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems](https://arxiv.org/abs/2506.07347)
*Armin Lederer,Erfaun Noorani,Andreas Krause*

Main category: eess.SY

TL;DR: The paper introduces a risk-sensitive safety filter for uncertain multi-agent systems that ensures safety while allowing distributed implementation.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of ensuring safety in distributed multi-agent systems with uncertain dynamics.

Method: The methodology employs control barrier functions coupled with risk-sensitive safety conditions using exponential risk operators to formulate both centralized and distributed safety filters.

Result: The proposed approach successfully ensures safety in distributed multi-agent systems while avoiding excessive conservatism, validated through numerical experiments.

Conclusion: This work provides a robust and adaptable safety framework for multi-agent systems, addressing feasibility through agent strategy switching and distributed safety filter designs.

Abstract: Ensuring safety in multi-agent systems is a significant challenge,
particularly in settings where centralized coordination is impractical. In this
work, we propose a novel risk-sensitive safety filter for discrete-time
multi-agent systems with uncertain dynamics that leverages control barrier
functions (CBFs) defined through value functions. Our approach relies on
centralized risk-sensitive safety conditions based on exponential risk
operators to ensure robustness against model uncertainties. We introduce a
distributed formulation of the safety filter by deriving two alternative
strategies: one based on worst-case anticipation and another on proximity to a
known safe policy. By allowing agents to switch between strategies, feasibility
can be ensured. Through detailed numerical evaluations, we demonstrate the
efficacy of our approach in maintaining safety without being overly
conservative.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [855] [Hadamard-$Π$: Equational Quantum Programming](https://arxiv.org/abs/2506.06835)
*Wang Fang,Chris Heunen,Robin Kaarsgaard*

Main category: quant-ph

TL;DR: The paper introduces a quantum programming language that extends a universal classical reversible language by adding a Hadamard gate, providing a characterization of its computational behaviors.


<details>
  <summary>Details</summary>
Motivation: To clarify the unique computational behaviors enabled when a Hadamard gate is added to a set of classical reversible gates in quantum computing.

Method: The authors develop a small quantum programming language with a sound and complete categorical semantics based on an equational theory, and they show completeness through a new finite presentation and synthesis algorithm for certain groups of orthogonal matrices.

Result: The language enables automated reasoning about quantum program equivalence, supported by a novel approach to representing orthogonal matrices with specific entries.

Conclusion: The study provides a rigorous and automatable framework for understanding the computational impact of the Hadamard gate in quantum programming.

Abstract: Quantum computing offers advantages over classical computation, yet the
precise features that set the two apart remain unclear. In the standard quantum
circuit model, adding a 1-qubit basis-changing gate -- commonly chosen to be
the Hadamard gate -- to a universal set of classical reversible gates yields
computationally universal quantum computation. However, the computational
behaviours enabled by this addition are not fully characterised. We give such a
characterisation by introducing a small quantum programming language extending
the universal classical reversible programming language $\Pi$ with a single
primitive corresponding to the Hadamard gate. The language comes equipped with
a sound and complete categorical semantics that is specified by a purely
equational theory, enabling reasoning about the equivalence of quantum programs
in a way that can be automated. Completeness is shown by means of a novel
finite presentation, and corresponding synthesis algorithm, for the groups of
orthogonal matrices with entries in the ring $\mathbb{Z}[\tfrac{1}{\sqrt{2}}]$.

</details>


### [856] [Optimal quantum sampling on distributed databases](https://arxiv.org/abs/2506.07724)
*Longyun Chen,Jingcheng Liu,Penghui Yao*

Main category: quant-ph

TL;DR: This paper introduces distributed quantum sampling methods under restricted communication scenarios, including both sequential and parallel algorithms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of implementing quantum sampling in distributed systems due to limited quantum storage and practical constraints on communication between machines.

Method: The authors propose and analyze sequential and parallel algorithms where a coordinator queries distributed systems with basic oracles in an oblivious communication model.

Result: The paper shows that both the sequential and parallel algorithms achieve theoretical optimality in their respective communication settings.

Conclusion: Distributed quantum sampling is feasible and can be performed optimally under constrained communication settings, with algorithms tailored to distributed environments.

Abstract: Quantum sampling, a fundamental subroutine in numerous quantum algorithms,
involves encoding a given probability distribution in the amplitudes of a pure
state. Given the hefty cost of large-scale quantum storage, we initiate the
study of quantum sampling in a distributed setting. Specifically, we assume
that the data is distributed among multiple machines, and each machine solely
maintains a basic oracle that counts the multiplicity of individual elements.
Given a quantum sampling task, which is to sample from the joint database, a
coordinator can make oracle queries to all machines. We focus on the oblivious
communication model, where communications between the coordinator and the
machines are predetermined. We present both sequential and parallel algorithms:
the sequential algorithm queries the machines sequentially, while the parallel
algorithm allows the coordinator to query all machines simultaneously.
Furthermore, we prove that both algorithms are optimal in their respective
settings.

</details>


### [857] [Adam assisted Fully informed Particle Swarm Optimzation ( Adam-FIPSO ) based Parameter Prediction for the Quantum Approximate Optimization Algorithm (QAOA)](https://arxiv.org/abs/2506.06790)
*Shashank Sanjay Bhat,Peiyong Wang,Udaya Parampalli*

Main category: quant-ph

TL;DR: The paper proposes combining Fully Informed Particle Swarm Optimization with adaptive gradient correction to optimize QAOA parameters and avoid common issues, yielding superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: The need to efficiently identify suitable QAOA parameters for solving combinatorial optimization problems and overcoming challenges like barren plateaus and local minima.

Method: A framework combining Fully Informed Particle Swarm Optimization (FIPSO) with adaptive gradient correction using the Adam Optimizer to navigate the QAOA parameter space.

Result: Experimental results across different graph types and QAOA depths show the proposed framework consistently outperforms random initialization.

Conclusion: The proposed optimization framework is effective and robust in navigating QAOA parameters, providing high-quality solutions to combinatorial optimization problems.

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a prominent
variational algorithm used for solving combinatorial optimization problems such
as the Max-Cut problem. A key challenge in QAOA lies in efficiently identifying
suitable parameters (gamma, beta) that lead to high-quality solutions. In this
paper, we propose a framework that combines Fully Informed Particle Swarm
Optimization (FIPSO) with adaptive gradient correction using the Adam Optimizer
to navigate the QAOA parameter space. This approach aims to avoid issues such
as barren plateaus and convergence to local minima. The proposed algorithm is
evaluated against two classes of graph instances, Erdos Renyi and
Watts-Strogatz. Experimental results across multiple QAOA depths consistently
demonstrate superior performance compared to random initialization,
underscoring the effectiveness and robustness of the proposed optimization
framework.

</details>


### [858] [Depth-Optimal Quantum Layout Synthesis as SAT](https://arxiv.org/abs/2506.06752)
*Anna B. Jakobsen,Anders B. Clausen,Jaco van de Pol,Irfansha Shaik*

Main category: quant-ph

TL;DR: The paper introduces a new SAT encoding method for optimizing quantum-circuit layout synthesis by minimizing circuit or CX-gate depth, achieving speedups of up to 100x compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: Quantum hardware imposes connectivity restrictions on CX gates, and due to their noisiness, reducing CX count or depth for mapped circuits can improve performance, necessitating effective layout synthesis methods.

Method: The authors propose a new SAT encoding for quantum circuit layout synthesis that focuses on minimizing circuit depth or CX-gate depth using incremental SAT solving and parallel plans for efficiency.

Result: The SAT encoding achieves substantial speedups (10-100x) in minimizing depth compared to traditional methods like OLSQ2, while it correlates noise reduction better by optimizing both CX-count and CX-depth.

Conclusion: Minimizing both CX-count and CX-depth offers the best noise reduction for quantum circuits, making the improved SAT encoding highly effective for layout synthesis tasks.

Abstract: Quantum circuits consist of gates applied to qubits. Current quantum hardware
platforms impose connectivity restrictions on binary CX gates. Hence, Layout
Synthesis is an important step to transpile quantum circuits before they can be
executed. Since CX gates are noisy, it is important to reduce the CX count or
CX depth of the mapped circuits.
  We provide a new and efficient encoding of Quantum-circuit Layout Synthesis
in SAT. Previous SAT encodings focused on gate count and CX-gate count. Our
encoding instead guarantees that we find mapped circuits with minimal circuit
depth or minimal CX-gate depth. We use incremental SAT solving and parallel
plans for an efficient encoding. This results in speedups of more than 10-100x
compared to OLSQ2, which guarantees depth-optimality. But minimizing depth
still takes more time than minimizing gate count with Q-Synth.
  We correlate the noise reduction achieved by simulating circuits after
(CX)-count and (CX)-depth reduction. We find that minimizing for CX-count
correlates better with reducing noise than minimizing for CX-depth. However,
taking into account both CX-count and CX-depth provides the best noise
reduction.

</details>


### [859] [A weighted quantum ensemble of homogeneous quantum classifiers](https://arxiv.org/abs/2506.07810)
*Emiliano Tolotti,Enrico Blanzieri,Davide Pastorello*

Main category: quant-ph

TL;DR: The paper proposes a quantum-based ensemble method using quantum classifiers that leverages instance-based sampling and a weight learning process, achieving quantum-parallel execution and showing good performance in evaluations.


<details>
  <summary>Details</summary>
Motivation: To improve prediction accuracy using ensemble methods by combining diversity and weighted influence in quantum classifiers, addressing the need for better model combination techniques in quantum machine learning.

Method: The authors propose a weighted homogeneous quantum ensemble using quantum classifiers, which enables diverse classifier execution via superposition and controlled unitaries, integrating a classical weight optimization process for improved predictive performance.

Result: The method demonstrates its effectiveness empirically, showcasing improved performance through empirical evaluation and providing insights into its practical application.

Conclusion: The proposed method successfully combines quantum machine learning and ensemble techniques, offering improved prediction accuracy and contributing to the advancement of quantum machine learning ensemble methods.

Abstract: Ensemble methods in machine learning aim to improve prediction accuracy by
combining multiple models. This is achieved by ensuring diversity among
predictors to capture different data aspects. Homogeneous ensembles use
identical models, achieving diversity through different data subsets, and
weighted-average ensembles assign higher influence to more accurate models
through a weight learning procedure. We propose a method to achieve a weighted
homogeneous quantum ensemble using quantum classifiers with indexing registers
for data encoding. This approach leverages instance-based quantum classifiers,
enabling feature and training point subsampling through superposition and
controlled unitaries, and allowing for a quantum-parallel execution of diverse
internal classifiers with different data compositions in superposition. The
method integrates a learning process involving circuit execution and classical
weight optimization, for a trained ensemble execution with weights encoded in
the circuit at test-time. Empirical evaluation demonstrate the effectiveness of
the proposed method, offering insights into its performance.

</details>


### [860] [Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing](https://arxiv.org/abs/2506.07859)
*Amanuel Anteneh Léandre Brunel,Carlos González-Arciniegas,Olivier Pfister*

Main category: quant-ph

TL;DR: Deep neural networks, trained via reinforcement learning, achieve a 96% success rate in controlling a quantum optical circuit to generate cubic-phase states, enabling continuous-variable universal quantum computing.


<details>
  <summary>Details</summary>
Motivation: To explore efficient and practical methods of generating cubic-phase states, which are essential for universal quantum computing in continuous-variable systems.

Method: Deep neural networks were trained using reinforcement learning to control a quantum optical circuit. The resources employed included photon-number-resolving measurements as the sole non-Gaussian element.

Result: A successful generation of cubic-phase states with an average success rate of 96%. Additionally, the same resources enabled the direct generation of a quartic-phase gate without needing decomposition.

Conclusion: The method demonstrates that reinforcement learning-driven control of quantum circuits can effectively generate essential quantum states and gates, advancing the feasibility of continuous-variable quantum computing.

Abstract: Cubic-phase states are a sufficient resource for universal quantum computing
over continuous variables. We present results from numerical experiments in
which deep neural networks are trained via reinforcement learning to control a
quantum optical circuit for generating cubic-phase states, with an average
success rate of 96%. The only non-Gaussian resource required is
photon-number-resolving measurements. We also show that the exact same
resources enable the direct generation of a quartic-phase gate, with no need
for a cubic gate decomposition.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [861] [\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality](https://arxiv.org/abs/2506.06622)
*Yifan Zeng*

Main category: cs.CE

TL;DR: The paper introduces QuantMCP, a framework to address limitations of large language models (LLMs) in handling financial data by securely integrating real-time APIs.


<details>
  <summary>Details</summary>
Motivation: To overcome issues like data hallucination and lack of real-time verifiable information in LLMs, improving their usability in financial decision-making.

Method: Uses the Model Context Protocol (MCP) to securely connect LLMs with Python-accessible financial data APIs, enabling accurate and real-time data retrieval.

Result: QuantMCP enhances LLMs' analytical capabilities by providing verified, structured financial data for interpreting and generating insights.

Conclusion: QuantMCP acts as a secure and extensible bridge, significantly improving the reliability and depth of LLM applications in financial analysis.

Abstract: Large Language Models (LLMs) hold immense promise for revolutionizing
financial analysis and decision-making, yet their direct application is often
hampered by issues of data hallucination and lack of access to real-time,
verifiable financial information. This paper introduces QuantMCP, a novel
framework designed to rigorously ground LLMs in financial reality. By
leveraging the Model Context Protocol (MCP) for standardized and secure tool
invocation, QuantMCP enables LLMs to accurately interface with a diverse array
of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can
interact via natural language to precisely retrieve up-to-date financial data,
thereby overcoming LLM's inherent limitations in factual data recall. More
critically, once furnished with this verified, structured data, the LLM's
analytical capabilities are unlocked, empowering it to perform sophisticated
data interpretation, generate insights, and ultimately support more informed
financial decision-making processes. QuantMCP provides a robust, extensible,
and secure bridge between conversational AI and the complex world of financial
data, aiming to enhance both the reliability and the analytical depth of LLM
applications in finance.

</details>


### [862] [Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market](https://arxiv.org/abs/2506.06356)
*Yimin Du*

Main category: cs.CE

TL;DR: The paper introduces a quantitative trading algorithm for the Chinese A-share market, showing strong performance metrics like a 15.2% annualized return and a Sharpe ratio of 1.87.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve capital efficiency and risk management in multi-day turnover trading for the Chinese A-share market.

Method: The algorithm integrates deep learning, stock prediction, arbitrage identification, dynamic sizing, and optimized entry/exit timing based on market volatility.

Result: The method achieves 15.2% annualized returns, a drawdown below 5%, and a Sharpe ratio of 1.87 through rigorous backtesting on 2021-2024 data.

Conclusion: The trading strategy is scalable, efficient in capital turnover, robust across market regimes, and ideal for institutional use in the Chinese A-share market.

Abstract: This paper presents a sophisticated multi-day turnover quantitative trading
algorithm that integrates advanced deep learning techniques with comprehensive
cross-sectional stock prediction for the Chinese A-share market. Our framework
combines five interconnected modules: initial stock selection through deep
cross-sectional prediction networks, opening signal distribution analysis using
mixture models for arbitrage identification, market capitalization and
liquidity-based dynamic position sizing, grid-search optimized profit-taking
and stop-loss mechanisms, and multi-granularity volatility-based market timing
models. The algorithm employs a novel approach to balance capital efficiency
with risk management through adaptive holding periods and sophisticated
entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and
rigorously backtested on 2021-2024 data, our method achieves remarkable
performance with 15.2\% annualized returns, maximum drawdown constrained below
5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional
scalability by maintaining 50-100 daily positions with a 9-day maximum holding
period, incorporating dynamic profit-taking and stop-loss mechanisms that
enhance capital turnover efficiency while preserving risk-adjusted returns. Our
approach exhibits robust performance across various market regimes while
maintaining high capital capacity suitable for institutional deployment.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [863] [Scientific machine learning in Hydrology: a unified perspective](https://arxiv.org/abs/2506.06308)
*Adoubi Vincent De Paul Adombi*

Main category: physics.comp-ph

TL;DR: This paper develops a unified methodological framework for scientific machine learning (SciML) in hydrology to address fragmentation in existing approaches and promote systematic research.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the fragmentation in SciML methods for hydrology by creating conceptual clarity to enable meaningful advances in the field.

Method: The authors propose unified methodological frameworks for different SciML families, categorizing and organizing existing approaches into coherent structures.

Result: A structured and unified framework for SciML families is established, providing conceptual clarity and identifying future opportunities for hydrological modeling.

Conclusion: The review promotes systematic progress in SciML for hydrology by highlighting limitations, consolidating fragmented approaches, and suggesting future directions for underused methods in the field.

Abstract: Scientific machine learning (SciML) provides a structured approach to
integrating physical knowledge into data-driven modeling, offering significant
potential for advancing hydrological research. In recent years, multiple
methodological families have emerged, including physics-informed machine
learning, physics-guided machine learning, hybrid physics-machine learning, and
data-driven physics discovery. Within each of these families, a proliferation
of heterogeneous approaches has developed independently, often without
conceptual coordination. This fragmentation complicates the assessment of
methodological novelty and makes it difficult to identify where meaningful
advances can still be made in the absence of a unified conceptual framework.
This review, the first focused overview of SciML in hydrology, addresses these
limitations by proposing a unified methodological framework for each SciML
family, bringing together representative contributions into a coherent
structure that fosters conceptual clarity and supports cumulative progress in
hydrological modeling. Finally, we highlight the limitations and future
opportunities of each unified family to guide systematic research in hydrology,
where these methods remain underutilized.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [864] [Pendulum Tracker -- SimuFísica: A Web-based Tool for Real-time Measurement of Oscillatory Motion](https://arxiv.org/abs/2506.07301)
*Marco P. M. de Souza,Juciane G. Maia,Lilian N. de Andrade*

Main category: physics.ed-ph

TL;DR: Pendulum Tracker is a browser-based tool using computer vision to measure pendulum motion in real time for educational purposes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create an accessible, intuitive tool for teaching experimental physics via real-time pendulum measurements.

Method: The system uses the OpenCV.js library to detect pendulum's position through a device's camera and generates angle-time graphs and oscillation period estimates.

Result: Experimental case studies demonstrated high accuracy in measuring periods, determining gravitational acceleration, and analyzing damped oscillations.

Conclusion: Pendulum Tracker is effective, accurate, and accessible, making it a valuable resource for teaching experimental physics.

Abstract: We present Pendulum Tracker, a computer vision-based application that enables
real-time measurement of the oscillatory motion of a physical pendulum.
Integrated into the educational platform SimuF\'isica, the system uses the
OpenCV.js library and runs directly in the browser, working on computers,
tablets, and smartphones. The application automatically detects the pendulum's
position via the device's camera, displaying in real time the angle-versus-time
graph and estimates of the oscillation period. Experimental case studies
demonstrate its effectiveness in measuring the period, determining
gravitational acceleration, and analyzing damped oscillations. The results show
excellent agreement with theoretical predictions, confirming the system's
accuracy and its applicability in educational contexts. The accessible
interface and the ability to export raw data make Pendulum Tracker a versatile
tool for experimental physics teaching.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [865] [Inverse Design of Metamaterials with Manufacturing-Guiding Spectrum-to-Structure Conditional Diffusion Model](https://arxiv.org/abs/2506.07083)
*Jiawen Li,Jiang Guo,Yuanzhe Li,Zetian Mao,Jiaxing Shen,Tashi Xu,Diptesh Das,Jinming He,Run Hu,Yaerim Lee,Koji Tsuda,Junichiro Shiomi*

Main category: physics.optics

TL;DR: The paper addresses challenges in designing complex metamaterials using machine learning, proposing a framework based on conditional diffusion models for inverse design and practical manufacturing.


<details>
  <summary>Details</summary>
Motivation: Existing challenges include the highly nonlinear relationship between metamaterials' structures and their optical behaviors, alongside fabrication difficulties during manufacturing.

Method: The paper introduces a framework based on conditional diffusion models tailored for spectrum-to-shape and size parameters to solve one-to-many inverse design problems.

Result: The proposed framework achieves high spectral prediction accuracy, generates diverse designs, and provides insights for experimental fabrication.

Conclusion: The method's efficacy is validated with the successful design and fabrication of a free-form metamaterial that enhances thermal camouflage applications.

Abstract: Metamaterials are artificially engineered structures that manipulate
electromagnetic waves, having optical properties absent in natural materials.
Recently, machine learning for the inverse design of metamaterials has drawn
attention. However, the highly nonlinear relationship between the metamaterial
structures and optical behaviour, coupled with fabrication difficulties, poses
challenges for using machine learning to design and manufacture complex
metamaterials. Herein, we propose a general framework that implements
customised spectrum-to-shape and size parameters to address one-to-many
metamaterial inverse design problems using conditional diffusion models. Our
method exhibits superior spectral prediction accuracy, generates a diverse
range of patterns compared to other typical generative models, and offers
valuable prior knowledge for manufacturing through the subsequent analysis of
the diverse generated results, thereby facilitating the experimental
fabrication of metamaterial designs. We demonstrate the efficacy of the
proposed method by successfully designing and fabricating a free-form
metamaterial with a tailored selective emission spectrum for thermal camouflage
applications.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [866] [Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking](https://arxiv.org/abs/2506.07351)
*Jun Chen,Lina Liu,Tianyi Zhu,Yong Liu,Guang Dai,Yunliang Jiang,Ivor W. Tsang*

Main category: math.OC

TL;DR: This paper introduces the Quantized Riemannian Gradient Tracking (Q-RGT) algorithm to improve decentralized optimization on compact submanifolds, minimizing communication bottlenecks via quantized gradients with an efficient convergence rate.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of communication bottlenecks in decentralized optimization, particularly when agents operate on compact submanifolds and use accurate computational operations such as Riemannian projection.

Method: The Q-RGT algorithm utilizes quantized gradients in decentralized optimization to bypass precise Riemannian projection constraints while preserving efficiency. It achieves an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization.

Result: The Q-RGT algorithm mitigates communication inefficiencies, matches the convergence rate of non-quantized methods, and derives lower bounds for decentralized consensus accounting for quantization levels. Numerical experiments confirm its comparable performance to non-quantized methods.

Conclusion: Q-RGT is an effective decentralized optimization approach that reduces communication limitations and computational overhead by harnessing quantization noise, maintaining both accuracy and efficiency comparable to non-quantized methods.

Abstract: This paper considers the problem of decentralized optimization on compact
submanifolds, where a finite sum of smooth (possibly non-convex) local
functions is minimized by $n$ agents forming an undirected and connected graph.
However, the efficiency of distributed optimization is often hindered by
communication bottlenecks. To mitigate this, we propose the Quantized
Riemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local
variables using quantized gradients. The introduction of quantization noise
allows our algorithm to bypass the constraints of the accurate Riemannian
projection operator (such as retraction), further improving iterative
efficiency. To the best of our knowledge, this is the first algorithm to
achieve an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization,
matching the convergence rate of methods without quantization. Additionally, we
explicitly derive lower bounds on decentralized consensus associated with a
function of quantization levels. Numerical experiments demonstrate that Q-RGT
performs comparably to non-quantized methods while reducing communication
bottlenecks and computational overhead.

</details>


### [867] [Discrete and Continuous Difference of Submodular Minimization](https://arxiv.org/abs/2506.07952)
*George Orfanides,Tim Hoheisel,Marwa El Halabi*

Main category: math.OC

TL;DR: The paper addresses the minimization of the difference of two submodular functions over discrete and continuous domains, proposing an algorithm adaptable to both.


<details>
  <summary>Details</summary>
Motivation: Submodular function minimization has wide applications, and prior research mainly focused on set functions, leaving a gap in extending to discrete and continuous domains.

Method: The paper proposes a novel variant of the Difference of Convex Algorithm (DCA) and applies it to DC programs. For continuous domains, discretization is used to adapt the algorithm.

Result: Experimental results show the algorithm performs better compared to baselines in applications like integer compressive sensing and integer least squares.

Conclusion: The proposed algorithm generalizes DS minimization to broader domains, showing both theoretical guarantees and practical superiority in specific applications.

Abstract: Submodular functions, defined on continuous or discrete domains, arise in
numerous applications. We study the minimization of the difference of two
submodular (DS) functions, over both domains, extending prior work restricted
to set functions. We show that all functions on discrete domains and all smooth
functions on continuous domains are DS. For discrete domains, we observe that
DS minimization is equivalent to minimizing the difference of two convex (DC)
functions, as in the set function case. We propose a novel variant of the DC
Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable
theoretical guarantees as in the set function case. The algorithm can be
applied to continuous domains via discretization. Experiments demonstrate that
our method outperforms baselines in integer compressive sensing and integer
least squares.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [868] [From Axioms to Algorithms: Mechanized Proofs of the vNM Utility Theorem](https://arxiv.org/abs/2506.07066)
*Li Jingyuan*

Main category: econ.TH

TL;DR: This paper formalizes the von Neumann-Morgenstern expected utility theorem in Lean 4, providing machine-verified proofs of utility representation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a rigorous and precise formalization of the vNM expected utility theorem, enabling reliable computational applications in economics, AI, and decision-making.

Method: The paper implements the vNM axioms (completeness, transitivity, continuity, independence) in Lean 4, offering granularity and formally verified proofs of utility existence and equivalence to classical models.

Result: The formalization proves the existence and uniqueness of utility functions via computational experiments, achieving equivalence to classical theory and precision at decision boundaries.

Conclusion: This work bridges theoretical decision theory with computational tools, advancing applications in economic modeling, AI, and decision systems.

Abstract: This paper presents a comprehensive formalization of the von
Neumann-Morgenstern (vNM) expected utility theorem using the Lean 4 interactive
theorem prover. We implement the classical axioms of preference-completeness,
transitivity, continuity, and independence-enabling machine-verified proofs of
both the existence and uniqueness of utility representations. Our formalization
captures the mathematical structure of preference relations over lotteries,
verifying that preferences satisfying the vNM axioms can be represented by
expected utility maximization.
  Our contributions include a granular implementation of the independence
axiom, formally verified proofs of fundamental claims about mixture lotteries,
constructive demonstrations of utility existence, and computational experiments
validating the results. We prove equivalence to classical presentations while
offering greater precision at decision boundaries.
  This formalization provides a rigorous foundation for applications in
economic modeling, AI alignment, and management decision systems, bridging the
gap between theoretical decision theory and computational implementation.

</details>
