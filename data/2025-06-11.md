<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 34]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 81]
- [cs.CV](#cs.CV) [Total: 98]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 125]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 24]
- [cs.SE](#cs.SE) [Total: 17]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 6]
- [nlin.CG](#nlin.CG) [Total: 1]
- [math.OC](#math.OC) [Total: 5]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.DB](#cs.DB) [Total: 3]
- [math.ST](#math.ST) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.IV](#eess.IV) [Total: 7]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [math.NA](#math.NA) [Total: 4]
- [cs.GR](#cs.GR) [Total: 5]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load](https://arxiv.org/abs/2506.08026)
*Xibai Wang*

Main category: cs.AI

TL;DR: The paper introduces TIP-Search, a scheduling framework for making real-time financial predictions using deep learning while meeting strict deadlines.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the latency demands in high-frequency trading systems to ensure high predictive accuracy without missing task deadlines.

Method: The method involves offline profiling of latency and accuracy of a heterogeneous model pool, followed by an online, input-agnostic task-aware model selection for each task.

Result: TIP-Search was evaluated on datasets (FI-2010, Binance BTC/USDT, LOBSTER AAPL) and showed up to 8.5% improved accuracy with 100% deadline adherence.

Conclusion: TIP-Search is highly effective for ensuring robust, low-latency financial inferences in conditions with uncertain workloads.

Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling
framework for real-time market prediction under uncertain workloads. Motivated
by the strict latency demands in high-frequency financial systems, TIP-Search
dynamically selects a deep learning model from a heterogeneous pool, aiming to
maximize predictive accuracy while satisfying per-task deadline constraints.
Our approach profiles latency and generalization performance offline, then
performs online task-aware selection without relying on explicit input domain
labels. We evaluate TIP-Search on three real-world limit order book datasets
(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms
static baselines with up to 8.5% improvement in accuracy and 100% deadline
satisfaction. Our results highlight the effectiveness of TIP-Search in robust
low-latency financial inference under uncertainty.

</details>


### [2] [Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph](https://arxiv.org/abs/2506.08098)
*Akash Vishwakarma,Hojin Lee,Mohith Suresh,Priyam Shankar Sharma,Rahul Vishwakarma,Sparsh Gupta,Yuvraj Anupam Chauhan*

Main category: cs.AI

TL;DR: The paper introduces Cognitive Weave, a novel memory framework for large language model (LLM) agents, addressing limitations in memory architectures with a sophisticated graph-based approach delivering superior performance and insights.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in current memory systems for LLMs, such as rigidity, lack of temporal awareness, and insufficient insight synthesis, and to enable these systems to support continuous learning, nuanced reasoning, and dynamic adaptation.

Method: The authors propose Cognitive Weave, which leverages a multi-layered spatio-temporal resonance graph (STRG) to organize semantically rich insight particles (IPs) interconnected as relational strands. It uses a semantic oracle interface for enrichment and features an autonomous cognitive refinement process to derive higher-level insight aggregates (IAs).

Result: Cognitive Weave shows significant performance improvements: 34% increase in task completion rates, 42% faster query latency, and better results in long-term planning, evolving QA scenarios, and multi-session dialogues compared to state-of-the-art baselines.

Conclusion: Cognitive Weave represents a groundbreaking advancement in memory systems for LLMs, enhancing their capabilities in complex tasks. It also raises important ethical and research considerations, paving the way for further innovations in the field.

Abstract: The emergence of capable large language model (LLM) based agents necessitates
memory architectures that transcend mere data storage, enabling continuous
learning, nuanced reasoning, and dynamic adaptation. Current memory systems
often grapple with fundamental limitations in structural flexibility, temporal
awareness, and the ability to synthesize higher-level insights from raw
interaction data. This paper introduces Cognitive Weave, a novel memory
framework centered around a multi-layered spatio-temporal resonance graph
(STRG). This graph manages information as semantically rich insight particles
(IPs), which are dynamically enriched with resonance keys, signifiers, and
situational imprints via a dedicated semantic oracle interface (SOI). These IPs
are interconnected through typed relational strands, forming an evolving
knowledge tapestry. A key component of Cognitive Weave is the cognitive
refinement process, an autonomous mechanism that includes the synthesis of
insight aggregates (IAs) condensed, higher-level knowledge structures derived
from identified clusters of related IPs. We present comprehensive experimental
results demonstrating Cognitive Weave's marked enhancement over existing
approaches in long-horizon planning tasks, evolving question-answering
scenarios, and multi-session dialogue coherence. The system achieves a notable
34% average improvement in task completion rates and a 42% reduction in mean
query latency when compared to state-of-the-art baselines. Furthermore, this
paper explores the ethical considerations inherent in such advanced memory
systems, discusses the implications for long-term memory in LLMs, and outlines
promising future research trajectories.

</details>


### [3] [SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents](https://arxiv.org/abs/2506.08119)
*Subhrangshu Nandi,Arghya Datta,Nikhil Vichare,Indranil Bhattacharya,Huzefa Raja,Jing Xu,Shayan Ray,Giuseppe Carenini,Abhi Srivastava,Aaron Chan,Man Ho Woo,Amar Kandola,Brandon Theresa,Francesco Carbone*

Main category: cs.AI

TL;DR: The paper introduces SOP-Bench, a benchmark designed to test the ability of Large Language Models (LLMs) to handle complex, long-horizon workflows following Standard Operating Procedures (SOPs). It highlights the limitations of LLMs in real-world industrial automation tasks.


<details>
  <summary>Details</summary>
Motivation: To address the gap in LLMs' capability to execute structured workflows based on SOPs, which is critical for real-world industrial automation, and the lack of public benchmarks representing SOP complexity.

Method: The authors developed a synthetic data generation framework to create industry-grade SOPs and constructed SOP-Bench with over 1,800 tasks across 10 domains, complete with APIs and validated test cases. They evaluated LLM agents using this benchmark.

Result: Function-Calling and ReAct agent architectures showed average success rates of 27% and 48%, respectively. Agents struggled significantly with tool selection, especially when the tool registry was large, invoking incorrect tools almost 100% of the time.

Conclusion: Current LLM agentic capabilities fall short of the requirements for automating SOPs, indicating a need for domain-specific evaluation and improvements. SOP-Bench serves as a foundation for further research and community contributions.

Abstract: Large Language Models (LLMs) demonstrate impressive general-purpose reasoning
and problem-solving abilities. However, they struggle with executing complex,
long-horizon workflows that demand strict adherence to Standard Operating
Procedures (SOPs), a critical requirement for real-world industrial automation.
Despite this need, there is a lack of public benchmarks that reflect the
complexity, structure, and domain-specific nuances of SOPs. To address this, we
present three main contributions. First, we introduce a synthetic data
generation framework to create realistic, industry-grade SOPs that rigorously
test the planning, reasoning, and tool-use capabilities of LLM-based agents.
Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800
tasks across 10 industrial domains, each with APIs, tool interfaces, and
human-validated test cases. Third, we evaluate two prominent agent
architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing
average success rates of only 27% and 48%, respectively. Remarkably, when the
tool registry is much larger than necessary, agents invoke incorrect tools
nearly 100% of the time. These findings underscore a substantial gap between
current agentic capabilities of LLMs and the demands of automating real-world
SOPs. Performance varies significantly by task and domain, highlighting the
need for domain-specific benchmarking and architectural choices before
deployment. SOP-Bench is publicly available at
http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the
prompts underpinning the data generation framework to support new
domain-specific SOP benchmarks. We invite the community to extend SOP-Bench
with SOPs from their industrial domains.

</details>


### [4] [The AI Imperative: Scaling High-Quality Peer Review in Machine Learning](https://arxiv.org/abs/2506.08134)
*Qiyao Wei,Samuel Holt,Jing Yang,Markus Wulfmeier,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: Peer review in machine learning faces growing challenges due to rapid manuscript submission increases and limited reviewer capacity. The paper advocates for AI-assisted systems to address issues of quality, consistency, and reviewer workload, suggesting specific roles for AI tools.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper stems from the exponential growth in submissions to top machine learning conferences, which has overwhelmed the peer review process, causing concerns about review quality, reviewer fatigue, and scalability.

Method: The authors propose an AI-augmented ecosystem where Large Language Models (LLMs) assist authors, reviewers, and area chairs in tasks such as factual verification, reviewer guidance, and decision-making. They also highlight the need for structured and ethically-sourced peer review data.

Result: The research agenda includes experiments to design and validate AI assistants while addressing technical and ethical challenges. It envisions a structured approach to incorporate AI systems in the peer review process of ML.

Conclusion: The authors emphasize the urgency of integrating AI systems into peer review to ensure scalability, maintain scientific standards, and address current limitations. They urge the ML community to prioritize this development to safeguard the integrity of scientific validation.

Abstract: Peer review, the bedrock of scientific advancement in machine learning (ML),
is strained by a crisis of scale. Exponential growth in manuscript submissions
to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite
capacity of qualified reviewers, leading to concerns about review quality,
consistency, and reviewer fatigue. This position paper argues that AI-assisted
peer review must become an urgent research and infrastructure priority. We
advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language
Models (LLMs) not as replacements for human judgment, but as sophisticated
collaborators for authors, reviewers, and Area Chairs (ACs). We propose
specific roles for AI in enhancing factual verification, guiding reviewer
performance, assisting authors in quality improvement, and supporting ACs in
decision-making. Crucially, we contend that the development of such systems
hinges on access to more granular, structured, and ethically-sourced peer
review process data. We outline a research agenda, including illustrative
experiments, to develop and validate these AI assistants, and discuss
significant technical and ethical challenges. We call upon the ML community to
proactively build this AI-assisted future, ensuring the continued integrity and
scalability of scientific validation, while maintaining high standards of peer
review.

</details>


### [5] [Compiling Metric Temporal Answer Set Programming](https://arxiv.org/abs/2506.08150)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Javier Romero,Susana Hahn,Torsten Schaub*

Main category: cs.AI

TL;DR: This paper introduces a scalable computational method for Metric Answer Set Programming (ASP) with quantitative temporal constraints.


<details>
  <summary>Details</summary>
Motivation: To address the grounding bottleneck in ASP caused by fine-grained timing constraints during problem-solving.

Method: The authors extend ASP with difference constraints to externally manage time-related aspects, thus decoupling metric ASP from time granularity.

Result: The proposed solution removes the dependence of ASP's performance on the precision of time constraints.

Conclusion: This approach advances the scalability and effectiveness of ASP for applications requiring precise quantitative temporal constraints.

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to
allow for expressing quantitative temporal constrains, like durations and
deadlines. A central challenge is to maintain scalability when dealing with
fine-grained timing constraints, which can significantly exacerbate ASP's
grounding bottleneck. To address this issue, we leverage extensions of ASP with
difference constraints, a simplified form of linear constraints, to handle
time-related aspects externally. Our approach effectively decouples metric ASP
from the granularity of time, resulting in a solution that is unaffected by
time precision.

</details>


### [6] [AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data](https://arxiv.org/abs/2506.08306)
*Tuan Truong,Rithwik Sudharsan,Yibo Yang,Peter Xiangyuan Ma,Ruihan Yang,Stephan Mandt,Joshua S. Bloom*

Main category: cs.AI

TL;DR: The paper introduces AstroCompress, a challenge aiming to enhance astrophysical data compression using neural algorithms, which outperform traditional methods in specific conditions.


<details>
  <summary>Details</summary>
Motivation: Astronomical observatories face data transmission bottlenecks due to their unique remote conditions, creating a need for better lossless data compression to maximize scientific output.

Method: Four new astrophysical datasets with 16-bit image data are introduced alongside a benchmark of seven lossless compression techniques, comprising neural and traditional methods.

Result: The study shows that neural lossless compression algorithms can effectively improve data handling at observatories, often surpassing classical techniques.

Conclusion: Neural compression techniques offer promising benefits for astrophysical data collection, and future studies could explore their potential in lossy compression.

Abstract: The site conditions that make astronomical observatories in space and on the
ground so desirable -- cold and dark -- demand a physical remoteness that leads
to limited data transmission capabilities. Such transmission limitations
directly bottleneck the amount of data acquired and in an era of costly modern
observatories, any improvements in lossless data compression has the potential
scale to billions of dollars worth of additional science that can be
accomplished on the same instrument. Traditional lossless methods for
compressing astrophysical data are manually designed. Neural data compression,
on the other hand, holds the promise of learning compression algorithms
end-to-end from data and outperforming classical techniques by leveraging the
unique spatial, temporal, and wavelength structures of astronomical images.
This paper introduces AstroCompress: a neural compression challenge for
astrophysics data, featuring four new datasets (and one legacy dataset) with
16-bit unsigned integer imaging data in various modes: space-based,
ground-based, multi-wavelength, and time-series imaging. We provide code to
easily access the data and benchmark seven lossless compression methods (three
neural and four non-neural, including all practical state-of-the-art
algorithms). Our results on lossless compression indicate that lossless neural
compression techniques can enhance data collection at observatories, and
provide guidance on the adoption of neural compression in scientific
applications. Though the scope of this paper is restricted to lossless
compression, we also comment on the potential exploration of lossy compression
methods in future studies.

</details>


### [7] [LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs](https://arxiv.org/abs/2506.08321)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.AI

TL;DR: LeanTutor is an LLM-based tutoring system designed to teach math proofs by verifying, providing next steps, and giving instructional guidance using formal Lean tactics.


<details>
  <summary>Details</summary>
Motivation: Many students face challenges in learning math proofs, especially when bridging the gap between natural language reasoning and formal proof writing. Existing tools don't integrate formal proof checking and interactive guidance effectively.

Method: LeanTutor includes three modules: (1) An autoformalizer that translates student proofs into Lean code and checks validity via compilation, identifying errors when present; (2) A next-step generator leveraging LLMs to generate valid tactics for continuing the proof; and (3) A natural language feedback generator offering hints based on Lean's proof data.

Result: Evaluation on the PeanoBench dataset showed that the autoformalizer correctly formalizes 57% of tactics in correct proofs and identifies incorrect steps in 30% of faulty proofs. Additionally, its natural language hint generation outperforms a baseline in accuracy and relevance.

Conclusion: LeanTutor demonstrates the feasibility of combining LLMs with formal proof systems to effectively teach math proofs. Despite promising results, there's scope for improvement in autoformalization and error detection accuracy.

Abstract: We present LeanTutor, a Large Language Model (LLM)-based tutoring system for
math proofs. LeanTutor interacts with the student in natural language, formally
verifies student-written math proofs in Lean, generates correct next steps, and
provides the appropriate instructional guidance. LeanTutor is composed of three
modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and
(iii) a natural language feedback generator. The first module faithfully
autoformalizes student proofs into Lean and verifies proof accuracy via
successful code compilation. If the proof has an error, the incorrect step is
identified. The next-step generator module outputs a valid next Lean tactic for
incorrect proofs via LLM-based candidate generation and proof search. The
feedback generator module leverages Lean data to produce a
pedagogically-motivated natural language hint for the student user. To evaluate
our system, we introduce PeanoBench, a human-written dataset derived from the
Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each
natural language proof step is paired with the corresponding logically
equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of
tactics in correct proofs and accurately identifies the incorrect step in 30%
of incorrect proofs. In generating natural language hints for erroneous proofs,
LeanTutor outperforms a simple baseline on accuracy and relevance metrics.

</details>


### [8] [ORFS-agent: Tool-Using Agents for Chip Design Optimization](https://arxiv.org/abs/2506.08332)
*Amur Ghose,Andrew B. Kahng,Sayak Kundu,Zhiang Wang*

Main category: cs.AI

TL;DR: The paper proposes ORFS-agent, an LLM-based iterative optimization tool that refines hardware design parameters, showing notable efficiency and performance improvements in circuit design compared to Bayesian optimization methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of complex parameter optimization in integrated circuit design workflows due to the high dimensionality and significant downstream impacts of parameter adjustments.

Method: The authors introduced ORFS-agent, a modular and model-agnostic optimization agent utilizing large language models (LLMs) for iterative parameter tuning in hardware design flows. It adaptively explores parameter configurations and applies natural language guidance to trade off between objectives.

Result: Empirical results show ORFS-agent improves routed wirelength and clock period by over 13%, cuts optimization iterations by 40%, and supports flexible natural language multi-objective optimization.

Conclusion: ORFS-agent provides an efficient, adaptable, and interpretable framework for parameter optimization in circuit design workflows, outperforming standard optimization methods while remaining compatible with various LLMs without additional fine-tuning.

Abstract: Machine learning has been widely used to optimize complex engineering
workflows across numerous domains. In the context of integrated circuit design,
modern flows (e.g., going from a register-transfer level netlist to physical
layouts) involve extensive configuration via thousands of parameters, and small
changes to these parameters can have large downstream impacts on desired
outcomes - namely design performance, power, and area. Recent advances in Large
Language Models (LLMs) offer new opportunities for learning and reasoning
within such high-dimensional optimization tasks. In this work, we introduce
ORFS-agent, an LLM-based iterative optimization agent that automates parameter
tuning in an open-source hardware design flow. ORFS-agent adaptively explores
parameter configurations, demonstrating clear improvements over standard
Bayesian optimization approaches in terms of resource efficiency and final
design metrics. Our empirical evaluations on two different technology nodes and
a range of circuit benchmarks indicate that ORFS-agent can improve both routed
wirelength and effective clock period by over 13%, all while using 40% fewer
optimization iterations. Moreover, by following natural language objectives to
trade off certain metrics for others, ORFS-agent demonstrates a flexible and
interpretable framework for multi-objective optimization. Crucially, RFS-agent
is modular and model-agnostic, and can be plugged in to any frontier LLM
without any further fine-tuning.

</details>


### [9] [FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs](https://arxiv.org/abs/2506.08363)
*Jun Yin,Jing Zhong,Pengyu Zeng,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.AI

TL;DR: The paper introduces FloorplanMAE, a self-supervised learning model based on Masked Autoencoders (MAE), capable of predicting complete architectural floorplans from incomplete sketches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiencies in the architectural design process caused by the iterative nature of floorplan creation, by providing a method to predict complete floorplans from partial ones.

Method: The authors developed a floorplan reconstruction dataset named FloorplanNet and utilized Masked Autoencoders (MAE) with a lightweight Vision Transformer (ViT) for predicting and reconstructing incomplete floor plans.

Result: FloorplanMAE was evaluated against state-of-the-art benchmarks using experimental results and real architectural sketches, showcasing its ability to generate high-quality complete floorplans.

Conclusion: FloorplanMAE offers a scalable and efficient solution for reconstructing incomplete floorplans, potentially improving design efficiency and minimizing workload in architectural planning.

Abstract: In the architectural design process, floorplan design is often a dynamic and
iterative process. Architects progressively draw various parts of the floorplan
according to their ideas and requirements, continuously adjusting and refining
throughout the design process. Therefore, the ability to predict a complete
floorplan from a partial one holds significant value in the design process.
Such prediction can help architects quickly generate preliminary designs,
improve design efficiency, and reduce the workload associated with repeated
modifications. To address this need, we propose FloorplanMAE, a self-supervised
learning framework for restoring incomplete floor plans into complete ones.
First, we developed a floor plan reconstruction dataset, FloorplanNet,
specifically trained on architectural floor plans. Secondly, we propose a floor
plan reconstruction method based on Masked Autoencoders (MAE), which
reconstructs missing parts by masking sections of the floor plan and training a
lightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy
of FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally,
we validated the model using real sketches from the early stages of
architectural design. Experimental results show that the FloorplanMAE model can
generate high-quality complete floor plans from incomplete partial plans. This
framework provides a scalable solution for floor plan generation, with broad
application prospects.

</details>


### [10] [On Reasoning Strength Planning in Large Reasoning Models](https://arxiv.org/abs/2506.08390)
*Leheng Sheng,An Zhang,Zijian Wu,Weixiang Zhao,Changshuo Shen,Yi Zhang,Xiang Wang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: This paper explores how large reasoning models (LRMs) pre-plan their reasoning strength through directional activation vectors, providing insights into their difficulty-aware behavior.


<details>
  <summary>Details</summary>
Motivation: The underlying mechanism of LRMs automatically adjusting reasoning strength for problem difficulty remains unexplored, despite its frequent observation.

Method: The study investigates model activations, identifies a directional vector determining reasoning strength, and conducts causal interventions to validate its influence.

Result: LRMs pre-plan reasoning strengths in activations through a directional vector whose magnitude controls reasoning length. Adjusting this vector modulates model performance.

Conclusion: The findings enhance understanding of LRMs' internal reasoning mechanisms and provide tools for controlling reasoning behavior for tasks like overthinking detection and simplifying reasoning.

Abstract: Recent studies empirically reveal that large reasoning models (LRMs) can
automatically allocate more reasoning strengths (i.e., the number of reasoning
tokens) for harder problems, exhibiting difficulty-awareness for better task
performance. While this automatic reasoning strength allocation phenomenon has
been widely observed, its underlying mechanism remains largely unexplored. To
this end, we provide explanations for this phenomenon from the perspective of
model activations. We find evidence that LRMs pre-plan the reasoning strengths
in their activations even before generation, with this reasoning strength
causally controlled by the magnitude of a pre-allocated directional vector.
Specifically, we show that the number of reasoning tokens is predictable solely
based on the question activations using linear probes, indicating that LRMs
estimate the required reasoning strength in advance. We then uncover that LRMs
encode this reasoning strength through a pre-allocated directional vector
embedded in the activations of the model, where the vector's magnitude
modulates the reasoning strength. Subtracting this vector can lead to reduced
reasoning token number and performance, while adding this vector can lead to
increased reasoning token number and even improved performance. We further
reveal that this direction vector consistently yields positive reasoning length
prediction, and it modifies the logits of end-of-reasoning token </think> to
affect the reasoning length. Finally, we demonstrate two potential applications
of our findings: overthinking behavior detection and enabling efficient
reasoning on simple problems. Our work provides new insights into the internal
mechanisms of reasoning in LRMs and offers practical tools for controlling
their reasoning behaviors. Our code is available at
https://github.com/AlphaLab-USTC/LRM-plans-CoT.

</details>


### [11] [SafeCoT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/abs/2506.08399)
*Jiachen Ma,Zhanhui Zhou,Chao Yang,Chaochao Lu*

Main category: cs.AI

TL;DR: Introduces SafeCoT, a light framework for improving VLMs' refusal behavior safely and effectively.


<details>
  <summary>Details</summary>
Motivation: The need for safer, context-aware responses in vision-language models, especially in critical or ambiguous situations.

Method: SafeCoT applies rule-based chain-of-thought supervision using minimal supervision, avoiding heavy annotations or complex modeling.

Result: SafeCoT improves refusal behaviors by reducing overrefusal and enhancing generalization, as shown in experiments across benchmarks.

Conclusion: SafeCoT provides a scalable approach to safer VLM alignment, even with limited training data, ensuring reliability in safety-critical scenarios.

Abstract: Ensuring safe and appropriate responses from vision-language models (VLMs)
remains a critical challenge, particularly in high-risk or ambiguous scenarios.
We introduce SafeCoT, a lightweight, interpretable framework that leverages
rule-based chain-of-thought (CoT) supervision to improve refusal behavior in
VLMs. Unlike prior methods that rely on large-scale safety annotations or
complex modeling, SafeCoT uses minimal supervision to help models reason about
safety risks and make context-aware refusals. Experiments across multiple
benchmarks show that SafeCoT significantly reduces overrefusal and enhances
generalization, even with limited training data. Our approach offers a scalable
solution for aligning VLMs with safety-critical objectives.

</details>


### [12] [Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems](https://arxiv.org/abs/2506.08401)
*Runze Li,Di Jin,Xiaobao Wang,Dongxiao He,Bingdao Feng,Zhen Wang*

Main category: cs.AI

TL;DR: The paper introduces a graph-based backdoor attack method for recommendation systems using a single fake user node to covertly expose target items to target users, minimizing the impact on unrelated nodes.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in graph recommendation systems, specifically low stealth and high destructiveness in prevailing shilling attacks.

Method: The authors developed a single-node trigger generator for graph backdoor attacks, which selectively exposes target items to users by adding only one fake user node and applying constraints to minimize impact on unrelated nodes.

Result: Experiments showed ≥50% exposure of target items for 99% of target users, while maintaining approximately a 5% impact on system performance.

Conclusion: The proposed method provides a stealthier and effective approach to manipulating recommendation results, highlighting vulnerabilities in graph-based recommendation systems.

Abstract: Graph recommendation systems have been widely studied due to their ability to
effectively capture the complex interactions between users and items. However,
these systems also exhibit certain vulnerabilities when faced with attacks. The
prevailing shilling attack methods typically manipulate recommendation results
by injecting a large number of fake nodes and edges. However, such attack
strategies face two primary challenges: low stealth and high destructiveness.
To address these challenges, this paper proposes a novel graph backdoor attack
method that aims to enhance the exposure of target items to the target user in
a covert manner, without affecting other unrelated nodes. Specifically, we
design a single-node trigger generator, which can effectively expose multiple
target items to the target user by inserting only one fake user node.
Additionally, we introduce constraint conditions between the target nodes and
irrelevant nodes to mitigate the impact of fake nodes on the recommendation
system's performance. Experimental results show that the exposure of the target
items reaches no less than 50% in 99% of the target users, while the impact on
the recommendation system's performance is controlled within approximately 5%.

</details>


### [13] [Transforming Expert Knowledge into Scalable Ontology via Large Language Models](https://arxiv.org/abs/2506.08422)
*Ikkei Itoku,David Theil,Evelyn Eichelsdoerfer Uehara,Sreyoshi Bhaduri,Junnosuke Kuroda,Toshi Yumoto,Alex Gil,Natalie Perez,Rajesh Cherukuri,Naumaan Nayyar*

Main category: cs.AI

TL;DR: The paper proposes a novel framework utilizing large language models (LLMs) combined with expert calibration and iterative prompt optimization to automate taxonomy alignment, achieving superior performance over human benchmarks.


<details>
  <summary>Details</summary>
Motivation: Taxonomy alignment is essential for knowledge representation in various domains but is challenging due to expensive manual methods and the limitations of existing automated approaches in handling nuanced semantics and consistency.

Method: The framework integrates expert-labeled examples, multi-stage prompt engineering, and human validation to guide LLMs in producing taxonomy linkages and rationales.

Result: On a concept essentiality mapping task, the framework achieved an F1-score of 0.97, significantly outperforming the human benchmark score of 0.68.

Conclusion: The proposed method scales taxonomy alignment effectively while ensuring high-quality mappings, preserving expert oversight for ambiguous cases, and demonstrating the potential superiority of combined automated strategies.

Abstract: Having a unified, coherent taxonomy is essential for effective knowledge
representation in domain-specific applications as diverse terminologies need to
be mapped to underlying concepts. Traditional manual approaches to taxonomy
alignment rely on expert review of concept pairs, but this becomes
prohibitively expensive and time-consuming at scale, while subjective
interpretations often lead to expert disagreements. Existing automated methods
for taxonomy alignment have shown promise but face limitations in handling
nuanced semantic relationships and maintaining consistency across different
domains. These approaches often struggle with context-dependent concept
mappings and lack transparent reasoning processes. We propose a novel framework
that combines large language models (LLMs) with expert calibration and
iterative prompt optimization to automate taxonomy alignment. Our method
integrates expert-labeled examples, multi-stage prompt engineering, and human
validation to guide LLMs in generating both taxonomy linkages and supporting
rationales. In evaluating our framework on a domain-specific mapping task of
concept essentiality, we achieved an F1-score of 0.97, substantially exceeding
the human benchmark of 0.68. These results demonstrate the effectiveness of our
approach in scaling taxonomy alignment while maintaining high-quality mappings
and preserving expert oversight for ambiguous cases.

</details>


### [14] [SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy](https://arxiv.org/abs/2506.08424)
*Yong Liang Goh,Zhiguang Cao,Yining Ma,Jianan Zhou,Mohammad Haroon Dupty,Wee Sun Lee*

Main category: cs.AI

TL;DR: This paper introduces SHIELD, a model designed for solving complex, real-world VRP problems using sparsity and hierarchy principles to achieve better generalization.


<details>
  <summary>Details</summary>
Motivation: Existing models for routing problems fail to adequately address the complexity of real-world customer distributions in multi-task settings.

Method: SHIELD incorporates a Mixture-of-Depths (MoD) technique to enforce sparsity and a context-based clustering layer to exploit hierarchical structures, enhancing efficiency and adaptability.

Result: SHIELD significantly outperforms existing models on 9 real-world maps, each containing 16 VRP variants.

Conclusion: SHIELD demonstrates improved generalization and efficiency for more realistic routing problems, addressing both task-specific and shared distribution challenges.

Abstract: Recent advances toward foundation models for routing problems have shown
great potential of a unified deep model for various VRP variants. However, they
overlook the complex real-world customer distributions. In this work, we
advance the Multi-Task VRP (MTVRP) setting to the more realistic yet
challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce
SHIELD, a novel model that leverages both sparsity and hierarchy principles.
Building on a deeper decoder architecture, we first incorporate the
Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both
efficiency and generalization by allowing the model to dynamically select nodes
to use or skip each decoder layer, providing the needed capacity to adaptively
allocate computation for learning the task/distribution specific and shared
representations. We also develop a context-based clustering layer that exploits
the presence of hierarchical structures in the problems to produce better local
representations. These two designs inductively bias the network to identify key
features that are common across tasks and distributions, leading to
significantly improved generalization on unseen ones. Our empirical results
demonstrate the superiority of our approach over existing methods on 9
real-world maps with 16 VRP variants each.

</details>


### [15] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
*Peng-Yuan Wang,Tian-Shuo Liu,Chenyang Wang,Yi-Di Wang,Shu Yan,Cheng-Xing Jia,Xu-Hui Liu,Xin-Wei Chen,Jia-Cheng Xu,Ziniu Li,Yang Yu*

Main category: cs.AI

TL;DR: This paper surveys progress in mathematical reasoning in large language models, detailing advancements in comprehension and answer generation, techniques for improvement, and outlining future research directions.


<details>
  <summary>Details</summary>
Motivation: Mathematical reasoning represents a challenging frontier for developing advanced AI systems, and recent progress in LLMs showcases their potential in addressing this complexity.

Method: The paper reviews developmental phases in LLMs' mathematical reasoning, discussing strategies like pretraining, supervised fine-tuning, reinforcement learning, and test-time scaling.

Result: Notable progress in comprehension and reasoning has been achieved, yet persistent challenges like efficiency and generalization remain.

Conclusion: The paper highlights promising directions for addressing challenges, including advanced pretraining and principled learning paradigms, aiming to guide researchers in improving LLM reasoning capabilities.

Abstract: Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [16] [Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing](https://arxiv.org/abs/2506.08462)
*Christos Margadji,Sebastian W. Pattinson*

Main category: cs.AI

TL;DR: CIPHER introduces a vision-language-action framework for industrial control, merging human-like reasoning and expert knowledge for autonomous precision and transparency.


<details>
  <summary>Details</summary>
Motivation: Industrial processes demand adaptability and precision in unpredictable environments where errors are costly and hard to detect.

Method: CIPHER integrates a hybrid model combining process expertise, a regression model for engineering precision, and retrieval-augmented generation for physics-informed reasoning.

Result: The framework demonstrates strong generalization to new tasks, interprets diverse inputs, explains decisions, and autonomously generates precise machine instructions without explicit annotations.

Conclusion: CIPHER sets the groundwork for autonomous, precise, and transparent AI systems in industries, enhancing trust and safety in deployment.

Abstract: Industrial processes must be robust and adaptable, as environments and tasks
are often unpredictable, while operational errors remain costly and difficult
to detect. AI-based control systems offer a path forward, yet typically depend
on supervised learning with extensive labelled datasets, which limits their
ability to generalize across variable and data-scarce industrial settings.
Foundation models could enable broader reasoning and knowledge integration, but
rarely deliver the quantitative precision demanded by engineering applications.
Here, we introduceControl and Interpretation of Production via Hybrid Expertise
and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming
to replicate human-like reasoning for industrial control, instantiated in a
commercial-grade 3D printer. It integrates a process expert, a regression model
enabling quantitative characterization of system states required for
engineering tasks. CIPHER also incorporates retrieval-augmented generation to
access external expert knowledge and support physics-informed, chain-of-thought
reasoning. This hybrid architecture exhibits strong generalization to
out-of-distribution tasks. It interprets visual or textual inputs from process
monitoring, explains its decisions, and autonomously generates precise machine
instructions, without requiring explicit annotations. CIPHER thus lays the
foundations for autonomous systems that act with precision, reason with
context, and communicate decisions transparently, supporting safe and trusted
deployment in industrial settings.

</details>


### [17] [RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being](https://arxiv.org/abs/2506.08486)
*Rahatara Ferdousi,M Anwar Hossain*

Main category: cs.AI

TL;DR: This paper introduces RHealthTwin, a responsible framework for AI-powered digital twins in healthcare, addressing hallucinations, bias, and ethical issues.


<details>
  <summary>Details</summary>
Motivation: Large language models create opportunities in healthcare, but their deployment faces challenges such as hallucinations, bias, and ethical concerns.

Method: RHealthTwin uses a Responsible Prompt Engine to dynamically structure inputs, ensure safer outputs, and adapt based on user feedback.

Result: The framework achieves state-of-the-art performance in health domains like mental support and symptom triage with high ethical compliance (over 90%).

Conclusion: RHealthTwin provides a foundation for responsible AI applications in healthcare, offering context-aware, personalized, and ethical responses.

Abstract: The rise of large language models (LLMs) has created new possibilities for
digital twins in healthcare. However, the deployment of such systems in
consumer health contexts raises significant concerns related to hallucination,
bias, lack of transparency, and ethical misuse. In response to recommendations
from health authorities such as the World Health Organization (WHO), we propose
Responsible Health Twin (RHealthTwin), a principled framework for building and
governing AI-powered digital twins for well-being assistance. RHealthTwin
processes multimodal inputs that guide a health-focused LLM to produce safe,
relevant, and explainable responses. At the core of RHealthTwin is the
Responsible Prompt Engine (RPE), which addresses the limitations of traditional
LLM configuration. Conventionally, users input unstructured prompt and the
system instruction to configure the LLM, which increases the risk of
hallucination. In contrast, RPE extracts predefined slots dynamically to
structure both inputs. This guides the language model to generate responses
that are context aware, personalized, fair, reliable, and explainable for
well-being assistance. The framework further adapts over time through a
feedback loop that updates the prompt structure based on user satisfaction. We
evaluate RHealthTwin across four consumer health domains including mental
support, symptom triage, nutrition planning, and activity coaching. RPE
achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and
BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical
compliance and instruction-following metrics using LLM-as-judge evaluation,
outperforming baseline strategies. We envision RHealthTwin as a forward-looking
foundation for responsible LLM-based applications in health and well-being.

</details>


### [18] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta,Nikita Jangid,Shounak Das,Amit Sethi*

Main category: cs.AI

TL;DR: FedTAIL is a federated domain generalization framework addressing domain shifts and class imbalance by using sharpness-guided, gradient-aligned optimization.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of training models that generalize reliably to unseen domains, particularly under conditions of domain shifts and long-tailed class distributions.

Method: FedTAIL introduces a sharpness-guided, gradient-aligned optimization framework. It employs a gradient coherence regularizer to stabilize optimization, sharpness minimization for class imbalance, and integrates sharpness-aware perturbations into entropy regularization.

Result: FedTAIL achieves state-of-the-art performance on domain generalization benchmarks, showing robustness to domain shifts and label imbalance in centralized and federated settings.

Conclusion: FedTAIL effectively unifies optimization harmonization, class-aware regularization, and conditional alignment and offers practical utility in improving generalization under varying conditions.

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [19] [Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness](https://arxiv.org/abs/2506.08532)
*Yanwei Gong,Xiaolin Chang*

Main category: cs.AI

TL;DR: A novel UAV trajectory planning framework integrating deep reinforcement learning (DRL) and large language model (LLM) reasoning improves safety, compliance, and economic viability in urban environments.


<details>
  <summary>Details</summary>
Motivation: The need for effective UAV trajectory planning in urban environments due to the rapid growth of the low-altitude economy and associated challenges such as airspace constraints and economic efficiency.

Method: Combining deep reinforcement learning (DRL) with large language model (LLM) reasoning to tackle trajectory planning problems, focusing on safety, compliance, and efficiency.

Result: Experimental results show the proposed method outperforms existing baselines in metrics like data collection rate, collision avoidance, landing success, regulatory compliance, and energy efficiency.

Conclusion: The approach is effective in addressing UAV trajectory planning challenges under low-altitude economy constraints, offering significant improvements in performance.

Abstract: The rapid growth of the low-altitude economy has driven the widespread
adoption of unmanned aerial vehicles (UAVs). This growing deployment presents
new challenges for UAV trajectory planning in complex urban environments.
However, existing studies often overlook key factors, such as urban airspace
constraints and economic efficiency, which are essential in low-altitude
economy contexts. Deep reinforcement learning (DRL) is regarded as a promising
solution to these issues, while its practical adoption remains limited by low
learning efficiency. To overcome this limitation, we propose a novel UAV
trajectory planning framework that combines DRL with large language model (LLM)
reasoning to enable safe, compliant, and economically viable path planning.
Experimental results demonstrate that our method significantly outperforms
existing baselines across multiple metrics, including data collection rate,
collision avoidance, successful landing, regulatory compliance, and energy
efficiency. These results validate the effectiveness of our approach in
addressing UAV trajectory planning key challenges under constraints of the
low-altitude economy networking.

</details>


### [20] [HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning](https://arxiv.org/abs/2506.08580)
*Yang Lv,Jinlong Lei,Peng Yi*

Main category: cs.AI

TL;DR: The paper introduces HGformer, a hierarchical graph Transformer framework, to solve the two-stage Colonel Blotto game for optimizing resource allocation in complex adversarial environments.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches struggle with achieving optimal strategies in two-stage Colonel Blotto games due to sequential dependencies and graph topology constraints.

Method: The authors propose HGformer, featuring an enhanced graph Transformer encoder and a two-agent hierarchical decision model. It is combined with a layer-by-layer feedback reinforcement learning algorithm for coordination between decision stages.

Result: HGformer outperforms hierarchical decision-making and graph neural network methods by significantly improving resource allocation efficiency and yielding better adversarial payoffs.

Conclusion: The HGformer framework is highly effective for addressing complex dynamic game scenarios, offering superior outcomes in adversarial resource allocation problems.

Abstract: Two-stage Colonel Blotto game represents a typical adversarial resource
allocation problem, in which two opposing agents sequentially allocate
resources in a network topology across two phases: an initial resource
deployment followed by multiple rounds of dynamic reallocation adjustments. The
sequential dependency between game stages and the complex constraints imposed
by the graph topology make it difficult for traditional approaches to attain a
globally optimal strategy. To address these challenges, we propose a
hierarchical graph Transformer framework called HGformer. By incorporating an
enhanced graph Transformer encoder with structural biases and a two-agent
hierarchical decision model, our approach enables efficient policy generation
in large-scale adversarial environments. Moreover, we design a layer-by-layer
feedback reinforcement learning algorithm that feeds the long-term returns from
lower-level decisions back into the optimization of the higher-level strategy,
thus bridging the coordination gap between the two decision-making stages.
Experimental results demonstrate that, compared to existing hierarchical
decision-making or graph neural network methods, HGformer significantly
improves resource allocation efficiency and adversarial payoff, achieving
superior overall performance in complex dynamic game scenarios.

</details>


### [21] [A Sample Efficient Conditional Independence Test in the Presence of Discretization](https://arxiv.org/abs/2506.08747)
*Boyang Sun,Yu Yao,Xinshuai Dong,Zongfang Liu,Tongliang Liu,Yumou Qiu,Kun Zhang*

Main category: cs.AI

TL;DR: The paper introduces a sample-efficient conditional independence test that avoids loss of information caused by binarization, using Generalized Method of Moments (GMM) and nodewise regression.


<details>
  <summary>Details</summary>
Motivation: Measurement limitations in real-world scenarios lead to variables being discretized, which hampers the accuracy of Conditional Independence tests.

Method: The method is based on addressing over-identifying restriction problems using Generalized Method of Moments (GMM) and deriving test statistics with nodewise regression.

Result: The paper establishes theoretical results and demonstrates empirical effectiveness of the proposed Conditional Independence test across various datasets.

Conclusion: The proposed test reliably addresses latent independence relationships without information loss and achieves superior performance.

Abstract: In many real-world scenarios, interested variables are often represented as
discretized values due to measurement limitations. Applying Conditional
Independence (CI) tests directly to such discretized data, however, can lead to
incorrect conclusions. To address this, recent advancements have sought to
infer the correct CI relationship between the latent variables through
binarizing observed data. However, this process inevitably results in a loss of
information, which degrades the test's performance. Motivated by this, this
paper introduces a sample-efficient CI test that does not rely on the
binarization process. We find that the independence relationships of latent
continuous variables can be established by addressing an over-identifying
restriction problem with Generalized Method of Moments (GMM). Based on this
insight, we derive an appropriate test statistic and establish its asymptotic
distribution correctly reflecting CI by leveraging nodewise regression.
Theoretical findings and Empirical results across various datasets demonstrate
that the superiority and effectiveness of our proposed test. Our code
implementation is provided in https://github.com/boyangaaaaa/DCT

</details>


### [22] [FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings](https://arxiv.org/abs/2506.08627)
*Douwe Geurtjens,Xixi Lu*

Main category: cs.AI

TL;DR: The paper introduces FoldA, a technique using Petri net unfoldings to improve conformance checking by reducing state explosion and improving concurrency representation, though it may increase computation time.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenges of conformance checking when current methods fail to efficiently handle complex models with high choice and concurrency, and struggle to fully represent concurrent behaviors.

Method: The authors propose FoldA, leveraging directed Petri net unfoldings to compute partial-order alignments dynamically. This alternative approach avoids state space explosion.

Result: Experimental evaluation on synthetic and real-world data demonstrates that FoldA reduces the number of queued states and improves concurrency representation, though computation time increases.

Conclusion: FoldA offers a more accurate and scalable alternative for conformance checking, particularly in processes with significant concurrency, despite its higher computational demands.

Abstract: Conformance checking is a fundamental task of process mining, which
quantifies the extent to which the observed process executions match a
normative process model. The state-of-the-art approaches compute alignments by
exploring the state space formed by the synchronous product of the process
model and the trace. This often leads to state space explosion, particularly
when the model exhibits a high degree of choice and concurrency. Moreover, as
alignments inherently impose a sequential structure, they fail to fully
represent the concurrent behavior present in many real-world processes. To
address these limitations, this paper proposes a new technique for computing
partial-order alignments {on the fly using directed Petri net unfoldings, named
FoldA. We evaluate our technique on 485 synthetic model-log pairs and compare
it against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6
benchmark pairs. The results show that our unfolding alignment, although it
requires more computation time, generally reduces the number of queued states
and provides a more accurate representation of concurrency.

</details>


### [23] [Modular Recurrence in Contextual MDPs for Universal Morphology Control](https://arxiv.org/abs/2506.08630)
*Laurens Engwegen,Daan Brinks,Wendelin Böhmer*

Main category: cs.AI

TL;DR: This paper presents a modular recurrent neural architecture for better multi-robot control through inference of partially observable contextual information, showing improved generalizability to unseen robots in various environments.


<details>
  <summary>Details</summary>
Motivation: A universal robot controller is desirable as it would enhance computational and data efficiency in multi-robot settings, but generalization to unseen robots remains a significant challenge.

Method: The authors utilize a modular recurrent neural architecture that infers partially observable contextual information about robots through interactions, enabling better generalization. They test the performance on MuJoCo simulations with diverse robot morphologies.

Result: The proposed approach demonstrates substantial improvement in controlling robots with unseen dynamics, kinematics, and topologies across four environments.

Conclusion: The study suggests that exploiting modular recurrent architectures with inferred contextual information can greatly enhance generalization of robot controllers to novel morphologies.

Abstract: A universal controller for any robot morphology would greatly improve
computational and data efficiency. By utilizing contextual information about
the properties of individual robots and exploiting their modular structure in
the architecture of deep reinforcement learning agents, steps have been made
towards multi-robot control. Generalization to new, unseen robots, however,
remains a challenge. In this paper we hypothesize that the relevant contextual
information is partially observable, but that it can be inferred through
interactions for better generalization to contexts that are not seen during
training. To this extent, we implement a modular recurrent architecture and
evaluate its generalization performance on a large set of MuJoCo robots. The
results show a substantial improved performance on robots with unseen dynamics,
kinematics, and topologies, in four different environments.

</details>


### [24] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Yingjie Wang,Baisheng Lai,Jieping Ye,Mingli Song,Dacheng Tao*

Main category: cs.AI

TL;DR: The paper introduces a self-rewarding reinforcement learning framework (CoVo) for improving large language model (LLM) reasoning without relying on external supervision. It utilizes consistency and volatility of reasoning trajectories as intrinsic reward signals.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning (RL) has shown potential in reasoning tasks, but its reliance on external supervision limits wider applications. This paper aims to eliminate such dependency for scalable and effective RL-based reasoning in LLMs.

Method: The authors propose CoVo, which combines Consistency and Volatility as intrinsic reward signals through a vector-space aggregation strategy. A curiosity bonus is also added to encourage exploration, creating a self-rewarding RL approach.

Result: Experiments on multiple reasoning benchmarks reveal that CoVo delivers comparable or superior performance to supervised RL, showcasing its effectiveness.

Conclusion: CoVo provides a novel and scalable way for LLMs to learn reasoning without external supervision, offering robust and high-performing results. Code is also shared for public usage.

Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [25] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti,Michael Färber*

Main category: cs.AI

TL;DR: The paper combines Knowledge Graphs (KGs) with Large Language Models (LLMs) to improve causal discovery methods in complex systems, outperforming existing approaches significantly.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the instability and inconsistency in causal inference results produced by current methods that rely on LLMs, proposing a novel approach to enhance reliability.

Method: The proposed method identifies informative metapath-based subgraphs within KGs, refines them using Learning-to-Rank models, and incorporates the selected subgraphs into LLM prompts for zero-shot causal inference.

Result: Experiments on biomedical and open-domain datasets show improvements of up to 44.4 F1 points compared to baseline methods, validated across diverse LLMs and KGs.

Conclusion: Integrating KGs with LLMs significantly enhances knowledge-driven causal discovery methods, setting new benchmarks in performance and reliability for inferring multivariate causal relationships.

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [26] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
*Irene Testini,José Hernández-Orallo,Lorenzo Pacchiardi*

Main category: cs.AI

TL;DR: This paper surveys the evaluation of Large Language Model (LLM) assistants and agents in data science, highlighting gaps in focus, collaboration levels, and task transformation.


<details>
  <summary>Details</summary>
Motivation: Data science processes are increasingly leveraging LLMs for support and automation, but their evaluation lacks comprehensive exploration of activities and collaboration levels.

Method: The authors conducted a survey of the evaluation methods used for LLMs and agents in data science, analyzing existing practices and identifying areas of neglect.

Result: They identified key trends such as a focus on goal-oriented activities, ignoring exploratory actions, leaning towards extremes in human collaboration levels, and neglecting task transformation potential.

Conclusion: There is a need for broader considerations in evaluating LLMs for data science, including exploratory tasks, intermediate collaboration, and task transformation frameworks.

Abstract: Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


### [27] [Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/abs/2506.08872)
*Nataliya Kosmyna,Eugene Hauptmann,Ye Tong Yuan,Jessica Situ,Xian-Hao Liao,Ashly Vivian Beresnitzky,Iris Braunstein,Pattie Maes*

Main category: cs.AI

TL;DR: This study investigates the impact of LLM-assisted writing on neural and behavioral performance, revealing reduced cognitive engagement and long-term underperformance among users.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the consequences of using large language models (LLMs) in essay writing, particularly in terms of neural activity, behavioral outcomes, and educational implications.

Method: Participants were divided into three groups—LLM, Search Engine, and Brain-only—and their essays were analyzed via NLP. EEG measured cognitive load, and essay quality was assessed by human teachers and an AI judge. Switching groups occurred in a fourth session.

Result: LLMs led to weaker brain connectivity and reduced cognitive activity, while Brain-only participants demonstrated the most engagement. Transitioning between conditions shifted neural and memory performance. Self-reported ownership of essays was lowest in the LLM group.

Conclusion: LLM use provides convenience but imposes cognitive and behavioral costs, potentially harming long-term educational outcomes. The study calls for further investigation into AI's role in learning.

Abstract: This study explores the neural and behavioral consequences of LLM-assisted
essay writing. Participants were divided into three groups: LLM, Search Engine,
and Brain-only (no tools). Each completed three sessions under the same
condition. In a fourth session, LLM users were reassigned to Brain-only group
(LLM-to-Brain), and Brain-only users were reassigned to LLM condition
(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18
completing session 4. We used electroencephalography (EEG) to assess cognitive
load during essay writing, and analyzed essays using NLP, as well as scoring
essays with the help from human teachers and an AI judge. Across groups, NERs,
n-gram patterns, and topic ontology showed within-group homogeneity. EEG
revealed significant differences in brain connectivity: Brain-only participants
exhibited the strongest, most distributed networks; Search Engine users showed
moderate engagement; and LLM users displayed the weakest connectivity.
Cognitive activity scaled down in relation to external tool use. In session 4,
LLM-to-Brain participants showed reduced alpha and beta connectivity,
indicating under-engagement. Brain-to-LLM users exhibited higher memory recall
and activation of occipito-parietal and prefrontal areas, similar to Search
Engine users. Self-reported ownership of essays was the lowest in the LLM group
and the highest in the Brain-only group. LLM users also struggled to accurately
quote their own work. While LLMs offer immediate convenience, our findings
highlight potential cognitive costs. Over four months, LLM users consistently
underperformed at neural, linguistic, and behavioral levels. These results
raise concerns about the long-term educational implications of LLM reliance and
underscore the need for deeper inquiry into AI's role in learning.

</details>


### [28] [Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation](https://arxiv.org/abs/2506.08898)
*Mingfeng Fan,Jianan Zhou,Yifeng Zhang,Yaoxin Wu,Jinbiao Chen,Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: The paper introduces POCCO, a framework that improves solving multi-objective combinatorial optimization problems by adaptively selecting specialized neural architectures for subproblems driven by preference signals, leading to superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for solving MOCOPs treat all subproblems equally and use a single model, limiting exploration and causing suboptimal results.

Method: POCCO incorporates conditional computation blocks for adaptive model selection and proposes a preference-driven optimization algorithm that learns pairwise preferences between solutions.

Result: POCCO outperformed current neural methods across four MOCOP benchmarks, showcasing its significant superiority and generalization capabilities.

Conclusion: The paradigm shift introduced by POCCO enhances both adaptability and optimization in solving MOCOPs, addressing prior limitations effectively.

Abstract: Recent deep reinforcement learning methods have achieved remarkable success
in solving multi-objective combinatorial optimization problems (MOCOPs) by
decomposing them into multiple subproblems, each associated with a specific
weight vector. However, these methods typically treat all subproblems equally
and solve them using a single model, hindering the effective exploration of the
solution space and thus leading to suboptimal performance. To overcome the
limitation, we propose POCCO, a novel plug-and-play framework that enables
adaptive selection of model structures for subproblems, which are subsequently
optimized based on preference signals rather than explicit reward values.
Specifically, we design a conditional computation block that routes subproblems
to specialized neural architectures. Moreover, we propose a preference-driven
optimization algorithm that learns pairwise preferences between winning and
losing solutions. We evaluate the efficacy and versatility of POCCO by applying
it to two state-of-the-art neural methods for MOCOPs. Experimental results
across four classic MOCOP benchmarks demonstrate its significant superiority
and strong generalization.

</details>


### [29] [IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections](https://arxiv.org/abs/2506.08957)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: The paper introduces a data-driven approach for traffic intersection simulation using deep generative trajectory prediction models, improving behavior realism and operational assessments.


<details>
  <summary>Details</summary>
Motivation: Traffic intersections are critical for safety and efficiency, yet current rule-based simulators fail to mimic realistic driving behavior effectively.

Method: The authors developed a multi-headed self-attention trajectory model incorporating traffic signal information and proposed traffic engineering metrics for evaluation within a simulation pipeline.

Result: The new trajectory prediction model outperformed previous approaches in traffic-related metrics and aided realistic intersection behavior modeling.

Conclusion: The study advanced generative modeling for traffic intersections, enabling improved simulation frameworks for safety and efficiency analysis.

Abstract: Traffic simulators are widely used to study the operational efficiency of
road infrastructure, but their rule-based approach limits their ability to
mimic real-world driving behavior. Traffic intersections are critical
components of the road infrastructure, both in terms of safety risk (nearly 28%
of fatal crashes and 58% of nonfatal crashes happen at intersections) as well
as the operational efficiency of a road corridor. This raises an important
question: can we create a data-driven simulator that can mimic the macro- and
micro-statistics of the driving behavior at a traffic intersection? Deep
Generative Modeling-based trajectory prediction models provide a good starting
point to model the complex dynamics of vehicles at an intersection. But they
are not tested in a "live" micro-simulation scenario and are not evaluated on
traffic engineering-related metrics. In this study, we propose traffic
engineering-related metrics to evaluate generative trajectory prediction models
and provide a simulation-in-the-loop pipeline to do so. We also provide a
multi-headed self-attention-based trajectory prediction model that incorporates
the signal information, which outperforms our previous models on the evaluation
metrics.

</details>


### [30] [Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics](https://arxiv.org/abs/2506.08963)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: The paper leverages deep generative models to understand traffic at intersections and proposes a new analytics tool for evaluating traffic models on engineering-relevant metrics.


<details>
  <summary>Details</summary>
Motivation: To address the deficiencies in evaluating traffic intersection models using traditional metrics that fail to account for traffic rule violations and other engineering-specific concerns.

Method: They trained multi-vehicle trajectory forecasting models on real-world intersection data and tested them in a live microsimulation environment. New metrics evaluating traffic-rule-related behaviors were introduced.

Result: Despite low trajectory reconstruction errors, the generated trajectories exhibited rule violations and other undesired behaviors, highlighting gaps in current model evaluations.

Conclusion: The study highlights the need for advanced evaluation metrics in traffic microsimulation to improve the practical applicability of generative models for urban intersections.

Abstract: Traffic Intersections are vital to urban road networks as they regulate the
movement of people and goods. However, they are regions of conflicting
trajectories and are prone to accidents. Deep Generative models of traffic
dynamics at signalized intersections can greatly help traffic authorities
better understand the efficiency and safety aspects. At present, models are
evaluated on computational metrics that primarily look at trajectory
reconstruction errors. They are not evaluated online in a `live'
microsimulation scenario. Further, these metrics do not adequately consider
traffic engineering-specific concerns such as red-light violations, unallowed
stoppage, etc. In this work, we provide a comprehensive analytics tool to
train, run, and evaluate models with metrics that give better insights into
model performance from a traffic engineering point of view. We train a
state-of-the-art multi-vehicle trajectory forecasting model on a large dataset
collected by running a calibrated scenario of a real-world urban intersection.
We then evaluate the performance of the prediction models, online in a
microsimulator, under unseen traffic conditions. We show that despite using
ideally-behaved trajectories as input, and achieving low trajectory
reconstruction errors, the generated trajectories show behaviors that break
traffic rules. We introduce new metrics to evaluate such undesired behaviors
and present our results.

</details>


### [31] [A Survey of Link Prediction in N-ary Knowledge Graphs](https://arxiv.org/abs/2506.08970)
*Jiyao Wei,Saiping Guan,Da Li,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: This paper surveys link prediction in N-ary Knowledge Graphs (NKGs), categorizing methods and analyzing their performance.


<details>
  <summary>Details</summary>
Motivation: To address the growing importance of completing NKGs for enhanced downstream application performance.

Method: The paper provides a systematic categorization of link prediction methods in NKGs and analyzes their performance and applications.

Result: A comprehensive review of existing methods and applications for link prediction in NKGs, with identified gaps.

Conclusion: The paper highlights promising future research directions and provides a foundational resource for further studies on link prediction in NKGs.

Abstract: N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph
designed to efficiently represent complex real-world facts. Unlike traditional
knowledge graphs, where a fact typically involves two entities, NKGs can
capture n-ary facts containing more than two entities. Link prediction in NKGs
aims to predict missing elements within these n-ary facts, which is essential
for completing NKGs and improving the performance of downstream applications.
This task has recently gained significant attention. In this paper, we present
the first comprehensive survey of link prediction in NKGs, providing an
overview of the field, systematically categorizing existing methods, and
analyzing their performance and application scenarios. We also outline
promising directions for future research.

</details>


### [32] [AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions](https://arxiv.org/abs/2506.09038)
*Polina Kirichenko,Mark Ibrahim,Kamalika Chaudhuri,Samuel J. Bell*

Main category: cs.AI

TL;DR: The paper introduces AbstentionBench, a benchmark to evaluate the ability of large language models (LLMs) to abstain from answering when queries are uncertain or inappropriate. Results show abstention is poorly handled by current LLMs, even when scaling models or fine-tuning them for reasoning.


<details>
  <summary>Details</summary>
Motivation: Reliably deploying LLMs requires ensuring they can abstain from answering uncertain or unanswerable queries, a critical feature for their use in practical, high-stakes domains.

Method: The authors developed AbstentionBench, evaluating 20 LLMs across 20 datasets covering diverse scenarios like subjective queries, false premises, and outdated information.

Result: Scaling LLMs does not improve abstention performance, and fine-tuning for reasoning worsens abstention accuracy by 24%. System prompts help mildly but do not resolve the issue.

Conclusion: Abstention remains a significant challenge for LLMs, and the release of AbstentionBench aims to encourage further research to improve LLM reliability.

Abstract: For Large Language Models (LLMs) to be reliably deployed in both everyday and
high-stakes domains, knowing when not to answer is equally critical as
answering correctly. Real-world user queries, which can be underspecified,
ill-posed, or fundamentally unanswerable, require LLMs to reason about
uncertainty and selectively abstain -- i.e., refuse to answer definitively.
However, abstention remains understudied, without a systematic evaluation
framework for modern LLMs. In this work, we introduce AbstentionBench, a
large-scale benchmark for holistically evaluating abstention across 20 diverse
datasets, including questions with unknown answers, underspecification, false
premises, subjective interpretations, and outdated information. Evaluating 20
frontier LLMs reveals abstention is an unsolved problem, and one where scaling
models is of little use. While recent reasoning LLMs have shown impressive
results in complex problem solving, surprisingly, we find that reasoning
fine-tuning degrades abstention (by $24\%$ on average), even for math and
science domains on which reasoning models are explicitly trained. We find that
while a carefully crafted system prompt can boost abstention in practice, it
does not resolve models' fundamental inability to reason about uncertainty. We
release AbstentionBench to foster research into advancing LLM reliability.

</details>


### [33] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/abs/2506.09049)
*Li Kang,Xiufeng Song,Heng Zhou,Yiran Qin,Jie Yang,Xiaohong Liu,Philip Torr,Lei Bai,Zhenfei Yin*

Main category: cs.AI

TL;DR: This paper introduces VIKI-Bench, a hierarchical benchmark for diverse embodied multi-agent cooperation, along with VIKI-R, a two-stage framework leveraging pretrained vision-language models for improved performance.


<details>
  <summary>Details</summary>
Motivation: The challenge is enabling effective coordination among embodied agents in dynamic environments, with scalable visual reasoning and cooperation strategies.

Method: The study introduces VIKI-Bench, a visual reasoning benchmark, and develops VIKI-R—a two-stage approach combining Chain-of-Thought fine-tuning and reinforcement learning under multi-level reward systems.

Result: VIKI-R significantly outperformed baseline methods across all benchmark levels, showcasing emergent cooperation patterns among heterogeneous agents.

Conclusion: VIKI-Bench and VIKI-R collectively advance visual-driven cooperation in embodied AI, bridging perception-based reasoning and embodied multi-agent problem solving.

Abstract: Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


### [34] [ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering](https://arxiv.org/abs/2506.09050)
*Yuki Imajuku,Kohki Horie,Yoichi Iwata,Kensho Aoki,Naohiro Takahashi,Takuya Akiba*

Main category: cs.AI

TL;DR: This paper introduces ALE-Bench, a new benchmark for evaluating AI systems on hard optimization problems with iterative, long-horizon solutions.


<details>
  <summary>Details</summary>
Motivation: To assess and enhance AI performance in algorithm engineering for challenging optimization tasks across various practical domains.

Method: The authors created ALE-Bench using tasks from AtCoder Heuristic Contests, which features computationally hard problems and iterative solution refinement. A framework supports interactive agent architectures and feedback mechanisms for evaluation.

Result: Frontier LLMs perform well on specific tasks but lag behind humans in consistency and long-term problem-solving ability.

Conclusion: The benchmark is critical for fostering the development of AI systems with improved consistency and long-horizon capabilities for optimization problems.

Abstract: How well do AI systems perform in algorithm engineering for hard optimization
problems in domains such as package-delivery routing, crew scheduling, factory
production planning, and power-grid balancing? We introduce ALE-Bench, a new
benchmark for evaluating AI systems on score-based algorithmic programming
contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench
presents optimization problems that are computationally hard and admit no known
exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench
encourages iterative solution refinement over long time horizons. Our software
framework supports interactive agent architectures that leverage test-run
feedback and visualizations. Our evaluation of frontier LLMs revealed that
while they demonstrate high performance on specific problems, a notable gap
remains compared to humans in terms of consistency across problems and
long-horizon problem-solving capabilities. This highlights the need for this
benchmark to foster future AI advancements.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [35] [ABC-FHE : A Resource-Efficient Accelerator Enabling Bootstrappable Parameters for Client-Side Fully Homomorphic Encryption](https://arxiv.org/abs/2506.08461)
*Sungwoong Yune,Hyojeong Lee,Adiwena Putra,Hyunjun Cho,Cuong Duong Manh,Jaeho Jeon,Joo-Young Kim*

Main category: cs.AR

TL;DR: The paper introduces ABC-FHE, a client-side homomorphic encryption accelerator addressing computational overhead, achieving major speed-ups in performance for encoding, encryption, decoding, and decryption compared to CPUs and previous accelerators.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the bottleneck of high computational overhead in fully homomorphic encryption (FHE), particularly on client-side tasks like encoding, encrypting, decoding, and decrypting large-sized parameters.

Method: The paper proposes ABC-FHE, an efficient FHE accelerator employing a streaming architecture, a reconfigurable Fourier engine for NTT and FFT modes, on-chip pseudo-random number generation, unified twiddle factor generation, and optimized task scheduling.

Result: The proposed ABC-FHE achieves significant performance gains, including a 1112x speed-up in encoding/encryption time compared to CPUs and 214x over previous accelerators, along with a 963x and 82x speed-up in decoding/decryption respectively.

Conclusion: ABC-FHE demonstrates major advancements in client-side FHE processing, offering high efficiency and performance improvements, thereby addressing a critical adoption challenge of computational overhead in FHE systems.

Abstract: As the demand for privacy-preserving computation continues to grow, fully
homomorphic encryption (FHE)-which enables continuous computation on encrypted
data-has become a critical solution. However, its adoption is hindered by
significant computational overhead, requiring 10000-fold more computation
compared to plaintext processing. Recent advancements in FHE accelerators have
successfully improved server-side performance, but client-side computations
remain a bottleneck, particularly under bootstrappable parameter
configurations, which involve combinations of encoding, encrypt, decoding, and
decrypt for large-sized parameters. To address this challenge, we propose
ABC-FHE, an area- and power-efficient FHE accelerator that supports
bootstrappable parameters on the client side. ABC-FHE employs a streaming
architecture to maximize performance density, minimize area usage, and reduce
off-chip memory access. Key innovations include a reconfigurable Fourier engine
capable of switching between NTT and FFT modes. Additionally, an on-chip
pseudo-random number generator and a unified on-the-fly twiddle factor
generator significantly reduce memory demands, while optimized task scheduling
enhances the CKKS client-side processing, achieving reduced latency. Overall,
ABC-FHE occupies a die area of 28.638 mm2 and consumes 5.654 W of power in 28
nm technology. It delivers significant performance improvements, achieving a
1112x speed-up in encoding and encryption execution time compared to a CPU, and
214x over the state-of-the-art client-side accelerator. For decoding and
decryption, it achieves a 963x speed-up over the CPU and 82x over the
state-of-the-art accelerator.

</details>


### [36] [CoQMoE: Co-Designed Quantization and Computation Orchestration for Mixture-of-Experts Vision Transformer on FPGA](https://arxiv.org/abs/2506.08496)
*Jiale Dong,Hao Wu,Zihao Wang,Wenqi Lou,Zhendong Zheng,Lei Gong,Chao Wang,Xuehai Zhou*

Main category: cs.AR

TL;DR: This paper introduces an efficient FPGA accelerator for running quantized Mixture-of-Experts Vision Transformers, achieving high throughput and significant energy savings with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of running Vision Transformers (ViTs) efficiently on resource-constrained devices like FPGAs, particularly since ViTs have high computational and memory demands.

Method: The authors propose a dual-stage quantization scheme combining complex and simplified quantizers, and a resource-aware accelerator architecture featuring optimized attention kernels and reusable linear operators.

Result: The proposed FPGA accelerator achieves nearly 155 frames per second, a 5.35× improvement in throughput, and over 80% energy reduction compared to state-of-the-art FPGA MoE accelerators, with less than 1% accuracy loss.

Conclusion: The work demonstrates that quantized MoE-ViTs can be efficiently deployed on FPGAs with hardware-conscious innovations, offering strong performance and energy efficiency while maintaining accuracy.

Abstract: Vision Transformers (ViTs) exhibit superior performance in computer vision
tasks but face deployment challenges on resource-constrained devices due to
high computational/memory demands. While Mixture-of-Experts Vision Transformers
(MoE-ViTs) mitigate this through a scalable architecture with sub-linear
computational growth, their hardware implementation on FPGAs remains
constrained by resource limitations. This paper proposes a novel accelerator
for efficiently implementing quantized MoE models on FPGAs through two key
innovations: (1) A dual-stage quantization scheme combining
precision-preserving complex quantizers with hardware-friendly simplified
quantizers via scale reparameterization, with only 0.28 $\%$ accuracy loss
compared to full precision; (2) A resource-aware accelerator architecture
featuring latency-optimized streaming attention kernels and reusable linear
operators, effectively balancing performance and resource consumption.
Experimental results demonstrate that our accelerator achieves nearly 155
frames per second, a 5.35$\times$ improvement in throughput, and over $80\%$
energy reduction compared to state-of-the-art (SOTA) FPGA MoE accelerators,
while maintaining $<1\%$ accuracy loss across vision benchmarks. Our
implementation is available at https://github.com/DJ000011/CoQMoE.

</details>


### [37] [POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration](https://arxiv.org/abs/2506.08785)
*Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: PARV-CE is a multi-precision AI hardware engine designed for energy-efficient edge computing, with support for various AI model formats and workloads.


<details>
  <summary>Details</summary>
Motivation: To address the need for flexible, energy-efficient hardware that supports diverse precision formats for modern AI models, especially on edge platforms.

Method: PARV-CE uses a unified SIMD-enabled data-path for multiple precision formats, layer adaptive precision strategies, and quantization-aware execution integrated with reconfigurable pipelines for efficient hardware-software co-design.

Result: PARV-CE achieved significant gains, including up to 2x improvement in power-delay product, 3x reduction in resource usage compared to state-of-the-art designs, with accuracy loss within 1.8% of FP32 baseline.

Conclusion: PARV-CE and its POLARON integration provide scalable, energy-efficient acceleration for precision-adaptive AI computations, suitable for diverse workloads like DNNs, RNNs, RL, and Transformers on edge devices.

Abstract: The increasing complexity of AI models requires flexible hardware capable of
supporting diverse precision formats, particularly for energy-constrained edge
platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC
engine that performs efficient multiply-accumulate operations using a unified
data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The
architecture incorporates a layer adaptive precision strategy to align
computational accuracy with workload sensitivity, optimizing both performance
and energy usage. PARV-CE integrates quantization-aware execution with a
reconfigurable SIMD pipeline, enabling high-throughput processing with minimal
overhead through hardware-software co-design. The results demonstrate up to 2x
improvement in PDP and 3x reduction in resource usage compared to SoTA designs,
while retaining accuracy within 1.8% FP32 baseline. The architecture supports
both on-device training and inference across a range of workloads, including
DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE
incorporated POLARON as a scalable and energy-efficient solution for
precision-adaptive AI acceleration at edge.

</details>


### [38] [STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN Accelerator with Algorithm and Hardware Co-Design](https://arxiv.org/abs/2506.08842)
*Kainan Wang,Chengyi Yang,Chengting Yu,Yee Sin Ang,Bo Wang,Aili Wang*

Main category: cs.AR

TL;DR: The paper introduces the STI-SNN accelerator, enhancing energy efficiency, flexibility, and low latency in Spiking Neural Networks (SNNs) with hardware and algorithm co-design.


<details>
  <summary>Details</summary>
Motivation: Improve Spiking Neural Networks' (SNNs) latency and energy efficiency, addressing challenges in hardware parallelism and data reuse due to the temporal nature of spikes.

Method: The STI-SNN accelerator uses a temporal pruning algorithm for single-timestep inference, output stationary (OS) dataflow for efficient data handling, spike representation for reduced memory access, and supports various convolution methods with flexible parallel processing.

Result: The STI-SNN accelerator provides high energy efficiency and flexibility suitable for resource-constrained applications, maintaining inference accuracy and enhancing data reuse.

Conclusion: STI-SNN successfully mitigates the temporal challenges in SNNs, offering a versatile and efficient solution for hardware acceleration in resource-limited environments.

Abstract: Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for
their event-driven characteristics and high energy efficiency. However, the
temporal dependency and irregularity of spikes present significant challenges
for hardware parallel processing and data reuse, leading to some existing
accelerators falling short in processing latency and energy efficiency. To
overcome these challenges, we introduce the STI-SNN accelerator, designed for
resource-constrained applications with high energy efficiency, flexibility, and
low latency. The accelerator is designed through algorithm and hardware
co-design. Firstly, STI-SNN can perform inference in a single timestep. At the
algorithm level, we introduce a temporal pruning approach based on the temporal
efficient training (TET) loss function. This approach alleviates spike
disappearance during timestep reduction, maintains inference accuracy, and
expands TET's application. In hardware design, we analyze data access patterns
and adopt the output stationary (OS) dataflow, eliminating the need to store
membrane potentials and access memory operations. Furthermore, based on the OS
dataflow, we propose a compressed and sorted representation of spikes, then
cached in the line buffer to reduce the memory access cost and improve reuse
efficiency. Secondly, STI-SNN supports different convolution methods. By
adjusting the computation mode of processing elements (PEs) and parameterizing
the computation array, STI-SNN can accommodate lightweight models based on
depthwise separable convolutions (DSCs), further enhancing hardware
flexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer
parallel processing. For inter-layer parallelism, we ...

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [39] [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/abs/2506.08120)
*Toyin Aguda,Erik Wilson,Allan Anzagira,Simerjot Kaur,Charese Smiley*

Main category: cs.CL

TL;DR: This paper studies the conservative bias in large language models (LLMs) during relation extraction tasks, where models often default to the No_Relation label, leading to information loss.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge posed by bias in LLMs that prioritizes conservative labeling to avoid errors while compromising valuable information in relation extraction tasks.

Method: The study uses systematic evaluation across multiple prompts, datasets, and relation types, while applying the concept of Hobson's choice and leveraging SBERT and LLM prompts to measure semantic similarity.

Result: The results highlight that conservative bias occurs twice as often as hallucinations, emphasizing the trade-off between safe labels and meaningful extraction.

Conclusion: LLMs' conservative bias aids in preventing incorrect judgments but sacrifices essential insights, urging a need for balanced approaches.

Abstract: Large language models (LLMs) exhibit pronounced conservative bias in relation
extraction tasks, frequently defaulting to No_Relation label when an
appropriate option is unavailable. While this behavior helps prevent incorrect
relation assignments, our analysis reveals that it also leads to significant
information loss when reasoning is not explicitly included in the output. We
systematically evaluate this trade-off across multiple prompts, datasets, and
relation types, introducing the concept of Hobson's choice to capture scenarios
where models opt for safe but uninformative labels over hallucinated ones. Our
findings suggest that conservative bias occurs twice as often as hallucination.
To quantify this effect, we use SBERT and LLM prompts to capture the semantic
similarity between conservative bias behaviors in constrained prompts and
labels generated from semi-constrained and open-ended prompts.

</details>


### [40] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
*Marek Kadlčík,Michal Štefánik,Timothee Mickus,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: This paper identifies flaws in pretrained language models' number embeddings and proposes a new probing technique to decode numeric values with high accuracy, addressing model errors in arithmetic.


<details>
  <summary>Details</summary>
Motivation: Pretrained language models struggle with arithmetic tasks due to unreliable embeddings for exact numeric values, as observed with previous probing methods.

Method: A novel probing technique is introduced to decode numeric values from input embeddings with near-perfect accuracy and analyze emergent sinusoidal patterns.

Result: The proposed probe demonstrates precise number representation in embeddings and explains many LM arithmetic errors. Aligning embeddings with the probe's pattern reduces these errors.

Conclusion: Pretrained language models can represent numbers precisely, and aligning embeddings using this probe can mitigate arithmetic errors effectively.

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


### [41] [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
*Jacob Dineen,Aswin RRV,Qin Liu,Zhikun Xu,Xiao Ye,Ming Shen,Zhaonan Li,Shijie Lu,Chitta Baral,Muhao Chen,Ben Zhou*

Main category: cs.CL

TL;DR: This paper introduces QA-LIGN, a method for automatic symbolic reward decomposition aimed at improving interpretability in the alignment of large language models with constitutional principles.


<details>
  <summary>Details</summary>
Motivation: Standard reward-based alignment methods for large language models often obscure objectives by collapsing feedback into a single scalar reward. The paper aims to improve alignment interpretability without compromising model performance.

Method: QA-LIGN employs principle-specific evaluation questions to derive separate reward components for each constitutional principle, replacing the traditional black-box reward model with a structured decomposition approach.

Result: QA-LIGN improves transparency and adaptability in aligning language models while maintaining or exceeding the performance of a DPO baseline.

Conclusion: QA-LIGN represents a step forward in creating more interpretable and controllable alignment mechanisms for language models, maintaining high end-task performance.

Abstract: Alignment of large language models with explicit principles (such as
helpfulness, honesty, and harmlessness) is crucial for ensuring safe and
reliable AI systems. However, standard reward-based alignment methods typically
collapse diverse feedback into a single scalar reward, entangling multiple
objectives into one opaque training signal, which hinders interpretability. In
this work, we introduce QA-LIGN, an automatic symbolic reward decomposition
approach that preserves the structure of each constitutional principle within
the reward mechanism. Instead of training a black-box reward model that outputs
a monolithic score, QA-LIGN formulates principle-specific evaluation questions
and derives separate reward components for each principle, making it a drop-in
reward model replacement. Experiments aligning an uncensored large language
model with a set of constitutional principles demonstrate that QA-LIGN offers
greater transparency and adaptability in the alignment process. At the same
time, our approach achieves performance on par with or better than a DPO
baseline. Overall, these results represent a step toward more interpretable and
controllable alignment of language models, achieved without sacrificing
end-task performance.

</details>


### [42] [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)
*Zefang Liu,Yinzhu Quan*

Main category: cs.CL

TL;DR: EconWebArena is a new benchmark for testing autonomous agents on economic tasks using web environments, featuring 360 curated tasks across multiple domains.


<details>
  <summary>Details</summary>
Motivation: To create a rigorous and realistic testbed for evaluating autonomous agents on web-based economic reasoning tasks using authoritative data and complex interfaces.

Method: Developed a benchmark by generating tasks using large language models, followed by human curation to ensure reliability, and evaluated multimodal LLMs on diverse tasks.

Result: Identified substantial performance gaps in agents' abilities to handle grounding, navigation, and multimodal understanding via evaluation, ablation studies, and failure case analysis.

Conclusion: EconWebArena serves as a robust evaluation framework, exposing key challenges in web-based economic reasoning for autonomous agents.

Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on
complex, multimodal economic tasks in realistic web environments. The benchmark
comprises 360 curated tasks from 82 authoritative websites spanning domains
such as macroeconomics, labor, finance, trade, and public policy. Each task
challenges agents to navigate live websites, interpret structured and visual
content, interact with real interfaces, and extract precise, time-sensitive
data through multi-step workflows. We construct the benchmark by prompting
multiple large language models (LLMs) to generate candidate tasks, followed by
rigorous human curation to ensure clarity, feasibility, and source reliability.
Unlike prior work, EconWebArena emphasizes fidelity to authoritative data
sources and the need for grounded web-based economic reasoning. We evaluate a
diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure
cases, and conduct ablation studies to assess the impact of visual grounding,
plan-based reasoning, and interaction design. Our results reveal substantial
performance gaps and highlight persistent challenges in grounding, navigation,
and multimodal understanding, positioning EconWebArena as a rigorous testbed
for economic web intelligence.

</details>


### [43] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
*Muhammad Usman,Muhammad Ahmad,M. Shahiki Tash,Irina Gelbukh,Rolando Quintero Tellez,Grigori Sidorov*

Main category: cs.CL

TL;DR: This paper introduces a trilingual dataset for hate speech detection in English, Urdu, and Spanish, and benchmarks it with both transformer-based and non-transformer models, achieving strong performance, especially improving results for underexplored Urdu.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to address the lack of research and resources for hate speech detection in underexplored languages, specifically Urdu, while ensuring online communities remain safe and inclusive.

Method: The authors created a trilingual dataset of 10,193 tweets labeled as Hateful or Not-Hateful. They utilized attention layers with transformer-based models and large language models (LLMs) like GPT-3.5 Turbo and Qwen 2.5 72B, and also employed TF-IDF for traditional machine learning models. Rigorous annotation achieved high quality, measured by a Fleiss' Kappa of 0.821.

Result: The proposed approach, combining attention layers and state-of-the-art models, delivered macro F1 scores of 0.87 (English), 0.85 (Spanish), 0.81 (Urdu), and 0.88 (joint multilingual). These results improved significantly over the SVM baseline, showing 5% to 9% enhancements depending on the language.

Conclusion: The study presents a robust framework for multilingual hate speech detection, demonstrating the efficacy of attention layers and advanced models. It significantly improves detection accuracy for underexplored languages like Urdu, promoting safer online environments.

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [44] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
*Chupei Wang,Jiaqiu Vince Sun*

Main category: cs.CL

TL;DR: This paper investigates the issue of information interference in Large Language Models (LLMs), which leads to declining performance during sequential information retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of proactive interference (PI), a phenomenon from cognitive science, on LLMs' retrieval abilities when dealing with sequentially updated information.

Method: The authors designed the PI-LLM evaluation framework, where semantically related key-value updates are streamed, and LLMs are tested on retrieving the final values while measuring retrieval performance under interference.

Result: LLMs' retrieval accuracy declines log-linearly as interference accumulates, with common errors involving retrieval of overwritten values. Attempts at mitigation, like prompt engineering, showed limited improvement.

Conclusion: LLMs face a working memory bottleneck that affects their ability to handle interference during retrieval, necessitating development of methods to better suppress irrelevant content for improved performance.

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [45] [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2506.08158)
*Lijing Zhu,Qizhen Lan,Qing Tian,Wenbo Sun,Li Yang,Lu Xia,Yixin Xie,Xi Xiao,Tiehang Duan,Cui Tao,Shuteng Niu*

Main category: cs.CL

TL;DR: The paper introduces ETT-CKGE, a method designed to improve Continual Knowledge Graph Embedding by enhancing efficiency and scalability through task-driven tokens.


<details>
  <summary>Details</summary>
Motivation: Existing CKGE methods struggle with inefficiency and scalability, as they rely on manual node/relation importance scores and computationally expensive graph traversal, both of which fail to adequately preserve knowledge and are resource-intensive.

Method: ETT-CKGE employs task-driven, learnable tokens to effectively transfer task-relevant knowledge between graph snapshots without explicit node scoring or traversal. Token-masked embedding alignment and simple matrix operations are used to simplify the process.

Result: ETT-CKGE achieves superior or competitive performance on six benchmark datasets while significantly reducing training time and memory requirements compared to state-of-the-art methods.

Conclusion: ETT-CKGE alleviates the limitations of traditional CKGE approaches by offering a scalable, efficient, and effective solution for preserving and transferring knowledge, with extensive experimental validation supporting its advantages.

Abstract: Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge
while preserving past information. However, existing methods struggle with
efficiency and scalability due to two key limitations: (1) suboptimal knowledge
preservation between snapshots caused by manually designed node/relation
importance scores that ignore graph dependencies relevant to the downstream
task, and (2) computationally expensive graph traversal for node/relation
importance calculation, leading to slow training and high memory overhead. To
address these limitations, we introduce ETT-CKGE (Efficient, Task-driven,
Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE
method that leverages efficient task-driven tokens for efficient and effective
knowledge transfer between snapshots. Our method introduces a set of learnable
tokens that directly capture task-relevant signals, eliminating the need for
explicit node scoring or traversal. These tokens serve as consistent and
reusable guidance across snapshots, enabling efficient token-masked embedding
alignment between snapshots. Importantly, knowledge transfer is achieved
through simple matrix operations, significantly reducing training time and
memory usage. Extensive experiments across six benchmark datasets demonstrate
that ETT-CKGE consistently achieves superior or competitive predictive
performance, while substantially improving training efficiency and scalability
compared to state-of-the-art CKGE methods. The code is available at:
https://github.com/lijingzhu1/ETT-CKGE/tree/main

</details>


### [46] [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/abs/2506.08172)
*Gerardo Aleman Manzanarez,Nora de la Cruz Arana,Jorge Garcia Flores,Yobany Garcia Medina,Raul Monroy,Nathalie Pernelle*

Main category: cs.CL

TL;DR: This paper introduces GrAImes, a protocol for evaluating AI-generated microfiction in terms of literary criteria like thematic coherence and aesthetic quality.


<details>
  <summary>Details</summary>
Motivation: While AI has advanced in generating coherent short fiction, evaluation of such works for literary merit, especially aesthetic qualities, remains underexplored.

Method: Development of GrAImes, an evaluation protocol grounded in literary theory to objectively assess AI-generated microfiction. Validation involved literature experts and enthusiasts.

Result: GrAImes was effectively validated, providing a structured approach to objectively evaluate microfiction by AI.

Conclusion: The GrAImes protocol lays a foundational framework for assessing the literary merit of AI-generated microfiction systematically.

Abstract: Automated story writing has been a subject of study for over 60 years. Large
language models can generate narratively consistent and linguistically coherent
short fiction texts. Despite these advancements, rigorous assessment of such
outputs for literary merit - especially concerning aesthetic qualities - has
received scant attention. In this paper, we address the challenge of evaluating
AI-generated microfictions and argue that this task requires consideration of
literary criteria across various aspects of the text, such as thematic
coherence, textual clarity, interpretive depth, and aesthetic quality. To
facilitate this, we present GrAImes: an evaluation protocol grounded in
literary theory, specifically drawing from a literary perspective, to offer an
objective framework for assessing AI-generated microfiction. Furthermore, we
report the results of our validation of the evaluation protocol, as answered by
both literature experts and literary enthusiasts. This protocol will serve as a
foundation for evaluating automatically generated microfictions and assessing
their literary value.

</details>


### [47] [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/abs/2506.08174)
*Li Weigang,Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: The paper proposes LLM-BT, a back-translation framework using large language models (LLMs) for multilingual terminology standardization and validation via cross-lingual semantic alignment.


<details>
  <summary>Details</summary>
Motivation: The manual expert-driven process for standardizing technical terms across languages is inefficient and inconsistent, especially in fast-evolving areas like AI and quantum computing.

Method: The LLM-BT framework utilizes back-translation workflows in both serial and parallel forms, combining these with a Retrieve--Generate--Verify--Optimize pipeline. Cross-validation metrics like BLEU scores and semantic alignment are employed to measure performance and robustness.

Result: LLM-BT achieves over 90% semantic or exact term consistency and demonstrates strong cross-lingual performance (BLEU > 0.45; 100% term accuracy in Portuguese). It also conceptualizes back-translation as a form of dynamic semantic embedding, providing transparent trajectories of meaning.

Conclusion: The LLM-BT framework acts as a systematic infrastructure for human-AI collaboration in multilingual terminology standardization, ensuring semantic accuracy while allowing for cultural interpretation.

Abstract: The rapid growth of English technical terms challenges traditional
expert-driven standardization, especially in fast-evolving fields like AI and
quantum computing. Manual methods struggle to ensure multilingual consistency.
We propose \textbf{LLM-BT}, a back-translation framework powered by large
language models (LLMs) to automate terminology verification and standardization
via cross-lingual semantic alignment. Our contributions are: \textbf{(1)
Term-Level Consistency Validation:} Using English $\rightarrow$ intermediate
language $\rightarrow$ English back-translation, LLM-BT achieves high term
consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies
showing over 90\% exact or semantic matches. \textbf{(2) Multi-Path
Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''
pipeline integrates serial (e.g., EN $\rightarrow$ ZHcn $\rightarrow$ ZHtw
$\rightarrow$ EN) and parallel (e.g., EN $\rightarrow$ Chinese/Portuguese
$\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong
cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\%).
\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as
dynamic semantic embedding, revealing latent meaning trajectories. Unlike
static embeddings, LLM-BT provides transparent path-based embeddings shaped by
model evolution. LLM-BT transforms back-translation into an active engine for
multilingual terminology standardization, enabling human--AI collaboration:
machines ensure semantic fidelity, humans guide cultural interpretation. This
infrastructure supports terminology governance across scientific and
technological fields worldwide.

</details>


### [48] ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/abs/2506.08221)
*Samra Zafar,Shaheer Minhas,Syed Ali Hassan Zaidi,Arfa Naeem,Zahra Ali*

Main category: cs.CL

TL;DR: The study explores using keystroke data and writing evolution snapshots to make LLM feedback more process-aware, showing that students preferred this feedback.


<details>
  <summary>Details</summary>
Motivation: Current large language models typically only consider the final essay for feedback, lacking insights into the writing process, which could improve feedback relevancy.

Method: A digital writing tool that logs keystrokes and snapshots was created. Twenty students wrote essays with this tool, and feedback was provided by LLMs informed by both the final product and the entire writing process. Students also completed surveys about their experience.

Result: Students found process-aware feedback more useful and relatable. Specific types of edits, like adding content or reorganizing text, correlated with higher scores in coherence and elaboration.

Conclusion: Incorporating writing process data can improve LLMs' ability to provide more meaningful, personalized, and supportive feedback.

Abstract: Large language models(LLMs) like Gemini are becoming common tools for
supporting student writing. But most of their feedback is based only on the
final essay missing important context about how that text was written. In this
paper, we explore whether using writing process data, collected through
keystroke logging and periodic snapshots, can help LLMs give feedback that
better reflects how learners think and revise while writing. We built a digital
writing tool that captures both what students type and how their essays evolve
over time. Twenty students used this tool to write timed essays, which were
then evaluated in two ways: (i) LLM generated feedback using both the final
essay and the full writing trace, and (ii) After the task, students completed
surveys about how useful and relatable they found the feedback. Early results
show that learners preferred the process-aware LLM feedback, finding it more in
tune with their own thinking. We also found that certain types of edits, like
adding new content or reorganizing paragraphs, aligned closely with higher
scores in areas like coherence and elaboration. Our findings suggest that
making LLMs more aware of the writing process can lead to feedback that feels
more meaningful, personal, and supportive.

</details>


### [49] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
*Yu-Ang Lee,Guan-Ting Yi,Mei-Yi Liu,Jui-Chao Lu,Guan-Bo Yang,Yun-Nung Chen*

Main category: cs.CL

TL;DR: The paper reviews the optimization strategies for complex compound AI systems, exploring both numerical and natural language feedback-based techniques, and outlines challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The paper targets the emerging complexities in optimizing AI systems composed of multiple integrated components, especially with the rise of language-based feedback.

Method: A systematic review and classification of existing optimization techniques for compound AI systems, focusing on numerical and language-based methods.

Result: The study provides a structured understanding of optimization strategies and identifies key challenges and areas for potential research in compound AI system design.

Conclusion: Optimizing compound AI systems requires innovative methods that bridge traditional numerical techniques with modern language-based approaches, paving the way for advancements in this domain.

Abstract: Recent advancements in large language models (LLMs) and AI systems have led
to a paradigm shift in the design and optimization of complex AI workflows. By
integrating multiple components, compound AI systems have become increasingly
adept at performing sophisticated tasks. However, as these systems grow in
complexity, new challenges arise in optimizing not only individual components
but also their interactions. While traditional optimization methods such as
supervised fine-tuning (SFT) and reinforcement learning (RL) remain
foundational, the rise of natural language feedback introduces promising new
approaches, especially for optimizing non-differentiable systems. This paper
provides a systematic review of recent progress in optimizing compound AI
systems, encompassing both numerical and language-based techniques. We
formalize the notion of compound AI system optimization, classify existing
methods along several key dimensions, and highlight open research challenges
and future directions in this rapidly evolving field. A list of surveyed papers
is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [50] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
*Shashidhar Reddy Javaji,Yupeng Cao,Haohang Li,Yangyang Yu,Nikhil Muralidhar,Zining Zhu*

Main category: cs.CL

TL;DR: This study introduces CLAIM-BENCH, a benchmark for assessing large language models (LLMs) in extracting and validating scientific claims and evidence, revealing notable performance disparities among models.


<details>
  <summary>Details</summary>
Motivation: Investigate the ability of LLMs to deeply comprehend scientific argumentation, particularly the relationships between claims and supporting evidence, which remains largely unexplored.

Method: Researchers developed CLAIM-BENCH, systematically tested six diverse LLMs on claim-evidence extraction tasks, and assessed strategies like three-pass and one-by-one prompting approaches.

Result: Closed-source models such as GPT-4 and Claude showed superior performance in precision and recall compared to open-source ones, while specific prompting strategies improved linking accuracy at higher computational costs.

Conclusion: CLAIM-BENCH establishes a benchmark for evaluating scientific understanding in LLMs, providing insights for improving their reasoning capabilities in handling complex research papers.

Abstract: Large language models (LLMs) are increasingly being used for complex research
tasks such as literature review, idea generation, and scientific paper
analysis, yet their ability to truly understand and process the intricate
relationships within complex research papers, such as the logical links between
claims and supporting evidence remains largely unexplored. In this study, we
present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'
capabilities in scientific claim-evidence extraction and validation, a task
that reflects deeper comprehension of scientific argumentation. We
systematically compare three approaches which are inspired by divide and
conquer approaches, across six diverse LLMs, highlighting model-specific
strengths and weaknesses in scientific comprehension. Through evaluation
involving over 300 claim-evidence pairs across multiple research domains, we
reveal significant limitations in LLMs' ability to process complex scientific
content. Our results demonstrate that closed-source models like GPT-4 and
Claude consistently outperform open-source counterparts in precision and recall
across claim-evidence identification tasks. Furthermore, strategically designed
three-pass and one-by-one prompting approaches significantly improve LLMs'
abilities to accurately link dispersed evidence with claims, although this
comes at increased computational cost. CLAIM-BENCH sets a new standard for
evaluating scientific comprehension in LLMs, offering both a diagnostic tool
and a path forward for building systems capable of deeper, more reliable
reasoning across full-length papers.

</details>


### [51] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
*Wanjing Anya Ma,Michael Flor,Zuowei Wang*

Main category: cs.CL

TL;DR: The paper explores using GPT-4 to generate reading comprehension (RC) questions targeting inference-making skills and evaluates the outcomes for quality and suitability.


<details>
  <summary>Details</summary>
Motivation: Improving diagnostic RC tools for better reading interventions and instruction through automatic question generation.

Method: Introduced a taxonomy for inference types, used GPT-4 for few-shot prompting to generate bridging-inference RC items, and evaluated the questions’ quality and accuracy.

Result: 93.8% of questions were of good quality, but only 42.6% accurately matched the targeted inference type. High inter-rater agreement (>0.90) was reported.

Conclusion: Combining automatic item generation with human judgment can enhance scalable and high-quality diagnostic RC assessments.

Abstract: Inference making is an essential but complex skill in reading comprehension
(RC). Some inferences require resolving references across sentences, and some
rely on using prior knowledge to fill in the detail that is not explicitly
written in the text. Diagnostic RC questions can help educators provide more
effective and targeted reading instruction and interventions for school-age
students. We introduce a taxonomy of inference types for RC and use it to
analyze the distribution of items within a diagnostic RC item bank. Next, we
present experiments using GPT-4o to generate bridging-inference RC items for
given reading passages via few-shot prompting, comparing conditions with and
without chain-of-thought prompts. Generated items were evaluated on three
aspects: overall item quality, appropriate inference type, and LLM reasoning,
achieving high inter-rater agreements above 0.90. Our results show that GPT-4o
produced 93.8% good-quality questions suitable for operational use in grade
3-12 contexts; however, only 42.6% of the generated questions accurately
matched the targeted inference type. We conclude that combining automatic item
generation with human judgment offers a promising path toward scalable,
high-quality diagnostic RC assessments.

</details>


### [52] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernán Maina,Nicolás Wolovick,Luciana Benotti*

Main category: cs.CL

TL;DR: The paper analyzes how numerical precisions and data parallelization strategies affect training speed and model accuracy to enable energy-efficient domain adaptation for LLMs in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: The training of Large Language Models (LLMs) is resource-intensive, both computationally and in terms of energy and annotated data, which limits accessibility and diversity in cultural alignment.

Method: The authors systematically evaluate different numerical precisions and data parallelization strategies as optimization methods to reduce computational costs while preserving model accuracy.

Result: These strategies can improve training efficiency and lower resource consumption without compromising model performance, aiding domain adaptation in low-resource settings.

Conclusion: Optimized numerical and parallelization methods can make LLM domain adaptation more accessible and sustainable, particularly for groups with limited infrastructure.

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [53] [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300)
*Matteo Cargnelutti,Catherine Brobston,John Hess,Jack Cushman,Kristi Mukk,Aristana Scourtas,Kyle Courtney,Greg Leppert,Amanda Watson,Martha Whitehead,Jonathan Zittrain*

Main category: cs.CL

TL;DR: The technical report introduces Institutional Books 1.0, a dataset of 983,004 public domain books, originally digitized by Harvard Library, to enhance the quality and sustainability of language model training datasets.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement and reliance on LLMs have revealed a critical shortage of high-quality, publicly-available datasets that come with clear provenance and sustainable stewardship.

Method: The dataset, based on Harvard Library’s Google Books digitization project, underwent extraction, analysis, and processing to produce a well-documented collection of public domain texts and metadata comprising 242 billion tokens from 983,004 volumes.

Result: A processed and extensively-documented dataset of public domain books was created, including both OCR-extracted text (original and post-processed) and detailed metadata.

Conclusion: Institutional Books 1.0 provides a valuable, publicly-accessible resource to support the development of high-quality language models, grounded in sustainable data stewardship practices.

Abstract: Large language models (LLMs) use data to learn about the world in order to
produce meaningful correlations and predictions. As such, the nature, scale,
quality, and diversity of the datasets used to train these models, or to
support their work at inference time, have a direct impact on their quality.
The rapid development and adoption of LLMs of varying quality has brought into
focus the scarcity of publicly available, high-quality training data and
revealed an urgent need to ground the stewardship of these datasets in
sustainable practices with clear provenance chains. To that end, this technical
report introduces Institutional Books 1.0, a large collection of public domain
books originally digitized through Harvard Library's participation in the
Google Books project, beginning in 2006. Working with Harvard Library, we
extracted, analyzed, and processed these volumes into an extensively-documented
dataset of historic texts. This analysis covers the entirety of Harvard
Library's collection scanned as part of that project, originally spanning
1,075,899 volumes written in over 250 different languages for a total of
approximately 250 billion tokens. As part of this initial release, the
OCR-extracted text (original and post-processed) as well as the metadata
(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,
identified as being in the public domain have been made available. This report
describes this project's goals and methods as well as the results of the
analyses we performed, all in service of making this historical collection more
accessible and easier for humans and machines alike to filter, read and use.

</details>


### [54] [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
*Chenlong Wang,Yuanning Feng,Dongping Chen,Zhaoyang Chu,Ranjay Krishna,Tianyi Zhou*

Main category: cs.CL

TL;DR: The study introduces NoWait, a technique to suppress self-reflective tokens, reducing redundancy in reasoning models without impacting their performance.


<details>
  <summary>Details</summary>
Motivation: Overthinking in reasoning models often leads to verbose outputs, reducing their efficiency.

Method: The approach disables explicit self-reflection by suppressing specific tokens like 'Wait' and 'Hmm' during inference.

Result: NoWait achieves up to 27%-51% reduction in chain-of-thought length without compromising the utility of reasoning models.

Conclusion: NoWait is a practical, plug-and-play solution to enhance efficiency in multimodal reasoning tasks.

Abstract: Recent advances in large reasoning models have enabled complex, step-by-step
reasoning but often introduce significant overthinking, resulting in verbose
and redundant outputs that hinder efficiency. In this study, we examine whether
explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is
necessary for advanced reasoning. We propose NoWait, a simple yet effective
approach that disables explicit self-reflection by suppressing these tokens
during inference. Extensive experiments on ten benchmarks across textual,
visual, and video reasoning tasks show that NoWait reduces chain-of-thought
trajectory length by up to 27%-51% in five R1-style model series, without
compromising model utility. NoWait thus offers a plug-and-play solution for
efficient and utility-preserving multimodal reasoning.

</details>


### [55] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
*Yuxuan Zhou,Xien Liu,Chenwei Yan,Chen Ning,Xiao Zhang,Boxun Li,Xiangling Fu,Shijin Wang,Guoping Hu,Yu Wang,Ji Wu*

Main category: cs.CL

TL;DR: The paper proposes a framework to evaluate large language models’ (LLMs) performance in the medical domain across three cognitive levels, revealing performance declines at higher cognitive complexities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore and understand LLMs' capabilities in the medical domain across different cognitive levels, as previous benchmarks focus on general performance without assessing cognitive complexity.

Method: The authors developed an evaluation framework inspired by Bloom's Taxonomy, integrating existing datasets and introducing tasks for three cognitive levels: knowledge grasp, knowledge application, and problem solving. They tested six LLM families.

Result: The research demonstrates a significant performance drop in models as tasks require higher cognitive complexity, with model size being more influential at these advanced levels.

Conclusion: The study emphasizes the need to enhance LLMs' capabilities at higher cognitive levels for applicability in real-world medical settings and offers directions for future LLM improvements.

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
various medical benchmarks, but their capabilities across different cognitive
levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a
multi-cognitive-level evaluation framework for assessing LLMs in the medical
domain in this study. The framework integrates existing medical datasets and
introduces tasks targeting three cognitive levels: preliminary knowledge grasp,
comprehensive knowledge application, and scenario-based problem solving. Using
this framework, we systematically evaluate state-of-the-art general and medical
LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.
Our findings reveal a significant performance decline as cognitive complexity
increases across evaluated models, with model size playing a more critical role
in performance at higher cognitive levels. Our study highlights the need to
enhance LLMs' medical capabilities at higher cognitive levels and provides
insights for developing LLMs suited to real-world medical applications.

</details>


### [56] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
*Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu*

Main category: cs.CL

TL;DR: This paper advocates for advancing text embedding research to focus on implicit semantics instead of surface-level semantics.


<details>
  <summary>Details</summary>
Motivation: Current text embedding models are foundational in NLP but focus narrowly on surface-level semantics, neglecting implicit aspects of meaning shaped by pragmatics, speaker intent, and sociocultural context.

Method: The researchers conducted a pilot study showing that state-of-the-art models perform only slightly better than simple baselines for tasks involving implicit semantics.

Result: Results demonstrated that current models struggle with tasks requiring interpretive reasoning, speaker stance, or social meaning, revealing significant limitations.

Conclusion: The paper proposes a paradigm shift to prioritize diverse training data, design benchmarks for deeper semantic understanding, and emphasize implicit meaning as a core goal to enhance alignment of embeddings with real-world language complexity.

Abstract: This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.

</details>


### [57] [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/abs/2506.08359)
*Li-Ming Zhan,Bo Liu,Zexin Lu,Chengqiang Xie,Jiannong Cao,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: The paper introduces a causal-attribution framework to identify behavior-relevant attention heads in transformers for inference-time steering of LLMs, achieving improved truthfulness-steering capabilities.


<details>
  <summary>Details</summary>
Motivation: Current strategies to modify LLMs' response characteristics at inference time often rely on superficial methods, leading to suboptimal outcomes. There is a need for a principled way to identify internal modules, like attention heads, that influence specific behaviors.

Method: The authors propose training a vector-quantized autoencoder (VQ-AE) on attention activations to partition latent spaces into behavior-relevant and irrelevant subspaces. They evaluate behavioral relevance based on encodings' separability for correct versus incorrect responses, giving each attention head a relevance score.

Result: The proposed method shows superior inference-time intervention performance on truthfulness-steering tasks across seven LLMs and five datasets. Additionally, selected heads exhibit robust zero-shot generalization in new truthfulness-related contexts.

Conclusion: The causal-attribution framework enables more targeted and effective interventions in LLM behavior, outperforming existing approaches and supporting generalizability in cross-domain applications.

Abstract: Inference-time steering aims to alter the response characteristics of large
language models (LLMs) without modifying their underlying parameters. A
critical step in this process is the identification of internal modules within
LLMs that are associated with the target behavior. However, current approaches
to module selection often depend on superficial cues or ad-hoc heuristics,
which can result in suboptimal or unintended outcomes. In this work, we propose
a principled causal-attribution framework for identifying behavior-relevant
attention heads in transformers. For each head, we train a vector-quantized
autoencoder (VQ-AE) on its attention activations, partitioning the latent space
into behavior-relevant and behavior-irrelevant subspaces, each quantized with a
shared learnable codebook. We assess the behavioral relevance of each head by
quantifying the separability of VQ-AE encodings for behavior-aligned versus
behavior-violating responses using a binary classification metric. This yields
a behavioral relevance score that reflects each head discriminative capacity
with respect to the target behavior, guiding both selection and importance
weighting. Experiments on seven LLMs from two model families and five
behavioral steering datasets demonstrate that our method enables more accurate
inference-time interventions, achieving superior performance on the
truthfulness-steering task. Furthermore, the heads selected by our approach
exhibit strong zero-shot generalization in cross-domain truthfulness-steering
scenarios.

</details>


### [58] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
*Fariz Ikhwantri,Dusica Marijan*

Main category: cs.CL

TL;DR: The paper introduces EXCLAIM, a Natural Language Inference-based method for detecting regulatory compliance, tackling challenges like legal complexity, assurance case scarcity, and explaining compliance decisions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in verifying regulatory compliance in complex systems, including the complexity of assurance cases and limited accessible data.

Method: EXCLAIM relies on Natural Language Inference, establishes multi-hop reasoning for assurance cases, and uses Large Language Models (LLMs) for generating assurance cases.

Result: EXCLAIM is effective in demonstrating compliance detection when tested on GDPR requirements through multi-hop inference tasks.

Conclusion: The research underscores the promise of NLI approaches in automating regulatory compliance with enhanced explainability and traceability.

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [59] [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/abs/2506.08364)
*Jash Rajesh Parekh,Pengcheng Jiang,Jiawei Han*

Main category: cs.CL

TL;DR: The paper introduces CC-RAG, a method to improve how LLMs understand cause-effect relationships by using structured causal graphs instead of flat retrieval contexts, achieving better results in two domains.


<details>
  <summary>Details</summary>
Motivation: Understanding cause-effect relationships remains challenging for LLMs, especially in specialized domains requiring reasoning beyond simple correlations. Standard RAG pipelines fail to model true causal dependencies.

Method: The authors propose CC-RAG, which combines zero-shot triple extraction and graph chaining in the RAG pipeline. It constructs a DAG of <cause, relation, effect> triples and uses forward/backward chaining for guided answer generation.

Result: CC-RAG outperformed standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity, as shown in experiments on Bitcoin price fluctuations and Gaucher disease. Both LLM-as-a-Judge and human evaluations favored CC-RAG.

Conclusion: Explicitly modeling causal structures enhances LLMs' ability to produce accurate and interpretable responses in specialized domains where flat retrieval methods perform poorly.

Abstract: Understanding cause and effect relationships remains a formidable challenge
for Large Language Models (LLMs), particularly in specialized domains where
reasoning requires more than surface-level correlations. Retrieval-Augmented
Generation (RAG) improves factual accuracy, but standard RAG pipelines treat
evidence as flat context, lacking the structure required to model true causal
dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that
integrates zero-shot triple extraction and theme-aware graph chaining into the
RAG pipeline, enabling structured multi-hop inference. Given a domain specific
corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,
effect> triples and uses forward/backward chaining to guide structured answer
generation. Experiments on two real-world domains: Bitcoin price fluctuations
and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot
LLMs in chain similarity, information density, and lexical diversity. Both
LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results
demonstrate that explicitly modeling causal structure enables LLMs to generate
more accurate and interpretable responses, especially in specialized domains
where flat retrieval fails.

</details>


### [60] [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/abs/2506.08371)
*Zikai Xiao,Ziyang Wang,Wen Ma,Yan Zhang,Wei Shen,Yan Wang,Luqi Gong,Zuozhu Liu*

Main category: cs.CL

TL;DR: The paper introduces Positional Contrastive Decoding (PCD) to tackle long-context degradation issues in LLMs without requiring additional training.


<details>
  <summary>Details</summary>
Motivation: Large Language Models face performance degradation within long context windows, and existing solutions are high-cost, creating a need for a cost-effective alternative.

Method: A training-free PCD method contrasts logits from long-aware attention with those from local-aware attention to focus on the gains of short-to-long training.

Result: The PCD method effectively addresses attention score degradation and achieves state-of-the-art results on long-context benchmarks.

Conclusion: PCD offers a cost-effective, training-free solution to mitigate long-context performance degradation in LLMs, demonstrating improved performance on relevant tasks.

Abstract: While Large Language Models (LLMs) support long contexts, they struggle with
performance degradation within the context window. Current solutions incur
prohibitive training costs, leaving statistical behaviors and cost-effective
approaches underexplored. From the decoding perspective, we identify the
Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio
correlates with long-text performance degradation. Notably, despite the
attenuation, gold tokens still occupy high-ranking positions in the decoding
space. Motivated by it, we propose the training-free Positional Contrastive
Decoding (PCD) that contrasts the logits derived from long-aware attention with
those from designed local-aware attention, enabling the model to focus on the
gains introduced by large-scale short-to-long training. Through the analysis of
long-term decay simulation, we demonstrate that PCD effectively alleviates
attention score degradation. Experimental results show that PCD achieves
state-of-the-art performance on long-context benchmarks.

</details>


### [61] [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
*Kevin Galim,Ethan Ewer,Wonjun Kang,Minjae Lee,Hyung Il Koo,Kangwook Lee*

Main category: cs.CL

TL;DR: The paper introduces SpecKV and SpecPC, approaches to optimize LLM inference by predicting token and key-value importance using smaller draft models, achieving higher accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of long-context inference in large language models, which involves quadratic compute and linear memory complexity.

Method: The proposed methods, SpecKV and SpecPC, use draft models to predict token and key-value importance, with SpecKV focusing on KV cache management and SpecPC identifying less critical prompt tokens.

Result: Experiments demonstrate improved accuracy over existing methods while maintaining memory, latency, and throughput benefits in long-context benchmarks.

Conclusion: Draft models are effective for approximate inference in LLMs, enhancing both accuracy and efficiency, suggesting a new direction for LLM optimization methodologies.

Abstract: Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(KV) cache dropping, sparse attention, and prompt compression, typically rely
on rough predictions of token or KV pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and KV pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) SpecKV, which
leverages a draft output to accurately assess the importance of each KV pair
for more effective KV cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference acceleration, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [62] [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)
*Tao Zou,Xinghua Zhang,Haiyang Yu,Minzheng Wang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: The paper introduces EIFBENCH, a benchmark for evaluating LLMs under complex, multi-task constraints, and proposes the SegPO algorithm to improve LLM performance in such scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the insufficiency of existing benchmarks, which lack the complexity and constraints of real-world multi-task environments, and to better evaluate LLM capabilities to meet intricate user needs.

Method: The paper introduces EIFBENCH, a benchmark designed with multi-task scenarios and operational constraints, and develops the Segment Policy Optimization (SegPO) algorithm for improving LLM workflow execution.

Result: Experiments on EIFBENCH reveal significant performance gaps in existing LLMs under extremely complex instructions, highlighting areas for improvement.

Conclusion: Current LLMs need further optimization to handle the complexities of real-world multi-task scenarios, as demonstrated by their performance on the EIFBENCH benchmark.

Abstract: With the development and widespread application of large language models
(LLMs), the new paradigm of "Model as Product" is rapidly evolving, and demands
higher capabilities to address complex user needs, often requiring precise
workflow execution which involves the accurate understanding of multiple tasks.
However, existing benchmarks focusing on single-task environments with limited
constraints lack the complexity required to fully reflect real-world scenarios.
To bridge this gap, we present the Extremely Complex Instruction Following
Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and
robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that
enable comprehensive assessment across diverse task types concurrently, but
also integrates a variety of constraints, replicating complex operational
environments. Furthermore, we propose the Segment Policy Optimization (SegPO)
algorithm to enhance the LLM's ability to accurately fulfill multi-task
workflow. Evaluations on EIFBENCH have unveiled considerable performance
discrepancies in existing LLMs when challenged with these extremely complex
instructions. This finding underscores the necessity for ongoing optimization
to navigate the intricate challenges posed by LLM applications.

</details>


### [63] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
*Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: This paper introduces the mSTEB benchmark for evaluating LLMs on tasks in low-resource languages and identifies significant performance gaps requiring attention.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a standardized evaluation benchmark for low-resource languages in LLMs, especially in tasks spanning multiple modalities.

Method: Introduced mSTEB, a benchmark for evaluating LLMs' capabilities across multiple tasks and languages, and assessed leading models like Gemini 2.0 Flash and GPT-4o (Audio).

Result: Revealed a large performance disparity between high-resource and low-resource languages, particularly in Africa and Americas/Oceania.

Conclusion: Highlighted the need for increased investment and attention to improve LLM support for low-resource languages.

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [64] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
*Weiya Li,Junjie Chen,Bei Li,Boyang Liu,Zichen Wen,Nuanqiao Shan,Xiaoqian Liu,Anping Liu,Huajie Liu,Youyan Wang,Wujiuge Yin,Hu Song,Bing Huang,Zhiyuan Xia,Jialiang Chen,Linfeng Zhang*

Main category: cs.CL

TL;DR: The paper introduces a new multi-agent translation framework called TACTIC, inspired by cognitive translation strategies, to improve machine translation quality. It achieves state-of-the-art results on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in existing multi-agent translation systems that neglect cognitive translation insights and to unlock the full translation potential of large language models (LLMs).

Method: The proposed framework, TACTIC, consists of six functionally distinct agents representing cognitive processes in human translation: drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. It simulates a theory-based interactive workflow.

Result: TACTIC achieves state-of-the-art performance on FLORES-200 and WMT24 benchmarks, outperforming both GPT-4.1 and DeepSeek-R1 by substantial margins in XCOMET and COMETKIWI-23 scores.

Conclusion: Integrating cognitive-theoretic strategies into multi-agent frameworks can significantly enhance translation quality, leveraging the full potential of LLMs. The publicly available TACTIC framework demonstrates its efficacy through superior performance metrics.

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [65] [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)
*Ziyang Ma,Qingyue Yuan,Zhenglin Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: This paper studies and enhances the meta-cognitive abilities of large language models (LLMs) by introducing an evaluation framework (AutoMeco) and an improvement method (MIRA) for reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gap in understanding and improving LLMs' meta-cognitive abilities, specifically their awareness and adaptation to step-level reasoning errors.

Method: The study introduces AutoMeco, a meta-cognition evaluation framework, and MIRA, a reward adjustment strategy to improve existing meta-cognitive measures. Experiments were conducted on three reasoning datasets using three LLMs.

Result: AutoMeco provides reasonable benchmarking compared to existing verification approaches, such as Best-of-N, and MIRA enhances the accuracy of meta-cognition evaluations.

Conclusion: The paper demonstrates the effectiveness of AutoMeco and MIRA in better evaluating and boosting the meta-cognitive capabilities of LLMs, marking progress in their reliability for reasoning tasks.

Abstract: Previous research has primarily focused on the cognitive error detection
capabilities of Large Language Models (LLMs), often prompting them to analyze
mistakes in reasoning chains. However, few studies have examined the
meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),
which are crucial for their reliability. While studies on LLM self-evaluation
present some measures, such as perplexity, which can reflect the answer
correctness and be viewed as the lens of meta-cognition, they lack step-level
analysis and adaptation. This paper studies the evaluation of LLM
meta-cognition using the current lenses and how to improve these lenses.
Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation
framework for benchmarking the existing lenses. Furthermore, a training-free
Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost
current meta-cognition lenses. Experimental results on three mathematical
reasoning datasets and three LLMs show the reasonableness of AutoMeco by
comparing it with Best-of-N verification. Moreover, the meta-cognition ability
of LLMs can be better evaluated using MIRA.

</details>


### [66] [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/abs/2506.08427)
*Jiaxiang Liu,Boxuan Xing,Chenhao Yuan,Chenxiang Zhang,Di Wu,Xiusheng Huang,Haida Yu,Chuhan Lang,Pengfei Cao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: The paper introduces Know-MRI, an open-source tool that systematically analyzes LLMs' knowledge mechanisms by integrating diverse interpretation methods.


<details>
  <summary>Details</summary>
Motivation: Advancing the interpretability of large language models due to increasing complexity and the need for systematic analysis.

Method: Developed an extensible core module for Know-MRI that automatically matches various input formats with interpretation methods and consolidates the outputs.

Result: Know-MRI enables users to select interpretation methods flexibly to diagnose LLMs from multiple perspectives via unified inputs and outputs.

Conclusion: Know-MRI addresses the limitations in current interpretation tools, facilitating a comprehensive understanding of LLM knowledge mechanisms.

Abstract: As large language models (LLMs) continue to advance, there is a growing
urgency to enhance the interpretability of their internal knowledge mechanisms.
Consequently, many interpretation methods have emerged, aiming to unravel the
knowledge mechanisms of LLMs from various perspectives. However, current
interpretation methods differ in input data formats and interpreting outputs.
The tools integrating these methods are only capable of supporting tasks with
specific inputs, significantly constraining their practical applications. To
address these challenges, we present an open-source Knowledge Mechanisms
Revealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms
within LLMs systematically. Specifically, we have developed an extensible core
module that can automatically match different input data with interpretation
methods and consolidate the interpreting outputs. It enables users to freely
choose appropriate interpretation methods based on the inputs, making it easier
to comprehensively diagnose the model's internal knowledge mechanisms from
multiple perspectives. Our code is available at
https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on
https://youtu.be/NVWZABJ43Bs.

</details>


### [67] [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)
*Ziqi. Liu,Ziyang. Zhou,Mingxuan. Hu*

Main category: cs.CL

TL;DR: This paper proposes the Collaborative Agent Framework for Irony (CAF-I), a multi-agent system leveraging LLMs for improved sarcasm detection, overcoming key limitations.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the challenges of sarcasm detection with LLMs, including single-perspective limitations, insufficient comprehensive understanding, and a lack of interpretability.

Method: CAF-I uses specialized agents focused on Context, Semantics, and Rhetoric, performing multidimensional analysis, collaborative optimization, and a Decision Agent consolidating insights with feedback from a Refinement Evaluator.

Result: CAF-I achieves state-of-the-art zero-shot performance with an average Macro-F1 score of 76.31, surpassing prior baselines by 4.98 points on benchmark datasets.

Conclusion: CAF-I demonstrates that simulating human-like multi-perspective analysis in sarcasm detection models improves both accuracy and interpretability, setting new performance benchmarks.

Abstract: Large language model (LLM) have become mainstream methods in the field of
sarcasm detection. However, existing LLM methods face challenges in irony
detection, including: 1. single-perspective limitations, 2. insufficient
comprehensive understanding, and 3. lack of interpretability. This paper
introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven
multi-agent system designed to overcome these issues. CAF-I employs specialized
agents for Context, Semantics, and Rhetoric, which perform multidimensional
analysis and engage in interactive collaborative optimization. A Decision Agent
then consolidates these perspectives, with a Refinement Evaluator Agent
providing conditional feedback for optimization. Experiments on benchmark
datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving
SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of
76.31, a 4.98 absolute improvement over the strongest prior baseline. This
success is attained by its effective simulation of human-like multi-perspective
analysis, enhancing detection accuracy and interpretability.

</details>


### [68] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
*Jiujun He,Huazhen Lin*

Main category: cs.CL

TL;DR: The paper introduces Olica, a framework for pruning LLMs without retraining. It uses PCA for multi-head attention optimization and SVD for error reconstruction in feed-forward layers.


<details>
  <summary>Details</summary>
Motivation: Existing structured pruning methods for LLMs demand high computational and data resources for retraining, making such processes expensive and impractical.

Method: The authors propose Olica, which utilizes PCA to compress multi-head attention layers and linear calibration with SVD in feed-forward layers to reconstruct errors, eliminating the need for retraining.

Result: Olica is computationally efficient, reducing complexity and resource usage while achieving superior performance across benchmarks.

Conclusion: Olica provides an effective and resource-efficient pruning framework for LLMs, maintaining accuracy while eliminating the cost of retraining.

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [69] [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/abs/2506.08477)
*Fengjun Pan,Anh Tuan Luu,Xiaobao Wu*

Main category: cs.CL

TL;DR: The paper introduces U-CoT+, a novel framework for detecting harmful memes by using textual descriptions derived from a meme-to-text pipeline and targeted reasoning through zero-shot prompting.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of resource inefficiency, inflexibility, and lack of explainability associated with current harmful meme detection approaches.

Method: The authors create a high-fidelity meme-to-text pipeline to convert visual memes into textual descriptions, enabling classification via large language models with zero-shot prompting and interpretable guidelines.

Result: Experiments on seven benchmark datasets show the framework's effectiveness in performing explainable and resource-efficient harmful meme detection using small language models.

Conclusion: The U-CoT+ framework offers a practical, adaptable, and explainable solution for harmful meme detection, suitable for varying criteria across platforms and regions.

Abstract: Detecting harmful memes is essential for maintaining the integrity of online
environments. However, current approaches often struggle with resource
efficiency, flexibility, or explainability, limiting their practical deployment
in content moderation systems. To address these challenges, we introduce
U-CoT+, a novel framework for harmful meme detection. Instead of relying solely
on prompting or fine-tuning multimodal models, we first develop a high-fidelity
meme-to-text pipeline that converts visual memes into detail-preserving textual
descriptions. This design decouples meme interpretation from meme
classification, thus avoiding immediate reasoning over complex raw visual
content and enabling resource-efficient harmful meme detection with general
large language models (LLMs). Building on these textual descriptions, we
further incorporate targeted, interpretable human-crafted guidelines to guide
models' reasoning under zero-shot CoT prompting. As such, this framework allows
for easy adaptation to different harmfulness detection criteria across
platforms, regions, and over time, offering high flexibility and
explainability. Extensive experiments on seven benchmark datasets validate the
effectiveness of our framework, highlighting its potential for explainable and
low-resource harmful meme detection using small-scale LLMs. Codes and data are
available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.

</details>


### [70] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
*Chihiro Taguchi,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: This paper proposes Adaptive-$k$ retrieval, a one-pass method to optimize retrieval size in open-domain QA, achieving efficiency and accuracy without additional fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of determining the optimal size of external context to retrieve in open-domain QA, where fixed-size or iterative methods can be inefficient or suboptimal.

Method: It introduces Adaptive-$k$, which selects the number of retrieved passages dynamically based on the distribution of similarity scores. This approach requires no model fine-tuning and integrates seamlessly into existing pipelines.

Result: Adaptive-$k$ outperforms fixed-size retrievals in benchmarks, saving up to 10x tokens while maintaining a high retrieval rate of 70% for relevant passages. It demonstrates improved accuracy across multiple models.

Conclusion: Dynamically adjusting the context size enhances the efficiency and accuracy of open-domain QA, offering a practical and adaptive alternative to existing methods.

Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.

</details>


### [71] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: The paper assesses challenges in evaluating image-text alignment in text-to-image generation models and offers recommendations to enhance these evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of image-text alignment in text-to-image tasks focus mainly on human assessment agreement, ignoring other crucial evaluation framework properties.

Method: The authors identify critical aspects of reliable evaluations, analyze shortcomings of current frameworks, and provide recommendations to address them.

Result: Mainstream evaluation frameworks were empirically found inadequate in fully addressing the identified aspects.

Conclusion: Recommendations are presented to enhance trustworthiness and comprehensiveness in image-text alignment evaluations.

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [72] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
*Sumanth Manduru,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: The study audits mid-tier instruction-tuned Small Language Models (SLMs) to understand their utility and fairness. It identifies that models like Phi balance competence and fairness, but bias patterns vary across architectures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the under-researched ethical risks and performance trade-offs in small, instruction-tuned language models (SLMs), which are increasingly being used in on-device, resource-constrained applications.

Method: The authors audited nine open-source SLMs (0.5 to 5 billion parameters) by analyzing their performance on the BBQ benchmark using zero-shot prompting. Fairness and utility were evaluated across ambiguous and disambiguated contexts.

Result: Insights showed that fairness and competence are not conflicting goals (e.g., Phi models balance both). Bias varies across architectures, as Qwen models tend to display neutrality and LLaMA models show stereotypes. Compression techniques like quantization improve certain F1 scores but can amplify biases.

Conclusion: Ethically aligned and efficient SLMs are achievable, but biases vary significantly across families, architectures, and optimization techniques. The findings guide responsible deployment of SLMs, notably aiding small enterprises and constrained environments.

Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.

</details>


### [73] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
*Ashutosh Dwivedi,Siddhant Shivdutt Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: The paper introduces EtiCor++, a resource for evaluating large language models (LLMs) on etiquette understanding, identifying biases toward certain regions.


<details>
  <summary>Details</summary>
Motivation: There is limited research and resources to evaluate large language models (LLMs) for understanding and biases regarding culturally significant etiquettes.

Method: The authors created EtiCor++, a global etiquette corpus, designed tasks for evaluating LLMs on etiquette knowledge, and proposed metrics to measure bias in LLMs.

Result: Extensive experimentation demonstrated that LLMs exhibit inherent biases favoring specific regions in their etiquette responses.

Conclusion: EtiCor++ provides a benchmark to enhance cultural sensitivity in LLMs and address regional biases, contributing to a more inclusive development of AI.

Abstract: In recent years, researchers have started analyzing the cultural sensitivity
of LLMs. In this respect, Etiquettes have been an active area of research.
Etiquettes are region-specific and are an essential part of the culture of a
region; hence, it is imperative to make LLMs sensitive to etiquettes. However,
there needs to be more resources in evaluating LLMs for their understanding and
bias with regard to etiquettes. In this resource paper, we introduce EtiCor++,
a corpus of etiquettes worldwide. We introduce different tasks for evaluating
LLMs for knowledge about etiquettes across various regions. Further, we
introduce various metrics for measuring bias in LLMs. Extensive experimentation
with LLMs shows inherent bias towards certain regions.

</details>


### [74] [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/abs/2506.08490)
*Xiao Wei,Xiaobao Wang,Ning Zhuang,Chenyang Wang,Longbiao Wang,Jianwu dang*

Main category: cs.CL

TL;DR: This paper proposes a new method for improved Generalized Intent Discovery (GID), blending old and new knowledge for better OOD intent detection.


<details>
  <summary>Details</summary>
Motivation: Current supervised intent detection methods lack practical applicability as they struggle with identifying out-of-domain intents and rely heavily on labeled in-domain data.

Method: The authors introduced a consistency-driven prototype-prompting framework that uses prototype-prompting for old knowledge transfer and a hierarchical consistency constraint for new knowledge learning during domain adaptation.

Result: Experimental results demonstrate that the proposed method significantly outperforms existing baseline methods, achieving state-of-the-art results in GID.

Conclusion: The proposed framework effectively enhances the identification of OOD intents in GID scenarios, validating its effectiveness and generalizability.

Abstract: Intent detection aims to identify user intents from natural language inputs,
where supervised methods rely heavily on labeled in-domain (IND) data and
struggle with out-of-domain (OOD) intents, limiting their practical
applicability. Generalized Intent Discovery (GID) addresses this by leveraging
unlabeled OOD data to discover new intents without additional annotation.
However, existing methods focus solely on clustering unsupervised data while
neglecting domain adaptation. Therefore, we propose a consistency-driven
prototype-prompting framework for GID from the perspective of integrating old
and new knowledge, which includes a prototype-prompting framework for
transferring old knowledge from external sources, and a hierarchical
consistency constraint for learning new knowledge from target domains. We
conducted extensive experiments and the results show that our method
significantly outperforms all baseline methods, achieving state-of-the-art
results, which strongly demonstrates the effectiveness and generalization of
our methods. Our source code is publicly available at
https://github.com/smileix/cpp.

</details>


### [75] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
*Arie Cattan,Alon Jacovi,Ori Ram,Jonathan Herzig,Roee Aharoni,Sasha Goldshtein,Eran Ofek,Idan Szpektor,Avi Caciularu*

Main category: cs.CL

TL;DR: The paper introduces a taxonomy and benchmark to analyze how large language models handle conflicting information in Retrieval Augmented Generation (RAG) settings, finding substantial challenges and room for improvement.


<details>
  <summary>Details</summary>
Motivation: To address the issue of conflicting information within retrieved sources in RAG and provide a structured way to analyze and improve LLM behavior in such scenarios.

Method: Developed a novel taxonomy of knowledge conflict types, created the CONFLICTS benchmark with expert annotations, and conducted experiments on how LLMs resolve conflicts using reasoning prompts.

Result: Experiments reveal models struggle to resolve conflicts appropriately but show improvements when explicitly prompted to reason about retrieved conflicts.

Conclusion: Significant progress is needed in addressing knowledge conflicts in RAG, even though prompts improve the situation slightly.

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [76] [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
*Qihui Fan,Enfu Nan,Wenbo Li,Lei Lu,Pu Zhao,Yanzhi Wang*

Main category: cs.CL

TL;DR: This paper introduces a new LLM-based system for the social deduction game Werewolf utilizing Text-to-Speech (TTS) models to enhance user compatibility and engagement, eliminating the need for fine-tuning or advanced components.


<details>
  <summary>Details</summary>
Motivation: The growing capabilities of LLMs provide an opportunity to elevate experiences in social deduction games such as Werewolf, leveraging reasoning and persuasion improvements seen in recent advancements like DeepSeek R1 and V3.

Method: The authors propose a straightforward approach combining LLMs with optimized Text-to-Speech models to create an engaging game system without reliance on fine-tuning, advanced prompting, or additional experience pools.

Result: They suggest their approach streamlines the integration of LLMs into the Werewolf game system while maintaining or enhancing user engagement.

Conclusion: This method demonstrates that with improved reasoning capabilities of LLMs, traditional additional game system components may no longer be necessary.

Abstract: The growing popularity of social deduction game systems for both business
applications and AI research has greatly benefited from the rapid advancements
in Large Language Models (LLMs), which now demonstrate stronger reasoning and
persuasion capabilities. Especially with the raise of DeepSeek R1 and V3
models, LLMs should enable a more engaging experience for human players in
LLM-agent-based social deduction games like Werewolf. Previous works either
fine-tuning, advanced prompting engineering, or additional experience pool to
achieve engaging text-format Werewolf game experience. We propose a novel yet
straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)
models designed for enhanced compatibility with various LLM models, and
improved user engagement. We argue with ever enhancing LLM reasoning, extra
components will be unnecessary in the case of Werewolf.

</details>


### [77] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
*Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi*

Main category: cs.CL

TL;DR: The paper introduces CoMuMDR, a corpus containing code-mixed audio and text in Hindi and English, aimed at enhancing discourse parsing in conversations.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing discourse parsing datasets by creating a multi-modal and multi-domain resource that includes code-mixed conversations.

Method: Development of the CoMuMDR corpus with annotations for nine discourse relations, and evaluation using State-of-the-Art (SoTA) models.

Result: Experiments show that current SoTA models struggle with multi-domain, code-mixed settings present in the corpus.

Conclusion: The findings highlight the need for improved discourse parsing models tailored to real-world, code-mixed, multi-domain scenarios.

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [78] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
*Xinyuan Wang,Dongjie Wang,Wangyang Ying,Haoyue Bai,Nanxu Gong,Sixun Dong,Kunpeng Liu,Yanjie Fu*

Main category: cs.CL

TL;DR: The paper introduces a new post-training framework that refines latent reasoning trajectories in Large Language Models (LLMs) to enhance reasoning accuracy, achieving improvements such as a 5% accuracy gain on MathQA.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to improve reasoning capabilities in LLMs by addressing the limitations of Chain-of-Thought prompting (e.g., token overhead and rigidity) and latent reasoning (e.g., lack of effective embedding updates).

Method: The authors propose a post-training framework incorporating two strategies: contrastive reasoning feedback to enhance embeddings by comparing strong and weak baselines, and residual embedding refinement to stabilize updates through progressive gradient integration.

Result: The proposed framework demonstrates improved reasoning performance, particularly achieving a 5% accuracy increase on the MathQA benchmark, along with success across five different reasoning benchmarks.

Conclusion: The framework effectively refines the reasoning processes in LLMs by improving the latent-space representations, offering a practical strategy to guide models toward more accurate solutions without additional full-scale training.

Abstract: Reasoning is a key component of language understanding in Large Language
Models. While Chain-of-Thought prompting enhances performance via explicit
intermediate steps, it suffers from sufficient token overhead and a fixed
reasoning trajectory, preventing step-wise refinement. Recent advances in
latent reasoning address these limitations by refining internal reasoning
processes directly in the model's latent space, without producing explicit
outputs. However, a key challenge remains: how to effectively update reasoning
embeddings during post-training to guide the model toward more accurate
solutions. To overcome this challenge, we propose a lightweight post-training
framework that refines latent reasoning trajectories using two novel
strategies: 1) Contrastive reasoning feedback, which compares reasoning
embeddings against strong and weak baselines to infer effective update
directions via embedding enhancement; 2) Residual embedding refinement, which
stabilizes updates by progressively integrating current and historical
gradients, enabling fast yet controlled convergence. Extensive experiments and
case studies are conducted on five reasoning benchmarks to demonstrate the
effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA
without additional training.

</details>


### [79] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: A novel curriculum learning framework, inspired by the progressive overload principle, is introduced to stabilize and improve the efficiency of knowledge distillation (KD) for large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing Knowledge Distillation methods for LLMs often face challenges like catastrophic forgetting and training-inference mismatch due to significant distributional shifts in student models during training.

Method: The proposed framework, POCL, integrates two components: (1) a difficulty measurer to rank and partition training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets with progressively increasing loss function temperatures during distillation.

Result: Experiments in instruction-following scenarios show that POCL consistently enhances the performance of distilled student models across different KD methods and model families.

Conclusion: POCL improves the stability and performance of knowledge distillation for LLMs, demonstrating the benefits of structuring training data to ensure efficient and stable learning processes.

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [80] [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/abs/2506.08564)
*Tuukka Törö,Antti Suni,Juraj Šimko*

Main category: cs.CL

TL;DR: The study uses speech embeddings from a fine-tuned language identification model to analyze relationships among 106 languages, aligning their distances with traditional linguistic measures.


<details>
  <summary>Details</summary>
Motivation: To explore scalable, data-driven methods for analyzing global linguistic relationships using speech embeddings, overcoming reliance on expert analysis of linguistic features.

Method: The study utilizes embeddings from the fine-tuned XLS-R model and applies linear discriminant analysis to cluster language distances, comparing results with genealogical, lexical, and geographical measures.

Result: The embedding-based distances align with traditional linguistic measures and capture global and local patterns, although visualization challenges reveal the complexity of language change.

Conclusion: This approach enables scalable analysis of linguistic variation, aiding research into low-resource languages and integrating macro- and micro-level dynamics for a broader understanding of linguistic diversity.

Abstract: Investigating linguistic relationships on a global scale requires analyzing
diverse features such as syntax, phonology and prosody, which evolve at varying
rates influenced by internal diversification, language contact, and
sociolinguistic factors. Recent advances in machine learning (ML) offer
complementary alternatives to traditional historical and typological
approaches. Instead of relying on expert labor in analyzing specific linguistic
features, these new methods enable the exploration of linguistic variation
through embeddings derived directly from speech, opening new avenues for
large-scale, data-driven analyses.
  This study employs embeddings from the fine-tuned XLS-R self-supervised
language identification model voxlingua107-xls-r-300m-wav2vec, to analyze
relationships between 106 world languages based on speech recordings. Using
linear discriminant analysis (LDA), language embeddings are clustered and
compared with genealogical, lexical, and geographical distances. The results
demonstrate that embedding-based distances align closely with traditional
measures, effectively capturing both global and local typological patterns.
Challenges in visualizing relationships, particularly with hierarchical
clustering and network-based methods, highlight the dynamic nature of language
change.
  The findings show potential for scalable analyses of language variation based
on speech embeddings, providing new perspectives on relationships among
languages. By addressing methodological considerations such as corpus size and
latent space dimensionality, this approach opens avenues for studying
low-resource languages and bridging macro- and micro-level linguistic
variation. Future work aims to extend these methods to underrepresented
languages and integrate sociolinguistic variation for a more comprehensive
understanding of linguistic diversity.

</details>


### [81] [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/abs/2506.08584)
*Yahan Li,Jifan Yao,John Bosco S. Bunyi,Adam C. Frank,Angel Hwang,Ruishan Liu*

Main category: cs.CL

TL;DR: The paper introduces CounselBench, a benchmark to evaluate large language models (LLMs) in mental health counseling. While LLMs often outperform online human therapists in perceived quality, they have safety concerns. A specialized adversarial dataset also exposes consistent failure patterns in LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for rigorous testing of LLMs in high-stakes mental health counseling scenarios to ensure both effectiveness and safety.

Method: Developing CounselBench, including CounselBench-EVAL with expert evaluations of LLM and human therapist responses, and CounselBench-Adv, an adversarial dataset to probe model weaknesses.

Result: LLMs outperform human therapists in perceived quality but are flagged for safety concerns. LLM judges overrate model responses, and experiments reveal failure patterns under adversarial testing.

Conclusion: CounselBench provides a clinically grounded framework to benchmark and improve the safety and effectiveness of LLMs in mental health applications.

Abstract: Large language models (LLMs) are increasingly proposed for use in mental
health support, yet their behavior in realistic counseling scenarios remains
largely untested. We introduce CounselBench, a large-scale benchmark developed
with 100 mental health professionals to evaluate and stress-test LLMs in
single-turn counseling. The first component, CounselBench-EVAL, contains 2,000
expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human
therapists to real patient questions. Each response is rated along six
clinically grounded dimensions, with written rationales and span-level
annotations. We find that LLMs often outperform online human therapists in
perceived quality, but experts frequently flag their outputs for safety
concerns such as unauthorized medical advice. Follow-up experiments show that
LLM judges consistently overrate model responses and overlook safety issues
identified by human experts. To probe failure modes more directly, we construct
CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling
questions designed to trigger specific model issues. Evaluation across 2,880
responses from eight LLMs reveals consistent, model-specific failure patterns.
Together, CounselBench establishes a clinically grounded framework for
benchmarking and improving LLM behavior in high-stakes mental health settings.

</details>


### [82] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
*Liyan Xu,Zhenlin Su,Mo Yu,Jiangnan Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: The paper introduces CapRetrieval, a dataset for evaluating text encoders on fine-grained entity and event recognition, and proposes finetuning strategies to address encoder limitations.


<details>
  <summary>Details</summary>
Motivation: Text encoders struggle to accurately capture fine-grained entities and events in semantics, leading to poor performance in certain dense retrieval tasks.

Method: The authors created the CapRetrieval dataset with image captions as passages and fine-grained queries, tested encoder performance in a zero-shot setting, and proposed data generation strategies for finetuning.

Result: Finetuned encoders significantly outperformed baseline models, demonstrating enhanced capabilities in recognizing fine-grained semantic entities and events.

Conclusion: Fine-grained entity and event recognition in text encoders can be improved through targeted data generation and finetuning, addressing intrinsic limitations and granularity dilemmas.

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [83] [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/abs/2506.08593)
*Shuzhou Yuan,Ercong Nie,Mario Tawfelis,Helmut Schmid,Hinrich Schütze,Michael Färber*

Main category: cs.CL

TL;DR: The paper investigates how persona prompts, based on MBTI personality traits, influence hate speech detection by Large Language Models (LLMs). Using human annotations and LLM evaluations, it reveals significant persona-driven variations and biases.


<details>
  <summary>Details</summary>
Motivation: Understanding how subjective traits, such as personality, influence hate speech detection annotations to improve fairness and align machine outputs with human values.

Method: Conducted a human annotation survey examining MBTI personality traits' effects on labeling behavior and extended these observations by testing four LLMs using MBTI-based persona prompts across three hate speech datasets.

Result: Findings reveal substantial persona-driven variations including discrepancies with ground truth, disagreements between personas, and biases at the logit level.

Conclusion: Defining persona prompts cautiously is critical in LLM workflows to ensure fairness and alignment with societal values.

Abstract: Hate speech detection is a socially sensitive and inherently subjective task,
with judgments often varying based on personal traits. While prior work has
examined how socio-demographic factors influence annotation, the impact of
personality traits on Large Language Models (LLMs) remains largely unexplored.
In this paper, we present the first comprehensive study on the role of persona
prompts in hate speech classification, focusing on MBTI-based traits. A human
annotation survey confirms that MBTI dimensions significantly affect labeling
behavior. Extending this to LLMs, we prompt four open-source models with MBTI
personas and evaluate their outputs across three hate speech datasets. Our
analysis uncovers substantial persona-driven variation, including
inconsistencies with ground truth, inter-persona disagreement, and logit-level
biases. These findings highlight the need to carefully define persona prompts
in LLM-based annotation workflows, with implications for fairness and alignment
with human values.

</details>


### [84] [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)
*Minhae Oh,Jeonghye Kim,Nakyung Lee,Donggeon Seo,Taeuk Kim,Jungwoo Lee*

Main category: cs.CL

TL;DR: RAISE, a new retrieval-augmented framework for scientific reasoning, outperforms other methods by retrieving logically relevant documents to support reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in scientific reasoning like long-chain reasoning, domain-specific knowledge, and adapting to updated findings.

Method: RAISE operates in three steps: problem decomposition, logical query generation, and logical retrieval, using an in-the-wild document corpus.

Result: RAISE successfully delivers better performance compared to existing baselines in scientific reasoning tasks by retrieving documents that are both domain-relevant and logically significant.

Conclusion: RAISE enhances scientific reasoning benchmarks by introducing a logical retrieval process, making it a superior framework over existing baselines.

Abstract: Scientific reasoning requires not only long-chain reasoning processes, but
also knowledge of domain-specific terminologies and adaptation to updated
findings. To deal with these challenges for scientific reasoning, we introduce
RAISE, a step-by-step retrieval-augmented framework which retrieves logically
relevant documents from in-the-wild corpus. RAISE is divided into three steps:
problem decomposition, logical query generation, and logical retrieval. We
observe that RAISE consistently outperforms other baselines on scientific
reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves
documents that are not only similar in terms of the domain knowledge, but also
documents logically more relevant.

</details>


### [85] [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/abs/2506.08643)
*Son The Nguyen,Theja Tulabandhula*

Main category: cs.CL

TL;DR: MEMETRON introduces a framework for more effective control of LLM responses, leveraging hybrid metaheuristic algorithms for decoding instead of traditional heuristic approaches.


<details>
  <summary>Details</summary>
Motivation: To address the lack of task-specific optimization in the inference-time decoding of LLMs.

Method: MEMETRON utilizes GENETRON and ANNETRON metaheuristic algorithms within a task-agnostic framework to optimize decoding strategies based on reward models.

Result: Experimentation shows significant improvement in human preference alignment compared to standard decoding methods.

Conclusion: MEMETRON can enhance response alignment for diverse tasks without retraining large language models.

Abstract: Large language models (LLMs) are increasingly used for both open-ended and
structured tasks, yet their inference-time behavior is still largely dictated
by heuristic decoding strategies such as greedy search, sampling, or reranking.
These methods provide limited control and do not explicitly optimize for
task-specific objectives. We introduce MEMETRON, a task-agnostic framework that
formulates LLM decoding as a discrete black-box optimization problem. MEMETRON
leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the
response space, guided by reward models and contextual operations performed by
the LLM itself. This approach enables efficient discovery of high-reward
responses without requiring model retraining or gradient access. The framework
is modular and generalizes across diverse tasks, requiring only a reward
function and lightweight prompt templates. We evaluate our framework on the
critical human preference alignment task and demonstrate that it significantly
outperforms standard decoding and reranking methods, highlighting its potential
to improve alignment without model retraining.

</details>


### [86] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
*Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang*

Main category: cs.CL

TL;DR: The paper introduces TableDreamer, a framework for progressive and weakness-guided table instruction data synthesis for large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based data synthesis methods for table instruction tuning lack data diversity and fail to address the weaknesses of target LLMs, leading to inefficiency.

Method: TableDreamer starts by generating diverse seed tables and instructions, followed by iterative exploration of the input space based on identified weaknesses to improve training data quality.

Result: Experiments show that TableDreamer significantly improves the accuracy of Llama3.1-8B-instruct by 11.62% on average with 27,000 GPT-4o synthetic data and surpasses state-of-the-art data synthesis methods using fewer data.

Conclusion: TableDreamer enhances table instruction tuning by addressing both data diversity and LLM weaknesses, achieving better accuracy and efficiency compared to existing methods.

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [87] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
*Oumaima El Khettari,Solen Quiniou,Samuel Chaffron*

Main category: cs.CL

TL;DR: The study explores using generative RE pipelines with LLMs in the context of intestinal microbiome interactions, finding that summarization improves performance but traditional BERT-based RE methods still outperform generative approaches.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to study interactions in the intestinal microbiome, a complex and low-resource biomedical domain, and to explore whether generative RE methods can be effective in such settings.

Method: The method uses large language models (LLMs) for summarization to refine context, followed by instruction-tuned generation for relation extraction (RE) in data from the intestinal microbiome.

Result: Preliminary results indicate that summarization helps reduce noise and guide generative RE models, but traditional BERT-based RE methods still achieve better performance.

Conclusion: Generative methods hold promise for supporting research in specialized and low-resource domains, but further improvement is needed to match traditional methods like BERT in performance.

Abstract: We explore a generative relation extraction (RE) pipeline tailored to the
study of interactions in the intestinal microbiome, a complex and low-resource
biomedical domain. Our method leverages summarization with large language
models (LLMs) to refine context before extracting relations via
instruction-tuned generation. Preliminary results on a dedicated corpus show
that summarization improves generative RE performance by reducing noise and
guiding the model. However, BERT-based RE approaches still outperform
generative models. This ongoing work demonstrates the potential of generative
methods to support the study of specialized domains in low-resources setting.

</details>


### [88] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: This paper introduces "RuleReasoner," a method that uses reinforcement learning and domain-aware dynamic sampling to enable small reasoning models (SRMs) to perform rule-based reasoning effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore whether small reasoning models (SRMs) can perform rule-based reasoning effectively, with robust generalization across diverse tasks and domains, as large reasoning models (LRMs) have limitations in efficiency and adaptability.

Method: The paper presents "RuleReasoner," which employs domain-aware dynamic sampling to adjust training batches based on historical rewards, facilitating flexible learning schedules without human-engineered mix-training processes.

Result: RuleReasoner outperformed leading LRMs significantly, achieving 4.1% higher accuracy on in-distribution tasks and 10.4% higher accuracy on out-of-distribution tasks, while also being computationally efficient.

Conclusion: RuleReasoner demonstrates that SRMs can achieve robust rule-based reasoning capabilities without requiring extensive human intervention, opening avenues for efficient, adaptive reasoning methods.

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [89] [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)
*Soham Poddar,Paramita Koley,Janardan Misra,Sanjay Podder,Navveen Balani,Niloy Ganguly,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: This paper explores the issue of energy inefficiency in LLM inference due to overly lengthy responses and proposes prompt-engineering strategies to reduce response length effectively.


<details>
  <summary>Details</summary>
Motivation: Energy consumption during inference in LLMs is a growing concern, and most existing methods don't focus significantly on output compression, an area ripe for optimization.

Method: The authors benchmark 12 decoder-only LLMs, assess their response quality across six information categories, and use prompt-engineering techniques to optimize output length while retaining response quality.

Result: Prompt-engineering strategies targeting minimal answers achieved energy consumption reductions between 25-60%, showcasing their effectiveness.

Conclusion: Output compression through prompt-engineering can significantly optimize energy efficiency in LLM inference without compromising response quality, presenting a promising direction for future research.

Abstract: A significant portion of the energy consumed by Large Language Models (LLMs)
arises from their inference processes; hence developing energy-efficient
methods for inference is crucial. While several techniques exist for inference
optimization, output compression remains relatively unexplored, with only a few
preliminary efforts addressing this aspect. In this work, we first benchmark 12
decoder-only LLMs across 5 datasets, revealing that these models often produce
responses that are substantially longer than necessary. We then conduct a
comprehensive quality assessment of LLM responses, formally defining six
information categories present in LLM responses. We show that LLMs often tend
to include redundant or additional information besides the minimal answer. To
address this issue of long responses by LLMs, we explore several simple and
intuitive prompt-engineering strategies. Empirical evaluation shows that
appropriate prompts targeting length reduction and controlling information
content can achieve significant energy optimization between 25-60\% by reducing
the response length while preserving the quality of LLM responses.

</details>


### [90] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
*Ruiran Su,Jiasheng Si,Zhijiang Guo,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: The paper introduces ClimateViz, a dataset and benchmark for scientific fact-checking using expert-curated charts, revealing that state-of-the-art models struggle with chart-based reasoning compared to humans.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in scientific fact-checking, which has largely ignored the critical role of charts in presenting quantitative evidence and statistical reasoning.

Method: The authors created the ClimateViz dataset with 49,862 claims linked to 2,896 visualizations, accompanied by structured knowledge graph explanations. They evaluated state-of-the-art multimodal language models under zero-shot and few-shot settings.

Result: The best systems achieve 76.2–77.8% accuracy in chart-based reasoning, significantly lower than human performance (89.3–92.7%), although explanation-augmented outputs sometimes improve performance.

Conclusion: The paper highlights that current models have significant limitations in chart-based reasoning and emphasizes the need for advancements in this domain. The dataset and code have been made publicly available to foster further research.

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


### [91] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
*Hee Suk Yoon,Eunseop Yoon,Mark A. Hasegawa-Johnson,Sungwoong Kim,Chang D. Yoo*

Main category: cs.CL

TL;DR: ConfPO introduces a method for improving preference learning in LLMs by focusing on optimizing preference-critical tokens using the model's confidence, avoiding the limitations of other methods like DPO and delivering better results with no extra computational overhead.


<details>
  <summary>Details</summary>
Motivation: To enhance alignment in LLMs while addressing issues like overoptimization and inefficiency in prior methods, and to avoid reliance on external credit-assignment tools.

Method: The method selectively identifies and optimizes the most impactful (preference-critical) tokens based only on the policy's confidence, without needing additional models or computations.

Result: ConfPO outperforms traditional uniform DAAs in alignment benchmarks (e.g., AlpacaEval 2 and Arena-Hard) and improves efficiency without additional computational costs.

Conclusion: The paper demonstrates that ConfPO's targeted optimization approach results in better alignment quality, mitigates overoptimization, and is scalable, lightweight, and resource-efficient compared to existing token-level methods.

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [92] [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/abs/2506.08717)
*Mehedi Hasan Bijoy,Dejan Porjazovski,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: The paper introduces a multilingual speech emotion recognition system that employs a novel language-aware multi-teacher knowledge distillation method, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extending monolingual speech emotion recognition systems to a multilingual setting for better human-computer interaction.

Method: A novel language-aware multi-teacher knowledge distillation approach was used, combining Wav2Vec2.0-based monolingual teacher models into a single multilingual student model.

Result: The student model achieved state-of-the-art performance, including a weighted recall of 72.9 for English and an unweighted recall of 63.4 for Finnish, surpassing baselines.

Conclusion: The proposed method effectively trains a multilingual SER system that handles multiple languages and emotions well, but challenges remain in detecting anger and happiness.

Abstract: Speech Emotion Recognition (SER) is crucial for improving human-computer
interaction. Despite strides in monolingual SER, extending them to build a
multilingual system remains challenging. Our goal is to train a single model
capable of multilingual SER by distilling knowledge from multiple teacher
models. To address this, we introduce a novel language-aware multi-teacher
knowledge distillation method to advance SER in English, Finnish, and French.
It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and
then distills their knowledge into a single multilingual student model. The
student model demonstrates state-of-the-art performance, with a weighted recall
of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish
dataset, surpassing fine-tuning and knowledge distillation baselines. Our
method excels in improving recall for sad and neutral emotions, although it
still faces challenges in recognizing anger and happiness.

</details>


### [93] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
*Nelvin Tan,Zian Seng,Liang Zhang,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: The paper addresses numerical question answering in financial documents using large language models and proposes an improved critic and calculator agent framework, outperforming prior state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with handling numerical data in financial documents, and traditional critic agents do not perform well when oracle labels are unavailable.

Method: The authors introduce an enhanced critic agent and a calculator agent to address these limitations, analyzing their interaction and synergy.

Result: The proposed agents outperform the previous state-of-the-art program-of-thought approach, demonstrating improved performance and safety.

Conclusion: The improved critic and calculator agents are effective for numerical QA in financial tasks, addressing the limitations of prior methods and opening new directions for interaction between intelligent agents.

Abstract: Large language models (LLMs) have shown impressive capabilities on numerous
natural language processing tasks. However, LLMs still struggle with numerical
question answering for financial documents that include tabular and textual
data. Recent works have showed the effectiveness of critic agents (i.e.,
self-correction) for this task given oracle labels. Building upon this
framework, this paper examines the effectiveness of the traditional critic
agent when oracle labels are not available, and show, through experiments, that
this critic agent's performance deteriorates in this scenario. With this in
mind, we present an improved critic agent, along with the calculator agent
which outperforms the previous state-of-the-art approach (program-of-thought)
and is safer. Furthermore, we investigate how our agents interact with each
other, and how this interaction affects their performance.

</details>


### [94] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
*Dror Kris Markus,Fabrizio Gilardi,Daria Stetsenko*

Main category: cs.CL

TL;DR: This paper studies over 100,000 AI-related papers to explore interdisciplinary and technical teams' integration of societal concerns into AI research, finding that computer science teams increasingly lead such efforts.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore whether interdisciplinary collaboration actually drives the integration of societal and ethical values into AI research.

Method: The study employs a classifier to identify societal content among over 100,000 AI papers from ArXiv (2014-2024) and analyzes societal concerns expressed in the research.

Result: Interdisciplinary teams produce societally-oriented research, but computer science-exclusive teams are increasingly addressing societal values and exploring domains like fairness, safety, and misinformation.

Conclusion: The findings challenge assumptions that interdisciplinary teams are primary drivers of societal AI research, raising questions about AI governance and the future contributions of social sciences and humanities.

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [95] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
*Muhammad Anwar,Mishca de Costa,Issam Hammad,Daniel Lau*

Main category: cs.CL

TL;DR: This paper develops a specialized language model for nuclear applications using materials from the Essential CANDU textbook, emphasizing cybersecurity and confidentiality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create domain-specific language models that meet strict cybersecurity and confidentiality standards for nuclear operations.

Method: A Transformer-based architecture was trained on a single GPU with a dataset derived exclusively from the Essential CANDU textbook.

Result: The model successfully captures specialized nuclear vocabulary but occasionally struggles with syntactic coherence. Promising early results highlight its utility and potential for improvement.

Conclusion: The feasibility of in-house large language models for specialized applications like nuclear is demonstrated, but further dataset expansion, preprocessing, and fine-tuning are needed for real-world readiness.

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [96] [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/abs/2506.08750)
*Muhammad Anwar,Daniel Lau,Mishca de Costa,Issam Hammad*

Main category: cs.CL

TL;DR: The paper addresses the challenge of utilizing unstructured nuclear industry text data for LLM applications by generating synthetic Q&A datasets.


<details>
  <summary>Details</summary>
Motivation: The scarcity and privacy challenges in the nuclear sector hinder the use of advanced LLMs, motivating the need for synthetic data solutions.

Method: The method involves using LLMs to extract key information from text, generate Q&A pairs, and evaluate dataset quality for training or fine-tuning.

Result: Synthetic data enables LLM applications in the nuclear domain by converting unstructured text into structured, usable data.

Conclusion: Synthetic data generation can improve knowledge sharing, information retrieval, and decision-making within the nuclear industry.

Abstract: The nuclear industry possesses a wealth of valuable information locked away
in unstructured text data. This data, however, is not readily usable for
advanced Large Language Model (LLM) applications that require clean, structured
question-answer pairs for tasks like model training, fine-tuning, and
evaluation. This paper explores how synthetic data generation can bridge this
gap, enabling the development of robust LLMs for the nuclear domain. We discuss
the challenges of data scarcity and privacy concerns inherent in the nuclear
industry and how synthetic data provides a solution by transforming existing
text data into usable Q&A pairs. This approach leverages LLMs to analyze text,
extract key information, generate relevant questions, and evaluate the quality
of the resulting synthetic dataset. By unlocking the potential of LLMs in the
nuclear industry, synthetic data can pave the way for improved information
retrieval, enhanced knowledge sharing, and more informed decision-making in
this critical sector.

</details>


### [97] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
*Pradyoth Hegde,Santosh Kesiraju,Jan Švec,Šimon Sedláček,Bolaji Yusuf,Oldřich Plchot,Deepak K T,Jan Černocký*

Main category: cs.CL

TL;DR: The paper examines in-context learning (ICL) for dialogue state tracking (DST), utilizing a k-nearest neighbor approach for demonstration retrieval and analyzing factors affecting its performance.


<details>
  <summary>Details</summary>
Motivation: To understand how in-context learning can enhance dialogue state tracking in large language models (LLMs).

Method: The study used sentence embeddings for demonstration retrieval, structured test samples in a template, and evaluated performance on three LLMs (OLMo-7B-instruct, Mistral-7B-Instruct, and Llama3.2-3B-Instruct) with the MultiWoZ2.4 dataset.

Result: The paper provides insights into the effectiveness of ICL for DST and identifies factors that influence its performance.

Conclusion: ICL has significant potential for DST when optimized through demonstration selection and prompt structuring, offering valuable understanding into how LLMs can be fine-tuned for such tasks.

Abstract: This study explores the application of in-context learning (ICL) to the
dialogue state tracking (DST) problem and investigates the factors that
influence its effectiveness. We use a sentence embedding based k-nearest
neighbour method to retrieve the suitable demonstrations for ICL. The selected
demonstrations, along with the test samples, are structured within a template
as input to the LLM. We then conduct a systematic study to analyse the impact
of factors related to demonstration selection and prompt context on DST
performance. This work is conducted using the MultiWoZ2.4 dataset and focuses
primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and
Llama3.2-3B-Instruct models. Our findings provide several useful insights on
in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [98] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
*Mishca de Costa,Muhammad Anwar,Dave Mercier,Mark Randall,Issam Hammad*

Main category: cs.CL

TL;DR: The paper proposes using function-calling large language models (LLMs) as an alternative to traditional NL-to-SQL approaches for querying nuclear power plant operational data to improve accuracy and maintainability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the risks of using NL-to-SQL for nuclear plant databases, such as difficulty in validating generated SQL queries and complexities due to legacy database structures.

Method: The method involves leveraging a set of pre-approved, purpose-specific functions that represent common use cases, instead of directly generating SQL queries. These functions encapsulate validated SQL logic and mitigate risks.

Result: The results show that the function-based approach improves the accuracy and maintainability of query generation and reduces the risks compared to direct NL-to-SQL methods.

Conclusion: The paper concludes that the function-based strategy ensures a balance between user accessibility and operational safety, providing a more robust framework for data retrieval in critical systems.

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [99] [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)
*Ahmed Hasanaath,Aisha Alansari,Ahmed Ashraf,Chafik Salmane,Hamzah Luqman,Saad Ezzini*

Main category: cs.CL

TL;DR: This study benchmarks reasoning abilities of large language models on Arabic NLP tasks using zero-shot, few-shot, and fine-tuning approaches, with notable performance improvements.


<details>
  <summary>Details</summary>
Motivation: Investigate the reasoning capabilities of LLMs on Arabic datasets, which are underexplored due to Arabic's unique linguistic features.

Method: Conduct experiments with DeepSeek models and other LLMs across fifteen Arabic NLP tasks using zero-shot, few-shot, and fine-tuning strategies.

Result: DeepSeek models demonstrate significant performance gains over GPT-4-mini, and selective few-shot examples and LoRA-based fine-tuning markedly improve outcomes.

Conclusion: The study highlights effective methodologies to enhance reasoning capabilities of LLMs on Arabic tasks, providing insights for further NLP improvements in morphologically-rich languages.

Abstract: Large language models (LLMs) have shown remarkable progress in reasoning
abilities and general natural language processing (NLP) tasks, yet their
performance on Arabic data, characterized by rich morphology, diverse dialects,
and complex script, remains underexplored. This paper presents a comprehensive
benchmarking study of multiple reasoning-focused LLMs, with a special emphasis
on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP
tasks. We experiment with various strategies, including zero-shot, few-shot,
and fine-tuning. This allows us to systematically evaluate performance on
datasets covering a range of applications to examine their capacity for
linguistic reasoning under different levels of complexity. Our experiments
reveal several key findings. First, carefully selecting just three in-context
examples delivers an average uplift of over 13 F1 points on classification
tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection
from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures
outperform a strong GPT o4-mini baseline by an average of 12 F1 points on
complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning
yields up to an additional 8 points in F1 and BLEU compared to equivalent
increases in model scale. The code is available at
https://anonymous.4open.science/r/AraReasoner41299

</details>


### [100] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
*Francisco Vargas,Alejandro González Coene,Gaston Escalante,Exequiel Lobón,Manuel Pulido*

Main category: cs.CL

TL;DR: This paper proposes a method to extract traffic accident information from legal documents using document segmentation and entity extraction, leveraging advanced multilingual and large language models.


<details>
  <summary>Details</summary>
Motivation: Extracting accurate data about traffic accidents from legal documents is essential for quantifying insurance costs, but the complex reasoning in court decisions makes this process challenging.

Method: A two-step procedure is used: segmenting documents using regular expressions or semantic vectorization with multilingual models, followed by entity extraction with fine-tuned large language models including LLaMA and GPT-4 Turbo.

Result: The vectorization-based methodology outperformed others with accuracy levels up to 86.1% using GPT-4 Turbo. Model finetuning significantly reduced hallucinations in LLaMA-2 70B, yielding 79.4% accuracy, while the base LLaMA-3 8B model achieved competitive results with 76.6%.

Conclusion: The proposed approach proves effective in extracting complex information from legal documents, with substantial improvements in accuracy, demonstrating the capabilities of modern language models like LLaMA and GPT-4 Turbo.

Abstract: The extraction of information about traffic accidents from legal documents is
crucial for quantifying insurance company costs. Extracting entities such as
percentages of physical and/or psychological disability and the involved
compensation amounts is a challenging process, even for experts, due to the
subtle arguments and reasoning in the court decision. A two-step procedure is
proposed: first, segmenting the document identifying the most relevant
segments, and then extracting the entities. For text segmentation, two
methodologies are compared: a classic method based on regular expressions and a
second approach that divides the document into blocks of n-tokens, which are
then vectorized using multilingual models for semantic searches
(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models
(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to
the selected segments for entity extraction. For the LLaMA models, fine-tuning
is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a
significant number of hallucinations in extractions which are an important
contention point for named entity extraction. This work shows that these
hallucinations are substantially reduced after finetuning the model. The
performance of the methodology based on segment vectorization and subsequent
use of LLMs significantly surpasses the classic method which achieves an
accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning
achieves the highest accuracy 79.4%, surpassing its base version 61.7%.
Notably, the base LLaMA-3 8B model already performs comparably to the finetuned
LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model
development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [101] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)
*Flavio D'Intino,Hans-Peter Hutter*

Main category: cs.CL

TL;DR: This paper introduces the SRB-300 dataset consisting of 300 hours of real-world Swiss German speech, enabling improved transcription models through fine-tuning OpenAI Whisper.


<details>
  <summary>Details</summary>
Motivation: Swiss German transcription struggles with spontaneous speech due to its lack of standardized written forms and diverse dialects.

Method: The researchers collected real-world speech data from 39 radio and TV stations, covering all major Swiss dialects. OpenAI Whisper models were fine-tuned on this dataset.

Result: Fine-tuned models achieved significant improvements with WER reductions of 19–33% and BLEU score increases of 8–40%. The best model, large-v3, achieved a WER of 17.1% and BLEU score of 74.8.

Conclusion: The dataset and methodology mark substantial progress in developing robust STT systems for Swiss German, addressing limitations of existing solutions.

Abstract: Swiss German is a low-resource language represented by diverse dialects that
differ significantly from Standard German and from each other, lacking a
standardized written form. As a result, transcribing Swiss German involves
translating into Standard German. Existing datasets have been collected in
controlled environments, yielding effective speech-to-text (STT) models, but
these models struggle with spontaneous conversational speech.
  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour
annotated speech corpus featuring real-world long-audio recordings from 39
Swiss German radio and TV stations. It captures spontaneous speech across all
major Swiss dialects recorded in various realistic environments and overcomes
the limitation of prior sentence-level corpora.
  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,
achieving notable enhancements over previous zero-shot performance metrics.
Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores
increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a
WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for
developing effective and robust STT systems for Swiss German and other
low-resource languages in real-world contexts.

</details>


### [102] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
*Danush Khanna,Krishna Kumar,Basab Ghosh,Vinija Jain,Vasu Sharma,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: This paper investigates geometric vulnerabilities in aligning LLMs to safety, introduces an adversarial benchmark called ALKALI, and proposes a mitigation framework named GRACE to combat latent camouflage in unsafe completions.


<details>
  <summary>Details</summary>
Motivation: Adversarial threats against large language models (LLMs) are evolving rapidly, exploiting latent geometric vulnerabilities and bypassing surface-level defenses, highlighting the need for more robust approaches.

Method: The authors developed a benchmark, ALKALI, with 9,000 adversarial prompts and proposed GRACE, a latent geometry-aware enhancement framework using latent space regularization and contrastive learning.

Result: The evaluation of 21 LLMs showed high attack success rates across various models. GRACE achieved up to a 39% reduction in adversarial success rates.

Conclusion: Geometric vulnerabilities in LLMs need urgent attention. GRACE addresses latent camouflage by reshaping internal geometry and AVQI provides insights into latent alignment failures, making strides toward more robust model alignment.

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [103] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
*Hiba Khey,Amine Lakhder,Salma Rouichi,Imane El Ghabi,Kamal Hejjaoui,Younes En-nahli,Fahd Kalloubi,Moez Amri*

Main category: cs.CL

TL;DR: The paper introduces PlantBert, a language model tailored for extracting structured knowledge from plant stress-response literature, especially for lentils, and emphasizes its utility in agricultural NLP.


<details>
  <summary>Details</summary>
Motivation: Plant science has lacked domain-adapted NLP tools similar to those in biomedical fields, necessitating a focused approach to handle plant-specific literature challenges.

Method: Leveraging the DeBERTa architecture, the model is fine-tuned on a curated corpus of annotated plant literature, incorporating rule-enhanced post-processing and ontology-based entity normalization.

Result: PlantBert effectively recognizes entities and relationships relevant to plant stress responses, demonstrating robust generalization even in low-resource domains.

Conclusion: PlantBert bridges a critical gap in plant science NLP, providing a scalable framework to enhance research in plant genomics, phenomics, and agronomy through better information extraction.

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.

</details>


### [104] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
*Elias Horner,Cristinel Mateis,Guido Governatori,Agata Ciabattoni*

Main category: cs.CL

TL;DR: The paper presents a method using large language models (LLMs) to automatically analyze legal texts and formalize them in Defeasible Deontic Logic (DDL).


<details>
  <summary>Details</summary>
Motivation: To address the challenge of processing and formalizing complex normative legal language automatically, thereby aiding scalable legal informatics.

Method: Developed a structured pipeline that uses LLMs to segment legal texts, extract deontic rules, and assess coherence. The approach tested different LLM configurations like prompt engineering, fine-tuning, and multi-stage pipelines.

Result: Empirical tests with Australian telecom legal norms showed strong alignment between machine-generated formalizations and expert-created ones, with the effectiveness of optimized LLM prompting.

Conclusion: LLMs, when effectively configured, hold significant promise for transforming legal texts into scalable, formal representations.

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [105] [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/abs/2506.08907)
*Antonios Dimakis,John Pavlopoulos,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: This paper introduces a method combining linguistic rules and large language models (LLMs) with few-shot prompting to normalize Greek dialects without parallel data for downstream use in NLP.


<details>
  <summary>Details</summary>
Motivation: Natural language systems struggle with low-resource languages, like dialects, that are not easily supported by standard tools.

Method: A combination of rule-based linguistic transformations and large language models with few-shot prompting is proposed for dialect-to-standard normalization.

Result: The approach was applied to Greek dialect proverbs, evaluated using human annotators, and results highlighted the capability to observe deeper semantics beyond superficial linguistic traits.

Conclusion: The proposed method overcomes reliance on orthographic artifacts, enabling access to deeper-level linguistic insights and improving linguistic tool utility for low-resource languages.

Abstract: Natural language understanding systems struggle with low-resource languages,
including many dialects of high-resource ones. Dialect-to-standard
normalization attempts to tackle this issue by transforming dialectal text so
that it can be used by standard-language tools downstream. In this study, we
tackle this task by introducing a new normalization method that combines
rule-based linguistically informed transformations and large language models
(LLMs) with targeted few-shot prompting, without requiring any parallel data.
We implement our method for Greek dialects and apply it on a dataset of
regional proverbs, evaluating the outputs using human annotators. We then use
this dataset to conduct downstream experiments, finding that previous results
regarding these proverbs relied solely on superficial linguistic information,
including orthographic artifacts, while new observations can still be made
through the remaining semantics.

</details>


### [106] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
*Zeyu Leo Liu,Greg Durrett,Eunsol Choi*

Main category: cs.CL

TL;DR: PropMEND is a hypernetwork-based method for propagating injected knowledge in LLMs, achieving better reasoning ability and multi-hop question answering.


<details>
  <summary>Details</summary>
Motivation: Current methods inject knowledge into large language models, but struggle to enable reasoning and multi-hop question answering with the added knowledge.

Method: PropMEND uses meta-learning to modify gradients during language modeling loss to propagate the injected knowledge, enabling reasoning over multi-hop questions.

Result: PropMEND achieves almost 2x higher accuracy compared to existing methods on multi-hop questions within the RippleEdit dataset, and shows strong generalization on unseen entity-relation pairs in the Controlled RippleEdit dataset.

Conclusion: PropMEND improves knowledge reasoning and propagation in LLMs but highlights a need for further study to generalize propagation across diverse relations.

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [107] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
*Andrew Shin*

Main category: cs.CL

TL;DR: This paper shows that it's possible to train a high-performing mathematical reasoning model using a single RTX 3080 Ti GPU, challenging the notion that massive computational resources are necessary.


<details>
  <summary>Details</summary>
Motivation: The motivation is to make high-performance AI research more accessible by reducing the prohibitive computational and hardware costs typically associated with training large language models.

Method: The authors combine reinforcement learning and memory optimization techniques to train a 1.5B parameter mathematical reasoning model on a single average gaming GPU (RTX 3080 Ti with 16GB memory).

Result: The trained model achieved comparable or superior performance on mathematical reasoning benchmarks compared to much larger models operating in resource-heavy environments.

Conclusion: This research demonstrates that state-of-the-art mathematical reasoning can be achieved without massive computational infrastructure, potentially democratizing AI development and access.

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [108] [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)
*Qinggang Zhang,Zhishang Xiang,Yilin Xiao,Le Wang,Junhui Li,Xinrun Wang,Jinsong Su*

Main category: cs.CL

TL;DR: The paper introduces FaithfulRAG, a framework designed to improve faithfulness in LLMs during knowledge-intensive tasks by addressing conflicts between model's inherent knowledge and retrieved context.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the unfaithfulness issues in LLMs that arise when handling conflicting knowledge between the parametric knowledge and retrieved context, leading to inconsistent or misleading outputs.

Method: The proposed method, FaithfulRAG, identifies knowledge conflicts at the fact level and employs a self-thinking process to integrate and reason about conflicting information before response generation.

Result: Extensive experiments show that FaithfulRAG surpasses state-of-the-art methods in terms of faithfulness and response quality.

Conclusion: FaithfulRAG addresses critical shortcomings in faithful RAG methods by allowing LLMs to reason and handle conflicts effectively, achieving improved performance without suppressing the model's internal knowledge structures.

Abstract: Large language models (LLMs) augmented with retrieval systems have
demonstrated significant potential in handling knowledge-intensive tasks.
However, these models often struggle with unfaithfulness issues, generating
outputs that either ignore the retrieved context or inconsistently blend it
with the LLM`s parametric knowledge. This issue is particularly severe in cases
of knowledge conflict, where the retrieved context conflicts with the model`s
parametric knowledge. While existing faithful RAG approaches enforce strict
context adherence through well-designed prompts or modified decoding
strategies, our analysis reveals a critical limitation: they achieve
faithfulness by forcibly suppressing the model`s parametric knowledge, which
undermines the model`s internal knowledge structure and increases the risk of
misinterpreting the context. To this end, this paper proposes FaithfulRAG, a
novel framework that resolves knowledge conflicts by explicitly modeling
discrepancies between the model`s parametric knowledge and retrieved context.
Specifically, FaithfulRAG identifies conflicting knowledge at the fact level
and designs a self-thinking process, allowing LLMs to reason about and
integrate conflicting facts before generating responses. Extensive experiments
demonstrate that our method outperforms state-of-the-art methods. The code is
available at https:// github.com/DeepLearnXMU/Faithful-RAG

</details>


### [109] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
*Clara Lachenmaier,Judith Sieker,Sina Zarrieß*

Main category: cs.CL

TL;DR: The paper examines how large language models (LLMs) handle conversational grounding in politically sensitive scenarios, analyzing their ability to manage misinformation and resolve discrepancies in user beliefs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how LLMs manage common ground and address misinformation, especially in the politically sensitive domain where grounding failures and misinformation can have serious impacts.

Method: The study evaluates LLMs' ability to answer direct and loaded questions, assessing their active grounding capabilities against false presuppositions while considering their knowledge levels and political bias.

Result: The findings reveal significant shortcomings in LLMs' ability to effectively address false user beliefs and engage in grounding, especially when political misinformation is present.

Conclusion: LLMs face critical challenges in mitigating misinformation in political discourse, underlining concerns about their adequacy in handling sensitive conversational grounding scenarios.

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [110] [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/abs/2506.08972)
*Yuan Guo,Tingjia Miao,Zheng Wu,Pengzhou Cheng,Ming Zhou,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: This paper introduces UI-NEXUS, a new benchmark for evaluating mobile agents on complex compositional tasks, and proposes AGENT-NEXUS to improve their task success rates.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing mobile agents, which struggle to generalize from atomic to compositional tasks necessary for real-world applications.

Method: The paper presents the UI-NEXUS benchmark for testing mobile agents in 50 different app environments and proposes AGENT-NEXUS, a scheduling system that decomposes complex tasks into atomic subtasks for better execution.

Result: UI-NEXUS reveals significant challenges for existing agents, highlighting their inefficiencies in compositional tasks. AGENT-NEXUS improves task success rates by 24% to 40% without adding notable inference overhead.

Conclusion: AGENT-NEXUS enhances the compositional task-handling capabilities of mobile agents, addressing the generalization gap and setting a foundation for further development in this domain.

Abstract: Autonomous agents powered by multimodal large language models have been
developed to facilitate task execution on mobile devices. However, prior work
has predominantly focused on atomic tasks -- such as shot-chain execution tasks
and single-screen grounding tasks -- while overlooking the generalization to
compositional tasks, which are indispensable for real-world applications. This
work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile
agents on three categories of compositional operations: Simple Concatenation,
Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in
20 fully controllable local utility app environments, as well as 30 online
Chinese and English service apps. It comprises 100 interactive task templates
with an average optimal step count of 14.05. Experimental results across a
range of mobile agents with agentic workflow or agent-as-a-model show that
UI-NEXUS presents significant challenges. Specifically, existing agents
generally struggle to balance performance and efficiency, exhibiting
representative failure modes such as under-execution, over-execution, and
attention drift, causing visible atomic-to-compositional generalization gap.
Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient
scheduling system to tackle compositional mobile tasks. AGENT-NEXUS
extrapolates the abilities of existing mobile agents by dynamically decomposing
long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS
achieves 24% to 40% task success rate improvement for existing mobile agents on
compositional operation tasks within the UI-NEXUS benchmark without
significantly sacrificing inference overhead. The demo video, dataset, and code
are available on the project page at https://ui-nexus.github.io.

</details>


### [111] [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/abs/2506.08981)
*Satu Hopponen,Tomi Kinnunen,Alexandre Nikolaev,Rosa González Hautamäki,Lauri Tavi,Einar Meister*

Main category: cs.CL

TL;DR: The FROST-EMA corpus is introduced, consisting of bilingual speakers producing speech in different language scenarios, enabling phonetic and technological research.


<details>
  <summary>Details</summary>
Motivation: To study language variability and its implications on technological systems and articulatory patterns.

Method: Created a dataset with 18 bilingual Finnish-Russian speakers producing speech in L1, L2, and imitated L2, accompanied by case studies.

Result: The dataset demonstrated impacts of L2 and fake accents on speaker verification and helped reveal speaker-specific articulatory patterns.

Conclusion: FROST-EMA enables deeper exploration of speech variability, offering insights for both linguistic and technological applications.

Abstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of
Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers,
who produced speech in their native language (L1), second language (L2), and
imitated L2 (fake foreign accent). The new corpus enables research into
language variability from phonetic and technological points of view.
Accordingly, we include two preliminary case studies to demonstrate both
perspectives. The first case study explores the impact of L2 and imitated L2 on
the performance of an automatic speaker verification system, while the second
illustrates the articulatory patterns of one speaker in L1, L2, and a fake
accent.

</details>


### [112] [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/abs/2506.08986)
*Yuejiao Wang,Xianmin Gong,Xixin Wu,Patrick Wong,Hoi-lam Helene Fung,Man Wai Mak,Helen Meng*

Main category: cs.CL

TL;DR: The paper presents a novel fMRI task for early detection of cognitive decline in older adults, achieving promising accuracy using machine-learning models.


<details>
  <summary>Details</summary>
Motivation: To develop an effective tool for early detection of neurocognitive disorder, which is crucial for timely intervention among aging populations.

Method: A novel naturalistic language-related fMRI task was tested on 97 older adults in Hong Kong, using machine-learning models to classify cognitive status based on fMRI and demographic data.

Result: The classification models achieved an average area under the curve (AUC) of 0.86, with key features localized to brain regions related to language processing.

Conclusion: The proposed language-related fMRI task shows strong potential for early detection of cognition-related decline in aging populations.

Abstract: Early detection is crucial for timely intervention aimed at preventing and
slowing the progression of neurocognitive disorder (NCD), a common and
significant health problem among the aging population. Recent evidence has
suggested that language-related functional magnetic resonance imaging (fMRI)
may be a promising approach for detecting cognitive decline and early NCD. In
this paper, we proposed a novel, naturalistic language-related fMRI task for
this purpose. We examined the effectiveness of this task among 97 non-demented
Chinese older adults from Hong Kong. The results showed that machine-learning
classification models based on fMRI features extracted from the task and
demographics (age, gender, and education year) achieved an average area under
the curve of 0.86 when classifying participants' cognitive status (labeled as
NORMAL vs DECLINE based on their scores on a standard neurcognitive test).
Feature localization revealed that the fMRI features most frequently selected
by the data-driven approach came primarily from brain regions associated with
language processing, such as the superior temporal gyrus, middle temporal
gyrus, and right cerebellum. The study demonstrated the potential of the
naturalistic language-related fMRI task for early detection of aging-related
cognitive decline and NCD.

</details>


### [113] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
*Theo Zhang,Madurya Suresh,Anne S. Warlaumont,Kasia Hitczenko,Alejandrina Cristia,Margaret Cychosz*

Main category: cs.CL

TL;DR: This paper addresses the limitations in processing child speech by introducing the novel SpeechMaturity dataset and training transformer models to classify child vocalizations with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Child speech poses challenges for speech technology systems due to limited training data and speech characteristics. The authors aim to improve the performance of classification systems by leveraging a more ecologically representative dataset.

Method: The authors developed the SpeechMaturity dataset containing 242,004 labeled child vocalizations across diverse cultures and languages. Transformer models were trained to classify cry, laughter, mature speech, and immature speech.

Result: The models trained on the new dataset outperformed state-of-the-art models trained on older datasets. They achieved classification accuracy comparable to humans and demonstrated robustness across diverse geographical contexts.

Conclusion: Using the SpeechMaturity dataset significantly enhances the ability of models to classify child vocalizations effectively, providing a crucial step forward for speech technologies tailored to children.

Abstract: Speech technology systems struggle with many downstream tasks for child
speech due to small training corpora and the difficulties that child speech
pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer
models to address a fundamental classification task: identifying child
vocalizations. Unlike previous corpora, our dataset captures maximally
ecologically-valid child vocalizations across an unprecedented sample,
comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,
Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004
labeled vocalizations, magnitudes larger than previous work. Models were
trained to distinguish between cry, laughter, mature (consonant+vowel), and
immature speech (just consonant or vowel). Models trained on the dataset
outperform state-of-the-art models trained on previous datasets, achieved
classification accuracy comparable to humans, and were robust across rural and
urban settings.

</details>


### [114] [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003)
*Lei Zhang,Jiaxi Yang,Min Yang,Jian Yang,Mouxiang Chen,Jiajun Zhang,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.CL

TL;DR: This paper introduces a data synthesis framework, SWE-Flow, focusing on Test-Driven Development (TDD) to generate incremental development steps. It uses runtime dependency analysis and provides benchmarks for TDD coding.


<details>
  <summary>Details</summary>
Motivation: The motivation in this paper is to address the lack of structured data for incremental development steps in Test-Driven Development (TDD), moving beyond human-generated issue tracking.

Method: The authors developed a framework called SWE-Flow, which builds a Runtime Dependency Graph (RDG) to track function interactions. Using RDG, it generates partial codebases, unit tests, and task-vs-modification schedules.

Result: 16,061 training and 2,020 test instances were created from GitHub projects into the SWE-Flow-Eval benchmark. Experiments show improved performance with models fine-tuned on this dataset.

Conclusion: SWE-Flow proves effective in advancing TDD-based coding practices and data synthesis. It provides a valuable dataset and benchmarks, and the authors release all related resources to the public.

Abstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in
Test-Driven Development (TDD). Unlike existing software engineering data that
rely on human-submitted issues, **SWE-Flow** automatically infers incremental
development steps directly from unit tests, which inherently encapsulate
high-level requirements. The core of **SWE-Flow** is the construction of a
Runtime Dependency Graph (RDG), which precisely captures function interactions,
enabling the generation of a structured, step-by-step *development schedule*.
At each step, **SWE-Flow** produces a partial codebase, the corresponding unit
tests, and the necessary code modifications, resulting in fully verifiable TDD
tasks. With this approach, we generated 16,061 training instances and 2,020
test instances from real-world GitHub projects, creating the **SWE-Flow-Eval**
benchmark. Our experiments show that fine-tuning open model on this dataset
significantly improves performance in TDD-based coding. To facilitate further
research, we release all code, datasets, models, and Docker images at
[Github](https://github.com/Hambaobao/SWE-Flow).

</details>


### [115] [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/abs/2506.09009)
*Hakyung Sung,Gyu-Ho Shin,Chanyoung Lee,You Kyung Sung,Boo Kyung Jung*

Main category: cs.CL

TL;DR: The paper introduces a semi-automated framework for analyzing L2-Korean morphosyntactic structures and assesses its impact on NLP model accuracy.


<details>
  <summary>Details</summary>
Motivation: Expand the capabilities and accuracy of NLP tools for analyzing second-language Korean texts by improving annotation methods.

Method: The study developed a framework for aligning XPOS sequences with UPOS categories and expanded the L2-Korean corpus with annotations. The impact is evaluated through fine-tuning models using datasets with and without alignments.

Result: The aligned dataset led to better annotation consistency and improved accuracy in morphosyntactic tagging and dependency parsing, especially for limited data scenarios.

Conclusion: The alignment method enhances NLP performance and consistency in L2-Korean language analysis.

Abstract: The present study extends recent work on Universal Dependencies annotations
for second-language (L2) Korean by introducing a semi-automated framework that
identifies morphosyntactic constructions from XPOS sequences and aligns those
constructions with corresponding UPOS categories. We also broaden the existing
L2-Korean corpus by annotating 2,998 new sentences from argumentative essays.
To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean
morphosyntactic analysis models on datasets both with and without these
alignments, using two NLP toolkits. Our results indicate that the aligned
dataset not only improves consistency across annotation layers but also
enhances morphosyntactic tagging and dependency-parsing accuracy, particularly
in cases of limited annotated data.

</details>


### [116] [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/abs/2506.09014)
*Jianing Qi,Xi Ye,Hao Tang,Zhigang Zhu,Eunsol Choi*

Main category: cs.CL

TL;DR: The paper introduces a method called Sample Set Aggregator (SSA) to improve the aggregation of multiple sampled answers from large language models for better reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of large language models (LLMs) by better leveraging multiple test-time samples, especially when aggregating responses in complex reasoning tasks.

Method: A compact language model called Sample Set Aggregator (SSA) is trained using reinforcement learning to process and aggregate multiple sampled answers into a final response for improved accuracy.

Result: SSA outperforms existing test-time methods for reasoning tasks, demonstrating superior generalization across various model scales, base families, and sample set sizes.

Conclusion: Separating the generation and aggregation of answers via SSA enables the efficient utilization of outputs from black-box language models, showing scalability and generalization advantages.

Abstract: Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.

</details>


### [117] [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/abs/2506.09021)
*Hakyung Sung,Karla Csuros,Min-Chang Sung*

Main category: cs.CL

TL;DR: This study compares human and LLM proofreading on second language writings, finding both improve intelligibility through enhanced lexical features, with LLMs showing consistency and generativity.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of human and LLM proofreading on improving second language writings' intelligibility and coherence.

Method: Comparison of lexical and syntactic interventions made by humans and three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b) on identical second language texts.

Result: Both human and LLM proofreading enhance bigram lexical features, with LLM proofreading more generative, using diverse vocabulary and structural changes. Consistency in outcomes was observed across LLMs.

Conclusion: LLMs offer more generative lexical and syntactic revisions than humans, with consistent proofreading results across models for improving second language intelligibility.

Abstract: This study examines the lexical and syntactic interventions of human and LLM
proofreading aimed at improving overall intelligibility in identical second
language writings, and evaluates the consistency of outcomes across three LLMs
(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and
LLM proofreading enhance bigram lexical features, which may contribute to
better coherence and contextual connectedness between adjacent words. However,
LLM proofreading exhibits a more generative approach, extensively reworking
vocabulary and sentence structures, such as employing more diverse and
sophisticated vocabulary and incorporating a greater number of adjective
modifiers in noun phrases. The proofreading outcomes are highly consistent in
major lexical and syntactic features across the three models.

</details>


### [118] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
*Haozhen Zhang,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: This paper introduces Router-R1, a reinforcement learning framework for dynamically routing and aggregating outputs from multiple large language models (LLMs) to tackle complex tasks, outperforming existing methods and managing performance-cost trade-offs efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing LLM routers are limited by their approach of one-to-one mappings for query assignments, restricting their effectiveness for complex tasks that require multi-LLM collaboration.

Method: Router-R1 is based on reinforcement learning, where the router itself is an LLM capable of sequentially making decisions by integrating reasoning and model invocation. It uses lightweight rule-based rewards for task performance and cost optimization. The approach is generalizable due to reliance on simple model descriptors.

Result: Router-R1 achieved superior performance on seven general and multi-hop QA benchmarks compared to strong existing baselines while maintaining effective cost management and generalization capabilities.

Conclusion: The proposed Router-R1 framework demonstrates the potential of reinforcement learning to manage multi-LLM routing and aggregation, optimizing performance while balancing costs, and exhibits strong generalization to new scenarios.

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


### [119] [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)
*Yaniv Nikankin,Dana Arad,Yossi Gandelsman,Yonatan Belinkov*

Main category: cs.CL

TL;DR: The paper analyzes the performance disparity of Vision-Language models between visual and textual data processing and introduces an intervention to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to investigate and address the accuracy gap in Vision-Language models when processing visual inputs compared to analogous text tasks.

Method: The study involves identifying computational sub-graphs ('circuits') in VLMs across modalities to understand the source of incongruity and proposes patching later-layer visual data representations back into earlier layers.

Result: Patching visual data representations to earlier layers significantly improves accuracy, closing approximately one-third of the performance gap on average across tasks and models.

Conclusion: The paper provides insight into the multi-modal performance limitation in VLMs and proposes a simple, training-free intervention to reduce the gap.

Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions
on visual inputs (e.g., counting objects in an image), yet demonstrate higher
accuracies when performing an analogous task on text (e.g., counting words in a
text). We investigate this accuracy gap by identifying and comparing the
\textit{circuits} - the task-specific computational sub-graphs - in different
modalities. We show that while circuits are largely disjoint between
modalities, they implement relatively similar functionalities: the differences
lie primarily in processing modality-specific data positions (an image or a
text sequence). Zooming in on the image data representations, we observe they
become aligned with the higher-performing analogous textual representations
only towards later layers, too late in processing to effectively influence
subsequent positions. To overcome this, we patch the representations of visual
data tokens from later layers back into earlier layers. In experiments with
multiple tasks and models, this simple intervention closes a third of the
performance gap between the modalities, on average. Our analysis sheds light on
the multi-modal performance gap in VLMs and suggests a training-free approach
for reducing it.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [120] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/abs/2506.08048)
*Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou*

Main category: cs.CV

TL;DR: The paper introduces a data-driven algorithm for AR-guided surgeries, enhancing accuracy and efficiency in deformation modeling and surgeon interaction.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiency and inaccuracies in existing deformation modeling for AR-guided surgeries, especially for handling large anatomical changes.

Method: Proposes a data-driven biomechanics algorithm paired with a surgeon-interactive framework to refine preoperative model alignment through human-in-the-loop corrections.

Result: The algorithm achieves a mean target registration error of 3.42 mm or 2.78 mm with surgeon input, surpassing current volumetric accuracy standards.

Conclusion: The framework proves effective for delivering accurate, real-time deformation modeling with enhanced collaboration between surgeons and algorithms, offering safer surgery systems.

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [121] [ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.08052)
*Yongkang Li,Kaixin Xiong,Xiangyu Guo,Fang Li,Sixu Yan,Gangwei Xu,Lijun Zhou,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: The paper introduces ReCogDrive, a new system for end-to-end autonomous driving that integrates Vision-Language Models (VLMs) with a diffusion-based planner to mitigate performance issues in rare and complex scenarios.


<details>
  <summary>Details</summary>
Motivation: End-to-end autonomous driving struggles in rare and long-tail scenarios. Existing approaches leveraging Vision-Language Models face challenges such as domain gaps and limitations in handling continuous action spaces effectively.

Method: ReCogDrive adopts a three-stage training paradigm: (1) training VLMs with driving-specific datasets, (2) employing a diffusion-based planner for imitation learning, and (3) fine-tuning the planner using reinforcement learning with the NAVSIM simulator.

Result: The approach achieves a new state-of-the-art performance on the NAVSIM benchmark with a Planning Dynamism Score (PDMS) of 89.6, surpassing previous vision-only methods by 5.6 PDMS.

Conclusion: ReCogDrive effectively integrates VLMs with diffusion-based planning to bridge domain discrepancies and generate safer, human-like driving trajectories, setting a new performance standard in autonomous driving.

Abstract: Although end-to-end autonomous driving has made remarkable progress, its
performance degrades significantly in rare and long-tail scenarios. Recent
approaches attempt to address this challenge by leveraging the rich world
knowledge of Vision-Language Models (VLMs), but these methods suffer from
several limitations: (1) a significant domain gap between the pre-training data
of VLMs and real-world driving data, (2) a dimensionality mismatch between the
discrete language space and the continuous action space, and (3) imitation
learning tends to capture the average behavior present in the dataset, which
may be suboptimal even dangerous. In this paper, we propose ReCogDrive, an
autonomous driving system that integrates VLMs with diffusion planner, which
adopts a three-stage paradigm for training. In the first stage, we use a
large-scale driving question-answering datasets to train the VLMs, mitigating
the domain discrepancy between generic content and real-world driving
scenarios. In the second stage, we employ a diffusion-based planner to perform
imitation learning, mapping representations from the latent language space to
continuous driving actions. Finally, we fine-tune the diffusion planner using
reinforcement learning with NAVSIM non-reactive simulator, enabling the model
to generate safer, more human-like driving trajectories. We evaluate our
approach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6
and setting a new state-of-the-art that surpasses the previous vision-only SOTA
by 5.6 PDMS.

</details>


### [122] [CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems](https://arxiv.org/abs/2506.08071)
*Aniket Rege,Zinnia Nie,Mahesh Ramesh,Unmesh Raskar,Zhuoran Yu,Aditya Kusupati,Yong Jae Lee,Ramya Korlakai Vinayak*

Main category: cs.CV

TL;DR: The paper presents CuRe, a benchmarking suite to evaluate cultural representativeness in text-to-image (T2I) systems, highlighting the biases in existing models towards Global North cultures.


<details>
  <summary>Details</summary>
Motivation: Identify and address biases in T2I systems that are skewed toward Amero- and Euro-centric perspectives, underrepresenting cultures of the Global South.

Method: The authors introduced CuRe, a dataset with a novel categorical hierarchy based on Wikimedia knowledge graph to analyze cultural biases in T2I systems. CuRe evaluates systems by examining the effect of increasing text conditioning informativeness and measures correlation to human judgments.

Result: CuRe demonstrated strong correlations with human judgments in assessing perceptual similarity, image-text alignment, and cultural diversity across various image encoders, vision-language models, and T2I systems.

Conclusion: CuRe is a scalable, open-source benchmarking suite that offers a robust and cultural sensitivity-focused way to evaluate T2I systems, addressing the need for diverse representation in AI models.

Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is
heavily Amero and Euro-centric, underrepresenting the cultures of the Global
South. To analyze these biases, we introduce CuRe, a novel and scalable
benchmarking and scoring suite for cultural representativeness that leverages
the marginal utility of attribute specification to T2I systems as a proxy for
human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy
built from the crowdsourced Wikimedia knowledge graph, with 300 cultural
artifacts across 32 cultural subcategories grouped into six broad cultural axes
(food, art, fashion, architecture, celebrations, and people). Our dataset's
categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing
their response to increasing the informativeness of text conditioning, enabling
fine-grained cultural comparisons. We empirically observe much stronger
correlations of our class of scorers to human judgments of perceptual
similarity, image-text alignment, and cultural diversity across image encoders
(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,
Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three
variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,
and DALL-E 3. The code and dataset is open-sourced and available at
https://aniketrege.github.io/cure/.

</details>


### [123] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/abs/2506.08137)
*Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup*

Main category: cs.CV

TL;DR: The paper introduces IGraSS, an iterative method combining image segmentation and graph analysis to improve canal network identification from satellite data.


<details>
  <summary>Details</summary>
Motivation: Accurate mapping of canal networks is vital for efficient water resource management, but ground truth annotation often lacks completeness or quality.

Method: The authors propose IGraSS, a framework that integrates a semantic segmentation model using satellite imagery and a graph-based refinement module to iteratively improve data accuracy.

Result: The IGraSS framework reduces unreachable canal segments from 18% to 3%, enhancing canal identification performance. Furthermore, its approach is generalized to road navigation networks.

Conclusion: IGraSS provides a reliable solution for refining noisy ground truth and improving infrastructure mapping using remote sensing, emphasizing its flexibility and effectiveness on multiple network types.

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [124] [Spectral Domain Neural Reconstruction for Passband FMCW Radars](https://arxiv.org/abs/2506.08163)
*Harshvardhan Takawale,Nirupam Roy*

Main category: cs.CV

TL;DR: SpINRv2 is a neural framework for accurate volumetric reconstruction using FMCW radar, addressing phase aliasing and sub-bin ambiguity problems.


<details>
  <summary>Details</summary>
Motivation: Improve FMCW radar-based 3D imaging methods, especially under high frequencies, by addressing ambiguities and adopting efficient learning models.

Method: Introduces a frequency-domain forward model and implicit neural representation (INR), along with regularizations for sparsity and smoothness.

Result: SpINRv2 outperformed both classical and learning-based methods for 3D imaging, particularly in high-frequency regimes.

Conclusion: The study establishes SpINRv2 as a benchmark for neural radar-based 3D imaging by improving spectral fidelity and computational efficiency.

Abstract: We present SpINRv2, a neural framework for high-fidelity volumetric
reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.
Extending our prior work (SpINR), this version introduces enhancements that
allow accurate learning under high start frequencies-where phase aliasing and
sub-bin ambiguity become prominent. Our core contribution is a fully
differentiable frequency-domain forward model that captures the complex radar
response using closed-form synthesis, paired with an implicit neural
representation (INR) for continuous volumetric scene modeling. Unlike
time-domain baselines, SpINRv2 directly supervises the complex frequency
spectrum, preserving spectral fidelity while drastically reducing computational
overhead. Additionally, we introduce sparsity and smoothness regularization to
disambiguate sub-bin ambiguities that arise at fine range resolutions.
Experimental results show that SpINRv2 significantly outperforms both classical
and learning-based baselines, especially under high-frequency regimes,
establishing a new benchmark for neural radar-based 3D imaging.

</details>


### [125] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: The paper proposes an AI method to model and personalize surgeon-specific styles in robotic surgery while addressing privacy concerns.


<details>
  <summary>Details</summary>
Motivation: Surgeons have unique operating styles due to variations in training, experience, and motor behavior, but current AI systems often overlook this personalization aspect.

Method: The proposed method uses a discrete diffusion framework coupled with a vision-language-action (VLA) pipeline to predict surgical gestures, incorporating multimodal inputs such as video, language, and privacy-aware surgeon embeddings.

Result: The method performs well on the JIGSAWS dataset by accurately reconstructing gesture sequences and identifying unique motion fingerprints for surgeons. However, more expressive embeddings increase the risk of identity leakage.

Conclusion: While personalized models enhance task performance, it is crucial to manage the trade-off between personalization and privacy risks in surgical modeling approaches.

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [126] [Open World Scene Graph Generation using Vision Language Models](https://arxiv.org/abs/2506.08189)
*Amartya Dutta,Kazi Sajeed Mehrab,Medha Sawhney,Abhilash Neog,Mridul Khurana,Sepideh Fatemi,Aanish Pradhan,M. Maruf,Ismini Lourentzou,Arka Daw,Anuj Karpatne*

Main category: cs.CV

TL;DR: The paper introduces Open-World Scene-Graph Generation, a training-free framework leveraging Vision Language Models (VLMs) for relational understanding without additional learning.


<details>
  <summary>Details</summary>
Motivation: Traditional Scene-Graph Generation methods struggle with open-world settings involving novel objects and relationships, and rely on dataset-specific supervision, limiting their generalizability.

Method: The method reframes SGG as a zero-shot structured-reasoning problem by utilizing multimodal prompting, embedding alignment, and a lightweight pair-refinement strategy within pretrained VLMs.

Result: The model outperformed benchmarks on Visual Genome, Open Images V6, and Panoptic Scene Graph datasets, showcasing its ability to handle unseen object vocabularies and relation sets effectively.

Conclusion: Pretrained VLMs can successfully generate scene graphs and understand relationships in an open-world context without the need for task-specific training.

Abstract: Scene-Graph Generation (SGG) seeks to recognize objects in an image and
distill their salient pairwise relationships. Most methods depend on
dataset-specific supervision to learn the variety of interactions, restricting
their usefulness in open-world settings, involving novel objects and/or
relations. Even methods that leverage large Vision Language Models (VLMs)
typically require benchmark-specific fine-tuning. We introduce Open-World SGG,
a training-free, efficient, model-agnostic framework that taps directly into
the pretrained knowledge of VLMs to produce scene graphs with zero additional
learning. Casting SGG as a zero-shot structured-reasoning problem, our method
combines multimodal prompting, embedding alignment, and a lightweight
pair-refinement strategy, enabling inference over unseen object vocabularies
and relation sets. To assess this setting, we formalize an Open-World
evaluation protocol that measures performance when no SGG-specific data have
been observed either in terms of objects and relations. Experiments on Visual
Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate
the capacity of pretrained VLMs to perform relational understanding without
task-level training.

</details>


### [127] [Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes](https://arxiv.org/abs/2506.08191)
*Antoni Nowinowski,Krzysztof Krawiec*

Main category: cs.CV

TL;DR: This paper extends the architecture of the Disentangler of Visual Priors (DVP) to handle multiple objects using improved training techniques, achieving better reconstruction and object decomposition.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in improving DVP's ability to handle multi-object scenes and addressing training difficulties arising from reconstruction loss plateaus.

Method: The authors extend DVP to manage scenes with multiple objects by sampling additional training examples via its decoder and using alternative loss functions in both image and latent spaces.

Result: The extended DVP outperforms baseline methods (MONet and LIVE) in terms of reconstruction quality and overlapping object decomposition on a new benchmark dataset.

Conclusion: The paper highlights the advantages of the improved DVP in multi-object decomposition and discusses the challenges of differentiable rendering in autoencoders, offering insights into mitigating these limitations.

Abstract: This study builds on the architecture of the Disentangler of Visual Priors
(DVP), a type of autoencoder that learns to interpret scenes by decomposing the
perceived objects into independent visual aspects of shape, size, orientation,
and color appearance. These aspects are expressed as latent parameters which
control a differentiable renderer that performs image reconstruction, so that
the model can be trained end-to-end with gradient using reconstruction loss. In
this study, we extend the original DVP so that it can handle multiple objects
in a scene. We also exploit the interpretability of its latent by using the
decoder to sample additional training examples and devising alternative
training modes that rely on loss functions defined not only in the image space,
but also in the latent space. This significantly facilitates training, which is
otherwise challenging due to the presence of extensive plateaus in the
image-space reconstruction loss. To examine the performance of this approach,
we propose a new benchmark featuring multiple 2D objects, which subsumes the
previously proposed Multi-dSprites dataset while being more parameterizable. We
compare the DVP extended in these ways with two baselines (MONet and LIVE) and
demonstrate its superiority in terms of reconstruction quality and capacity to
decompose overlapping objects. We also analyze the gradients induced by the
considered loss functions, explain how they impact the efficacy of training,
and discuss the limitations of differentiable rendering in autoencoders and the
ways in which they can be addressed.

</details>


### [128] [GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra](https://arxiv.org/abs/2506.08194)
*Mateusz Michalkiewicz,Anekha Sokhal,Tadeusz Michalkiewicz,Piotr Pawlikowski,Mahsa Baktashmotlagh,Varun Jampani,Guha Balakrishnan*

Main category: cs.CV

TL;DR: GIQ introduces a benchmark for assessing the geometric intelligence of vision and vision-language models, especially in handling diverse polyhedral structures, but reveals major performance gaps.


<details>
  <summary>Details</summary>
Motivation: Current vision and vision-language models show impressive benchmark performance but their understanding of geometric properties remains unclear, prompting a need for standardized evaluation.

Method: The authors developed the GIQ benchmark comprising synthetic and real-world images of 224 polyhedra. They systematically tested models on tasks like 3D reconstruction, symmetry detection, mental rotation, and zero-shot classification.

Result: Experiments showed that models struggle with basic geometric reconstruction, symmetry differentiation, mental rotation tasks, and interpreting polyhedral properties, despite some strengths in detecting basic 3D symmetry elements.

Conclusion: GIQ exposes critical flaws in the geometric reasoning capabilities of modern models, providing a platform for addressing these shortcomings to advance robust representation learning.

Abstract: Monocular 3D reconstruction methods and vision-language models (VLMs)
demonstrate impressive results on standard benchmarks, yet their true
understanding of geometric properties remains unclear. We introduce GIQ , a
comprehensive benchmark specifically designed to evaluate the geometric
reasoning capabilities of vision and vision-language foundation models. GIQ
comprises synthetic and real-world images of 224 diverse polyhedra - including
Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and
compound shapes - covering varying levels of complexity and symmetry. Through
systematic experiments involving monocular 3D reconstruction, 3D symmetry
detection, mental rotation tests, and zero-shot shape classification tasks, we
reveal significant shortcomings in current models. State-of-the-art
reconstruction algorithms trained on extensive 3D datasets struggle to
reconstruct even basic geometric forms accurately. While foundation models
effectively detect specific 3D symmetry elements via linear probing, they
falter significantly in tasks requiring detailed geometric differentiation,
such as mental rotation. Moreover, advanced vision-language assistants exhibit
remarkably low accuracy on complex polyhedra, systematically misinterpreting
basic properties like face geometry, convexity, and compound structures. GIQ is
publicly available, providing a structured platform to highlight and address
critical gaps in geometric intelligence, facilitating future progress in
robust, geometry-aware representation learning.

</details>


### [129] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: The paper studies the use of modern decoder-only large language models (LLMs) as text encoders for text-to-image diffusion models, revealing performance improvements via refined embedding extraction techniques.


<details>
  <summary>Details</summary>
Motivation: To improve the outdated text encoding methods used in text-to-image generation models by evaluating modern decoder-only LLMs.

Method: The authors built a standardized training and evaluation pipeline, trained 27 text-to-image models with 12 text encoders, and extensively analyzed various aspects of LLMs such as embedding extraction methods, model variants, and sizes.

Result: Using layer-normalized averaging across all layers for embedding extraction outperforms the baseline approach of last-layer embeddings, significantly improving performance in complex visio-linguistic tasks.

Conclusion: Modern LLMs, with refined embedding conditioning approaches, show enhanced capabilities and superior alignment compared to older methods like T5 in text-to-image generation models.

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [130] [Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation](https://arxiv.org/abs/2506.08214)
*Ioannis Iakovidis,Zahra Kalantari,Amir Hossein Payberah,Fernando Jaramillo,Francisco Pena Escobar*

Main category: cs.CV

TL;DR: The study proposes a self-supervised model combining deep clustering and negative sampling to segment radar satellite images into land and water areas, avoiding manual annotations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for large annotated datasets required by computer vision models for wetland surface monitoring, which are time-consuming and expensive to produce.

Method: Self-supervised training combining deep clustering and negative sampling for segmentation, alongside an ensemble model approach to enhance variance and performance.

Result: The ensemble self-supervised model shows an improvement of 0.02 in the Intersection Over Union (IoU) metric compared to a single fully-supervised model on the test dataset.

Conclusion: The paper demonstrates that self-supervised learning combined with an ensemble approach can effectively segment radar satellite images, reducing the dependency on manual annotations while improving performance metrics.

Abstract: In recent years the wide availability of high-resolution radar satellite
images along with the advancement of computer vision models have enabled the
remote monitoring of the surface area of wetlands. However, these models
require large amounts of manually annotated satellite images, which are slow
and expensive to produce. To overcome this problem, self-supervised training
methods have been deployed to train models without using annotated data. In
this paper we use a combination of deep clustering and negative sampling to
train a model to segment radar satellite images into areas that separate water
from land without the use of any manual annotations. Furthermore, we implement
an ensemble version of the model to reduce variance and improve performance.
Compared to a single fully-supervised model using the same architecture, our
ensemble of self-supervised models achieves a 0.02 improvement in the
Intersection Over Union metric over our test dataset.

</details>


### [131] [Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence](https://arxiv.org/abs/2506.08220)
*Octave Mariotti,Zhipeng Du,Yash Bhalgat,Oisin Mac Aodha,Hakan Bilen*

Main category: cs.CV

TL;DR: This paper proposes a method for semantic correspondence using a 3D canonical manifold derived from monocular depth estimation, eliminating the need for explicit 3D or camera supervision, and introduces SPair-U to test generalization.


<details>
  <summary>Details</summary>
Motivation: Supervised SC methods are limited in generalizing to dense correspondences beyond sparsely annotated keypoints, restricting them to keypoint detection.

Method: The paper lifts 2D keypoints into a canonical 3D space via monocular depth estimation, creating a continuous manifold that captures geometry without the need for explicit 3D or camera annotations.

Result: Their method significantly outperforms supervised baselines on unseen keypoints and shows that unsupervised baselines can generalize better across different datasets than supervised approaches.

Conclusion: The proposed approach provides a robust way to learn semantic correspondences, breaking away from the dependency on sparse annotations while highlighting the generalization strength of unsupervised methods.

Abstract: Semantic correspondence (SC) aims to establish semantically meaningful
matches across different instances of an object category. We illustrate how
recent supervised SC methods remain limited in their ability to generalize
beyond sparsely annotated training keypoints, effectively acting as keypoint
detectors. To address this, we propose a novel approach for learning dense
correspondences by lifting 2D keypoints into a canonical 3D space using
monocular depth estimation. Our method constructs a continuous canonical
manifold that captures object geometry without requiring explicit 3D
supervision or camera annotations. Additionally, we introduce SPair-U, an
extension of SPair-71k with novel keypoint annotations, to better assess
generalization. Experiments not only demonstrate that our model significantly
outperforms supervised baselines on unseen keypoints, highlighting its
effectiveness in learning robust correspondences, but that unsupervised
baselines outperform supervised counterparts when generalized across different
datasets.

</details>


### [132] [A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks](https://arxiv.org/abs/2506.08227)
*Vishaal Udandarao,Mehdi Cherti,Shyamgopal Karthik,Jenia Jitsev,Samuel Albanie,Matthias Bethge*

Main category: cs.CV

TL;DR: The paper critically evaluates 17 benchmarks for compositional understanding in vision-language models, finding inherent biases that reduce their efficacy, and offers recommendations to improve them.


<details>
  <summary>Details</summary>
Motivation: The authors aim to assess whether current benchmarks for vision-language model compositional understanding are reliable measurements, as these benchmarks are essential for progress in the field.

Method: The authors scrutinize 17 benchmarks, analyze their design choices, and test them with simplistic heuristics to reveal biases and flaws.

Result: They discovered that these benchmarks contain biases due to distribution asymmetry between positive and negative examples, making them less effective in measuring true compositional understanding.

Conclusion: The paper concludes that current benchmarks fail to adequately test VLM capabilities and provides key recommendations for constructing more robust and unbiased benchmarks.

Abstract: We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for
measuring compositional understanding capabilities of vision-language models
(VLMs). We scrutinize design choices in their construction, including data
source (e.g. MS-COCO) and curation procedures (e.g. constructing negative
images/captions), uncovering several inherent biases across most benchmarks. We
find that blind heuristics (e.g. token-length, log-likelihood under a language
model) perform on par with CLIP models, indicating that these benchmarks do not
effectively measure compositional understanding. We demonstrate that the
underlying factor is a distribution asymmetry between positive and negative
images/captions, induced by the benchmark construction procedures. To mitigate
these issues, we provide a few key recommendations for constructing more robust
vision-language compositional understanding benchmarks, that would be less
prone to such simple attacks.

</details>


### [133] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: This paper explores 1D image tokenizers for effective image editing and generation, utilizing heuristic token manipulation and test-time optimization.


<details>
  <summary>Details</summary>
Motivation: Investigating the potential of 1D image tokenizers for compressed latent representations and their applications in image editing and generation.

Method: Utilization of vector quantization for token compression, heuristic token manipulation for image editing, and gradient-based optimization for generation tasks using plug-and-play loss functions.

Result: Demonstrates fine-grained image editing and generation capabilities such as inpainting and text-guided editing without training generative models.

Conclusion: 1D image tokenizers enable expressive and practical image manipulation and generation due to their highly compressed latent space and optimization methods.

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [134] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: Mirage is introduced as an audio-to-video foundation model capable of generating realistic videos from audio input, surpassing existing methods.


<details>
  <summary>Details</summary>
Motivation: Creators and consumers value videos that harmoniously combine audio and visuals, but many current models either ignore sound entirely or are constrained to specific use cases.

Method: A unified self-attention-based training method is employed to train Mirage for general-purpose audio-to-video generation while maintaining high output quality.

Result: Mirage generates expressive, realistic videos conditioned on audio input, particularly excelling in creating vivid interpretations of speech performances.

Conclusion: Mirage demonstrates the ability to generalize in audio-to-video generation tasks while achieving superior performance compared to existing methods, establishing itself as a new multimodal video creation tool.

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [135] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/abs/2506.08297)
*Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin*

Main category: cs.CV

TL;DR: This paper introduces Scalable and Efficient Mamba-like Attention (SEMA), improving transformer attention mechanisms by addressing issues of computational efficiency and focusing capability.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of quadratic computational complexity in vanilla attention and the lack of focusing in linear attention, specifically in computer vision tasks.

Method: They define a generalized attention framework, mathematically analyze its dispersion property, and design SEMA using token localization to avoid key weight dispersion and arithmetic averaging for global attention.

Result: SEMA outperforms recent attention models in classification tasks on the Imagenet-1k dataset, demonstrating scalability and efficiency for larger image scales while maintaining similar model sizes.

Conclusion: The SEMA method proves to be a scalable, efficient, and effective alternative to existing attention mechanisms, particularly applicable to vision tasks.

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [136] [OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal](https://arxiv.org/abs/2506.08299)
*Kangning Yang,Ling Ouyang,Huiming Sun,Jie Cai,Lan Fu,Jiaming Ding,Chiu Man Ho,Zibo Meng*

Main category: cs.CV

TL;DR: The paper introduces a new paradigm for collecting reflection datasets that are high-quality and diverse, resulting in the OpenRR-1k dataset containing 1,000 transmission-reflection pairs from real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing reflection removal methods lack robust and natural datasets that represent real-world conditions, limiting their effectiveness.

Method: The authors developed a scalable and cost-effective approach to collect natural and perfectly aligned image pairs, ensuring diversity and quality, and created the OpenRR-1k dataset.

Result: The dataset contains 1,000 high-quality real-world transmission-reflection image pairs and benchmarks show improved robustness in reflection removal tasks.

Conclusion: The OpenRR-1k dataset addresses the challenges of reflection removal in real-world environments and is expected to advance related techniques and applications.

Abstract: Reflection removal technology plays a crucial role in photography and
computer vision applications. However, existing techniques are hindered by the
lack of high-quality in-the-wild datasets. In this paper, we propose a novel
paradigm for collecting reflection datasets from a fresh perspective. Our
approach is convenient, cost-effective, and scalable, while ensuring that the
collected data pairs are of high quality, perfectly aligned, and represent
natural and diverse scenarios. Following this paradigm, we collect a
Real-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which
contains 1,000 high-quality transmission-reflection image pairs collected in
the wild. Through the analysis of several reflection removal methods and
benchmark evaluation experiments on our dataset, we demonstrate its
effectiveness in improving robustness in challenging real-world environments.
Our dataset is available at https://github.com/caijie0620/OpenRR-1k.

</details>


### [137] [Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating](https://arxiv.org/abs/2506.08324)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: This paper introduces STNet, a new network architecture with a novel Spatial-Spectral Transformer to enhance hyperspectral image classification.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in hyperspectral image classification such as high-dimensional data, spectral redundancy, sparse ground object distribution, overfitting, and weak generalization.

Method: STNet employs a Spatial-Spectral Transformer module with explicit spatial and spectral attention decoupling and uses gating mechanisms for adaptive fusion and internal feature transformation.

Result: STNet surpasses traditional methods and shows superior performance on three benchmark hyperspectral datasets (IN, UP, KSC).

Conclusion: STNet improves feature extraction and fusion for hyperspectral images, reduces overfitting in high-noise scenarios, and enhances representation without increasing network complexity.

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more effectively extract
and fuse spatial context with fine spectral information in hyperspectral image
(HSI) classification, this paper proposes a novel network architecture called
STNet. The core advantage of STNet stems from the dual innovative design of its
Spatial-Spectral Transformer module: first, the fundamental explicit decoupling
of spatial and spectral attention ensures targeted capture of key information
in HSI; second, two functionally distinct gating mechanisms perform intelligent
regulation at both the fusion level of attention flows (adaptive attention
fusion gating) and the internal level of feature transformation (GFFN). This
characteristic demonstrates superior feature extraction and fusion capabilities
compared to traditional convolutional neural networks, while reducing
overfitting risks in small-sample and high-noise scenarios. STNet enhances
model representation capability without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>


### [138] [Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera](https://arxiv.org/abs/2506.08327)
*Yuto Kase,Kai Ishibe,Ryoma Yasuda,Yudai Washida,Sakiko Hashimoto*

Main category: cs.CV

TL;DR: This paper presents a method to evaluate tennis ball impact location on a racket using event cameras for real-time measurement, overcoming limitations of high-speed cameras.


<details>
  <summary>Details</summary>
Motivation: To address challenges in tennis ball impact detection, such as memory consumption, manual errors, and lack of prolonged monitoring methods.

Method: Utilize event cameras for efficient brightness variation detection and introduce PATS to measure impact timing alongside computer vision techniques.

Result: The proposed method achieved accurate tennis performance measurements and exhibited computational efficiency suitable for real-time use.

Conclusion: The method enables extended monitoring of racket impacts, making it promising for enhancing player performance analysis and personalized equipment design.

Abstract: In racket sports, such as tennis, locating the ball's position at impact is
important in clarifying player and equipment characteristics, thereby aiding in
personalized equipment design. High-speed cameras are used to measure the
impact location; however, their excessive memory consumption limits prolonged
scene capture, and manual digitization for position detection is time-consuming
and prone to human error. These limitations make it difficult to effectively
capture the entire playing scene, hindering the ability to analyze the player's
performance. We propose a method for locating the tennis ball impact on the
racket in real time using an event camera. Event cameras efficiently measure
brightness changes (called `events') with microsecond accuracy under high-speed
motion while using lower memory consumption. These cameras enable users to
continuously monitor their performance over extended periods. Our method
consists of three identification steps: time range of swing, timing at impact,
and contours of ball and racket. Conventional computer vision techniques are
utilized along with an original event-based processing to detect the timing at
impact (PATS: the amount of polarity asymmetry in time symmetry). The results
of the experiments were within the permissible range for measuring tennis
players' performance. Moreover, the computation time was sufficiently short for
real-time applications.

</details>


### [139] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: The paper introduces Step AG, a universally applicable adaptive guidance strategy for text-to-vision diffusion models, achieving 20%-30% faster generation while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Current classifier-free guidance methods in text-to-vision diffusion models are computationally expensive, requiring twice as many steps as unconditional generation.

Method: The authors propose Step AG, an adaptive strategy that restricts classifier-free guidance to the initial denoising steps, addressing limitations of previous adaptive guidance methods.

Result: Step AG showed consistent improvements in speed (20%-30% faster) while maintaining high image quality and alignment across various models, including video generation.

Conclusion: Step AG offers a simple, efficient, and universally applicable solution for improving generation speed without sacrificing quality or alignment in diverse diffusion model settings.

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [140] [MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding](https://arxiv.org/abs/2506.08356)
*Shivang Chopra,Lingchao Mao,Gabriela Sanchez-Rodriguez,Andrew J Feola,Jing Li,Zsolt Kira*

Main category: cs.CV

TL;DR: MedMoE is a modular vision-language framework that dynamically adapts visual representation to diagnostic needs using modality-specific visual semantics.


<details>
  <summary>Details</summary>
Motivation: Current vision-language medical frameworks apply uniform strategies for local feature extraction, neglecting modality-specific considerations across different imaging types.

Method: MedMoE uses a Mixture-of-Experts (MoE) module conditioned on the report type and specialized expert branches trained on feature pyramids from a Swin Transformer backbone.

Result: MedMoE improves alignment and retrieval performance across diverse medical imaging modalities, as demonstrated by empirical results on various medical benchmarks.

Conclusion: Modality-specialized visual representations offer strong potential for enhancing clinical vision-language systems by dynamically adapting to diagnostic context.

Abstract: Different medical imaging modalities capture diagnostic information at
varying spatial resolutions, from coarse global patterns to fine-grained
localized structures. However, most existing vision-language frameworks in the
medical domain apply a uniform strategy for local feature extraction,
overlooking the modality-specific demands. In this work, we present MedMoE, a
modular and extensible vision-language processing framework that dynamically
adapts visual representation based on the diagnostic context. MedMoE
incorporates a Mixture-of-Experts (MoE) module conditioned on the report type,
which routes multi-scale image features through specialized expert branches
trained to capture modality-specific visual semantics. These experts operate
over feature pyramids derived from a Swin Transformer backbone, enabling
spatially adaptive attention to clinically relevant regions. This framework
produces localized visual representations aligned with textual descriptions,
without requiring modality-specific supervision at inference. Empirical results
on diverse medical benchmarks demonstrate that MedMoE improves alignment and
retrieval performance across imaging modalities, underscoring the value of
modality-specialized visual representations in clinical vision-language
systems.

</details>


### [141] [Image Demoiréing Using Dual Camera Fusion on Mobile Phones](https://arxiv.org/abs/2506.08361)
*Yanting Mei,Zhilu Zhang,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: The paper addresses moiré pattern removal in screen-shot images by leveraging ultra-wide-angle (UW) images to enhance the quality of wide-angle (W) images through dual camera fusion. It proposes a novel method and dataset for this task.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of removing severe moiré patterns from images captured off electronic screens, and the observation that UW camera images often remain unaffected when W camera images suffer from moiré artifacts.

Method: The method incorporates a lightweight UW image encoder into an existing demoiréing network, alongside a two-stage image alignment approach. A comprehensive real-world dataset with 9,000 samples is created to aid the method's development and evaluation.

Result: The proposed method achieves superior performance compared to state-of-the-art techniques in removing moiré patterns, as demonstrated in experiments conducted on the newly constructed dataset.

Conclusion: Dual camera fusion using data from UW and W image lenses is effective for demoiréing, offering a promising enhancement to image quality in modern smartphones.

Abstract: When shooting electronic screens, moir\'e patterns usually appear in captured
images, which seriously affects the image quality. Existing image demoir\'eing
methods face great challenges in removing large and heavy moir\'e. To address
the issue, we propose to utilize Dual Camera fusion for Image Demoir\'eing
(DCID), \ie, using the ultra-wide-angle (UW) image to assist the moir\'e
removal of wide-angle (W) image. This is inspired by two motivations: (1) the
two lenses are commonly equipped with modern smartphones, (2) the UW image
generally can provide normal colors and textures when moir\'e exists in the W
image mainly due to their different focal lengths. In particular, we propose an
efficient DCID method, where a lightweight UW image encoder is integrated into
an existing demoir\'eing network and a fast two-stage image alignment manner is
present. Moreover, we construct a large-scale real-world dataset with diverse
mobile phones and monitors, containing about 9,000 samples. Experiments on the
dataset show our method performs better than state-of-the-art methods. Code and
dataset are available at https://github.com/Mrduckk/DCID.

</details>


### [142] [SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding](https://arxiv.org/abs/2506.08391)
*Woohyeon Park,Woojin Kim,Jaeik Kim,Jaeyoung Do*

Main category: cs.CV

TL;DR: This paper presents SECOND, a novel method to address object hallucination in vision-language models (VLMs) by leveraging multi-scale visual information.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models suffer from object hallucination, which impedes accurate visual understanding.

Method: The authors introduce SECOND (Selective and Contrastive Decoding), which prioritizes and contrasts multi-scale visual information in a human-like, object-centric manner to reduce hallucination.

Result: SECOND is shown to significantly reduce perceptual hallucinations and outperform various benchmarks while illustrating the potential of multi-scale information in VLMs through theoretical analysis and experiments.

Conclusion: SECOND effectively improves VLM performance by reducing object hallucinations through a novel approach of selective and contrastive decoding of multi-scale information.

Abstract: Despite significant advancements in Vision-Language Models (VLMs), the
performance of existing VLMs remains hindered by object hallucination, a
critical challenge to achieving accurate visual understanding. To address this
issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach
that enables VLMs to effectively leverage multi-scale visual information with
an object-centric manner, closely aligning with human visual perception. SECOND
progressively selects and integrates multi-scale visual information,
facilitating a more precise interpretation of images. By contrasting these
visual information iteratively, SECOND significantly reduces perceptual
hallucinations and outperforms a wide range of benchmarks. Our theoretical
analysis and experiments highlight the largely unexplored potential of
multi-scale application in VLMs, showing that prioritizing and contrasting
across scales outperforms existing methods.

</details>


### [143] [RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation](https://arxiv.org/abs/2506.08418)
*Taiqin Chen,Zikun Zhou,Zheng Fang,Wenzhen Zou,Kanjun Liu,Ke Chen,Yongbing Zhang,Yaowei Wang*

Main category: cs.CV

TL;DR: The paper addresses the challenge of constructing dense radio maps from sparse data samples and proposes RadioDUN, a deep unfolding network. It incorporates physical characteristics and obstacle factors for improved performance.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to incorporate physical characteristics into radio map construction from sparse data, limiting effectiveness in resource allocation and interference mitigation.

Method: The RadioDUN framework unfolds optimization, integrates a dynamic reweighting module for factor importance, and utilizes shadowing-related constraints to improve sparse signal recovery.

Result: RadioDUN significantly outperforms state-of-the-art methods in constructing dense radio maps through extensive experimental validations.

Conclusion: The proposed RadioDUN framework integrates physical modeling and deep learning methods to effectively and adaptively estimate dense radio maps from sparse samples for improved spectrum resource utilization.

Abstract: The radio map represents the spatial distribution of spectrum resources
within a region, supporting efficient resource allocation and interference
mitigation. However, it is difficult to construct a dense radio map as a
limited number of samples can be measured in practical scenarios. While
existing works have used deep learning to estimate dense radio maps from sparse
samples, they are hard to integrate with the physical characteristics of the
radio map. To address this challenge, we cast radio map estimation as the
sparse signal recovery problem. A physical propagation model is further
incorporated to decompose the problem into multiple factor optimization
sub-problems, thereby reducing recovery complexity. Inspired by the existing
compressive sensing methods, we propose the Radio Deep Unfolding Network
(RadioDUN) to unfold the optimization process, achieving adaptive parameter
adjusting and prior fitting in a learnable manner. To account for the radio
propagation characteristics, we develop a dynamic reweighting module (DRM) to
adaptively model the importance of each factor for the radio map. Inspired by
the shadowing factor in the physical propagation model, we integrate
obstacle-related factors to express the obstacle-induced signal stochastic
decay. The shadowing loss is further designed to constrain the factor
prediction and act as a supplementary supervised objective, which enhances the
performance of RadioDUN. Extensive experiments have been conducted to
demonstrate that the proposed method outperforms the state-of-the-art methods.
Our code will be made publicly available upon publication.

</details>


### [144] [Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring](https://arxiv.org/abs/2506.08429)
*Mingjie Xu,Andrew Estornell,Hongzheng Yang,Yuzhi Zhao,Zhaowei Zhu,Qi Xuan,Jiaheng Wei*

Main category: cs.CV

TL;DR: The paper introduces SCALE, a quality-driven data selection pipeline aimed at improving Vision-Language Models (VLMs) by addressing challenges like noisy alignments and ambiguous texts in multimodal datasets.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle two key challenges in Vision-Language Models: noisy alignments between images and text, and ambiguous or misleading text, which hinder the performance of VLMs.

Method: SCALE employs a cross-modality assessment framework that assigns each dataset entry to relevant tasks, generates captions for evaluation, and analyzes aspects such as alignment, clarity, task rarity, text coherence, and image clarity. It evaluates quality using both general and task-specific captions.

Result: The authors found that current unimodal quality assessments fail to account for multimodal interactions, leading to valuable entries being underestimated and poor entries being included. Generated captions enable efficient multimodal task evaluation.

Conclusion: SCALE enhances VLM instruction tuning datasets by addressing data quality issues and converting multimodal challenges into a unified text modality using generated image captions.

Abstract: The application of visual instruction tuning and other post-training
techniques has significantly enhanced the capabilities of Large Language Models
(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with
more comprehensive visual language datasets. However, the effectiveness of VLMs
is highly dependent on large-scale, high-quality datasets that ensure precise
recognition and accurate reasoning. Two key challenges hinder progress: (1)
noisy alignments between images and the corresponding text, which leads to
misinterpretation, and (2) ambiguous or misleading text, which obscures visual
content. To address these challenges, we propose SCALE (Single modality data
quality and Cross modality Alignment Evaluation), a novel quality-driven data
selection pipeline for VLM instruction tuning datasets. Specifically, SCALE
integrates a cross-modality assessment framework that first assigns each data
entry to its appropriate vision-language task, generates general and
task-specific captions (covering scenes, objects, style, etc.), and evaluates
the alignment, clarity, task rarity, text coherence, and image clarity of each
entry based on the generated captions. We reveal that: (1) current unimodal
quality assessment methods evaluate one modality while overlooking the rest,
which can underestimate samples essential for specific tasks and discard the
lower-quality instances that help build model robustness; and (2) appropriately
generated image captions provide an efficient way to transfer the image-text
multimodal task into a unified text modality.

</details>


### [145] [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](https://arxiv.org/abs/2506.08456)
*June Suk Choi,Kyungmin Lee,Sihyun Yu,Yisol Choi,Jinwoo Shin,Kimin Lee*

Main category: cs.CV

TL;DR: Adaptive low-pass guidance (ALG) improves motion dynamics in Image-to-Video (I2V) generation while maintaining image quality and text alignment.


<details>
  <summary>Details</summary>
Motivation: Current image-to-video (I2V) generation methods fail to produce dynamic motion in videos due to premature exposure to input image details, causing overfitting to static appearance.

Method: The proposed adaptive low-pass guidance (ALG) applies low-pass filtering during the early denoising stage to mitigate the static bias in I2V generation.

Result: Extensive experiments show ALG significantly enhances the temporal dynamics by 36% on the VBench-I2V test while preserving image fidelity and video quality.

Conclusion: ALG offers a simple yet effective improvement to I2V video generation, resolving motion dynamics issues without sacrificing quality.

Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in
producing high-quality, dynamic videos. To improve the visual controllability,
recent works have considered fine-tuning pre-trained T2V models to support
image-to-video (I2V) generation. However, such adaptation frequently suppresses
motion dynamics of generated outputs, resulting in more static videos compared
to their T2V counterparts. In this work, we analyze this phenomenon and
identify that it stems from the premature exposure to high-frequency details in
the input image, which biases the sampling process toward a shortcut trajectory
that overfits to the static appearance of the reference image. To address this,
we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model
sampling procedure to generate more dynamic videos without compromising
per-frame image quality. Specifically, ALG adaptively modulates the frequency
content of the conditioning image by applying low-pass filtering at the early
stage of denoising. Extensive experiments demonstrate that ALG significantly
improves the temporal dynamics of generated videos, while preserving image
fidelity and text alignment. Especially, under VBench-I2V test suite, ALG
achieves an average improvement of 36% in dynamic degree without a significant
drop in video quality or image fidelity.

</details>


### [146] [MARMOT: Masked Autoencoder for Modeling Transient Imaging](https://arxiv.org/abs/2506.08470)
*Siyuan Shen,Ziheng Wang,Xingyue Peng,Suan Xia,Ruiqian Li,Shiying Li,Jingyi Yu*

Main category: cs.CV

TL;DR: MARMOT introduces a self-supervised pretraining approach for imaging hidden objects using transient modalities in non-line-of-sight (NLOS) scenarios.


<details>
  <summary>Details</summary>
Motivation: Pretrained models have shown success in language and vision, but transient imaging—particularly for NLOS applications—has lacked a method to leverage dataset priors, prompting the development of MARMOT.

Method: MARMOT employs a masked autoencoder with a Transformer-based encoder-decoder architecture to learn features from partially masked transients via a scanning pattern mask (SPM). It is pretrained on a synthesized dataset, TransVerse, containing 500K 3D models.

Result: MARMOT demonstrates efficient performance on downstream imaging tasks through direct feature transfer or fine-tuning, outperforming state-of-the-art methods in experiments.

Conclusion: MARMOT makes significant advances in transient imaging by leveraging self-supervised pretraining, which enables effective representation learning and downstream adaptability.

Abstract: Pretrained models have demonstrated impressive success in many modalities
such as language and vision. Recent works facilitate the pretraining paradigm
in imaging research. Transients are a novel modality, which are captured for an
object as photon counts versus arrival times using a precisely time-resolved
sensor. In particular for non-line-of-sight (NLOS) scenarios, transients of
hidden objects are measured beyond the sensor's direct line of sight. Using
NLOS transients, the majority of previous works optimize volume density or
surfaces to reconstruct the hidden objects and do not transfer priors learned
from datasets. In this work, we present a masked autoencoder for modeling
transient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a
self-supervised model pretrianed on massive and diverse NLOS transient
datasets. Using a Transformer-based encoder-decoder, MARMOT learns features
from partially masked transients via a scanning pattern mask (SPM), where the
unmasked subset is functionally equivalent to arbitrary sampling, and predicts
full measurements. Pretrained on TransVerse-a synthesized transient dataset of
500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature
transfer or decoder finetuning. Comprehensive experiments are carried out in
comparisons with state-of-the-art methods. Quantitative and qualitative results
demonstrate the efficiency of our MARMOT.

</details>


### [147] [Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization](https://arxiv.org/abs/2506.08493)
*Qilin Yin,Wei Lu,Xiangyang Luo,Xiaochun Cao*

Main category: cs.CV

TL;DR: The paper introduces a novel framework, UniCaCLF, for temporal forgery localization in videos, leveraging supervised contrastive learning and context-aware strategies to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current deepfake detection methods, specifically their focus on classification tasks and inability to handle tampered segments within real videos.

Method: The authors propose the UniCaCLF framework, which uses supervised contrastive learning and introduces a context-aware perception layer with heterogeneous activation and adaptive context updating for better detection of forged video instants.

Result: The framework demonstrated significant performance improvements over state-of-the-art methods across five public datasets.

Conclusion: UniCaCLF enables precise temporal localization of forged segments in videos and sets a new benchmark for realistic deepfake detection scenarios.

Abstract: Most research efforts in the multimedia forensics domain have focused on
detecting forgery audio-visual content and reached sound achievements. However,
these works only consider deepfake detection as a classification task and
ignore the case where partial segments of the video are tampered with. Temporal
forgery localization (TFL) of small fake audio-visual clips embedded in real
videos is still challenging and more in line with realistic application
scenarios. To resolve this issue, we propose a universal context-aware
contrastive learning framework (UniCaCLF) for TFL. Our approach leverages
supervised contrastive learning to discover and identify forged instants by
means of anomaly detection, allowing for the precise localization of temporal
forged segments. To this end, we propose a novel context-aware perception layer
that utilizes a heterogeneous activation operation and an adaptive context
updater to construct a context-aware contrastive objective, which enhances the
discriminability of forged instant features by contrasting them with genuine
instant features in terms of their distances to the global context. An
efficient context-aware contrastive coding is introduced to further push the
limit of instant feature distinguishability between genuine and forged instants
in a supervised sample-by-sample manner, suppressing the cross-sample influence
to improve temporal forgery localization performance. Extensive experimental
results over five public datasets demonstrate that our proposed UniCaCLF
significantly outperforms the state-of-the-art competing algorithms.

</details>


### [148] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)
*Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang*

Main category: cs.CV

TL;DR: The paper introduces MLVTG, a framework for Video Temporal Grounding (VTG) that improves multi-modal alignment and temporal localization using novel modules "MambaAligner" and "LLMRefiner."


<details>
  <summary>Details</summary>
Motivation: Existing methods for VTG face challenges like redundant attention mechanisms and poor multi-modal alignment, limiting their effectiveness in video understanding tasks.

Method: MLVTG replaces Transformers with Vision Mamba blocks in the MambaAligner module for better temporal video representation and introduces LLMRefiner, which uses pre-trained language models to refine multi-modal alignment without fine-tuning.

Result: MLVTG achieves state-of-the-art performance in VTG tasks, outperforming prior methods in datasets like QVHighlights, Charades-STA, and TVSum.

Conclusion: The dual strategies of structured temporal modeling and semantic refinement lead to better video clip localization, showcasing MLVTG's potential in advancing VTG research.

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [149] [Robust Visual Localization via Semantic-Guided Multi-Scale Transformer](https://arxiv.org/abs/2506.08526)
*Zhongtao Tian,Wenhao Huang,Zhidong Chen,Xiao Wei Sun*

Main category: cs.CV

TL;DR: Improves visual localization in dynamic environments by integrating multi-scale feature learning with semantic scene understanding.


<details>
  <summary>Details</summary>
Motivation: Visual localization is challenging due to fluctuating conditions like lighting changes, weather, and dynamic objects.

Method: A hierarchical Transformer with cross-scale attention and semantic supervision using neural scene representation.

Result: Tested on TartanAir dataset, outperforming existing pose regression methods in dynamic scenarios.

Conclusion: Combining multi-scale features and semantic guidance enhances robust visual localization in dynamic environments.

Abstract: Visual localization remains challenging in dynamic environments where
fluctuating lighting, adverse weather, and moving objects disrupt appearance
cues. Despite advances in feature representation, current absolute pose
regression methods struggle to maintain consistency under varying conditions.
To address this challenge, we propose a framework that synergistically combines
multi-scale feature learning with semantic scene understanding. Our approach
employs a hierarchical Transformer with cross-scale attention to fuse geometric
details and contextual cues, preserving spatial precision while adapting to
environmental changes. We improve the performance of this architecture with
semantic supervision via neural scene representation during training, guiding
the network to learn view-invariant features that encode persistent structural
information while suppressing complex environmental interference. Experiments
on TartanAir demonstrate that our approach outperforms existing pose regression
methods in challenging scenarios with dynamic objects, illumination changes,
and occlusions. Our findings show that integrating multi-scale processing with
semantic guidance offers a promising strategy for robust visual localization in
real-world dynamic environments.

</details>


### [150] [LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s](https://arxiv.org/abs/2506.08529)
*Xijun Wang,Xin Li,Bingchen Li,Zhibo Chen*

Main category: cs.CV

TL;DR: LiftVSR introduces an efficient video super-resolution (VSR) framework leveraging image-wise diffusion priors, balancing computational cost and temporal consistency through Dynamic Temporal Attention and Attention Memory Cache mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current VSR methods face challenges in temporal coherence and high computational demands for processing long videos.

Method: LiftVSR uses Dynamic Temporal Attention for short-term temporal modeling and Attention Memory Cache for long-term modeling, along with asymmetric sampling for inference stability.

Result: LiftVSR achieves state-of-the-art VSR performance while reducing computational cost, requiring significantly fewer GPUs compared to existing methods.

Conclusion: LiftVSR improves video super-resolution by ensuring efficient temporal coherence and stabilization with reduced computation.

Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by
enhancing perceptual quality, largely through elaborately designed temporal
modeling to ensure inter-frame consistency. However, existing methods usually
suffer from limited temporal coherence and prohibitively high computational
costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for
long videos. In this work, we propose LiftVSR, an efficient VSR framework that
leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$,
achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To
balance long-term consistency and efficiency, we introduce a hybrid temporal
modeling mechanism that decomposes temporal learning into two complementary
components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal
modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii)
Attention Memory Cache (AMC) for long-term temporal modeling across segments
($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token
flows across frames within multi-head query and key tokens to warp inter-frame
contexts in the value tokens. AMC adaptively aggregates historical segment
information via a cache unit, ensuring long-term coherence with minimal
overhead. To further stabilize the cache interaction during inference, we
introduce an asymmetric sampling strategy that mitigates feature mismatches
arising from different diffusion sampling steps. Extensive experiments on
several typical VSR benchmarks have demonstrated that LiftVSR achieves
impressive performance with significantly lower computational costs.

</details>


### [151] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/abs/2506.08541)
*Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao*

Main category: cs.CV

TL;DR: TrajFlow is a motion prediction framework for autonomous driving that predicts diverse trajectories in a single pass, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for scalable, efficient, and accurate multi-modal motion prediction in autonomous vehicles, especially in dynamic real-world settings.

Method: TrajFlow integrates flow matching for single-pass multi-modal trajectory prediction, a ranking loss using the Plackett-Luce distribution for better uncertainty estimation, and self-conditioning training for improved generalization.

Result: TrajFlow achieves state-of-the-art results on the Waymo Open Motion Dataset, demonstrating its robustness in key safety and performance metrics.

Conclusion: TrajFlow is a highly efficient and reliable framework for motion prediction, enhancing safety and decision-making in autonomous driving.

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [152] [Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs](https://arxiv.org/abs/2506.08543)
*Bowei Tian,Xuntao Lyu,Meng Liu,Hongyi Wang,Ang Li*

Main category: cs.CV

TL;DR: The paper focuses on how structured, human-interpretable representations evolve in AI models, proposing a new hypothesis (ISLH) and introducing a framework (SPP) to understand and validate their formation.


<details>
  <summary>Details</summary>
Motivation: To improve AI transparency and control through understanding how representations in deep networks align with human-concepts, surpassing analysis at individual neuron levels.

Method: Propose the Input-Space Linearity Hypothesis (ISLH) and introduce the Spectral Principal Path (SPP) framework to study how deep networks distill linear representations, validated in Vision-Language Models (VLMs).

Result: The study demonstrates that concept-aligned representations originate from the input space and are progressively amplified in deep networks, showcasing robustness across modalities in VLMs.

Conclusion: The research bridges theoretical insights into representation formation with practical experiments, aiming to enhance AI robustness, fairness, and transparency through structured representation theories.

Abstract: High-level representations have become a central focus in enhancing AI
transparency and control, shifting attention from individual neurons or
circuits to structured semantic directions that align with human-interpretable
concepts. Motivated by the Linear Representation Hypothesis (LRH), we propose
the Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned
directions originate in the input space and are selectively amplified with
increasing depth. We then introduce the Spectral Principal Path (SPP)
framework, which formalizes how deep networks progressively distill linear
representations along a small set of dominant spectral directions. Building on
this framework, we further demonstrate the multimodal robustness of these
representations in Vision-Language Models (VLMs). By bridging theoretical
insights with empirical validation, this work advances a structured theory of
representation formation in deep networks, paving the way for improving AI
robustness, fairness, and transparency.

</details>


### [153] [From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge](https://arxiv.org/abs/2506.08553)
*Agnese Taluzzi,Davide Gesualdi,Riccardo Santambrogio,Chiara Plizzari,Francesca Palermo,Simone Mentasti,Matteo Matteucci*

Main category: cs.CV

TL;DR: The paper introduces SceneNet and KnowledgeNet as methods for the HD-EPIC VQA Challenge, achieving a combined accuracy of 44.21%.


<details>
  <summary>Details</summary>
Motivation: To improve performance on complex egocentric VQA tasks by leveraging multi-modal and external knowledge-intensive methods.

Method: Introduced SceneNet, which uses scene graphs from a multi-modal LLM for detailed object and event interactions, and KnowledgeNet, which utilizes ConceptNet for commonsense knowledge integration.

Result: Achieved an overall accuracy of 44.21% on the HD-EPIC benchmark by combining SceneNet and KnowledgeNet.

Conclusion: SceneNet and KnowledgeNet provide distinct yet complementary strengths, showing promise for enhancing egocentric VQA task performance.

Abstract: This report presents SceneNet and KnowledgeNet, our approaches developed for
the HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with
a multi-modal large language model (MLLM) to capture fine-grained object
interactions, spatial relationships, and temporally grounded events. In
parallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge
to introduce high-level semantic connections between entities, enabling
reasoning beyond directly observable visual evidence. Each method demonstrates
distinct strengths across the seven categories of the HD-EPIC benchmark, and
their combination within our framework results in an overall accuracy of 44.21%
on the challenge, highlighting its effectiveness for complex egocentric VQA
tasks.

</details>


### [154] [Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement](https://arxiv.org/abs/2506.08555)
*Xinyue Niu,Akira Furui*

Main category: cs.CV

TL;DR: This paper introduces a dual-branch adversarial neural network to tackle cross-subject variability in electromyography (EMG) pattern recognition, eliminating the need for user-specific calibration.


<details>
  <summary>Details</summary>
Motivation: Inter-subject variability in EMG signals makes traditional calibration-based models impractical for large-scale applications. There is a need for methods that generalize effectively across users without requiring calibration.

Method: The authors propose a dual-branch adversarial neural network that disentangles EMG features into pattern-specific and subject-specific components. This enables the model to recognize patterns from new users while maintaining functionality for biometric identification.

Result: The proposed method achieves robust performance in cross-subject scenarios, outperforming baseline models when tested with data from unseen users.

Conclusion: This work demonstrates the potential for eliminating model calibration needs in EMG pattern recognition while enabling broad applications such as task-independent biometric identification.

Abstract: Cross-subject electromyography (EMG) pattern recognition faces significant
challenges due to inter-subject variability in muscle anatomy, electrode
placement, and signal characteristics. Traditional methods rely on
subject-specific calibration data to adapt models to new users, an approach
that is both time-consuming and impractical for large-scale, real-world
deployment. This paper presents an approach to eliminate calibration
requirements through feature disentanglement, enabling effective cross-subject
generalization. We propose an end-to-end dual-branch adversarial neural network
that simultaneously performs pattern recognition and individual identification
by disentangling EMG features into pattern-specific and subject-specific
components. The pattern-specific components facilitate robust pattern
recognition for new users without model calibration, while the subject-specific
components enable downstream applications such as task-invariant biometric
identification. Experimental results demonstrate that the proposed model
achieves robust performance on data from unseen users, outperforming various
baseline methods in cross-subject scenarios. Overall, this study offers a new
perspective for cross-subject EMG pattern recognition without model calibration
and highlights the proposed model's potential for broader applications, such as
task-independent biometric systems.

</details>


### [155] [Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection](https://arxiv.org/abs/2506.08562)
*Duc Thanh Pham,Hong Dang Nguyen,Nhat Minh Nguyen Quoc,Linh Ngo Van,Sang Dinh Viet,Duc Anh Nguyen*

Main category: cs.CV

TL;DR: A novel framework called Hier-DETR improves incremental object detection by addressing class imbalance and hierarchical class relationships.


<details>
  <summary>Details</summary>
Motivation: Improving incremental object detection models to avoid catastrophic forgetting and to achieve better performance with efficiency.

Method: Hier-DETR leverages Neural Collapse for addressing dataset imbalance and explores the hierarchical relationships among class labels.

Result: Hier-DETR achieves better performance and inference efficiency compared to existing incremental object detection models.

Conclusion: Hier-DETR offers a practical and effective solution for incremental object detection challenges.

Abstract: Recently, object detection models have witnessed notable performance
improvements, particularly with transformer-based models. However, new objects
frequently appear in the real world, requiring detection models to continually
learn without suffering from catastrophic forgetting. Although Incremental
Object Detection (IOD) has emerged to address this challenge, these existing
models are still not practical due to their limited performance and prolonged
inference time. In this paper, we introduce a novel framework for IOD, called
Hier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both
efficiency and competitive performance by leveraging Neural Collapse for
imbalance dataset and Hierarchical relation of classes' labels.

</details>


### [156] [Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations](https://arxiv.org/abs/2506.08566)
*Yibo Cui,Liang Xie,Yu Zhao,Jiawei Sun,Erwei Yin*

Main category: cs.CV

TL;DR: This paper introduces FCA-NIG, a generative framework designed to create a dataset with fine-grained cross-modal annotations for Vision-Language Navigation (VLN), improving training for VLN agents.


<details>
  <summary>Details</summary>
Motivation: Existing VLN datasets lack fine-grained cross-modal alignment annotations, hindering accurate navigation action decisions.

Method: The paper proposes FCA-NIG, a framework that utilizes tools like GLIP for landmark detection, CLIP for entity selection, and OFA-Speaker for instruction generation to create a dataset (FCA-R2R) with sub-instruction-trajectory and entity-landmark annotations.

Result: Training VLN agents with the FCA-R2R dataset leads to significant performance improvements in state awareness, decision accuracy, and generalization.

Conclusion: FCA-NIG successfully generates high-quality training data without manual effort, advancing fine-grained cross-modal learning in VLN tasks.

Abstract: Vision-Language Navigation (VLN) enables intelligent agents to navigate
environments by integrating visual perception and natural language
instructions, yet faces significant challenges due to the scarcity of
fine-grained cross-modal alignment annotations. Existing datasets primarily
focus on global instruction-trajectory matching, neglecting
sub-instruction-level and entity-level alignments critical for accurate
navigation action decision-making. To address this limitation, we propose
FCA-NIG, a generative framework that automatically constructs navigation
instructions with dual-level fine-grained cross-modal annotations. In this
framework, an augmented trajectory is first divided into sub-trajectories,
which are then processed through GLIP-based landmark detection, crafted
instruction construction, OFA-Speaker based R2R-like instruction generation,
and CLIP-powered entity selection, generating sub-instruction-trajectory pairs
with entity-landmark annotations. Finally, these sub-pairs are aggregated to
form a complete instruction-trajectory pair. The framework generates the
FCA-R2R dataset, the first large-scale augmentation dataset featuring precise
sub-instruction-sub-trajectory and entity-landmark alignments. Extensive
experiments demonstrate that training with FCA-R2R significantly improves the
performance of multiple state-of-the-art VLN agents, including SF, EnvDrop,
RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances
agents' state awareness and decision accuracy, while entity-landmark alignment
further boosts navigation performance and generalization. These results
highlight the effectiveness of FCA-NIG in generating high-quality, scalable
training data without manual annotation, advancing fine-grained cross-modal
learning in complex navigation tasks.

</details>


### [157] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: The paper proposes the Diversity-Guided MLP Reduction (DGMR) method to significantly reduce parameters in large vision transformers, achieving major reductions in computational costs and memory while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Transformer models perform well but fail to scale efficiently due to the unaffordable computational and memory costs driven by large parameter counts.

Method: DGMR uses a Gram-Schmidt weight pruning strategy to eliminate redundant neurons in MLP layers while retaining weight diversity for effective performance recovery through distillation.

Result: The method reduced model parameters by more than 57.0% across large vision transformers, and achieved a 71.5% reduction for EVA-CLIP-E with negligible performance loss.

Conclusion: DGMR successfully diminishes the parameter and computational overhead of large vision transformers, showing scalability and near lossless reduction, making it suitable for further adoption.

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [158] [Rethinking Range-View LiDAR Segmentation in Adverse Weather](https://arxiv.org/abs/2506.08979)
*Longyu Yang,Ping Hu,Lu Zhang,Jun Liu,Yap-Peng Tan,Heng Tao Shen,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: This paper addresses LiDAR segmentation under severe weather conditions, proposing a modular framework that significantly enhances performance without altering existing model architectures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the performance and robustness of LiDAR segmentation methods, particularly in adverse weather conditions, where current methods lack reliability.

Method: The authors introduce a modular framework that processes geometric attributes and reflectance intensity separately. This includes a Geometric Abnormality Suppression (GAS) module to minimize spatial noise and a Reflectance Distortion Calibration (RDC) module for reflectance corrections.

Result: Experiments on various benchmarks show significant improvements in range-view LiDAR segmentation under severe weather conditions with negligible computational overhead.

Conclusion: This study presents an effective and computationally efficient solution to enhance the generalization of LiDAR segmentation models for real-world deployment in challenging weather scenarios.

Abstract: LiDAR segmentation has emerged as an important task to enrich multimedia
experiences and analysis. Range-view-based methods have gained popularity due
to their high computational efficiency and compatibility with real-time
deployment. However, their generalized performance under adverse weather
conditions remains underexplored, limiting their reliability in real-world
environments. In this work, we identify and analyze the unique challenges that
affect the generalization of range-view LiDAR segmentation in severe weather.
To address these challenges, we propose a modular and lightweight framework
that enhances robustness without altering the core architecture of existing
models. Our method reformulates the initial stem block of standard range-view
networks into two branches to process geometric attributes and reflectance
intensity separately. Specifically, a Geometric Abnormality Suppression (GAS)
module reduces the influence of weather-induced spatial noise, and a
Reflectance Distortion Calibration (RDC) module corrects reflectance
distortions through memory-guided adaptive instance normalization. The
processed features are then fused and passed to the original segmentation
pipeline. Extensive experiments on different benchmarks and baseline models
demonstrate that our approach significantly improves generalization to adverse
weather with minimal inference overhead, offering a practical and effective
solution for real-world LiDAR segmentation.

</details>


### [159] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/abs/2506.08596)
*Guyang Zhang,Waleed Abdulla*

Main category: cs.CV

TL;DR: This paper reviews over 300 published studies on Transformer-based hyperspectral image (HSI) classification, offering a comprehensive survey of the field.


<details>
  <summary>Details</summary>
Motivation: To address challenges in adopting Transformer architectures for HSI classification, such as data scarcity, high spectral dimensionality, and computational demands.

Method: The paper systematically categorizes and evaluates Transformer-based approaches for HSI classification, focusing on each component of the typical pipeline and analyzing persistent challenges.

Result: The survey identifies key obstacles in Transformer-based HSI classification, such as limited labeled data and computational overhead, while providing a research agenda to overcome these issues.

Conclusion: The study offers guidance to researchers on selecting Transformer components tailored to HSI challenges, encouraging advances in the field for future applications.

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [160] [SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction](https://arxiv.org/abs/2506.08997)
*Fabian Immel,Jan-Hendrik Pauls,Richard Fehler,Frank Bieder,Jonas Merkert,Christoph Stiller*

Main category: cs.CV

TL;DR: SDTagNet enhances autonomous vehicle map construction by integrating standard definition map data, including OpenStreetMap, to improve far-range detection accuracy using NLP-derived features and point-level encoders.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles need detailed environmental info for safety, but HD maps are costly to maintain. Using SD maps as priors offers a scalable solution.

Method: SDTagNet leverages SD maps through NLP-derived features and a point-level SD map encoder to uniformly merge semantic information and enhance accuracy.

Result: Experiments show performance improvements of up to +5.9 mAP (+45%) compared to no priors, and +3.2 mAP (+20%) compared to prior approaches using SD maps.

Conclusion: SDTagNet outperforms earlier methods by better utilizing SD maps for scalable, accurate online HD map construction suited for autonomous driving systems.

Abstract: Autonomous vehicles rely on detailed and accurate environmental information
to operate safely. High definition (HD) maps offer a promising solution, but
their high maintenance cost poses a significant barrier to scalable deployment.
This challenge is addressed by online HD map construction methods, which
generate local HD maps from live sensor data. However, these methods are
inherently limited by the short perception range of onboard sensors. To
overcome this limitation and improve general performance, recent approaches
have explored the use of standard definition (SD) maps as prior, which are
significantly easier to maintain. We propose SDTagNet, the first online HD map
construction method that fully utilizes the information of widely available SD
maps, like OpenStreetMap, to enhance far range detection accuracy. Our approach
introduces two key innovations. First, in contrast to previous work, we
incorporate not only polyline SD map data with manually selected classes, but
additional semantic information in the form of textual annotations. In this
way, we enrich SD vector map tokens with NLP-derived features, eliminating the
dependency on predefined specifications or exhaustive class taxonomies. Second,
we introduce a point-level SD map encoder together with orthogonal element
identifiers to uniformly integrate all types of map elements. Experiments on
Argoverse 2 and nuScenes show that this boosts map perception performance by up
to +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP
(+20%) w.r.t. previous approaches that already use SD map priors. Code is
available at https://github.com/immel-f/SDTagNet

</details>


### [161] [Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation](https://arxiv.org/abs/2506.08611)
*Shiji Zhao,Chi Chen,Ranjie Duan,Xizhe Wang,Xingxing Wei*

Main category: cs.CV

TL;DR: This paper proposes Anti-Bias Soft Label Distillation (ABSLD) to address the robust fairness issue in adversarial training, improving both adversarial robustness and fairness across classes.


<details>
  <summary>Details</summary>
Motivation: Adversarial Training (AT) and Adversarial Robustness Distillation (ARD) face a robust fairness issue where the adversarial robustness of models differs across classes, being higher for easier classes and lower for harder ones.

Method: The paper introduces ABSLD, a method that adjusts the class-wise smoothness degree of teacher’s soft labels during training through varying temperatures, thereby reducing the robustness gap between classes.

Result: Experiments demonstrate that ABSLD achieves superior robustness and fairness compared to state-of-the-art methods.

Conclusion: ABSLD effectively improves both adversarial robustness and fairness across classes and is adaptable to other label-based or sample-based approaches, making it an important contribution to adversarial training.

Abstract: Adversarial Training (AT) is widely recognized as an effective approach to
enhance the adversarial robustness of Deep Neural Networks. As a variant of AT,
Adversarial Robustness Distillation (ARD) has shown outstanding performance in
enhancing the robustness of small models. However, both AT and ARD face robust
fairness issue: these models tend to display strong adversarial robustness
against some classes (easy classes) while demonstrating weak adversarial
robustness against others (hard classes). This paper explores the underlying
factors of this problem and points out the smoothness degree of soft labels for
different classes significantly impacts the robust fairness from both empirical
observation and theoretical analysis. Based on the above exploration, we
propose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge
Distillation framework to enhance the adversarial robust fairness.
Specifically, ABSLD adaptively reduces the student's error risk gap between
different classes, which is accomplished by adjusting the class-wise smoothness
degree of teacher's soft labels during the training process, and the adjustment
is managed by assigning varying temperatures to different classes.
Additionally, as a label-based approach, ABSLD is highly adaptable and can be
integrated with the sample-based methods. Extensive experiments demonstrate
ABSLD outperforms state-of-the-art methods on the comprehensive performance of
robustness and fairness.

</details>


### [162] [Data-Efficient Challenges in Visual Inductive Priors: A Retrospective](https://arxiv.org/abs/2506.08612)
*Robert-Jan Bruintjes,Attila Lengyel,Osman Semih Kayhan,Davide Zambrano,Nergis Tömen,Hadi Jamali-Rad,Jan van Gemert*

Main category: cs.CV

TL;DR: The paper discusses the challenges and insights from the VIPriors workshop series, focusing on training computer vision models with limited data and restrictions on transfer learning.


<details>
  <summary>Details</summary>
Motivation: To address the issue of data insufficiency in deep learning, particularly in computer vision, and to encourage novel techniques for training models under constrained data conditions.

Method: Organizing the "VIPriors" workshop series with data-deficient challenges limiting participants to train models from scratch using few training samples, excluding transfer learning, and encouraging inductive priors.

Result: Successful entries utilized model ensembles combining Transformers and CNNs, heavy data augmentation techniques, and novel prior knowledge-based approaches.

Conclusion: The workshops demonstrate that leveraging inductive priors, data augmentation, and ensemble methods can enhance data efficiency in deep learning, promoting development of innovative techniques for constrained environments.

Abstract: Deep Learning requires large amounts of data to train models that work well.
In data-deficient settings, performance can be degraded. We investigate which
Deep Learning methods benefit training models in a data-deficient setting, by
organizing the "VIPriors: Visual Inductive Priors for Data-Efficient Deep
Learning" workshop series, featuring four editions of data-impaired challenges.
These challenges address the problem of training deep learning models for
computer vision tasks with limited data. Participants are limited to training
models from scratch using a low number of training samples and are not allowed
to use any form of transfer learning. We aim to stimulate the development of
novel approaches that incorporate prior knowledge to improve the data
efficiency of deep learning models. Successful challenge entries make use of
large model ensembles that mix Transformers and CNNs, as well as heavy data
augmentation. Novel prior knowledge-based methods contribute to success in some
entries.

</details>


### [163] [SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything](https://arxiv.org/abs/2506.08613)
*Joost van Dalen,Yuki M. Asano,Marc Russwurm*

Main category: cs.CV

TL;DR: The paper introduces "SAMSelect," an algorithm for creating effective visualizations of multispectral images to help in identifying marine debris using Sentinel-2 imagery.


<details>
  <summary>Details</summary>
Motivation: Marine debris is hard to visualize and interpret due to its diverse composition and medium-resolution imagery. Despite this challenge, domain experts rely on common practices and heuristics for visual band selection.

Method: SAMSelect uses the Segment Anything Model to determine the best three-channel band or index combinations based on classification accuracy on a small annotated dataset.

Result: Evaluation of SAMSelect in Sentinel-2 scenes shows that it discovers new, effective band combinations that outperform traditional literature-based indices.

Conclusion: SAMSelect improves visual photo interpretation of marine debris and offers an open-source solution useful for domain scientists, especially in marine studies.

Abstract: This work proposes SAMSelect, an algorithm to obtain a salient three-channel
visualization for multispectral images. We develop SAMSelect and show its use
for marine scientists visually interpreting floating marine debris in
Sentinel-2 imagery. These debris are notoriously difficult to visualize due to
their compositional heterogeneity in medium-resolution imagery. Out of these
difficulties, a visual interpretation of imagery showing marine debris remains
a common practice by domain experts, who select bands and spectral indices on a
case-by-case basis informed by common practices and heuristics. SAMSelect
selects the band or index combination that achieves the best classification
accuracy on a small annotated dataset through the Segment Anything Model. Its
central assumption is that the three-channel visualization achieves the most
accurate segmentation results also provide good visual information for
photo-interpretation.
  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine
debris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets
from the Plastic Litter Project. This reveals the potential of new previously
unused band combinations (e.g., a normalized difference index of B8, B2), which
demonstrate improved performance compared to literature-based indices. We
describe the algorithm in this paper and provide an open-source code repository
that will be helpful for domain scientists doing visual photo interpretation,
especially in the marine field.

</details>


### [164] [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://arxiv.org/abs/2506.08619)
*Gonçalo Dias Pais,Valter Piedade,Moitreya Chatterjee,Marcus Greiff,Pedro Miraldo*

Main category: cs.CV

TL;DR: This paper improves 3D scene rendering and surface reconstruction by developing a targeted sampling strategy and a new surface reconstruction loss.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF-based methods face scalability limitations, preventing them from utilizing all possible input data, such as every pixel and potential 3D point along projection rays. This affects the accuracy of synthesized images and surface reconstruction.

Method: The authors leverage implicit surface representation to model a probability density function in a 3D image projection space. This enables targeted sampling toward regions of interest. They also propose a novel surface reconstruction loss with near-to-surface and empty space components.

Result: Integrating the novel sampling strategy and loss into state-of-the-art neural surface renderers leads to more accurate and detailed 3D reconstructions and improved image rendering.

Conclusion: The proposed methods enhance the overall performance of neural implicit surface rendering, particularly in regions of interest, improving the accuracy and rendering quality of 3D scenes.

Abstract: Several variants of Neural Radiance Fields (NeRFs) have significantly
improved the accuracy of synthesized images and surface reconstruction of 3D
scenes/objects. In all of these methods, a key characteristic is that none can
train the neural network with every possible input data, specifically, every
pixel and potential 3D point along the projection rays due to scalability
issues. While vanilla NeRFs uniformly sample both the image pixels and 3D
points along the projection rays, some variants focus only on guiding the
sampling of the 3D points along the projection rays. In this paper, we leverage
the implicit surface representation of the foreground scene and model a
probability density function in a 3D image projection space to achieve a more
targeted sampling of the rays toward regions of interest, resulting in improved
rendering. Additionally, a new surface reconstruction loss is proposed for
improved performance. This new loss fully explores the proposed 3D image
projection space model and incorporates near-to-surface and empty space
components. By integrating our novel sampling strategy and novel loss into
current state-of-the-art neural implicit surface renderers, we achieve more
accurate and detailed 3D reconstructions and improved image rendering,
especially for the regions of interest in any given scene.

</details>


### [165] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/abs/2506.08629)
*Feixiang Du,Shengkun Wu*

Main category: cs.CV

TL;DR: The paper introduces ECMNet, a lightweight model for semantic segmentation, combining CNNs and Mamba in a capsule-based framework to enhance global context modeling.


<details>
  <summary>Details</summary>
Motivation: To address the complementary weaknesses of CNNs and Transformers in semantic segmentation, mainly in global context modeling.

Method: Proposes Efficient CNN-Mamba Network (ECMNet) that combines CNN with Mamba in a dual-attention block and multi-scale attention framework, along with a feature fusion module.

Result: Achieves competitive performance on Cityscapes (70.6% mIoU) and CamVid (73.6% mIoU) datasets with only 0.87M parameters and 8.27G FLOPs using an RTX 3090 GPU.

Conclusion: ECMNet successfully balances accuracy and efficiency, making it a strong contender for semantic segmentation tasks.

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [166] [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](https://arxiv.org/abs/2506.08632)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Dong Chen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: This paper introduces RoboSwap, a video editing framework that enables swapping robotic arms across videos using unpaired data and incorporating GANs and diffusion models.


<details>
  <summary>Details</summary>
Motivation: Address the lack of diverse, high-quality datasets hindering cross-embodiment robotic learning by developing a method to swap robotic arms in videos without needing paired data from identical environments.

Method: RoboSwap employs a novel video editing pipeline integrating GANs and diffusion models. Robotic arms are segmented from video backgrounds, translated using GANs, and then refined with diffusion models for realism and interaction coherence.

Result: RoboSwap achieved superior performance compared to state-of-the-art editing methods on three benchmarks, demonstrating strong structural coherence and motion consistency.

Conclusion: RoboSwap provides a robust solution for enabling cross-embodiment learning in robotic systems by efficiently generating diverse, high-quality datasets through unpaired video editing techniques.

Abstract: Recent advancements in generative models have revolutionized video synthesis
and editing. However, the scarcity of diverse, high-quality datasets continues
to hinder video-conditioned robotic learning, limiting cross-platform
generalization. In this work, we address the challenge of swapping a robotic
arm in one video with another: a key step for crossembodiment learning. Unlike
previous methods that depend on paired video demonstrations in the same
environmental settings, our proposed framework, RoboSwap, operates on unpaired
data from diverse environments, alleviating the data collection needs. RoboSwap
introduces a novel video editing pipeline integrating both GANs and diffusion
models, combining their isolated advantages. Specifically, we segment robotic
arms from their backgrounds and train an unpaired GAN model to translate one
robotic arm to another. The translated arm is blended with the original video
background and refined with a diffusion model to enhance coherence, motion
realism and object interaction. The GAN and diffusion stages are trained
independently. Our experiments demonstrate that RoboSwap outperforms
state-of-the-art video and image editing models on three benchmarks in terms of
both structural coherence and motion consistency, thereby offering a robust
solution for generating reliable, cross-embodiment data in robotic learning.

</details>


### [167] [SurfR: Surface Reconstruction with Multi-scale Attention](https://arxiv.org/abs/2506.08635)
*Siddhant Ranade,Gonçalo Dias Pais,Ross Tyler Whitaker,Jacinto C. Nascimento,Pedro Miraldo,Srikumar Ramalingam*

Main category: cs.CV

TL;DR: The paper introduces a fast and accurate surface reconstruction algorithm for unorganized point clouds that utilizes a novel implicit representation, optimizing speed and accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D shape reconstruction are either highly detailed but require per-object training, or generalized but lack detail and are computationally slow.

Method: The approach involves three innovations: 1) lazy query feature extraction, 2) parallel multi-scale grid representation for robust feature development, and 3) attention across scales for improved reconstruction.

Result: The proposed method is faster than all baseline methods at their optimum resolution, with only a small loss in performance compared to state-of-the-art techniques.

Conclusion: The study provides an effective solution that balances speed and accuracy, contributing to advancements in surface reconstruction for unorganized point clouds.

Abstract: We propose a fast and accurate surface reconstruction algorithm for
unorganized point clouds using an implicit representation. Recent learning
methods are either single-object representations with small neural models that
allow for high surface details but require per-object training or generalized
representations that require larger models and generalize to newer shapes but
lack details, and inference is slow. We propose a new implicit representation
for general 3D shapes that is faster than all the baselines at their optimum
resolution, with only a marginal loss in performance compared to the
state-of-the-art. We achieve the best accuracy-speed trade-off using three key
contributions. Many implicit methods extract features from the point cloud to
classify whether a query point is inside or outside the object. First, to speed
up the reconstruction, we show that this feature extraction does not need to
use the query point at an early stage (lazy query). Second, we use a parallel
multi-scale grid representation to develop robust features for different noise
levels and input resolutions. Finally, we show that attention across scales can
provide improved reconstruction results.

</details>


### [168] [Orientation Matters: Making 3D Generative Models Orientation-Aligned](https://arxiv.org/abs/2506.08640)
*Yichong Lu,Yuzhuo Tian,Zijin Jiang,Yikun Zhao,Yuanbo Yang,Hao Ouyang,Haoji Hu,Huimin Yu,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: This paper proposes a method to generate consistently oriented 3D objects from images and introduces a corresponding dataset, Objaverse-OA, for training.


<details>
  <summary>Details</summary>
Motivation: Existing 3D generative models struggle with misalignment due to inconsistent training data, hindering usability in practical applications.

Method: The authors fine-tune two 3D generative models (multi-view diffusion and 3D variational autoencoder frameworks) using a newly constructed dataset, Objaverse-OA, containing 14,832 orientation-aligned 3D models.

Result: The fine-tuned models produce better-aligned 3D objects, outperforming post-hoc alignment methods and generalizing well to unseen object categories.

Conclusion: The proposed method improves 3D object generation alignment, enabling practical applications like object orientation estimation and manipulation.

Abstract: Humans intuitively perceive object shape and orientation from a single image,
guided by strong priors about canonical poses. However, existing 3D generative
models often produce misaligned results due to inconsistent training data,
limiting their usability in downstream tasks. To address this gap, we introduce
the task of orientation-aligned 3D object generation: producing 3D objects from
single images with consistent orientations across categories. To facilitate
this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D
models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two
representative 3D generative models based on multi-view diffusion and 3D
variational autoencoder frameworks to produce aligned objects that generalize
well to unseen objects across various categories. Experimental results
demonstrate the superiority of our method over post-hoc alignment approaches.
Furthermore, we showcase downstream applications enabled by our aligned object
generation, including zero-shot object orientation estimation via
analysis-by-synthesis and efficient arrow-based object rotation manipulation.

</details>


### [169] [Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization](https://arxiv.org/abs/2506.08649)
*Zhiyi Zhu,Xiaoyu Wu,Youwei Lu*

Main category: cs.CV

TL;DR: This paper introduces a novel model (TMCCL) to predict video memorability by enhancing motion feature representation using text-motion cross-modal contrastive loss. It achieves state-of-the-art performance and proposes an application (MWCVS) for video summarization.


<details>
  <summary>Details</summary>
Motivation: Current video memorability prediction models inadequately use motion cues due to limited labeled data in the fine-tuning process.

Method: The proposed TMCCL model uses text description similarities across videos to create motion sample sets for improved motion feature representation in memorability prediction.

Result: TMCCL outperforms existing models on two prediction datasets, and MWCVS improves video summarization accuracy on two summarization datasets.

Conclusion: The integration of enhanced motion features and cross-modal contrastive loss significantly advances video memorability prediction and its practical applications like video summarization.

Abstract: Video memorability refers to the ability of videos to be recalled after
viewing, playing a crucial role in creating content that remains memorable.
Existing models typically focus on extracting multimodal features to predict
video memorability scores but often fail to fully utilize motion cues. The
representation of motion features is compromised during the fine-tuning phase
of the motion feature extractor due to a lack of labeled data. In this paper,
we introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal
video memorability prediction model designed to enhance the representation of
motion features. We tackle the challenge of improving motion feature
representation by leveraging text description similarities across videos to
establish positive and negative motion sample sets for a given target. This
enhancement allows the model to learn similar feature representations for
semantically related motion content, resulting in more accurate memorability
predictions. Our model achieves state-of-the-art performance on two video
memorability prediction datasets. Moreover, the potential applications of video
memorability prediction have been underexplored. To address this gap, we
present Memorability Weighted Correction for Video Summarization (MWCVS), using
video memorability prediction to reduce subjectivity in video summarization
labels. Experimental results on two video summarization datasets demonstrate
the effectiveness of MWCVS, showcasing the promising applications of video
memorability prediction.

</details>


### [170] [Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping](https://arxiv.org/abs/2506.08650)
*Peter Grönquist,Stepan Tulyakov,Dengxin Dai*

Main category: cs.CV

TL;DR: The paper introduces "Neural Physical Model" (NPM), an approach addressing camera color consistency across devices, outperforming existing methods in robustness and illumination adaptation.


<details>
  <summary>Details</summary>
Motivation: Ensuring consistent color reproduction across camera systems is critical for image fusion and compatibility in modern devices, but challenging due to variations in sensors and optics.

Method: The proposed Neural Physical Model uses a lightweight, physically-informed framework to simulate camera raw images under varying illumination, estimating device transformations both with and without paired data.

Result: Experiments on datasets like NUS and BeyondRGB show that NPM surpasses state-of-the-art methods in achieving chromatic consistency across various sensors and optical systems.

Conclusion: NPM provides an adaptable, efficient solution for camera color consistency, overcoming limitations of older methods, and ensuring better compatibility in imaging pipelines.

Abstract: Achieving consistent color reproduction across multiple cameras is essential
for seamless image fusion and Image Processing Pipeline (ISP) compatibility in
modern devices, but it is a challenging task due to variations in sensors and
optics. Existing raw-to-raw conversion methods face limitations such as poor
adaptability to changing illumination, high computational costs, or impractical
requirements such as simultaneous camera operation and overlapping
fields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,
physically-informed approach that simulates raw images under specified
illumination to estimate transformations between devices. The NPM effectively
adapts to varying illumination conditions, can be initialized with physical
measurements, and supports training with or without paired data. Experiments on
public datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent
state-of-the-art methods, providing robust chromatic consistency across
different sensors and optical systems.

</details>


### [171] [LLaVA-c: Continual Improved Visual Instruction Tuning](https://arxiv.org/abs/2506.08666)
*Wenzhuo Liu,Fei Zhu,Haiyang Guo,Longhui Wei,Cheng-Lin Liu*

Main category: cs.CV

TL;DR: The paper introduces a method for continual learning leveraging modifications on LLaVA-1.5 that address task balance and prevent base model degradation, achieving strong performance compared to multitask learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in multitask learning such as task balancing, expansion costs, and catastrophic forgetting, as well as the shortcomings of existing continual learning methods that often lead to base model degradation.

Method: The proposed method makes two modifications to the LLaVA-1.5 model: spectral-aware consolidation for better task balance and unsupervised inquiry regularization to avoid base model degradation.

Result: The experiments demonstrate that the proposed method excels in both general and task-specific performance. It achieves benchmark results comparable or superior to multitask joint learning while preserving general capabilities.

Conclusion: Task-by-task continual learning, enhanced by the proposed method, is a viable and strong alternative to multitask joint learning, overcoming previous limitations and enabling sustained high performance.

Abstract: Multimodal models like LLaVA-1.5 achieve state-of-the-art visual
understanding through visual instruction tuning on multitask datasets, enabling
strong instruction-following and multimodal performance. However, multitask
learning faces challenges such as task balancing, requiring careful adjustment
of data proportions, and expansion costs, where new tasks risk catastrophic
forgetting and need costly retraining. Continual learning provides a promising
alternative to acquiring new knowledge incrementally while preserving existing
capabilities. However, current methods prioritize task-specific performance,
neglecting base model degradation from overfitting to specific instructions,
which undermines general capabilities. In this work, we propose a simple but
effective method with two modifications on LLaVA-1.5: spectral-aware
consolidation for improved task balance and unsupervised inquiry regularization
to prevent base model degradation. We evaluate both general and task-specific
performance across continual pretraining and fine-tuning. Experiments
demonstrate that LLaVA-c consistently enhances standard benchmark performance
and preserves general capabilities. For the first time, we show that
task-by-task continual learning can achieve results that match or surpass
multitask joint learning. The code will be publicly released.

</details>


### [172] [ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction](https://arxiv.org/abs/2506.08678)
*Juan Yeo,Soonwoo Cha,Jiwoo Song,Hyunbin Jin,Taesup Kim*

Main category: cs.CV

TL;DR: The paper proposes Any-to-Any Self-Distillation (ATAS), a method for improving CLIP-based models in dense prediction tasks by enhancing semantic coherence and fine-grained alignment through self-distillation on unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models like CLIP struggle with fine-grained, region-level understanding, impacting their efficacy in dense prediction tasks.

Method: The authors introduce ATAS, which leverages self-distillation across all representation levels of the model using only unlabeled images. This approach refines the vision encoder components of CLIP to balance semantic coherence and local detail recognition.

Result: The ATAS method achieves significant performance improvements over baseline CLIP models on benchmarks for open-vocabulary object detection and semantic segmentation.

Conclusion: The study demonstrates the necessity of maintaining both semantic coherence and fine-grained alignment for enhancing dense prediction tasks and validates ATAS as an effective unsupervised strategy.

Abstract: Vision-language models such as CLIP have recently propelled open-vocabulary
dense prediction tasks by enabling recognition of a broad range of visual
concepts. However, CLIP still struggles with fine-grained, region-level
understanding, hindering its effectiveness on these dense prediction tasks. We
identify two pivotal factors required to address this limitation: semantic
coherence and fine-grained vision-language alignment. Current adaptation
methods often improve fine-grained alignment at the expense of semantic
coherence, and often rely on extra modules or supervised fine-tuning. To
overcome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel
approach that simultaneously enhances semantic coherence and fine-grained
alignment by leveraging own knowledge of a model across all representation
levels. Unlike prior methods, ATAS uses only unlabeled images and an internal
self-distillation process to refine representations of CLIP vision encoders,
preserving local semantic consistency while sharpening local detail
recognition. On open-vocabulary object detection and semantic segmentation
benchmarks, ATAS achieves substantial performance gains, outperforming baseline
CLIP models. These results validate the effectiveness of our approach and
underscore the importance of jointly maintaining semantic coherence and
fine-grained alignment for advanced open-vocabulary dense prediction.

</details>


### [173] [CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities](https://arxiv.org/abs/2506.08690)
*Hugo Porta,Emanuele Dalsasso,Jessica L. McCarty,Devis Tuia*

Main category: cs.CV

TL;DR: The paper introduces CanadaFireSat, a high-resolution benchmark dataset for wildfire forecasting in Canada, leveraging multi-modal data to improve prediction performance.


<details>
  <summary>Details</summary>
Motivation: The 2023 severe wildfire season in Canada highlighted the need for better mitigation and forecasting solutions, especially in boreal ecosystems impacted by climate change.

Method: This study uses multi-modal data combining Sentinel-2 L1C satellite images, MODIS products, and ERA5 reanalysis data to train deep learning-based wildfire forecasting models at 100 m resolution.

Result: Multi-modal temporal inputs outperform single-modal ones, achieving a peak F1 score of 60.3% for forecasting wildfires in the 2023 season, demonstrating efficacy despite no prior exposure during model training.

Conclusion: High-resolution multi-modal deep learning models show strong potential for improving wildfire probability mapping across continental scales, aiding mitigation strategies.

Abstract: Canada experienced in 2023 one of the most severe wildfire seasons in recent
history, causing damage across ecosystems, destroying communities, and emitting
large quantities of CO2. This extreme wildfire season is symptomatic of a
climate-change-induced increase in the length and severity of the fire season
that affects the boreal ecosystem. Therefore, it is critical to empower
wildfire management in boreal communities with better mitigation solutions.
Wildfire probability maps represent an important tool for understanding the
likelihood of wildfire occurrence and the potential severity of future
wildfires. The massive increase in the availability of Earth observation data
has enabled the development of deep learning-based wildfire forecasting models,
aiming at providing precise wildfire probability maps at different spatial and
temporal scales. A main limitation of such methods is their reliance on
coarse-resolution environmental drivers and satellite products, leading to
wildfire occurrence prediction of reduced resolution, typically around $\sim
0.1${\deg}. This paper presents a benchmark dataset: CanadaFireSat, and
baseline methods for high-resolution: 100 m wildfire forecasting across Canada,
leveraging multi-modal data from high-resolution multi-spectral satellite
images (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and
environmental factors (ERA5 reanalysis data). Our experiments consider two
major deep learning architectures. We observe that using multi-modal temporal
inputs outperforms single-modal temporal inputs across all metrics, achieving a
peak performance of 60.3% in F1 score for the 2023 wildfire season, a season
never seen during model training. This demonstrates the potential of
multi-modal deep learning models for wildfire forecasting at high-resolution
and continental scale.

</details>


### [174] [VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism](https://arxiv.org/abs/2506.08691)
*Congzhi Zhang,Jiawei Peng,Zhenglin Wang,Yilong Lai,Haowen Sun,Heng Chang,Fei Ma,Weijiang Yu*

Main category: cs.CV

TL;DR: The paper introduces VReST, a novel training-free approach to improve reasoning in Large Vision-Language Models (LVLMs) via Monte Carlo Tree Search and Self-Reward mechanisms, achieving state-of-the-art performance in multimodal reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs struggle with complex visual reasoning, particularly when using Chain-of-Thought prompting techniques.

Method: The authors propose VReST, which employs Monte Carlo Tree Search and a multimodal Self-Reward mechanism to systematically evaluate reasoning steps without requiring additional models.

Result: VReST achieves state-of-the-art results on three multimodal mathematical reasoning benchmarks, exceeding prior prompting methods.

Conclusion: The approach demonstrates the potential of test-time scaling laws in multimodal reasoning and offers a new direction for enhancing reasoning in LVLMs.

Abstract: Large Vision-Language Models (LVLMs) have shown exceptional performance in
multimodal tasks, but their effectiveness in complex visual reasoning is still
constrained, especially when employing Chain-of-Thought prompting techniques.
In this paper, we propose VReST, a novel training-free approach that enhances
Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.
VReST meticulously traverses the reasoning landscape by establishing a search
tree, where each node encapsulates a reasoning step, and each path delineates a
comprehensive reasoning sequence. Our innovative multimodal Self-Reward
mechanism assesses the quality of reasoning steps by integrating the utility of
sub-questions, answer correctness, and the relevance of vision-language clues,
all without the need for additional models. VReST surpasses current prompting
methods and secures state-of-the-art performance across three multimodal
mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy
of test-time scaling laws in multimodal tasks, offering a promising direction
for future research.

</details>


### [175] [MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning](https://arxiv.org/abs/2506.08694)
*Mohammadreza Salehi,Shashanka Venkataramanan,Ioana Simion,Efstratios Gavves,Cees G. M. Snoek,Yuki M Asano*

Main category: cs.CV

TL;DR: This paper introduces a motion-guided self-supervised learning framework for videos, addressing challenges in spatiotemporal representation learning due to motion dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing dense self-supervised learning methods struggle with inconsistencies caused by object deformations, occlusions, and camera movement in videos, limiting their ability to learn spatiotemporal representations.

Method: The method clusters dense point tracks using an off-the-shelf point tracker and a momentum-encoder-based optimal transport mechanism. Temporal coherence is enforced through the propagation of cluster assignments across tracked points, ensuring consistency across dynamic frames.

Result: The proposed framework shows 1% to 6% improvement over state-of-the-art methods across six datasets and four evaluation benchmarks, showcasing robustness in dynamic and occlusion-heavy scenarios.

Conclusion: By integrating motion as an implicit supervisory signal, the method effectively learns generalized spatiotemporally coherent representations, outperforming existing benchmarks while leveraging video data on top of image-pretrained models.

Abstract: Dense self-supervised learning has shown great promise for learning pixel-
and patch-level representations, but extending it to videos remains challenging
due to the complexity of motion dynamics. Existing approaches struggle as they
rely on static augmentations that fail under object deformations, occlusions,
and camera movement, leading to inconsistent feature learning over time. We
propose a motion-guided self-supervised learning framework that clusters dense
point tracks to learn spatiotemporally consistent representations. By
leveraging an off-the-shelf point tracker, we extract long-range motion
trajectories and optimize feature clustering through a momentum-encoder-based
optimal transport mechanism. To ensure temporal coherence, we propagate cluster
assignments along tracked points, enforcing feature consistency across views
despite viewpoint changes. Integrating motion as an implicit supervisory
signal, our method learns representations that generalize across frames,
improving robustness in dynamic scenes and challenging occlusion scenarios. By
initializing from strong image-pretrained models and leveraging video data for
training, we improve state-of-the-art by 1% to 6% on six image and video
datasets and four evaluation benchmarks. The implementation is publicly
available at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main

</details>


### [176] [ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds](https://arxiv.org/abs/2506.08699)
*Frederik Hagelskjaer*

Main category: cs.CV

TL;DR: The paper introduces a fast neural network for detecting and estimating the 5 DoF pose of objects in colorless point clouds with state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective, fast, and accurate methods for detecting and estimating object poses in colorless point clouds.

Method: Trained a neural network on synthetic data to predict object pose using center and top points, with testing conducted on a benchmark dataset.

Result: Achieved state-of-the-art performance, outperforming other colorless methods, with an inference time of 250 milliseconds.

Conclusion: The proposed network is efficient, accurate, and practical for real-time applications involving colorless point clouds.

Abstract: This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose
estimation network for colorless point clouds. The pose estimation is
calculated from center and top points of the object, predicted by the neural
network. The network is trained on synthetic data, and tested on a benchmark
dataset, where it demonstrates state-of-the-art performance and outperforms all
colorless methods. The network is able to run inference in only 250
milliseconds making it usable in many scenarios. Project page with code at
arrowpose.github.io

</details>


### [177] [TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering](https://arxiv.org/abs/2506.08704)
*Xiaohan Zhang,Sitong Wang,Yushen Yan,Yi Yang,Mingda Xu,Qi Liu*

Main category: cs.CV

TL;DR: Novel view synthesis for large-scale scenes is improved using TraGraph-GS, which utilizes a trajectory graph for better rendering precision and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing approaches partition scenes into regions for rendering, but struggle with arbitrary camera trajectories and texture distortions due to overlapping regions.

Method: A trajectory graph-based spatial partitioning method with regularization constraints, coupled with a progressive rendering strategy to address Gaussian overlap artifacts.

Result: TraGraph-GS demonstrated better rendering performance with gains of 1.86 dB and 1.62 dB in PSNR for aerial and ground datasets, respectively.

Conclusion: The proposed method overcomes generalization challenges in large-scale scene rendering and achieves high-quality outputs efficiently.

Abstract: High-quality novel view synthesis for large-scale scenes presents a
challenging dilemma in 3D computer vision. Existing methods typically partition
large scenes into multiple regions, reconstruct a 3D representation using
Gaussian splatting for each region, and eventually merge them for novel view
rendering. They can accurately render specific scenes, yet they do not
generalize effectively for two reasons: (1) rigid spatial partition techniques
struggle with arbitrary camera trajectories, and (2) the merging of regions
results in Gaussian overlap to distort texture details. To address these
challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable
high-precision rendering for arbitrarily large-scale scenes. We present a
spatial partitioning method for large-scale scenes based on graphs, which
incorporates a regularization constraint to enhance the rendering of textures
and distant objects, as well as a progressive rendering strategy to mitigate
artifacts caused by Gaussian overlap. Experimental results demonstrate its
superior performance both on four aerial and four ground datasets and highlight
its remarkable efficiency: our method achieves an average improvement of 1.86
dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to
state-of-the-art approaches.

</details>


### [178] [SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting](https://arxiv.org/abs/2506.08710)
*Mengjiao Ma,Qi Ma,Yue Li,Jiahuan Cheng,Runyi Yang,Bin Ren,Nikola Popovic,Mingqiang Wei,Nicu Sebe,Luc Van Gool,Theo Gevers,Martin R. Oswald,Danda Pani Paudel*

Main category: cs.CV

TL;DR: The paper introduces a large-scale benchmark to evaluate three language-grounded 3D Gaussian Splatting methods directly in 3D space and proposes a 49K-scene dataset to enable better generalizable approaches for scene understanding.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian Splatting approaches for scene understanding are limited in their evaluation scope, focusing only on rendered 2D views and specific training data. There is a need for comprehensive assessment in holistic 3D space to improve their generalization capabilities.

Method: The authors propose a large-scale benchmark to assess three method paradigms (optimization-based, optimization-free, and generalizable) across 1060 scenes and four datasets. They also create "GaussianWorld-49K," a diverse dataset of nearly 49,000 scenes, for advancing generalizable models.

Result: Benchmark results show that the generalizable approach outperforms others, especially in enabling faster inference for novel scenes and achieving better segmentation performance.

Conclusion: The study highlights the importance of generalizable models for 3D Gaussian Splatting in scene understanding and provides tools and datasets to foster further research in this domain.

Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient
encoding of scene geometry, appearance, and semantics. Moreover, grounding
language in 3D scenes has proven to be an effective strategy for 3D scene
understanding. Current Language Gaussian Splatting line of work fall into three
main groups: (i) per-scene optimization-based, (ii) per-scene
optimization-free, and (iii) generalizable approach. However, most of them are
evaluated only on rendered 2D views of a handful of scenes and viewpoints close
to the training views, limiting ability and insight into holistic 3D
understanding. To address this gap, we propose the first large-scale benchmark
that systematically assesses these three groups of methods directly in 3D
space, evaluating on 1060 scenes across three indoor datasets and one outdoor
dataset. Benchmark results demonstrate a clear advantage of the generalizable
paradigm, particularly in relaxing the scene-specific limitation, enabling fast
feed-forward inference on novel scenes, and achieving superior segmentation
performance. We further introduce GaussianWorld-49K a carefully curated 3DGS
dataset comprising around 49K diverse indoor and outdoor scenes obtained from
multiple sources, with which we demonstrate the generalizable approach could
harness strong data priors. Our codes, benchmark, and datasets will be made
public to accelerate research in generalizable 3DGS scene understanding.

</details>


### [179] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/abs/2506.08729)
*Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: The paper presents a method using SE(3)-symmetric transformer models to predict abdominal aortic aneurysm (AAA) growth based on vascular surfaces, outperforming standardized monitoring methods.


<details>
  <summary>Details</summary>
Motivation: Current AAA monitoring with standardized intervals, based on maximum diameter, does not account for the complex relationship between 3D AAA shape and its growth, creating a need for personalized predictions.

Method: An SE(3)-symmetric transformer model is employed to predict AAA growth by analyzing the vascular surface enriched with local, multi-physical features, maintaining anatomical and geometric fidelity.

Result: The model achieved a median diameter prediction error of 1.18 mm and accuracy of 0.93 in identifying patients eligible for surgery within two years. It generalizes well on external validation datasets.

Conclusion: Local directional AAA growth prediction using vascular surfaces is feasible and has the potential to improve personalized monitoring strategies for patients.

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [180] [InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba](https://arxiv.org/abs/2506.08735)
*Yuhang Wang,Jun Li,Zhijian Wu,Jianhua Xu*

Main category: cs.CV

TL;DR: InceptionMamba is a novel model architecture addressing limitations in parallel convolution techniques, achieving state-of-the-art performance in image classification and related tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address shortcomings in InceptionNeXt's spatial and global contextual modeling capabilities in convolution neural networks.

Method: The authors propose replacing one-dimensional strip convolutions with orthogonal band convolutions and integrating a bottleneck Mamba module for enhanced cross-channel fusion and broader receptive fields.

Result: Extensive evaluations show InceptionMamba outperforms existing models in various tasks with better parameter and computational efficiency.

Conclusion: InceptionMamba successfully advances spatial and contextual modeling techniques, demonstrating superior performance across several applications.

Abstract: Within the family of convolutional neural networks, InceptionNeXt has shown
excellent competitiveness in image classification and a number of downstream
tasks. Built on parallel one-dimensional strip convolutions, however, it
suffers from limited ability of capturing spatial dependencies along different
dimensions and fails to fully explore spatial modeling in local neighborhood.
Besides, inherent locality constraints of convolution operations are
detrimental to effective global context modeling. To overcome these
limitations, we propose a novel backbone architecture termed InceptionMamba in
this study. More specifically, the traditional one-dimensional strip
convolutions are replaced by orthogonal band convolutions in our InceptionMamba
to achieve cohesive spatial modeling. Furthermore, global contextual modeling
can be achieved via a bottleneck Mamba module, facilitating enhanced
cross-channel information fusion and enlarged receptive field. Extensive
evaluations on classification and various downstream tasks demonstrate that the
proposed InceptionMamba achieves state-of-the-art performance with superior
parameter and computational efficiency. The source code will be available at
https://github.com/Wake1021/InceptionMamba.

</details>


### [181] [RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2506.08772)
*Jiayi Song,Kaiyu Li,Xiangyong Cao,Deyu Meng*

Main category: cs.CV

TL;DR: The paper introduces RS-MTDF, a framework using pre-trained Vision Foundation Models (VFMs) for semi-supervised semantic segmentation of remote sensing images, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation of remote sensing images requires costly pixel-wise annotations. Semi-supervised semantic segmentation (SSS) reduces this dependency but struggles with distribution mismatches between labeled and unlabeled data.

Method: RS-MTDF leverages pre-trained VFMs like DINOv2 and CLIP as teachers for feature-level distillation. It uses their semantic knowledge to align student features, and incorporates the knowledge into a student decoder for improved segmentation.

Result: RS-MTDF achieves state-of-the-art performance across three remote sensing datasets (ISPRS Potsdam, LoveDA, DeepGlobe), outperforming existing methods in label-scarce scenarios and securing high IoU for semantic categories.

Conclusion: Multi-teacher VFM guidance significantly improves generalization and semantic understanding in semi-supervised remote sensing image segmentation, as validated by experiments and ablation studies.

Abstract: Semantic segmentation in remote sensing images is crucial for various
applications, yet its performance is heavily reliant on large-scale,
high-quality pixel-wise annotations, which are notoriously expensive and
time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a
promising alternative to mitigate this data dependency. However, existing SSS
methods often struggle with the inherent distribution mismatch between limited
labeled data and abundant unlabeled data, leading to suboptimal generalization.
We propose that Vision Foundation Models (VFMs), pre-trained on vast and
diverse datasets, possess robust generalization capabilities that can
effectively bridge this distribution gap and provide strong semantic priors for
SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and
Fusion), a novel framework that leverages the powerful semantic knowledge
embedded in VFMs to guide semi-supervised learning in remote sensing.
Specifically, RS-MTDF employs multiple frozen VFMs (\textit{e.g.}, DINOv2 and
CLIP) as expert teachers, utilizing feature-level distillation to align student
features with their robust representations. To further enhance discriminative
power, the distilled knowledge is seamlessly fused into the student decoder.
Extensive experiments on three challenging remote sensing datasets (ISPRS
Potsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves
state-of-the-art performance. Notably, our method outperforms existing
approaches across various label ratios on LoveDA and secures the highest IoU in
the majority of semantic categories. These results underscore the efficacy of
multi-teacher VFM guidance in significantly enhancing both generalization and
semantic understanding for remote sensing segmentation. Ablation studies
further validate the contribution of each proposed module.

</details>


### [182] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/abs/2506.08777)
*Keyi Liu,Weidong Yang,Ben Fei,Ying He*

Main category: cs.CV

TL;DR: Gaussian2Scene introduces a novel self-supervised learning framework for point cloud pre-training, leveraging 3D Gaussian Splattings to efficiently improve 3D geometric and cross-modal scene understanding.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods for scene-level point cloud pre-training face limitations due to reliance on high-memory-demanding volume rendering and implicit scene representations. Additionally, they struggle to capture 3D geometric structures effectively.

Method: Gaussian2Scene employs 3D Gaussian Splatting (3DGS) for efficient and explicit 3D scene reconstruction. It utilizes a two-stage training process: a dual-branch masked autoencoder for 2D/3D representation learning in stage one, and further refinement using geometric locations of Gaussians and rendered RGB in stage two.

Result: The approach shows consistent improvements in 3D object detection tasks over existing methods, demonstrating its effectiveness in both geometric and cross-modal learning.

Conclusion: Gaussian2Scene offers an efficient and geometry-aware SSL framework for 3D scene understanding, addressing the limitations of prior methods and achieving superior performance in downstream tasks.

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a
cornerstone for many 3D vision tasks, enabling effective learning from
large-scale unannotated data. At the scene level, existing SSL methods often
incorporate volume rendering into the pre-training framework, using RGB-D
images as reconstruction signals to facilitate cross-modal learning. This
strategy promotes alignment between 2D and 3D modalities and enables the model
to benefit from rich visual cues in the RGB-D inputs. However, these approaches
are limited by their reliance on implicit scene representations and high memory
demands. Furthermore, since their reconstruction objectives are applied only in
2D space, they often fail to capture underlying 3D geometric structures. To
address these challenges, we propose Gaussian2Scene, a novel scene-level SSL
framework that leverages the efficiency and explicit nature of 3D Gaussian
Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the
computational burden associated with volume rendering but also supports direct
3D scene reconstruction, thereby enhancing the geometric understanding of the
backbone network. Our approach follows a progressive two-stage training
strategy. In the first stage, a dual-branch masked autoencoder learns both 2D
and 3D scene representations. In the second stage, we initialize training with
reconstructed point clouds and further supervise learning using the geometric
locations of Gaussian primitives and rendered RGB images. This process
reinforces both geometric and cross-modal learning. We demonstrate the
effectiveness of Gaussian2Scene across several downstream 3D object detection
tasks, showing consistent improvements over existing pre-training methods.

</details>


### [183] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Main category: cs.CV

TL;DR: The paper presents Landsat-Bench, a benchmark suite for Landsat imagery, showcasing superior performance of SSL4EO-L pretrained models over ImageNet.


<details>
  <summary>Details</summary>
Motivation: The lack of benchmarks for Landsat data has limited advancements in developing Landsat-based Geospatial Foundation Models (GFMs).

Method: The authors propose Landsat-Bench, which includes three benchmarks adapted from existing datasets (EuroSAT-L, BigEarthNet-L, and LC100-L). They also evaluate performance using SSL4EO-L pretrained models and various architectures.

Result: SSL4EO-L pretrained models demonstrate improved downstream task performance compared to ImageNet, with notable gains of +4% OA and +5.1% mAP on EuroSAT-L and BigEarthNet-L.

Conclusion: Landsat-Bench facilitates the development of Landsat-based GFMs and demonstrates the effectiveness of SSL4EO-L pretrained models for remote sensing tasks.

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [184] [HomographyAD: Deep Anomaly Detection Using Self Homography Learning](https://arxiv.org/abs/2506.08784)
*Jongyub Seok,Chanjin Kang*

Main category: cs.CV

TL;DR: This paper introduces HomographyAD, a novel anomaly detection approach designed for real-world industrial environments where datasets are often misaligned.


<details>
  <summary>Details</summary>
Motivation: Despite the success of recent anomaly detection methods on benchmark datasets like MVTec, their poor performance on misaligned data limits their usability in real-world industrial applications.

Method: The authors propose using ImageNet-pretrained networks combined with deep homography estimation for input foreground alignment and self homography learning for capturing additional shape information. Anomalies are detected by measuring the distance between test sample features and the distribution of normal features.

Result: HomographyAD, when applied to various existing anomaly detection methods, showed performance improvements in extensive experiments.

Conclusion: Deep homography alignment and self-learning can improve anomaly detection in real-world industrial datasets, making the models robust to dataset misalignments.

Abstract: Anomaly detection (AD) is a task that distinguishes normal and abnormal data,
which is important for applying automation technologies of the manufacturing
facilities. For MVTec dataset that is a representative AD dataset for
industrial environment, many recent works have shown remarkable performances.
However, the existing anomaly detection works have a limitation of showing good
performance for fully-aligned datasets only, unlike real-world industrial
environments. To solve this limitation, we propose HomographyAD, a novel deep
anomaly detection methodology based on the ImageNet-pretrained network, which
is specially designed for actual industrial dataset. Specifically, we first
suggest input foreground alignment using the deep homography estimation method.
In addition, we fine-tune the model by self homography learning to learn
additional shape information from normal samples. Finally, we conduct anomaly
detection based on the measure of how far the feature of test sample is from
the distribution of the extracted normal features. By applying our proposed
method to various existing AD approaches, we show performance enhancement
through extensive experiments.

</details>


### [185] [A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory](https://arxiv.org/abs/2506.08793)
*Zhuoran Zheng*

Main category: cs.CV

TL;DR: This paper introduces a novel PDE framework for image dehazing by combining atmospheric scattering models with advanced regularization techniques, demonstrating effective results through GPU computation.


<details>
  <summary>Details</summary>
Motivation: To improve single-image dehazing techniques by addressing both edge preservation and computational efficiency challenges.

Method: An improved PDE framework is proposed using edge-preserving diffusion, Gaussian convolution, and adaptive regularization parameters, alongside weak solution proofs and GPU-accelerated fixed-point iteration.

Result: The experimental results show that the proposed method is effective for dehazing tasks and has potential for integration with deep learning models.

Conclusion: The method offers an advanced and reliable single-image dehazing approach that balances mathematical robustness and practical implementation through efficient computation.

Abstract: This paper presents a novel partial differential equation (PDE) framework for
single-image dehazing. By integrating the atmospheric scattering model with
nonlocal regularization and dark channel prior, we propose the improved PDE: \[
-\text{div}\left(D(\nabla u)\nabla u\right) + \lambda(t) G(u) = \Phi(I,t,A) \]
where $D(\nabla u) = (|\nabla u| + \epsilon)^{-1}$ is the edge-preserving
diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and
$\lambda(t)$ is the adaptive regularization parameter based on transmission map
$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\Omega)$
using Lax-Milgram theorem, and implement an efficient fixed-point iteration
scheme accelerated by PyTorch GPU computation. The experimental results
demonstrate that this method is a promising deghazing solution that can be
generalized to the deep model paradigm.

</details>


### [186] [Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling](https://arxiv.org/abs/2506.08796)
*Zhiyuan Ma,Ruixun Liu,Sixian Liu,Jianjun Li,Bowen Zhou*

Main category: cs.CV

TL;DR: Discretized-RF improves upon existing rectified flow models by introducing momentum fields, which enhance diversity and multi-scale noise modeling in diffusion-based image generation.


<details>
  <summary>Details</summary>
Motivation: Address the shortcomings of straight-path rectified flow models, particularly in terms of sampling diversity and noise modeling efficiencies.

Method: Proposes a discretized approach to RF, incorporating momentum fields with random velocity components to expand the sampling path and improve diversity.

Result: Experiments show enhanced trajectory diversity, noise modeling, and consistent high-quality image generation across datasets.

Conclusion: Discretized-RF succeeds in broadening sampling space and improving generation quality and diversity, showcasing advantages over previous straight-path RF models.

Abstract: Recently, the rectified flow (RF) has emerged as the new state-of-the-art
among flow-based diffusion models due to its high efficiency advantage in
straight path sampling, especially with the amazing images generated by a
series of RF models such as Flux 1.0 and SD 3.0. Although a straight-line
connection between the noisy and natural data distributions is intuitive, fast,
and easy to optimize, it still inevitably leads to: 1) Diversity concerns,
which arise since straight-line paths only cover a fairly restricted sampling
space. 2) Multi-scale noise modeling concerns, since the straight line flow
only needs to optimize the constant velocity field $\bm v$ between the two
distributions $\bm\pi_0$ and $\bm\pi_1$. In this work, we present
Discretized-RF, a new family of rectified flow (also called momentum flow
models since they refer to the previous velocity component and the random
velocity component in each diffusion step), which discretizes the straight path
into a series of variable velocity field sub-paths (namely ``momentum fields'')
to expand the search space, especially when close to the distribution
$p_\text{noise}$. Different from the previous case where noise is directly
superimposed on $\bm x$, we introduce noise on the velocity $\bm v$ of the
sub-path to change its direction in order to improve the diversity and
multi-scale noise modeling abilities. Experimental results on several
representative datasets demonstrate that learning momentum flow matching by
sampling random velocity fields will produce trajectories that are both diverse
and efficient, and can consistently generate high-quality and diverse results.
Code is available at https://github.com/liuruixun/momentum-fm.

</details>


### [187] [HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation](https://arxiv.org/abs/2506.08797)
*Ziyao Huang,Zixiang Zhou,Juan Cao,Yifeng Ma,Yi Chen,Zejing Rao,Zhiyong Xu,Hongmei Wang,Qin Lin,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: HunyuanVideo-HOMA is a framework for generating human-object interaction (HOI) videos with weak supervision, emphasizing sparse guidance and multimodal integration for enhanced generalization and naturalness.


<details>
  <summary>Details</summary>
Motivation: To address limitations in HOI video generation, such as reliance on curated motion data, poor generalization, and restricted accessibility.

Method: The framework uses a multimodal diffusion transformer (MMDiT) with sparse motion guidance, dual input space encoding, shared context fusion, HOI and facial adapters, and weak supervision strategies.

Result: The framework achieves state-of-the-art performance in naturalness and generalization, demonstrating its effectiveness under weak supervision conditions.

Conclusion: HunyuanVideo-HOMA enhances the controllability, generalizability, and accessibility of HOI video generation, supporting novel scenarios and offering a user-friendly interface.

Abstract: To address key limitations in human-object interaction (HOI) video generation
-- specifically the reliance on curated motion data, limited generalization to
novel objects/scenarios, and restricted accessibility -- we introduce
HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.
HunyuanVideo-HOMA enhances controllability and reduces dependency on precise
inputs through sparse, decoupled motion guidance. It encodes appearance and
motion signals into the dual input space of a multimodal diffusion transformer
(MMDiT), fusing them within a shared context space to synthesize temporally
consistent and physically plausible interactions. To optimize training, we
integrate a parameter-space HOI adapter initialized from pretrained MMDiT
weights, preserving prior knowledge while enabling efficient adaptation, and a
facial cross-attention adapter for anatomically accurate audio-driven lip
synchronization. Extensive experiments confirm state-of-the-art performance in
interaction naturalness and generalization under weak supervision. Finally,
HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and
interactive object manipulation, supported by a user-friendly demo interface.
The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.

</details>


### [188] [HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference](https://arxiv.org/abs/2506.08809)
*Jiaze E,Srutarshi Banerjee,Tekin Bicer,Guannan Wang,Yanfu Zhang,Bin Ren*

Main category: cs.CV

TL;DR: HiSin is a new diffusion-based framework for high-resolution sinogram inpainting that reduces memory usage and inference time while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: In computed tomography reconstruction, missing high-frequency projections cause artifacts and diagnostic errors, requiring robust and detail-preserving inpainting methods.

Method: HiSin employs resolution-guided progressive inference, frequency-aware patch skipping, and structure-adaptive step allocation for efficient and accurate sinogram inpainting.

Result: HiSin decreases memory usage by 31.25%, inference time by 18.15%, and maintains accuracy across datasets, resolutions, and mask conditions.

Conclusion: HiSin effectively addresses the challenges of memory and computational demands in high-resolution sinogram inpainting, supporting diagnostic precision and efficiency.

Abstract: High-resolution sinogram inpainting is essential for computed tomography
reconstruction, as missing high-frequency projections can lead to visible
artifacts and diagnostic errors. Diffusion models are well-suited for this task
due to their robustness and detail-preserving capabilities, but their
application to high-resolution inputs is limited by excessive memory and
computational demands. To address this limitation, we propose HiSin, a novel
diffusion based framework for efficient sinogram inpainting via
resolution-guided progressive inference. It progressively extracts global
structure at low resolution and defers high-resolution inference to small
patches, enabling memory-efficient inpainting. It further incorporates
frequency-aware patch skipping and structure-adaptive step allocation to reduce
redundant computation. Experimental results show that HiSin reduces peak memory
usage by up to 31.25% and inference time by up to 18.15%, and maintains
inpainting accuracy across datasets, resolutions, and mask conditions.

</details>


### [189] [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/abs/2506.08817)
*Shuyi Zhang,Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Pengwei Wang,Zhongyuan Wang,Hongxuan Ma,Shanghang Zhang*

Main category: cs.CV

TL;DR: The paper introduces Video-CoT, a dataset aimed at improving spatiotemporal video comprehension through Chain-of-Thought methodologies, comprising 192,000 QA pairs and a benchmark.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of current vision-language models (VLMs) in capturing nuanced spatiotemporal details essential for video comprehension.

Method: The authors develop the Video-CoT dataset, featuring fine-grained spatiotemporal Q&A pairs and Chain-of-Thought annotations, along with a benchmark for evaluation.

Result: Experiments show that current VLMs struggle with satisfactory performance in spatiotemporal understanding when tested against the dataset.

Conclusion: The Video-CoT dataset offers resources that can advance research in multimedia understanding and spatiotemporal video analysis, encouraging innovation in intelligent systems.

Abstract: Video content comprehension is essential for various applications, ranging
from video analysis to interactive systems. Despite advancements in large-scale
vision-language models (VLMs), these models often struggle to capture the
nuanced, spatiotemporal details essential for thorough video analysis. To
address this gap, we introduce Video-CoT, a groundbreaking dataset designed to
enhance spatiotemporal understanding using Chain-of-Thought (CoT)
methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal
question-answer pairs and 23,000 high-quality CoT-annotated samples, providing
a solid foundation for evaluating spatiotemporal understanding in video
comprehension. Additionally, we provide a comprehensive benchmark for assessing
these tasks, with each task featuring 750 images and tailored evaluation
metrics. Our extensive experiments reveal that current VLMs face significant
challenges in achieving satisfactory performance, high-lighting the
difficulties of effective spatiotemporal understanding. Overall, the Video-CoT
dataset and benchmark open new avenues for research in multimedia understanding
and support future innovations in intelligent systems requiring advanced video
analysis capabilities. By making these resources publicly available, we aim to
encourage further exploration in this critical area. Project
website:https://video-cot.github.io/ .

</details>


### [190] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: This paper studies the cultural alignment of text-to-image (T2I) models, revealing high rates of failure in meeting explicit and implicit cultural expectations, and introduces a benchmark 'CulturalFrames' for evaluating this issue.


<details>
  <summary>Details</summary>
Motivation: The use of T2I models is growing, but their ability to accurately represent diverse cultural contexts is under scrutiny, prompting the need to assess and improve their cultural alignment.

Method: The authors introduce 'CulturalFrames,' a benchmark covering visual content from 10 countries and 5 socio-cultural domains, and apply it to 4 state-of-the-art T2I models, collecting over 10,000 human annotations.

Result: The study finds that T2I models fail to meet cultural expectations an average of 44% of the time, with explicit expectations missed at a rate of 68% and implicit ones at 49%. Moreover, existing evaluation metrics poorly correlate with human judgments.

Conclusion: There are significant gaps in T2I models' cultural alignment and their evaluation methodologies, highlighting the need for developing more culturally informed models and evaluation criteria.

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [191] [Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis](https://arxiv.org/abs/2506.08849)
*Jingguo Qu,Xinyang Han,Tonghuan Xiao,Jia Ai,Juan Wu,Tong Zhao,Jing Qin,Ann Dorothy King,Winnie Chiu-Wing Chu,Jing Cai,Michael Tin-Cheung Yingınst*

Main category: cs.CV

TL;DR: This study improves the performance of vision-language foundation models for ultrasound image analysis using fine-tuning and domain adaptation strategies, outperforming the state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: To address the labor-intensive and inconsistent manual contouring in ultrasound imaging and leverage vision-language foundation models for improved analysis while overcoming challenges posed by differences in natural and medical imaging domains.

Method: The researchers developed a fine-tuning pipeline using a large language model as a text refiner, special-designed adaptation strategies, and task-driven heads. They evaluated their method on six ultrasound datasets for segmentation and classification tasks.

Result: The proposed approach significantly enhances the performance of vision-language foundation models in ultrasound image analysis, surpassing existing state-of-the-art models.

Conclusion: The study demonstrates the effectiveness of domain adaptation and fine-tuning in improving vision-language foundation models for analyzing ultrasound images, providing a promising direction for medical imaging technologies.

Abstract: Medical ultrasonography is an essential imaging technique for examining
superficial organs and tissues, including lymph nodes, breast, and thyroid. It
employs high-frequency ultrasound waves to generate detailed images of the
internal structures of the human body. However, manually contouring regions of
interest in these images is a labor-intensive task that demands expertise and
often results in inconsistent interpretations among individuals.
Vision-language foundation models, which have excelled in various computer
vision applications, present new opportunities for enhancing ultrasound image
analysis. Yet, their performance is hindered by the significant differences
between natural and medical imaging domains. This research seeks to overcome
these challenges by developing domain adaptation methods for vision-language
foundation models. In this study, we explore the fine-tuning pipeline for
vision-language foundation models by utilizing large language model as text
refiner with special-designed adaptation strategies and task-driven heads. Our
approach has been extensively evaluated on six ultrasound datasets and two
tasks: segmentation and classification. The experimental results show that our
method can effectively improve the performance of vision-language foundation
models for ultrasound image analysis, and outperform the existing
state-of-the-art vision-language and pure foundation models. The source code of
this study is available at
\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}.

</details>


### [192] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/abs/2506.08854)
*Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: This paper presents a deep learning method using contrastive learning to predict spatial gene expression from whole-slide images, offering improved prediction accuracy and clinical potential.


<details>
  <summary>Details</summary>
Motivation: The research addresses the high cost of acquiring spatial transcriptomics data for tumor microenvironment analysis and cancer diagnosis, aiming to make data generation more accessible.

Method: The researchers developed a contrastive learning-based deep learning model to predict spatially resolved gene expression from histopathology images across six disease datasets.

Result: The method demonstrated improvements in Pearson Correlation Coefficient for predicting highly expressed, highly variable, and marker genes by 6.27%, 6.11%, and 11.26%, respectively, while preserving gene-gene correlations.

Conclusion: The proposed method is a promising tool for spatial transcriptomics prediction, particularly benefiting scenarios with limited data availability and potential applications in cancer diagnosis and tissue localization.

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [193] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: The paper presents StreamSplat, a framework for real-time dynamic 3D scene reconstruction from uncalibrated video streams using a 3D Gaussian Splatting representation, solving challenges in real-time processing, dynamic modeling, and long-term stability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods, which struggle with real-time processing of uncalibrated inputs, dynamic scene evolution modeling, and computational efficiency.

Method: StreamSplat introduces a feed-forward pipeline with a probabilistic sampling mechanism for 3D Gaussian Splatting position prediction and a bidirectional deformation field for dynamic modeling. These enable online reconstruction of arbitrarily long video streams.

Result: Experiments show that StreamSplat outperforms previous methods in both static and dynamic scene reconstruction quality, handling dynamic evolution efficiently while providing online support.

Conclusion: StreamSplat is a robust solution for real-time 3D scene reconstruction, setting a new standard with improved quality and scalability for dynamic video inputs.

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [194] [DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval](https://arxiv.org/abs/2506.08887)
*Leqi Shen,Guoqiang Gong,Tianxiang Hao,Tao He,Yifeng Zhang,Pengzhang Liu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: The paper introduces DiscoVLA, a method for improving video-text retrieval using CLIP by addressing discrepancies in vision, language, and alignment.


<details>
  <summary>Details</summary>
Motivation: The research focuses on enhancing video-text retrieval using CLIP, which was originally designed for image-text pretraining. Unlike image-level tasks, video-level retrieval requires handling multi-frame contexts and deeper semantic correspondence, which current methods fail to fully address.

Method: DiscoVLA addresses discrepancies in vision, language, and alignment by: 1) Image-Video Features Fusion to combine image-level and video-level features, 2) generating pseudo image captions for fine-grained alignment, and 3) using Image-to-Video Alignment Distillation to transfer image alignment knowledge to video.

Result: DiscoVLA achieves state-of-the-art results, such as 50.5% R@1 on the MSRVTT dataset with CLIP (ViT-B/16), outperforming prior methods by 1.5%.

Conclusion: The DiscoVLA method proves effective in mitigating key discrepancies in adapting CLIP for video-text retrieval, suggesting its potential for advancing this task.

Abstract: The parameter-efficient adaptation of the image-text pretraining model CLIP
for video-text retrieval is a prominent area of research. While CLIP is focused
on image-level vision-language matching, video-text retrieval demands
comprehensive understanding at the video level. Three key discrepancies emerge
in the transfer from image-level to video-level: vision, language, and
alignment. However, existing methods mainly focus on vision while neglecting
language and alignment. In this paper, we propose Discrepancy Reduction in
Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all
three discrepancies. Specifically, we introduce Image-Video Features Fusion to
integrate image-level and video-level features, effectively tackling both
vision and language discrepancies. Additionally, we generate pseudo image
captions to learn fine-grained image-level alignment. To mitigate alignment
discrepancies, we propose Image-to-Video Alignment Distillation, which
leverages image-level alignment knowledge to enhance video-level alignment.
Extensive experiments demonstrate the superiority of our DiscoVLA. In
particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous
methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is
available at https://github.com/LunarShen/DsicoVLA.

</details>


### [195] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: This paper introduces a framework for integrating diverse knowledge from multiple sources, including neural models, human-crafted tools, visual generative models, and physics simulators.


<details>
  <summary>Details</summary>
Motivation: The integration of complementary knowledge from diverse models in shared data domains like images and videos remains underexplored.

Method: A Product of Experts (PoE) framework using Annealed Importance Sampling (AIS) is devised for inference-time knowledge composition without requiring training.

Result: The framework improves controllability in image and video synthesis tasks and offers flexible user interfaces for visual generation goals.

Conclusion: The proposed approach demonstrates practical advantages over monolithic methods, enabling rich and diverse knowledge composition during synthesis.

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [196] [WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos](https://arxiv.org/abs/2506.08896)
*Negin Ghamsarian,Raphael Sznitman,Klaus Schoeffmann,Jens Kowal*

Main category: cs.CV

TL;DR: This paper introduces WetCat, the first dataset designed specifically for automated skill assessment in wetlab cataract surgery.


<details>
  <summary>Details</summary>
Motivation: The demand for efficient, objective, and scalable methods to evaluate ophthalmic surgical training has risen due to the dependency on manual evaluations, which are laborious and inconsistent.

Method: WetCat, a dataset of high-resolution wetlab cataract surgery videos, is developed with detailed annotations for critical surgical phases and key anatomical structures to support AI-driven skill evaluations.

Result: The dataset enables interpretable and standardized automated evaluation tools for assessing surgical skill, adhering to clinical metrics, and supporting scalable education.

Conclusion: WetCat sets a benchmark in ophthalmic training by enabling objective assessments, advancing computer vision applications, and enhancing cataract surgery training efficiency. It is openly accessible through Synapse.

Abstract: To meet the growing demand for systematic surgical training, wetlab
environments have become indispensable platforms for hands-on practice in
ophthalmology. Yet, traditional wetlab training depends heavily on manual
performance evaluations, which are labor-intensive, time-consuming, and often
subject to variability. Recent advances in computer vision offer promising
avenues for automated skill assessment, enhancing both the efficiency and
objectivity of surgical education. Despite notable progress in ophthalmic
surgical datasets, existing resources predominantly focus on real surgeries or
isolated tasks, falling short of supporting comprehensive skill evaluation in
controlled wetlab settings. To address these limitations, we introduce WetCat,
the first dataset of wetlab cataract surgery videos specifically curated for
automated skill assessment. WetCat comprises high-resolution recordings of
surgeries performed by trainees on artificial eyes, featuring comprehensive
phase annotations and semantic segmentations of key anatomical structures.
These annotations are meticulously designed to facilitate skill assessment
during the critical capsulorhexis and phacoemulsification phases, adhering to
standardized surgical skill assessment frameworks. By focusing on these
essential phases, WetCat enables the development of interpretable, AI-driven
evaluation tools aligned with established clinical metrics. This dataset lays a
strong foundation for advancing objective, scalable surgical education and sets
a new benchmark for automated workflow analysis and skill assessment in
ophthalmology training. The dataset and annotations are publicly available in
Synapse https://www.synapse.org/Synapse:syn66401174/files.

</details>


### [197] [MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis](https://arxiv.org/abs/2506.08900)
*José Morano,Botond Fazekas,Emese Sükei,Ronald Fecso,Taha Emre,Markus Gumpinger,Georg Faustmann,Marzieh Oghbaie,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: The paper introduces MIRAGE, a multimodal foundation model for OCT and SLO image analysis in ophthalmology, and establishes its superiority over existing models for classification and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust AI models in ophthalmology that can work with multiple imaging modalities and overcome limitations of data dependency and underperformance on unseen data.

Method: MIRAGE combines multimodal training on OCT and SLO images and is validated through a new benchmark with classification and segmentation tasks to demonstrate its effectiveness.

Result: MIRAGE outperformed both general and specialized foundation models in ophthalmic image classification and segmentation, demonstrating its robustness and versatility.

Conclusion: MIRAGE is a superior foundation model for ophthalmic analysis, and its openness enhances reproducibility and further research in this field.

Abstract: Artificial intelligence (AI) has become a fundamental tool for assisting
clinicians in analyzing ophthalmic images, such as optical coherence tomography
(OCT). However, developing AI models often requires extensive annotation, and
existing models tend to underperform on independent, unseen data. Foundation
models (FMs), large AI models trained on vast unlabeled datasets, have shown
promise in overcoming these challenges. Nonetheless, available FMs for
ophthalmology lack extensive validation, especially for segmentation tasks, and
focus on a single imaging modality. In this context, we propose MIRAGE, a novel
multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)
images. Additionally, we propose a new evaluation benchmark with OCT/SLO
classification and segmentation tasks. The comparison with general and
specialized FMs and segmentation methods shows the superiority of MIRAGE in
both types of tasks, highlighting its suitability as a basis for the
development of robust AI systems for retinal OCT image analysis. Both MIRAGE
and the evaluation benchmark are publicly available:
https://github.com/j-morano/MIRAGE.

</details>


### [198] [Hyperbolic Dual Feature Augmentation for Open-Environment](https://arxiv.org/abs/2506.08906)
*Peilin Yu,Yuwei Wu,Zhi Gao,Xiaomeng Fan,Shuo Yang,Yunde Jia*

Main category: cs.CV

TL;DR: The paper introduces a method for augmenting features for both seen and unseen classes in hyperbolic space, improving performance in open-environment tasks.


<details>
  <summary>Details</summary>
Motivation: Existing hyperbolic feature augmentation methods work only for seen classes and fail to adapt to open environments requiring unseen class augmentation.

Method: The authors propose a hyperbolic dual feature augmentation method using neural ODE modules with meta-learning, a hierarchical preservation regularizer, and an infinite augmentation upper bound loss.

Result: The proposed method demonstrated improved performance across five open-environment tasks, outperforming prior hyperbolic feature augmentation approaches.

Conclusion: Hyperbolic algorithms benefit from dual feature augmentation in open-environments, enhancing generalization and adaptability in various tasks.

Abstract: Feature augmentation generates novel samples in the feature space, providing
an effective way to enhance the generalization ability of learning algorithms
with hyperbolic geometry. Most hyperbolic feature augmentation is confined to
closed-environment, assuming the number of classes is fixed (\emph{i.e.}, seen
classes) and generating features only for these classes. In this paper, we
propose a hyperbolic dual feature augmentation method for open-environment,
which augments features for both seen and unseen classes in the hyperbolic
space. To obtain a more precise approximation of the real data distribution for
efficient training, (1) we adopt a neural ordinary differential equation
module, enhanced by meta-learning, estimating the feature distributions of both
seen and unseen classes; (2) we then introduce a regularizer to preserve the
latent hierarchical structures of data in the hyperbolic space; (3) we also
derive an upper bound for the hyperbolic dual augmentation loss, allowing us to
train a hyperbolic model using infinite augmentations for seen and unseen
classes. Extensive experiments on five open-environment tasks:
class-incremental learning, few-shot open-set recognition, few-shot learning,
zero-shot learning, and general image classification, demonstrate that our
method effectively enhances the performance of hyperbolic algorithms in
open-environment.

</details>


### [199] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/abs/2506.08908)
*Jiajun Li,Yue Ma,Xinyu Zhang,Qingyan Wei,Songhua Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces SkipVAR, a sample-adaptive framework to address inefficiencies in Visual Autoregressive models, achieving significant acceleration without compromising model quality.


<details>
  <summary>Details</summary>
Motivation: To resolve inefficiencies in the inference process of Visual Autoregressive (VAR) models, particularly focusing on step redundancy and unconditional branch redundancy.

Method: The authors propose an automatic step-skipping strategy, introduce unconditional branch replacement, and develop SkipVAR, which dynamically selects acceleration strategies based on sample-specific frequency information.

Result: SkipVAR demonstrates over 0.88 average SSIM with up to 1.81x overall acceleration and 2.62x speedup on the GenEval benchmark while maintaining model performance.

Conclusion: The research confirms that frequency-aware, training-free adaptive acceleration effectively reduces computational inefficiency in VAR models for scalable image generation.

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that
high-frequency components, or later steps, in the generation process contribute
disproportionately to inference latency. However, the underlying computational
redundancy involved in these steps has yet to be thoroughly investigated. In
this paper, we conduct an in-depth analysis of the VAR inference process and
identify two primary sources of inefficiency: step redundancy and unconditional
branch redundancy. To address step redundancy, we propose an automatic
step-skipping strategy that selectively omits unnecessary generation steps to
improve efficiency. For unconditional branch redundancy, we observe that the
information gap between the conditional and unconditional branches is minimal.
Leveraging this insight, we introduce unconditional branch replacement, a
technique that bypasses the unconditional branch to reduce computational cost.
Notably, we observe that the effectiveness of acceleration strategies varies
significantly across different samples. Motivated by this, we propose SkipVAR,
a sample-adaptive framework that leverages frequency information to dynamically
select the most suitable acceleration strategy for each instance. To evaluate
the role of high-frequency information, we introduce high-variation benchmark
datasets that test model sensitivity to fine details. Extensive experiments
show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall
acceleration and 2.62x speedup on the GenEval benchmark, maintaining model
quality. These results confirm the effectiveness of frequency-aware,
training-free adaptive acceleration for scalable autoregressive image
generation. Our code is available at https://github.com/fakerone-li/SkipVAR and
has been publicly released.

</details>


### [200] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/abs/2506.08915)
*Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos*

Main category: cs.CV

TL;DR: The paper proposes an attention-based two-stage method to handle spurious correlations and out-of-distribution contexts in object perception.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of object perception where contextual bias or out-of-distribution backgrounds affect accuracy and robustness.

Method: Develop a two-stage framework with binary attention masks: stage 1 locates relevant regions, and stage 2 focuses on analyzing these areas while filtering spurious information. Both stages are trained jointly for refinement.

Result: Demonstrates significant improvements in robustness against spurious correlations and diverse out-of-distribution benchmarks.

Conclusion: The method provides an effective mechanism to enhance reliability in object-centric tasks by attention-based filtering and adaptive learning strategies.

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [201] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: The paper investigates enhancing pre-trained vision-language models for reasoning tasks without retraining, using a Monte Carlo Tree Search-inspired algorithm to generate subquestions and subanswers.


<details>
  <summary>Details</summary>
Motivation: To repurpose already trained non-reasoning vision-language models for improved reasoning performance without requiring additional training or supervision.

Method: The authors use a Monte Carlo Tree Search-inspired algorithm to inject subquestion-subanswer pairs into the model's output stream, framing reasoning as a search process.

Result: The method enhances reasoning abilities in non-reasoning models, achieving a 2% overall improvement on the MMMU-PRO benchmark and a 9% improvement in Liberal Arts.

Conclusion: Reasoning as a search process using subquestions and subanswers effectively unlocks hidden reasoning capabilities in non-reasoning vision-language models, eliminating the need for retraining.

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [202] [What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities](https://arxiv.org/abs/2506.08933)
*Wendong Bu,Yang Wu,Qifan Yu,Minghe Gao,Bingchen Miao,Zhenkui Zhang,Kaihang Pan,Yunfei Li,Mengze Li,Wei Ji,Juncheng Li,Siliang Tang,Yueting Zhuang*

Main category: cs.CV

TL;DR: OmniBench is a new benchmark for evaluating virtual agents, introducing automated task generation, multidimensional evaluation, and graph-based metrics.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing benchmarks, such as uncontrollable task complexity, heavy reliance on manual annotation, and lack of multidimensional evaluation.

Method: Developed a self-generating, graph-based benchmark (OmniBench) and evaluation framework (OmniEval) for synthesizing tasks and assessing capabilities of virtual agents.

Result: Generated 36k graph-structured tasks with a 91% human acceptance rate, and demonstrated the training efficiency of graph-structured data over manually annotated data.

Conclusion: OmniBench and OmniEval enable robust evaluation of virtual agents across diverse capabilities, providing insights into performance and guiding future developments.

Abstract: As multimodal large language models (MLLMs) advance, MLLM-based virtual
agents have demonstrated remarkable performance. However, existing benchmarks
face significant limitations, including uncontrollable task complexity,
extensive manual annotation with limited scenarios, and a lack of
multidimensional evaluation. In response to these challenges, we introduce
OmniBench, a self-generating, cross-platform, graph-based benchmark with an
automated pipeline for synthesizing tasks of controllable complexity through
subtask composition. To evaluate the diverse capabilities of virtual agents on
the graph, we further present OmniEval, a multidimensional evaluation framework
that includes subtask-level evaluation, graph-based metrics, and comprehensive
tests across 10 capabilities. Our synthesized dataset contains 36k
graph-structured tasks across 20 scenarios, achieving a 91\% human acceptance
rate. Training on our graph-structured data shows that it can more efficiently
guide agents compared to manually annotated data. We conduct multidimensional
evaluations for various open-source and closed-source models, revealing their
performance across various capabilities and paving the way for future
advancements. Our project is available at https://omni-bench.github.io/.

</details>


### [203] [SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation](https://arxiv.org/abs/2506.08949)
*Hongjie Zhu,Xiwei Liu,Rundong Xue,Zeyu Zhang,Yong Xu,Daji Ergu,Ying Cai,Yang Zhao*

Main category: cs.CV

TL;DR: This paper introduces SSS, a semi-supervised learning method leveraging Vision Foundation Models like SAM-2 to enhance medical image segmentation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To efficiently use large-scale unlabeled medical imaging data while minimizing the reliance on pixel-level annotations, addressing a critical challenge in medical image analysis.

Method: The approach integrates SAM-2's features with a Discriminative Feature Enhancement mechanism for feature optimization and a PCSW prompt generator to satisfy SAM-2's input requirements, achieving effective semi-supervised learning.

Result: SSS demonstrates improved performance on medical image segmentation tasks, achieving an average Dice score of 53.15 on the BHSD dataset with a +3.65 improvement over the previous best method.

Conclusion: By leveraging SAM-2's capabilities and introducing innovative mechanisms, SSS advances semi-supervised learning in medical imaging and sets a new state-of-the-art in multi-label segmentation tasks.

Abstract: In the era of information explosion, efficiently leveraging large-scale
unlabeled data while minimizing the reliance on high-quality pixel-level
annotations remains a critical challenge in the field of medical imaging.
Semi-supervised learning (SSL) enhances the utilization of unlabeled data by
facilitating knowledge transfer, significantly improving the performance of
fully supervised models and emerging as a highly promising research direction
in medical image analysis. Inspired by the ability of Vision Foundation Models
(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised
SAM-2), a novel approach that leverages SAM-2's robust feature extraction
capabilities to uncover latent knowledge in unlabeled medical images, thus
effectively enhancing feature support for fully supervised medical image
segmentation. Specifically, building upon the single-stream "weak-to-strong"
consistency regularization framework, this paper introduces a Discriminative
Feature Enhancement (DFE) mechanism to further explore the feature
discrepancies introduced by various data augmentation strategies across
multiple views. By leveraging feature similarity and dissimilarity across
multi-scale augmentation techniques, the method reconstructs and models the
features, thereby effectively optimizing the salient regions. Furthermore, a
prompt generator is developed that integrates Physical Constraints with a
Sliding Window (PCSW) mechanism to generate input prompts for unlabeled data,
fulfilling SAM-2's requirement for additional prompts. Extensive experiments
demonstrate the superiority of the proposed method for semi-supervised medical
image segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,
SSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous
state-of-the-art method by +3.65 Dice. Code will be available at
https://github.com/AIGeeksGroup/SSS.

</details>


### [204] [Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF](https://arxiv.org/abs/2506.08953)
*Anirudh Nanduri,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: This study adapts Vision Transformers (ViTs) for cross-spectral body recognition, exploring their performance in matching visible (VIS) and infrared (IR) images using Side Information Embedding (SIE).


<details>
  <summary>Details</summary>
Motivation: To tackle the challenging problem of cross-spectral body recognition, which involves matching images from visible and infrared domains, and to address the lack of occlusion studies in visible-infrared re-identification (Re-ID).

Method: A ViT pretrained on visible imagery was enhanced with Side Information Embedding (SIE) to encode camera and domain information. The study investigates the performance on datasets like LLCM and IJB-MDF under varying scenarios, including occlusions.

Result: Encoding only camera information achieved state-of-the-art performance on the LLCM dataset. Additionally, the effects of range-induced occlusions were explored using the IJB-MDF dataset, offering insights into cross-range, cross-spectral evaluations.

Conclusion: Results indicate that incorporating camera information in ViT leads to significant performance improvements for cross-spectral matching. The study also highlights the importance of addressing occlusions in VI-ReID tasks.

Abstract: Vision Transformers (ViTs) have demonstrated impressive performance across a
wide range of biometric tasks, including face and body recognition. In this
work, we adapt a ViT model pretrained on visible (VIS) imagery to the
challenging problem of cross-spectral body recognition, which involves matching
images captured in the visible and infrared (IR) domains. Recent ViT
architectures have explored incorporating additional embeddings beyond
traditional positional embeddings. Building on this idea, we integrate Side
Information Embedding (SIE) and examine the impact of encoding domain and
camera information to enhance cross-spectral matching. Surprisingly, our
results show that encoding only camera information - without explicitly
incorporating domain information - achieves state-of-the-art performance on the
LLCM dataset. While occlusion handling has been extensively studied in
visible-spectrum person re-identification (Re-ID), occlusions in
visible-infrared (VI) Re-ID remain largely underexplored - primarily because
existing VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly
feature full-body, unoccluded images. To address this gap, we analyze the
impact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain
Face (IJB-MDF) dataset, which provides a diverse set of visible and infrared
images captured at various distances, enabling cross-range, cross-spectral
evaluations.

</details>


### [205] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Main category: cs.CV

TL;DR: This paper introduces SEE, a method utilizing the Segment Anything Model to tackle ISCOS by creating pseudo-labels, ensuring segmentation coherence, and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Segmentation of concealed objects from incompletely annotated data is challenging due to limited supervision and intrinsic similarities of concealed scenarios.

Method: SEE employs a unified mean-teacher framework with strategies for pseudo-label management and a hybrid-granularity feature grouping module for segmentation coherence.

Result: The proposed method achieves state-of-the-art performance across multiple ISCOS tasks and enhances existing models.

Conclusion: SEE provides an effective and general solution for ISCOS, improving segmentation performance and adaptability.

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [206] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Main category: cs.CV

TL;DR: The study addresses the challenge of detecting small objects in computer vision by using an optimal data augmentation method with Fast AutoAugment, achieving a 20% performance boost on the DOTA dataset.


<details>
  <summary>Details</summary>
Motivation: Despite significant progress in object detection, small object detection remains challenging and requires improvement due to its inferior performance compared to large object detection.

Method: The authors utilized Fast AutoAugment to design an optimal data augmentation method for finding augmentation policies tailored for small object detection.

Result: Their method achieved a significant 20% improvement in detection performance on the DOTA dataset.

Conclusion: Optimal data augmentation through Fast AutoAugment is an effective approach to overcoming the limitations of small object detection, demonstrating notable performance gains.

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [207] [ORIDa: Object-centric Real-world Image Composition Dataset](https://arxiv.org/abs/2506.08964)
*Jinwoo Kim,Sangmin Han,Jinho Jeong,Jiwoo Choi,Dongyoung Kim,Seon Joo Kim*

Main category: cs.CV

TL;DR: ORIDa is a large-scale dataset of real-world images for object compositing, including over 30,000 images with 200 unique objects in varied positions and scenes.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation of existing datasets in diversity and scale for exploring real-world object compositing scenarios.

Method: Researchers created ORIDa, a large-scale dataset featuring factual-counterfactual sets and factual-only scenes to incorporate diverse object positions and contexts.

Result: ORIDa offers over 30,000 images with detailed scene complexities, improving the resources available for object compositing research.

Conclusion: Extensive analyses validate ORIDa as a significant dataset to advance real-world image composition research and applications.

Abstract: Object compositing, the task of placing and harmonizing objects in images of
diverse visual scenes, has become an important task in computer vision with the
rise of generative models. However, existing datasets lack the diversity and
scale required to comprehensively explore real-world scenarios. We introduce
ORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,
real-captured dataset containing over 30,000 images featuring 200 unique
objects, each of which is presented across varied positions and scenes. ORIDa
has two types of data: factual-counterfactual sets and factual-only scenes. The
factual-counterfactual sets consist of four factual images showing an object in
different positions within a scene and a single counterfactual (or background)
image of the scene without the object, resulting in five images per scene. The
factual-only scenes include a single image containing an object in a specific
context, expanding the variety of environments. To our knowledge, ORIDa is the
first publicly available dataset with its scale and complexity for real-world
image composition. Extensive analysis and experiments highlight the value of
ORIDa as a resource for advancing further research in object compositing.

</details>


### [208] [ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations](https://arxiv.org/abs/2506.08968)
*Amirreza Rouhi,Solmaz Arezoomandan,Knut Peterson,Joseph T. Woods,David K. Han*

Main category: cs.CV

TL;DR: Introducing ADAM, a training-free framework for open-world object labeling that combines contextual cues with visual embeddings to label new objects without predefined categories.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of traditional object detection models that rely on predefined categories, restricting their usability in open-world scenarios.

Method: ADAM uses large language models to generate candidate object labels and matches them with visual embeddings from CLIP, forming an Embedding-Label Repository (ELR). A self-refinement loop improves consistency.

Result: ADAM successfully annotates novel categories in the COCO and PASCAL datasets using only visual and contextual information, requiring no model fine-tuning or retraining.

Conclusion: The proposed model demonstrates an effective, autonomous solution for open-world object labeling, expanding the applicability of object detection beyond predefined categories.

Abstract: Object detection models typically rely on predefined categories, limiting
their ability to identify novel objects in open-world scenarios. To overcome
this constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,
a training-free, self-refining framework for open-world object labeling. ADAM
leverages large language models (LLMs) to generate candidate labels for unknown
objects based on contextual information from known entities within a scene.
These labels are paired with visual embeddings from CLIP to construct an
Embedding-Label Repository (ELR) that enables inference without category
supervision. For a newly encountered unknown object, ADAM retrieves visually
similar instances from the ELR and applies frequency-based voting and
cross-modal re-ranking to assign a robust label. To further enhance
consistency, we introduce a self-refinement loop that re-evaluates repository
labels using visual cohesion analysis and k-nearest-neighbor-based majority
re-labeling. Experimental results on the COCO and PASCAL datasets demonstrate
that ADAM effectively annotates novel categories using only visual and
contextual signals, without requiring any fine-tuning or retraining.

</details>


### [209] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Main category: cs.CV

TL;DR: The paper introduces ALTA, a method for efficient and effective medical vision-language alignment, achieving superior performance in tasks like retrieval and zero-shot classification while being computationally lightweight.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve cross-modal contrastive learning methods by addressing the limitation of suboptimal visual representation in conventional approaches and leveraging the strengths of multimodal masked models in visual representation.

Method: ALTA adapts a pretrained vision model from masked record modeling for vision-language alignment, integrating temporal-multiview radiograph inputs to enhance consistency between radiographs and descriptions. It uses only a small portion of trainable parameters and computational costs compared to masked modeling.

Result: ALTA outperforms other methods, achieving over 4% improvement in text-to-image accuracy and approximately 6% in image-to-text retrieval accuracy in vision-language matching tasks.

Conclusion: ALTA demonstrates that adapting vision models for alignment significantly enhances performance in medical vision-language tasks, emphasizing efficient use of resources. This approach promotes improved vision and language understanding.

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [210] [Do Concept Replacement Techniques Really Erase Unacceptable Concepts?](https://arxiv.org/abs/2506.08991)
*Anudeep Das,Gurjot Singh,Prach Chantasantitam,N. Asokan*

Main category: cs.CV

TL;DR: The paper discusses challenges with Concept Replacement Techniques (CRTs) in generative models, showing that existing methods do not effectively erase unacceptable concepts in image-to-image (I2I) scenarios and introduces a new technique, AntiMirror, to improve effectiveness and fidelity.


<details>
  <summary>Details</summary>
Motivation: Current CRTs fail to align generative models to prevent unacceptable concept generation in evolving image editing (I2I) scenarios.

Method: The authors empirically test I2I models to evaluate CRTs, introduce the notion of "fidelity" for replacing unacceptable concepts while preserving desired ones, and propose AntiMirror, a targeted image-editing technique.

Result: Existing CRTs are ineffective with I2I models. AntiMirror demonstrates promising results in balancing concept replacement effectiveness and fidelity.

Conclusion: The proposed AntiMirror technique addresses shortcomings in CRTs by ensuring fidelity while effectively replacing unacceptable concepts, advancing generative model alignment.

Abstract: Generative models, particularly diffusion-based text-to-image (T2I) models,
have demonstrated astounding success. However, aligning them to avoid
generating content with unacceptable concepts (e.g., offensive or copyrighted
content, or celebrity likenesses) remains a significant challenge. Concept
replacement techniques (CRTs) aim to address this challenge, often by trying to
"erase" unacceptable concepts from models. Recently, model providers have
started offering image editing services which accept an image and a text prompt
as input, to produce an image altered as specified by the prompt. These are
known as image-to-image (I2I) models. In this paper, we first use an I2I model
to empirically demonstrate that today's state-of-the-art CRTs do not in fact
erase unacceptable concepts. Existing CRTs are thus likely to be ineffective in
emerging I2I scenarios, despite their proven ability to remove unwanted
concepts in T2I pipelines, highlighting the need to understand this discrepancy
between T2I and I2I settings. Next, we argue that a good CRT, while replacing
unacceptable concepts, should preserve other concepts specified in the inputs
to generative models. We call this fidelity. Prior work on CRTs have neglected
fidelity in the case of unacceptable concepts. Finally, we propose the use of
targeted image-editing techniques to achieve both effectiveness and fidelity.
We present such a technique, AntiMirror, and demonstrate its viability.

</details>


### [211] [Do MIL Models Transfer?](https://arxiv.org/abs/2506.09022)
*Daniel Shao,Richard J. Chen,Andrew H. Song,Joel Runevic,Ming Y. Lu,Tong Ding,Faisal Mahmood*

Main category: cs.CV

TL;DR: This paper evaluates the performance of pretrained Multiple Instance Learning (MIL) models in computational pathology, showing their advantage over training models from scratch, even across different organs or tasks.


<details>
  <summary>Details</summary>
Motivation: MIL is challenged by limited clinical datasets, hindering its utility in computational pathology. The paper aims to explore transfer learning as a solution to improve MIL's performance under data constraints.

Method: The study systematically evaluates the transferability of MIL models by analyzing 11 model architectures across 21 pretraining tasks for morphological and molecular subtype predictions.

Result: Pretrained MIL models outperform models trained from scratch, even when applied to tasks involving different organs. Pretraining on multi-organ pancancer datasets enhances generalization and outperforms other methods using significantly less pretraining data.

Conclusion: Transfer learning significantly boosts MIL model performance in computational pathology, offering adaptability and efficiency across diverse tasks. The study also provides standardized MIL resources for broader adoption in the field.

Abstract: Multiple Instance Learning (MIL) is a cornerstone approach in computational
pathology (CPath) for generating clinically meaningful slide-level embeddings
from gigapixel tissue images. However, MIL often struggles with small, weakly
supervised clinical datasets. In contrast to fields such as NLP and
conventional computer vision, where transfer learning is widely used to address
data scarcity, the transferability of MIL models remains poorly understood. In
this study, we systematically evaluate the transfer learning capabilities of
pretrained MIL models by assessing 11 models across 21 pretraining tasks for
morphological and molecular subtype prediction. Our results show that
pretrained MIL models, even when trained on different organs than the target
task, consistently outperform models trained from scratch. Moreover,
pretraining on pancancer datasets enables strong generalization across organs
and tasks, outperforming slide foundation models while using substantially less
pretraining data. These findings highlight the robust adaptability of MIL
models and demonstrate the benefits of leveraging transfer learning to boost
performance in CPath. Lastly, we provide a resource which standardizes the
implementation of MIL models and collection of pretrained model weights on
popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab

</details>


### [212] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: This paper introduces Decentralized Isolation Networks (DIsoN), a method for detecting out-of-distribution (OOD) data in machine learning by allowing secure comparison of test samples with training data without directly sharing the data, addressing challenges of privacy and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable safe deployment of machine learning models in sensitive domains like medical imaging by effectively handling OOD detection while preserving data privacy and overcoming the practical limitation of centralized data storage.

Method: The paper proposes Isolation Networks to evaluate how separable a test sample is from the training data using binary classification. It then introduces DIsoN, which shares only model parameters instead of data between training and deployment sites, with an extension for class-conditioned comparisons.

Result: DIsoN was tested on four medical imaging datasets across 12 OOD detection tasks and performed competitively compared to existing methods, while ensuring data privacy was maintained.

Conclusion: The proposed DIsoN framework enables effective and privacy-preserving OOD detection in decentralized settings, offering a practical solution for secure OOD detection services and potentially redefining ML services in safety-critical applications.

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


### [213] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: The paper introduces 'Dispersive Loss,' a new regularizer enhancing diffusion-based generative models by promoting dispersed representations in hidden space. The method is minimalist and improves performance on ImageNet.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explicit regularization and representation learning integration in diffusion-based generative models.

Method: The authors propose 'Dispersive Loss,' which encourages dispersed internal representations without requiring positive sample pairs, pre-training, extra parameters, or external data.

Result: Evaluation on the ImageNet dataset shows consistent performance improvements over established baselines.

Conclusion: Dispersive Loss enhances generative modeling by bridging it with representation learning, offering a simple yet effective solution.

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [214] [Princeton365: A Diverse Dataset with Accurate Camera Pose](https://arxiv.org/abs/2506.09035)
*Karhan Kayan,Stamatis Alexandropoulos,Rishabh Jain,Yiming Zuo,Erich Liang,Jia Deng*

Main category: cs.CV

TL;DR: The paper introduces Princeton365, a detailed and diverse SLAM dataset with an innovative ground-truth collection framework and new evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To address the gap between data diversity and accuracy in SLAM benchmarks and allow better evaluation of SLAM performance across different scenarios.

Method: It uses a novel ground-truth collection approach involving calibration boards and 360-degree cameras to produce accurate camera pose data. The dataset includes synchronized RGB video and IMU data.

Result: The dataset provides a rich source for diverse SLAM research. It introduces a new metric to compare SLAM performance across scenes and a novel view synthesis benchmark for unique challenges.

Conclusion: Princeton365 is a robust contribution to SLAM research, with its diverse dataset, a scene-scale-aware metric, and benchmarks that reflect real-world challenges.

Abstract: We introduce Princeton365, a large-scale diverse dataset of 365 videos with
accurate camera pose. Our dataset bridges the gap between accuracy and data
diversity in current SLAM benchmarks by introducing a novel ground truth
collection framework that leverages calibration boards and a 360-camera. We
collect indoor, outdoor, and object scanning videos with synchronized monocular
and stereo RGB video outputs as well as IMU. We further propose a new scene
scale-aware evaluation metric for SLAM based on the the optical flow induced by
the camera pose estimation error. In contrast to the current metrics, our new
metric allows for comparison between the performance of SLAM methods across
scenes as opposed to existing metrics such as Average Trajectory Error (ATE),
allowing researchers to analyze the failure modes of their methods. We also
propose a challenging Novel View Synthesis benchmark that covers cases not
covered by current NVS benchmarks, such as fully non-Lambertian scenes with
360-degree camera trajectories. Please visit
https://princeton365.cs.princeton.edu for the dataset, code, videos, and
submission.

</details>


### [215] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: Autoregressive Semantic Visual Reconstruction (ASVR) proposes a new multimodal learning approach that improves vision-language models by focusing on semantic representation reconstruction rather than raw visual input.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models (LVLMs) fail to fully integrate visual modalities, leading to issues like reliance on captions, omission of visual details, and ineffective alignment between visual and textual data.

Method: The proposed method, ASVR, reconstructs a semantic representation of images within an autoregressive framework instead of raw visual appearance. This semantically-guided reconstruction enables better integration of visual and textual information.

Result: ASVR consistently improves performance across 14 multimodal benchmarks, demonstrating a 5% improvement in average scores for LLaVA-1.5, even with varying data sizes (556k-2M) and LLM backbones.

Conclusion: Autoregressive semantic reconstruction, as opposed to raw visual reconstruction, enhances the multimodal understanding of LVLMs, making ASVR a promising approach for future model development and applications.

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [216] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/abs/2506.09042)
*Xuanchi Ren,Yifan Lu,Tianshi Cao,Ruiyuan Gao,Shengyu Huang,Amirmojtaba Sabour,Tianchang Shen,Tobias Pfaff,Jay Zhangjie Wu,Runjian Chen,Seung Wook Kim,Jun Gao,Laura Leal-Taixe,Mike Chen,Sanja Fidler,Huan Ling*

Main category: cs.CV

TL;DR: The paper introduces Cosmos-Drive-Dreams, a synthetic data generation pipeline for creating rare, high-fidelity driving scenarios to improve performance in AI systems for autonomous vehicles, addressing issues like dataset scarcity and long-tail problems.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the time-consuming and costly nature of collecting and annotating real-world data for autonomous vehicles, especially rare edge cases critical for their training and testing.

Method: The authors develop Cosmos-Drive-Dreams, which leverages the Cosmos-Drive model, a version of NVIDIA's Cosmos foundation model adapted for driving scenarios, capable of generating spatiotemporally consistent and multi-view synthetic driving videos.

Result: The study experimentally demonstrates that the synthetic data generated by Cosmos-Drive-Dreams improves downstream tasks in 3D lane detection, 3D object detection, and driving policy learning, mitigating long-tail problems.

Conclusion: Cosmos-Drive-Dreams effectively scales and diversifies driving datasets, enhancing model generalization and performance on safety-critical tasks. The pipeline, dataset, and model weights are publicly available via NVIDIA's Cosmos platform.

Abstract: Collecting and annotating real-world data for safety-critical physical AI
systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is
especially challenging to capture rare edge cases, which play a critical role
in training and testing of an AV system. To address this challenge, we
introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline
that aims to generate challenging scenarios to facilitate downstream tasks such
as perception and driving policy training. Powering this pipeline is
Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation
model for the driving domain and are capable of controllable, high-fidelity,
multi-view, and spatiotemporally consistent driving video generation. We
showcase the utility of these models by applying Cosmos-Drive-Dreams to scale
the quantity and diversity of driving datasets with high-fidelity and
challenging scenarios. Experimentally, we demonstrate that our generated data
helps in mitigating long-tail distribution problems and enhances generalization
in downstream tasks such as 3D lane detection, 3D object detection and driving
policy learning. We open source our pipeline toolkit, dataset and model weights
through the NVIDIA's Cosmos platform.
  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [217] [MagCache: Fast Video Generation with Magnitude-Aware Cache](https://arxiv.org/abs/2506.09045)
*Zehong Ma,Longhui Wei,Feng Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: This paper introduces MagCache, a method to make video diffusion models faster and more robust by leveraging a discovered magnitude pattern.


<details>
  <summary>Details</summary>
Motivation: Current acceleration techniques often require extensive calibration and are prone to inconsistent outputs due to overfitting to specific prompts.

Method: MagCache utilizes a magnitude-aware error modeling mechanism and adaptive caching strategy to skip less important timesteps, based on the observed magnitude law.

Result: MagCache achieves up to 2.68x speedup, while surpassing existing methods in metrics like LPIPS, SSIM, and PSNR under similar computational limitations.

Conclusion: MagCache offers a novel and scalable way to optimize video diffusion models, requiring minimal calibration while ensuring high visual fidelity and computational efficiency.

Abstract: Existing acceleration techniques for video diffusion models often rely on
uniform heuristics or time-embedding variants to skip timesteps and reuse
cached features. These approaches typically require extensive calibration with
curated prompts and risk inconsistent outputs due to prompt-specific
overfitting. In this paper, we introduce a novel and robust discovery: a
unified magnitude law observed across different models and prompts.
Specifically, the magnitude ratio of successive residual outputs decreases
monotonically and steadily in most timesteps while rapidly in the last several
steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)
that adaptively skips unimportant timesteps using an error modeling mechanism
and adaptive caching strategy. Unlike existing methods requiring dozens of
curated samples for calibration, MagCache only requires a single sample for
calibration. Experimental results show that MagCache achieves 2.1x and 2.68x
speedups on Open-Sora and Wan 2.1, respectively, while preserving superior
visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,
and PSNR, under comparable computational budgets.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [218] [PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production](https://arxiv.org/abs/2506.08528)
*Yu Guan,Zhiyu Yin,Haoyu Chen,Sheng Cheng,Chaojie Yang,Tianyin Xu,Yang Zhang,Hanyu Zhao,Yong Li,Dennis Cai,Ennan Zhai*

Main category: cs.DC

TL;DR: PerfTracker is an online troubleshooting system for diagnosing performance issues in large model training on large-scale GPU clusters using fine-grained profiling.


<details>
  <summary>Details</summary>
Motivation: The unprecedented scale of GPU clusters, complexity in software-hardware interactions, and data-intensive nature of LMT make troubleshooting performance problems immensely challenging. Traditional methods are insufficient for real-world systems.

Method: PerfTracker uses online fine-grained profiling to diagnose performance issues in both hardware (e.g., GPUs, interconnects) and software (Python functions, GPU operations). It employs differential observability to localize root causes while minimizing production impacts.

Result: PerfTracker has been deployed in production, operating on GPU clusters with up to 10,000 GPUs. It has successfully diagnosed various difficult performance issues.

Conclusion: PerfTracker addresses critical troubleshooting gaps in LMT by providing scalable, fine-grained profiling and efficient root-cause localization for modern GPU cluster environments.

Abstract: Troubleshooting performance problems of large model training (LMT) is
immensely challenging, due to unprecedented scales of modern GPU clusters, the
complexity of software-hardware interactions, and the data intensity of the
training process. Existing troubleshooting approaches designed for traditional
distributed systems or datacenter networks fall short and can hardly apply to
real-world training systems. In this paper, we present PerfTracker, the first
online troubleshooting system utilizing fine-grained profiling, to diagnose
performance issues of large-scale model training in production. PerfTracker can
diagnose performance issues rooted in both hardware (e.g., GPUs and their
interconnects) and software (e.g., Python functions and GPU operations). It
scales to LMT on modern GPU clusters. PerfTracker effectively summarizes
runtime behavior patterns of fine-grained LMT functions via online profiling,
and leverages differential observability to localize the root cause with
minimal production impact. PerfTracker has been deployed as a production
service for large-scale GPU clusters of O(10, 000) GPUs (product homepage
https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).
It has been used to diagnose a variety of difficult performance issues.

</details>


### [219] [Towards Provenance-Aware Earth Observation Workflows: the openEO Case Study](https://arxiv.org/abs/2506.08597)
*H. Omidi,L. Sacco,V. Hutter,G. Irsiegler,M. Claus,M. Schobben,A. Jacob,M. Schramm,S. Fiore*

Main category: cs.DC

TL;DR: The paper discusses integrating the data provenance library yProv4WFs in openEO to enhance metadata collection and workflow understanding for Earth Observation.


<details>
  <summary>Details</summary>
Motivation: Efficient metadata collection and understanding of computational workflows are vital for Earth Observation to track data lineage and facilitate analysis.

Method: The integration of a data provenance library (yProv4WFs) into openEO, allowing users better access and tracking capabilities during EO processes.

Result: The integration improves understanding of data flows, dependencies, and transformations within analytical workflows for Earth Observation.

Conclusion: Embedding provenance concepts in EO workflows enhances transparency, traceability, and simplifies the connection to cloud back-ends for researchers and stakeholders.

Abstract: Capturing the history of operations and activities during a computational
workflow is significantly important for Earth Observation (EO). The data
provenance helps to collect the metadata that records the lineage of data
products, providing information about how data are generated, transferred,
manipulated, by whom all these operations are performed and through which
processes, parameters, and datasets. This paper presents an approach to improve
those aspects, by integrating the data provenance library yProv4WFs within
openEO, a platform to let users connect to Earth Observation cloud back-ends in
a simple and unified way. In addition, it is demonstrated how the integration
of data provenance concepts across EO processing chains enables researchers and
stakeholders to better understand the flow, the dependencies, and the
transformations involved in analytical workflows.

</details>


### [220] [Blockchain and Edge Computing Nexus: A Large-scale Systematic Literature Review](https://arxiv.org/abs/2506.08636)
*Zeinab Nezami,Zhuolun Li,Chuhao Qin,Fatemeh Banaie,Rabiya Khalid,Evangelos Pournaras*

Main category: cs.DC

TL;DR: The paper conducts a systematic review on blockchain and edge computing, identifying synergies to address research challenges.


<details>
  <summary>Details</summary>
Motivation: To explore and understand the connection and synergy between blockchain and edge computing paradigms to boost innovation in Smart Cities applications.

Method: Collected nearly 6000 papers from three databases, analyzing 1000 papers to create a taxonomy with 22 features and 287 attributes. Quantitative and machine learning methods were applied.

Result: Identified four interaction patterns between blockchain and edge computing. Highlighted blockchain-assisted edge computing's role in improving privacy and security, especially for mobile computing.

Conclusion: The interplay of blockchain and edge computing offers significant advancements in various applications, particularly improving mobile privacy and security.

Abstract: Blockchain and edge computing are two instrumental paradigms of decentralized
computation, driving key advancements in Smart Cities applications such as
supply chain, energy and mobility. Despite their unprecedented impact on
society, they remain significantly fragmented as technologies and research
areas, while they share fundamental principles of distributed systems and
domains of applicability. This paper introduces a novel and large-scale
systematic literature review on the nexus of blockchain and edge computing with
the aim to unravel a new understanding of how the interfacing of the two
computing paradigms can boost innovation to provide solutions to timely but
also long-standing research challenges. By collecting almost 6000 papers from 3
databases and putting under scrutiny almost 1000 papers, we build a novel
taxonomy and classification consisting of 22 features with 287 attributes that
we study using quantitative and machine learning methods. They cover a broad
spectrum of technological, design, epistemological and sustainability aspects.
Results reveal 4 distinguishing patterns of interplay between blockchain and
edge computing with key determinants the public (permissionless) vs. private
(permissioned) design, technology and proof of concepts. They also demonstrate
the prevalence of blockchain-assisted edge computing for improving privacy and
security, in particular for mobile computing applications.

</details>


### [221] [Parallel FFTW on RISC-V: A Comparative Study including OpenMP, MPI, and HPX](https://arxiv.org/abs/2506.08653)
*Alexander Strack,Christopher Taylor,Dirk Pflüger*

Main category: cs.DC

TL;DR: This paper evaluates parallel scaling of FFTW library on RISC-V and compares its performance to x86-64 for MPI and OpenMP implementations, highlighting memory optimization challenges and performance differences.


<details>
  <summary>Details</summary>
Motivation: With emerging RISC-V processors featuring high core counts (e.g., SOPHON SG2042 with 64 cores), optimizing parallelization becomes essential, mirroring challenges faced by established architectures like x86-64.

Method: The study benchmarks FFTW library on RISC-V using parallelization approaches (MPI and OpenMP) and compares with AMD EPYC 7742 (64-core) for diverse FFT planning types. HPX-FFT memory optimizations are evaluated for RISC-V architecture.

Result: RISC-V processors show an 8x performance delta in double-precision 2D FFT compared to x86-64. MPI scaling is strong on both architectures up to 64 cores, while OpenMP scaling requires more careful planning. Memory optimizations effective on x86-64 fail on RISC-V.

Conclusion: This research highlights parallelization strengths and limitations on RISC-V processors, marking early progress towards developing large-scale parallel applications for RISC-V architectures.

Abstract: Rapid advancements in RISC-V hardware development shift the focus from
low-level optimizations to higher-level parallelization. Recent RISC-V
processors, such as the SOPHON SG2042, have 64 cores. RISC-V processors with
core counts comparable to the SG2042, make efficient parallelization as crucial
for RISC-V as the more established processors such as x86-64. In this work, we
evaluate the parallel scaling of the widely used FFTW library on RISC-V for MPI
and OpenMP. We compare it to a 64-core AMD EPYC 7742 CPU side by side for
different types of FFTW planning. Additionally, we investigate the effect of
memory optimization on RISC-V in HPX-FFT, a parallel FFT library based on the
asynchronous many-task runtime HPX using an FFTW backend. We generally observe
a performance delta between the x86-64 and RISC-V chips of factor eight for
double-precision 2D FFT. Effective memory optimizations in HPX-FFT on x86-64 do
not translate to the RISC-V chip. FFTW with MPI shows good scaling up to 64
cores on x86-64 and RISC-V regardless of planning. In contrast, FFTW with
OpenMP requires measured planning on both architectures to achieve good scaling
up to 64 cores. The results of our study mark an early step on the journey to
large-scale parallel applications running on RISC-V.

</details>


### [222] [Synchronization in Anonymous Networks Under Continuous Dynamics](https://arxiv.org/abs/2506.08661)
*Rida Bazzi,Anya Chaturvedi,Andréa W. Richa,Peter Vargas*

Main category: cs.DC

TL;DR: The $
abla$-Synchronizer enables a deterministic synchronization in dynamic, non-synchronous networks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of synchronization in dynamic networks with arbitrary edge dynamics and minimal assumptions.

Method: Propose $
abla$-Synchronizer that uses an extended Pull model with 1-bit multi-writer atomic register, enabling semi-synchronous to synchronous network simulation.

Result: A synchronizer capable of simulating synchronous algorithms while handling continuous edge dynamics and weakly fair activation.

Conclusion: The paper introduces a scalable synchronizer with linear memory overhead, expanding the applicability in dynamic networking environments.

Abstract: We present the $\kappa$-Synchronizer that works in non-synchronous dynamic
networks under minimal assumptions. Our model allows continuous topological
changes without any guarantee of eventual global or partial stabilization and
assumes that nodes are anonymous. This deterministic synchronizer is the first
to enable nodes to simulate a dynamic network synchronous algorithm for
executions in a semi-synchronous dynamic environment under a weakly-fair node
activation scheduler, despite the absence of a global clock, node ids,
persistent connectivity or any assumptions about the edge dynamics (in both the
synchronous and semi-synchronous environments). In summary, we make the
following contributions: (1) we extend the definition of synchronizers to
networks with continuous arbitrary edge dynamics; (2) we present the first
synchronizer from the semi-synchronous to the synchronous model in a network
with continuous arbitrary edge dynamics; and (3) we present non-trivial
applications of the proposed synchronizer to existing algorithms. We assume an
extension of the Pull communication model by adding a single 1-bit multi-writer
atomic register at each edge-port of a node, since we show that the standard
Pull model is not sufficient to allow for non-trivial synchronization in our
scenario. The $\kappa$-Synchronizer operates with memory overhead at the nodes
that is linear on the maximum node degree and logarithmic on the runtime of the
underlying synchronous algorithm being simulated.

</details>


### [223] [Balancing Fixed Number of Nodes Among Multiple Fixed Clusters](https://arxiv.org/abs/2506.08715)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy,Bhuban Padhan*

Main category: cs.DC

TL;DR: The paper presents a system for dynamic node rebalancing among container clusters to optimize resource utilization and reduce costs.


<details>
  <summary>Details</summary>
Motivation: Current cloud cluster systems often lead to underutilized resources due to static allocation strategies.

Method: The authors propose a Node Balancing Cluster Group (NBCG) that uses real-time utilization thresholds and a controlled reallocation mechanism involving a Balancer and Resizing Rule Engine.

Result: The system efficiently reallocates nodes to balance workloads without additional provisioning costs, reversing reallocations to maintain stability when necessary.

Conclusion: The method improves resource utilization and cost-efficiency while maintaining cluster stability, offering a competitive advantage for cloud providers like IBM Cloud.

Abstract: Cloud infrastructure users often allocate a fixed number of nodes to
individual container clusters (e.g., Kubernetes, OpenShift), resulting in
underutilization of computing resources due to asynchronous and variable
workload peaks across clusters. This research proposes a novel system and
method for dynamic rebalancing of a fixed total number of nodes among multiple
fixed clusters based on real-time resource utilization thresholds. By
introducing a Node Balancing Cluster Group (NBCG), clusters are grouped and
allowed to dynamically share nodes through a controlled reallocation mechanism,
managed by a Node Balancing Cluster Balancer and a Resizing Rule Engine. The
system identifies overutilized and underutilized clusters using threshold
parameters, and reassigns nodes without incurring additional provisioning
costs. If reallocation causes a violation of utilization thresholds, the system
reverses the operation to maintain cluster stability. The proposed architecture
not only optimizes resource utilization and operational cost but also
introduces a strategic advantage for cloud service providers like IBM Cloud.
Unlike existing solutions, this approach enables intra-account node sharing
across clusters with strict adherence to user-defined constraints and ensures
consistent cluster state management. This invention has the potential to
significantly reduce computing resource waste and position IBM Cloud services
as more efficient and competitive.

</details>


### [224] [Mycelium: A Transformation-Embedded LSM-Tree](https://arxiv.org/abs/2506.08923)
*Holly Casaletto,Jeff Lefevre,Aldrin Montana,Peter Alvaro*

Main category: cs.DC

TL;DR: This paper introduces Transformation-Embedded LSM-trees (TE-LSM), which integrate data transformations into the compaction process, reducing costs and improving performance.


<details>
  <summary>Details</summary>
Motivation: Compaction in write-optimized data structures like LSM-trees is costly, and this work seeks to make compaction more efficient by embedding useful data transformations into the process, rather than treating transformations as separate operations.

Method: The authors propose TE-LSMs, implemented in a prototype called Mycelium atop RocksDB, which integrates cross-column-family merging to transparently embed transformations such as splitting column groups, changing data formats, and building indexes during compaction.

Result: The Mycelium prototype achieves a 20% write throughput overhead compared to 35-60% in naive transformation approaches, while improving read latency by up to 425% compared to the baseline RocksDB.

Conclusion: TE-LSMs demonstrate that embedding transformations during compaction is a cost-effective way to enhance future data access efficiency and reduce IO amplification, showing clear performance benefits over traditional methods.

Abstract: Compaction is a necessary, but often costly background process in
write-optimized data structures like LSM-trees that reorganizes incoming data
that is sequentially appended to logs. In this paper, we introduce
Transformation-Embedded LSM-trees (TE-LSM), a novel approach that transparently
embeds a variety of data transformations into the compaction process. While
many others have sought to reduce the high cost of compaction, TE-LSMs leverage
the opportunity to embed other useful work to amortize IO costs and
amplification. We illustrate the use of a TE-LSM in Mycelium, our prototype
built on top of RocksDB that extends the compaction process through a
cross-column-family merging mechanism. Mycelium enables seamless integration of
a transformer interface and aims to better prepare data for future accesses
based on access patterns. We use Mycelium to explore three types of
transformations: splitting column groups, converting data formats, and index
building. In addition to providing a cost model analysis, we evaluate
Mycelium's write and read performance using YCSB workloads. Our results show
that Mycelium incurs a 20% write throughput overhead - significantly lower than
the 35% to 60% overhead observed in naive approaches that perform data
transformations outside of compaction-while achieving up to 425% improvements
in read latency compared to RocksDB baseline.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [225] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)
*Melissa Estevez,Nisha Singh,Lauren Dyson,Blythe Adamson,Qianyu Yuan,Megan W. Hildner,Erin Fidyk,Olive Mbah,Farhad Khan,Kathi Seidl-Rathkopf,Aaron B. Cohen*

Main category: cs.LG

TL;DR: The paper proposes a quality assurance framework for Large Language Models (LLMs) extracting clinical data from Electronic Health Records (EHRs), covering error detection, consistency checking, and bias assessment.


<details>
  <summary>Details</summary>
Motivation: Current frameworks fail to address the specific challenges and error modes associated with data extracted by LLMs from EHRs, necessitating a new comprehensive method.

Method: The framework combines variable-level performance benchmarking, automated verification for consistency, replication analyses for validation, and bias assessment by demographic subgroup stratification.

Result: The multidimensional framework successfully identifies errors, ensures data suitability for research, and evaluates biases in LLM-extracted clinical data.

Conclusion: The proposed framework enhances reliability and industry standards for AI-driven evidence generation in oncology, ensuring trustworthy use in real-world applications.

Abstract: Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [226] [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/abs/2506.08018)
*Fei Li,Song Liu,Weiguo Wu,Shiqiang Nie,Jinyu Wang*

Main category: cs.LG

TL;DR: KVmix is a mixed-precision quantization method for reducing memory and computational demands in Key-Value Cache during LLM inference.


<details>
  <summary>Details</summary>
Motivation: Deploying Large Language Models on resource-constrained platforms requires addressing high memory demands caused by Key-Value Cache.

Method: KVmix employs gradient-based importance analysis for layer-specific bit-width allocation and dynamic long-context optimization for memory efficiency.

Result: KVmix significantly improves memory efficiency (4.9x compression) and inference throughput (5.3x speedup) while maintaining near-lossless performance.

Conclusion: KVmix provides an effective balance between accuracy and efficiency, enabling resource-efficient deployment of LLMs.

Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of
Large Language Models (LLMs) severely restrict their deployment in
resource-constrained platforms. Quantization can effectively alleviate the
memory pressure caused by KV Cache. However, existing methods either rely on
static one-size-fits-all precision allocation or fail to dynamically prioritize
critical KV in long-context tasks, forcing memory-accuracy-throughput
tradeoffs. In this work, we propose a novel mixed-precision quantization method
for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to
evaluate how individual Key and Value projection matrices affect the model
loss, enabling layer-specific bit-width allocation for mix-precision
quantization. It dynamically prioritizes higher precision for important layers
while aggressively quantizing less influential ones, achieving a tunable
balance between accuracy and efficiency. KVmix also introduces a dynamic
long-context optimization strategy that adaptively keeps full-precision KV
pairs for recent pivotal tokens and compresses older ones, achieving
high-quality sequence generation with low memory usage. Additionally, KVmix
provides efficient low-bit quantization and CUDA kernels to optimize
computational overhead. On LLMs such as Llama and Mistral, KVmix achieves
near-lossless inference performance with extremely low quantization
configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x
memory compression and a 5.3x speedup in inference throughput.

</details>


### [227] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: The paper introduces a semi-supervised approach to map refugees' locations at a granular scale across 25 Sub-Saharan African countries using various types of spatial data.


<details>
  <summary>Details</summary>
Motivation: Refugee statistics are often aggregated at broad administrative levels, which obscures detailed displacement patterns. The paper aims to provide high-resolution, spatially explicit refugee data to better understand localized displacement dynamics.

Method: The researchers employed a label spreading algorithm that integrates UNHCR registration data, satellite-derived building footprints, and coordinate data from OpenStreetMap to distribute refugee statistics into 0.5-degree grid cells.

Result: The approach achieved an average accuracy of 92.9% in assigning over 10 million refugee observations to their appropriate grid cells, offering detailed insights into displacement patterns across the study area.

Conclusion: The generated high-resolution dataset enables finer analyses of displacement drivers, supporting efforts to understand and respond to refugee movements more effectively.

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [228] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Main category: cs.LG

TL;DR: The study addresses partial domain adaptation challenges by introducing a Bi-level Unbalanced Optimal Transport (BUOT) model for better cross-domain sample alignment and outlier class distinction.


<details>
  <summary>Details</summary>
Motivation: Existing weighting approaches for partial domain adaptation struggle to adequately explore sample-wise and cluster structures and are sensitive to incorrect predictions. This paper aims to overcome these limitations.

Method: The proposed Bi-level Unbalanced Optimal Transport (BUOT) model characterizes both sample-level and class-level relations for cross-domain alignment. A cooperation mechanism ensures mutual enhancement between these levels, supported by a label-aware transport cost for local structure preservation.

Result: The BUOT model demonstrated competitive results in aligning cross-domain samples and distinguishing outlier classes with improved computation efficiency on benchmark datasets.

Conclusion: BUOT effectively addresses the limitations of existing weighting frameworks in partial domain adaptation by simultaneously leveraging sample-wise and class-wise relations in a unified transport structure.

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [229] [FlowBERT: Prompt-tuned BERT for variable flow field prediction](https://arxiv.org/abs/2506.08021)
*Weihao Zou,Weibing Feng,Pin Wu*

Main category: cs.LG

TL;DR: This paper introduces a rapid, accurate flow field prediction framework using knowledge transfer from large language models.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional CFD methods (high cost) and existing deep learning models (limited generalization).

Method: Combines POD dimensionality reduction with fine-tuned LLMs designed for fluid dynamics using text templates.

Result: Achieves faster predictions (seconds), over 90% accuracy, and better generalization compared to conventional models.

Conclusion: Proposes a novel knowledge transfer paradigm, paving the way for faster fluid dynamics applications in areas like optimization and control.

Abstract: This study proposes a universal flow field prediction framework based on
knowledge transfer
  from large language model (LLM), addressing the high computational costs of
traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition
transfer capability
  of existing deep learning models. The framework innovatively integrates
Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for
pretrained LLM,
  where POD facilitates compressed representation of flow field features while
the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's
adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that
improve predictive
  performance through enriched contextual semantic information. Experimental
results demonstrate
  that our framework outperforms conventional Transformer models in few-shot
learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and
airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT
architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation,
our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed
knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction,
with potential
  applications extending to aerodynamic optimization, flow control, and other
engineering domains.

</details>


### [230] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: This paper proposes the Modality-Balancing Preference Optimization (MBPO) framework to address modality imbalance in Large Multimodal Models (LMMs), enhancing performance and reducing hallucinations in vision-language tasks.


<details>
  <summary>Details</summary>
Motivation: LMMs often prioritize language over visual input during reasoning, limiting their generalization and causing hallucinations. Current preference optimization methods fail to adequately address this modality imbalance or dynamically adapt responses during training.

Method: The authors introduce MBPO, which generates hard negatives through adversarial image perturbation to create an improved offline preference dataset. It also uses verified rewards from close-ended tasks for generating online responses. GRPO is applied to train models with both offline and online data.

Result: MBPO enhances LMM performance on vision-language tasks and effectively reduces hallucinations, as validated by extensive experiments.

Conclusion: MBPO is a novel and effective framework to mitigate modality imbalance in LMMs, leveraging both offline and online preference data to improve reasoning and downstream capabilities.

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [231] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)
*Asit Mishra,Dusan Stosic,Simon Layton*

Main category: cs.LG

TL;DR: This paper investigates Microscaling (MX) formats in NVIDIA’s Blackwell GPUs to enhance precision scaling with fewer bits for improved pre-training of LLMs, addressing numeric stability challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to leverage MX formats for better GPU efficiency during LLM pre-training on massive datasets, while minimizing numeric instability traditionally present in reduced-precision techniques.

Method: The method involves analyzing the MX formats, particularly evaluating a new rounding mode (round-to-infinity) to compute scaling factors, as opposed to the default rounding mode, and testing it on an 8B model with 15T tokens.

Result: The improved rounding mode successfully enabled pre-training an LLM using MXFP8 on an 8-billion parameter model over a massive dataset of 15 trillion tokens.

Conclusion: Using round-to-infinity rounding mode for MXFP8 precision scaling improves numeric stability and allows successful pre-training in Large Language Models, marking a significant step forward in GPU efficiency.

Abstract: Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [232] [GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity](https://arxiv.org/abs/2210.16402)
*Artavazd Maranjyan,Mher Safaryan,Peter Richtárik*

Main category: cs.LG

TL;DR: The paper introduces GradSkip and its generalization GradSkip+ as an improvement over ProxSkip, optimizing communication in distributed algorithms with varying local gradient steps.


<details>
  <summary>Details</summary>
Motivation: To address high communication costs in distributed optimization by allowing adaptable local training steps for clients, thus enhancing efficiency.

Method: Redesigns the ProxSkip algorithm to create GradSkip, enabling fewer local training steps for some clients based on specific criteria. Extends GradSkip to GradSkip+ with probabilistic alternations and unbiased compression operators.

Result: Proved GradSkip's convergence rate is linear under strong convexity, matches ProxSkip's communication efficiency, and allows reductions in local gradient steps. GradSkip+ generalizes related methods. Empirical results support theoretical findings.

Conclusion: GradSkip and GradSkip+ provide provable communication efficiency improvements and flexibility in distributed optimization, with empirical validation of their theoretical benefits.

Abstract: We study a class of distributed optimization algorithms that aim to alleviate
high communication costs by allowing clients to perform multiple local
gradient-type training steps before communication. In a recent breakthrough,
Mishchenko et al. (2022) proved that local training, when properly executed,
leads to provable communication acceleration, and this holds in the strongly
convex regime without relying on any data similarity assumptions. However,
their ProxSkip method requires all clients to take the same number of local
training steps in each communication round. We propose a redesign of the
ProxSkip method, allowing clients with ``less important'' data to get away with
fewer local training steps without impacting the overall communication
complexity of the method. In particular, we prove that our modified method,
GradSkip, converges linearly under the same assumptions and has the same
accelerated communication complexity, while the number of local gradient steps
can be reduced relative to a local condition number. We further generalize our
method by extending the randomness of probabilistic alternations to arbitrary
unbiased compression operators and by considering a generic proximable
regularizer. This generalization, which we call GradSkip+, recovers several
related methods in the literature as special cases. Finally, we present an
empirical study on carefully designed toy problems that confirm our theoretical
claims.

</details>


### [233] [ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity](https://arxiv.org/abs/2506.08051)
*Mahmuda Sultana Mimi,Md Monzurul Islam,Anannya Ghosh Tusti,Shriyank Somvanshi,Subasish Das*

Main category: cs.LG

TL;DR: The paper introduces ST-GraphNet, a spatio-temporal graph neural network framework, to model and predict automated vehicle (AV) crash severity using multimodal data and achieves a high test accuracy of 97.74%.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to enhance urban mobility safety and infrastructure planning by understanding and predicting the spatial and temporal complexities of AV crash severity.

Method: The method involves constructing fine-grained and coarse-grained graph representations of AV crashes using spatio-temporal relationships, multimodal data enrichment, and evaluating the performance of various graph neural network architectures including ST-GraphNet, which leverages a DSTGCN on an aggregated H3-based spatial graph.

Result: ST-GraphNet achieves a test accuracy of 97.74%, significantly outperforming the best fine-grained model, which achieved 64.7%, thereby effectively capturing spatio-temporal patterns in AV crash severity.

Conclusion: The study concludes that spatial aggregation, dynamic message passing, and multimodal feature integration are crucial for accurately modeling and predicting AV crash severity, highlighting ST-GraphNet's potential for improving urban mobility safety.

Abstract: Understanding the spatial and temporal dynamics of automated vehicle (AV)
crash severity is critical for advancing urban mobility safety and
infrastructure planning. In this work, we introduce ST-GraphNet, a
spatio-temporal graph neural network framework designed to model and predict AV
crash severity by using both fine-grained and region-aggregated spatial graphs.
Using a balanced dataset of 2,352 real-world AV-related crash reports from
Texas (2024), including geospatial coordinates, crash timestamps, SAE
automation levels, and narrative descriptions, we construct two complementary
graph representations: (1) a fine-grained graph with individual crash events as
nodes, where edges are defined via spatio-temporal proximity; and (2) a
coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical
Spatial Indexing (H3)-based spatial cells, connected through hexagonal
adjacency. Each node in the graph is enriched with multimodal data, including
semantic, spatial, and temporal attributes, including textual embeddings from
crash narratives using a pretrained Sentence-BERT model. We evaluate various
graph neural network (GNN) architectures, such as Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN
(DSTGCN), to classify crash severity and predict high-risk regions. Our
proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3
graph, achieves a test accuracy of 97.74\%, substantially outperforming the
best fine-grained model (64.7\% test accuracy). These findings highlight the
effectiveness of spatial aggregation, dynamic message passing, and multi-modal
feature integration in capturing the complex spatio-temporal patterns
underlying AV crash severity.

</details>


### [234] [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
*Diyuan Wu,Aleksandr Shevchenko,Samet Oymak,Marco Mondelli*

Main category: cs.LG

TL;DR: The paper characterizes the embeddings in a one-layer softmax attention model, showing how token importance aligns with output vectors based on frequency, and how the softmax mechanism selects predictive tokens for maximized margin.


<details>
  <summary>Details</summary>
Motivation: To fill the theoretical gap in understanding token embeddings in language modeling, which are practically important but poorly understood.

Method: The study uses a one-layer softmax attention model and employs gradient training and gradient flow analysis to investigate embedding alignment and token selection mechanisms.

Result: The embeddings capture token importance through alignment proportional to dataset frequency after a single gradient step, and the softmax selects predictive tokens to maximize margin after convergence.

Conclusion: Embeddings learned through gradient methods effectively identify important tokens based on dataset statistics and predictive utility, supported by experimental validation.

Abstract: Token embeddings play a crucial role in language modeling but, despite this
practical relevance, their theoretical understanding remains limited. Our paper
addresses the gap by characterizing the structure of embeddings obtained via
gradient descent. Specifically, we consider a one-layer softmax attention model
with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top
E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top
v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots,
E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the
embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output
vector. First, we show that, already after a single step of gradient training
with the logistic loss, the embeddings $E_X$ capture the importance of tokens
in the dataset by aligning with the output vector $v$ proportionally to the
frequency with which the corresponding tokens appear in the dataset. Then,
after training $p$ via gradient flow until convergence, the softmax selects the
important tokens in the sentence (i.e., those that are predictive of the
label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes
the margin for such a selection. Experiments on real-world datasets (IMDB,
Yelp) exhibit a phenomenology close to that unveiled by our theory.

</details>


### [235] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/abs/2506.08054)
*Yiming Wang,Hao Peng,Senzhang Wang,Haohua Du,Chunyang Liu,Jia Wu,Guanlin Wu*

Main category: cs.LG

TL;DR: This paper introduces STAMImputer, a novel method for imputing missing traffic data using a dynamic spatiotemporal approach, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for traffic data imputation struggle with block-wise missing scenarios and rely on static graph structures that can't adapt to nonstationary traffic data.

Method: A SpatioTemporal Attention Mixture of Experts network (STAMImputer) is proposed, combining a Mixture of Experts framework with a Low-rank guided Sampling Graph Attention mechanism to dynamically balance spatial correlations and capture latent features.

Result: Experiments on four traffic datasets show significant performance improvements of STAMImputer over state-of-the-art methods.

Conclusion: STAMImputer effectively addresses the challenges of traffic data imputation in dynamic and nonstationary scenarios, offering better adaptability and accuracy.

Abstract: Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [236] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/abs/2506.06990)
*Mingyi Li,Michael R. Metel,Akiko Takeda*

Main category: cs.LG

TL;DR: This paper analyzes K-means algorithm's lack of local optimality guarantees and proposes methods to address it.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the absence of rigorous analytical understanding of K-means algorithm's local optimality guarantees despite extensive research on its global optimization capabilities.

Method: The authors identify conditions for K-means' local optimality and propose modified versions of the algorithm using Bregman divergence as a measure, ensuring local optimality while retaining its computational efficiency.

Result: Experimental results show that the original K-means algorithm often fails to reach a locally optimal solution, whereas the proposed methods achieve better clustering results with reduced clustering loss.

Conclusion: The proposed modifications to K-means improve its capacity to achieve locally optimal solutions without added computational complexity, offering effective alternatives for clustering tasks.

Abstract: The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [237] [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques](https://arxiv.org/abs/2506.08060)
*Asankhaya Sharma*

Main category: cs.LG

TL;DR: The paper proves that fine-tuned capabilities of large language models can be approximated using inference-time in-context learning techniques, without the need for parameter updates, under idealized and practical conditions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of computational inefficiency in supervised fine-tuning of large language models by exploring alternative approaches.

Method: The authors leverage theoretical proofs under idealized assumptions, extending them to practical contexts, to show that fine-tuning capabilities can be replicated using in-context learning.

Result: The paper provides dataset size formulas necessary for tasks like text generation and linear classification to approximate fine-tuned model behavior within specific error margins.

Conclusion: The findings support resource-efficient deployment of large language models, emphasizing the practical role of techniques like retrieval-augmented generation in bridging theoretical results with real-world applications.

Abstract: Large language models have transformed natural language processing, yet
supervised fine-tuning (SFT) remains computationally intensive. This paper
formally proves that capabilities acquired through SFT can be approximated by a
base transformer model using inference-time techniques, specifically in-context
learning (ICL), without altering model parameters, under idealized assumptions
including unbounded computational resources and access to the fine-tuning
dataset. We extend these results to practical scenarios with finite context
lengths and partial dataset access. For text generation tasks with fixed output
length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log
\frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l
\log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate
fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$
is the vocabulary size and $\delta$ is the failure probability. For linear
classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon}
\right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log
\frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension.
Grounded in the Turing completeness of transformers, these results provide a
theoretical foundation for resource-efficient deployment of large language
models, with practical techniques like retrieval-augmented generation bridging
theory to real-world applications.

</details>


### [238] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: UniVarFL is a federated learning (FL) framework designed to improve performance under non-IID data conditions by addressing local classifier bias without relying on the global model.


<details>
  <summary>Details</summary>
Motivation: FL suffers from performance degradation under non-IID data, primarily because of local classifier bias, and existing solutions are either computationally expensive or fail to handle feature shifts effectively.

Method: The paper introduces UniVarFL, which incorporates two regularization techniques during local training: Classifier Variance Regularization for aligning class-wise probability distributions to IID-like conditions, and Hyperspherical Uniformity Regularization to ensure uniform feature representation distribution over a hypersphere.

Result: Experimental results on benchmark datasets show that UniVarFL achieves higher accuracy compared to existing approaches, demonstrating its scalability and efficiency.

Conclusion: UniVarFL offers an effective and scalable solution to mitigate non-IID challenges in federated learning, making it suitable for real-world and resource-constrained applications.

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [239] [FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.08062)
*Woosung Kim,Jinho Lee,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: The paper introduces FairDICE, an offline multi-objective reinforcement learning (MORL) framework designed to optimize nonlinear welfare objectives like fairness measures.


<details>
  <summary>Details</summary>
Motivation: To address the inability of traditional linear scalarization methods to capture nonlinear fairness-oriented goals in MORL, particularly in offline settings where a fixed dataset is used.

Method: The authors developed FairDICE, which uses distribution correction estimation both for welfare maximization and for ensuring distributional regularization during the learning process.

Result: FairDICE outperformed existing benchmarks across multiple offline environments in terms of fairness-aware performance.

Conclusion: FairDICE provides a unified and sample-efficient approach for optimizing nonlinear welfare objectives in offline MORL without requiring predefined preferences or exhaustive searches.

Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in
the presence of conflicting objectives, where linear scalarization is commonly
used to reduce vector-valued returns into scalar signals. While effective for
certain preferences, this approach cannot capture fairness-oriented goals such
as Nash social welfare or max-min fairness, which require nonlinear and
non-additive trade-offs. Although several online algorithms have been proposed
for specific fairness objectives, a unified approach for optimizing nonlinear
welfare criteria in the offline setting-where learning must proceed from a
fixed dataset-remains unexplored. In this work, we present FairDICE, the first
offline MORL framework that directly optimizes nonlinear welfare objective.
FairDICE leverages distribution correction estimation to jointly account for
welfare maximization and distributional regularization, enabling stable and
sample-efficient learning without requiring explicit preference weights or
exhaustive weight search. Across multiple offline benchmarks, FairDICE
demonstrates strong fairness-aware performance compared to existing baselines.

</details>


### [240] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/abs/2506.08169)
*Jingqiao Tang,Ryan Bausback,Feng Bao,Richard Archibald*

Main category: cs.LG

TL;DR: This paper introduces stochastic neural networks into federated learning to handle latent noise in local datasets and improve model performance in non-IID scenarios.


<details>
  <summary>Details</summary>
Motivation: Federated learning is susceptible to latent noise in client datasets caused by factors like measurement errors or human mistakes, which may degrade model performance.

Method: The paper employs stochastic neural networks as local models in federated learning to estimate true data states and quantify latent noise, naming this approach Federated Stochastic Neural Networks.

Result: Numerical experiments demonstrate that the method effectively handles non-IID data and improves federated learning performance.

Conclusion: Incorporating stochastic neural networks into federated learning can enhance model robustness and accommodate challenges like latent noise and non-IID data.

Abstract: Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [241] [Parameter-free approximate equivariance for tasks with finite group symmetry](https://arxiv.org/abs/2506.08244)
*Riccardo Ali,Pietro Liò,Jamie Vicary*

Main category: cs.LG

TL;DR: The paper introduces a zero-parameter method for equivariant neural networks by adding a loss term to enforce approximate equivariance in the latent space, showing competitive performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing equivariant neural networks are computationally intensive, tied to specific architectures, and often have high parameter counts, necessitating a simpler and more generalizable approach.

Method: A zero-parameter approach is proposed using a term in the loss function to enforce approximate equivariance in the latent space, allowing the network to learn the regular representation on a group.

Result: Experiments demonstrate that the network naturally learns regular representations in the latent space, achieving similar or superior performance compared to other methods with reduced computational burden.

Conclusion: The proposed method is a computationally efficient alternative to existing equivariant neural networks, generalizable across architectures and tasks, with a focus on simplicity and performance.

Abstract: Equivariant neural networks incorporate symmetries through group actions,
embedding them as an inductive bias to improve performance on a wide variety of
tasks. However, existing equivariant methods can be computationally intensive,
with high parameter counts, and are often tied to a specific architecture. We
propose a simple zero-parameter approach that imposes approximate equivariance
for a finite group in the latent representation, as an additional term in the
loss function. We conduct experiments which allow the network to learn a group
representation on the latent space, and show in every case it prefers to learn
the regular representation. Fixing this action on the latent space, this yields
a simple method to impose approximate equivariance as an additional loss
penalty. We benchmark our approach on three datasets and compare it against
several existing equivariant methods, showing that in many cases it achieves
similar or better performance for a fraction of the parameters.

</details>


### [242] [Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift](https://arxiv.org/abs/2506.08063)
*Songqiao Hu,Zeyi Liu,Xiao He*

Main category: cs.LG

TL;DR: The paper introduces Lite-RVFL, an efficient learning method to handle concept drift without requiring retraining or drift detection.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and high computational cost of existing methods for handling concept drift in online learning.

Method: Lite-RVFL employs a novel objective function with exponential weight emphasis on recent data and an efficient incremental update rule.

Result: The method demonstrated effectiveness in adapting to concept drift in real-world safety assessment tasks, validating its temporal pattern capture abilities.

Conclusion: Lite-RVFL offers a practical, computationally efficient solution for real-time learning in dynamic data environments.

Abstract: The change in data distribution over time, also known as concept drift, poses
a significant challenge to the reliability of online learning methods. Existing
methods typically require model retraining or drift detection, both of which
demand high computational costs and are often unsuitable for real-time
applications. To address these limitations, a lightweight, fast and efficient
random vector functional-link network termed Lite-RVFL is proposed, capable of
adapting to concept drift without drift detection and retraining. Lite-RVFL
introduces a novel objective function that assigns weights exponentially
increasing to new samples, thereby emphasizing recent data and enabling timely
adaptation. Theoretical analysis confirms the feasibility of this objective
function for drift adaptation, and an efficient incremental update rule is
derived. Experimental results on a real-world safety assessment task validate
the efficiency, effectiveness in adapting to drift, and potential to capture
temporal patterns of Lite-RVFL. The source code is available at
https://github.com/songqiaohu/Lite-RVFL.

</details>


### [243] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/abs/2506.08426)
*Zheng Lin,Zhe Chen,Xianhao Chen,Wei Ni,Yue Gao*

Main category: cs.LG

TL;DR: Split federated learning (SFL) suffers straggler effects due to heterogeneous edge devices. HASFL proposes adaptive batch size (BS) and model splitting (MS) control to address this issue.


<details>
  <summary>Details</summary>
Motivation: The paper targets overcoming the straggler effect caused by resource heterogeneity in edge devices during split federated learning.

Method: Derived convergence bound for SFL to quantify the impact of BS and MS, and propose HASFL framework to adaptively control these parameters and optimize latency and convergence.

Result: Experiments validate HASFL's effectiveness and superiority compared to existing benchmarks across various datasets.

Conclusion: HASFL is capable of addressing edge network heterogeneity, minimizing latency and ensuring efficient training convergence.

Abstract: Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [244] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/abs/2506.08274)
*João Manoel Herrera Pinheiro,Suzana Vilas Boas de Oliveira,Thiago Henrique Segreto Silva,Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Leonardo André Ambrosio,Marcelo Becker*

Main category: cs.LG

TL;DR: The paper evaluates 12 feature scaling techniques across 14 ML algorithms and 16 datasets, revealing scaler-dependency for most methods except ensemble ones.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic studies on the impact of feature scaling on various ML models.

Method: Empirical analysis of 12 scaling techniques across classification and regression models, assessing performance using various metrics.

Result: Ensemble methods showed robustness to scaling, while models like SVMs, Logistic Regression, and MLPs were highly dependent on scaling choices.

Conclusion: Selection of the appropriate scaler is critical for many ML models, and the study provides practical guidance for practitioners, offering transparency and reproducibility.

Abstract: This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [245] [Info-Coevolution: An Efficient Framework for Data Model Coevolution](https://arxiv.org/abs/2506.08070)
*Ziheng Qin,Hailun Xu,Wei Chee Yew,Qi Jia,Yang Luo,Kanchan Sarkar,Danhui Guan,Kai Wang,Yang You*

Main category: cs.LG

TL;DR: The paper proposes Info-Coevolution, a framework for efficient dataset annotation and training without introducing bias while reducing costs and maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The increasing size of real-world datasets necessitates strategies to reduce annotation and training inefficiencies while maintaining model performance.

Method: The paper introduces Info-Coevolution, a framework for online selective annotation and coevolution of models and data, leveraging task-specific and open-source models to selectively annotate and improve datasets.

Result: Info-Coevolution achieves a 32% reduction in annotation and training costs for datasets like ImageNet-1K without performance loss and can further achieve 50% annotation reduction with semi-supervised learning.

Conclusion: Info-Coevolution offers a bias-free, efficient solution for dataset enhancement and model training, balancing cost savings and performance with minimal manual tuning.

Abstract: Machine learning relies heavily on data, yet the continuous growth of
real-world data poses challenges for efficient dataset construction and
training. A fundamental yet unsolved question is: given our current model and
data, does a new data (sample/batch) need annotation/learning? Conventional
approaches retain all available data, leading to non-optimal data and training
efficiency. Active learning aims to reduce data redundancy by selecting a
subset of samples to annotate, while it increases pipeline complexity and
introduces bias. In this work, we propose Info-Coevolution, a novel framework
that efficiently enables models and data to coevolve through online selective
annotation with no bias. Leveraging task-specific models (and open-source
models), it selectively annotates and integrates online and web data to improve
datasets efficiently. For real-world datasets like ImageNet-1K,
Info-Coevolution reduces annotation and training costs by 32\% without
performance loss. It is able to automatically give the saving ratio without
tuning the ratio. It can further reduce the annotation ratio to 50\% with
semi-supervised learning. We also explore retrieval-based dataset enhancement
using unlabeled open-source data. Code is available at
https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.

</details>


### [246] [Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion](https://arxiv.org/abs/2506.08316)
*Alan N. Amin,Nate Gruver,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: The paper examines discrete diffusion models, proposing a method (SCUD) that incorporates jump time schedules into the generation process, improving over current masking diffusion models.


<details>
  <summary>Details</summary>
Motivation: Masking diffusion models outperform other discrete diffusion methods, yet their non-gradual denoising contradicts the theoretical advantages of gradual processes, prompting investigation.

Method: The study introduces Schedule-Conditioned Discrete Diffusion (SCUD), which incorporates the known distribution of jump times into discrete diffusion models, making them more effective at learning the target distribution.

Result: SCUD generalizes previous methods and demonstrates superior performance across diverse datasets, including images, text, and protein data.

Conclusion: Incorporating the jump time schedules directly into the model structure enhances the efficiency and accuracy of discrete diffusion models, surpassing existing benchmarks like masking diffusion.

Abstract: Discrete diffusion models, like continuous diffusion models, generate
high-quality samples by gradually undoing noise applied to datapoints with a
Markov process. Gradual generation in theory comes with many conceptual
benefits; for example, inductive biases can be incorporated into the noising
Markov process, and access to improved sampling algorithms. In practice,
however, the consistently best performing discrete diffusion model is,
surprisingly, masking diffusion, which does not denoise gradually. Here we
explain the superior performance of masking diffusion by noting that it makes
use of a fundamental difference between continuous and discrete Markov
processes: discrete Markov processes evolve by discontinuous jumps at a fixed
rate and, unlike other discrete diffusion models, masking diffusion builds in
the known distribution of jump times and only learns where to jump to. We show
that we can similarly bake in the known distribution of jump times into any
discrete diffusion model. The resulting models - schedule-conditioned discrete
diffusion (SCUD) - generalize classical discrete diffusion and masking
diffusion. By applying SCUD to models with noising processes that incorporate
inductive biases on images, text, and protein data, we build models that
outperform masking.

</details>


### [247] [Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting](https://arxiv.org/abs/2506.08113)
*Timothée Hornek Amir Sartipi,Igor Tchappi,Gilbert Fridgen*

Main category: cs.LG

TL;DR: This paper benchmarks state-of-the-art time series foundation models and traditional approaches for electricity price forecasting.


<details>
  <summary>Details</summary>
Motivation: Electricity price forecasting is essential for decision-making in spot market trading, and the potential of advanced time series models like pre-trained large language models remains unexplored.

Method: Benchmarked pre-trained models (e.g., Chronos-Bolt, Time-MoE, etc.) and traditional methods using 2024 auction electricity prices across multiple countries, evaluating daily forecasts with a one-day horizon.

Result: Chronos-Bolt and Time-MoE stood out among the new models but performed comparably with traditional methods. The biseasonal MSTL model showed the best and most consistent performance overall.

Conclusion: State-of-the-art time series foundation models are promising but fail to outperform traditional, seasonality-aware models for electricity price forecasting.

Abstract: Accurate electricity price forecasting (EPF) is crucial for effective
decision-making in power trading on the spot market. While recent advances in
generative artificial intelligence (GenAI) and pre-trained large language
models (LLMs) have inspired the development of numerous time series foundation
models (TSFMs) for time series forecasting, their effectiveness in EPF remains
uncertain. To address this gap, we benchmark several state-of-the-art
pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and
TimeGPT--against established statistical and machine learning (ML) methods for
EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,
France, the Netherlands, Austria, and Belgium, we generate daily forecasts with
a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the
TSFMs, performing on par with traditional models. However, the biseasonal MSTL
model, which captures daily and weekly seasonality, stands out for its
consistent performance across countries and evaluation metrics, with no TSFM
statistically outperforming it.

</details>


### [248] [A Simple Analysis of Discretization Error in Diffusion Models](https://arxiv.org/abs/2506.08337)
*Juhyeok Choi,Chenglin Fan*

Main category: cs.LG

TL;DR: The paper simplifies the theoretical framework for analyzing Euler--Maruyama discretization in variance-preserving SDEs, provides a convergence rate, and shows practical implications of replacing Gaussian noise with discrete noise in diffusion models.


<details>
  <summary>Details</summary>
Motivation: To simplify the complex probabilistic analyses of discretization error in diffusion models and explore alternatives to Gaussian noise for practical generative modeling.

Method: The authors use Grönwall's inequality for deriving a convergence rate in VP-SDEs and investigate substituting Gaussian noise with discrete noise in the sampling process.

Result: The study establishes a convergence rate of $\mathcal{O}(1/T^{1/2})$, confirms that discrete noise performs comparably to Gaussian noise, and highlights the negative impact of incorrect noise scaling.

Conclusion: The work streamlines analysis methods, demonstrates the feasibility of discrete noise substitution, and bridges theoretical and practical aspects of diffusion-based modeling.

Abstract: Diffusion models, formulated as discretizations of stochastic differential
equations (SDEs), achieve state-of-the-art generative performance. However,
existing analyses of their discretization error often rely on complex
probabilistic tools. In this work, we present a simplified theoretical
framework for analyzing the Euler--Maruyama discretization of
variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models
(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion
process. Our approach leverages Gr\"onwall's inequality to derive a convergence
rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly
streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise
in the discretization can be replaced by a discrete random variable (e.g.,
Rademacher or uniform noise) without sacrificing convergence guarantees-an
insight with practical implications for efficient sampling. Experiments
validate our theory, showing that (1) the error scales as predicted, (2)
discrete noise achieves comparable sample quality to Gaussian noise, and (3)
incorrect noise scaling degrades performance. By unifying simplified analysis
and discrete noise substitution, our work bridges theoretical rigor with
practical efficiency in diffusion-based generative modeling.

</details>


### [249] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: The paper introduces Bingo, an RL framework to improve reasoning efficiency in large language models by balancing brevity and accuracy, showcasing better trade-offs compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Large language models exhibit strong reasoning skills but are prone to inefficiencies such as verbose or redundant outputs. Current improvements focus on accuracy, with limited attention to efficiency.

Method: The Bingo framework uses a significance-aware length reward to prune insignificant tokens and a dynamic length reward that adapts reasoning elaboration for hard and easy questions to achieve efficient reasoning.

Result: Bingo improves both reasoning accuracy and efficiency across benchmarks, outperforming vanilla RL reward models and other length-based baselines in trade-offs between accuracy and brevity.

Conclusion: Explicitly training large language models for efficient reasoning using Bingo demonstrates the feasibility of enhancing both precision and conciseness simultaneously.

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [250] [Spatiotemporal deep learning models for detection of rapid intensification in cyclones](https://arxiv.org/abs/2506.08397)
*Vamshika Sutar,Amandeep Singh,Rohitash Chandra*

Main category: cs.LG

TL;DR: The paper focuses on detecting cyclone rapid intensification using deep learning and data augmentation, addressing the class imbalance issue in such rare events.


<details>
  <summary>Details</summary>
Motivation: To improve the detection of rare, extreme cyclone rapid intensifications, which are difficult due to class imbalance and diverse influencing factors.

Method: Deep learning models are used to generate synthetic spatial coordinates and wind intensity data for addressing class imbalance, coupled with classification models to differentiate intensification events.

Result: Data augmentation significantly enhances rapid intensification detection, and spatial coordinates are crucial input features.

Conclusion: The study highlights the efficacy of synthetic data generation for spatiotemporal extreme event analysis and shows that incorporating spatial features improves prediction accuracy.

Abstract: Cyclone rapid intensification is the rapid increase in cyclone wind
intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid
intensification is considered an extreme event during a cyclone, and its
occurrence is relatively rare, contributing to a class imbalance in the
dataset. A diverse array of factors influences the likelihood of a cyclone
undergoing rapid intensification, further complicating the task for
conventional machine learning models. In this paper, we evaluate deep learning,
ensemble learning and data augmentation frameworks to detect cyclone rapid
intensification based on wind intensity and spatial coordinates. We note that
conventional data augmentation methods cannot be utilised for generating
spatiotemporal patterns replicating cyclones that undergo rapid
intensification. Therefore, our framework employs deep learning models to
generate spatial coordinates and wind intensity that replicate cyclones to
address the class imbalance problem of rapid intensification. We also use a
deep learning model for the classification module within the data augmentation
framework to differentiate between rapid and non-rapid intensification events
during a cyclone. Our results show that data augmentation improves the results
for rapid intensification detection in cyclones, and spatial coordinates play a
critical role as input features to the given models. This paves the way for
research in synthetic data generation for spatiotemporal data with extreme
events.

</details>


### [251] [Nearness of Neighbors Attention for Regression in Supervised Finetuning](https://arxiv.org/abs/2506.08139)
*Aviad Susman,Mayte Suárez-Fariñas,Joseph T Colonel*

Main category: cs.LG

TL;DR: This paper proposes a new method called Noneness of Neighbors Attention (NONA) regression layer, a differentiable proxy for k-NN regression, which improves performance over traditional k-NN and dense layer predictions.


<details>
  <summary>Details</summary>
Motivation: Traditional algorithms like k-NN often outperform neural networks when used as predictors, suggesting there is room for improvement in how traditional algorithms are integrated into neural networks.

Method: The authors develop the NONA regression layer, which employs neural network attention mechanisms and a new attention-masking approach to approximate k-NN in a differentiable way.

Result: NONA demonstrates superior performance compared to dense layer predictions and k-NN applied on embeddings across multiple unstructured datasets.

Conclusion: The study shows that directly incorporating traditional algorithms like k-NN as differentiable layers, such as NONA, can enhance prediction performance in supervised machine learning tasks.

Abstract: It is common in supervised machine learning to combine the feature extraction
capabilities of neural networks with the predictive power of traditional
algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This
procedure involves performing supervised fine-tuning (SFT) on a
domain-appropriate feature extractor, followed by training a traditional
predictor on the resulting SFT embeddings. When used in this manner,
traditional predictors often deliver increased performance over the SFT model
itself, despite the fine-tuned feature extractor yielding embeddings
specifically optimized for prediction by the neural network's final dense
layer. This suggests that directly incorporating traditional algorithms into
SFT as prediction layers may further improve performance. However, many
traditional algorithms have not been implemented as neural network layers due
to their non-differentiable nature and their unique optimization requirements.
As a step towards solving this problem, we introduce the Nearness of Neighbors
Attention (NONA) regression layer. NONA uses the mechanics of neural network
attention and a novel learned attention-masking scheme to yield a
differentiable proxy of the k-NN regression algorithm. Results on multiple
unstructured datasets show improved performance over both dense layer
prediction and k-NN on SFT embeddings for regression.

</details>


### [252] [Improved Scaling Laws in Linear Regression via Data Reuse](https://arxiv.org/abs/2506.08415)
*Licong Lin,Jingfeng Wu,Peter L. Bartlett*

Main category: cs.LG

TL;DR: The paper demonstrates that reusing data in multi-pass SGD can lead to improved test error scaling in linear regression, especially in data-constrained settings.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of scaling laws for large models in data-constrained regimes.

Method: The authors derive theoretical test error bounds for linear models trained with multi-pass SGD under specific power-law assumptions and validate these bounds through simulations.

Result: Multi-pass SGD achieves better test error scaling compared to one-pass SGD by reusing data, especially when data is limited.

Conclusion: Data reuse via multi-pass SGD improves test error scaling laws, providing an effective strategy in data-constrained scenarios.

Abstract: Neural scaling laws suggest that the test error of large language models
trained online decreases polynomially as the model size and data size increase.
However, such scaling can be unsustainable when running out of new data. In
this work, we show that data reuse can improve existing scaling laws in linear
regression. Specifically, we derive sharp test error bounds on $M$-dimensional
linear models trained by multi-pass stochastic gradient descent (multi-pass
SGD) on $N$ data with sketched features. Assuming that the data covariance has
a power-law spectrum of degree $a$, and that the true parameter follows a prior
with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show
that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$,
where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting,
one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see
e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse
(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are
also provided to verify our theoretical findings.

</details>


### [253] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Main category: cs.LG

TL;DR: AutoSDT introduces an automatic pipeline for creating high-quality coding tasks tailored for scientific discovery using LLMs, resulting in a dataset, AutoSDT-5K, to train improved AI co-scientists.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the scarcity of high-quality datasets for training and evaluating AI models in scientific discovery tasks, which remains a bottleneck in building effective AI co-scientists.

Method: AutoSDT uses an automatic pipeline leveraging LLMs to search for diverse coding sources, select ecologically valid tasks, and synthesize task instructions and functional code solutions. It creates a dataset called AutoSDT-5K with 5,404 coding tasks across four disciplines and 756 Python packages.

Result: AutoSDT-5K achieves high quality with 93% ecological validity and 92.2% program correctness according to expert evaluation. Models trained on it, like AutoSDT-Coder-32B, show doubled success rate on ScienceAgentBench and 17.4% improvement on DiscoveryBench.

Conclusion: AutoSDT demonstrates that automatic data pipelines powered by LLMs can produce valuable datasets for scientific discovery, bridging gaps in open-weight model performance and setting a new benchmark for AI co-scientists.

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [254] [Online Learning-guided Learning Rate Adaptation via Gradient Alignment](https://arxiv.org/abs/2506.08419)
*Ruichen Jiang,Ali Kavis,Aryan Mokhtari*

Main category: cs.LG

TL;DR: This paper introduces GALA, an adaptive learning rate framework for optimizers, improving gradient alignment and simplifying hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning the learning rate for large-scale deep learning models is critical but often tedious, involving extensive grid searches.

Method: GALA dynamically adjusts learning rates using gradient alignment and local curvature estimates, framed as a one-dimensional online learning problem.

Result: Empirically, GALA enhances SGD and Adam optimizers, offering robust and competitive performance across varied learning rates without tuning.

Conclusion: GALA simplifies learning rate selection, combining theoretical convergence guarantees with effective empirical results.

Abstract: The performance of an optimizer on large-scale deep learning models depends
critically on fine-tuning the learning rate, often requiring an extensive grid
search over base learning rates, schedules, and other hyperparameters. In this
paper, we propose a principled framework called GALA (Gradient Alignment-based
Learning rate Adaptation), which dynamically adjusts the learning rate by
tracking the alignment between consecutive gradients and using a local
curvature estimate. Guided by the convergence analysis, we formulate the
problem of selecting the learning rate as a one-dimensional online learning
problem. When paired with an online learning algorithm such as
Follow-the-Regularized-Leader, our method produces a flexible, adaptive
learning rate schedule that tends to increase when consecutive gradients are
aligned and decrease otherwise. We establish a data-adaptive convergence rate
for normalized SGD equipped with GALA in the smooth, nonconvex setting.
Empirically, common optimizers such as SGD and Adam, when augmented with GALA,
demonstrate robust performance across a wide range of initial learning rates
and perform competitively without the need for tuning.

</details>


### [255] [Accelerating Spectral Clustering under Fairness Constraints](https://arxiv.org/abs/2506.08143)
*Francesco Tonin,Alex Lambert,Johan A. K. Suykens,Volkan Cevher*

Main category: cs.LG

TL;DR: This paper develops an efficient algorithm for spectral clustering that ensures group fairness, achieving proportional demographic group representation in clusters.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the fairness issue in algorithmic decision-making, focusing on achieving proportional demographic representation in spectral clustering.

Method: The method involves casting the fairness problem within the difference of convex functions framework, using a novel variable augmentation and an alternating direction method of multipliers algorithm for improved computational efficiency.

Result: The proposed method significantly speeds up computation time compared to prior approaches, especially for large problem sizes, and demonstrates effectiveness on synthetic and real-world benchmarks.

Conclusion: The work represents meaningful progress in applying fair clustering to practical scenarios, making it more computationally viable for real-world applications.

Abstract: Fairness of decision-making algorithms is an increasingly important issue. In
this paper, we focus on spectral clustering with group fairness constraints,
where every demographic group is represented in each cluster proportionally as
in the general population. We present a new efficient method for fair spectral
clustering (Fair SC) by casting the Fair SC problem within the difference of
convex functions (DC) framework. To this end, we introduce a novel variable
augmentation strategy and employ an alternating direction method of multipliers
type of algorithm adapted to DC problems. We show that each associated
subproblem can be solved efficiently, resulting in higher computational
efficiency compared to prior work, which required a computationally expensive
eigendecomposition. Numerical experiments demonstrate the effectiveness of our
approach on both synthetic and real-world benchmarks, showing significant
speedups in computation time over prior art, especially as the problem size
grows. This work thus represents a considerable step forward towards the
adoption of fair clustering in real-world applications.

</details>


### [256] [Learning to Lead: Incentivizing Strategic Agents in the Dark](https://arxiv.org/abs/2506.08438)
*Yuchen Wu,Xinyi Zhong,Zhuoran Yang*

Main category: cs.LG

TL;DR: This paper tackles an online learning generalization of the principal-agent model with a strategic agent and establishes a sample-efficient algorithm with near-optimal regret.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in learning optimal mechanisms in game-theoretic settings where agents are strategic and possess private information.

Method: The approach incorporates (i) a delaying mechanism to incentivize quasi-myopic behavior, (ii) a reward estimation framework using sector tests and matching, and (iii) a regret-minimizing exploration method with a modified LinUCB algorithm.

Result: The authors demonstrate a near-optimal $\tilde{O}(\sqrt{T})$ regret bound for the principal's policy learning.

Conclusion: Their findings contribute a robust solution for online learning problems in strategic game-theoretic contexts and provide a foundation for future research in these settings.

Abstract: We study an online learning version of the generalized principal-agent model,
where a principal interacts repeatedly with a strategic agent possessing
private types, private rewards, and taking unobservable actions. The agent is
non-myopic, optimizing a discounted sum of future rewards and may strategically
misreport types to manipulate the principal's learning. The principal,
observing only her own realized rewards and the agent's reported types, aims to
learn an optimal coordination mechanism that minimizes strategic regret. We
develop the first provably sample-efficient algorithm for this challenging
setting. Our approach features a novel pipeline that combines (i) a delaying
mechanism to incentivize approximately myopic agent behavior, (ii) an
innovative reward angle estimation framework that uses sector tests and a
matching procedure to recover type-dependent reward functions, and (iii) a
pessimistic-optimistic LinUCB algorithm that enables the principal to explore
efficiently while respecting the agent's incentive constraints. We establish a
near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's
optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our
results open up new avenues for designing robust online learning algorithms for
a wide range of game-theoretic settings involving private types and strategic
agents.

</details>


### [257] [Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields](https://arxiv.org/abs/2506.08146)
*Vahidullah Taç,Amirhossein Amiri-Hezaveh,Manuel K. Rausch,Grace N. Bechtel,Francisco Sahli Costabal,Adrian Buganza Tepole*

Main category: cs.LG

TL;DR: A new framework is introduced for identifying mechanical properties of heterogeneous materials using neural networks and a data-driven approach, eliminating the need for closed-form constitutive equations.


<details>
  <summary>Details</summary>
Motivation: There is a need to identify mechanical properties of heterogeneous materials without relying on traditional closed-form constitutive equations, especially in cases with complex gradients and noise.

Method: The authors use neural networks enhanced with Fourier features to approximate strain fields, employ NODEs to discover constitutive equations, and introduce a hyper-network to handle heterogeneity. Optimization is achieved through a multi-objective loss function addressing equilibrium constraints.

Result: The framework effectively identifies properties in various scenarios, such as material heterogeneities, isotropy-to-anisotropy transitions, noise resilience, and real experimental data, showcasing robustness and generality.

Conclusion: This approach demonstrates strong potential as a reliable alternative to classical inverse methods, making it suitable for complex material identification challenges.

Abstract: We propose a new framework for identifying mechanical properties of
heterogeneous materials without a closed-form constitutive equation. Given a
full-field measurement of the displacement field, for instance as obtained from
digital image correlation (DIC), a continuous approximation of the strain field
is obtained by training a neural network that incorporates Fourier features to
effectively capture sharp gradients in the data. A physics-based data-driven
method built upon ordinary neural differential equations (NODEs) is employed to
discover constitutive equations. The NODE framework can represent arbitrary
materials while satisfying constraints in the theory of constitutive equations
by default. To account for heterogeneity, a hyper-network is defined, where the
input is the material coordinate system, and the output is the NODE-based
constitutive equation. The parameters of the hyper-network are optimized by
minimizing a multi-objective loss function that includes penalty terms for
violations of the strong form of the equilibrium equations of elasticity and
the associated Neumann boundary conditions. We showcase the framework with
several numerical examples, including heterogeneity arising from variations in
material parameters, spatial transitions from isotropy to anisotropy, material
identification in the presence of noise, and, ultimately, application to
experimental data. As the numerical results suggest, the proposed approach is
robust and general in identifying the mechanical properties of heterogeneous
materials with very few assumptions, making it a suitable alternative to
classical inverse methods.

</details>


### [258] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)
*Samarth Sikand,Rohit Mehra,Priyavanshi Pathania,Nikhil Bamby,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.LG

TL;DR: This paper introduces R-ICE, a framework that estimates carbon emissions from Large Language Model (LLM) inferences using benchmarks, addressing sustainability challenges of LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the environmental impact of LLM usage, which burdens energy grids and poses challenges for organizations aiming to achieve sustainability goals.

Method: The authors propose R-ICE, a framework leveraging existing state-of-the-art LLM benchmarks to estimate carbon emissions at the prompt level in a non-intrusive and practical way.

Result: Validation results demonstrate that benchmark-based modeling is a promising approach for accurate inference emission estimation, overcoming challenges of current methods.

Conclusion: R-ICE can facilitate applications like dynamic LLM routing and carbon accounting, and the approach shows potential for future research on sustainable LLM operations.

Abstract: While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


### [259] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Main category: cs.LG

TL;DR: Large Language Models (LLMs) exhibit task-dependent "geometries of truth" in their activation patterns, limiting their reliability for cross-task generalization.


<details>
  <summary>Details</summary>
Motivation: Investigate the reliability of LLMs and whether their activations can reliably classify correct answers across different tasks.

Method: Analyze activation patterns during inference and evaluate the similarity of linear classifiers trained across tasks, introducing sparsity-enforcing regularizers and mixture models.

Result: Linear classifiers trained on activation patterns show limited similarity and disjoint supports across tasks; activation clusters remain task-specific despite advanced methods.

Conclusion: "Geometries of truth" from activation patterns are fundamentally task-dependent, posing challenges for cross-task reliability in LLM inference.

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [260] [BLUR: A Bi-Level Optimization Approach for LLM Unlearning](https://arxiv.org/abs/2506.08164)
*Hadi Reisizadeh,Jinghan Jia,Zhiqi Bu,Bhanukiran Vinzamuri,Anil Ramakrishna,Kai-Wei Chang,Volkan Cevher,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: The paper introduces a bi-level optimization framework for unlearning in large language models, proposing the BLUR algorithm which achieves superior performance.


<details>
  <summary>Details</summary>
Motivation: Ensuring compliance with data regulations and ethical practices in AI through effective unlearning in LLMs while addressing the trade-offs in existing methodologies.

Method: The study introduces the BLUR algorithm using a bi-level optimization approach, prioritizing unlearning (forget loss) at the lower level and preserving utility (retain loss) at the upper level.

Result: BLUR demonstrates superior performance and outperforms state-of-the-art methods across varied unlearning tasks, models, and metrics.

Conclusion: The hierarchical bi-level framework for unlearning, as implemented in BLUR, offers theoretical guarantees and consistently high performance, marking a significant improvement in the field.

Abstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities
acquired during training has proven vital for ensuring compliance with data
regulations and promoting ethical practices in generative AI. Although there
are growing interests in developing various unlearning algorithms, it remains
unclear how to best formulate the unlearning problem. The most popular
formulation uses a weighted sum of forget and retain loss, but it often leads
to performance degradation due to the inherent trade-off between forget and
retain losses. In this work, we argue that it is important to model the
hierarchical structure of the unlearning problem, where the forget problem
(which \textit{unlearns} certain knowledge and/or capabilities) takes priority
over the retain problem (which preserves model utility). This hierarchical
structure naturally leads to a bi-level optimization formulation where the
lower-level objective focuses on minimizing the forget loss, while the
upper-level objective aims to maintain the model's utility. Based on this new
formulation, we propose a novel algorithm, termed Bi-Level UnleaRning
(\texttt{BLUR}), which not only possesses strong theoretical guarantees but
more importantly, delivers superior performance. In particular, our extensive
experiments demonstrate that \texttt{BLUR} consistently outperforms all the
state-of-the-art algorithms across various unlearning tasks, models, and
metrics. Codes are available at
https://github.com/OptimAI-Lab/BLURLLMUnlearning.

</details>


### [261] [Local MDI+: Local Feature Importances for Tree-Based Models](https://arxiv.org/abs/2506.08928)
*Zhongyuan Liang,Zachary T. Rewolinski,Abhineet Agarwal,Tiffany M. Tang,Bin Yu*

Main category: cs.LG

TL;DR: Tree-based ensembles are preferred for tabular data due to their efficiency and prediction performance. This study proposes Local MDI+ (LMDI+), a method for sample-specific interpretability that improves on existing methods like LIME and TreeSHAP.


<details>
  <summary>Details</summary>
Motivation: High-stakes applications require trustworthy and interpretable predictions, but popular local feature importance methods face stability and structural issues.

Method: The paper introduces LMDI+, extending the global MDI+ framework to the local setting by leveraging structural equivalence between decision trees and linear models.

Result: LMDI+ achieves a 10% improvement in signal feature identification over LIME and TreeSHAP across twelve datasets, and shows superior stability in feature importance rankings.

Conclusion: LMDI+ enhances interpretability, enabling use cases like counterfactual identification and subgroup discovery, and outperforms existing methods in accuracy and stability.

Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data
over deep learning models due to their prediction performance and computational
efficiency. These advantages have led to their widespread deployment in
high-stakes domains, where interpretability is essential for ensuring
trustworthy predictions. This has motivated the development of popular local
(i.e. sample-specific) feature importance (LFI) methods such as LIME and
TreeSHAP. However, these approaches rely on approximations that ignore the
model's internal structure and instead depend on potentially unstable
perturbations. These issues are addressed in the global setting by MDI+, a
feature importance method which exploits an equivalence between decision trees
and linear models on a transformed node basis. However, the global MDI+ scores
are not able to explain predictions when faced with heterogeneous individual
characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel
extension of the MDI+ framework to the sample specific setting. LMDI+
outperforms existing baselines LIME and TreeSHAP in identifying
instance-specific signal features, averaging a 10% improvement in downstream
task performance across twelve real-world benchmark datasets. It further
demonstrates greater stability by consistently producing similar instance-level
feature importance rankings across multiple random forest fits. Finally, LMDI+
enables local interpretability use cases, including the identification of
closer counterfactuals and the discovery of homogeneous subgroups.

</details>


### [262] [FedGA-Tree: Federated Decision Tree using Genetic Algorithm](https://arxiv.org/abs/2506.08176)
*Anh V Nguyen,Diego Klabjan*

Main category: cs.LG

TL;DR: This paper introduces a novel Federated Learning approach for decision trees using Genetic Algorithms, enabling personalized models for classification and regression tasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing Federated Learning methods for decision trees, which often rely on greedy algorithms and differential privacy, and struggle with mixed or numerical data.

Method: The authors developed a Genetic Algorithm-enabled framework to create personalized decision trees. This approach accommodates both categorical and numerical data for classification and regression tasks.

Result: Experiments showed that this approach outperformed decision trees trained on local data and a benchmark algorithm.

Conclusion: The proposed method successfully enhances the efficacy of Federated Learning for decision trees, tackling previous limitations while showing superior performance in diverse scenarios.

Abstract: In recent years, with rising concerns for data privacy, Federated Learning
has gained prominence, as it enables collaborative training without the
aggregation of raw data from participating clients. However, much of the
current focus has been on parametric gradient-based models, while nonparametric
counterparts such as decision tree are relatively understudied. Existing
methods for adapting decision trees to Federated Learning generally combine a
greedy tree-building algorithm with differential privacy to produce a global
model for all clients. These methods are limited to classification trees and
categorical data due to the constraints of differential privacy. In this paper,
we explore an alternative approach that utilizes Genetic Algorithm to
facilitate the construction of personalized decision trees and accommodate
categorical and numerical data, thus allowing for both classification and
regression trees. Comprehensive experiments demonstrate that our method
surpasses decision trees trained solely on local data and a benchmark
algorithm.

</details>


### [263] [Correlated Noise Mechanisms for Differentially Private Learning](https://arxiv.org/abs/2506.08201)
*Krishna Pillutla,Jalaj Upadhyay,Christopher A. Choquette-Choo,Krishnamurthy Dvijotham,Arun Ganesh,Monika Henzinger,Jonathan Katz,Ryan McKenna,H. Brendan McMahan,Keith Rush,Thomas Steinke,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: The paper analyzes correlated noise mechanisms for differential privacy in AI and machine learning, aiming to improve privacy-utility trade-offs and highlighting industrial applications.


<details>
  <summary>Details</summary>
Motivation: Noise addition in DP safeguards training data privacy but may reduce utility. Correlated noise mechanisms aim to enhance this balance, an area gaining industry interest.

Method: Focuses on the design and evaluation of correlated noise strategies (e.g., DP-FTRL, matrix mechanisms) in private model training, leveraging weighted prefix sum estimation for better outcomes.

Result: Correlated noise mechanisms offer improved privacy-utility trade-offs compared to independent noise addition, influencing both academic research and industrial deployment.

Conclusion: Correlated noise mechanisms are a promising approach to advancing differential privacy in AI, with tangible benefits for both theoretical and practical applications.

Abstract: This monograph explores the design and analysis of correlated noise
mechanisms for differential privacy (DP), focusing on their application to
private training of AI and machine learning models via the core primitive of
estimation of weighted prefix sums. While typical DP mechanisms inject
independent noise into each step of a stochastic gradient (SGD) learning
algorithm in order to protect the privacy of the training data, a growing body
of recent research demonstrates that introducing (anti-)correlations in the
noise can significantly improve privacy-utility trade-offs by carefully
canceling out some of the noise added on earlier steps in subsequent steps.
Such correlated noise mechanisms, known variously as matrix mechanisms,
factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when
applied to learning algorithms, have also been influential in practice, with
industrial deployment at a global scale.

</details>


### [264] [A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts](https://arxiv.org/abs/2506.08205)
*Shadab Anwar Shaikh,Kranthi Balusu,Ayoub Soulami*

Main category: cs.LG

TL;DR: The paper proposes a machine learning model, Residual Stress Generator (RSG), to estimate full-field residual stresses accurately from limited data, reducing experimental requirements.


<details>
  <summary>Details</summary>
Motivation: Residual stresses can impair the performance of components, and determining their full-field distributions is crucial for optimizing structural integrity and longevity. The current experimental methods for achieving this are impractical due to high effort requirements.

Method: The authors create a large dataset through process simulations and use systematic hyperparameter tuning to train a U-Net-based ML model to infer full-field residual stresses from limited inputs.

Result: The model showed excellent predictive accuracy in simulated data and generalized well to experimental data, proving its ability to learn latent patterns in residual stress distributions.

Conclusion: The proposed RSG effectively estimates full-field residual stresses from limited data, making it a practical tool to reduce experimental efforts and enhance understanding of stress distributions.

Abstract: Residual stresses, which remain within a component after processing, can
deteriorate performance. Accurately determining their full-field distributions
is essential for optimizing the structural integrity and longevity. However,
the experimental effort required for full-field characterization is
impractical. Given these challenges, this work proposes a machine learning (ML)
based Residual Stress Generator (RSG) to infer full-field stresses from limited
measurements. An extensive dataset was initially constructed by performing
numerous process simulations with a diverse parameter set. A ML model based on
U-Net architecture was then trained to learn the underlying structure through
systematic hyperparameter tuning. Then, the model's ability to generate
simulated stresses was evaluated, and it was ultimately tested on actual
characterization data to validate its effectiveness. The model's prediction of
simulated stresses shows that it achieved excellent predictive accuracy and
exhibited a significant degree of generalization, indicating that it
successfully learnt the latent structure of residual stress distribution. The
RSG's performance in predicting experimentally characterized data highlights
the feasibility of the proposed approach in providing a comprehensive
understanding of residual stress distributions from limited measurements,
thereby significantly reducing experimental efforts.

</details>


### [265] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/abs/2506.08216)
*Shahaf Bassan,Guy Amir,Meirav Zehavi,Guy Katz*

Main category: cs.LG

TL;DR: The paper investigates the interpretability of ensemble models in machine learning using concepts from computational complexity theory, revealing varying complexity patterns based on factors like the number and type of base models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of rigorous mathematical understanding regarding the interpretability of ensemble models, which are widely considered black-boxes despite the interpretability of individual components like decision trees.

Method: The authors utilize computational complexity theory to analyze the interpretability of ensembles, exploring how factors such as the number, size, and types of base models affect the ability to generate explanations.

Result: The study finds that interpretability varies significantly with ensemble configurations. For example, small ensembles of decision trees are easy to interpret, but ensembles with a constant number of linear models remain intractable under standard complexity assumptions like P≠NP.

Conclusion: This work underscores the importance of examining ensemble interpretability through a computational complexity perspective, offering a robust foundation for understanding and addressing their black-box nature.

Abstract: Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [266] [Mondrian: Transformer Operators via Domain Decomposition](https://arxiv.org/abs/2506.08226)
*Arthur Feeney,Kuei-Hsiang Huang,Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: The paper introduces Mondrian, a transformer-based operator model designed for scalable and efficient learning of mappings in high-resolution, multiscale domains by using domain decomposition.


<details>
  <summary>Details</summary>
Motivation: Scalability and efficiency hinder the application of transformer models to high-resolution, multiscale PDE domains, due to the quadratic computational cost of attention and its dependence on discretization.

Method: Mondrian divides the computational domain into non-overlapping subdomains, employs neural operators within subdomains for expressiveness, and calculates attention across subdomains using function-based inner products. It supports hierarchical inter-domain interactions.

Result: Mondrian achieved strong performance on PDE tasks, including Allen-Cahn and Navier-Stokes equations, and demonstrated the ability to scale across resolutions without requiring retraining.

Conclusion: The study establishes Mondrian's effectiveness and scalability, suggesting that domain-decomposed attention can pave the way for generalized neural operator models.

Abstract: Operator learning enables data-driven modeling of partial differential
equations (PDEs) by learning mappings between function spaces. However, scaling
transformer-based operator models to high-resolution, multiscale domains
remains a challenge due to the quadratic cost of attention and its coupling to
discretization. We introduce \textbf{Mondrian}, transformer operators that
decompose a domain into non-overlapping subdomains and apply attention over
sequences of subdomain-restricted functions. Leveraging principles from domain
decomposition, Mondrian decouples attention from discretization. Within each
subdomain, it replaces standard layers with expressive neural operators, and
attention across subdomains is computed via softmax-based inner products over
functions. The formulation naturally extends to hierarchical windowed and
neighborhood attention, supporting both local and global interactions. Mondrian
achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating
resolution scaling without retraining. These results highlight the promise of
domain-decomposed attention for scalable and general-purpose neural operators.

</details>


### [267] [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/abs/2506.08228)
*Mustafa Baniodeh,Kratarth Goel,Scott Ettinger,Carlos Fuertes,Ari Seff,Tim Shen,Cole Gulino,Chenjie Yang,Ghassen Jerfel,Dokook Choe,Rui Wang,Vinutha Kallem,Sergio Casas,Rami Al-Rfou,Benjamin Sapp,Dragomir Anguelov*

Main category: cs.LG

TL;DR: This paper studies scaling laws for transformer models in autonomous driving, highlighting performance gains with increased compute and data budgets, and explores trade-offs in model and inference-time scaling.


<details>
  <summary>Details</summary>
Motivation: To investigate and optimize empirical scaling laws for encoder-decoder transformer models to improve motion forecasting and planning in autonomous driving scenarios.

Method: Utilized a large-scale dataset (500K driving hours) to analyze training and inference-time scaling properties, focusing on transformer parameters, dataset size, and closed-loop vs. open-loop performance metrics.

Result: Revealed power-law improvements in performance with compute, a 1.5x increase in model size relative to dataset growth for compute-optimal scaling, and competitive efficiency of smaller models via inference-time clustering up to a crossover point.

Conclusion: Optimizing both training and inference scaling is critical for enhancing autonomous driving models, with novel insights into the utility of general driving data for training and closed-loop metric improvements with scale.

Abstract: We study the empirical scaling laws of a family of encoder-decoder
autoregressive transformer models on the task of joint motion forecasting and
planning in the autonomous driving domain. Using a 500 thousand hours driving
dataset, we demonstrate that, similar to language modeling, model performance
improves as a power-law function of the total compute budget, and we observe a
strong correlation between model training loss and model evaluation metrics.
Most interestingly, closed-loop metrics also improve with scaling, which has
important implications for the suitability of open-loop metrics for model
development and hill climbing. We also study the optimal scaling of the number
of transformer parameters and the training data size for a training
compute-optimal model. We find that as the training compute budget grows,
optimal scaling requires increasing the model size 1.5x as fast as the dataset
size. We also study inference-time compute scaling, where we observe that
sampling and clustering the output of smaller models makes them competitive
with larger models, up to a crossover point beyond which a larger models
becomes more inference-compute efficient. Overall, our experimental results
demonstrate that optimizing the training and inference-time scaling properties
of motion forecasting and planning models is a key lever for improving their
performance to address a wide variety of driving scenarios. Finally, we briefly
study the utility of training on general logged driving data of other agents to
improve the performance of the ego-agent, an important research area to address
the scarcity of robotics data for large capacity models training.

</details>


### [268] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/abs/2506.08240)
*Dongkyu Cho,Rumi Chunara*

Main category: cs.LG

TL;DR: This paper investigates the limitations of random data augmentation and proposes a solution to address its inefficiency, showing improved performance in single source domain generalization benchmarks.


<details>
  <summary>Details</summary>
Motivation: Random data augmentation, although cost-effective, lacks efficiency in enhancing out-of-distribution generalization compared to targeted augmentations.

Method: The authors identify that random augmentation can lead to feature distortion akin to catastrophic forgetting and propose a simple fix to mitigate this issue.

Result: The proposed solution demonstrates strong generalization performance across different single source domain generalization benchmarks.

Conclusion: Their approach enhances the effectiveness of random augmentation in domain generalization tasks while providing a cost-efficient alternative to targeted augmentations.

Abstract: Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [269] [MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2506.08460)
*Yihong Guo,Yu Yang,Pan Xu,Anqi Liu*

Main category: cs.LG

TL;DR: MOBODY is a model-based offline RL approach that enables exploration in the target domain using learned dynamics, outperforming baselines on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of learning policies with mismatched dynamics between source and target domains in offline RL.

Method: A model-based algorithm generating synthetic transitions for the target domain using a shared latent representation and stabilizing training with Q-weighted behavior cloning.

Result: MOBODY achieves superior performance compared to current state-of-the-art methods, particularly in challenging scenarios.

Conclusion: The approach successfully enhances target-domain exploration and learning to improve offline RL performance in mismatched dynamics settings.

Abstract: We study the off-dynamics offline reinforcement learning problem, where the
goal is to learn a policy from offline datasets collected from source and
target domains with mismatched transition. Existing off-dynamics offline RL
methods typically either filter source transitions that resemble those of the
target domain or apply reward augmentation to source data, both constrained by
the limited transitions available from the target domain. As a result, the
learned policy is unable to explore target domain beyond the offline datasets.
We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that
addresses this limitation by enabling exploration of the target domain via
learned dynamics. MOBODY generates new synthetic transitions in the target
domain through model rollouts, which are used as data augmentation during
offline policy learning. Unlike existing model-based methods that learn
dynamics from a single domain, MOBODY tackles the challenge of mismatched
dynamics by leveraging both source and target datasets. Directly merging these
datasets can bias the learned model toward source dynamics. Instead, MOBODY
learns target dynamics by discovering a shared latent representation of states
and transitions across domains through representation learning. To stabilize
training, MOBODY incorporates a behavior cloning loss that regularizes the
policy. Specifically, we introduce a Q-weighted behavior cloning loss that
regularizes the policy toward actions with high target-domain Q-values, rather
than uniformly imitating all actions in the dataset. These Q-values are learned
from an enhanced target dataset composed of offline target data, augmented
source data, and rollout data from the learned target dynamics. We evaluate
MOBODY on MuJoCo benchmarks and show that it significantly outperforms
state-of-the-art baselines, with especially pronounced improvements in
challenging scenarios.

</details>


### [270] [Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic](https://arxiv.org/abs/2506.08243)
*Zhenjiang Mao,Artem Bisliouk,Rohith Reddy Nama,Ivan Ruchkin*

Main category: cs.LG

TL;DR: The paper introduces a framework using Signal Temporal Logic (STL) to improve confidence estimation in Large Language Models during stepwise mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: Large Language Models excel in mathematical reasoning, but their overconfidence in incorrect answers creates risks, especially where users may lack expertise, such as in education.

Method: The authors propose a method modeling stepwise confidence as a temporal signal, evaluated using STL-based formal constraints and robustness scores. They also introduce strategies to ensure smooth, monotonic, and causally consistent reasoning trajectories.

Result: The proposed framework improves calibration metrics and generates more reliable uncertainty estimates compared to traditional techniques.

Conclusion: The framework offers a structured and interpretable confidence estimation approach, mitigating overconfidence issues in LLMs and leading to safer deployment in critical applications.

Abstract: Large Language Models (LLMs) have shown impressive performance in
mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.
However, they tend to produce highly confident yet incorrect outputs, which
poses significant risks in domains like education, where users may lack the
expertise to assess reasoning steps. To address this, we propose a structured
framework that models stepwise confidence as a temporal signal and evaluates it
using Signal Temporal Logic (STL). In particular, we define formal STL-based
constraints to capture desirable temporal properties and compute robustness
scores that serve as structured, interpretable confidence estimates. Our
approach also introduces a set of uncertainty reshaping strategies to enforce
smoothness, monotonicity, and causal consistency across the reasoning
trajectory. Experiments show that our approach consistently improves
calibration metrics and provides more reliable uncertainty estimates than
conventional confidence aggregation and post-hoc calibration.

</details>


### [271] [How to Provably Improve Return Conditioned Supervised Learning?](https://arxiv.org/abs/2506.08463)
*Zhishuai Liu,Yu Yang,Ruhan Wang,Pan Xu,Dongruo Zhou*

Main category: cs.LG

TL;DR: The paper introduces Reinforced RCSL, which enhances Return-Conditioned Supervised Learning (RCSL) by addressing its limitations with a mechanism called in-distribution optimal return-to-go.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of stitching property in RCSL, which limits its performance to the quality of policies in offline datasets.

Method: The authors propose Reinforced RCSL, which uses in-distribution optimal return-to-go to determine the best achievable future returns from given states without relying on complex augmentation.

Result: Reinforced RCSL consistently outperforms standard RCSL, as validated by theoretical analysis and empirical benchmarks.

Conclusion: Reinforced RCSL is a simple yet effective improvement over RCSL that achieves better performance stability and generalizability in sequential decision-making problems.

Abstract: In sequential decision-making problems, Return-Conditioned Supervised
Learning (RCSL) has gained increasing recognition for its simplicity and
stability in modern decision-making tasks. Unlike traditional offline
reinforcement learning (RL) algorithms, RCSL frames policy learning as a
supervised learning problem by taking both the state and return as input. This
approach eliminates the instability often associated with temporal difference
(TD) learning in offline RL. However, RCSL has been criticized for lacking the
stitching property, meaning its performance is inherently limited by the
quality of the policy used to generate the offline dataset. To address this
limitation, we propose a principled and simple framework called Reinforced
RCSL. The key innovation of our framework is the introduction of a concept we
call the in-distribution optimal return-to-go. This mechanism leverages our
policy to identify the best achievable in-dataset future return based on the
current state, avoiding the need for complex return augmentation techniques.
Our theoretical analysis demonstrates that Reinforced RCSL can consistently
outperform the standard RCSL approach. Empirical results further validate our
claims, showing significant performance improvements across a range of
benchmarks.

</details>


### [272] [SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense](https://arxiv.org/abs/2506.08255)
*Patryk Krukowski,Łukasz Gorczyca,Piotr Helm,Kamil Książek,Przemysław Spurek*

Main category: cs.LG

TL;DR: The paper introduces SHIELD, a novel method that combats both catastrophic forgetting and adversarial vulnerability using hypernetworks and interval arithmetic.


<details>
  <summary>Details</summary>
Motivation: Traditional models struggle with catastrophic forgetting when learning new tasks and are not robust to adversarial attacks, demanding a unified approach to tackle both issues.

Method: The SHIELD framework combines hypernetworks for task-specific model generation with interval arithmetic to ensure strict robustness guarantees against adversarial attacks.

Result: SHIELD successfully enables dynamic network adaptation for different tasks and provides robust predictions resilient to adversarial attacks.

Conclusion: SHIELD addresses the dual challenges of safety and adaptability in continual learning, providing a robust and innovative solution.

Abstract: Traditional deep neural networks suffer from several limitations, including
catastrophic forgetting. When models are adapted to new datasets, they tend to
quickly forget previously learned knowledge. Another significant issue is the
lack of robustness to even small perturbations in the input data. In practice,
we can often easily perform adversarial attacks and change the network's
predictions, adding minimal noise to the input. Dedicated architectures and
training procedures can solve each of the above problems separately.
Unfortunately, currently, no model can simultaneously address both catastrophic
forgetting and vulnerability to adversarial attacks. We introduce SHIELD
(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel
approach that integrates a hypernetwork-based continual learning approach with
interval arithmetic. SHIELD use the hypernetwork to transfer trainable task
embedding vectors into the weights of a target model dedicated to specific
data. This paradigm allows for the dynamic generation of separate networks for
each subtask, while the hypernetwork aggregates and analyzes information across
all tasks. The target model takes in the input a data sample with a defined
interval range, and by creating a hypercube, produces a prediction for the
given range. Therefore, such target models provide strict guarantees against
all possible attacks for data samples within the interval range. Our approach
enhances security without sacrificing network adaptability, addressing the
overlooked challenge of safety in continual learning.

</details>


### [273] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Main category: cs.LG

TL;DR: This paper introduces HC-RLHF, a method to safely align language models by balancing harmlessness and helpfulness, yielding safer and better models than past approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aligning language models often present a tradeoff between safety and helpfulness, which can lead to unsafe or unacceptable outcomes in sensitive applications.

Method: The HC-RLHF approach first separates human preferences into helpfulness (reward model) and harmlessness (cost model). It uses a two-step process: 1) optimizing for helpfulness under a pessimistic cost constraint and 2) performing a safety test to verify that the results stay within safety bounds, with theoretical and empirical evidence to support its efficacy.

Result: HC-RLHF successfully aligns three different language models (Qwen2-1.5B, Qwen2.5-3B, LLaMa3.2-3B) with higher confidence in safety and an improved balance of harmlessness and helpfulness compared to prior methods.

Conclusion: HC-RLHF demonstrates that high-confidence safety guarantees and optimization for helpfulness can coexist, offering significant improvements in aligning language models safely without compromising their performance.

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [274] [Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression](https://arxiv.org/abs/2506.08267)
*Mansooreh Montazerin,Majd Al Aawar,Antonio Ortega,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: This paper introduces LIES, a neural network-based approach for symbolic regression, which outperforms traditional methods in generating sparse and accurate mathematical expressions.


<details>
  <summary>Details</summary>
Motivation: Symbolic regression struggles with scalability and maintaining symbolic consistency when using traditional methods like population-based search or autoregressive modeling.

Method: The authors propose LIES, a neural network with interpretable primitives (logarithm, identity, exponential, sine) and use oversampling, tailored loss functions, and pruning strategies to extract compact symbolic expressions.

Result: LIES shows superior performance in generating sparse and accurate symbolic expressions compared to baseline methods in SR benchmarks.

Conclusion: LIES offers a novel and effective framework for symbolic regression, addressing limitations of existing methods and advancing the generation of interpretable symbolic formulae.

Abstract: Symbolic regression (SR) aims to discover closed-form mathematical
expressions that accurately describe data, offering interpretability and
analytical insight beyond standard black-box models. Existing SR methods often
rely on population-based search or autoregressive modeling, which struggle with
scalability and symbolic consistency. We introduce LIES (Logarithm, Identity,
Exponential, Sine), a fixed neural network architecture with interpretable
primitive activations that are optimized to model symbolic expressions. We
develop a framework to extract compact formulae from LIES networks by training
with an appropriate oversampling strategy and a tailored loss function to
promote sparsity and to prevent gradient instability. After training, it
applies additional pruning strategies to further simplify the learned
expressions into compact formulae. Our experiments on SR benchmarks show that
the LIES framework consistently produces sparse and accurate symbolic formulae
outperforming all baselines. We also demonstrate the importance of each design
component through ablation studies.

</details>


### [275] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/abs/2506.08270)
*Zitong Huang,Mansooreh Montazerin,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: The paper presents a method to simultaneously optimize both the architecture and weights of neural networks using a continuous latent space.


<details>
  <summary>Details</summary>
Motivation: Current practices for designing neural networks either rely on manual effort or NAS, which are inefficient or separate architecture and weight optimization.

Method: The authors propose training a universal multi-scale autoencoder to embed architecture and parametric information into a continuous latent space. Gradient descent is then applied to optimize a network's structure and weights.

Result: The method successfully identifies sparse and efficient neural networks with strong performance on synthetic regression tasks.

Conclusion: The proposed approach introduces a novel framework to jointly optimize neural network structure and weights, obtaining efficient and high-performing models.

Abstract: Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [276] [Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids](https://arxiv.org/abs/2506.08272)
*Tarushri N. S.*

Main category: cs.LG

TL;DR: This paper introduces Universal Differential Equations (UDEs) for smart grid battery dynamics, blending neural networks with physics-based models to handle stochastic solar input and variable household loads.


<details>
  <summary>Details</summary>
Motivation: Modeling battery dynamics in smart grids is challenging due to stochastic solar input and variability in household loads. Traditional methods struggle with unmodeled residual dynamics and generalization.

Method: UDE framework incorporates a neural network as a residual component into physically-inspired battery ODEs, using synthetic solar generation and load data for simulations.

Result: Experiments confirm that the UDE accurately aligns with true battery trajectories, shows smooth convergence, and performs stably in long-term predictions.

Conclusion: UDE-based approaches are promising for decentralized energy networks, opening possibilities for real-time control and optimization in smart grids integrated with renewable energy.

Abstract: Universal Differential Equations (UDEs), which blend neural networks with
physical differential equations, have emerged as a powerful framework for
scientific machine learning (SciML), enabling data-efficient, interpretable,
and physically consistent modeling. In the context of smart grid systems,
modeling node-wise battery dynamics remains a challenge due to the
stochasticity of solar input and variability in household load profiles.
Traditional approaches often struggle with generalization and fail to capture
unmodeled residual dynamics. This work proposes a UDE-based approach to learn
node-specific battery evolution by embedding a neural residual into a
physically inspired battery ODE. Synthetic yet realistic solar generation and
load demand data are used to simulate battery dynamics over time. The neural
component learns to model unobserved or stochastic corrections arising from
heterogeneity in node demand and environmental conditions. Comprehensive
experiments reveal that the trained UDE aligns closely with ground truth
battery trajectories, exhibits smooth convergence behavior, and maintains
stability in long-term forecasts. These findings affirm the viability of
UDE-based SciML approaches for battery modeling in decentralized energy
networks and suggest broader implications for real-time control and
optimization in renewable-integrated smart grids.

</details>


### [277] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: The paper introduces a multi-agent reasoning framework for large language models (LLMs) called ECON, which achieves efficient coordination using game theory and reinforcement learning. ECON outperforms existing approaches while reducing computational costs and ensuring scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency and lack of convergence in multi-LLM reasoning frameworks while boosting their reasoning capabilities in complex tasks.

Method: The method involves recasting multi-LLM coordination as an incomplete-information game and introducing ECON, a hierarchical reinforcement learning framework. In ECON, each agent independently selects responses based on Bayesian Nash Equilibrium, avoiding expensive inter-agent communication.

Result: The authors demonstrate that ECON achieves a tighter regret bound compared to non-equilibrium methods. Empirical results show an average improvement of 11.2% over existing approaches across six reasoning and planning benchmarks.

Conclusion: ECON offers a scalable, efficient framework for multi-agent LLM coordination, paving the way for larger, more capable ensembles. Its higher performance and flexibility make it a significant step forward.

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [278] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Main category: cs.LG

TL;DR: AR-Bench is a benchmark to evaluate active reasoning skills in LLMs, which involves interaction with external systems to acquire missing data and solve tasks. Results indicate current LLMs struggle with active reasoning, suggesting further research is needed.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks mostly focus on passive reasoning abilities of LLMs, neglecting active reasoning that requires interaction with external systems to acquire additional evidence or data.

Method: Designed AR-Bench to explicitly assess active reasoning skills in LLMs through three diverse task families: detective cases, situation puzzles, and guessing numbers, emphasizing commonsense, logical, and symbolic reasoning.

Result: Empirical evaluation revealed that LLMs underperform in active reasoning tasks, struggling to obtain and use necessary external information effectively. Ablation studies showed limited improvement with advanced strategies.

Conclusion: Improving active reasoning in LLMs requires innovation in training methodologies, such as interactive learning, real-time feedback, and environment-aware objectives to address gaps and enable better real-world applications.

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [279] [H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs](https://arxiv.org/abs/2506.08298)
*Trung-Kien Nguyen,Heng Ping,Shixuan Li,Peiyu Zhang,Nikos Kanakaris,Nicholas Kotov,Paul Bogdan*

Main category: cs.LG

TL;DR: The paper introduces H$^2$GFM, a framework that generalizes across homogeneous and heterogeneous text-attributed graphs, designed to improve the Graph Foundation Model's adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing graph learning frameworks in handling heterogeneous text-attributed graphs (HeTAGs) and enhance the scope of the Graph Foundation Model.

Method: The proposed H$^2$GFM framework maps meta-relations under a unified textual space, employs context encoding for semantic relationship modeling, and uses a novel context-adaptive graph transformer (CGT) with a mixture of CGT experts for handling graph heterogeneity.

Result: Extensive experiments on various homogeneous and heterogeneous text-attributed graphs demonstrate the model's effectiveness across tasks and domains.

Conclusion: H$^2$GFM provides a robust and versatile solution for tackling both homogeneous and heterogeneous graph types, advancing the field of graph learning and its applications.

Abstract: The growing interests and applications of graph learning in diverse domains
have propelled the development of a unified model generalizing well across
different graphs and tasks, known as the Graph Foundation Model (GFM). Existing
research has leveraged text-attributed graphs (TAGs) to tackle the
heterogeneity in node features among graphs. However, they primarily focus on
homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple
types of nodes/edges reside, underexplored. To enhance the capabilities and
applications of GFM, we introduce H$^2$GFM, a novel framework designed to
generalize across both HoTAGs and HeTAGs. Our model projects diverse
meta-relations among graphs under a unified textual space, and employs a
context encoding to capture spatial and higher-order semantic relationships. To
achieve robust node representations, we propose a novel context-adaptive graph
transformer (CGT), effectively capturing information from both context
neighbors and their relationships. Furthermore, we employ a mixture of CGT
experts to capture the heterogeneity in structural patterns among graph types.
Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as
learning scenarios demonstrate the effectiveness of our model.

</details>


### [280] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/abs/2506.08309)
*Katherine Tieu,Dongqi Fu,Zihao Li,Ross Maciejewski,Jingrui He*

Main category: cs.LG

TL;DR: This paper introduces L-STEP, a novel Learnable Spatial-Temporal Positional Encoding model, addressing limitations in existing positional encoding methods for graph-based deep learning frameworks, and demonstrates its superiority in temporal link prediction tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in improving positional encoding mechanisms for graph-based learning frameworks, tackling limitations like pre-defined static functions, lack of temporal attribute consideration, and inefficiency for large-scale data.

Method: The authors developed L-STEP, utilizing learnable spatial-temporal positional encoding combined with MLPs, validated graph property preservation, and analyzed performance metrics including empirical running time and robustness across datasets.

Result: L-STEP proved superior for temporal link prediction tasks, outperforming existing methods across 13 datasets, 10 algorithms, and various settings, achieving top scores in large-scale benchmarks.

Conclusion: L-STEP offers a robust and efficient solution for spatial-temporal positional encoding, significantly enhancing temporal link prediction performance while reducing computational costs and complexity.

Abstract: Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, \name\ obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [281] [Private Evolution Converges](https://arxiv.org/abs/2506.08312)
*Tomás González,Giulia Fanti,Aaditya Ramdas*

Main category: cs.LG

TL;DR: The paper develops a new theoretical framework for the Private Evolution (PE) method in differentially private (DP) synthetic data generation, extending its worst-case convergence analysis and showing practical relevance via simulations.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the inconsistent performance of PE across different domains and evaluate its theoretical underpinnings in data privacy.

Method: The paper analyzes PE through a novel theoretical framework, deriving sufficient conditions for convergence and bounding the 1-Wasserstein distance of output data. They extend the analysis to Banach spaces and link to the Private Signed Measure Mechanism.

Result: PE is shown to converge in a worst-case scenario with bound $	ilde{O}(d(n \epsilon)^{-1/d})$ for high-dimensional datasets. The findings are supported by simulations demonstrating their practical application.

Conclusion: The research refines the understanding of PE's behavior and improves its theoretical foundation, enhancing its adaptability in DP synthetic data generation for various domains.

Abstract: Private Evolution (PE) is a promising training-free method for differentially
private (DP) synthetic data generation. While it achieves strong performance in
some domains (e.g., images and text), its behavior in others (e.g., tabular
data) is less consistent. To date, the only theoretical analysis of the
convergence of PE depends on unrealistic assumptions about both the algorithm's
behavior and the structure of the sensitive dataset. In this work, we develop a
new theoretical framework to explain PE's practical behavior and identify
sufficient conditions for its convergence. For $d$-dimensional sensitive
datasets with $n$ data points from a bounded domain, we prove that PE produces
an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein
distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original,
establishing worst-case convergence of the algorithm as $n \to \infty$. Our
analysis extends to general Banach spaces as well. We also connect PE to the
Private Signed Measure Mechanism, a method for DP synthetic data generation
that has thus far not seen much practical adoption. We demonstrate the
practical relevance of our theoretical findings in simulations.

</details>


### [282] [Graph Prompting for Graph Learning Models: Recent Advances and Future Directions](https://arxiv.org/abs/2506.08326)
*Xingbo Fu,Zehong Wang,Zihan Chen,Jiazheng Li,Yaochen Zhu,Zhenyu Lei,Cong Shen,Yanfang Ye,Chuxu Zhang,Jundong Li*

Main category: cs.LG

TL;DR: The paper reviews recent advancements in graph prompting, discussing pre-training methods, design of learnable prompts, applications, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To explore and summarize advancements in graph prompting as a promising approach for adapting pre-trained graph learning models to downstream tasks.

Method: Conducting a systematic review to introduce, categorize, and analyze graph pre-training methods, mainstream prompting techniques, real-world applications, and open challenges.

Result: The paper organizes advancements in graph prompting, highlights various methods and applications, and identifies promising future directions.

Conclusion: Graph prompting shows great potential in leveraging pre-trained models, with several promising directions for addressing challenges in the domain.

Abstract: Graph learning models have demonstrated great prowess in learning expressive
representations from large-scale graph data in a wide variety of real-world
scenarios. As a prevalent strategy for training powerful graph learning models,
the "pre-training, adaptation" scheme first pre-trains graph learning models on
unlabeled graph data in a self-supervised manner and then adapts them to
specific downstream tasks. During the adaptation phase, graph prompting emerges
as a promising approach that learns trainable prompts while keeping the
pre-trained graph learning models unchanged. In this paper, we present a
systematic review of recent advancements in graph prompting. First, we
introduce representative graph pre-training methods that serve as the
foundation step of graph prompting. Next, we review mainstream techniques in
graph prompting and elaborate on how they design learnable prompts for graph
prompting. Furthermore, we summarize the real-world applications of graph
prompting from different domains. Finally, we discuss several open challenges
in existing studies with promising future directions in this field.

</details>


### [283] [Dynamical System Optimization](https://arxiv.org/abs/2506.08340)
*Emo Todorov*

Main category: cs.LG

TL;DR: The paper proposes an optimization framework to directly optimize policy parameters, simplifying traditional Dynamic Programming and Reinforcement Learning processes.


<details>
  <summary>Details</summary>
Motivation: To provide a simpler and unified approach to policy optimization and address the complexity of traditional control methods like Dynamic Programming and Reinforcement Learning.

Method: The framework treats policies as autonomous systems and develops optimization algorithms that compute key quantities like policy gradients, Hessians, and natural gradients directly at the system level.

Result: The framework is applicable to various domains like behavioral cloning, mechanism design, system identification, and can better align with tuning generative AI models.

Conclusion: The proposed framework simplifies and broadens the application of policy optimization, offering a conceptually clearer alternative to Reinforcement Learning for policy tuning and system parameter adjustments.

Abstract: We develop an optimization framework centered around a core idea: once a
(parametric) policy is specified, control authority is transferred to the
policy, resulting in an autonomous dynamical system. Thus we should be able to
optimize policy parameters without further reference to controls or actions,
and without directly using the machinery of approximate Dynamic Programming and
Reinforcement Learning. Here we derive simpler algorithms at the autonomous
system level, and show that they compute the same quantities as policy
gradients and Hessians, natural gradients, proximal methods. Analogs to
approximate policy iteration and off-policy learning are also available. Since
policy parameters and other system parameters are treated uniformly, the same
algorithms apply to behavioral cloning, mechanism design, system
identification, learning of state estimators. Tuning of generative AI models is
not only possible, but is conceptually closer to the present framework than to
Reinforcement Learning.

</details>


### [284] [Differentially Private Relational Learning with Entity-level Privacy Guarantees](https://arxiv.org/abs/2506.08347)
*Yinan Huang,Haoteng Ying,Eli Chien,Rongzhe Wei,Pan Li*

Main category: cs.LG

TL;DR: The paper introduces a method to ensure differential privacy during relational learning, overcoming challenges in sensitivity and coupled sampling procedures, and demonstrating its utility in experiments.


<details>
  <summary>Details</summary>
Motivation: Protecting privacy in relational learning is critical as entities often appear in multiple relations, increasing privacy risks.

Method: The authors developed a DP-SGD variant with adaptive gradient clipping and extended privacy amplification to specific sampling scenarios.

Result: Rigorous sensitivity analysis and experiments showed effective privacy-utility trade-offs in relational learning.

Conclusion: The proposed framework offers formal differential privacy guarantees for entity-level relational learning, utilizing innovative techniques for improved results.

Abstract: Learning with relational and network-structured data is increasingly vital in
sensitive domains where protecting the privacy of individual entities is
paramount. Differential Privacy (DP) offers a principled approach for
quantifying privacy risks, with DP-SGD emerging as a standard mechanism for
private model training. However, directly applying DP-SGD to relational
learning is challenging due to two key factors: (i) entities often participate
in multiple relations, resulting in high and difficult-to-control sensitivity;
and (ii) relational learning typically involves multi-stage, potentially
coupled (interdependent) sampling procedures that make standard privacy
amplification analyses inapplicable. This work presents a principled framework
for relational learning with formal entity-level DP guarantees. We provide a
rigorous sensitivity analysis and introduce an adaptive gradient clipping
scheme that modulates clipping thresholds based on entity occurrence frequency.
We also extend the privacy amplification results to a tractable subclass of
coupled sampling, where the dependence arises only through sample sizes. These
contributions lead to a tailored DP-SGD variant for relational data with
provable privacy guarantees. Experiments on fine-tuning text encoders over
text-attributed network-structured relational data demonstrate the strong
utility-privacy trade-offs of our approach. Our code is available at
https://github.com/Graph-COM/Node_DP.

</details>


### [285] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: AdaAct is introduced as an optimization algorithm that adjusts learning rates based on activation variance, targeting better generalization and stability of neuron outputs.


<details>
  <summary>Details</summary>
Motivation: To address the need for better convergence speed and generalization capability in deep learning training, combining benefits of Adam and SGD while maintaining efficiency.

Method: AdaAct adjusts learning rates by incorporating neuron-wise adaptivity, leveraging activation variance for optimized learning during training.

Result: AdaAct shows competitive performance on image classification benchmarks like CIFAR and ImageNet, balancing Adam's convergence speed and SGD's generalization ability.

Conclusion: AdaAct offers an effective optimization approach, bridging the gap between Adam and SGD, with competitive execution times and improved generalization.

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [286] [NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation](https://arxiv.org/abs/2506.08360)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: NysAct balances the efficiency of first-order methods and the generalization ability of second-order methods by using an eigenvalue-shifted Nystrom method for gradient preconditioning.


<details>
  <summary>Details</summary>
Motivation: First-order methods are fast but generalize poorly, while second-order methods improve generalization but are computationally expensive. A balance between these two is needed.

Method: NysAct uses an eigenvalue-shifted Nystrom method to approximate the activation covariance matrix for gradient preconditioning, reducing the computational and memory cost of second-order methods.

Result: NysAct achieves better test accuracy compared to both first- and second-order methods while being computationally efficient, and its implementation reduces memory usage and time.

Conclusion: NysAct is an effective and scalable optimization approach that bridges the gap between first- and second-order methods, delivering enhanced generalization and performance at lower computational costs.

Abstract: Adaptive gradient methods are computationally efficient and converge quickly,
but they often suffer from poor generalization. In contrast, second-order
methods enhance convergence and generalization but typically incur high
computational and memory costs. In this work, we introduce NysAct, a scalable
first-order gradient preconditioning method that strikes a balance between
state-of-the-art first-order and second-order optimization methods. NysAct
leverages an eigenvalue-shifted Nystrom method to approximate the activation
covariance matrix, which is used as a preconditioning matrix, significantly
reducing time and memory complexities with minimal impact on test accuracy. Our
experiments show that NysAct not only achieves improved test accuracy compared
to both first-order and second-order methods but also demands considerably less
computational resources than existing second-order methods. Code is available
at https://github.com/hseung88/nysact.

</details>


### [287] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/abs/2506.08365)
*Cheng Tan,Zhenxiao Cao,Zhangyang Gao,Siyuan Li,Yufei Huang,Stan Z. Li*

Main category: cs.LG

TL;DR: The paper identifies a geometric bias in AFDB structures compared to PDB and introduces a Debiasing Structure AutoEncoder (DeSAE) to address this, improving results in tasks like inverse folding.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the realization that AFDB structures, despite their structural accuracy, exhibit geometric biases that hinder generalization in downstream tasks, such as inverse folding.

Method: The authors developed and trained the Debiasing Structure AutoEncoder (DeSAE) to reconstruct more realistic conformations from corrupted backbone geometries, aiming to create a natural structural manifold.

Result: Applying DeSAE transformed AFDB structures into debiased structures, significantly enhancing inverse folding performance across various benchmarks.

Conclusion: The study demonstrates the importance of addressing geometric biases in predicted protein structures and provides a systematic solution to improve structure-based learning tasks.

Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>


### [288] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Main category: cs.LG

TL;DR: This paper introduces DPSDP, a reinforcement learning method for refining LLM solutions using multi-turn feedback and demonstrates performance improvements.


<details>
  <summary>Details</summary>
Motivation: To improve the reasoning capabilities of LLMs by addressing limited feedback spaces and uncoordinated multi-party training in existing verify-and-improve methods.

Method: The authors model the multi-turn refinement process as a Markov Decision Process and propose DPSDP, a reinforcement learning framework with actor-critic training for preference learning on self-generated data.

Result: DPSDP, tested with various base models, enhanced performance on in- and out-of-distribution benchmarks, such as a 5% accuracy increase on MATH 500 after multiple refinement steps.

Conclusion: The study demonstrates the effectiveness of DPSDP for iterative answer refinement and highlights the benefits of multi-agent collaboration and improved generalization.

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [289] [Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest](https://arxiv.org/abs/2506.08383)
*Jiaqi Chen,Rongbin Ye*

Main category: cs.LG

TL;DR: The paper empirically analyzes machine learning techniques with imbalance treatment for IoT malware detection, finding gcForest performs best.


<details>
  <summary>Details</summary>
Motivation: The rapid growth in IoT networks creates a pressing need for real-time detection of malicious traffic to ensure cybersecurity.

Method: Used IoT-23 dataset along with three resampling strategies, implemented machine learning techniques including ensemble methods for empirical analysis.

Result: gcForest combined with imbalance treatment techniques achieved superior detection performance compared to traditional approaches.

Conclusion: The study contributes to advancements in automated IoT threat detection systems, providing both improved security and optimized computational efficiency.

Abstract: With the rapid expansion of Internet of Things (IoT) networks, detecting
malicious traffic in real-time has become a critical cybersecurity challenge.
This research addresses the detection challenges by presenting a comprehensive
empirical analysis of machine learning techniques for malware detection using
the IoT-23 dataset provided by the Stratosphere Laboratory. We address the
significant class imbalance within the dataset through three resampling
strategies. We implement and compare a few machine learning techniques. Our
findings demonstrate that the combination of appropriate imbalance treatment
techniques with ensemble methods, particularly gcForest, achieves better
detection performance compared to traditional approaches. This work contributes
significantly to the development of more intelligent and efficient automated
threat detection systems for IoT environments, helping to secure critical
infrastructure against sophisticated cyber attacks while optimizing
computational resource usage.

</details>


### [290] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Main category: cs.LG

TL;DR: The paper proposes a framework for training Reinforcement-Learned Teachers (RLTs) that improve reasoning language model training without reliance on initial task-solving ability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in reinforcement learning for reasoning LMs, particularly the reliance on exploration and the need for distillation and cold-starting frameworks.

Method: A new framework trains RLTs using dense reward signals by evaluating explanations against student comprehension and focuses on generating detailed, tailored solutions for downstream tasks.

Result: RLTs outperform traditional methods in training efficiency, performance on complex tasks, and adaptability to both larger students and out-of-distribution tasks.

Conclusion: Reinforcement-Learned Teachers provide an efficient and reusable solution for training reasoning LMs, reducing dependency on larger models and enhancing generalization in new domains.

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [291] [FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion](https://arxiv.org/abs/2506.08409)
*Fred Xu,Song Jiang,Zijie Huang,Xiao Luo,Shichang Zhang,Adrian Chen,Yizhou Sun*

Main category: cs.LG

TL;DR: The paper introduces Fuzzy Set Embedding (FUSE), a novel approach for set representation using fuzzy sets, achieving up to 23% improvement in taxonomy expansion tasks.


<details>
  <summary>Details</summary>
Motivation: Existing set representation methods fail to efficiently model uncertainty and semantic information while being limited in performing set operations.

Method: Proposed a Fuzzy Set Embedding (FUSE) framework based on volume approximation, satisfying set operations and requiring minimal neural architecture.

Result: FUSE significantly improves taxonomy expansion tasks, with performance gains of up to 23% over baselines.

Conclusion: FUSE represents a breakthrough in fuzzy set embedding, offering both computational efficiency and strong performance in modeling complex concepts for taxonomy expansion.

Abstract: Taxonomy Expansion, which models complex concepts and their relations, can be
formulated as a set representation learning task. The generalization of set,
fuzzy set, incorporates uncertainty and measures the information within a
semantic concept, making it suitable for concept modeling. Existing works
usually model sets as vectors or geometric objects such as boxes, which are not
closed under set operations. In this work, we propose a sound and efficient
formulation of set representation learning based on its volume approximation as
a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),
satisfies all set operations and compactly approximates the underlying fuzzy
set, hence preserving information while being efficient to learn, relying on
minimum neural architecture. We empirically demonstrate the power of FUSE on
the task of taxonomy expansion, where FUSE achieves remarkable improvements up
to 23% compared with existing baselines. Our work marks the first attempt to
understand and efficiently compute the embeddings of fuzzy sets.

</details>


### [292] [Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics](https://arxiv.org/abs/2506.08412)
*Saraa Ali,Aleksandr Khizhik,Stepan Svirin,Artem Ryzhikov,Denis Derkach*

Main category: cs.LG

TL;DR: The study introduces a framework combining machine learning with unsupervised anomaly generation to improve three-phase engine diagnostics.


<details>
  <summary>Details</summary>
Motivation: Improve the accuracy and reliability of diagnosing faults in three-phase engines using modern machine learning techniques.

Method: Signature-Guided Data Augmentation (SGDA) synthesizes engine anomalies based on physics models directly in the frequency domain, combined with Motor Current Signature Analysis.

Result: The proposed hybrid approach demonstrates enhanced diagnostic accuracy and reliability for engine fault detection.

Conclusion: This framework provides a scalable and accurate solution for diagnosing faults in three-phase engines, with potential widespread industrial application.

Abstract: The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining ML algorithms with a novel unsupervised anomaly generation
methodology that takes into account the engine physics model. We propose
Signature-Guided Data Augmentation (SGDA), an unsupervised framework that
synthesizes physically plausible faults directly in the frequency domain of
healthy current signals. Guided by Motor Current Signature Analysis, SGDA
creates diverse and realistic anomalies without resorting to computationally
intensive simulations. This hybrid approach leverages the strengths of both
supervised ML and unsupervised signature analysis, achieving superior
diagnostic accuracy and reliability along with wide industrial application. The
findings highlight the potential of our approach to contribute significantly to
the field of engine diagnostics, offering a robust and efficient solution for
real-world applications.

</details>


### [293] [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://arxiv.org/abs/2506.08417)
*Qingmao Yao,Zhichao Lei,Tianyuan Chen,Ziyue Yuan,Xuefan Chen,Jianxiang Liu,Faguo Wu,Xiao Zhang*

Main category: cs.LG

TL;DR: The paper proposes a novel Smooth Q-function OOD Generalization (SQOG) method that improves offline reinforcement learning by addressing $Q$-value estimation challenges in out-of-distribution regions while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the problem of $Q$-value overestimation due to distributional shifts in offline reinforcement learning, as existing methods impose overly conservative constraints that hinder $Q$-function generalization and policy improvement.

Method: The paper introduces the Smooth Bellman Operator (SBO), which smooths $Q$-values of out-of-distribution actions by leveraging neighboring in-sample $Q$-values within the Convex Hull and its Neighborhood (CHN).

Result: The proposed SQOG algorithm achieves near-accurate $Q$-value estimates, resolves over-constraint issues, and outperforms state-of-the-art methods on the D4RL benchmarks in both performance and computational efficiency.

Conclusion: The research enhances $Q$-function generalization in offline RL through SBO, showing that SQOG is effective, efficient, and capable of addressing existing challenges in $Q$-value estimation and policy improvement.

Abstract: Offline Reinforcement Learning (RL) struggles with distributional shifts,
leading to the $Q$-value overestimation for out-of-distribution (OOD) actions.
Existing methods address this issue by imposing constraints; however, they
often become overly conservative when evaluating OOD regions, which constrains
the $Q$-function generalization. This over-constraint issue results in poor
$Q$-value estimation and hinders policy improvement. In this paper, we
introduce a novel approach to achieve better $Q$-value estimation by enhancing
$Q$-function generalization in OOD regions within Convex Hull and its
Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we
propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by
smoothing them with neighboring in-sample $Q$-values. We theoretically show
that SBO approximates true $Q$-values for both in-sample and OOD actions within
the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),
empirically alleviates the over-constraint issue, achieving near-accurate
$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing
state-of-the-art methods in both performance and computational efficiency.

</details>


### [294] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Main category: cs.LG

TL;DR: This paper presents "FedLeak," a novel approach to reconstruct client data in Federated Learning systems under realistic conditions, challenging previous claims about the safety of FL in practical environments.


<details>
  <summary>Details</summary>
Motivation: To address the ongoing debate about privacy vulnerabilities in Federated Learning and to demonstrate that gradient leakage attacks can effectively compromise clients' data even in practical settings.

Method: The authors identify shortcomings in previous gradient leakage attacks (GLAs) and propose a new approach called "FedLeak," which incorporates partial gradient matching and gradient regularization techniques. They also create an evaluation protocol to assess FedLeak's performance in real-world FL environments.

Result: FedLeak successfully achieves high-fidelity data reconstruction within realistic Federated Learning environments, showcasing the privacy risks associated with FL systems.

Conclusion: The study highlights a significant privacy vulnerability in FL systems, urging the development and deployment of stronger defense mechanisms to safeguard client data.

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [295] [Time-Aware World Model for Adaptive Prediction and Control](https://arxiv.org/abs/2506.08441)
*Anh N. Nhu,Sanghyun Son,Ming Lin*

Main category: cs.LG

TL;DR: This paper presents the Time-Aware World Model (TAWM), a model-based approach that leverages variable time-step sizes for control tasks, delivering improved performance and data efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in conventional models that use fixed time-step sizes, which may not optimally capture system dynamics in diverse control problems.

Method: TAWM incorporates temporal dynamics by conditioning on variable time-step sizes ({\Delta}t) during training, allowing the model to learn from diverse sampling rates and adapt to both high- and low-frequency task dynamics.

Result: TAWM demonstrated superior performance and data efficiency across various observation rates in multiple control tasks, using the same training samples and iterations as conventional models.

Conclusion: TAWM's time-aware formulation is a significant improvement over fixed time-step models, offering more robust and efficient performance across diverse control scenarios.

Abstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based
approach that explicitly incorporates temporal dynamics. By conditioning on the
time-step size, {\Delta}t, and training over a diverse range of {\Delta}t
values -- rather than sampling at a fixed time-step -- TAWM learns both high-
and low-frequency task dynamics across diverse control problems. Grounded in
the information-theoretic insight that the optimal sampling rate depends on a
system's underlying dynamics, this time-aware formulation improves both
performance and data efficiency. Empirical evaluations show that TAWM
consistently outperforms conventional models across varying observation rates
in a variety of control tasks, using the same number of training samples and
iterations. Our code can be found online at:
github.com/anh-nn01/Time-Aware-World-Model.

</details>


### [296] [MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature](https://arxiv.org/abs/2506.08464)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: The paper introduces MAC, a computationally efficient second-order optimization method for neural networks that outperforms KFAC.


<details>
  <summary>Details</summary>
Motivation: Current second-order optimization methods like KFAC improve convergence using curvature information but are computationally expensive.

Method: The MAC algorithm proposes efficient approximations for the components of Fisher Information Matrix (FIM), applies Kronecker factorization to attention layers, and integrates attention scores into preconditioning.

Result: MAC demonstrates superior performance over KFAC and other state-of-the-art methods across various architectures and datasets, achieving better accuracy, faster training, and reduced memory usage.

Conclusion: MAC provides an efficient alternative to high-cost optimization methods, extending applicability to attention layers in transformers while ensuring strong convergence properties.

Abstract: Second-order optimization methods for training neural networks, such as KFAC,
exhibit superior convergence by utilizing curvature information of loss
landscape. However, it comes at the expense of high computational burden. In
this work, we analyze the two components that constitute the layer-wise Fisher
information matrix (FIM) used in KFAC: the Kronecker factors related to
activations and pre-activation gradients. Based on empirical observations on
their eigenspectra, we propose efficient approximations for them, resulting in
a computationally efficient optimization method called MAC. To the best of our
knowledge, MAC is the first algorithm to apply the Kronecker factorization to
the FIM of attention layers used in transformers and explicitly integrate
attention scores into the preconditioning. We also study the convergence
property of MAC on nonlinear neural networks and provide two conditions under
which it converges to global minima. Our extensive evaluations on various
network architectures and datasets show that the proposed method outperforms
KFAC and other state-of-the-art methods in terms of accuracy, end-to-end
training time, and memory usage. Code is available at
https://github.com/hseung88/mac.

</details>


### [297] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/abs/2506.08473)
*Shuo Yang,Qihui Zhang,Yuyang Liu,Yue Huang,Xiaojun Jia,Kunpeng Ning,Jiayu Yao,Jigang Wang,Hailiang Dai,Yibing Song,Li Yuan*

Main category: cs.LG

TL;DR: LLMs can face safety risks during fine-tuning, but using alignment direction as an anchor can mitigate risks and improve performance.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by concerns with the vulnerability of LLMs to safety risks during fine-tuning.

Method: The authors introduce AsFT, a safety fine-tuning methodology that uses alignment direction as a regularization anchor in the training objective to suppress harmful updates.

Result: Experiments show that AsFT reduces harmful behavior by 7.60%, improves model performance by 3.44%, and maintains robust performance across diverse settings.

Conclusion: The alignment direction-based anchoring in AsFT provides significant advancements in safety and performance for fine-tuned LLMs.

Abstract: Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [298] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/abs/2506.08475)
*Xiaolong He,Yeonjong Shin,Anthony Gruber,Sohyeon Jung,Kookjin Lee,Youngsoo Choi*

Main category: cs.LG

TL;DR: The paper introduces an efficient method for modeling nonlinear systems by combining dimensionality reduction, thermodynamics principles, and adaptive data sampling.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the high computational cost in modeling parametric nonlinear dynamical systems while ensuring thermodynamic principles are preserved.

Method: The framework combines autoencoders with a new parametric GENERIC formalism-informed neural network (pGFINN) for latent dynamics. It incorporates a physics-informed active learning strategy to optimize training data selection.

Result: The approach achieves up to 3,528x speed-up with minimal errors (1-3%), and substantial reductions in training (50-90%) and inference (57-61%) costs when tested on benchmark equations.

Conclusion: The framework improves computational efficiency while preserving key physical properties, offering deeper insights into the system dynamics.

Abstract: We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [299] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/abs/2506.08505)
*Shahaf Bassan,Yizhak Yisrael Elboher,Tobias Ladner,Matthias Althoff,Guy Katz*

Main category: cs.LG

TL;DR: The paper introduces a novel abstraction-refinement method to efficiently compute provably sufficient explanations for neural network predictions, addressing scalability issues in existing formal explainability approaches.


<details>
  <summary>Details</summary>
Motivation: Current explainability methods for neural networks often rely on heuristics and lack formal guarantees, with significant scalability issues in computing explanations with formal guarantees.

Method: The authors propose an abstraction-refinement technique. They build a reduced neural network to simplify computation. If explanations are insufficient, the network is iteratively refined by incrementally increasing its size until convergence.

Result: Experiments show that the proposed method improves the efficiency of generating provably sufficient explanations and provides detailed interpretations at different levels of abstraction.

Conclusion: The work presents an efficient and structured approach to produce formally guaranteed explanations of neural network predictions, addressing scalability and interpretability challenges.

Abstract: Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [300] [DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training](https://arxiv.org/abs/2506.08514)
*Jacob Piland,Chris Sweet,Adam Czakja*

Main category: cs.LG

TL;DR: The paper discusses vulnerabilities in Class Activation Maps (CAMs) and introduces a new adversarial benchmark (SHAMs) and a robust mapping approach (DiffGradCAM) to address these issues.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of existing CAM methods, such as susceptibility to adversarial manipulations like passive fooling, which compromise the robustness of saliency-based explanations.

Method: The authors propose SHAMs to benchmark the robustness of CAM methods under adversarial conditions and develop DiffGradCAM, a novel contrastive activation mapping approach that is resistant to passive fooling while retaining performance in non-adversarial scenarios.

Result: The introduced DiffGradCAM approach is shown to be resilient to passive fooling and achieves comparable performance to conventional CAM methods in non-adversarial conditions, validated across various multi-class tasks.

Conclusion: The study identifies limitations in standard CAM techniques, proposes a new benchmark for robustness, and offers an enhanced method (DiffGradCAM) that improves the reliability of saliency-based explanations in adversarial and normal scenarios.

Abstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g.,
GradCAM) have become standard tools for explaining Convolutional Neural Network
(CNN) predictions. However, these approaches typically focus on individual
logits, while for neural networks using softmax, the class membership
probability estimates depend \textit{only} on the \textit{differences} between
logits, not on their absolute values. This disconnect leaves standard CAMs
vulnerable to adversarial manipulation, such as passive fooling, where a model
is trained to produce misleading CAMs without affecting decision performance.
We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an
\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM
robustness under adversarial conditions. To address the passive fooling
vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and
contrastive approach to class activation mapping that is both non-suceptible to
passive fooling, but also matches the output of standard CAM methods such as
GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a
new framework for probing and improving the robustness of saliency-based
explanations. We validate both contributions across multi-class tasks with few
and many classes.

</details>


### [301] [NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis](https://arxiv.org/abs/2506.08516)
*Mouadh Yagoubi,David Danan,Milad Leyli-Abadi,Ahmed Mazari,Jean-Patrick Brunet,Abbas Kabalan,Fabien Casenave,Yuxin Ma,Giovanni Catalani,Jean Fesquet,Jacob Helwig,Xuan Zhang,Haiyang Yu,Xavier Bertrand,Frederic Tost,Michael Baurheim,Joseph Morlier,Shuiwang Ji*

Main category: cs.LG

TL;DR: The ML4CFD competition explored ML surrogates for aerodynamic simulations, demonstrating their promise against traditional methods under specific benchmarks.


<details>
  <summary>Details</summary>
Motivation: The persistent challenges of accuracy, generalization, and physical consistency hinder machine learning’s full applicability to scientific simulation tasks like CFD.

Method: A competition was organized with over 240 teams using OpenFOAM datasets, evaluated on predictive accuracy, physical reliability, computational speed, and ability to generalize to new conditions.

Result: The competition highlighted several ML-based approaches that surpassed baseline models, including one solution that outperformed OpenFOAM on aggregate metrics.

Conclusion: Machine learning surrogates have strong potential to advance physical simulations under tailored frameworks, and future competitions can enhance scientific ML development through systemic benchmarks.

Abstract: The integration of machine learning (ML) into the physical sciences is
reshaping computational paradigms, offering the potential to accelerate
demanding simulations such as computational fluid dynamics (CFD). Yet,
persistent challenges in accuracy, generalization, and physical consistency
hinder the practical deployment of ML models in scientific domains. To address
these limitations and systematically benchmark progress, we organized the
ML4CFD competition, centered on surrogate modeling for aerodynamic simulations
over two-dimensional airfoils. The competition attracted over 240 teams, who
were provided with a curated dataset generated via OpenFOAM and evaluated
through a multi-criteria framework encompassing predictive accuracy, physical
fidelity, computational efficiency, and out-of-distribution generalization.
This retrospective analysis reviews the competition outcomes, highlighting
several approaches that outperformed baselines under our global evaluation
score. Notably, the top entry exceeded the performance of the original OpenFOAM
solver on aggregate metrics, illustrating the promise of ML-based surrogates to
outperform traditional solvers under tailored criteria. Drawing from these
results, we analyze the key design principles of top submissions, assess the
robustness of our evaluation framework, and offer guidance for future
scientific ML challenges.

</details>


### [302] [Leveraging chaos in the training of artificial neural networks](https://arxiv.org/abs/2506.08523)
*Pedro Jiménez-González,Miguel C. Soriano,Lucas Lacasa*

Main category: cs.LG

TL;DR: The paper studies training artificial neural networks at very high learning rates, discovering a regime of chaos that speeds up learning while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Explore the role of unconventional, high learning rates in neural network training and identify benefits of chaotic dynamics.

Method: Investigate the trajectory of neural network training with varying learning rates, analyze dynamics via Lyapunov exponent, and experiment across tasks and architectures.

Result: Training at the onset of chaos minimizes required training time while maintaining effective learning and accuracy.

Conclusion: Transient chaotic dynamics can constructively accelerate neural network training by balancing exploration and exploitation.

Abstract: Traditional algorithms to optimize artificial neural networks when confronted
with a supervised learning task are usually exploitation-type relaxational
dynamics such as gradient descent (GD). Here, we explore the dynamics of the
neural network trajectory along training for unconventionally large learning
rates. We show that for a region of values of the learning rate, the GD
optimization shifts away from purely exploitation-like algorithm into a regime
of exploration-exploitation balance, as the neural network is still capable of
learning but the trajectory shows sensitive dependence on initial conditions --
as characterized by positive network maximum Lyapunov exponent --.
Interestingly, the characteristic training time required to reach an acceptable
accuracy in the test set reaches a minimum precisely in such learning rate
region, further suggesting that one can accelerate the training of artificial
neural networks by locating at the onset of chaos. Our results -- initially
illustrated for the MNIST classification task -- qualitatively hold for a range
of supervised learning tasks, learning architectures and other hyperparameters,
and showcase the emergent, constructive role of transient chaotic dynamics in
the training of artificial neural networks.

</details>


### [303] [Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)](https://arxiv.org/abs/2506.08533)
*Nihal Acharya Adde,Alexandra Gianzina,Hanno Gottschalk,Andreas Ebert*

Main category: cs.LG

TL;DR: The paper introduces EMNAS, a genetic algorithm-based approach to optimize neural network architectures in RL for Autonomous Driving. It outperforms manual models in both performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in optimizing neural network architectures for RL in Autonomous Driving, focusing on achieving better rewards and reducing model size without performance compromise.

Method: EMNAS employs genetic algorithms for automated network design. It incorporates parallelization for faster search and uses teacher-student methodologies for scalable optimization. Transfer learning is leveraged to improve iterative learning efficiency.

Result: Experimental results show EMNAS achieves higher rewards compared to manually designed models, while using fewer parameters.

Conclusion: EMNAS proves effective for optimizing RL networks in Autonomous Driving, advancing the field towards efficient and scalable solutions for real-world scenarios.

Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture
Search (EMNAS) for the first time to optimize neural network architectures in
large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses
genetic algorithms to automate network design, tailored to enhance rewards and
reduce model size without compromising performance. Additionally,
parallelization techniques are employed to accelerate the search, and
teacher-student methodologies are implemented to ensure scalable optimization.
This research underscores the potential of transfer learning as a robust
framework for optimizing performance across iterative learning processes by
effectively leveraging knowledge from earlier generations to enhance learning
efficiency and stability in subsequent generations. Experimental results
demonstrate that tailored EMNAS outperforms manually designed models, achieving
higher rewards with fewer parameters. The findings of these strategies
contribute positively to EMNAS for RL in autonomous driving, advancing the
field toward better-performing networks suitable for real-world scenarios.

</details>


### [304] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/abs/2506.08551)
*Panlong Wu,Ting Wang,Yifei Zhong,Haoqi Zhang,Zitong Wang,Fangxin Wang*

Main category: cs.LG

TL;DR: DeepForm introduces a reasoning-focused LLM for automated 6G communication system design, leveraging a new dataset and advanced training techniques.


<details>
  <summary>Details</summary>
Motivation: Formulating communication systems for 6G is intricate and requires specialized expertise, which generic LLMs currently lack.

Method: DeepForm uses a two-stage training approach: supervised fine-tuning with Chain-of-Thought data and a novel rule-based RL algorithm named C-ReMax.

Result: DeepForm achieves state-of-the-art performance, exceeding proprietary LLMs in diverse communication system scenarios.

Conclusion: DeepForm represents a significant advancement in automating communication system design and promises to accelerate innovation by releasing new resources.

Abstract: Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [305] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/abs/2506.08574)
*Alvise Dei Rossi,Matteo Metaldi,Michal Bechny,Irina Filchenko,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Athina Tzovara,Francesca D. Faraci,Luigi Fiorillo*

Main category: cs.LG

TL;DR: The paper introduces SLEEPYLAND, an open-source sleep staging framework, and evaluates SOMNUS, an ensemble model, which achieves state-of-the-art performance and robust generalization across datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in fair model evaluation, dataset generalization, model bias, and variability in human annotations in automatic sleep staging.

Method: Developed the SLEEPYLAND framework with both in-domain and out-of-domain sleep recordings, provided pre-trained models, introduced SOMNUS for ensemble modeling, and included comprehensive evaluations.

Result: SOMNUS outperformed state-of-the-art methods with robust performance across datasets, reducing bias, and even surpassing human scorers in certain evaluations.

Conclusion: SLEEPYLAND and SOMNUS advance fairness and performance in sleep staging, offering a benchmark tool and insights into the limitations of biases in clinical models.

Abstract: Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 22'0000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [306] [Diffusion-based Time Series Forecasting for Sewerage Systems](https://arxiv.org/abs/2506.08577)
*Nicholas A. Pearson,Francesca Cairoli,Luca Bortolussi,Davide Russo,Francesca Zanello*

Main category: cs.LG

TL;DR: A novel generative AI-based deep learning model is created for improving sewerage system forecasting using diffusion models and probabilistic conformal inference.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and reliability of contextual forecasting in sewerage systems, particularly under extreme weather conditions.

Method: Developing a diffusion-based deep learning model that processes multivariate time series data and integrating conformal inference techniques for accurate probabilistic prediction intervals.

Result: Empirical tests on real-world sewerage data validate the model's ability to generate reliable forecasts even under adverse weather scenarios.

Conclusion: The proposed approach successfully predicts sewerage system behavior with high accuracy and statistical reliability, suitable for extreme conditions.

Abstract: We introduce a novel deep learning approach that harnesses the power of
generative artificial intelligence to enhance the accuracy of contextual
forecasting in sewerage systems. By developing a diffusion-based model that
processes multivariate time series data, our system excels at capturing complex
correlations across diverse environmental signals, enabling robust predictions
even during extreme weather events. To strengthen the model's reliability, we
further calibrate its predictions with a conformal inference technique,
tailored for probabilistic time series data, ensuring that the resulting
prediction intervals are statistically reliable and cover the true target
values with a desired confidence level. Our empirical tests on real sewerage
system data confirm the model's exceptional capability to deliver reliable
contextual predictions, maintaining accuracy even under severe weather
conditions.

</details>


### [307] [CALT: A Library for Computer Algebra with Transformer](https://arxiv.org/abs/2506.08600)
*Hiroshi Kera,Shun Arakawa,Yuta Sato*

Main category: cs.LG

TL;DR: A Python library called CALT is introduced to facilitate training Transformer models for symbolic computation tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between symbolic computation and deep learning by making it accessible to non-experts through a user-friendly tool.

Method: CALT leverages Transformer models and provides a Python interface for training on symbolic computation tasks using example input-output pairs.

Result: The introduction of CALT simplifies the process for researchers and developers interested in symbolic computations with deep learning.

Conclusion: CALT serves as a bridge for the symbolic computation community to contribute to and benefit from advancements in AI-driven computations.

Abstract: Recent advances in artificial intelligence have demonstrated the learnability
of symbolic computation through end-to-end deep learning. Given a sufficient
number of examples of symbolic expressions before and after the target
computation, Transformer models - highly effective learners of
sequence-to-sequence functions - can be trained to emulate the computation.
This development opens up several intriguing challenges and new research
directions, which require active contributions from the symbolic computation
community. In this work, we introduce Computer Algebra with Transformer (CALT),
a user-friendly Python library designed to help non-experts in deep learning
train models for symbolic computation tasks.

</details>


### [308] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/abs/2506.08604)
*Giacomo Baldan,Qiang Liu,Alberto Guardone,Nils Thuerey*

Main category: cs.LG

TL;DR: The paper introduces Physics-Based Flow Matching (PBFM), a generative machine learning method embedding explicit physical constraints into flow matching, achieving higher accuracy in solving PDE problems.


<details>
  <summary>Details</summary>
Motivation: Current generative models like diffusion and flow matching lack explicit integration of physical constraints when modeling complex systems.

Method: PBFM enhances flow matching by embedding physical constraints such as PDE and algebraic relations into the objective function, utilizing temporal unrolling and requiring no hyperparameter tuning for loss weighting. It also incorporates a stochastic sampling strategy to reduce physical residuals.

Result: PBFM is up to 8x more accurate in reducing physical residuals compared to standard flow matching (FM) and surpasses existing methods in distributional accuracy across representative PDE benchmarks.

Conclusion: PBFM is a robust framework for surrogate modeling, uncertainty quantification, and accelerated simulation, offering principled enhancements in physics and engineering domains.

Abstract: Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


### [309] [Sample Efficient Demonstration Selection for In-Context Learning](https://arxiv.org/abs/2506.08607)
*Kiran Purohit,V Venktesh,Sourangshu Bhattacharya,Avishek Anand*

Main category: cs.LG

TL;DR: This paper introduces CASE, a method for efficient exemplar selection in in-context learning with LLMs, achieving significant speedup and fewer evaluations.


<details>
  <summary>Details</summary>
Motivation: The exemplar selection process in in-context learning is critical for effective prompts, but current methods are computationally expensive due to many evaluations.

Method: The proposed method, CASE, uses a selective exploration strategy with a shortlist of 'challenger' arms and models scores using a parameterized linear scoring function.

Result: CASE achieves up to 7x speedup, 87% fewer LLM calls, and maintains performance comparable to state-of-the-art methods.

Conclusion: CASE offers a highly efficient solution for exemplar selection, making it practical for large-scale LLM use.

Abstract: The in-context learning paradigm with LLMs has been instrumental in advancing
a wide range of natural language processing tasks. The selection of few-shot
examples (exemplars / demonstration samples) is essential for constructing
effective prompts under context-length budget constraints. In this paper, we
formulate the exemplar selection task as a top-m best arms identification
problem. A key challenge in this setup is the exponentially large number of
arms that need to be evaluated to identify the m-best arms. We propose CASE
(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient
selective exploration strategy that maintains a shortlist of "challenger" arms,
which are current candidates for the top-m arms. In each iteration, only one of
the arms from this shortlist or the current topm set is pulled, thereby
reducing sample complexity and, consequently, the number of LLM evaluations.
Furthermore, we model the scores of exemplar subsets (arms) using a
parameterized linear scoring function, leading to stochastic linear bandits
setting. CASE achieves remarkable efficiency gains of up to 7x speedup in
runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing
performance compared to state-of-the-art exemplar selection methods. We release
our code and data at https://github.com/kiranpurohit/CASE

</details>


### [310] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akgün,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Main category: cs.LG

TL;DR: The paper introduces HSG-12M, a large-scale dataset of spatial multigraphs maintaining distinct trajectories as separate edges, derived from crystal energy spectrum data. It also proposes an open-source pipeline, Poly2Graph, and explores challenges in graph learning.


<details>
  <summary>Details</summary>
Motivation: Existing graph benchmarks simplify spatial paths into single links, overlooking complex geometries. The authors aim to address this gap by creating a dataset that retains geometrically distinct paths.

Method: The authors developed HSG-12M containing millions of unique static and dynamic spatial multigraphs generated from crystal Hamiltonians. They introduced Poly2Graph for efficient mapping and tested graph neural networks to identify learning challenges.

Result: HSG-12M offers unprecedented geometry-aware graph representations with diverse physics-grounded topologies, revealing new challenges for graph learning on multigraphs.

Conclusion: HSG-12M paves the way for advanced graph-based learning approaches, linking algebraic structures to graph theory, and enabling scientific discovery in condensed matter physics and other fields.

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [311] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Main category: cs.LG

TL;DR: The paper introduces Time Vision Transformer (TiViT), which converts time series data into images for classification using pretrained Vision Transformers (ViTs).


<details>
  <summary>Details</summary>
Motivation: Time series classification faces challenges due to the limited availability of time series datasets, prompting novel approaches for leveraging existing resources like pretrained ViTs.

Method: The authors propose transforming time series data into image representations to utilize frozen ViTs pretrained on large-scale image datasets and incorporate theoretical and empirical analyses to validate their approach.

Result: TiViT achieves state-of-the-art results on time series classification benchmarks and displays complementarity when combined with traditional time series foundation models.

Conclusion: The findings highlight the potential of reusing vision-based representations for non-visual tasks, offering new pathways for time series data analysis.

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


### [312] [Semi-gradient DICE for Offline Constrained Reinforcement Learning](https://arxiv.org/abs/2506.08644)
*Woosung Kim,JunHo Seo,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: This paper addresses challenges with DICE-based offline constrained RL by identifying issues with semi-gradient optimization and proposes a novel solution enabling accurate cost estimation and high performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the reliability of DICE-based offline constrained RL, which struggles with accurate off-policy evaluation and constraints due to issues with existing semi-gradient optimization methods.

Method: The authors analyze the limitations of semi-gradient optimization for DICE, identify the root problems, and propose a new method combining semi-gradient DICE with accurate off-policy evaluation capabilities.

Result: The proposed method enables precise cost estimation and achieves state-of-the-art results on the DSRL offline constrained RL benchmark.

Conclusion: By addressing fundamental flaws in semi-gradient optimization, the proposed method improves both off-policy evaluation and constrained RL outcomes, offering a robust framework for offline RL.

Abstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch
between the stationary distribution induced by a policy and the target
distribution required for reliable off-policy evaluation (OPE) and policy
optimization. DICE-based offline constrained RL particularly benefits from the
flexibility of DICE, as it simultaneously maximizes return while estimating
costs in offline settings. However, we have observed that recent approaches
designed to enhance the offline RL performance of the DICE framework
inadvertently undermine its ability to perform OPE, making them unsuitable for
constrained RL scenarios. In this paper, we identify the root cause of this
limitation: their reliance on a semi-gradient optimization, which solves a
fundamentally different optimization problem and results in failures in cost
estimation. Building on these insights, we propose a novel method to enable OPE
and constrained RL through semi-gradient DICE. Our method ensures accurate cost
estimation and achieves state-of-the-art performance on the offline constrained
RL benchmark, DSRL.

</details>


### [313] [Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach](https://arxiv.org/abs/2506.08645)
*Youqi Wu,Jingwei Zhang,Farzan Farnia*

Main category: cs.LG

TL;DR: This paper proposes RP-KrossFuse, a fusion method for cross-modal and single-modal embeddings that combines their strengths and addresses performance gaps in specific tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the performance gap between cross-modal embeddings, which excel at aligning representations across modalities but often underperform on specific tasks, and single-modality embeddings, which excel within their domains but lack cross-modal capabilities.

Method: RP-KrossFuse is introduced as a method leveraging random projection-based Kronecker products, ensuring efficient computation in specific kernel spaces and scalability through random Fourier features.

Result: Through numerical experiments, RP-KrossFuse successfully combines cross-modal embeddings like CLIP with uni-modal image and text embeddings, achieving competitive single-modal performance while preserving cross-modal alignment.

Conclusion: RP-KrossFuse bridges the gap between cross-modal and single-modality embeddings, making it a promising method for tasks requiring cross-modal alignment without compromising single-modal expertise.

Abstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved
promising results in aligning representations across modalities. However, these
embeddings could underperform compared to state-of-the-art single-modality
embeddings on modality-specific tasks. On the other hand, single-modality
embeddings excel in their domains but lack cross-modal alignment capabilities.
In this work, we focus on the problem of unifying cross-modality and
single-modality embeddings to achieve the performance of modality-expert
embedding within individual modalities while preserving cross-modal alignment.
To this end, we propose RP-KrossFuse, a method that leverages a random
projection-based Kronecker product to integrate cross-modal embeddings with
single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise
similarity scores of the fused embeddings and operates efficiently in a
specified kernel space and supports scalable implementations via random Fourier
features for shift-invariant kernels such as the Gaussian kernel. We
demonstrate the effectiveness of RP-KrossFuse through several numerical
experiments, combining CLIP embeddings with uni-modal image and text
embeddings. Our numerical results indicate that RP-KrossFuse achieves
competitive modality-specific performance while retaining cross-modal
alignment, bridging the gap between cross-modal and single-modality embeddings.

</details>


### [314] [JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset](https://arxiv.org/abs/2506.08652)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: The paper introduces JoFormer, a Transformer model with a new way of integrating positional information using journey-based transformations. It outperforms a baseline method in language modeling tasks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in effectively integrating positional information into Transformer architectures.

Method: Developed JoFormer, which uses learnable directional transforms for representing relative positions and derives its attention mechanism from first principles.

Result: JoFormer achieves lower perplexity and faster convergence compared to the RoFormer baseline in language modeling tasks.

Conclusion: JoFormer showcases a promising direction for incorporating positional structure into Transformers, highlighting its potential advantages over existing methods.

Abstract: Transformers have demonstrated remarkable success in sequence modeling, yet
effectively incorporating positional information remains a challenging and
active area of research. In this paper, we introduce JoFormer, a journey-based
Transformer architecture grounded in a recently proposed non-commutative
algebra for composing transformations across positions. JoFormer represents
relative positions through learnable directional transforms that are
sequentially composed along the input, thereby extending and generalizing
existing approaches based on relative position representations. We derive the
JoFormer attention mechanism from first principles and show that it subsumes
standard methods such as rotary transformations as special cases. To evaluate
its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny
Shakespeare character-level language modeling task. Our results demonstrate
that
  JoFormer consistently achieves lower perplexity and faster convergence,
highlighting the advantages of its more expressive, journey-based treatment of
position. Notably, the per-token JoFormer is still a primitive, conceptual
variant with layer-independent angles, yet it already demonstrates strong
performance-underscoring its promise as a proof of concept for more expressive
architectures. We conclude by discussing how JoFormer offers a principled
approach to integrating positional structure into Transformer architectures.
The code used in this work is available at
https://github.com/mahesh-godavarti/joformer.

</details>


### [315] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/abs/2506.08655)
*Kamil Jerabek,Jan Luxemburk,Richard Plny,Josef Koumar,Jaroslav Pesek,Karel Hynek*

Main category: cs.LG

TL;DR: Machine learning for network traffic classification often demonstrates high accuracy, but this may be misleading due to dataset redundancy. A simple k-NN model based on packet metadata outperforms complex models on tasks tested across numerous datasets.


<details>
  <summary>Details</summary>
Motivation: To examine the surprising efficacy of a simple k-NN model in comparison to complex neural networks for network traffic classification and investigate issues such as dataset redundancy.

Method: The paper evaluates the k-NN model's performance across 12 datasets and 15 network traffic classification tasks. It analyzes dataset redundancy and contrasts machine learning practices in TC with those from other fields.

Result: Over 50% of samples in many datasets are redundant, inflating performance results due to common splitting practices. Complex models do not significantly outperform the simple k-NN model using packet metadata.

Conclusion: Standard machine learning practices may be unsuitable for network traffic classification. Addressing dataset redundancy and adopting new evaluation frameworks can help improve the field's alignment and task formulation.

Abstract: Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [316] [Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness](https://arxiv.org/abs/2506.08660)
*Jinkwan Jang,Hyungjin Park,Jinmyeong Choi,Taesup Kim*

Main category: cs.LG

TL;DR: ChannelTokenFormer is a Transformer-based model designed to address challenges in multivariate time series forecasting, such as channel dependency, asynchronous sampling, and missing values.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in existing forecasting models that assume synchronized sampling and complete data availability, which are unrealistic in many real-world scenarios.

Method: The researchers developed ChannelTokenFormer, a flexible Transformer model that captures cross-channel dependencies, accommodates asynchronous sampling, and handles missing values effectively.

Result: The model outperformed others in robustness and accuracy across three benchmark datasets and a real-world industrial dataset under practical, challenging conditions.

Conclusion: ChannelTokenFormer provides a significant advancement in robust and reliable forecasting for real-world multivariate time series with complex dependencies and irregularities.

Abstract: Real-world time series data are inherently multivariate, often exhibiting
complex inter-channel dependencies. Each channel is typically sampled at its
own period and is prone to missing values due to various practical and
operational constraints. These characteristics pose fundamental challenges
related to channel dependency, sampling asynchrony, and missingness, all of
which must be addressed to enable robust and reliable forecasting in practical
settings. However, most existing architectures are built on oversimplified
assumptions, such as identical sampling periods across channels and fully
observed inputs at test time, which often do not hold in real-world scenarios.
To bridge this gap, we propose ChannelTokenFormer, a Transformer-based
forecasting model with a flexible architecture designed to explicitly capture
cross-channel interactions, accommodate channel-wise asynchronous sampling, and
effectively handle missing values. Extensive experiments on three benchmark
datasets modified to reflect practical settings, along with one real-world
industrial dataset, demonstrate the superior robustness and accuracy of
ChannelTokenFormer under challenging real-world conditions.

</details>


### [317] [Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization](https://arxiv.org/abs/2506.08662)
*Florian Borzechowski,Michael Schäfer,Heiko Schwarz,Jonathan Pfaff,Detlev Marpe,Thomas Wiegand*

Main category: cs.LG

TL;DR: This paper addresses the issue of suboptimal methods for approximating quantization noise in image compression with variational autoencoders and introduces an additional fine-tuning step for enhanced performance.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to improve rate-distortion efficiency in image compression by addressing the flaws in modeling quantization noise during training, a key problem in learned image codecs.

Method: They propose a retraining step on quantized latents obtained during inference, after the conventional end-to-end training process. This approach especially benefits entropy-constraint quantization methods.

Result: After applying their fine-tuning method, the authors achieved coding gains of 1-2% bitrate savings on the Kodak test set and up to 2.2% savings on the TecNick test set.

Conclusion: Retraining on correctly quantized data enhances codec performance, providing consistent coding gains without increasing inference complexity, especially for entropy-constraint quantization methods.

Abstract: The continuous improvements on image compression with variational
autoencoders have lead to learned codecs competitive with conventional
approaches in terms of rate-distortion efficiency. Nonetheless, taking the
quantization into account during the training process remains a problem, since
it produces zero derivatives almost everywhere and needs to be replaced with a
differentiable approximation which allows end-to-end optimization. Though there
are different methods for approximating the quantization, none of them model
the quantization noise correctly and thus, result in suboptimal networks.
Hence, we propose an additional finetuning training step: After conventional
end-to-end training, parts of the network are retrained on quantized latents
obtained at the inference stage. For entropy-constraint quantizers like
Trellis-Coded Quantization, the impact of the quantizer is particularly
difficult to approximate by rounding or adding noise as the quantized latents
are interdependently chosen through a trellis search based on both the entropy
model and a distortion measure. We show that retraining on correctly quantized
data consistently yields additional coding gain for both uniform scalar and
especially for entropy-constraint quantization, without increasing inference
complexity. For the Kodak test set, we obtain average savings between 1% and
2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta
bitrate.

</details>


### [318] [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/abs/2506.08669)
*Dongge Han,Menglin Xia,Daniel Madrigal Diaz,Samuel Kessler,Ankur Mallick,Xuchao Zhang,Mirian Del Carmen Hipolito Garcia,Jin Xu,Victor Rühle,Saravan Rajmohan*

Main category: cs.LG

TL;DR: This paper introduces a framework where Large Language Models (LLMs) create structured reasoning blueprints to improve Small Language Models' (SLMs) performance in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: SLMs offer a lightweight alternative to LLMs, but their reasoning capabilities are limited and they are highly sensitive to prompt variations.

Method: The framework enhances SLM reasoning using LLM-generated high-level reasoning blueprints alongside a prompt template search mechanism to reduce sensitivity to prompt variations.

Result: The proposed framework improves SLM task performance in areas such as math (GSM8K), coding (MBPP), and logic (BBH), without increasing model size or requiring additional training.

Conclusion: This approach efficiently enhances SLM reasoning abilities, making it practical for deployment in resource-limited environments without trade-offs in size or training demands.

Abstract: Small language models (SLMs) offer promising and efficient alternatives to
large language models (LLMs). However, SLMs' limited capacity restricts their
reasoning capabilities and makes them sensitive to prompt variations. To
address these challenges, we propose a novel framework that enhances SLM
reasoning capabilities through LLM generated blueprints. The blueprints provide
structured, high-level reasoning guides that help SLMs systematically tackle
related problems. Furthermore, our framework integrates a prompt template
search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our
framework demonstrates improved SLM performance across various tasks, including
math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves
the reasoning capabilities of SLMs without increasing model size or requiring
additional training, offering a lightweight and deployment-friendly solution
for on-device or resource-constrained environments.

</details>


### [319] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/abs/2506.08673)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien Long Nguyen,Romina Nobahari*

Main category: cs.LG

TL;DR: The paper addresses the task of consensus clustering by ensuring fairness in proportionally representing protected groups within every cluster.


<details>
  <summary>Details</summary>
Motivation: To address fairness in consensus clustering and provide proportional representation for protected groups, a previously unexamined approach.

Method: Developed algorithms for minimally modifying existing clusterings to enforce fairness, including an optimal algorithm for equal group sizes and a near-linear approximation for unequal sizes.

Result: Provided a constant-factor approximation with near-linear time complexity and proved NP-hardness for the unequal-sized group case.

Conclusion: Closest Fair Clustering has broader implications for ensuring fairness in clustering problems and provides foundational results for fair clustering approaches.

Abstract: Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [320] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/abs/2506.08681)
*Phuc Minh Nguyen,Ngoc-Hieu Nguyen,Duy H. M. Nguyen,Anji Liu,An Mai,Binh T. Nguyen,Daniel Sonntag,Khoa D. Doan*

Main category: cs.LG

TL;DR: The paper introduces IS-DAAs, an importance-sampling approach to address the over-optimization problem in Direct Alignment Algorithms (DAAs) used for aligning large language models with human values.


<details>
  <summary>Details</summary>
Motivation: Direct Alignment Algorithms like Direct Preference Optimization can align large language models with human values but suffer from over-optimization, causing performance degradation.

Method: An importance-sampling technique is applied to DAAs, where the objective is scaled by an importance ratio reflecting the reference policy distribution. Clipping is used to minimize high variance.

Result: IS-DAAs reduce over-optimization, especially under low regularization and outperform existing methods tackling this issue.

Conclusion: IS-DAAs improve alignment effectiveness and address critical over-optimization problems in DAAs, leading to better LLM performance.

Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [321] [Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data](https://arxiv.org/abs/2506.08698)
*Boyu Xie,Tangtang Xie*

Main category: cs.LG

TL;DR: This paper introduces VAE-LF, a model based on Variational Autoencoder, for complementing missing data in power load monitoring datasets from smart grids.


<details>
  <summary>Details</summary>
Motivation: Incomplete power load monitoring data in smart grids creates challenges for accurate power load forecasting.

Method: The paper proposes a Variational Autoencoder-based model (VAE-LF), splitting the high-dimensional data into vectors, processing it sequentially, and generating complementary data.

Result: Experimental results on UK-DALE dataset show VAE-LF achieves lower RMSE and MAE compared to benchmarks, especially with low sparsity ratio data.

Conclusion: VAE-LF is an effective solution for handling incomplete electric load data, improving data completion in smart grid management systems.

Abstract: With the development of smart grids, High-Dimensional and Incomplete (HDI)
Power Load Monitoring (PLM) data challenges the performance of Power Load
Forecasting (PLF) models. In this paper, we propose a potential
characterization model VAE-LF based on Variational Autoencoder (VAE) for
efficiently representing and complementing PLM missing data. VAE-LF learns a
low-dimensional latent representation of the data using an Encoder-Decoder
structure by splitting the HDI PLM data into vectors and feeding them
sequentially into the VAE-LF model, and generates the complementary data.
Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark
models in both 5% and 10% sparsity test cases, with significantly lower RMSE
and MAE, and especially outperforms on low sparsity ratio data. The method
provides an efficient data-completion solution for electric load management in
smart grids.

</details>


### [322] [Exploration by Random Reward Perturbation](https://arxiv.org/abs/2506.08737)
*Haozhe Ma,Guoji Fu,Zhengding Luo,Jiele Wu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: The paper introduces Random Reward Perturbation (RRP), a simple yet effective exploration strategy for reinforcement learning that enhances policy diversity by injecting noise into rewards.


<details>
  <summary>Details</summary>
Motivation: Traditional exploration strategies in reinforcement learning (RL) often struggle with escaping local optima or achieving effective exploration across sparse and dense reward scenarios.

Method: RRP introduces zero-mean noise into environmental rewards, complementing existing action-perturbation-based exploration methods. It is lightweight and can be seamlessly incorporated into existing RL algorithms without significant computational costs.

Result: Through experiments, RRP boosts performance for algorithms like Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), improving sample efficiency and escaping local optima in various RL tasks.

Conclusion: RRP effectively enhances exploration in reinforcement learning, showcasing compatibility with existing methods and demonstrating practical benefits in achieving higher sample efficiency and better task performance.

Abstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.

</details>


### [323] [Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports](https://arxiv.org/abs/2506.08740)
*Sidhika Balachandar,Shuvom Sadhuka,Bonnie Berger,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: The paper introduces a multi-view, multi-output GNN model for predicting urban incident states using both sparse government inspection data and biased crowdsourced reports.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of heterogeneous and sparse data in urban spatiotemporal forecasting, where officials need accurate incident predictions, yet face challenges with biased crowdsourced reports and sparse official inspections.

Method: The authors propose a GNN-based model that combines unbiased government inspection data and biased crowdsourced reports to predict the true latent state of urban incidents.

Result: On both real and semi-synthetic datasets (over 3 years in NYC), the proposed model outperforms others that rely solely on either rating or reporting data, especially under data scarcity conditions.

Conclusion: The paper demonstrates that integrating sparse, unbiased, and biased data effectively predicts the latent state of urban issues and reveals reporting biases, providing an approach applicable to similar urban challenges.

Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal
forecasting, such as predicting infrastructure problems. In this setting,
government officials wish to know in which neighborhoods incidents like
potholes or rodent issues occur. The true state of incidents (e.g., street
conditions) for each neighborhood is observed via government inspection
ratings. However, these ratings are only conducted for a sparse set of
neighborhoods and incident types. We also observe the state of incidents via
crowdsourced reports, which are more densely observed but may be biased due to
heterogeneous reporting behavior. First, for such settings, we propose a
multiview, multioutput GNN-based model that uses both unbiased rating data and
biased reporting data to predict the true latent state of incidents. Second, we
investigate a case study of New York City urban incidents and collect,
standardize, and make publicly available a dataset of 9,615,863 crowdsourced
reports and 1,041,415 government inspection ratings over 3 years and across 139
types of incidents. Finally, we show on both real and semi-synthetic data that
our model can better predict the latent state compared to models that use only
reporting data or models that use only rating data, especially when rating data
is sparse and reports are predictive of ratings. We also quantify demographic
biases in crowdsourced reporting, e.g., higher-income neighborhoods report
problems at higher rates. Our analysis showcases a widely applicable approach
for latent state prediction using heterogeneous, sparse, and biased data.

</details>


### [324] [On the Stability of the Jacobian Matrix in Deep Neural Networks](https://arxiv.org/abs/2506.08764)
*Benjamin Dadoun,Soufiane Hayou,Hanan Salam,Mohamed El Amine Seddik,Pierre Youssef*

Main category: cs.LG

TL;DR: The paper develops a stability theorem for deep neural networks addressing sparsity and weakly correlated weights using random matrix theory.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks suffer from exploding/vanishing gradients due to Jacobian instability, which prior analyses have limited to simple network cases.

Method: The authors leverage random matrix theory to design a general stability theorem accommodating sparsity and non-i.i.d weakly correlated weights.

Result: The paper provides theoretical guarantees for spectral stability in neural networks with structured randomness and pruning effects.

Conclusion: The work expands the theoretical basis for designing robust initialization schemes for complex neural network architectures.

Abstract: Deep neural networks are known to suffer from exploding or vanishing
gradients as depth increases, a phenomenon closely tied to the spectral
behavior of the input-output Jacobian. Prior work has identified critical
initialization schemes that ensure Jacobian stability, but these analyses are
typically restricted to fully connected networks with i.i.d. weights. In this
work, we go significantly beyond these limitations: we establish a general
stability theorem for deep neural networks that accommodates sparsity (such as
that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.
induced by training). Our results rely on recent advances in random matrix
theory, and provide rigorous guarantees for spectral stability in a much
broader class of network models. This extends the theoretical foundation for
initialization schemes in modern neural networks with structured and dependent
randomness.

</details>


### [325] [IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)](https://arxiv.org/abs/2506.08844)
*Siyi Sun,David Antony Selby,Yunchuan Huang,Sebastian Vollmer,Seth Flaxman,Anisoara Calinescu*

Main category: cs.LG

TL;DR: The paper presents the IMAGIC-500 benchmark dataset to evaluate missing data imputation methods for socioeconomic research, addressing challenges like limited public access to real and synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of publicly available benchmarks for evaluating missing data imputation methods in socioeconomic datasets due to strict data protection protocols and the scarcity of synthetic datasets.

Method: The study utilized a publicly available synthetic World Bank dataset to derive the IMAGIC-500 dataset with hierarchical structure, and conducted systematic evaluations of various imputation techniques under different missing mechanisms and ratios.

Result: Results reveal strengths and weaknesses of statistical, machine learning, and deep learning imputation methods, emphasizing the impact of missing imputation on downstream tasks like predicting educational attainment.

Conclusion: The IMAGIC-500 dataset and benchmark enable robust imputation algorithm development and support reproducible social science research.

Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.

</details>


### [326] [Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing](https://arxiv.org/abs/2506.08850)
*Amin Avan,Akramul Azim,Qusay Mahmoud*

Main category: cs.LG

TL;DR: This paper proposes Agile Reinforcement Learning (aRL) to enhance task scheduling in edge computing by addressing the limitations of traditional RL algorithms, such as long adaptation times and inefficient exploration.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing complexity and challenges of scheduling tasks in edge computing environments under dynamic conditions, where traditional heuristic, metaheuristic, and standard reinforcement learning methods fall short.

Method: The study introduces Agile Reinforcement Learning (aRL), which incorporates informed exploration and action-masking methods to ensure more relevant actions are taken during the RL process, reducing the time spent on irrelevant explorations.

Result: Experimental results show that aRL achieves faster convergence and a higher hit-ratio compared to baseline approaches, showcasing its efficiency in task scheduling for edge computing.

Conclusion: By ensuring rapid adaptation and improving predictability through its novel techniques, aRL is validated as an effective approach for solving scheduling challenges in soft real-time applications within edge computing environments.

Abstract: Soft real-time applications are becoming increasingly complex, posing
significant challenges for scheduling offloaded tasks in edge computing
environments while meeting task timing constraints. Moreover, the exponential
growth of the search space, presence of multiple objectives and parameters, and
highly dynamic nature of edge computing environments further exacerbate the
complexity of task scheduling. As a result, schedulers based on heuristic and
metaheuristic algorithms frequently encounter difficulties in generating
optimal or near-optimal task schedules due to their constrained ability to
adapt to the dynamic conditions and complex environmental characteristics of
edge computing. Accordingly, reinforcement learning algorithms have been
incorporated into schedulers to address the complexity and dynamic conditions
inherent in task scheduling in edge computing. However, a significant
limitation of reinforcement learning algorithms is the prolonged learning time
required to adapt to new environments and to address medium- and large-scale
problems. This challenge arises from the extensive global action space and
frequent random exploration of irrelevant actions. Therefore, this study
proposes Agile Reinforcement learning (aRL), in which the RL-agent performs
informed exploration and executes only relevant actions. Consequently, the
predictability of the RL-agent is enhanced, leading to rapid adaptation and
convergence, which positions aRL as a suitable candidate for scheduling the
tasks of soft real-time applications in edge computing. The experiments
demonstrate that the combination of informed exploration and action-masking
methods enables aRL to achieve a higher hit-ratio and converge faster than the
baseline approaches.

</details>


### [327] [Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery](https://arxiv.org/abs/2506.08871)
*Victor M. Tenorio,Madeline Navarro,Samuel Rey,Santiago Segarra,Antonio G. Marques*

Main category: cs.LG

TL;DR: The paper addresses GNNs' struggles with heterophilic data by introducing Structure-Guided GNN (SG-GNN) that creates alternative graph structures and adaptively processes them, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: GNNs usually perform poorly with heterophilic data since they assume homophily and rely on local message passing. Improving performance on heterophilic graphs remains challenging.

Method: The authors propose creating alternative graph structures with higher label homophily, based on structural attributes rather than labels, alongside the original graph. They develop SG-GNN, which adaptively learns to weigh contributions from these graphs.

Result: SG-GNN achieves state-of-the-art or highly competitive performance on benchmark datasets, especially those with heterophilic characteristics.

Conclusion: Integrating structural information through multiple graph views significantly improves GNN performance, particularly for heterophilic datasets.

Abstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where
connected nodes may have dissimilar labels, as they typically assume homophily
and rely on local message passing. To address this, we propose creating
alternative graph structures by linking nodes with similar structural
attributes (e.g., role-based or global), thereby fostering higher label
homophily on these new graphs. We theoretically prove that GNN performance can
be improved by utilizing graphs with fewer false positive edges (connections
between nodes of different classes) and that considering multiple graph views
increases the likelihood of finding such beneficial structures. Building on
these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture
that processes the original graph alongside the newly created structural
graphs, adaptively learning to weigh their contributions. Extensive experiments
on various benchmark datasets, particularly those with heterophilic
characteristics, demonstrate that our SG-GNN achieves state-of-the-art or
highly competitive performance, highlighting the efficacy of exploiting
structural information to guide GNNs.

</details>


### [328] [Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data](https://arxiv.org/abs/2506.08882)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Ioannis Chatzigiannakis,Georgios Mylonas*

Main category: cs.LG

TL;DR: This paper investigates data imputation methods to address gaps in water meter data, improving network monitoring.


<details>
  <summary>Details</summary>
Motivation: To overcome data gaps caused by technical issues in IoT-based water monitoring systems, which hinder operational efficiency.

Method: Comparison of various imputation techniques including k-Nearest Neighbors, MissForest, Transformers, and Recurrent Neural Networks.

Result: Effective imputation methods enhance accuracy and reliability of water metering data and support applications like leak detection and maintenance scheduling.

Conclusion: Data imputation techniques can significantly improve operational decisions and efficiency in water distribution networks.

Abstract: In this work, we explore the application of recent data imputation techniques
to enhance monitoring and management of water distribution networks using smart
water meters, based on data derived from a real-world IoT water grid monitoring
deployment. Despite the detailed data produced by such meters, data gaps due to
technical issues can significantly impact operational decisions and efficiency.
Our results, by comparing various imputation methods, such as k-Nearest
Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate
that effective data imputation can substantially enhance the quality of the
insights derived from water consumption data as we study their effect on
accuracy and reliability of water metering data to provide solutions in
applications like leak detection and predictive maintenance scheduling.

</details>


### [329] [InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis](https://arxiv.org/abs/2506.08884)
*Shiqin Tang,Shujian Yu*

Main category: cs.LG

TL;DR: The paper introduces InfoDPCCA, a dynamic probabilistic CCA framework to extract shared and separate latent representations from interdependent sequential data.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of extracting meaningful latent representations from high-dimensional sequential data used in diverse fields.

Method: The proposed InfoDPCCA uses an information-theoretic objective to balance mutual structure, compression, and predictive sufficiency. It includes a two-step training scheme and residual connections.

Result: Experiments on synthetic and medical fMRI data highlight InfoDPCCA's strengths in robust, interpretable latent representation learning.

Conclusion: InfoDPCCA effectively extracts mutual and sequence-specific latent components, advancing dynamic probabilistic CCA methods while enhancing robustness and interpretability.

Abstract: Extracting meaningful latent representations from high-dimensional sequential
data is a crucial challenge in machine learning, with applications spanning
natural science and engineering. We introduce InfoDPCCA, a dynamic
probabilistic Canonical Correlation Analysis (CCA) framework designed to model
two interdependent sequences of observations. InfoDPCCA leverages a novel
information-theoretic objective to extract a shared latent representation that
captures the mutual structure between the data streams and balances
representation compression and predictive sufficiency while also learning
separate latent components that encode information specific to each sequence.
Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly
enforces the shared latent space to encode only the mutual information between
the sequences, improving interpretability and robustness. We further introduce
a two-step training scheme to bridge the gap between information-theoretic
representation learning and generative modeling, along with a residual
connection mechanism to enhance training stability. Through experiments on
synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool
for representation learning. Code of InfoDPCCA is available at
https://github.com/marcusstang/InfoDPCCA.

</details>


### [330] [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
*Yizhao Gao,Shuming Guo,Shijie Cao,Yuqing Xia,Yu Cheng,Lei Wang,Lingxiao Ma,Yutao Sun,Tianzhu Ye,Li Dong,Hayden Kwok-Hay So,Yu Hua,Ting Cao,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: SeerAttention-R is a sparse attention mechanism designed for long-decoding reasoning models. It is efficient, flexible, and achieves significant speedup with high sparsity.


<details>
  <summary>Details</summary>
Motivation: Current attention mechanisms face inefficiencies with long decoding scenarios, especially in reasoning models. This paper aims to create a sparse attention framework to address these challenges.

Method: The authors extend SeerAttention by removing query pooling and introducing a self-distilled gating mechanism. They also develop a sparse decoding kernel called TileLang for improved performance.

Result: SeerAttention-R maintains near lossless accuracy on reasoning tasks while achieving up to 9x speedup on sparse decoding compared to FlashAttention-3 at 90% sparsity.

Conclusion: SeerAttention-R effectively balances efficiency and accuracy for reasoning models' long-decoding, offering high sparsity and practical integration into existing architectures.

Abstract: We introduce SeerAttention-R, a sparse attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
sparsity through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large sparse attention
block sizes (64/128). Using TileLang, we develop a highly optimized sparse
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:
https://github.com/microsoft/SeerAttention.

</details>


### [331] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Main category: cs.LG

TL;DR: The paper introduces a framework called Self-aware Weakness-driven problem Synthesis (SwS) that improves reinforcement learning for large language models by systematically identifying and resolving weaknesses through problem augmentation, leading to significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning approaches for training large language models on reasoning tasks face limitations due to a lack of high-quality, verifiable datasets and inefficient problem synthesis that ignores model-specific weaknesses.

Method: The proposed Self-aware Weakness-driven problem Synthesis (SwS) framework identifies questions the model struggles with during training, extracts core concepts underlying these weaknesses, and generates new problems to strengthen the areas where the model is deficient.

Result: The framework achieved average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks, demonstrating its effectiveness in improving model capabilities.

Conclusion: By allowing models to self-identify and overcome their weaknesses without relying on external distillation-based datasets, the SwS framework enhances both the scalability and generalization of reinforcement learning in training language models.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [332] [Intention-Conditioned Flow Occupancy Models](https://arxiv.org/abs/2506.08902)
*Chongyi Zheng,Seohong Park,Sergey Levine,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: This paper introduces intention-conditioned flow occupancy models (InFOM) for pre-training in reinforcement learning (RL), achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To adapt the paradigm of large-scale pre-training from machine learning to reinforcement learning, aiming to address challenges like sample efficiency and robustness.

Method: The authors propose a probabilistic model that predicts temporally distant future states using flow matching and incorporates a latent variable for user intention.

Result: InFOM yields a $1.8 \times$ median improvement in returns and a 36% increase in success rates across various benchmark tasks.

Conclusion: The proposed approach, leveraging user intention, offers a scalable and efficient path to pre-training in RL, with demonstrated superiority over existing methods.

Abstract: Large-scale pre-training has fundamentally changed how machine learning
research is done today: large foundation models are trained once, and then can
be used by anyone in the community (including those without data or compute
resources to train a model from scratch) to adapt and fine-tune to specific
tasks. Applying this same framework to reinforcement learning (RL) is appealing
because it offers compelling avenues for addressing core challenges in RL,
including sample efficiency and robustness. However, there remains a
fundamental challenge to pre-train large models in the context of RL: actions
have long-term dependencies, so training a foundation model that reasons across
time is important. Recent advances in generative AI have provided new tools for
modeling highly complex distributions. In this paper, we build a probabilistic
model to predict which states an agent will visit in the temporally distant
future (i.e., an occupancy measure) using flow matching. As large datasets are
often constructed by many distinct users performing distinct tasks, we include
in our model a latent variable capturing the user intention. This intention
increases the expressivity of our model, and enables adaptation with
generalized policy improvement. We call our proposed method
intention-conditioned flow occupancy models (InFOM). Comparing with alternative
methods for pre-training, our experiments on $36$ state-based and $4$
image-based benchmark tasks demonstrate that the proposed method achieves $1.8
\times$ median improvement in returns and increases success rates by $36\%$.
Website: https://chongyi-zheng.github.io/infom Code:
https://github.com/chongyi-zheng/infom

</details>


### [333] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Main category: cs.LG

TL;DR: This paper examines test-time scaling for large language models (LLMs), focusing on "extrapolation" where LLM performance on complex tasks improves with extended thinking during inference. The authors identify a method (called e3) to enhance this process and present a 1.7B parameter model achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current LLMs do not extrapolate well on challenging problems when provided with additional inference time. Improving this capability can unlock significant advances in complex reasoning tasks.

Method: The authors propose a framework called e3, which includes three key components: (1) chaining tasks with different levels of difficulty for in-context exploration, (2) leveraging negative feedback to promote deeper exploration during reinforcement learning, and (3) employing a curriculum-based approach that adjusts task difficulty according to training token budgets.

Result: The proposed e3 recipe produces the best performance for a 1.7B parameter model, achieving leading scores on AIME'25 and HMMT'25 benchmarks. It also successfully extrapolates performance to use 2x the training token budget.

Conclusion: The e3 method enhances LLM extrapolation capabilities during test-time scaling, achieving superior reasoning performance on challenging tasks and demonstrating effective use of additional inference resources.

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [334] [Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)](https://arxiv.org/abs/2506.08916)
*Maria-Veronica Ciocanel,John T. Nardini,Kevin B. Flores,Erica M. Rutter,Suzanne S. Sindi,Alexandria Volkening*

Main category: cs.LG

TL;DR: The paper develops new equation learning methods, improving parameter recovery and generalizability for biological systems modeled via ABM.


<details>
  <summary>Details</summary>
Motivation: Equation learning methods are limited in generalizability due to high simulation demands for individual parameter sets, necessitating an advanced approach.

Method: Proposed two approaches: OAT ME-EQL for individual models interpolated across parameters, and ES ME-EQL for creating a unified model library.

Result: Demonstrated reduced error in parameter recovery from simulations, with OAT ME-EQL showing superior generalizability.

Conclusion: ME-EQL methods enhance equation learning by improving generalizability and model interpretability for complex biological systems.

Abstract: Agent-based modeling (ABM) is a powerful tool for understanding
self-organizing biological systems, but it is computationally intensive and
often not analytically tractable. Equation learning (EQL) methods can derive
continuum models from ABM data, but they typically require extensive
simulations for each parameter set, raising concerns about generalizability. In
this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by
introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns
individual models for each parameter set and connects them via interpolation,
and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library
across parameters. We demonstrate these methods using a birth--death mean-field
model and an on-lattice agent-based model of birth, death, and migration with
spatial structure. Our results show that both methods significantly reduce the
relative error in recovering parameters from agent-based simulations, with OAT
ME-EQL offering better generalizability across parameter space. Our findings
highlight the potential of equation learning from multiple experiments to
enhance the generalizability and interpretability of learned models for complex
biological systems.

</details>


### [335] [BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models](https://arxiv.org/abs/2506.08936)
*Amina Mollaysa,Artem Moskale,Pushpak Pati,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: BioLangFusion integrates DNA, mRNA, and protein language models into unified representations, outperforming unimodal methods in molecular property prediction tasks.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the central dogma of molecular biology, aiming to explore improved cross-modal molecular representations.

Method: The paper examines three fusion techniques: embedding concatenation, entropy-regularized pooling, and cross-modal multi-head attention, aligning embeddings at the codon level.

Result: BioLangFusion consistently performs better than unimodal baselines on five molecular property prediction tasks, successfully leveraging complementary multi-omic information.

Conclusion: Simple fusion of pre-trained molecular models proves effective in enhancing multi-omic analysis with minimal computational overhead.

Abstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA,
mRNA, and protein language models into unified molecular representations.
Motivated by the central dogma of molecular biology (information flow from gene
to transcript to protein), we align per-modality embeddings at the biologically
meaningful codon level (three nucleotides encoding one amino acid) to ensure
direct cross-modal correspondence. BioLangFusion studies three standard fusion
techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized
attention pooling inspired by multiple-instance learning, and (iii) cross-modal
multi-head attention -- each technique providing a different inductive bias for
combining modality-specific signals. These methods require no additional
pre-training or modification of the base models, allowing straightforward
integration with existing sequence-based foundation models. Across five
molecular property prediction tasks, BioLangFusion outperforms strong unimodal
baselines, showing that even simple fusion of pre-trained models can capture
complementary multi-omic information with minimal overhead.

</details>


### [336] [KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2506.08939)
*Hang Ye,Gaoxiang Duan,Haoran Zeng,Yangxin Zhu,Lingxue Meng,Xiaoying Zheng,Yongxin Zhu*

Main category: cs.LG

TL;DR: The paper proposes KARMA, a novel model for multivariate long-term time series forecasting using dynamic decomposition and efficient processing for improved results.


<details>
  <summary>Details</summary>
Motivation: Traditional decomposition methods and Transformer-based models are inadequate for handling the complex and dynamic characteristics of time series data, necessitating a more efficient and dynamic approach.

Method: KARMA uses two key modules: (1) Adaptive Time Channel Decomposition (ATCD) to dynamically extract trend and seasonal components, and (2) Hybrid Frequency-Time Decomposition (HFTD) to process data in both time and frequency domains. Additionally, it employs Mamba-based KarmaBlock for multi-scale, efficient processing of global and local information.

Result: KARMA significantly outperforms baseline methods in predictive accuracy and computational efficiency across eight real-world datasets.

Conclusion: The results prove that KARMA is highly effective and efficient for multivariate long-term time series forecasting, addressing the limitations of traditional and Transformer-based models.

Abstract: Multivariate long-term and efficient time series forecasting is a key
requirement for a variety of practical applications, and there are complex
interleaving time dynamics in time series data that require decomposition
modeling. Traditional time series decomposition methods are single and rely on
fixed rules, which are insufficient for mining the potential information of the
series and adapting to the dynamic characteristics of complex series. On the
other hand, the Transformer-based models for time series forecasting struggle
to effectively model long sequences and intricate dynamic relationships due to
their high computational complexity. To overcome these limitations, we
introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to
dynamically extract trend and seasonal components. It further integrates a
Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series
into frequency-domain and time-domain. These components are coupled with
multi-scale Mamba-based KarmaBlock to efficiently process global and local
information in a coordinated manner. Experiments on eight real-world datasets
from diverse domains well demonstrated that KARMA significantly outperforms
mainstream baseline methods in both predictive accuracy and computational
efficiency. Code and full results are available at this repository:
https://github.com/yedadasd/KARMA

</details>


### [337] [Towards Robust Deep Reinforcement Learning against Environmental State Perturbation](https://arxiv.org/abs/2506.08961)
*Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: This paper investigates the vulnerability of Deep Reinforcement Learning (DRL) agents to environmental state perturbations and proposes an improved defense framework called Boosted Adversarial Training (BAT) to enhance their robustness.


<details>
  <summary>Details</summary>
Motivation: Environmental state perturbations are a common challenge in embodied scenarios, yet there is limited exploration on how they impact the robustness of DRL agents.

Method: The study introduces a non-targeted calibration adversary for testing vulnerabilities and develops the BAT defense framework, which combines supervised learning to prevent catastrophic failures followed by adversarial training via reinforcement learning.

Result: Experiments reveal that mainstream DRL agents are vulnerable to environmental state perturbations. The BAT framework notably improves their resilience compared to existing robust reinforcement learning methods.

Conclusion: BAT effectively enhances the robustness of DRL agents and addresses gaps in defending against environmental state perturbations, outperforming existing algorithms in various scenarios.

Abstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have
been widely studied in various threat models; however, few consider
environmental state perturbations, which are natural in embodied scenarios. To
improve the robustness of DRL agents, we formulate the problem of environmental
state perturbation, introducing a preliminary non-targeted attack method as a
calibration adversary, and then propose a defense framework, named Boosted
Adversarial Training (BAT), which first tunes the agents via supervised
learning to avoid catastrophic failure and subsequently adversarially trains
the agent with reinforcement learning. Extensive experimental results
substantiate the vulnerability of mainstream agents under environmental state
perturbations and the effectiveness of our proposed attack. The defense results
demonstrate that while existing robust reinforcement learning algorithms may
not be suitable, our BAT framework can significantly enhance the robustness of
agents against environmental state perturbations across various situations.

</details>


### [338] [GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO](https://arxiv.org/abs/2506.08965)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.LG

TL;DR: The paper introduces a data-efficient framework enabling reward models trained on small datasets to perform comparably to those trained on large datasets, addressing inefficiencies in sample pairing and data diversity.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and scalability of Reinforcement Learning from Human Feedback (RLHF), especially in low-resource situations where few-shot data is available.

Method: The framework leverages preference refinement using Chain-of-Thought sampling, perplexity-based scoring for nuanced preferences, and Multi-level Direct Preference Optimization (M-DPO) to capture finer-grained preference differences.

Result: The method achieves significant data efficiency and model performance, allowing few-shot-trained models to match the effectiveness of large-scale-trained ones.

Conclusion: Data-efficient strategies like the proposed framework are crucial for optimizing reward models, making RLHF feasible and effective in low-resource scenarios.

Abstract: The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.

</details>


### [339] [Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data](https://arxiv.org/abs/2506.08977)
*Victoria Hankemeier,Malte Schilling*

Main category: cs.LG

TL;DR: The study explores relationships between time series features and model architectures using a custom dataset and proposes a modular model, TimeFlex, for versatile performance.


<details>
  <summary>Details</summary>
Motivation: Understand how time series characteristics align with specific model architectures to improve temporal forecasting.

Method: The authors generate a Gaussian Process-based dataset with distinct features and introduce TimeFlex, a modular architecture, comparing its performance under different temporal conditions.

Result: TimeFlex is proposed with tailored adaptability, exhibiting stronger performance compared to state-of-the-art models across diverse time series characteristics.

Conclusion: The research improves comprehension of model-dataset interactions and advances modular architectures for varied temporal dynamics.

Abstract: Developments in Deep Learning have significantly improved time series
forecasting by enabling more accurate modeling of complex temporal dependencies
inherent in sequential data. The effectiveness of such models is often
demonstrated on limited sets of specific real-world data. Although this allows
for comparative analysis, it still does not demonstrate how specific data
characteristics align with the architectural strengths of individual models.
Our research aims at uncovering clear connections between time series
characteristics and particular models. We introduce a novel dataset generated
using Gaussian Processes, specifically designed to display distinct, known
characteristics for targeted evaluations of model adaptability to them.
Furthermore, we present TimeFlex, a new model that incorporates a modular
architecture tailored to handle diverse temporal dynamics, including trends and
periodic patterns. This model is compared to current state-of-the-art models,
offering a deeper understanding of how models perform under varied time series
conditions.

</details>


### [340] [Propositional Logic for Probing Generalization in Neural Networks](https://arxiv.org/abs/2506.08978)
*Anna Langedijk,Jaap Jumelet,Willem Zuidema*

Main category: cs.LG

TL;DR: This paper studies the generalization ability of three neural architectures (Transformers, Graph Convolutional Networks, LSTMs) using a propositional logic task and finds significant challenges in generalizing to unseen patterns.


<details>
  <summary>Details</summary>
Motivation: To address the ongoing debate on whether neural networks can learn and represent symbolic rules, particularly in reasoning and systematic generalization tasks.

Method: The authors used a balanced dataset based on propositional logic to test the generalization behavior of three architectures, focusing on their ability to generate satisfying assignments for logical formulas.

Result: All models performed well within the training distribution but struggled to generalize to unseen operator combinations, especially involving negation. Transformers failed unless structural biases were introduced.

Conclusion: Current neural architectures lack the ability to learn systematic representations of logical rules, highlighting the need for stronger inductive biases to enable robust compositional reasoning.

Abstract: The extent to which neural networks are able to acquire and represent
symbolic rules remains a key topic of research and debate. Much current work
focuses on the impressive capabilities of large language models, as well as
their often ill-understood failures on a wide range of reasoning tasks. In this
paper, in contrast, we investigate the generalization behavior of three key
neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a
controlled task rooted in propositional logic. The task requires models to
generate satisfying assignments for logical formulas, making it a structured
and interpretable setting for studying compositionality. We introduce a
balanced extension of an existing dataset to eliminate superficial patterns and
enable testing on unseen operator combinations. Using this dataset, we evaluate
the ability of the three architectures to generalize beyond the training
distribution. While all models perform well in-distribution, we find that
generalization to unseen patterns, particularly those involving negation,
remains a significant challenge. Transformers fail to apply negation
compositionally, unless structural biases are introduced. Our findings
highlight persistent limitations in the ability of standard architectures to
learn systematic representations of logical operators, suggesting the need for
stronger inductive biases to support robust rule-based reasoning.

</details>


### [341] [On Finetuning Tabular Foundation Models](https://arxiv.org/abs/2506.08982)
*Ivan Rubachev,Akim Kotelnikov,Nikolay Kartashev*

Main category: cs.LG

TL;DR: The paper evaluates finetuning methods for TabPFNv2, a tabular foundational model, showing full finetuning is effective and enhances prediction by improving similarity in query-representation matching.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the underexplored area of optimal finetuning for tabular foundational models, particularly for TabPFNv2, which uses a unique in-context learning paradigm and has shown advantages over traditional methods.

Method: The authors systematically evaluate various finetuning strategies for TabPFNv2 across diverse datasets, analyze its internal mechanisms post-finetuning, and test its scalability on larger datasets.

Result: Full finetuning is found to be the most effective and time-efficient approach for TabPFNv2, improving its performance on most tasks and enabling state-of-the-art results on I.I.D. split datasets. However, its stability is less robust on datasets with temporal shifts.

Conclusion: Finetuning enhances the retrieval logic of TabPFNv2 by improving the query-key similarity, thus making it a practical method for small to medium-sized I.I.D. datasets, while noting its limitations in scenarios with temporal shifts.

Abstract: Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


### [342] [Branched Schrödinger Bridge Matching](https://arxiv.org/abs/2506.09007)
*Sophia Tang,Yinuo Zhang,Alexander Tong,Pranam Chatterjee*

Main category: cs.LG

TL;DR: The paper introduces Branched Schrödinger Bridge Matching (BranchSBM), a framework for modeling branched trajectories between distributions, improving on existing unimodal transition methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like flow matching struggle with multi-modal transitions, limiting their capacity to model divergent or branched paths from a single distribution to multiple distributions.

Method: BranchSBM uses time-dependent velocity fields and growth processes to enable the modeling of branching dynamics and population-level divergence.

Result: BranchSBM proves to be more expressive and capable in tasks like multi-path navigation, cell fate bifurcations, and modeling responses to perturbations.

Conclusion: BranchSBM expands the capacity of generative modeling by addressing the limitations of existing unimodal approaches, making it crucial for complex systems with branched outcomes.

Abstract: Predicting the intermediate trajectories between an initial and target
distribution is a central problem in generative modeling. Existing approaches,
such as flow matching and Schr\"odinger Bridge Matching, effectively learn
mappings between two distributions by modeling a single stochastic path.
However, these methods are inherently limited to unimodal transitions and
cannot capture branched or divergent evolution from a common origin to multiple
distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge
Matching (BranchSBM), a novel framework that learns branched Schr\"odinger
bridges. BranchSBM parameterizes multiple time-dependent velocity fields and
growth processes, enabling the representation of population-level divergence
into multiple terminal distributions. We show that BranchSBM is not only more
expressive but also essential for tasks involving multi-path surface
navigation, modeling cell fate bifurcations from homogeneous progenitor states,
and simulating diverging cellular responses to perturbations.

</details>


### [343] [Effective Data Pruning through Score Extrapolation](https://arxiv.org/abs/2506.09010)
*Sebastian Schmidt,Prasanga Dhungel,Christoffer Löffler,Björn Nieth,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: The authors introduce a novel technique for extrapolating data importance scores using a small data subset to improve efficiency in training machine learning models, tested on multiple datasets and paradigms.


<details>
  <summary>Details</summary>
Motivation: Training modern machine learning models is computationally expensive due to large dataset sizes, and current data pruning techniques offer limited efficiency benefits.

Method: A framework for importance score extrapolation is proposed using k-nearest neighbors and graph neural networks to predict importance scores based on a small subset of data.

Result: The approach is validated using 2 pruning methods, 4 datasets, and 3 training paradigms, showcasing its effectiveness in reducing computational costs.

Conclusion: Importance score extrapolation demonstrates promise for scaling data pruning and similar tasks in computationally intensive training processes.

Abstract: Training advanced machine learning models demands massive datasets, resulting
in prohibitive computational costs. To address this challenge, data pruning
techniques identify and remove redundant training samples while preserving
model performance. Yet, existing pruning techniques predominantly require a
full initial training pass to identify removable samples, negating any
efficiency benefits for single training runs. To overcome this limitation, we
introduce a novel importance score extrapolation framework that requires
training on only a small subset of data. We present two initial approaches in
this framework - k-nearest neighbors and graph neural networks - to accurately
predict sample importance for the entire dataset using patterns learned from
this minimal subset. We demonstrate the effectiveness of our approach for 2
state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different
datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training
paradigms (supervised, unsupervised, and adversarial). Our results indicate
that score extrapolation is a promising direction to scale expensive score
calculation methods, such as pruning, data attribution, or other tasks.

</details>


### [344] [SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning](https://arxiv.org/abs/2506.09016)
*Ruiqi Zhang,Daman Arora,Song Mei,Andrea Zanette*

Main category: cs.LG

TL;DR: The paper proposes SPEED, an RL curriculum, for efficiency in training language models, boosting training speed by 2x to 6x.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning with verifiable rewards improves reasoning in language models but is computationally costly due to inefficient sampling.

Method: The SPEED method adaptively selects intermediate-difficulty prompts to improve efficiency, supported by theoretical and practical analysis.

Result: Training becomes 2x to 6x faster without accuracy loss, requires no manual parameter tuning, and integrates smoothly with RL algorithms.

Conclusion: SPEED enhances training efficiency for large language models, maintaining performance while substantially reducing computational demands.

Abstract: Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.

</details>


### [345] [Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)
*Marton Havasi,Brian Karrer,Itai Gat,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: This paper introduces Edit Flows, a non-autoregressive model leveraging edits like insertions, deletions, and substitutions to improve sequence generation.


<details>
  <summary>Details</summary>
Motivation: Non-autoregressive models often face challenges with generating variable-length sequences effectively, in contrast to autoregressive models.

Method: Edit Flows uses a discrete flow framework over sequence data, modeled via a Continuous-time Markov Chain and supported with auxiliary variables to improve efficiency and tractability during training.

Result: Edit Flows achieves better performance than autoregressive and mask-based models in tasks like image captioning, text generation, and code generation.

Conclusion: The proposed Edit Flows method provides more flexible, accurate sequence generation by aligning closely with the structure of sequence data.

Abstract: Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations-insertions, deletions, and substitutions. By modeling these
operations within a Continuous-time Markov Chain over the sequence space, Edit
Flows enable flexible, position-relative generation that aligns more closely
with the structure of sequence data. Our training method leverages an expanded
state space with auxiliary variables, making the learning process efficient and
tractable. Empirical results show that Edit Flows outperforms both
autoregressive and mask models on image captioning and significantly
outperforms the mask construction in text and code generation.

</details>


### [346] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/abs/2506.09034)
*Sizhe Dang,Yangyang Guo,Yanjun Zhao,Haishan Ye,Xiaodong Zheng,Guang Dai,Ivor Tsang*

Main category: cs.LG

TL;DR: Introduces FZOO, a zeroth-order optimizer, which consumes significantly less memory than Adam and achieves competitive convergence speeds while requiring fewer forward passes.


<details>
  <summary>Details</summary>
Motivation: To address the GPU memory bottlenecks of fine-tuning large language models using first-order optimizers like Adam and to improve convergence speed in zeroth-order optimization.

Method: Developed FZOO, utilizing batched one-sided gradient estimates with adaptive step sizes and Rademacher perturbations for faster computation, executed with CUDA parallel processing.

Result: FZOO outperformed MeZO in accuracy and required far fewer forward passes, showing speeds competitive with Adam, enabling single-GPU fine-tuning across various large models.

Conclusion: FZOO combines memory efficiency and convergence speed, making it a practical tool for fine-tuning large language models while laying the foundation for future work in memory-efficient pre-training.

Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [347] [The Decoupled Risk Landscape in Performative Prediction](https://arxiv.org/abs/2506.09044)
*Javier Sanguino,Thomas Kehrenberg,Jose A. Lozano,Novi Quadrianto*

Main category: cs.LG

TL;DR: This paper introduces a visualization method for performative prediction scenarios, exploring how models adapt to distribution shifts induced by actions like user feature changes, and proposing an extended predictive framework.


<details>
  <summary>Details</summary>
Motivation: To provide practical insights into the theoretical advancements of performative prediction by visualizing the risk landscape and exploring model-data interactions.

Method: A decoupled risk visualization technique is developed, analyzing the risk landscape based on model and data parameter shifts, and the extended setting of Performative Prediction is proposed for cases where the decision-driven distribution reacts to an unseen model.

Result: The visualization uncovers new properties of interest points in the risk landscape and evaluates the performance of algorithms in scenarios like strategic classification using nonlinear models.

Conclusion: The proposed visualization method enhances understanding of performative prediction landscapes and lays groundwork for considering scenarios where the distribution depends on unseen deployed models.

Abstract: Performative Prediction addresses scenarios where deploying a model induces a
distribution shift in the input data, such as individuals modifying their
features and reapplying for a bank loan after rejection. Literature has had a
theoretical perspective giving mathematical guarantees for convergence (either
to the stable or optimal point). We believe that visualization of the loss
landscape can complement this theoretical advances with practical insights.
Therefore, (1) we introduce a simple decoupled risk visualization method
inspired in the two-step process that performative prediction is. Our approach
visualizes the risk landscape with respect to two parameter vectors: model
parameters and data parameters. We use this method to propose new properties of
the interest points, to examine how existing algorithms traverse the risk
landscape and perform under more realistic conditions, including strategic
classification with non-linear models. (2) Building on this decoupled risk
visualization, we introduce a novel setting - extended Performative Prediction
- which captures scenarios where the distribution reacts to a model different
from the decision-making one, reflecting the reality that agents often lack
full access to the deployed model.

</details>


### [348] [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/abs/2506.09046)
*Xiaowen Ma,Chenyang Lin,Yao Zhang,Volker Tresp,Yunpu Ma*

Main category: cs.LG

TL;DR: The paper introduces Agentic Neural Network (ANN), a framework that uses neural-network-like principles for optimizing multi-agent collaboration among Large Language Models (LLMs), achieving significant gains in accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent approaches relying on manual engineering are limited in scalability and adaptability, prompting the need for a more dynamic and efficient framework.

Method: The ANN framework conceptualizes agents as nodes within a layered neural network, employing a two-phase strategy: Forward Phase for dynamic task decomposition and team construction, and Backward Phase for refining collaboration via iterative feedback.

Result: Experiments on four benchmark datasets show that ANN consistently outperforms leading multi-agent baselines in accuracy and adaptability under the same configurations.

Conclusion: ANN provides a scalable and flexible solution for multi-agent systems, leveraging neuro-symbolic approaches to integrate collaboration and efficiency in LLMs. The framework is set to be open-sourced for accessibility.

Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for
addressing complex, high-dimensional tasks, but current approaches often rely
on static, manually engineered multi-agent configurations. To overcome these
constraints, we present the Agentic Neural Network(ANN), a framework that
conceptualizes multi-agent collaboration as a layered neural network
architecture. In this design, each agent operates as a node, and each layer
forms a cooperative "team" focused on a specific subtask. Agentic Neural
Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing
inspiration from neural network forward passes, tasks are dynamically
decomposed into subtasks, and cooperative agent teams with suitable aggregation
methods are constructed layer by layer. (2) Backward Phase-Mirroring
backpropagation, we refine both global and local collaboration through
iterative feedback, allowing agents to self-evolve their roles, prompts, and
coordination. This neuro-symbolic approach enables ANN to create new or
specialized agent teams post-training, delivering notable gains in accuracy and
adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent
baselines under the same configurations, showing consistent performance
improvements. Our findings indicate that ANN provides a scalable, data-driven
framework for multi-agent systems, combining the collaborative capabilities of
LLMs with the efficiency and flexibility of neural network principles. We plan
to open-source the entire framework.

</details>


### [349] [Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations](https://arxiv.org/abs/2506.09048)
*Yuxin Dong,Jiachen Jiang,Zhihui Zhu,Xia Ning*

Main category: cs.LG

TL;DR: The paper proposes the Linear Combination Conjecture, stating that task vectors in in-context learning act as single in-context demonstrations created from linear combinations of original inputs.


<details>
  <summary>Details</summary>
Motivation: To understand the emergence and functionality of task vectors in in-context learning and explore their role in transformer-based models.

Method: The authors analyzed the loss landscapes in linear transformers trained on triplet-formatted prompts and combined theoretical predictions with saliency analyses and parameter visualizations.

Result: The findings show that task vectors naturally emerge as linear combinations of original prompts, their limitations in high-rank mappings were predicted, validated, and explored.

Conclusion: The study deepens the understanding of task vectors, highlights the Linear Combination Conjecture, and suggests enhancements by incorporating multiple task vectors into few-shot prompts.

Abstract: Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [350] [A Practical Guide to Tuning Spiking Neuronal Dynamics](https://arxiv.org/abs/2506.08138)
*William Gebhardt,Alexander G. Ororbia,Nathan McDonald,Clare Thiem,Jack Lombardi*

Main category: cs.NE

TL;DR: This study investigates spiking neural networks (SNNs), focusing on LIF and RAF neuron models, their equations, hyperparameters, and design impacts.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize fundamental elements of spiking neural networks for improved behavior and dynamics.

Method: Analyzed LIF and RAF neuron models, examining equations, hyperparameters, input encoding, and excitatory-inhibitory setups.

Result: Key influences of hyperparameters, input encoding, and design elements on the dynamics of LIF and RAF neurons are identified.

Conclusion: A detailed examination of SNN foundational elements provides insights into designing and tuning LIF and RAF neurons effectively.

Abstract: In this work, we examine fundamental elements of spiking neural networks
(SNNs) as well as how to tune them. Concretely, we focus on two different
foundational neuronal units utilized in SNNs -- the leaky integrate-and-fire
(LIF) and the resonate-and-fire (RAF) neuron. We explore key equations and how
hyperparameter values affect behavior. Beyond hyperparameters, we discuss other
important design elements of SNNs -- the choice of input encoding and the setup
for excitatory-inhibitory populations -- and how these impact LIF and RAF
dynamics.

</details>


### [351] [Efficient Fireworks Algorithm Equipped with an Explosion Mechanism based on Student's T-distribution](https://arxiv.org/abs/2506.08484)
*Cen Shipeng,Tan Ying*

Main category: cs.NE

TL;DR: The paper introduces a student's t-distribution based Fireworks Algorithm (TFWA) for better optimization in both convex and non-convex problems. It performs well on benchmarks and certain complex scenarios.


<details>
  <summary>Details</summary>
Motivation: While convex optimization problems are well-studied, non-convex problems remain challenging. The existing Fireworks Algorithm (FWA) underperforms in convex problems and faces inefficiencies in high-dimensional cases.

Method: The authors propose TFWA, leveraging the adjustable parameters of the student's t-distribution to enhance the exploitation capability of the Fireworks Algorithm.

Result: Experiments on benchmark datasets (CEC2013 and CEC2017) show that TFWA outperforms existing FWA variants and achieves results comparable to state-of-the-art (SOTA) algorithms. In some extreme-point scenarios, its performance surpasses SOTA.

Conclusion: TFWA is a significant improvement over previous FWA variants and offers a competitive solution for complex optimization problems.

Abstract: Many real-world problems can be transformed into optimization problems, which
can be classified into convex and non-convex. Although convex problems are
almost completely studied in theory, many related algorithms to many non-convex
problems do not work well and we need more optimization techniques. As a swarm
intelligence optimization algorithm, the Fireworks Algorithm(FWA) has been
widely studied and applied to many real-world scenarios, even including large
language model fine-tuning. But the current fireworks algorithm still has a
number of problems. Firstly, as a heuristic algorithm, its performance on
convex problems cannot match the SOTA results, and can even be said to be
unsatisfactory; secondly, the sampling methods (explosion) of most FWA variants
are still uniform sampling, which is actually inefficient in high dimensional
cases. This work of ours proposes a new student's t-distribution based
FWA(TFWA) with a solid theoretical foundation, which fully utilizes the
advantage that student's t-distribution can adjust the parameters (degrees of
freedom) and thus adjust the exploitation capability. We have fully
experimented on mainstream benchmarks CEC2013 and CEC2017, which proves that
TFWA not only becomes the strongest variant of the fireworks algorithm, but
also achieves results comparable to SOTA on the test set, and its performance
is far superior to that of the SOTA algorithm in some scenarios with a large
number of extreme points.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [352] [Verification of the Release-Acquire Semantics](https://arxiv.org/abs/2506.08238)
*Parosh Abdulla,Elli Anastasiadi,Mohamed Faouzi Atig,Samuel Grahn*

Main category: cs.PL

TL;DR: This paper addresses the verification problem for Release-Acquire (RA) semantics and its variants, focusing on checking consistency of all program runs with the memory model, and analyzes the associated computational complexities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap in studying memory model verification by focusing on verifying whether all runs of a program, not just a single execution, are consistent with Release-Acquire semantics and its Strong (SRA) and Weak (WRA) variants.

Method: The authors approach the problem by using register machines to check for violations of the RA, SRA, and WRA semantics for all possible program runs. They analyze the computational complexity of these verifications.

Result: The paper finds that verifying WRA semantics is computationally feasible (O([)n5 ]), while verifying RA and SRA semantics is both NP- and coNP-hard, with a PSPACE upper bound.

Conclusion: This work clarifies the complexity of verifying memory models, highlights the computational challenges involved, and provides insights into the capabilities of register machines in this context.

Abstract: The Release-Acquire (RA) semantics and its variants are some of the most
fundamental models of concurrent semantics for architectures, programming
languages, and distributed systems. Several steps have been taken in the
direction of testing such semantics, where one is interested in whether a
single program execution is consistent with a memory model. The more general
verification problem, i.e., checking whether all allowed program runs are
consistent with a memory model, has still not been studied as much. The purpose
of this work is to bridge this gap. We tackle the verification problem, where,
given an implementation described as a register machine, we check if any of its
runs violates the RA semantics or its Strong (SRA) and Weak (WRA) variants. We
show that verifying WRA in this setup is in O([)n5 ], while verifying the RA
and SRA is in both NP- and coNP-hard, and provide a PSPACE upper bound. This
both answers some fundamental questions about the complexity of these problems,
but also provides insights on the expressive power of register machines as a
model.

</details>


### [353] [Linguine: A Natural-Language Programming Language with Formal Semantics and a Clean Compiler Pipeline](https://arxiv.org/abs/2506.08396)
*Lifan Hu*

Main category: cs.PL

TL;DR: Linguine is a programming language based on natural English constructs with features like pronoun variables, ensuring they are unambiguous and well-typed, aimed at improving readability and static verification.


<details>
  <summary>Details</summary>
Motivation: Design a programming system that leverages users' natural language intuition while maintaining rigorous formal semantics for static verification and error detection.

Method: Develop and formalize the Linguine language with anaphoric constructs, type systems, and a compiler pipeline that translates Linguine to Python, incorporating static verification through type inference and referent-tracking.

Result: Linguine was shown to enable concise, readable programs while supporting early detection of semantic errors, proving its pronoun resolution mechanism's soundness.

Conclusion: Linguine bridges human language intuition with formal programming methods, demonstrating viability for human-friendly yet rigorous software development.

Abstract: Linguine is a natural-language-inspired programming language that enables
users to write programs in a fluent, controlled subset of English while
preserving formal semantics. The language introduces anaphoric constructs, such
as pronoun variables (e.g., "it", "them"), that are statically resolved through
referent-tracking analysis combined with a Hindley-Milner-style type system.
Each pronoun is guaranteed to be unambiguous and well-typed at compile time.
  The Linguine compiler pipeline includes lexing, parsing, clause graph
construction, desugaring into a typed intermediate representation, type
inference, and abstract interpretation. This enables the early detection of
semantic errors, such as undefined or type-inconsistent references. A
lightweight backend currently generates Python code.
  This paper formalizes the core language, defines its typing and operational
semantics, and proves the soundness of its pronoun resolution mechanism. An
initial evaluation shows that Linguine allows the expression of concise and
readable programs while supporting static verification.
  Linguine represents a step toward programming systems that prioritize human
linguistic intuition while remaining grounded in formal methods and
type-theoretic rigor.

</details>


### [354] [Gradual Metaprogramming](https://arxiv.org/abs/2506.09043)
*Tianyu Chen,Darshal Shetty,Jeremy G. Siek,Chao-Hong Chen,Weixi Ma,Arnaud Venet,Rocky Liu*

Main category: cs.PL

TL;DR: The paper addresses debugging challenges in Python-embedded DSLs for data pipelines. The authors propose gradual metaprogramming to improve type error detection and localization in such DSLs, specifically targeting Meta's F3 DSL.


<details>
  <summary>Details</summary>
Motivation: Data engineers face issues debugging Python DSL scripts used for data pipelines, as errors are often detected late and are difficult to trace to their source location.

Method: The authors introduce gradual metaprogramming, combining gradual and static typing in metaprogramming, and formalize this approach via MetaGTLC calculus with semantics proven in Agda.

Result: The proposed approach ensures successful metaevaluation generates a well-typed object program, with an earlier and accurate detection of type errors.

Conclusion: Gradual metaprogramming offers a viable path toward statically-typed DSLs with better error detection and debugging, providing practical benefits for data pipeline engineering.

Abstract: Data engineers increasingly use domain-specific languages (DSLs) to generate
the code for data pipelines. Such DSLs are often embedded in Python.
Unfortunately, there are challenges in debugging the generation of data
pipelines: an error in a Python DSL script is often detected too late, after
the execution of the script, and the source code location that triggers the
error is hard to pinpoint.
  In this paper, we focus on the F3 DSL of Meta (Facebook), which is a DSL
embedded in Python (so it is dynamically-typed) to generate data pipeline
description code that is statically-typed. We propose gradual metaprogramming
to (1) provide a migration path toward statically typed DSLs, (2) immediately
provide earlier detection of code generation type errors, and (3) report the
source code location responsible for the type error. Gradual metaprogramming
accomplishes this by type checking code fragments and incrementally performing
runtime checks as they are spliced together. We define MetaGTLC, a
metaprogramming calculus in which a gradually-typed metalanguage manipulates a
statically-typed object language, and give semantics to it by translation to
the cast calculus MetaCC. We prove that successful metaevaluation always
generates a well-typed object program and mechanize the proof in Agda.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [355] [AI Magnetic Levitation (Maglev) Conveyor for Automated Assembly Production](https://arxiv.org/abs/2506.08039)
*Ray Wai Man Kong*

Main category: cs.RO

TL;DR: This paper introduces an AI-driven Maglev Conveyor system, combining magnetic levitation and AI to enhance production efficiency, reduce costs, and adapt to diverse manufacturing needs.


<details>
  <summary>Details</summary>
Motivation: Modern manufacturing demands higher efficiency, speed, and precision while addressing sustainability and the need for adaptable processes.

Method: The paper integrates maglev technology with AI algorithms for real-time monitoring, adaptive control, and optimized routing. It includes an electromagnetic controller and multiple movers for automation.

Result: The system minimizes energy consumption, cycle times, and treatment durations while ensuring precision. It provides significant cost savings compared to traditional setups like six-axis robots.

Conclusion: The AI Maglev Conveyor is positioned as an innovative solution transforming production by balancing efficiency, flexibility, and cost-effectiveness for industries handling flat parts and delicate components.

Abstract: Efficiency, speed, and precision are essential in modern manufacturing. AI
Maglev Conveyor system, combining magnetic levitation (maglev) technology with
artificial intelligence (AI), revolutionizes automated production processes.
This system reduces maintenance costs and downtime by eliminating friction,
enhancing operational efficiency. It transports goods swiftly with minimal
energy consumption, optimizing resource use and supporting sustainability. AI
integration enables real-time monitoring and adaptive control, allowing
businesses to respond to production demand fluctuations and streamline supply
chain operations.
  The AI Maglev Conveyor offers smooth, silent operation, accommodating diverse
product types and sizes for flexible manufacturing without extensive
reconfiguration. AI algorithms optimize routing, reduce cycle times, and
improve throughput, creating an agile production line adaptable to market
changes.
  This applied research paper introduces the Maglev Conveyor system, featuring
an electromagnetic controller and multiple movers to enhance automation. It
offers cost savings as an alternative to setups using six-axis robots or linear
motors, with precise adjustments for robotic arm loading. Operating at high
speeds minimizes treatment time for delicate components while maintaining
precision. Its adaptable design accommodates various materials, facilitating
integration of processing stations alongside electronic product assembly.
Positioned between linear-axis and robotic systems in cost, the Maglev Conveyor
is ideal for flat parts requiring minimal travel, transforming production
efficiency across industries. It explores its technical advantages,
flexibility, cost reductions, and overall benefits.

</details>


### [356] [UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs](https://arxiv.org/abs/2506.08045)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.RO

TL;DR: The paper introduces Agentic UAVs as advanced autonomous systems integrating intelligent AI capabilities and provides a detailed foundation, application areas, challenges, and future roadmap.


<details>
  <summary>Details</summary>
Motivation: To redefine UAV capabilities by introducing Agentic AI enabling adaptive, goal-driven, and contextual autonomous systems.

Method: The study provides a comparative analysis of Agentic UAVs versus traditional UAVs, explored key applications, challenges, solutions, and a proposed roadmap.

Result: Agentic UAVs excel in autonomy and mission flexibility with impactful applications across industries such as agriculture, construction, and disaster response.

Conclusion: Agentic UAVs offer significant societal and industrial value but require advancements in technology, regulation, and collaboration to achieve broader adoption.

Abstract: Agentic UAVs represent a new frontier in autonomous aerial intelligence,
integrating perception, decision-making, memory, and collaborative planning to
operate adaptively in complex, real-world environments. Driven by recent
advances in Agentic AI, these systems surpass traditional UAVs by exhibiting
goal-driven behavior, contextual reasoning, and interactive autonomy. We
provide a comprehensive foundation for understanding the architectural
components and enabling technologies that distinguish Agentic UAVs from
traditional autonomous UAVs. Furthermore, a detailed comparative analysis
highlights advancements in autonomy with AI agents, learning, and mission
flexibility. This study explores seven high-impact application domains
precision agriculture, construction & mining, disaster response, environmental
monitoring, infrastructure inspection, logistics, security, and wildlife
conservation, illustrating the broad societal value of agentic aerial
intelligence. Furthermore, we identify key challenges in technical constraints,
regulatory limitations, and data-model reliability, and we present emerging
solutions across hardware innovation, learning architectures, and human-AI
interaction. Finally, a future roadmap is proposed, outlining pathways toward
self-evolving aerial ecosystems, system-level collaboration, and sustainable,
equitable deployments. This survey establishes a foundational framework for the
future development, deployment, and governance of agentic aerial systems
(Agentic UAVs) across diverse societal and industrial domains.

</details>


### [357] [Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards](https://arxiv.org/abs/2506.08061)
*Ali Abedi,Fernando Cladera,Mohsen Farajijalal,Reza Ehsani*

Main category: cs.RO

TL;DR: The paper introduces a real-time system for measuring per-tree canopy volume in diverse orchard environments using mobile LiDAR data.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable and accurate tree monitoring that adapts to diverse orchard structures, avoiding reliance on static scans and uniform assumptions.

Method: The approach integrates LiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction, combined with a hybrid clustering method using DBSCAN and spectral clustering for per-tree segmentation.

Result: Achieved 93% segmentation success in pistachio orchards and 80% in almond orchards, with accurate canopy volume estimations compared to drone-derived data.

Conclusion: The system provides a scalable, real-time, non-intrusive solution for monitoring structurally diverse orchards and improves upon existing static or uniform-based methods.

Abstract: We present a real-time system for per-tree canopy volume estimation using
mobile LiDAR data collected during routine robotic navigation. Unlike prior
approaches that rely on static scans or assume uniform orchard structures, our
method adapts to varying field geometries via an integrated pipeline of
LiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.
We evaluate the system across two commercial orchards, one pistachio orchard
with regular spacing and one almond orchard with dense, overlapping crowns. A
hybrid clustering strategy combining DBSCAN and spectral clustering enables
robust per-tree segmentation, achieving 93% success in pistachio and 80% in
almond, with strong agreement to drone derived canopy volume estimates. This
work advances scalable, non-intrusive tree monitoring for structurally diverse
orchard environments.

</details>


### [358] [Ego-centric Learning of Communicative World Models for Autonomous Driving](https://arxiv.org/abs/2506.08149)
*Hang Wang,Dechen Gao,Junshan Zhang*

Main category: cs.RO

TL;DR: This paper introduces CALL, a communicative world model for multi-agent reinforcement learning that improves performance in complex environments using lightweight information sharing.


<details>
  <summary>Details</summary>
Motivation: Multi-agent reinforcement learning (MARL) faces challenges such as partial observability and non-stationarity. Although information sharing can help, it often encounters practical issues like high communication overhead and limited scalability.

Method: The proposed method, CALL, employs generative AI embodied in world models to encode agent states and intentions into low-dimensional latent representations for efficient communication. Agents perform ego-centric learning and utilize shared information to enhance predictive ability and planning.

Result: Experiments in the CARLA platform on local trajectory planning tasks demonstrate improved performance through CALL's lightweight communication and enhanced prediction accuracy.

Conclusion: The study shows that CALL effectively addresses MARL challenges by balancing communication efficiency and performance gains, enabling better planning in complex environments.

Abstract: We study multi-agent reinforcement learning (MARL) for tasks in complex
high-dimensional environments, such as autonomous driving. MARL is known to
suffer from the \textit{partial observability} and \textit{non-stationarity}
issues. To tackle these challenges, information sharing is often employed,
which however faces major hurdles in practice, including overwhelming
communication overhead and scalability concerns. By making use of generative AI
embodied in world model together with its latent representation, we develop
{\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d
Mode\underline{l}, for MARL, where 1) each agent first learns its world model
that encodes its state and intention into low-dimensional latent representation
with smaller memory footprint, which can be shared with other agents of
interest via lightweight communication; and 2) each agent carries out
ego-centric learning while exploiting lightweight information sharing to enrich
her world model, and then exploits its generalization capacity to improve
prediction for better planning. We characterize the gain on the prediction
accuracy from the information sharing and its impact on performance gap.
Extensive experiments are carried out on the challenging local trajectory
planning tasks in the CARLA platform to demonstrate the performance gains of
using \textit{CALL}.

</details>


### [359] [TensorTouch: Calibration of Tactile Sensors for High Resolution Stress Tensor and Deformation for Dexterous Manipulation](https://arxiv.org/abs/2506.08291)
*Won Kyung Do,Matthew Strong,Aiden Swann,Boshu Lei,Monroe Kennedy III*

Main category: cs.RO

TL;DR: TensorTouch integrates finite element analysis and deep learning with optical tactile sensors to achieve high-accuracy contact information extraction, enabling advanced robotic manipulation capabilities.


<details>
  <summary>Details</summary>
Motivation: Robots struggle with multi-contact, high-precision manipulation tasks that exceed the capabilities of vision and proprioception, requiring advanced tactile sensing.

Method: TensorTouch combines finite element analysis and deep learning to interpret optical tactile sensor data into stress tensors, deformation fields, and pixel-level force distributions.

Result: TensorTouch achieves sub-millimeter position accuracy and 90% success in motion-intent selective string grasping tasks, even with large sensor deformations.

Conclusion: TensorTouch extends robotic manipulation capabilities to complex contact-rich tasks, overcoming limitations in current tactile sensing technologies.

Abstract: Advanced dexterous manipulation involving multiple simultaneous contacts
across different surfaces, like pinching coins from ground or manipulating
intertwined objects, remains challenging for robotic systems. Such tasks exceed
the capabilities of vision and proprioception alone, requiring high-resolution
tactile sensing with calibrated physical metrics. Raw optical tactile sensor
images, while information-rich, lack interpretability and cross-sensor
transferability, limiting their real-world utility. TensorTouch addresses this
challenge by integrating finite element analysis with deep learning to extract
comprehensive contact information from optical tactile sensors, including
stress tensors, deformation fields, and force distributions at pixel-level
resolution. The TensorTouch framework achieves sub-millimeter position accuracy
and precise force estimation while supporting large sensor deformations crucial
for manipulating soft objects. Experimental validation demonstrates 90% success
in selectively grasping one of two strings based on detected motion, enabling
new contact-rich manipulation capabilities previously inaccessible to robotic
systems.

</details>


### [360] [HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation](https://arxiv.org/abs/2506.08296)
*Hongjun Wu,Heng Zhang,Pengsong Zhang,Jin Wang,Cong Wang*

Main category: cs.RO

TL;DR: HiBerNAC leverages neuro-inspired methods and multi-agent systems to overcome challenges in robotic manipulation tasks, significantly outperforming prior VLA models.


<details>
  <summary>Details</summary>
Motivation: Current robotic systems struggle with complex task planning due to limitations in contextual memory, multi-agent coordination under uncertainty, and dynamic long-horizon planning.

Method: HiBerNAC combines multimodal VLA planning with neuro-inspired hierarchical decision-making and decentralized multi-agent collaboration for robust robotic manipulation.

Result: HiBerNAC reduces average task completion time by 23% and achieves success rates of 12–31% on complex tasks where previous models consistently fail.

Conclusion: HiBerNAC bridges biological cognition and robotics, offering improved efficiency and success rates for complex manipulation tasks.

Abstract: Recent advances in multimodal vision-language-action (VLA) models have
revolutionized traditional robot learning, enabling systems to interpret
vision, language, and action in unified frameworks for complex task planning.
However, mastering complex manipulation tasks remains an open challenge,
constrained by limitations in persistent contextual memory, multi-agent
coordination under uncertainty, and dynamic long-horizon planning across
variable sequences. To address this challenge, we propose \textbf{HiBerNAC}, a
\textbf{Hi}erarchical \textbf{B}rain-\textbf{e}mulated \textbf{r}obotic
\textbf{N}eural \textbf{A}gent \textbf{C}ollective, inspired by breakthroughs
in neuroscience, particularly in neural circuit mechanisms and hierarchical
decision-making. Our framework combines: (1) multimodal VLA planning and
reasoning with (2) neuro-inspired reflection and multi-agent mechanisms,
specifically designed for complex robotic manipulation tasks. By leveraging
neuro-inspired functional modules with decentralized multi-agent collaboration,
our approach enables robust and enhanced real-time execution of complex
manipulation tasks. In addition, the agentic system exhibits scalable
collective intelligence via dynamic agent specialization, adapting its
coordination strategy to variable task horizons and complexity. Through
extensive experiments on complex manipulation tasks compared with
state-of-the-art VLA models, we demonstrate that \textbf{HiBerNAC} reduces
average long-horizon task completion time by 23\%, and achieves non-zero
success rates (12\textendash 31\%) on multi-path tasks where prior
state-of-the-art VLA models consistently fail. These results provide indicative
evidence for bridging biological cognition and robotic learning mechanisms.

</details>


### [361] [Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning](https://arxiv.org/abs/2506.08344)
*Neşet Ünver Akmandor,Sarvesh Prajapati,Mark Zolotas,Taşkın Padır*

Main category: cs.RO

TL;DR: Re4MPC is a multi-model motion planning system for robots that uses Nonlinear Model Predictive Control (NMPC) and Deep Reinforcement Learning (DRL) to create efficient trajectories based on task complexity and robot state.


<details>
  <summary>Details</summary>
Motivation: Traditional motion planning methods are computationally expensive and impractical for robots with many degrees of freedom in real-world scenarios.

Method: Re4MPC adapts motion planning dynamically by learning a policy through DRL to select appropriate NMPC models, costs, and constraints based on task and state. It introduces a mathematical integration of NMPC into DRL.

Result: The system shows better computational efficiency and higher success rates in simulated experiments compared to traditional NMPC.

Conclusion: Re4MPC provides an effective, robust alternative for motion planning in mobile manipulators by leveraging DRL for improved performance and efficiency.

Abstract: Traditional motion planning methods for robots with many degrees-of-freedom,
such as mobile manipulators, are often computationally prohibitive for
real-world settings. In this paper, we propose a novel multi-model motion
planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear
Model Predictive Control (NMPC). Re4MPC generates trajectories in a
computationally efficient manner by reactively selecting the model, cost, and
constraints of the NMPC problem depending on the complexity of the task and
robot state. The policy for this reactive decision-making is learned via a Deep
Reinforcement Learning (DRL) framework. We introduce a mathematical formulation
to integrate NMPC into this DRL framework. To validate our methodology and
design choices, we evaluate DRL training and test outcomes in a physics-based
simulation involving a mobile manipulator. Experimental results demonstrate
that Re4MPC is more computationally efficient and achieves higher success rates
in reaching end-effector goals than the NMPC baseline, which computes
whole-body trajectories without our learning mechanism.

</details>


### [362] [Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots](https://arxiv.org/abs/2506.08416)
*Bolin Li,Linwei Sun,Xuecong Huang,Yuzhi Jiang,Lijun Zhu*

Main category: cs.RO

TL;DR: The paper introduces a method for humanoid robots to learn bipedal gait using a novel planner and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Develop a method to efficiently teach humanoid robots periodic bipedal gait for improved locomotion performance.

Method: A hybrid inverted pendulum-based gait planner runs in parallel with reinforcement learning guided by reward compositions.

Result: The proposed method reduces learning time and improves locomotion compared to alternatives.

Conclusion: The method effectively integrates planning and learning to achieve high-performing, periodic bipedal gait in humanoid robots.

Abstract: This paper presents a periodic bipedal gait learning method using reward
composition, integrated with a real-time gait planner for humanoid robots.
First, we introduce a novel gait planner that incorporates dynamics to design
the desired joint trajectory. In the gait design process, the 3D robot model is
decoupled into two 2D models, which are then approximated as hybrid inverted
pendulums (H-LIP) for trajectory planning. The gait planner operates in
parallel in real time within the robot's learning environment. Second, based on
this gait planner, we design three effective reward functions within a
reinforcement learning framework, forming a reward composition to achieve
periodic bipedal gait. This reward composition reduces the robot's learning
time and enhances locomotion performance. Finally, a gait design example and
performance comparison are presented to demonstrate the effectiveness of the
proposed method.

</details>


### [363] [Attention-based Learning for 3D Informative Path Planning](https://arxiv.org/abs/2506.08434)
*Rui Zhao,Xingjian Zhang,Yuhong Cao,Yizhuo Wang,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: The paper introduces an attention-based deep reinforcement learning model for adaptive informative path planning (IPP) in 3D environments, designed for aerial robots to dynamically optimize sensing and exploration.


<details>
  <summary>Details</summary>
Motivation: To address the problem of maximizing information collection in dynamic 3D path planning tasks under real-world constraints such as time and distance.

Method: An attention mechanism is integrated into deep reinforcement learning to model global spatial dependencies, enabling the aerial agent to optimize sequential movement based on contextual belief representations.

Result: The approach reduces environmental uncertainty more effectively than state-of-the-art planners while accommodating constrained budgets and generalizing to various domains.

Conclusion: The proposed model demonstrates strong performance in balancing exploration and exploitation, proving its suitability for diverse real-world applications in adaptive IPP tasks.

Abstract: In this work, we propose an attention-based deep reinforcement learning
approach to address the adaptive informative path planning (IPP) problem in 3D
space, where an aerial robot equipped with a downward-facing sensor must
dynamically adjust its 3D position to balance sensing footprint and accuracy,
and finally obtain a high-quality belief of an underlying field of interest
over a given domain (e.g., presence of specific plants, hazardous gas,
geological structures, etc.). In adaptive IPP tasks, the agent is tasked with
maximizing information collected under time/distance constraints, continuously
adapting its path based on newly acquired sensor data. To this end, we leverage
attention mechanisms for their strong ability to capture global spatial
dependencies across large action spaces, allowing the agent to learn an
implicit estimation of environmental transitions. Our model builds a contextual
belief representation over the entire domain, guiding sequential movement
decisions that optimize both short- and long-term search objectives.
Comparative evaluations against state-of-the-art planners demonstrate that our
approach significantly reduces environmental uncertainty within constrained
budgets, thus allowing the agent to effectively balance exploration and
exploitation. We further show our model generalizes well to environments of
varying sizes, highlighting its potential for many real-world applications.

</details>


### [364] [TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization](https://arxiv.org/abs/2506.08440)
*Zengjue Chen,Runliang Niu,He Kong,Qi Wang*

Main category: cs.RO

TL;DR: The paper introduces TGRPO, an improvement over GRPO, to enhance online reinforcement learning for Vision-Language-Action models and demonstrates superior performance on manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action models, though effective, rely on supervised fine-tuning, which limits scalability by not utilizing live execution or environment feedback. This paper aims to explore reinforcement learning as an alternative.

Method: The authors propose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method, combining step-level and trajectory-level advantage signals to improve the accuracy of group-level advantage estimation in reinforcement learning.

Result: TGRPO outperforms various baselines on ten manipulation tasks from the libero-object benchmark, producing robust and efficient policies.

Conclusion: TGRPO enhances existing reinforcement learning methods, making them more suited for training Vision-Language-Action models and ensuring improved task performance.

Abstract: Recent advances in Vision-Language-Action (VLA) model have demonstrated
strong generalization capabilities across diverse scenes, tasks, and robotic
platforms when pretrained at large-scale datasets. However, these models still
require task-specific fine-tuning in novel environments, a process that relies
almost exclusively on supervised fine-tuning (SFT) using static trajectory
datasets. Such approaches neither allow robot to interact with environment nor
do they leverage feedback from live execution. Also, their success is
critically dependent on the size and quality of the collected trajectories.
Reinforcement learning (RL) offers a promising alternative by enabling
closed-loop interaction and aligning learned policies directly with task
objectives. In this work, we draw inspiration from the ideas of GRPO and
propose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.
By fusing step-level and trajectory-level advantage signals, this method
improves GRPO's group-level advantage estimation, thereby making the algorithm
more suitable for online reinforcement learning training of VLA. Experimental
results on ten manipulation tasks from the libero-object benchmark demonstrate
that TGRPO consistently outperforms various baseline methods, capable of
generating more robust and efficient policies across multiple tested scenarios.
Our source codes are available at: https://github.com/hahans/TGRPO

</details>


### [365] [Diffusion Models for Safety Validation of Autonomous Driving Systems](https://arxiv.org/abs/2506.08459)
*Juanran Wang,Marc R. Schlichting,Harrison Delecki,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: The research develops a denoising diffusion model to generate potential failure scenarios for autonomous driving systems, addressing safety validation challenges.


<details>
  <summary>Details</summary>
Motivation: Safety validation for autonomous systems is difficult due to high costs, risks, and the rarity of failure scenarios.

Method: The proposed method uses denoising diffusion models trained without external datasets to generate realistic failure cases in traffic scenarios.

Result: The model successfully creates diverse and realistic failure scenarios for a four-way intersection without requiring high resources or prior knowledge.

Conclusion: The approach is practical and efficient for enhancing safety validation in autonomous driving, especially in traffic intersections.

Abstract: Safety validation of autonomous driving systems is extremely challenging due
to the high risks and costs of real-world testing as well as the rarity and
diversity of potential failures. To address these challenges, we train a
denoising diffusion model to generate potential failure cases of an autonomous
vehicle given any initial traffic state. Experiments on a four-way intersection
problem show that in a variety of scenarios, the diffusion model can generate
realistic failure samples while capturing a wide variety of potential failures.
Our model does not require any external training dataset, can perform training
and inference with modest computing resources, and does not assume any prior
knowledge of the system under test, with applicability to safety validation for
traffic intersections.

</details>


### [366] [Noise Analysis and Hierarchical Adaptive Body State Estimator For Biped Robot Walking With ESVC Foot](https://arxiv.org/abs/2506.08578)
*Boyang Chen,Xizhe Zang,Chao Song,Yue Zhang,Xuehe Zhang,Jie Zhao*

Main category: cs.RO

TL;DR: This paper develops a hierarchical adaptive state estimation method for biped robots using ESVC feet to enhance state estimation precision and convergence under variable noise.


<details>
  <summary>Details</summary>
Motivation: To address challenges in robot state estimation caused by amplified noise errors in ESVC-foot-based walking robots.

Method: A noise-time regression model is coupled with a two-stage (pre- and post-estimation) hierarchical adaptive state estimator, incorporating data fusion and Extended Kalman Filter adjustments.

Result: The proposed method achieves higher precision and faster convergence in state estimation compared to conventional EKF and Adaptive EKF approaches.

Conclusion: The noise regression model and hierarchical adaptive state estimator improve the accuracy and robustness of state estimation for robots using ESVC feet.

Abstract: The ESVC(Ellipse-based Segmental Varying Curvature) foot, a robot foot design
inspired by the rollover shape of the human foot, significantly enhances the
energy efficiency of the robot walking gait. However, due to the tilt of the
supporting leg, the error of the contact model are amplified, making robot
state estimation more challenging. Therefore, this paper focuses on the noise
analysis and state estimation for robot walking with the ESVC foot. First,
through physical robot experiments, we investigate the effect of the ESVC foot
on robot measurement noise and process noise. and a noise-time regression model
using sliding window strategy is developed. Then, a hierarchical adaptive state
estimator for biped robots with the ESVC foot is proposed. The state estimator
consists of two stages: pre-estimation and post-estimation. In the
pre-estimation stage, a data fusion-based estimation is employed to process the
sensory data. During post-estimation, the acceleration of center of mass is
first estimated, and then the noise covariance matrices are adjusted based on
the regression model. Following that, an EKF(Extended Kalman Filter) based
approach is applied to estimate the centroid state during robot walking.
Physical experiments demonstrate that the proposed adaptive state estimator for
biped robot walking with the ESVC foot not only provides higher precision than
both EKF and Adaptive EKF, but also converges faster under varying noise
conditions.

</details>


### [367] [Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators](https://arxiv.org/abs/2506.08639)
*Amir Hossein Barjini,Seyed Adel Alizadeh Kolagar,Sadeq Yaqubi,Jouni Mattila*

Main category: cs.RO

TL;DR: This paper proposes a novel framework combining DRL with nonlinear PDE control to both plan motions and suppress vibrations in flexible robotic manipulators.


<details>
  <summary>Details</summary>
Motivation: Vibration suppression and precise motion tracking in flexible robotic manipulators are challenging and require innovative approaches beyond conventional control methods.

Method: Integrating a DRL-based motion planner (using SAC algorithm) with a nonlinear PDE controller to create optimized trajectories minimizing vibrations and ensuring closed-loop stability.

Result: Simulation and real-world experiments show the framework's superior performance in vibration suppression and tracking accuracy compared to traditional approaches.

Conclusion: The integration of learning-based motion planning with model-based control offers enhanced precision and stability for flexible robotic manipulators.

Abstract: This article presents a motion planning and control framework for flexible
robotic manipulators, integrating deep reinforcement learning (DRL) with a
nonlinear partial differential equation (PDE) controller. Unlike conventional
approaches that focus solely on control, we demonstrate that the desired
trajectory significantly influences endpoint vibrations. To address this, a DRL
motion planner, trained using the soft actor-critic (SAC) algorithm, generates
optimized trajectories that inherently minimize vibrations. The PDE nonlinear
controller then computes the required torques to track the planned trajectory
while ensuring closed-loop stability using Lyapunov analysis. The proposed
methodology is validated through both simulations and real-world experiments,
demonstrating superior vibration suppression and tracking accuracy compared to
traditional methods. The results underscore the potential of combining
learning-based motion planning with model-based control for enhancing the
precision and stability of flexible robotic manipulators.

</details>


### [368] [ROS-related Robotic Systems Development with V-model-based Application of MeROS Metamodel](https://arxiv.org/abs/2506.08706)
*Tomasz Winiarski,Jan Kaniuka,Daniel Giełdowski,Jakub Ostrysz,Krystian Radlak,Dmytro Kushnir*

Main category: cs.RO

TL;DR: This paper integrates the V-model development paradigm with the SysML-based MeROS modeling language for ROS-based systems, validated via a multi-robot platform case study.


<details>
  <summary>Details</summary>
Motivation: The complexity and safety-critical nature of modern robotic systems make it essential to have well-structured development methodologies. Existing tools like ROS and MBSE are valuable but often lack proper integration when used together.

Method: The authors combined the V-model development paradigm with the MeROS SysML-based modeling language, providing a domain-specific methodology that formalizes robotic systems' structure, behavior, and validation. Their approach is adaptable and project-specific.

Result: The proposed methodology was validated through a case study involving HeROS, a heterogeneous multi-robot platform, showcasing improved traceability, system consistency, and accessibility for adaptability.

Conclusion: The paper offers a structured yet flexible approach for MBSE practices in ROS-based projects, contributing a tool-agnostic and extensible framework to enhance development and research in robotics.

Abstract: As robotic systems grow increasingly complex, heterogeneous, and
safety-critical, the need for structured development methodologies becomes
paramount. Although frameworks like the Robot Operating System (ROS) and
Model-Based Systems Engineering (MBSE) offer foundational tools, they often
lack integration when used together. This paper addresses that gap by aligning
the widely recognized V-model development paradigm with the MeROS metamodel
SysML-based modeling language tailored for ROS-based systems.
  We propose a domain-specific methodology that bridges ROS-centric modelling
with systems engineering practices. Our approach formalises the structure,
behaviour, and validation processes of robotic systems using MeROS, while
extending it with a generalized, adaptable V-model compatible with both ROS and
ROS 2. Rather than prescribing a fixed procedure, the approach supports
project-specific flexibility and reuse, offering guidance across all stages of
development.
  The approach is validated through a comprehensive case study on HeROS, a
heterogeneous multi-robot platform comprising manipulators, mobile units, and
dynamic test environments. This example illustrates how the MeROS-compatible
V-model enhances traceability and system consistency while remaining accessible
and extensible for future adaptation. The work contributes a structured,
tool-agnostic foundation for developers and researchers seeking to apply MBSE
practices in ROS-based projects.

</details>


### [369] [PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly](https://arxiv.org/abs/2506.08708)
*Liang Ma,Jiajun Wen,Min Lin,Rongtao Xu,Xiwen Liang,Bingqian Lin,Jun Ma,Yongxin Wang,Ziming Wei,Haokun Lin,Mingfei Han,Meng Cao,Bokui Chen,Ivan Laptev,Xiaodan Liang*

Main category: cs.RO

TL;DR: The paper introduces PhyBlock, a benchmark to evaluate vision-language models (VLMs) on physical understanding and planning via robotic 3D block assembly tasks, exposing their difficulties in spatial reasoning, planning robustness, and intuitive comprehension.


<details>
  <summary>Details</summary>
Motivation: VLMs excel in reasoning and planning but struggle with physical phenomena and structured 3D environments; the paper aims to close this gap.

Method: PhyBlock uses a novel four-level cognitive hierarchy task with 2600 tasks (400 assembly and 2200 VQA samples) to evaluate spatial reasoning, physical comprehension, and planning. It benchmarks 21 models based on three dimensions: partial completion, failure diagnosis, and planning robustness.

Result: Findings reveal VLMs face challenges in high-level planning, reasoning, spatial orientation, and dependency comprehension, with minimal improvement from chain-of-thought prompting.

Conclusion: PhyBlock serves as a testbed for advancing VLMs in embodied reasoning, strengthening their physical problem-solving and bridging vision-language understanding with real-world scenarios.

Abstract: While vision-language models (VLMs) have demonstrated promising capabilities
in reasoning and planning for embodied agents, their ability to comprehend
physical phenomena, particularly within structured 3D environments, remains
severely limited. To close this gap, we introduce PhyBlock, a progressive
benchmark designed to assess VLMs on physical understanding and planning
through robotic 3D block assembly tasks. PhyBlock integrates a novel four-level
cognitive hierarchy assembly task alongside targeted Visual Question Answering
(VQA) samples, collectively aimed at evaluating progressive spatial reasoning
and fundamental physical comprehension, including object properties, spatial
relationships, and holistic scene understanding. PhyBlock includes 2600 block
tasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three
key dimensions: partial completion, failure diagnosis, and planning robustness.
We benchmark 21 state-of-the-art VLMs, highlighting their strengths and
limitations in physically grounded, multi-step planning. Our empirical findings
indicate that the performance of VLMs exhibits pronounced limitations in
high-level planning and reasoning capabilities, leading to a notable decline in
performance for the growing complexity of the tasks. Error analysis reveals
persistent difficulties in spatial orientation and dependency reasoning.
Surprisingly, chain-of-thought prompting offers minimal improvements,
suggesting spatial tasks heavily rely on intuitive model comprehension. We
position PhyBlock as a unified testbed to advance embodied reasoning, bridging
vision-language understanding and real-world physical problem-solving.

</details>


### [370] [Bayesian Inverse Physics for Neuro-Symbolic Robot Learning](https://arxiv.org/abs/2506.08756)
*Octavio Arriaga,Rebecca Adam,Melvin Laux,Lisa Gutzeit,Marco Ragni,Jan Peters,Frank Kirchner*

Main category: cs.RO

TL;DR: This paper analyzes the limitations of current robotic learning frameworks, proposing hybrid neuro-symbolic architectures that integrate data-driven learning with structured reasoning to enable robots to adapt better to unknown and dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the need for robotic systems to adapt, interpret, and learn efficiently in unknown and dynamic real-world environments, which current technologies fail to achieve adequately.

Method: The paper proposes combining data-driven learning with structured reasoning, leveraging differentiable physics for world modeling, Bayesian inference for decision-making, and meta-learning for rapid adaptation.

Result: A conceptual framework is introduced that embeds physical symbolic reasoning within neural models, allowing robots to generalize beyond training data, handle novel scenarios, and continuously learn.

Conclusion: Hybrid neuro-symbolic architectures are critical for advancing autonomous robotics, and a roadmap is provided for developing these systems.

Abstract: Real-world robotic applications, from autonomous exploration to assistive
technologies, require adaptive, interpretable, and data-efficient learning
paradigms. While deep learning architectures and foundation models have driven
significant advances in diverse robotic applications, they remain limited in
their ability to operate efficiently and reliably in unknown and dynamic
environments. In this position paper, we critically assess these limitations
and introduce a conceptual framework for combining data-driven learning with
deliberate, structured reasoning. Specifically, we propose leveraging
differentiable physics for efficient world modeling, Bayesian inference for
uncertainty-aware decision-making, and meta-learning for rapid adaptation to
new tasks. By embedding physical symbolic reasoning within neural models,
robots could generalize beyond their training data, reason about novel
situations, and continuously expand their knowledge. We argue that such hybrid
neuro-symbolic architectures are essential for the next generation of
autonomous systems, and to this end, we provide a research roadmap to guide and
accelerate their development.

</details>


### [371] [Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning](https://arxiv.org/abs/2506.08795)
*Kaijie Shi,Wanglong Lu,Hanli Zhao,Vinicius Prado da Fonseca,Ting Zou,Xianta Jiang*

Main category: cs.RO

TL;DR: This study introduces a fully autonomous prosthetic hand control system driven by a wrist-mounted camera, which uses imitation learning to grasp and release objects automatically without generating myoelectric signals.


<details>
  <summary>Details</summary>
Motivation: Many current prosthetic systems require users to produce specific myoelectric signals for controlling movements, which is physically and mentally exhausting.

Method: The researchers developed a teleoperation system to collect human demonstration data and trained an imitation learning model to drive prosthetic hand actions using visual inputs.

Result: The system demonstrated high success rates using data from a single participant, successfully generalizing to other users and unseen objects.

Conclusion: The proposed approach provides an intuitive and low-effort prosthetic control system, making prosthetics more accessible and effective for users with limb loss.

Abstract: Limb loss affects millions globally, impairing physical function and reducing
quality of life. Most traditional surface electromyographic (sEMG) and
semi-autonomous methods require users to generate myoelectric signals for each
control, imposing physically and mentally taxing demands. This study aims to
develop a fully autonomous control system that enables a prosthetic hand to
automatically grasp and release objects of various shapes using only a camera
attached to the wrist. By placing the hand near an object, the system will
automatically execute grasping actions with a proper grip force in response to
the hand's movements and the environment. To release the object being grasped,
just naturally place the object close to the table and the system will
automatically open the hand. Such a system would provide individuals with limb
loss with a very easy-to-use prosthetic control interface and greatly reduce
mental effort while using. To achieve this goal, we developed a teleoperation
system to collect human demonstration data for training the prosthetic hand
control model using imitation learning, which mimics the prosthetic hand
actions from human. Through training the model using only a few objects' data
from one single participant, we have shown that the imitation learning
algorithm can achieve high success rates, generalizing to more individuals and
unseen objects with a variation of weights. The demonstrations are available at
\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}

</details>


### [372] [FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency](https://arxiv.org/abs/2506.08822)
*Yifei Su,Ning Liu,Dong Chen,Zhen Zhao,Kun Wu,Meng Li,Zhiyuan Xu,Zhengping Che,Jian Tang*

Main category: cs.RO

TL;DR: FreqPolicy improves visuomotor policy for robotic manipulation by imposing frequency consistency, enabling efficient and high-quality one-step action generation in real-time applications.


<details>
  <summary>Details</summary>
Motivation: Current generative modeling-based visuomotor policies for robotic manipulation face limitations due to high inference costs, especially given the temporal dependencies inherent in these tasks.

Method: The proposed FreqPolicy imposes frequency consistency constraints and uses an adaptive consistency loss to ensure temporal coherence and structural variation in action trajectories, supporting efficient one-step action generation.

Result: FreqPolicy outperforms existing methods on 53 tasks in 3 simulation benchmarks, integrates effectively into VLA models, achieves 93.5Hz inference frequency in real-world settings, and does so without performance degradation.

Conclusion: FreqPolicy demonstrates the ability to reconcile temporal coherence with efficiency in robotic applications, proving effective in both simulated and real-world tasks while accelerating visuomotor policy execution.

Abstract: Generative modeling-based visuomotor policies have been widely adopted in
robotic manipulation attributed to their ability to model multimodal action
distributions. However, the high inference cost of multi-step sampling limits
their applicability in real-time robotic systems. To address this issue,
existing approaches accelerate the sampling process in generative
modeling-based visuomotor policies by adapting acceleration techniques
originally developed for image generation. Despite this progress, a major
distinction remains: image generation typically involves producing independent
samples without temporal dependencies, whereas robotic manipulation involves
generating time-series action trajectories that require continuity and temporal
coherence. To effectively exploit temporal information in robotic manipulation,
we propose FreqPolicy, a novel approach that first imposes frequency
consistency constraints on flow-based visuomotor policies. Our work enables the
action model to capture temporal structure effectively while supporting
efficient, high-quality one-step action generation. We introduce a frequency
consistency constraint that enforces alignment of frequency-domain action
features across different timesteps along the flow, thereby promoting
convergence of one-step action generation toward the target distribution. In
addition, we design an adaptive consistency loss to capture structural temporal
variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53
tasks across 3 simulation benchmarks, proving its superiority over existing
one-step action generators. We further integrate FreqPolicy into the
vision-language-action (VLA) model and achieve acceleration without performance
degradation on the 40 tasks of Libero. Besides, we show efficiency and
effectiveness in real-world robotic scenarios with an inference frequency
93.5Hz. The code will be publicly available.

</details>


### [373] [MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains](https://arxiv.org/abs/2506.08840)
*Dewei Wang,Xinmiao Wang,Xinzhe Liu,Jiyuan Shi,Yingnan Zhao,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: This paper introduces a new RL-based framework that allows humanoid robots to traverse complex terrains with human-like and controllable gaits using exteroception.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based humanoid locomotion methods struggle to traverse complex terrains with human-like gaits due to reliance on proprioception and flat terrain limitations.

Method: The paper introduces a two-stage training pipeline using a mixture of latent residual experts, multi-discriminators, gait rewards, exteroception, and human-like gait-commanded transitions.

Result: The proposed framework achieved excellent performance in navigating complex terrains and showed smooth transitions between various human-like gait patterns, in both simulation and real-world tests.

Conclusion: The approach demonstrates the potential for humanoid robots to achieve robust, human-like locomotion on complex terrains with enhanced adaptability and versatility.

Abstract: Humanoid robots have demonstrated robust locomotion capabilities using
Reinforcement Learning (RL)-based approaches. Further, to obtain human-like
behaviors, existing methods integrate human motion-tracking or motion prior in
the RL framework. However, these methods are limited in flat terrains with
proprioception only, restricting their abilities to traverse challenging
terrains with human-like gaits. In this work, we propose a novel framework
using a mixture of latent residual experts with multi-discriminators to train
an RL policy, which is capable of traversing complex terrains in controllable
lifelike gaits with exteroception. Our two-stage training pipeline first
teaches the policy to traverse complex terrains using a depth camera, and then
enables gait-commanded switching between human-like gait patterns. We also
design gait rewards to adjust human-like behaviors like robot base height.
Simulation and real-world experiments demonstrate that our framework exhibits
exceptional performance in traversing complex terrains, and achieves seamless
transitions between multiple human-like gait patterns.

</details>


### [374] [Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization](https://arxiv.org/abs/2506.08851)
*Sepehr Samavi,Garvish Bhutani,Florian Shkurti,Angela P. Schoellig*

Main category: cs.RO

TL;DR: The paper introduces SICNav, a model predicting human-robot interactions for navigating crowded spaces, evaluated across 7km in two hours.


<details>
  <summary>Details</summary>
Motivation: Improving navigation for service robots in crowded spaces where human and robot interactions impact efficiency and safety.

Method: Development of SICNav, a bilevel Model Predictive Control framework to integrate human prediction and robot motion planning, accounting for agent interactions.

Result: SICNav was tested in indoor and outdoor environments, autonomously navigating 7 kilometers in 2 hours while analyzing performance.

Conclusion: SICNav effectively addresses human-robot interaction challenges in navigation, demonstrating practical deployment potential in various settings.

Abstract: Safe and efficient navigation in crowded environments remains a critical
challenge for robots that provide a variety of service tasks such as food
delivery or autonomous wheelchair mobility. Classical robot crowd navigation
methods decouple human motion prediction from robot motion planning, which
neglects the closed-loop interactions between humans and robots. This lack of a
model for human reactions to the robot plan (e.g. moving out of the way) can
cause the robot to get stuck. Our proposed Safe and Interactive Crowd
Navigation (SICNav) method is a bilevel Model Predictive Control (MPC)
framework that combines prediction and planning into one optimization problem,
explicitly modeling interactions among agents. In this paper, we present a
systems overview of the crowd navigation platform we use to deploy SICNav in
previously unseen indoor and outdoor environments. We provide a preliminary
analysis of the system's operation over the course of nearly 7 km of autonomous
navigation over two hours in both indoor and outdoor environments.

</details>


### [375] [Fast Estimation of Globally Optimal Independent Contact Regions for Robust Grasping and Manipulation](https://arxiv.org/abs/2506.08856)
*Jonathan P. King,Harnoor Ahluwalia,Michael Zhang,Nancy S. Pollard*

Main category: cs.RO

TL;DR: This paper introduces a fast divide-and-conquer algorithm using n-dimensional Delaunay triangulation to compute independent contact regions (ICRs) for grasp planning in real-time.


<details>
  <summary>Details</summary>
Motivation: Challenges in modern applications for independent contact regions (ICRs) stem from their complex and computationally expensive nature, limiting their use in real-time grasp planning.

Method: A divide-and-conquer approach based on incremental n-dimensional Delaunay triangulation is proposed, enabling efficient computation of planar ICRs with bounded suboptimality.

Result: Experiments demonstrate significant runtime improvements (100x speedups) over competing methods and better robustness with ICR-guided policies.

Conclusion: The proposed algorithm provides a practical solution for real-time ICR computation, presenting substantial benefits for grasp and manipulation planning and paving the way for future 3D applications with publicly available code.

Abstract: This work presents a fast anytime algorithm for computing globally optimal
independent contact regions (ICRs). ICRs are regions such that one contact
within each region enables a valid grasp. Locations of ICRs can provide
guidance for grasp and manipulation planning, learning, and policy transfer.
However, ICRs for modern applications have been little explored, in part due to
the expense of computing them, as they have a search space exponential in the
number of contacts. We present a divide and conquer algorithm based on
incremental n-dimensional Delaunay triangulation that produces results with
bounded suboptimality in times sufficient for real-time planning. This paper
presents the base algorithm for grasps where contacts lie within a plane. Our
experiments show substantial benefits over competing grasp quality metrics and
speedups of 100X and more for competing approaches to computing ICRs. We
explore robustness of a policy guided by ICRs and outline a path to general 3D
implementation. Code will be released on publication to facilitate further
development and applications.

</details>


### [376] [MOMAV: A highly symmetrical fully-actuated multirotor drone using optimizing control allocation](https://arxiv.org/abs/2506.08868)
*Marco Ruggia*

Main category: cs.RO

TL;DR: MOMAV is a multirotor drone with omnidirectional control and high efficiency due to its unique and symmetrical design.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to create a fully actuated and efficient drone capable of independent control of its orientation and position, overcoming limitations of traditional multirotor designs.

Method: The paper introduces a unique design where the drone's six rotor arms align with an octahedron's vertices, allowing arm rotation along their axes. It also uses a novel sequential quadratic programming-based control algorithm.

Result: MOMAV demonstrated high precision during flight tests, achieving position/orientation errors of 6.6mm/2.1° for position setpoints and 11.8mm/3.3° for orientation setpoints.

Conclusion: MOMAV showcases superior flight efficiency and precision compared to traditional designs, thanks to its novel design, arm rotation capability, and advanced control algorithm.

Abstract: MOMAV (Marco's Omnidirectional Micro Aerial Vehicle) is a multirotor drone
that is fully actuated, meaning it can control its orientation independently of
its position. MOMAV is also highly symmetrical, making its flight efficiency
largely unaffected by its current orientation. These characteristics are
achieved by a novel drone design where six rotor arms align with the vertices
of an octahedron, and where each arm can actively rotate along its long axis.
Various standout features of MOMAV are presented: The high flight efficiency
compared to arm configuration of other fully-actuated drones, the design of an
original rotating arm assembly featuring slip-rings used to enable continuous
arm rotation, and a novel control allocation algorithm based on sequential
quadratic programming (SQP) used to calculate throttle and arm-angle setpoints
in flight. Flight tests have shown that MOMAV is able to achieve remarkably low
mean position/orientation errors of 6.6mm, 2.1{\deg} ({\sigma}: 3.0mm,
1.0{\deg}) when sweeping position setpoints, and 11.8mm, 3.3{\deg} ({\sigma}:
8.6mm, 2.0{\deg}) when sweeping orientation setpoints.

</details>


### [377] [Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication](https://arxiv.org/abs/2506.08890)
*Tauhid Tanjim,Promise Ekpo,Huajie Cao,Jonathan St. George,Kevin Ching,Hee Rin Lee,Angelique Taylor*

Main category: cs.RO

TL;DR: The study investigates the effectiveness of verbal versus non-verbal communication in robotic crash carts (RCCs) to reduce workload and enhance healthcare teamwork.


<details>
  <summary>Details</summary>
Motivation: To address the challenge healthcare workers face with traditional crash carts in delivering rapid patient care and explore the untapped potential of RCCs for improving task performance.

Method: A between-subjects experiment comparing the effectiveness of RCC's verbal and non-verbal communications against traditional crash carts in resuscitation scenarios with an emphasis on mental workload and ease of use.

Result: Verbal communication via RCCs significantly reduced mental demand and effort in contrast to visual cues and traditional crash carts, though it slightly increased frustration levels.

Conclusion: The study provides valuable guidance for implementing RCCs in healthcare, highlighting verbal communication as a promising tool for reducing workload while identifying trade-offs in user satisfaction.

Abstract: Healthcare workers (HCWs) encounter challenges in hospitals, such as
retrieving medical supplies quickly from crash carts, which could potentially
result in medical errors and delays in patient care. Robotic crash carts (RCCs)
have shown promise in assisting healthcare teams during medical tasks through
guided object searches and task reminders. Limited exploration has been done to
determine what communication modalities are most effective and least disruptive
to patient care in real-world settings. To address this gap, we conducted a
between-subjects experiment comparing the RCC's verbal and non-verbal
communication of object search with a standard crash cart in resuscitation
scenarios to understand the impact of robot communication on workload and
attitudes toward using robots in the workplace. Our findings indicate that
verbal communication significantly reduced mental demand and effort compared to
visual cues and with a traditional crash cart. Although frustration levels were
slightly higher during collaborations with the robot compared to a traditional
cart, these research insights provide valuable implications for human-robot
teamwork in high-stakes environments.

</details>


### [378] [CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks](https://arxiv.org/abs/2506.08931)
*Yixuan Li,Yutang Lin,Jieming Cui,Tengyu Liu,Wei Liang,Yixin Zhu,Siyuan Huang*

Main category: cs.RO

TL;DR: The paper introduces CLONE, a teleoperation system that solves issues of positional drift and uncoordinated movement in humanoid teleoperation using real-time error correction.


<details>
  <summary>Details</summary>
Motivation: Current humanoid teleoperation systems suffer from limitations such as decoupled upper and lower body controls and unbounded positional drift.

Method: CLONE uses a Mixture of Experts (MoE) model with closed-loop error correction based on real-time feedback, leveraging head and hand tracking via a MR headset.

Result: The system achieves high fidelity in whole-body teleoperation, minimizing positional drift and supporting coordinated, complex motions like picking up objects.

Conclusion: CLONE sets a new benchmark for stable, extended-duration humanoid teleoperation in complex scene-interaction tasks.

Abstract: Humanoid teleoperation plays a vital role in demonstrating and collecting
data for complex humanoid-scene interactions. However, current teleoperation
systems face critical limitations: they decouple upper- and lower-body control
to maintain stability, restricting natural coordination, and operate open-loop
without real-time position feedback, leading to accumulated drift. The
fundamental challenge is achieving precise, coordinated whole-body
teleoperation over extended durations while maintaining accurate global
positioning. Here we show that an MoE-based teleoperation system, CLONE, with
closed-loop error correction enables unprecedented whole-body teleoperation
fidelity, maintaining minimal positional drift over long-range trajectories
using only head and hand tracking from an MR headset. Unlike previous methods
that either sacrifice coordination for stability or suffer from unbounded
drift, CLONE learns diverse motion skills while preventing tracking error
accumulation through real-time feedback, enabling complex coordinated movements
such as ``picking up objects from the ground.'' These results establish a new
milestone for whole-body humanoid teleoperation for long-horizon humanoid-scene
interaction tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [379] [A Systematic Literature Review on Continuous Integration and Deployment (CI/CD) for Secure Cloud Computing](https://arxiv.org/abs/2506.08055)
*Sabbir M. Saleh,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: The paper presents a systematic review of 66 studies focusing on cloud security challenges in Continuous Integration/Continuous Delivery (CI/CD), highlighting tools, challenges, and research gaps.


<details>
  <summary>Details</summary>
Motivation: Rapid increase in cloud environments and cyberattacks has made securing app deployment in cloud systems a priority across different sectors.

Method: A Systematic Literature Review (SLR) of 66 papers to analyze tools, challenges, and gaps in CI/CD pipeline security within cloud environments.

Result: Identified popular tools like Harbor and SonarQube, challenges like unauthorized access and weak authentication, and research gaps in addressing these issues in CI/CD pipelines.

Conclusion: Further research is necessary to strengthen security solutions in cloud-based CI/CD environments by addressing existing challenges and gaps.

Abstract: As cloud environments become widespread, cybersecurity has emerged as a top
priority across areas such as networks, communication, data privacy, response
times, and availability. Various sectors, including industries, healthcare, and
government, have recently faced cyberattacks targeting their computing systems.
Ensuring secure app deployment in cloud environments requires substantial
effort. With the growing interest in cloud security, conducting a systematic
literature review (SLR) is critical to identifying research gaps. Continuous
Software Engineering, which includes continuous integration (CI), delivery
(CDE), and deployment (CD), is essential for software development and
deployment. In our SLR, we reviewed 66 papers, summarising tools, approaches,
and challenges related to the security of CI/CD in the cloud. We addressed key
aspects of cloud security and CI/CD and reported on tools such as Harbor,
SonarQube, and GitHub Actions. Challenges such as image manipulation,
unauthorised access, and weak authentication were highlighted. The review also
uncovered research gaps in how tools and practices address these security
issues in CI/CD pipelines, revealing a need for further study to improve
cloud-based security solutions.

</details>


### [380] [A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.08153)
*Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: The paper investigates managing the complexity of ML-enabled systems and introduces metrics-based architectural models.


<details>
  <summary>Details</summary>
Motivation: To address how complexity impacts ML-enabled systems and help architects make informed decisions and guidelines for their growth.

Method: Introduces an extension to a reference architecture to characterize MLES and collect complexity-related metrics.

Result: Provides a foundational step toward a metrics-based framework for assessing complexity in ML-enabled systems.

Conclusion: This study lays the groundwork for managing complexity in ML-enabled systems through metrics-based architectural guidance.

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper showcases the first step for
creating the metrics-based architectural model: an extension of a reference
architecture that can describe MLES to collect their metrics.

</details>


### [381] [Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models](https://arxiv.org/abs/2506.08171)
*Daniel Koh,Yannic Noller,Corina S. Pasareanu,Adrians Skapars,Youcheng Sun*

Main category: cs.SE

TL;DR: This paper explores the capability of large language models (LLMs) to handle symbolic reasoning tasks, particularly worst-case symbolic constraints analysis in programs. The researchers improved performance through symbolic reasoning-guided fine-tuning, creating a dataset and their model WARP-1.0-3B.


<details>
  <summary>Details</summary>
Motivation: LLMs have demonstrated proficiency in coding tasks but underperformed in complex symbolic reasoning. The authors aim to investigate their abilities in worst-case symbolic constraint analysis and connect LLMs with formal symbolic reasoning methods.

Method: The authors evaluated existing LLMs' performance on symbolic reasoning tasks, fine-tuned them using reinforcement learning with SMT constraint solving, and developed a specialized dataset to assist training.

Result: Their model, WARP-1.0-3B, outperformed both size-matched and larger models in recovering symbolic constraints related to worst-case algorithm behavior.

Conclusion: The study highlights that LLMs exhibit the potential for deeper symbolic reasoning, advocating for tighter integration of neural-network-based learning and formal methods in program analysis.

Abstract: Large language models (LLMs) have been successfully applied to a variety of
coding tasks, including code generation, completion, and repair. However, more
complex symbolic reasoning tasks remain largely unexplored by LLMs. This paper
investigates the capacity of LLMs to reason about worst-case executions in
programs through symbolic constraints analysis, aiming to connect LLMs and
symbolic reasoning approaches. Specifically, we define and address the problem
of worst-case symbolic constraints analysis as a measure to assess the
comprehension of LLMs. We evaluate the performance of existing LLMs on this
novel task and further improve their capabilities through symbolic
reasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)
constraint solving and supported by a specially designed dataset of symbolic
constraints. Experimental results show that our solver-aligned model,
WARP-1.0-3B, consistently surpasses size-matched and even much larger
baselines, demonstrating that a 3B LLM can recover the very constraints that
pin down an algorithm's worst-case behaviour through reinforcement learning
methods. These findings suggest that LLMs are capable of engaging in deeper
symbolic reasoning, supporting a closer integration between neural
network-based learning and formal methods for rigorous program analysis.

</details>


### [382] [Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles](https://arxiv.org/abs/2506.08173)
*Nguyen Phu Vinh,Anh Chung Hoang,Chris Ngo,Truong-Son Hy*

Main category: cs.SE

TL;DR: Repeton, an open-source framework, uses LLMs in a structured patch-and-test pipeline for precise and automated code fixes in Git repositories, enhancing precision and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address limitations of LLMs in complex software engineering tasks, including low precision and poor interpretability.

Method: Repeton breaks down software tasks into a patch-and-test pipeline involving diagnosis, code changes, and validation using lightweight heuristics and automated testing.

Result: Repeton outperforms Retrieval-Augmented Generation (RAG)-based methods on SWE-bench Lite in terms of patch validity and interpretability.

Conclusion: Repeton offers a modular and transparent approach to debugging, demonstrating scalable and practical autonomous debugging potential.

Abstract: Large Language Models (LLMs) have shown strong capabilities in code
generation and comprehension, yet their application to complex software
engineering tasks often suffers from low precision and limited
interpretability. We present Repeton, a fully open-source framework that
leverages LLMs for precise and automated code manipulation in real-world Git
repositories. Rather than generating holistic fixes, Repeton operates through a
structured patch-and-test pipeline: it iteratively diagnoses issues, proposes
code changes, and validates each patch through automated testing. This stepwise
process is guided by lightweight heuristics and development tools, avoiding
reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite
benchmark, our method shows good performance compared to RAG-based methods in
both patch validity and interpretability. By decomposing software engineering
tasks into modular, verifiable stages, Repeton provides a practical path toward
scalable and transparent autonomous debugging.

</details>


### [383] [MBTModelGenerator: A software tool for reverse engineering of Model-based Testing (MBT) models from clickstream data of web applications](https://arxiv.org/abs/2506.08179)
*Sasidhar Matta,Vahid Garousi*

Main category: cs.SE

TL;DR: The paper introduces an open-source tool that creates Model-Based Testing (MBT) models by converting user clickstream data into state-transition models.


<details>
  <summary>Details</summary>
Motivation: Manual creation of test models and suites for software testing is time-consuming and labor-intensive, necessitating a more efficient, automated approach.

Method: The tool captures user interface events during web app interaction, converts them into state-transition models, and exports these in a format compatible with the GraphWalker MBT tool for testing.

Result: The tool enables automatic test generation and test execution without manually creating models, making MBT more accessible by reflecting actual user behavior.

Conclusion: The tool reduces the effort required for MBT adoption and contributes to the field by providing an open-source, automated solution compatible with existing MBT systems.

Abstract: Automated testing has become a standard practice in software engineering, yet
the creation of test models and suites remains labor-intensive. To reduce this
effort, we developed an open-source tool that automatically generates
Model-Based Testing (MBT) models from clickstream data collected during user
interaction with web applications. The tool captures UI events, transforms them
into state-transition models, and exports the result in a format compatible
with the GraphWalker MBT tool. This enables immediate test execution without
the need for manual model creation. The approach lowers the barrier to MBT
adoption by leveraging actual usage behavior and reducing the reliance on
upfront modeling. This technical report documents the system requirements,
design decisions, implementation details, testing process, and empirical
evaluation of the tool, which is publicly available as open-source.

</details>


### [384] [Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study](https://arxiv.org/abs/2506.08311)
*Ira Ceka,Saurabh Pujar,Shyam Ramji,Luca Buratti,Gail Kaiser,Baishakhi Ray*

Main category: cs.SE

TL;DR: This paper systematically analyzes the decision-making processes of software engineering (SWE) agents using execution traces, categorizing their decision-making pathways and investigating core agent functionalities and performance impacts.


<details>
  <summary>Details</summary>
Motivation: While SWE agents show strong empirical results, their decision-making workflows remain poorly understood. The authors aim to improve agent reliability and efficiency by examining these workflows systematically.

Method: The study provides a taxonomy of decision-making pathways in agents, analyzes core agent components (bug localization, patch generation, test generation), and compares agent-generated patches with developer-written ones using code clone analysis and qualitative evaluation.

Result: The analysis uncovers core components crucial to agent success, reveals the effects of test generation on patch production, and identifies structural and stylistic differences between agent-generated and human-created patches.

Conclusion: These insights pave the way for enhancing SWE agent design, making them more effective and aligned with human development practices.

Abstract: With the advent of large language models (LLMs), software engineering agents
(SWE agents) have emerged as a powerful paradigm for automating a range of
software tasks -- from code generation and repair to test case synthesis. These
agents operate autonomously by interpreting user input and responding to
environmental feedback. While various agent architectures have demonstrated
strong empirical performance, the internal decision-making worfklows that drive
their behavior remain poorly understood. Deeper insight into these workflows
hold promise for improving both agent reliability and efficiency. In this work,
we present the first systematic study of SWE agent behavior through the lens of
execution traces. Our contributions are as follows: (1) we propose the first
taxonomy of decision-making pathways across five representative agents; (2)
using this taxonomy, we identify three core components essential to agent
success -- bug localization, patch generation, and reproduction test generation
-- and study each in depth; (3) we study the impact of test generation on
successful patch production; and analyze strategies that can lead to successful
test generation; (4) we further conduct the first large-scale code clone
analysis comparing agent-generated and developer-written patches and provide a
qualitative study revealing structural and stylistic differences in patch
content. Together, these findings offer novel insights into agent design and
open avenues for building agents that are both more effective and more aligned
with human development practices.

</details>


### [385] [Detecting State Manipulation Vulnerabilities in Smart Contracts Using LLM and Static Analysis](https://arxiv.org/abs/2506.08561)
*Hao Wu,Haijun Wang,Shangwang Li,Yin Wu,Ming Fan,Yitao Zhao,Ting Liu*

Main category: cs.SE

TL;DR: The paper introduces PriceSleuth, a novel method combining Large Language Models (LLM) and static analysis to detect Price Manipulation attacks in DeFi contracts through logic identification, dependency analysis, and propagation analysis.


<details>
  <summary>Details</summary>
Motivation: DeFi's increased popularity has also led to rising vulnerabilities, particularly Price Manipulation attacks that exploit token prices for illicit gains. Detecting such attacks proactively is critical to securing these platforms.

Method: PriceSleuth employs LLM and static analysis to detect Price Manipulation attacks in three stages: identifying core logic functions of price calculation, performing backward dependency analysis on price variables, and using propagation analysis to evaluate malicious exploitation.

Result: Preliminary experimental evaluations validate PriceSleuth's effectiveness in detecting Price Manipulation attacks, showcasing its potential as an automated security mechanism.

Conclusion: PriceSleuth demonstrates a promising approach to safeguarding DeFi protocols from Price Manipulation attacks, and future research efforts are discussed to further enhance its capabilities.

Abstract: An increasing number of DeFi protocols are gaining popularity, facilitating
transactions among multiple anonymous users. State Manipulation is one of the
notorious attacks in DeFi smart contracts, with price variable being the most
commonly exploited state variable-attackers manipulate token prices to gain
illicit profits. In this paper, we propose PriceSleuth, a novel method that
leverages the Large Language Model (LLM) and static analysis to detect Price
Manipulation (PM) attacks proactively. PriceSleuth firstly identifies core
logic function related to price calculation in DeFi contracts. Then it guides
LLM to locate the price calculation code statements. Secondly, PriceSleuth
performs backward dependency analysis of price variables, instructing LLM in
detecting potential price manipulation. Finally, PriceSleuth utilizes
propagation analysis of price variables to assist LLM in detecting whether
these variables are maliciously exploited. We presented preliminary
experimental results to substantiate the effectiveness of PriceSleuth . And we
outline future research directions for PriceSleuth.

</details>


### [386] [Evaluating the Performance and Efficiency of Sentence-BERT for Code Comment Classification](https://arxiv.org/abs/2506.08581)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: The paper fine-tunes Sentence-BERT for multi-label code comment classification, balancing classification performance and inference efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve classification accuracy for multi-label code comment tasks while minimizing efficiency penalties during inference.

Method: Using a dataset of 13,216 labeled comments, applying Sentence-BERT fine-tuning with various classification heads, and analyzing the trade-offs between model size, performance, and efficiency.

Result: Larger models achieve better F1 performance, but smaller models excel in runtime and GFLOPS efficiency. An optimal trade-off improves F1 by 0.0346 with minimal efficiency degradation.

Conclusion: The study concludes that fine-tuned Sentence-BERT models can achieve a reasonable balance between performance and efficiency for practical application in code comment classification.

Abstract: This work evaluates Sentence-BERT for a multi-label code comment
classification task seeking to maximize the classification performance while
controlling efficiency constraints during inference. Using a dataset of 13,216
labeled comment sentences, Sentence-BERT models are fine-tuned and combined
with different classification heads to recognize comment types. While larger
models outperform smaller ones in terms of F1, the latter offer outstanding
efficiency, both in runtime and GFLOPS. As result, a balance between a
reasonable F1 improvement (+0.0346) and a minimal efficiency degradation (+1.4x
in runtime and +2.1x in GFLOPS) is reached.

</details>


### [387] [RE-oriented Model Development with LLM Support and Deduction-based Verification](https://arxiv.org/abs/2506.08606)
*Radoslaw Klimek*

Main category: cs.SE

TL;DR: The paper proposes a framework using UML diagrams, LLMs, and formal verification to enhance requirements engineering, ensuring better system designs and automatic program skeleton generation.


<details>
  <summary>Details</summary>
Motivation: To improve the quality and reliability of software development during the critical requirements engineering phase by integrating advanced modelling and verification techniques.

Method: Develop a framework utilizing UML diagrams for modelling, integrate large language models for generating behavioural models, and use formal deductive verification to address logical requirements. Transition automatically from design to code by generating program skeletons.

Result: The framework enhances the RE phase by providing formal logical specifications, ensuring consistency, and enabling seamless implementation transitions.

Conclusion: Integrating LLMs with formal verification techniques within a coherent framework significantly improves software development efficiency and reliability by bridging design and implementation phases.

Abstract: The requirements engineering (RE) phase is pivotal in developing high-quality
software. Integrating advanced modelling techniques with large language models
(LLMs) and formal verification in a logical style can significantly enhance
this process. We propose a comprehensive framework that focuses on specific
Unified Modelling Language (UML) diagrams for preliminary system development.
This framework offers visualisations at various modelling stages and seamlessly
integrates large language models and logical reasoning engines. The behavioural
models generated with the assistance of LLMs are automatically translated into
formal logical specifications. Deductive formal verification ensures that
logical requirements and interrelations between software artefacts are
thoroughly addressed. Ultimately, the framework facilitates the automatic
generation of program skeletons, streamlining the transition from design to
implementation.

</details>


### [388] [Logic Mining from Process Logs: Towards Automated Specification and Verification](https://arxiv.org/abs/2506.08628)
*Radoslaw Klimek,Julia Witek*

Main category: cs.SE

TL;DR: This paper introduces a method to automate logical specification creation from workflow-based models, validated using general and real-world data.


<details>
  <summary>Details</summary>
Motivation: Manual creation of logical specifications is time-intensive and prone to errors, especially in complex systems.

Method: Combining workflow mining to discover process models with pattern-based translation and automated reasoning techniques. The approach employs theorem provers to evaluate logical properties.

Result: The method was tested and successfully validated on general-purpose and real-world data, demonstrating its effectiveness in diverse scenarios.

Conclusion: The approach is practical and applicable in realistic software engineering contexts, with a reduction in manual effort and an increase in reliability.

Abstract: Logical specifications play a key role in the formal analysis of behavioural
models. Automating the derivation of such specifications is particularly
valuable in complex systems, where manual construction is time-consuming and
error-prone. This article presents an approach for generating logical
specifications from process models discovered via workflow mining, combining
pattern-based translation with automated reasoning techniques. In contrast to
earlier work, we evaluate the method on both general-purpose and real-case
event logs, enabling a broader empirical assessment. The study examines the
impact of data quality, particularly noise, on the structure and testability of
generated specifications. Using automated theorem provers, we validate a
variety of logical properties, including satisfiability, internal consistency,
and alignment with predefined requirements. The results support the
applicability of the approach in realistic settings and its potential
integration into empirical software engineering practices.

</details>


### [389] [Proceedings of the 23rd International Overture Workshop](https://arxiv.org/abs/2506.08680)
*Hugo Daniel Macedo,Ken Pierce*

Main category: cs.SE

TL;DR: The abstract summarizes the 23rd International Overture Workshop held in 2025, focusing on the Vienna Development Method (VDM) and associated tools.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bring together researchers and practitioners to discuss and advance the VDM and related modelling tools for effective system development.

Method: The workshop serves as a platform for presenting updates on collaborative modelling, co-simulation, and technological advancements in VDM/Overture.

Result: Updates on VDM/Overture technologies and insights into collaborative tools and methods for Cyber-Physical Systems were shared at the workshop.

Conclusion: The workshop highlighted ongoing developments and established the relevance of VDM/Overture in modeling and analyzing technologies for complex systems.

Abstract: This volume contains the papers presented at the 23rd International Overture
Workshop, held on the 11th of June 2025. This event was the latest in a series
of workshops around the Vienna Development Method (VDM), the open-source
project Overture, and related tools and formalisms. VDM is one of the longest
established formal methods for systems development. A lively community of
researchers and practitioners has grown up in academia and industry has grown
around the modelling languages (VDM-SL, VDM++, VDM-RT, CML) and tools
(VDMTools, Overture, Crescendo, Symphony, the INTO-CPS chain, and ViennaTalk).
Together, these provide a platform for work on modelling and analysis
technology that includes static and dynamic analysis, test generation,
execution support, and model checking. This workshop provided updates on the
emerging technology of VDM/Overture, including collaboration infrastructure,
collaborative modelling and co-simulation for Cyber-Physical Systems.

</details>


### [390] [Causality-aware Safety Testing for Autonomous Driving Systems](https://arxiv.org/abs/2506.08688)
*Wenbing Tang,Mingfei Cheng,Renzhi Wang,Yuan Zhou,Chengwei Liu,Yang Liu,Zuohua Ding*

Main category: cs.SE

TL;DR: The paper introduces Causal-Fuzzer, a novel causality-aware fuzzing technique for efficiently and comprehensively testing Autonomous Driving Systems (ADSs) through causally diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing ADS testing methods fail to capture complex interrelationships among input scenarios, motion commands, and system violations, leading to incomplete testing coverage.

Method: Causal-Fuzzer constructs a causal graph to model interrelationships between diversities in input scenarios, ADS commands, and violations. It uses this graph for a feedback mechanism to quantify causal diversity and guides scenario mutations to prioritize impactful elements.

Result: Causal-Fuzzer demonstrated superiority over existing methods in identifying diverse violations, increasing testing sufficiency through better causal coverage, and rapidly detecting critical scenarios, validated on the Apollo ADS.

Conclusion: Causal-Fuzzer is an efficient and effective technique that enhances testing comprehensiveness and efficiency by leveraging causality in scenario generation and evaluation, closing the gaps left by previous methods.

Abstract: Simulation-based testing is essential for evaluating the safety of Autonomous
Driving Systems (ADSs). Comprehensive evaluation requires testing across
diverse scenarios that can trigger various types of violations under different
conditions. While existing methods typically focus on individual diversity
metrics, such as input scenarios, ADS-generated motion commands, and system
violations, they often fail to capture the complex interrelationships among
these elements. This oversight leads to gaps in testing coverage, potentially
missing critical issues in the ADS under evaluation. However, quantifying these
interrelationships presents a significant challenge. In this paper, we propose
a novel causality-aware fuzzing technique, Causal-Fuzzer, to enable efficient
and comprehensive testing of ADSs by exploring causally diverse scenarios. The
core of Causal-Fuzzer is constructing a causal graph to model the
interrelationships among the diversities of input scenarios, ADS motion
commands, and system violations. Then the causal graph will guide the process
of critical scenario generation. Specifically, Causal-Fuzzer proposes (1) a
causality-based feedback mechanism that quantifies the combined diversity of
test scenarios by assessing whether they activate new causal relationships, and
(2) a causality-driven mutation strategy that prioritizes mutations on input
scenario elements with higher causal impact on ego action changes and violation
occurrence, rather than treating all elements equally. We evaluated
Causal-Fuzzer on an industry-grade ADS Apollo, with a high-fidelity. Our
empirical results demonstrate that Causal-Fuzzer significantly outperforms
existing methods in (1) identifying a greater diversity of violations, (2)
providing enhanced testing sufficiency with improved coverage of causal
relationships, and (3) achieving greater efficiency in detecting the first
critical scenarios.

</details>


### [391] [Do Generative AI Tools Ensure Green Code? An Investigative Study](https://arxiv.org/abs/2506.08790)
*Samarth Sikand,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: This paper investigates the sustainability of AI-generated code from tools like ChatGPT, BARD, and Copilot and finds that they generally fail to follow green coding practices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how environmentally friendly AI-generated code is, given the focus on software sustainability and the growing reliance on generative AI tools for coding.

Method: The authors assessed the sustainability aspects of AI-generated code by analyzing outputs from ChatGPT, BARD, and Copilot and evaluating their adherence to sustainable coding practices.

Result: The investigation found that default code generated by these tools often does not adhere to green coding principles.

Conclusion: AI code generation tools require further investigation and remediation strategies to encourage sustainable and environmentally friendly coding practices.

Abstract: Software sustainability is emerging as a primary concern, aiming to optimize
resource utilization, minimize environmental impact, and promote a greener,
more resilient digital ecosystem. The sustainability or "greenness" of software
is typically determined by the adoption of sustainable coding practices. With a
maturing ecosystem around generative AI, many software developers now rely on
these tools to generate code using natural language prompts. Despite their
potential advantages, there is a significant lack of studies on the
sustainability aspects of AI-generated code. Specifically, how environmentally
friendly is the AI-generated code based upon its adoption of sustainable coding
practices? In this paper, we present the results of an early investigation into
the sustainability aspects of AI-generated code across three popular generative
AI tools - ChatGPT, BARD, and Copilot. The results highlight the default
non-green behavior of tools for generating code, across multiple rules and
scenarios. It underscores the need for further in-depth investigations and
effective remediation strategies.

</details>


### [392] [Towards a Knowledge Base of Common Sustainability Weaknesses in Green Software Development](https://arxiv.org/abs/2506.08812)
*Priyavanshi Pathania,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: The paper highlights the need for standardized knowledge to develop tools addressing sustainability weaknesses in software systems and proposes an initial framework for this.


<details>
  <summary>Details</summary>
Motivation: The climate crisis necessitates developing software systems that optimize resource utilization and minimize environmental impact. There is a pressing need for automated tools to analyze code for sustainability, hampered by a lack of standardized domain knowledge.

Method: The paper proposes creating a standardized knowledge base of sustainability weaknesses in code and conducts preliminary experiments to demonstrate deficiencies in reusing existing knowledge without adaptation.

Result: Preliminary experiments reveal that existing software knowledge cannot be directly repurposed for sustainability, emphasizing the importance of creating tailored solutions.

Conclusion: The development of a dedicated knowledge base for sustainability weaknesses in software code is critical to fostering environmentally friendly development practices, and further research is strongly recommended.

Abstract: With the climate crisis looming, engineering sustainable software systems
become crucial to optimize resource utilization, minimize environmental impact,
and foster a greener, more resilient digital ecosystem. For developers, getting
access to automated tools that analyze code and suggest sustainabilityrelated
optimizations becomes extremely important from a learning and implementation
perspective. However, there is currently a dearth of such tools due to the lack
of standardized knowledge, which serves as the foundation of these tools. In
this paper, we motivate the need for the development of a standard knowledge
base of commonly occurring sustainability weaknesses in code, and propose an
initial way of doing that. Furthermore, through preliminary experiments, we
demonstrate why existing knowledge regarding software weaknesses cannot be
re-tagged "as is" to sustainability without significant due diligence, thereby
urging further explorations in this ecologically significant domain.

</details>


### [393] [On The Impact of Merge Request Deviations on Code Review Practices](https://arxiv.org/abs/2506.08860)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: The paper addresses non-standard practices in software code reviews, termed 'deviations,' and proposes a machine learning method for detection and its impact on model performance.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability of ML-based analytics for code reviews by addressing biases introduced by deviations in Merge Request workflows.

Method: The authors identify seven categories of deviations, use few-shot learning to achieve 91% detection accuracy, and analyze how excluding deviations improves ML models for review predictions.

Result: Excluding deviations resulted in performance improvements in 53.33% of ML models, with accuracy gains up to 2.25x, along with significant shifts in feature importance.

Conclusion: Addressing deviations in MR workflows enhances the effectiveness and reliability of review-focused ML models, benefiting software engineering practices.

Abstract: Code review is a key practice in software engineering, ensuring quality and
collaboration. However, industrial Merge Request (MR) workflows often deviate
from standardized review processes, with many MRs serving non-review purposes
(e.g., drafts, rebases, or dependency updates). We term these cases deviations
and hypothesize that ignoring them biases analytics and undermines ML models
for review analysis.
  We identify seven deviation categories, occurring in 37.02% of MRs, and
propose a few-shot learning detection method (91% accuracy). By excluding
deviations, ML models predicting review completion time improve performance in
53.33% of cases (up to 2.25x) and exhibit significant shifts in feature
importance (47% overall, 60% top-*k*).
  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven
detection approach, and (3) empirical evidence of their impact on ML-based
review analytics. This work aids practitioners in optimizing review efforts and
ensuring reliable insights.

</details>


### [394] [AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code Generation](https://arxiv.org/abs/2506.08980)
*Kaifeng He,Mingwei Liu,Chong Wang,Zike Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: The paper identifies issues in code generation by LLMs due to poor token selection at high-uncertainty points and introduces AdaDec, an adaptive decoding framework to address these challenges, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard decoding strategies inadequately handle token uncertainty during code generation, leading to errors at critical decision points that impact program logic.

Method: AdaDec integrates a token-level pause-then-rerank mechanism based on token uncertainty (Shannon entropy), using learned uncertainty thresholds and lookahead strategies for reranking.

Result: AdaDec achieves up to a 15.5% improvement in Pass@1 accuracy on HumanEval and MBPP benchmarks, surpassing or matching beam search performance while reducing computational cost and latency.

Conclusion: Uncertainty-aware adaptive decoding, as implemented in AdaDec, can significantly enhance the reliability and efficiency of LLM-based code generation.

Abstract: Code generation with large language models (LLMs) is highly sensitive to
token selection during decoding, particularly at uncertain decision points that
influence program logic. While standard strategies like greedy and beam search
treat all tokens uniformly, they overlook code-specific uncertainty patterns,
leading to suboptimal performance. This paper presents an empirical study
revealing that many generation errors stem from ranking mistakes at
high-uncertainty steps, where the correct token is present but not top-ranked.
  Motivated by these findings, we propose AdaDec, an uncertainty-guided
adaptive decoding framework that integrates a token-level pause-then-rerank
mechanism driven by token uncertainty (Shannon entropy). AdaDec learns
model-specific uncertainty thresholds and applies a lookahead-based reranking
strategy when uncertainty is high. Experiments on HumanEval and MBPP benchmarks
show that AdaDec improves Pass@1 accuracy by up to 15.5% over greedy decoding,
outperforms or matches beam search, and reduces computational cost and latency
through efficient, selective pausing. Our results highlight the promise of
uncertainty-aware adaptive decoding for improving the reliability and
efficiency of LLM-based code generation.

</details>


### [395] [Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models](https://arxiv.org/abs/2506.09002)
*Bei Chu,Yang Feng,Kui Liu,Hange Shi,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: PALM is a novel approach leveraging large language models (LLMs) and program analysis to significantly enhance unit test generation, achieving high coverage and acceptance rates.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing methods for unit test generation, which struggle with complex program units and achieve low coverage.

Method: Combine branching conditions through program analysis to create path constraints, which guide LLMs in generating high-coverage unit tests.

Result: PALM boosts test coverage by over 50% in specific instances, achieving an average coverage of 75.77% across projects, comparable to human efforts, and with high acceptance rates in code submissions.

Conclusion: PALM demonstrates the effectiveness of integrating program analysis and LLMs for automated software testing, paving the way for further research in this area.

Abstract: Unit testing is essential for ensuring software reliability and correctness.
Classic Search-Based Software Testing (SBST) methods and concolic
execution-based approaches for generating unit tests often fail to achieve high
coverage due to difficulties in handling complex program units, such as
branching conditions and external dependencies. Recent work has increasingly
utilized large language models (LLMs) to generate test cases, improving the
quality of test generation by providing better context and correcting errors in
the model's output. However, these methods rely on fixed prompts, resulting in
relatively low compilation success rates and coverage. This paper presents
PALM, an approach that leverages large language models (LLMs) to enhance the
generation of high-coverage unit tests. PALM performs program analysis to
identify branching conditions within functions, which are then combined into
path constraints. These constraints and relevant contextual information are
used to construct prompts that guide the LLMs in generating unit tests. We
implement the approach and evaluate it in 10 open-source Rust crates.
Experimental results show that within just two or three hours, PALM can
significantly improves test coverage compared to classic methods, with
increases in overall project coverage exceeding 50% in some instances and its
generated tests achieving an average coverage of 75.77%, comparable to human
effort (71.30%), highlighting the potential of LLMs in automated test
generation. We submitted 91 PALM-generated unit tests targeting new code. Of
these submissions, 80 were accepted, 5 were rejected, and 6 remain pending
review. The results demonstrate the effectiveness of integrating program
analysis with AI and open new avenues for future research in automated software
testing.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [396] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: This paper investigates how instruction-tuned multimodal large language models (MLLMs) align with human brain activity during naturalistic movie stimuli, showing improved multimodal functional processing and hierarchical brain alignment.


<details>
  <summary>Details</summary>
Motivation: To address the gap in studying brain alignment of instruction-tuned MLLMs in multimodal settings, particularly during naturalistic stimuli such as movies.

Method: The study used instruction-specific embeddings from six video and two audio instruction-tuned MLLMs, evaluating their alignment with neural activity during multimodal movie watching via experiments involving 13 video task-specific instructions.

Result: Instruction-tuned video MLLMs outperformed non-instruction-tuned multimodal models by 15% and unimodal models by 20%. Findings also show hierarchical alignment between MLLM layers and brain regions.

Conclusion: Task-specific instructions significantly enhance brain alignment with MLLMs, highlighting the potential for better mapping of joint information processing, and the study's findings advance understanding of multimodal functional processing in the brain.

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


### [397] [The Predictive Brain: Neural Correlates of Word Expectancy Align with Large Language Model Prediction Probabilities](https://arxiv.org/abs/2506.08511)
*Nikola Kölbl,Konstantin Tziridis,Andreas Maier,Thomas Kinfe,Ricardo Chavarriaga,Achim Schilling,Patrick Krauss*

Main category: q-bio.NC

TL;DR: This paper investigates predictive coding in naturalistic speech by recording EEG and MEG data during audiobook listening and linking neural responses to predictability scores generated by BERT.


<details>
  <summary>Details</summary>
Motivation: To understand how the brain anticipates upcoming words to optimize language comprehension, especially in natural speech scenarios.

Method: Simultaneous EEG and MEG recordings were conducted while participants listened to audiobooks; predictability scores for nouns were assigned using the BERT model.

Result: Higher word predictability was linked to reduced neural responses during recognition (e.g., lower N400 amplitudes), increased anticipatory activity, pre-activation in left fronto-temporal regions (EEG), and a sensorimotor engagement tendency in low-predictability contexts (MEG).

Conclusion: The study supports the dynamic integration of top-down predictions and bottom-up sensory input for language comprehension, highlighting novel insights into predictive processing and paving the way for neuroscience-inspired AI development.

Abstract: Predictive coding theory suggests that the brain continuously anticipates
upcoming words to optimize language processing, but the neural mechanisms
remain unclear, particularly in naturalistic speech. Here, we simultaneously
recorded EEG and MEG data from 29 participants while they listened to an audio
book and assigned predictability scores to nouns using the BERT language model.
Our results show that higher predictability is associated with reduced neural
responses during word recognition, as reflected in lower N400 amplitudes, and
with increased anticipatory activity before word onset. EEG data revealed
increased pre-activation in left fronto-temporal regions, while MEG showed a
tendency for greater sensorimotor engagement in response to low-predictability
words, suggesting a possible motor-related component to linguistic
anticipation. These findings provide new evidence that the brain dynamically
integrates top-down predictions with bottom-up sensory input to facilitate
language comprehension. To our knowledge, this is the first study to
demonstrate these effects using naturalistic speech stimuli, bridging
computational language models with neurophysiological data. Our findings
provide novel insights for cognitive computational neuroscience, advancing the
understanding of predictive processing in language and inspiring the
development of neuroscience-inspired AI. Future research should explore the
role of prediction and sensory precision in shaping neural responses and
further refine models of language processing.

</details>


### [398] [Geometric Hyperscanning under Active Inference](https://arxiv.org/abs/2506.08599)
*Nicolas Hinrichs,Mahault Albarracin,Dimitris Bolis,Yuyue Jiang,Leonardo Christov-Moore,Leonhard Schilbach*

Main category: q-bio.NC

TL;DR: The paper develops a model of social cognition based on coupled active inference and introduces a novel empirical method, geometric hyperscanning, to measure brain interaction patterns.


<details>
  <summary>Details</summary>
Motivation: To better understand and empirically measure the dynamics of social cognition and reciprocal interaction between individuals.

Method: The authors model social cognition as coupled active inference processes and propose geometric hyperscanning, using Forman-Ricci curvature to track changes in inter-brain networks.

Result: The study enables the analysis of inter-brain network reconfigurations, providing insight into affective co-regulation between individuals.

Conclusion: This work advances second-person neuroscience by linking theoretical models of social cognition with empirical tools to study affective dynamics in interactions.

Abstract: Second-person neuroscience holds social cognition as embodied meaning
co-regulation through reciprocal interaction, modeled here as coupled active
inference with affect emerging as inference over identity-relevant surprise.
Each agent maintains a self-model that tracks violations in its predictive
coherence while recursively modeling the other. Valence is computed from
self-model prediction error, weighted by self-relevance, and modulated by prior
affective states and by what we term temporal aiming, which captures affective
appraisal over time. This accommodates shifts in the self-other boundary,
allowing affect to emerge at individual and dyadic levels. We propose a novel
method termed geometric hyperscanning, based on the Forman-Ricci curvature, to
empirically operationalize these processes: it tracks topological
reconfigurations in inter-brain networks, with its entro-py serving as a proxy
for affective phase transitions such as rupture, co-regulation, and
re-attunement.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [399] [Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting](https://arxiv.org/abs/2506.08049)
*Tengfei Lyu,Weijia Zhang,Hao Liu*

Main category: stat.ML

TL;DR: This paper introduces TelePiT, a deep learning model improving subseasonal-to-seasonal (S2S) climate forecasts by incorporating physics and teleconnection awareness.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of forecasting at S2S scales, due to the chaotic nature of atmospheric systems and the lack of explicit modeling for physical processes and teleconnections in current approaches.

Method: The paper proposes TelePiT, comprising three components: Spherical Harmonic Embedding for encoding global atmospheric variables, Multi-Scale Physics-Informed Neural ODE for capturing physical processes, and a Teleconnection-Aware Transformer to integrate teleconnections into self-attention mechanisms.

Result: TelePiT significantly surpasses current data-driven and numerical weather prediction systems, reducing RMSE for 2-meter temperature by 57.7% compared to earlier models.

Conclusion: TelePiT demonstrates the efficacy of integrating multi-scale physics and teleconnection patterns for more accurate S2S climate forecasting.

Abstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions
from several weeks to months in advance, presents significant challenges due to
the chaotic dynamics of atmospheric systems and complex interactions across
multiple scales. Current approaches often fail to explicitly model underlying
physical processes and teleconnections that are crucial at S2S timescales. We
introduce TelePiT, a novel deep learning architecture that enhances global S2S
forecasting through integrated multi-scale physics and teleconnection
awareness. Our approach consists of three key components: (1) Spherical
Harmonic Embedding, which accurately encodes global atmospheric variables onto
spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which
explicitly captures atmospheric physical processes across multiple learnable
frequency bands; (3) Teleconnection-Aware Transformer, which models critical
global climate interactions through tactfully injecting teleconnection patterns
into the self-attention. Extensive experiments demonstrate that TelePiT
significantly outperforms state-of-the-art data-driven baselines and
operational numerical weather prediction systems, with remarkable improvements
for atmospheric variables including a 57.7% reduction in RMSE for 2-meter
temperature compared to previous best models.

</details>


### [400] [WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection](https://arxiv.org/abs/2506.08066)
*Alexander Stepikin,Evgenia Romanenkova,Alexey Zaytsev*

Main category: stat.ML

TL;DR: The paper introduces a novel ensemble aggregation method called WWAggr for improving deep neural network-based Change Point Detection (CPD) and addresses decision threshold selection.


<details>
  <summary>Details</summary>
Motivation: Real-world high-dimensional CPD is challenging due to complex data patterns and violations of key assumptions. Current standalone deep neural network detectors are insufficient in achieving robust detection.

Method: The authors propose WWAggr, an ensemble aggregation method based on the Wasserstein distance, which improves upon standard aggregation techniques used in CPD detection.

Result: WWAggr demonstrates versatility and effectiveness across different ensembles of CPD models, overcoming limitations of common aggregation methods and resolving the challenge of decision threshold selection.

Conclusion: WWAggr offers a robust, task-specific solution for improving CPD detection by leveraging ensemble methods and addressing threshold selection issues.

Abstract: Change Point Detection (CPD) aims to identify moments of abrupt distribution
shifts in data streams. Real-world high-dimensional CPD remains challenging due
to data pattern complexity and violation of common assumptions. Resorting to
standalone deep neural networks, the current state-of-the-art detectors have
yet to achieve perfect quality. Concurrently, ensembling provides more robust
solutions, boosting the performance. In this paper, we investigate ensembles of
deep change point detectors and realize that standard prediction aggregation
techniques, e.g., averaging, are suboptimal and fail to account for problem
peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific
method of ensemble aggregation based on the Wasserstein distance. Our procedure
is versatile, working effectively with various ensembles of deep CPD models.
Moreover, unlike existing solutions, we practically lift a long-standing
problem of the decision threshold selection for CPD.

</details>


### [401] [Constrained Pareto Set Identification with Bandit Feedback](https://arxiv.org/abs/2506.08127)
*Cyrille Kone,Emilie Kaufmann,Laura Richert*

Main category: stat.ML

TL;DR: This paper focuses on identifying the Pareto Set in multivariate bandit problems with feasibility constraints and proposes a near-optimal, theoretically-backed algorithm.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the challenge of identifying optimal arms in $K$-armed bandits that satisfy linear feasibility constraints, which express minimal performance requirements across multiple objectives.

Method: The authors propose a fixed-confidence identification algorithm for constrained Pareto Set identification and derive an information-theoretic lower bound on the sample complexity of any such algorithm.

Result: The proposed algorithm is shown to significantly outperform other approaches, such as racing-like methods and two-stage strategies, both theoretically and empirically.

Conclusion: The method is near-optimal in terms of sample complexity and is supported by extensive evaluations on benchmark problems.

Abstract: In this paper, we address the problem of identifying the Pareto Set under
feasibility constraints in a multivariate bandit setting. Specifically, given a
$K$-armed bandit with unknown means $\mu_1, \dots, \mu_K \in \mathbb{R}^d$, the
goal is to identify the set of arms whose mean is not uniformly worse than that
of another arm (i.e., not smaller for all objectives), while satisfying some
known set of linear constraints, expressing, for example, some minimal
performance on each objective. Our focus lies in fixed-confidence
identification, for which we introduce an algorithm that significantly
outperforms racing-like algorithms and the intuitive two-stage approach that
first identifies feasible arms and then their Pareto Set. We further prove an
information-theoretic lower bound on the sample complexity of any algorithm for
constrained Pareto Set identification, showing that the sample complexity of
our approach is near-optimal. Our theoretical results are supported by an
extensive empirical evaluation on a series of benchmarks.

</details>


### [402] [Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces](https://arxiv.org/abs/2506.08325)
*Marcos Matabuena,Rahul Ghosal,Pavlo Mozharovskyi,Oscar Hernan Madrid Padilla,Jukka-Pekka Onnela*

Main category: stat.ML

TL;DR: The paper introduces model-free algorithms leveraging conditional depth measures for prediction and tolerance regions, suitable for use in separable Hilbert spaces with benefits like faster convergence rates and conformal prediction for finite samples.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the underutilized area of integrating depth measures into regression models to create prediction regions for high-dimensional or complex data.

Method: The method involves developing algorithms using conditional depth measures based on kernel mean embeddings and integrating depth measures. Conformal prediction is incorporated to ensure finite-sample, non-asymptotic guarantees.

Result: The approach achieves conditional and unconditional consistency, ensures fast convergence rates under certain settings, and shows strong performance in simulations with functional and Euclidean data.

Conclusion: The proposed methods effectively address uncertainty quantification challenges for complex data types and demonstrate practical utility in applications such as personalized digital health.

Abstract: Depth measures are powerful tools for defining level sets in emerging,
non--standard, and complex random objects such as high-dimensional multivariate
data, functional data, and random graphs. Despite their favorable theoretical
properties, the integration of depth measures into regression modeling to
provide prediction regions remains a largely underexplored area of research. To
address this gap, we propose a novel, model-free uncertainty quantification
algorithm based on conditional depth measures--specifically, conditional kernel
mean embeddings and an integrated depth measure. These new algorithms can be
used to define prediction and tolerance regions when predictors and responses
are defined in separable Hilbert spaces. The use of kernel mean embeddings
ensures faster convergence rates in prediction region estimation. To enhance
the practical utility of the algorithms with finite samples, we also introduce
a conformal prediction variant that provides marginal, non-asymptotic
guarantees for the derived prediction regions. Additionally, we establish both
conditional and unconditional consistency results, as well as fast convergence
rates in certain homoscedastic settings. We evaluate the finite--sample
performance of our model in extensive simulation studies involving various
types of functional data and traditional Euclidean scenarios. Finally, we
demonstrate the practical relevance of our approach through a digital health
application related to physical activity, aiming to provide personalized
recommendations

</details>


### [403] [Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification](https://arxiv.org/abs/2506.08548)
*Moria Mayala,Erwan Scornet,Charles Tillier,Olivier Wintenberger*

Main category: stat.ML

TL;DR: This paper studies rebalancing techniques for classification problems with imbalanced datasets, focusing on Centered Random Forests (CRF), and introduces a debiasing method with variance reduction benefits.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of imbalanced datasets in classification tasks and evaluate rebalancing techniques for improving machine learning performance.

Method: Developed a theoretical framework for an infinite CRF, established a Central Limit Theorem, introduced a debiasing method using importance sampling (IS), and analyzed its variance reduction on rebalanced datasets.

Result: Showed that rebalanced CRFs exhibit bias removable through the IS-ICRF method, which provides better performance in high imbalance scenarios with reduced variance.

Conclusion: Training CRFs on rebalanced datasets followed by debiasing techniques offers performance improvements compared to using the original imbalanced data, with implications for variance reduction observed experimentally and theoretically.

Abstract: Many classification tasks involve imbalanced data, in which a class is
largely underrepresented. Several techniques consists in creating a rebalanced
dataset on which a classifier is trained. In this paper, we study theoretically
such a procedure, when the classifier is a Centered Random Forests (CRF). We
establish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates
and exact constant. We then prove that the CRF trained on the rebalanced
dataset exhibits a bias, which can be removed with appropriate techniques.
Based on an importance sampling (IS) approach, the resulting debiased
estimator, called IS-ICRF, satisfies a CLT centered at the prediction function
value. For high imbalance settings, we prove that the IS-ICRF estimator enjoys
a variance reduction compared to the ICRF trained on the original data.
Therefore, our theoretical analysis highlights the benefits of training random
forests on a rebalanced dataset (followed by a debiasing procedure) compared to
using the original data. Our theoretical results, especially the variance rates
and the variance reduction, appear to be valid for Breiman's random forests in
our experiments.

</details>


### [404] [Flexible and Efficient Drift Detection without Labels](https://arxiv.org/abs/2506.08734)
*Nelvin Tan,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: stat.ML

TL;DR: The paper proposes a label-less concept drift detection algorithm using statistical process control, offering better performance and efficiency compared to prior methods in computationally constrained settings.


<details>
  <summary>Details</summary>
Motivation: Automation in decision-making through machine learning demands efficient model monitoring to detect concept drift early, especially in scenarios where labeled data is not instantly available.

Method: The authors developed a label-less concept drift detection algorithm based on classical statistical process control and incorporated it into a novel framework for drift detection without labels.

Result: Empirical results showed that the proposed approach has better statistical power under computational constraints compared to existing methods. It also performed well in numerical simulations within the introduced framework.

Conclusion: The proposed algorithm effectively detects concept drift in scenarios without access to labels, improving detection accuracy and adaptability for large-scale data monitoring environments.

Abstract: Machine learning models are being increasingly used to automate decisions in
almost every domain, and ensuring the performance of these models is crucial
for ensuring high quality machine learning enabled services. Ensuring concept
drift is detected early is thus of the highest importance. A lot of research on
concept drift has focused on the supervised case that assumes the true labels
of supervised tasks are available immediately after making predictions.
Controlling for false positives while monitoring the performance of predictive
models used to make inference from extremely large datasets periodically, where
the true labels are not instantly available, becomes extremely challenging. We
propose a flexible and efficient concept drift detection algorithm that uses
classical statistical process control in a label-less setting to accurately
detect concept drifts. We shown empirically that under computational
constraints, our approach has better statistical power than previous known
methods. Furthermore, we introduce a new drift detection framework to model the
scenario of detecting drift (without labels) given prior detections, and show
our how our drift detection algorithm can be incorporated effectively into this
framework. We demonstrate promising performance via numerical simulations.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [405] [Flow-Lenia: Emergent evolutionary dynamics in mass conservative continuous cellular automata](https://arxiv.org/abs/2506.08569)
*Erwan Plantec,Gautier Hamon,Mayalen Etcheverry,Bert Wang-Chak Chan,Pierre-Yves Oudeyer,Clément Moulin-Frier*

Main category: nlin.CG

TL;DR: The paper introduces Flow-Lenia, an extension of Lenia, for generating complex, life-like patterns and studying their evolutionary behaviors in multispecies simulations.


<details>
  <summary>Details</summary>
Motivation: The research aims to advance artificial life by creating artificial systems that exhibit life-like properties such as autopoiesis, self-replication, and evolution, building on the success of cellular automata like Lenia.

Method: The authors developed Flow-Lenia, a mass-conservative version of Lenia, and demonstrated how its parameters could be optimized to generate complex spatially-localized patterns and multispecies interactions.

Result: Flow-Lenia successfully created spatially-localized patterns with complex behaviors and demonstrated emergent evolutionary dynamics through multispecies simulations.

Conclusion: Flow-Lenia serves as an effective and innovative tool for exploring life-like phenomena and evolutionary patterns, embedding its dynamics within the parameters of its models.

Abstract: Central to the artificial life endeavour is the creation of artificial
systems spontaneously generating properties found in the living world such as
autopoiesis, self-replication, evolution and open-endedness. While numerous
models and paradigms have been proposed, cellular automata (CA) have taken a
very important place in the field notably as they enable the study of
phenomenons like self-reproduction and autopoiesis. Continuous CA like Lenia
have been showed to produce life-like patterns reminiscent, on an aesthetic and
ontological point of view, of biological organisms we call creatures. We
propose in this paper Flow-Lenia, a mass conservative extension of Lenia. We
present experiments demonstrating its effectiveness in generating
spatially-localized patters (SLPs) with complex behaviors and show that the
update rule parameters can be optimized to generate complex creatures showing
behaviors of interest. Furthermore, we show that Flow-Lenia allows us to embed
the parameters of the model, defining the properties of the emerging patterns,
within its own dynamics thus allowing for multispecies simulations. By using
the evolutionary activity framework as well as other metrics, we shed light on
the emergent evolutionary dynamics taking place in this system.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [406] [MOSS: Multi-Objective Optimization for Stable Rule Sets](https://arxiv.org/abs/2506.08030)
*Brian Liu,Rahul Mazumder*

Main category: math.OC

TL;DR: MOSS is a framework focused on optimizing sparse rule sets by balancing accuracy, stability, and sparsity. It employs a cutting-plane algorithm to compute the trade-offs quickly.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in deriving sparse, interpretable, and stable decision rule sets, addressing the trade-offs often faced between accuracy and stability while maintaining practicality in explainable AI applications.

Method: MOSS uses a multi-objective optimization framework that incorporates sparsity, accuracy, and stability. It employs a cutting-plane algorithm to compute the Pareto frontier and is designed to handle computations beyond commercial solvers' capabilities.

Result: MOSS outperforms state-of-the-art rule ensembles in predictive performance and stability, providing an efficient way to balance interpretability and robustness.

Conclusion: MOSS is validated as an effective framework for optimizing sparse rule sets, providing a useful tool for practitioners who require a balance between interpretability, accuracy, and stability.

Abstract: We present MOSS, a multi-objective optimization framework for constructing
stable sets of decision rules. MOSS incorporates three important criteria for
interpretability: sparsity, accuracy, and stability, into a single
multi-objective optimization framework. Importantly, MOSS allows a practitioner
to rapidly evaluate the trade-off between accuracy and stability in sparse rule
sets in order to select an appropriate model. We develop a specialized cutting
plane algorithm in our framework to rapidly compute the Pareto frontier between
these two objectives, and our algorithm scales to problem instances beyond the
capabilities of commercial optimization solvers. Our experiments show that MOSS
outperforms state-of-the-art rule ensembles in terms of both predictive
performance and stability.

</details>


### [407] [Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence](https://arxiv.org/abs/2506.08121)
*Qi Feng,Gu Wang*

Main category: math.OC

TL;DR: The paper proposes an advanced algorithm to simultaneously update value functions and optimal controls for stochastic problems using Langevin dynamics.


<details>
  <summary>Details</summary>
Motivation: Current methods for solving stochastic control problems often face challenges in updating value functions and optimal controls effectively, especially in continuous spaces and infinite horizons.

Method: The authors develop a continuous policy-value iteration algorithm that incorporates Langevin-type dynamics for simultaneous updates, ensuring monotonicity of the Hamiltonian for policy improvement and convergence.

Result: They establish theoretical policy improvement and convergence guarantees while enabling practical implementation leveraging machine learning techniques like distribution sampling and non-convex optimization.

Conclusion: This approach significantly extends the scope of solving entropy-regularized and classical control problems by integrating stochastic dynamics with advanced learning methods.

Abstract: We introduce a continuous policy-value iteration algorithm where the
approximations of the value function of a stochastic control problem and the
optimal control are simultaneously updated through Langevin-type dynamics. This
framework applies to both the entropy-regularized relaxed control problems and
the classical control problems, with infinite horizon. We establish policy
improvement and demonstrate convergence to the optimal control under the
monotonicity condition of the Hamiltonian. By utilizing Langevin-type
stochastic differential equations for continuous updates along the policy
iteration direction, our approach enables the use of distribution sampling and
non-convex learning techniques in machine learning to optimize the value
function and identify the optimal control simultaneously.

</details>


### [408] [Solving Convex-Concave Problems with $\tilde{\mathcal{O}}(ε^{-4/7})$ Second-Order Oracle Complexity](https://arxiv.org/abs/2506.08362)
*Lesi Chen,Chengchang Liu,Luo Luo,Jingzhao Zhang*

Main category: math.OC

TL;DR: The study improves the complexity for solving convex-concave minimax problems from O(ε^(-2/3)) to Õ(ε^(-4/7)) using a novel second-order method.


<details>
  <summary>Details</summary>
Motivation: Existing convex-concave minimax problem-solving methods were considered optimal with O(ε^(-2/3)) complexity, creating a need to explore potential improvements.

Method: The paper generalizes the optimal second-order methods for convex optimization and adopts a Catalyst-like framework to accelerate minimax problem solvers.

Result: The proposed algorithm achieves an improved complexity upper bound of Õ(ε^(-4/7)), surpassing previous methods.

Conclusion: The research contributes a more efficient second-order algorithm for convex-concave minimax problems with broader applicability to globally convergent algorithms.

Abstract: Previous algorithms can solve convex-concave minimax problems $\min_{x \in
\mathcal{X}} \max_{y \in \mathcal{Y}} f(x,y)$ with
$\mathcal{O}(\epsilon^{-2/3})$ second-order oracle calls using Newton-type
methods. This result has been speculated to be optimal because the upper bound
is achieved by a natural generalization of the optimal first-order method. In
this work, we show an improved upper bound of
$\tilde{\mathcal{O}}(\epsilon^{-4/7})$ by generalizing the optimal second-order
method for convex optimization to solve the convex-concave minimax problem. We
further apply a similar technique to lazy Hessian algorithms and show that our
proposed algorithm can also be seen as a second-order ``Catalyst'' framework
(Lin et al., JMLR 2018) that could accelerate any globally convergent
algorithms for solving minimax problems.

</details>


### [409] [Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings](https://arxiv.org/abs/2506.08428)
*Evan Markou,Thalaiyasingam Ajanthan,Stephen Gould*

Main category: math.OC

TL;DR: The paper investigates how reduction mappings, which reparametrize part of the parameter space using structural information, can improve optimization convergence and condition properties.


<details>
  <summary>Details</summary>
Motivation: Optimization problems in high-dimensional parameter spaces often exhibit geometric structure in solution sets, but exploiting this structure to improve computation speed and accuracy remains challenging.

Method: The authors introduce a general analytical framework to study reduction mappings that reparameterize the parameter space and investigate their impact on optimization curvature and convergence properties.

Result: The study demonstrates that well-designed reduction mappings enhance curvature properties of objective functions, resulting in better-conditioned problems and faster convergence for gradient-based optimization methods.

Conclusion: The framework provides a unified understanding of how structural information at optimality expedites convergence in optimization algorithms, offering theoretical backing for observed empirical gains.

Abstract: Many high-dimensional optimisation problems exhibit rich geometric structures
in their set of minimisers, often forming smooth manifolds due to
over-parametrisation or symmetries. When this structure is known, at least
locally, it can be exploited through reduction mappings that reparametrise part
of the parameter space to lie on the solution manifold. These reductions
naturally arise from inner optimisation problems and effectively remove
redundant directions, yielding a lower-dimensional objective. In this work, we
introduce a general framework to understand how such reductions influence the
optimisation landscape. We show that well-designed reduction mappings improve
curvature properties of the objective, leading to better-conditioned problems
and theoretically faster convergence for gradient-based methods. Our analysis
unifies a range of scenarios where structural information at optimality is
leveraged to accelerate convergence, offering a principled explanation for the
empirical gains observed in such optimisation algorithms.

</details>


### [410] [Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees](https://arxiv.org/abs/2506.08558)
*William de Vazelhes,Xiao-Tong Yuan,Bin Gu*

Main category: math.OC

TL;DR: The paper introduces a novel algorithm for sparse optimization with mixed constraints, providing global guarantees and improved convergence analysis, addressing limitations of prior approaches.


<details>
  <summary>Details</summary>
Motivation: The need to offer controlled sparsity while addressing additional real-world constraints in sparse optimization, and overcoming the limitations of existing methods in terms of projection feasibility and global guarantees.

Method: A new iterative hard-thresholding algorithm using a two-step consecutive projection operator tailored for mixed constraints to replace traditional Euclidean projection, supported by extended proof techniques.

Result: Global guarantees in objective value under restricted convexity/smoothness assumptions for deterministic, stochastic, and zeroth-order setups, along with enhanced convergence tools removing prior errors in zeroth-order cases.

Conclusion: The study successfully bridges the gap in sparse optimization with mixed constraints by introducing a globally guaranteed method supported by novel analytical techniques, improving upon prior state-of-the-art results.

Abstract: In sparse optimization, enforcing hard constraints using the $\ell_0$
pseudo-norm offers advantages like controlled sparsity compared to convex
relaxations. However, many real-world applications demand not only sparsity
constraints but also some extra constraints. While prior algorithms have been
developed to address this complex scenario with mixed combinatorial and convex
constraints, they typically require the closed form projection onto the mixed
constraints which might not exist, and/or only provide local guarantees of
convergence which is different from the global guarantees commonly sought in
sparse optimization. To fill this gap, in this paper, we study the problem of
sparse optimization with extra \qw{\textit{support-preserving}} constraints
commonly encountered in the literature. We present a new variant of iterative
hard-thresholding algorithm equipped with a two-step consecutive projection
operator customized for these mixed constraints, serving as a simple
alternative to the Euclidean projection onto the mixed constraint. By
introducing a novel trade-off between sparsity relaxation and sub-optimality,
we provide global guarantees in objective value for the output of our
algorithm, in the deterministic, stochastic, and zeroth-order settings, under
the conventional restricted strong-convexity/smoothness assumptions. As a
fundamental contribution in proof techniques, we develop a novel extension of
the classic three-point lemma to the considered two-step non-convex projection
operator, which allows us to analyze the convergence in objective value in an
elegant way that has not been possible with existing techniques. In the
zeroth-order case, such technique also improves upon the state-of-the-art
result from de Vazelhes et. al. (2022), even in the case without additional
constraints, by allowing us to remove a non-vanishing system error present in
their work.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [411] [Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming](https://arxiv.org/abs/2506.08263)
*Pouya Agheli,Tugce Kobal,François Durand,Matthew Andrews*

Main category: cs.IT

TL;DR: The study addresses multiuser scheduling in MIMO-OFDM mmWave systems with hybrid beamforming, proposing methods to enhance proportional fairness by optimizing beamforming design and user selection.


<details>
  <summary>Details</summary>
Motivation: To improve spectral efficiency and long-term proportional fairness in hybrid beamforming systems, which face challenges due to limited multiplexing gain.

Method: The authors propose a two-timescale protocol: long timescale assigns analog beams to users, while short timescale handles user scheduling and digital precoder design using combinatorial algorithms and machine learning approaches.

Result: Numerical experiments reveal a trade-off between performance and complexity in the proposed approaches, dependent on scenario-specific criteria.

Conclusion: The proposed techniques are effective in enhancing system performance, and the optimal approach depends on the desired balance between complexity and performance in specific scenarios.

Abstract: We investigate the multiuser scheduling problem in multiple-input
multiple-output (MIMO) systems using orthogonal frequency division multiplexing
(OFDM) and hybrid beamforming in which a base station (BS) communicates with
multiple users over millimeter wave (mmWave) channels in the downlink. Improved
scheduling is critical for enhancing spectral efficiency and the long-term
performance of the system from the perspective of proportional fairness (PF)
metric in hybrid beamforming systems due to its limited multiplexing gain. Our
objective is to maximize PF by properly designing the analog and digital
precoders within the hybrid beamforming and selecting the users subject to the
number of radio frequency (RF) chains. Leveraging the characteristics of mmWave
channels, we apply a two-timescale protocol. On a long timescale, we assign an
analog beam to each user. Scheduling the users and designing the digital
precoder are done accordingly on a short timescale. To conduct scheduling, we
propose combinatorial solutions, such as greedy and sorting algorithms,
followed by a machine learning (ML) approach. Our numerical results highlight
the trade-off between the performance and complexity of the proposed
approaches. Consequently, we show that the choice of approach depends on the
specific criteria within a given scenario.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [412] [The enteric nervous system is 10 times stiffer than the brain](https://arxiv.org/abs/2506.08583)
*Nicolas R. Chevalier,Alexis Peaucelle,Thomas Guilbert,Pierre Bourdoncle,Wang Xi*

Main category: physics.bio-ph

TL;DR: The enteric nervous system is much more resistant to deformation compared to the brain due to its collagen shell and tissue composition.


<details>
  <summary>Details</summary>
Motivation: To investigate whether neurons and glia within the enteric nervous system exhibit distinctive mechanical properties to endure high-magnitude mechanical stresses.

Method: Nano-indentation, immunohistochemistry, and second harmonic generation imaging of collagen were used to measure mechanical properties of tissues.

Result: Enteric ganglia in adult mice showed significantly higher stiffness compared to brain tissue. Both glia-rich and neuron-rich regions displayed similar stiffness values, with additional structural support from surrounding collagen.

Conclusion: The enteric nervous system's enhanced stiffness and collagen shell enable it to withstand chronic mechanical forces in its environment.

Abstract: Neural tissues of the central nervous system are among the softest and most
fragile in the human body, protected from mechanical perturbation by the skull
and the spine. In contrast, the enteric nervous system is embedded in a
compliant, contractile tissue and subject to chronic, high-magnitude mechanical
stress. Do neurons and glia of the enteric nervous system display specific
mechanical properties to withstand these forces? Using nano-indentation
combined with immunohistochemistry and second harmonic generation imaging of
collagen, we discovered that enteric ganglia in adult mice are an order of
magnitude more resistant to deformation than brain tissue. We found that
glia-rich regions in ganglia have a similar stiffness to neuron-rich regions
and to the surrounding smooth muscle, of ~3 kPa at 3 $\mu$m indentation depth
and of ~7 kPa at 8 $\mu$m depth. Differences in the adhesion strength of the
different tissue layers to the glass indenter were scarce. The collagen shell
surrounding ganglia and inter-ganglionic fibers may play a key role in
strengthening the enteric nervous system to resist the manifold mechanical
challenges it faces.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [413] [CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction](https://arxiv.org/abs/2506.08059)
*Huong Van Le,Weibin Ren,Junhong Kim,Yukyung Yun,Young Bin Park,Young Jun Kim,Bok Kyung Han,Inho Choi,Jong IL Park,Hwi-Yeol Yun,Jae-Mun Choi*

Main category: q-bio.QM

TL;DR: The study examined eight molecular feature representations combined with automated machine learning (AutoML) to improve Caco-2 permeability prediction. Specific descriptors like PaDEL, Mordred, and RDKit were effective, and AutoML models, particularly CaliciBoost, excelled.


<details>
  <summary>Details</summary>
Motivation: The research aims to enhance oral drug absorption prediction during early-stage drug discovery by systematically evaluating different molecular feature representations and integrating automated machine learning techniques.

Method: Researchers tested eight molecular feature types such as 2D/3D descriptors, structural fingerprints, and deep learning embeddings. They evaluated these using two datasets and applied AutoML models for prediction analysis.

Result: The AutoML model CaliciBoost achieved the best performance (lowest mean absolute error, MAE). Adding 3D descriptors to PaDEL and Mordred yielded a 15.73% MAE reduction compared to 2D-only features.

Conclusion: The study demonstrates the potential of AutoML methods in ADMET modeling and provides insights into effective feature selection for small dataset predictions.

Abstract: Caco-2 permeability serves as a critical in vitro indicator for predicting
the oral absorption of drug candidates during early-stage drug discovery. To
enhance the accuracy and efficiency of computational predictions, we
systematically investigated the impact of eight molecular feature
representation types including 2D/3D descriptors, structural fingerprints, and
deep learning-based embeddings combined with automated machine learning
techniques to predict Caco-2 permeability. Using two datasets of differing
scale and diversity (TDC benchmark and curated OCHEM data), we assessed model
performance across representations and identified PaDEL, Mordred, and RDKit
descriptors as particularly effective for Caco-2 prediction. Notably, the
AutoML-based model CaliciBoost achieved the best MAE performance. Furthermore,
for both PaDEL and Mordred representations, the incorporation of 3D descriptors
resulted in a 15.73% reduction in MAE compared to using 2D features alone, as
confirmed by feature importance analysis. These findings highlight the
effectiveness of AutoML approaches in ADMET modeling and offer practical
guidance for feature selection in data-limited prediction tasks.

</details>


### [414] [Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction](https://arxiv.org/abs/2506.08954)
*Ruben Weitzman,Peter Mørch Groth,Lood Van Niekerk,Aoi Otani,Yarin Gal,Debora Marks,Pascal Notin*

Main category: q-bio.QM

TL;DR: Protriever is a novel framework that streamlines homologous protein sequence retrieval and training, achieving superior performance and speed compared to traditional alignment-based methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional MSA-based retrieval methods, including computational cost, inefficiency with divergent sequences, and lack of integration with downstream modeling tasks.

Method: The paper proposes Protriever, an end-to-end differentiable framework that combines homolog retrieval with target task training, employing efficient vector search instead of traditional MSAs.

Result: Protriever outperforms MSA-based models for protein fitness prediction and is two orders of magnitude faster, demonstrating its scalability and flexibility.

Conclusion: Protriever offers a scalable, adaptable, and efficient alternative to alignment-based protein sequence retrieval, positioning itself as a task-agnostic solution for a wide range of protein modeling tasks.

Abstract: Retrieving homologous protein sequences is essential for a broad range of
protein modeling tasks such as fitness prediction, protein design, structure
modeling, and protein-protein interactions. Traditional workflows have relied
on a two-step process: first retrieving homologs via Multiple Sequence
Alignments (MSA), then training models on one or more of these alignments.
However, MSA-based retrieval is computationally expensive, struggles with
highly divergent sequences or complex insertions & deletions patterns, and
operates independently of the downstream modeling objective. We introduce
Protriever, an end-to-end differentiable framework that learns to retrieve
relevant homologs while simultaneously training for the target task. When
applied to protein fitness prediction, Protriever achieves state-of-the-art
performance compared to sequence-based models that rely on MSA-based homolog
retrieval, while being two orders of magnitude faster through efficient vector
search. Protriever is both architecture- and task-agnostic, and can flexibly
adapt to different retrieval strategies and protein databases at inference time
-- offering a scalable alternative to alignment-centric approaches.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [415] [midr: Learning from Black-Box Models by Maximum Interpretation Decomposition](https://arxiv.org/abs/2506.08338)
*Ryoichi Asashiba,Reiji Kozuma,Hirokazu Iwasawa*

Main category: stat.ME

TL;DR: This paper presents the R package 'midr,' which supports explaining black-box machine learning models using Maximum Interpretation Decomposition (MID).


<details>
  <summary>Details</summary>
Motivation: Many fields requiring model explainability face challenges when dealing with black-box models. This work seeks to address this with interpretable and explainable ML tools.

Method: The authors developed the midr package that uses MID to derive a low-order additive model as a global surrogate for black-box prediction models.

Result: The midr package enables users to create interpretable models by minimizing the error between black-box model predictions and their additive surrogate models.

Conclusion: MID provides a structured method for interpreting black-box models, making the midr tool helpful for fostering adoption of complex ML models in explainability-critical domains.

Abstract: The use of appropriate methods of Interpretable Machine Learning (IML) and
eXplainable Artificial Intelligence (XAI) is essential for adopting black-box
predictive models in fields where model and prediction explainability is
required. As a novel tool for interpreting black-box models, we introduce the R
package midr, which implements Maximum Interpretation Decomposition (MID). MID
is a functional decomposition approach that derives a low-order additive
representation of a black-box model by minimizing the squared error between the
model's prediction function and this additive representation. midr enables
learning from black-box models by constructing a global surrogate model with
advanced analytical capabilities. After reviewing related work and the
theoretical foundation of MID, we demonstrate the package's usage and discuss
some of its key features.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [416] [syren-baryon: Analytic emulators for the impact of baryons on the matter power spectrum](https://arxiv.org/abs/2506.08783)
*Lukas Kammerer,Deaglan J. Bartlett,Gabriel Kronberger,Harry Desmond,Pedro G. Ferreira*

Main category: astro-ph.CO

TL;DR: This paper uses symbolic regression to create analytic expressions describing the effect of baryonic physics on the matter power spectrum, offering model-specific equations for forecasts and discrimination in cosmological analyses.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the impact of baryonic physics, which introduces systematic uncertainties in cosmological measurements, by providing simple and interpretable analytic functions for its influence on the matter power spectrum.

Method: They employed symbolic regression to derive analytic functions for the ratio of the matter power spectrum with and without baryonic effects, using data from multiple hydrodynamical simulation suites and a baryonification algorithm.

Result: The derived functions have errors comparable to previous numerical emulators and are physically consistent, demonstrating strong accuracy on large scales and at high redshifts. The authors also quantified uncertainties due to baryonic physics and fitting errors.

Conclusion: These symbolic approximations are accurate, interpretable, and useful for analyzing and discriminating between different baryonic feedback models. The publicly available code supports their broader application in cosmological studies.

Abstract: Baryonic physics has a considerable impact on the distribution of matter in
our Universe on scales probed by current and future cosmological surveys,
acting as a key systematic in such analyses. We seek simple symbolic
parametrisations for the impact of baryonic physics on the matter power
spectrum for a range of physically motivated models, as a function of
wavenumber, redshift, cosmology, and parameters controlling the baryonic
feedback. We use symbolic regression to construct analytic approximations for
the ratio of the matter power spectrum in the presence of baryons to that
without such effects. We obtain separate functions of each of four distinct
sub-grid prescriptions of baryonic physics from the CAMELS suite of
hydrodynamical simulations (Astrid, IllustrisTNG, SIMBA and Swift-EAGLE) as
well as for a baryonification algorithm. We also provide functions which
describe the uncertainty on these predictions, due to both the stochastic
nature of baryonic physics and the errors on our fits. The error on our
approximations to the hydrodynamical simulations is comparable to the sample
variance estimated through varying initial conditions, and our baryonification
expression has a root mean squared error of better than one percent, although
this increases on small scales. These errors are comparable to those of
previous numerical emulators for these models. Our expressions are enforced to
have the physically correct behaviour on large scales and at high redshift. Due
to their analytic form, we are able to directly interpret the impact of varying
cosmology and feedback parameters, and we can identify parameters which have
little to no effect. Each function is based on a different implementation of
baryonic physics, and can therefore be used to discriminate between these
models when applied to real data. We provide publicly available code for all
symbolic approximations found.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [417] [KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks](https://arxiv.org/abs/2506.08563)
*Siyuan Yang,Cheng Song,Zhilu Lai,Wenjia Wang*

Main category: cs.CE

TL;DR: The paper introduces KP-PINNs, a new framework to enhance the stability and efficiency of PINNs for solving differential equations.


<details>
  <summary>Details</summary>
Motivation: Address the instability and inaccuracy of the standard L2 loss function in PINNs for solving complex differential equations.

Method: Introduced a loss function based on the reproducing kernel Hilbert space (RKHS) norm and employed the Kernel Packet (KP) method to improve computational performance.

Result: KP-PINNs provided stable and efficient solutions for various differential equations as demonstrated by theoretical analysis and numerical experiments.

Conclusion: KP-PINNs show promise for enhancing the accuracy and stability of PINNs in scientific computing.

Abstract: Differential equations are involved in modeling many engineering problems.
Many efforts have been devoted to solving differential equations. Due to the
flexibility of neural networks, Physics Informed Neural Networks (PINNs) have
recently been proposed to solve complex differential equations and have
demonstrated superior performance in many applications. While the L2 loss
function is usually a default choice in PINNs, it has been shown that the
corresponding numerical solution is incorrect and unstable for some complex
equations. In this work, we propose a new PINNs framework named Kernel Packet
accelerated PINNs (KP-PINNs), which gives a new expression of the loss function
using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel
Packet (KP) method to accelerate the computation. Theoretical results show that
KP-PINNs can be stable across various differential equations. Numerical
experiments illustrate that KP-PINNs can solve differential equations
effectively and efficiently. This framework provides a promising direction for
improving the stability and accuracy of PINNs-based solvers in scientific
computing.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [418] [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/abs/2506.08524)
*Weiguo Wang,Andy Nie,Wenrui Zhou,Yi Kai,Chengchen Hu*

Main category: cs.SD

TL;DR: The paper addresses the lack of physical awareness in Large Language Models (LLMs) by introducing ACORN, a framework enhanced with sound-based physical phenomena such as the Doppler effect and spatial relationships.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) possess notable textual and multimodal processing abilities but lack understanding of real-world physical phenomena.

Method: The study introduces ACORN, which uses a physics-based simulator to generate diverse sound-based training data and proposes an audio encoder to incorporate magnitude and phase information. The encoder is integrated with LLMs.

Result: ACORN demonstrates reasonable performance in tasks such as line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation, both in simulated and real-world scenarios.

Conclusion: This research establishes a pathway for LLMs to comprehend physical phenomena through audio processing, enhancing their real-world application capabilities.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and
multimodal processing, yet they fundamentally lack physical
awareness--understanding of real-world physical phenomena. In this work, we
present ACORN, a framework that teaches LLMs physical awareness through sound,
focusing on fundamental physical phenomena like the Doppler effect, multipath
effect, and spatial relationships. To overcome data scarcity, ACORN introduce a
physics-based simulator combining real-world sound sources with controlled
physical channels to generate diverse training data. Using this simulator, we
build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an
audio encoder that processes both magnitude and phase information. By
connecting our audio encoder to state-of-the-art LLMs, we demonstrate
reasonable results in both simulated and real-world tasks, such as
line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival
estimation, paving the way for enabling LLMs to understand physical world.

</details>


### [419] [SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models](https://arxiv.org/abs/2506.08346)
*Wenhan Yao,Fen Xiao,Xiarun Chen,Jia Liu,YongQiang He,Weiping Wen*

Main category: cs.SD

TL;DR: The paper explores vulnerabilities in speech classification tasks by proposing a Speech Prompt Backdoor Attack (SPBA) utilizing speech timbre and emotion triggers generated via Speech Large Language Models.


<details>
  <summary>Details</summary>
Motivation: To address security vulnerabilities in speech classification tasks, which are increasingly vital in human-computer interaction but prone to backdoor attacks.

Method: The paper implements a Speech Prompt Backdoor Attack (SPBA) using diverse triggers generated by Speech Large Language Models and mitigates its challenges using the Multiple Gradient Descent Algorithm (MGDA).

Result: The SPBA demonstrates high effectiveness in triggering backdoors and achieves superior performance metrics in attack experiments on keyword spotting and speaker verification tasks.

Conclusion: SPBA highlights potential security risks in speech-based models, emphasizing the need to address these vulnerabilities and offering MGDA as a mitigation solution.

Abstract: Deep speech classification tasks, including keyword spotting and speaker
verification, are vital in speech-based human-computer interaction. Recently,
the security of these technologies has been revealed to be susceptible to
backdoor attacks. Specifically, attackers use noisy disruption triggers and
speech element triggers to produce poisoned speech samples that train models to
become vulnerable. However, these methods typically create only a limited
number of backdoors due to the inherent constraints of the trigger function. In
this paper, we propose that speech backdoor attacks can strategically focus on
speech elements such as timbre and emotion, leveraging the Speech Large
Language Model (SLLM) to generate diverse triggers. Increasing the number of
triggers may disproportionately elevate the poisoning rate, resulting in higher
attack costs and a lower success rate per trigger. We introduce the Multiple
Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this
challenge. The proposed attack is called the Speech Prompt Backdoor Attack
(SPBA). Building on this foundation, we conducted attack experiments on two
speech classification tasks, demonstrating that SPBA shows significant trigger
effectiveness and achieves exceptional performance in attack metrics.

</details>


### [420] [MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion](https://arxiv.org/abs/2506.08357)
*Franck Meyer,Kyunghoon Hur,Edward Choi*

Main category: cs.SD

TL;DR: The paper introduces MD-ViSCo, a unified model capable of transforming one vital sign waveform into another, outperforming existing models in accuracy and versatility.


<details>
  <summary>Details</summary>
Motivation: Existing deep-learning models for vital sign waveform conversion are task-specific, requiring separate architectures and setups, limiting clinical usability.

Method: The researchers propose MD-ViSCo, a shallow 1D U-Net combined with a Swin Transformer and Adaptive Instance Normalization for waveform style adaptation, tested using multi-directional waveform generation on public datasets.

Result: MD-ViSCo improves average Mean Absolute Error (MAE) by 8.8% and Pearson Correlation (PC) by 4.9% over state-of-the-art methods on two datasets and satisfies clinical standards like AAMI and BHS criteria.

Conclusion: MD-ViSCo simplifies healthcare monitoring by providing a unified framework for converting vital sign waveforms, having clinical potential without requiring task-specific models.

Abstract: Despite the remarkable progress of deep-learning methods generating a target
vital sign waveform from a source vital sign waveform, most existing models are
designed exclusively for a specific source-to-target pair. This requires
distinct model architectures, optimization procedures, and pre-processing
pipelines, resulting in multiple models that hinder usability in clinical
settings. To address this limitation, we propose the Multi-Directional
Vital-Sign Converter (MD-ViSCo), a unified framework capable of generating any
target waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or
arterial blood pressure (ABP) from any single input waveform with a single
model. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin
Transformer that leverages Adaptive Instance Normalization (AdaIN) to capture
distinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct
multi-directional waveform generation on two publicly available datasets. Our
framework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average
across all waveform types, lowering Mean absolute error (MAE) by 8.8% and
improving Pearson correlation (PC) by 4.9% over two datasets. In addition, the
generated ABP waveforms satisfy the Association for the Advancement of Medical
Instrumentation (AAMI) criterion and achieve Grade B on the British
Hypertension Society (BHS) standard, outperforming all baselines. By
eliminating the need for developing a distinct model for each task, we believe
that this work offers a unified framework that can deal with any kind of vital
sign waveforms with a single model in healthcare monitoring.

</details>


### [421] [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/abs/2506.08967)
*Ailin Huang,Bingxin Li,Bruce Wang,Boyong Wu,Chao Yan,Chengli Feng,Heng Wang,Hongyu Zhou,Hongyuan Wang,Jingbei Li,Jianjian Sun,Joanna Wang,Mingrui Chen,Peng Liu,Ruihang Miao,Shilei Jiang,Tian Fei,Wang You,Xi Chen,Xuerui Yang,Yechang Huang,Yuxiang Zhang,Zheng Ge,Zheng Gong,Zhewei Huang,Zixin Zhang,Bin Wang,Bo Li,Buyun Ma,Changxin Miao,Changyi Wan,Chen Xu,Dapeng Shi,Dingyuan Hu,Enle Liu,Guanzhe Huang,Gulin Yan,Hanpeng Hu,Haonan Jia,Jiahao Gong,Jiaoren Wu,Jie Wu,Jie Yang,Junzhe Lin,Kaixiang Li,Lei Xia,Longlong Gu,Ming Li,Nie Hao,Ranchen Ming,Shaoliang Pang,Siqi Liu,Song Yuan,Tiancheng Cao,Wen Li,Wenqing He,Xu Zhao,Xuelin Zhang,Yanbo Yu,Yinmin Zhong,Yu Zhou,Yuanwei Liang,Yuanwei Lu,Yuxiang Yang,Zidong Yang,Zili Zhang,Binxing Jiao,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Xinhao Zhang,Yibo Zhu,Daxin Jiang,Shuchang Zhou,Chen Hu*

Main category: cs.SD

TL;DR: The paper introduces Step-Audio-AQAA, an end-to-end large audio-language model designed for audio query and audio answer tasks, overcoming limitations in natural speech generation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current large audio-language models (LALMs), which rely heavily on text-based outputs and struggle with direct, natural speech response generation in audio interactions.

Method: The model integrates a dual-codebook audio tokenizer for feature extraction, a 130-billion-parameter language model backbone, and a neural vocoder for speech synthesis. Its training includes interleaved text-audio token output and optimization via Direct Preference Optimization (DPO).

Result: Evaluated on the StepEval-Audio-360 benchmark, Step-Audio-AQAA outperforms current state-of-the-art LALMs, particularly excelling in semantic coherence and speech control.

Conclusion: This work advances end-to-end LALMs with its innovative use of token-based vocoders and sets a new standard for AQAA tasks, enabling improved human-computer audio interaction.

Abstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent
human-computer interaction, yet their reliance on text-based outputs limits
their ability to generate natural speech responses directly, hindering seamless
audio interactions. To address this, we introduce Step-Audio-AQAA, a fully
end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model
integrates a dual-codebook audio tokenizer for linguistic and semantic feature
extraction, a 130-billion-parameter backbone LLM and a neural vocoder for
high-fidelity speech synthesis. Our post-training approach employs interleaved
token-output of text and audio to enhance semantic coherence and combines
Direct Preference Optimization (DPO) with model merge to improve performance.
Evaluations on the StepEval-Audio-360 benchmark demonstrate that
Step-Audio-AQAA excels especially in speech control, outperforming the
state-of-art LALMs in key areas. This work contributes a promising solution for
end-to-end LALMs and highlights the critical role of token-based vocoder in
enhancing overall performance for AQAA tasks.

</details>


### [422] [Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation](https://arxiv.org/abs/2506.08570)
*Or Tal,Felix Kreuk,Yossi Adi*

Main category: cs.SD

TL;DR: This paper conducts a systematic comparison of two paradigms in text-to-music generation: Auto-Regressive decoding and Conditional Flow-Matching, under controlled settings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the difficulty of fairly evaluating diverse state-of-the-art text-to-music models, focusing specifically on paradigms rather than datasets or architectures.

Method: Using identical datasets, training configurations, and similar architectures, the study compares the paradigms based on multiple generation quality factors, inference robustness, scalability, conditioning adherence, and audio inpainting capabilities.

Result: Distinct strengths and weaknesses of both paradigms were identified, shedding light on which factors influence text-to-music generation performance.

Conclusion: The findings offer actionable insights for future text-to-music systems development, guiding architectural and training decisions.

Abstract: Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly across many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and pinpoint which design choices most
influence performance. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: Auto-Regressive decoding and Conditional
Flow-Matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [423] [PoSyn: Secure Power Side-Channel Aware Synthesis](https://arxiv.org/abs/2506.08252)
*Amisha Srivastava,Samit S. Miftah,Hyunmin Kim,Debjit Pal,Kanad Basu*

Main category: cs.CR

TL;DR: The paper presents PoSyn, a novel logic synthesis framework to improve resistance of cryptographic hardware against Power Side-Channel (PSC) attacks by optimizing RTL-to-standard cell mapping, resulting in reduced attack success rates and improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Power Side-Channel (PSC) attacks compromise sensitive information in cryptographic operations, and traditional countermeasures like masking face issues such as high overhead and susceptibility during synthesis. A better integrated solution is needed to bolster security.

Method: The paper introduces PoSyn, a logic synthesis framework that modifies RTL-to-netlist mapping with a cost function considering both RTL and standard cell library characteristics to minimize PSC leakage without affecting design functionality.

Result: PoSyn was tested on various cryptographic algorithms (AES, RSA, Saber, etc.) and achieved a significant reduction in Differential Power Analysis (DPA) and Correlation Power Analysis (CPA) success rates to 3% and 6%, respectively, with negligible leakage as confirmed by TVLA analysis. It also reduced attack success rates by up to 72% compared to masking and shuffling, while improving area efficiency by up to 3.79 times.

Conclusion: PoSyn offers an effective and efficient countermeasure to PSC vulnerabilities, outperforming traditional methods in reducing attack success rates and design overhead, while ensuring strong resistance and functionality retention.

Abstract: Power Side-Channel (PSC) attacks exploit power consumption patterns to
extract sensitive information, posing risks to cryptographic operations crucial
for secure systems. Traditional countermeasures, such as masking, face
challenges including complex integration during synthesis, substantial area
overhead, and susceptibility to optimization removal during logic synthesis. To
address these issues, we introduce PoSyn, a novel logic synthesis framework
designed to enhance cryptographic hardware resistance against PSC attacks. Our
method centers on optimal bipartite mapping of vulnerable RTL components to
standard cells from the technology library, aiming to minimize PSC leakage. By
utilizing a cost function integrating critical characteristics from both the
RTL design and the standard cell library, we strategically modify mapping
criteria during RTL-to-netlist conversion without altering design
functionality. Furthermore, we theoretically establish that PoSyn minimizes
mutual information leakage, strengthening its security against PSC
vulnerabilities. We evaluate PoSyn across various cryptographic hardware
implementations, including AES, RSA, PRESENT, and post-quantum cryptographic
algorithms such as Saber and CRYSTALS-Kyber, at technology nodes of 65nm, 45nm,
and 15nm. Experimental results demonstrate a substantial reduction in success
rates for Differential Power Analysis (DPA) and Correlation Power Analysis
(CPA) attacks, achieving lows of 3% and 6%, respectively. TVLA analysis further
confirms that synthesized netlists exhibit negligible leakage. Additionally,
compared to conventional countermeasures like masking and shuffling, PoSyn
significantly lowers attack success rates, achieving reductions of up to 72%,
while simultaneously enhancing area efficiency by as much as 3.79 times.

</details>


### [424] [ZTaint-Havoc: From Havoc Mode to Zero-Execution Fuzzing-Driven Taint Inference](https://arxiv.org/abs/2506.08838)
*Yuchong Xie,Wenhui Zhang,Dongdong She*

Main category: cs.CR

TL;DR: This paper introduces ZTaint-Havoc, a method to identify hot bytes for fuzz testing, leveraging a lightweight approach that minimizes runtime overhead while enhancing edge coverage.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to address the challenge of identifying hot bytes that affect program behavior in fuzzing, while avoiding the scalability issues of white-box taint analysis and runtime overheads of black-box alternatives.

Method: The paper proposes adapting the traditional havoc mutation scheme of fuzzing for black-box Fuzzing-Driven Taint Inference (FTI) without additional program executions. A computational model and an efficient mutation algorithm are developed based on this approach.

Result: ZTaint-Havoc achieves minimal runtime overhead (3.84% on UniBench, 12.58% on FuzzBench) and improves edge coverage significantly, by up to 33.71% on FuzzBench and 51.12% on UniBench, with average gains in 24-hour campaigns.

Conclusion: ZTaint-Havoc offers an efficient and low-overhead solution for Fuzzing-Driven Taint Inference (FTI), enhancing the capability of fuzz testing through improved edge coverage and hot byte identification.

Abstract: Fuzzing is a widely used technique for discovering software vulnerabilities,
but identifying hot bytes that influence program behavior remains challenging.
Traditional taint analysis can track such bytes white-box, but suffers from
scalability issue. Fuzzing-Driven Taint Inference (FTI) offers a black-box
alternative, yet typically incurs significant runtime overhead due to extra
program executions. We observe that the commonly used havoc mutation scheme in
fuzzing can be adapted for lightweight FTI with zero extra executions. We
present a computational model of havoc mode, demonstrating that it can perform
FTI while generating new test cases. Building on this, we propose ZTaint-Havoc,
a novel, efficient FTI with minimal overhead (3.84% on UniBench, 12.58% on
FuzzBench). We further design an effective mutation algorithm utilizing the
identified hot bytes. Our comprehensive evaluation shows that ZTaint-Havoc,
implemented in AFL++, improves edge coverage by up to 33.71% on FuzzBench and
51.12% on UniBench over vanilla AFL++, with average gains of 2.97% and 6.12% in
24-hour fuzzing campaigns.

</details>


### [425] [GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors](https://arxiv.org/abs/2506.08188)
*Wenlong Meng,Shuguo Fan,Chengkun Wei,Min Chen,Yuwei Li,Yuanchao Zhang,Zhikun Zhang,Wenzhi Chen*

Main category: cs.CR

TL;DR: This paper introduces GradEscape, a novel gradient-based approach to evade AI-generated text detectors, achieving high success with minimal adjustments and addressing tokenizer mismatches.


<details>
  <summary>Details</summary>
Motivation: To counter the increasing effectiveness of AI-generated text detectors by developing a more adaptable and less detectable evasion method.

Method: GradEscape uses weighted embeddings to bypass the non-differentiability of text and continuously updates its parameters using feedback from detectors. It also utilizes warm-start techniques for tokenizer mismatches and employs model extraction and inference for query-only detection scenarios.

Result: GradEscape outperforms existing methods in evading detection by AI-generated text detectors, shows robustness across datasets and language models, and works effectively even with commercial detectors.

Conclusion: GradEscape demonstrates effectiveness and adaptability in overcoming current vulnerabilities of AIGT detectors, inspiring future research into more robust detector development strategies.

Abstract: In this paper, we introduce GradEscape, the first gradient-based evader
designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the
undifferentiable computation problem, caused by the discrete nature of text, by
introducing a novel approach to construct weighted embeddings for the detector
input. It then updates the evader model parameters using feedback from victim
detectors, achieving high attack success with minimal text modification. To
address the issue of tokenizer mismatch between the evader and the detector, we
introduce a warm-started evader method, enabling GradEscape to adapt to
detectors across any language model architecture. Moreover, we employ novel
tokenizer inference and model extraction techniques, facilitating effective
evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language
models, benchmarking it against four state-of-the-art AIGT evaders.
Experimental results demonstrate that GradEscape outperforms existing evaders
in various scenarios, including with an 11B paraphrase model, while utilizing
only 139M parameters. We have successfully applied GradEscape to two real-world
commercial AIGT detectors. Our analysis reveals that the primary vulnerability
stems from disparity in text expression styles within the training data. We
also propose a potential defense strategy to mitigate the threat of AIGT
evaders. We open-source our GradEscape for developing more robust AIGT
detectors.

</details>


### [426] [How Good LLM-Generated Password Policies Are?](https://arxiv.org/abs/2506.08320)
*Vivek Vaidya,Aditya Patwardhan,Ashish Kundu*

Main category: cs.CR

TL;DR: This paper examines the use of Large Language Models (LLMs) for generating consistent and secure password policy configurations in Cybersecurity Access Control Systems, identifying significant challenges in their accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: The rise of Generative AI and LLMs has brought remarkable capabilities to the field of natural language processing; however, their inconsistency and unpredictability pose risks, particularly in secure domains like access control.

Method: The authors experimented with LLMs to generate password policy configurations from natural language prompts, both without guidance and with the inclusion of official configuration documentation. They evaluated the soundness, accuracy, and consistency of the generated outputs.

Result: The study reveals significant challenges with the accuracy and consistency of LLM-generated password policy configurations, indicating limitations in their current application for access control systems.

Conclusion: Current LLMs face critical limitations in secure and reliable configurations for Cybersecurity Access Control Systems, necessitating further research and refinement for practical deployment.

Abstract: Generative AI technologies, particularly Large Language Models (LLMs), are
rapidly being adopted across industry, academia, and government sectors, owing
to their remarkable capabilities in natural language processing. However,
despite their strengths, the inconsistency and unpredictability of LLM outputs
present substantial challenges, especially in security-critical domains such as
access control. One critical issue that emerges prominently is the consistency
of LLM-generated responses, which is paramount for ensuring secure and reliable
operations.
  In this paper, we study the application of LLMs within the context of
Cybersecurity Access Control Systems. Specifically, we investigate the
consistency and accuracy of LLM-generated password policies, translating
natural language prompts into executable pwquality.conf configuration files.
Our experimental methodology adopts two distinct approaches: firstly, we
utilize pre-trained LLMs to generate configuration files purely from natural
language prompts without additional guidance. Secondly, we provide these models
with official pwquality.conf documentation to serve as an informative baseline.
We systematically assess the soundness, accuracy, and consistency of these
AI-generated configurations. Our findings underscore significant challenges in
the current generation of LLMs and contribute valuable insights into refining
the deployment of LLMs in Access Control Systems.

</details>


### [427] [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336)
*Li Changjiang,Liang Jiacheng,Cao Bochuan,Chen Jinghui,Wang Ting*

Main category: cs.CR

TL;DR: The paper introduces ReAgent, a defense mechanism for mitigating backdoor attacks in Large Language Model (LLM)-based agents by identifying inconsistencies during planning and execution phases.


<details>
  <summary>Details</summary>
Motivation: Large Language Model agents are vulnerable to backdoor attacks during training or fine-tuning, leading to potential exploitation when triggered by specific inputs or contexts.

Method: ReAgent employs a two-level approach to detect backdoors: verifying consistency between the agent's thoughts and actions (execution level) and checking alignment between reconstructed instructions and the user's original instructions (planning level).

Result: ReAgent demonstrates high effectiveness, reducing attack success rates by up to 90% in database operation tasks, significantly surpassing existing defensive solutions.

Conclusion: ReAgent showcases the viability of utilizing LLM-based agents themselves for defending against backdoor attacks, emphasizing the importance of consistency checks in both planning and execution phases.

Abstract: Despite their growing adoption across domains, large language model
(LLM)-powered agents face significant security risks from backdoor attacks
during training and fine-tuning. These compromised agents can subsequently be
manipulated to execute malicious operations when presented with specific
triggers in their inputs or environments. To address this pressing risk, we
present ReAgent, a novel defense against a range of backdoor attacks on
LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies
among the user's instruction, the agent's planning, and its execution. Drawing
on this insight, ReAgent employs a two-level approach to detect potential
backdoors. At the execution level, ReAgent verifies consistency between the
agent's thoughts and actions; at the planning level, ReAgent leverages the
agent's capability to reconstruct the instruction based on its thought
trajectory, checking for consistency between the reconstructed instruction and
the user's instruction. Extensive evaluation demonstrates ReAgent's
effectiveness against various backdoor attacks across tasks. For instance,
ReAgent reduces the attack success rate by up to 90\% in database operation
tasks, outperforming existing defenses by large margins. This work reveals the
potential of utilizing compromised agents themselves to mitigate backdoor
risks.

</details>


### [428] [WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks](https://arxiv.org/abs/2506.08602)
*Tingzhi Li,Xuefeng Liu*

Main category: cs.CR

TL;DR: The paper introduces WGLE, a novel watermarking paradigm for Graph Neural Networks (GNNs) that embeds ownership information without using backdoors.


<details>
  <summary>Details</summary>
Motivation: Existing methods of watermarking for GNNs, such as fingerprinting and black-box watermarking, have limitations. Fingerprinting is computationally expensive and vulnerable to model modifications, while backdoor-based methods risk security compromises. Both approaches lack efficiency and additional data embedding capabilities.

Method: WGLE employs the Layer-wise Distance Difference on an Edge (LDDE) to embed multi-bit ownership information into GNNs. Predefined positive or negative LDDE values are used for embedding without disrupting the primary tasks of the GNN.

Result: Evaluations on six datasets and six GNN architectures demonstrate that WGLE offers 100% ownership verification accuracy, only 0.85% average fidelity degradation, robustness against attacks, and low embedding overhead.

Conclusion: WGLE resolves key drawbacks of existing methods by providing an effective and secure approach for watermarking GNNs without backdoors, ensuring high accuracy, robustness, and scalability.

Abstract: Graph Neural Networks (GNNs) are increasingly deployed in graph-related
applications, making ownership verification critical to protect their
intellectual property against model theft. Fingerprinting and black-box
watermarking are two main methods. However, the former relies on determining
model similarity, which is computationally expensive and prone to ownership
collisions after model post-processing such as model pruning or fine-tuning.
The latter embeds backdoors, exposing watermarked models to the risk of
backdoor attacks. Moreover, both methods enable ownership verification but do
not convey additional information. As a result, each distributed model requires
a unique trigger graph, and all trigger graphs must be used to query the
suspect model during verification. Multiple queries increase the financial cost
and the risk of detection.
  To address these challenges, this paper proposes WGLE, a novel black-box
watermarking paradigm for GNNs that enables embedding the multi-bit string as
the ownership information without using backdoors. WGLE builds on a key insight
we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the
difference between the feature distance and the prediction distance of two
connected nodes. By predefining positive or negative LDDE values for multiple
selected edges, WGLE embeds the watermark encoding the intended information
without introducing incorrect mappings that compromise the primary task. WGLE
is evaluated on six public datasets and six mainstream GNN architectures along
with state-of-the-art methods. The results show that WGLE achieves 100%
ownership verification accuracy, an average fidelity degradation of 0.85%,
comparable robustness against potential attacks, and low embedding overhead.
The code is available in the repository.

</details>


### [429] [Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms](https://arxiv.org/abs/2506.08192)
*Jared Claypoole,Steven Cheung,Ashish Gehani,Vinod Yegneswaran,Ahmad Ridley*

Main category: cs.CR

TL;DR: The paper evaluates deep reinforcement learning agents in a cyber defense simulation, analyzing their actions and effectiveness in defending against rule-based attacks.


<details>
  <summary>Details</summary>
Motivation: To understand the strengths and weaknesses of deep reinforcement learning agents in dealing with simulated cyber attacks and improve interpretability of their actions.

Method: Analyzed two agents submitted to the CAGE Challenge 2, simplifying state and action spaces, and tracking significant events to assess their performance.

Result: Identified patterns in attack and defense behaviors, demonstrated the effectiveness of decoy services (up to 94% successful), and revealed that some actions were up to 99% ineffective.

Conclusion: The paper offers insights into improving cyber defense agents, evaluates attack/defense dynamics, and highlights areas for improvement in future challenges.

Abstract: We analyze two open source deep reinforcement learning agents submitted to
the CAGE Challenge 2 cyber defense challenge, where each competitor submitted
an agent to defend a simulated network against each of several provided
rules-based attack agents. We demonstrate that one can gain interpretability of
agent successes and failures by simplifying the complex state and action spaces
and by tracking important events, shedding light on the fine-grained behavior
of both the defense and attack agents in each experimental scenario. By
analyzing important events within an evaluation episode, we identify patterns
in infiltration and clearing events that tell us how well the attacker and
defender played their respective roles; for example, defenders were generally
able to clear infiltrations within one or two timesteps of a host being
exploited. By examining transitions in the environment's state caused by the
various possible actions, we determine which actions tended to be effective and
which did not, showing that certain important actions are between 40% and 99%
ineffective. We examine how decoy services affect exploit success, concluding
for instance that decoys block up to 94% of exploits that would directly grant
privileged access to a host. Finally, we discuss the realism of the challenge
and ways that the CAGE Challenge 4 has addressed some of our concerns.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [430] [QUITE: A Query Rewrite System Beyond Rules with LLM Agents](https://arxiv.org/abs/2506.07675)
*Yuyang Song,Hanxu Yan,Jiale Lao,Yibo Wang,Yufei Li,Yuanchun Zhou,Jianguo Wang,Mingjie Tang*

Main category: cs.DB

TL;DR: The paper introduces QUITE, a training-free, feedback-aware system utilizing large language models (LLMs) to rewrite SQL queries for better performance, overcoming limitations of rule-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing rule-based SQL query rewrite approaches have limited coverage, are challenging to extend, and can cause performance regressions. The motivation is to leverage the human-like reasoning abilities of LLMs to automate query rewrites more broadly and efficiently.

Method: QUITE combines a multi-agent framework controlled by a finite state machine (FSM), a specialized rewrite middleware, and a hint injection technique to rewrite queries. This approach incorporates real-time database feedback, tool integration, and optimization techniques to improve query performance.

Result: QUITE achieves a reduction of up to 35.8% in query execution time compared to state-of-the-art methods and generates 24.1% more optimized rewrites, handling broader query patterns.

Conclusion: The proposed system demonstrates the potential of LLMs, paired with supporting mechanisms, to surpass traditional rule-based methods in SQL query rewriting, enhancing both performance and query coverage.

Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that
run more efficiently. Existing approaches mainly rely on predefined rewrite
rules, but they handle a limited subset of queries and can cause performance
regressions. This limitation stems from three challenges of rule-based query
rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite
rules do not generalize to new query patterns, and (3) some rewrite techniques
cannot be expressed as fixed rules. Motivated by the fact that human experts
exhibit significantly better rewrite ability but suffer from scalability, and
Large Language Models (LLMs) have demonstrated nearly human-level semantic and
reasoning abilities, we propose a new approach of using LLMs to rewrite SQL
queries beyond rules. Due to the hallucination problems in LLMs, directly
applying LLMs often leads to nonequivalent and suboptimal queries. To address
this issue, we propose QUITE (query rewrite), a training-free and
feedback-aware system based on LLM agents that rewrites SQL queries into
semantically equivalent forms with significantly better performance, covering a
broader range of query patterns and rewrite strategies compared to rule-based
methods. Firstly, we design a multi-agent framework controlled by a finite
state machine (FSM) to equip LLMs with the ability to use external tools and
enhance the rewrite process with real-time database feedback. Secondly, we
develop a rewrite middleware to enhance the ability of LLMs to generate
optimized query equivalents. Finally, we employ a novel hint injection
technique to improve execution plans for rewritten queries. Extensive
experiments show that QUITE reduces query execution time by up to 35.8% over
state-of-the-art approaches and produces 24.1% more rewrites than prior
methods, covering query cases that earlier systems did not handle.

</details>


### [431] [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
*Ken Gu,Zhihan Zhang,Kate Lin,Yuwei Zhang,Akshay Paruchuri,Hong Yu,Mehran Kazemi,Kumar Ayush,A. Ali Heydari,Maxwell A. Xu,Girish Narayanswamy,Yun Liu,Ming-Zher Poh,Yuzhe Yang,Mark Malhotra,Shwetak Patel,Hamid Palangi,Xuhai Xu,Daniel McDuff,Tim Althoff,Xin Liu*

Main category: cs.DB

TL;DR: The paper introduces RADAR, a benchmark to test how well language models handle data artifacts in tabular data. It reveals significant performance drops in models when faced with artifacts.


<details>
  <summary>Details</summary>
Motivation: Language models are increasingly used for autonomous data analysis. However, they often fail to handle common real-world data artifacts like missing values, outliers, and logical inconsistencies in tabular data.

Method: The authors create a benchmark called RADAR that generates data artifacts using programmatic perturbations. It evaluates model performance on 2980 table-query pairs across 9 domains and 5 artifact types. Table size is also systematically varied.

Result: Frontier language models perform well on clean tables but show significant degradation in performance when data artifacts are introduced, highlighting major gaps in their data-aware reasoning capabilities.

Conclusion: RADAR serves as a comprehensive and extensible benchmark to explore and improve data-aware reasoning in language models, particularly for robustness in real-world tabular data scenarios.

Abstract: Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.

</details>


### [432] [LEANN: A Low-Storage Vector Index](https://arxiv.org/abs/2506.08276)
*Yichuan Wang,Shu Liu,Zhifei Li,Yongji Wu,Ziming Mao,Yilong Zhao,Xiao Yan,Zhiying Xu,Yang Zhou,Ion Stoica,Sewon Min,Matei Zaharia,Joseph E. Gonzalez*

Main category: cs.DB

TL;DR: The paper introduces LEANN, a storage-efficient ANN search index for personal devices, dramatically reducing storage needs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of high storage overhead in embedding-based search, while enabling local deployment on resource-constrained personal devices.

Method: LEANN combines a compact graph-based structure with an on-the-fly recomputation strategy for efficient embedding-based search.

Result: LEANN reduces storage overhead to under 5% of the original data, achieving up to 50x smaller storage while maintaining 90% top-3 recall within 2 seconds on benchmarks.

Conclusion: LEANN makes embedding-based search feasible on personal devices by offering drastic storage reduction without sacrificing retrieval quality or latency.

Abstract: Embedding-based search is widely used in applications such as recommendation
and retrieval-augmented generation (RAG). Recently, there is a growing demand
to support these capabilities over personal data stored locally on devices.
However, maintaining the necessary data structure associated with the
embedding-based search is often infeasible due to its high storage overhead.
For example, indexing 100 GB of raw data requires 150 to 700 GB of storage,
making local deployment impractical. Reducing this overhead while maintaining
search quality and latency becomes a critical challenge. In this paper, we
present LEANN, a storage-efficient approximate nearest neighbor (ANN) search
index optimized for resource-constrained personal devices. LEANN combines a
compact graph-based structure with an efficient on-the-fly recomputation
strategy to enable fast and accurate retrieval with minimal storage overhead.
Our evaluation shows that LEANN reduces index size to under 5% of the original
raw data, achieving up to 50 times smaller storage than standard indexes, while
maintaining 90% top-3 recall in under 2 seconds on real-world question
answering benchmarks.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [433] [Generalizing while preserving monotonicity in comparison-based preference learning models](https://arxiv.org/abs/2506.08616)
*Julien Fageot,Peva Blanchard,Gilles Bareilles,Lê-Nguyên Hoang*

Main category: math.ST

TL;DR: The paper introduces a new class of preference learning models that are monotone and capable of generalization, addressing limitations in existing models.


<details>
  <summary>Details</summary>
Motivation: Preference learning models, especially widely deployed ones, often lack monotonicity guarantees despite being fed preference orders. Improving this is crucial for better model behavior.

Method: The authors propose Linear Generalized Bradley-Terry models integrated with Diffusion Priors and specify conditions on embeddings to ensure monotonicity.

Result: The newly proposed models demonstrate better accuracy, particularly in data-constrained scenarios, while maintaining monotonicity.

Conclusion: The work successfully expands the scope of monotone comparison-based preference learning models, introducing a solution that balances generalization and accuracy.

Abstract: If you tell a learning model that you prefer an alternative $a$ over another
alternative $b$, then you probably expect the model to be monotone, that is,
the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps
surprisingly, many widely deployed comparison-based preference learning models,
including large language models, fail to have this guarantee. Until now, the
only comparison-based preference learning algorithms that were proved to be
monotone are the Generalized Bradley-Terry models. Yet, these models are unable
to generalize to uncompared data. In this paper, we advance the understanding
of the set of models with generalization ability that are monotone. Namely, we
propose a new class of Linear Generalized Bradley-Terry models with Diffusion
Priors, and identify sufficient conditions on alternatives' embeddings that
guarantee monotonicity. Our experiments show that this monotonicity is far from
being a general guarantee, and that our new class of generalizing models
improves accuracy, especially when the dataset is limited.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [434] [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
*Issa Sugiura,Takashi Ishida,Taro Makino,Chieko Tazuke,Takanori Nakagawa,Kosuke Nakago,David Ha*

Main category: q-fin.ST

TL;DR: The paper introduces EDINET-Bench, a Japanese financial benchmark for evaluating large language models (LLMs) on financial tasks. Results show that state-of-the-art LLMs struggle in these applications, underscoring the need for domain-specific adaptations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the scarcity of challenging and accessible financial datasets, particularly in Japanese finance, which hinders academic innovation and the development of LLMs in financial analytics.

Method: The authors constructed EDINET-Bench by gathering 10 years of annual reports from Japan's EDINET system and automatically labeling them for tasks like accounting fraud detection, earnings forecasting, and industry prediction.

Result: Experiments revealed that state-of-the-art LLMs performed only slightly better than logistic regression in tasks like binary fraud detection and earnings forecasting, indicating significant challenges in their application to financial data.

Conclusion: The paper concludes that existing LLMs require domain-specific adaptation to handle real-world financial analysis effectively. The authors make their dataset and code publicly accessible to support future advancements in this area.

Abstract: Financial analysis presents complex challenges that could leverage large
language model (LLM) capabilities. However, the scarcity of challenging
financial datasets, particularly for Japanese financial data, impedes academic
innovation in financial analytics. As LLMs advance, this lack of accessible
research resources increasingly hinders their development and evaluation in
this specialized domain. To address this gap, we introduce EDINET-Bench, an
open-source Japanese financial benchmark designed to evaluate the performance
of LLMs on challenging financial tasks including accounting fraud detection,
earnings forecasting, and industry prediction. EDINET-Bench is constructed by
downloading annual reports from the past 10 years from Japan's Electronic
Disclosure for Investors' NETwork (EDINET) and automatically assigning labels
corresponding to each evaluation task. Our experiments reveal that even
state-of-the-art LLMs struggle, performing only slightly better than logistic
regression in binary classification for fraud detection and earnings
forecasting. These results highlight significant challenges in applying LLMs to
real-world financial applications and underscore the need for domain-specific
adaptation. Our dataset, benchmark construction code, and evaluation code is
publicly available to facilitate future research in finance with LLMs.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [435] [A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck](https://arxiv.org/abs/2506.08654)
*Ciro Benito Raggio,Paolo Zaffino,Maria Francesca Spadea*

Main category: physics.med-ph

TL;DR: The paper proposes a federated learning (FL) method to synthesize synthetic CT (sCT) images from CBCT while addressing issues like noise and data privacy barriers across medical centers.


<details>
  <summary>Details</summary>
Motivation: CBCT images are noisy and unreliable for direct dose calculation due to artifacts, institutional differences, and scanner variations, necessitating robust methods for synthetic CT generation.

Method: A conditional generative adversarial network (GAN) is trained under a cross-silo horizontal federated learning framework using data from three European centers without centralized data sharing.

Result: The federated model achieved effective generalization across datasets, yielding consistent performance metrics like MAE, SSIM, and PSNR, even on an external validation dataset.

Conclusion: FL demonstrates its ability to address scanner variability and privacy challenges, enabling the development of generalizable sCT models without site-specific fine-tuning or shared datasets.

Abstract: Shortened Abstract
  Cone-beam computed tomography (CBCT) has become a widely adopted modality for
image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise,
limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield
unit values and hindering direct dose calculation. Synthetic CT (sCT)
generation from CBCT addresses these issues, especially using deep learning
(DL) methods. Existing approaches are limited by institutional heterogeneity,
scanner-dependent variations, and data privacy regulations that prevent
multi-center data sharing.
  To overcome these challenges, we propose a cross-silo horizontal federated
learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region,
extending our FedSynthCT framework. A conditional generative adversarial
network was collaboratively trained on data from three European medical centers
in the public SynthRAD2025 challenge dataset.
  The federated model demonstrated effective generalization across centers,
with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$
HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$,
and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB.
Notably, on an external validation dataset of 60 patients, comparable
performance was achieved (MAE: $75.22\pm11.81$ HU, SSIM: $0.904\pm0.034$, PSNR:
$33.52\pm2.06$ dB) without additional training, confirming robust
generalization despite protocol, scanner differences and registration errors.
  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT
synthesis while preserving data privacy and offer a collaborative solution for
developing generalizable models across institutions without centralized data
sharing or site-specific fine-tuning.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [436] [Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning](https://arxiv.org/abs/2506.08893)
*Kai Zhou,Youbiao He,Chong Zhong,Yifu Wu*

Main category: physics.soc-ph

TL;DR: The paper proposes a Markov Decision Process (MDP) model solved using reinforcement learning to mitigate cascading outages in power systems under uncertainty.


<details>
  <summary>Details</summary>
Motivation: Modern power systems face escalating risks of cascading outages due to increased renewable energy integration, requiring fast operational decisions to mitigate these events.

Method: The authors extend the influence graph into an MDP framework incorporating uncertainties in generation, load, and contingencies, and solve this using a novel policy gradient reinforcement learning algorithm.

Result: The model demonstrates faster algorithm convergence than conventional methods, learns conservative yet effective actions, and is validated with IEEE test systems to show reduced cascading risks.

Conclusion: Proactive line disconnections enabled by the MDP approach significantly contribute to mitigating cascade propagation risks, highlighting critical lines in the system.

Abstract: Despite high reliability, modern power systems with growing renewable
penetration face an increasing risk of cascading outages. Real-time cascade
mitigation requires fast, complex operational decisions under uncertainty. In
this work, we extend the influence graph into a Markov decision process model
(MDP) for real-time mitigation of cascading outages in power transmission
systems, accounting for uncertainties in generation, load, and initial
contingencies. The MDP includes a do-nothing action to allow for conservative
decision-making and is solved using reinforcement learning. We present a policy
gradient learning algorithm initialized with a policy corresponding to the
unmitigated case and designed to handle invalid actions. The proposed learning
method converges faster than the conventional algorithm. Through careful reward
design, we learn a policy that takes conservative actions without deteriorating
system conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus
systems. The results show that proactive line disconnections can effectively
reduce cascading risk, and certain lines consistently emerge as critical in
mitigating cascade propagation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [437] [Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs](https://arxiv.org/abs/2506.08633)
*Šimon Sedláček,Bolaji Yusuf,Ján Švec,Pradyoth Hegde,Santosh Kesiraju,Oldřich Plchot,Jan Černocký*

Main category: eess.AS

TL;DR: The paper proposes a method for improving spoken Dialogue State Tracking (DST) using speech encoder and LLM integration via a connector module, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of spoken Dialogue State Tracking by integrating components of speech encoders and large language models (LLMs) while using open-source data and tools.

Method: A connector module bridges the representation spaces of WavLM-large and OLMo, with experiments on full/LoRA fine-tuning, agent turn effects, and fuzzy matching-based post-processing. Tested on the SpokenWOZ dataset, with additional training data from Speech-Aware MultiWOZ.

Result: The proposed system achieves state-of-the-art performance on the SpokenWOZ test set with 34.66% JGA using WavLM + OLMo-1B, and reaches 42.17% JGA with Gemma-2-9B-instruct.

Conclusion: The integration of speech encoders and LLMs via a connector module significantly improves Dialogue State Tracking, offering a state-of-the-art solution.

Abstract: In this work, we approach spoken Dialogue State Tracking (DST) by bridging
the representation spaces of speech encoders and LLMs via a small connector
module, with a focus on fully open-sourced and open-data components
(WavLM-large, OLMo). We focus on ablating different aspects of such systems
including full/LoRA adapter fine-tuning, the effect of agent turns in the
dialogue history, as well as fuzzy matching-based output post-processing, which
greatly improves performance of our systems on named entities in the dialogue
slot values. We conduct our experiments on the SpokenWOZ dataset, and
additionally utilize the Speech-Aware MultiWOZ dataset to augment our training
data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned
models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our
system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%
JGA on SpokenWOZ test.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [438] [Superposed Parameterised Quantum Circuits](https://arxiv.org/abs/2506.08749)
*Viktoria Patapovich,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: The paper presents superposed parameterised quantum circuits, a novel architecture that incorporates non-linear features and enhanced scalability for quantum machine learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current quantum machine learning models, which rely on linear unitary operations and shared trainable parameters, thus restricting expressivity and scalability compared to classical deep learning architectures.

Method: The authors propose superposed parameterised quantum circuits that embed an exponential number of parameterised sub-models in a single circuit. This is achieved using flip-flop quantum random-access memory and repeat-until-success protocols, enabling polynomial activation functions through amplitude transformations and post-selection.

Result: The proposed circuits demonstrate superior performance: a two-qubit version reduces mean-squared error by three orders of magnitude on a 1D regression task and significantly improves accuracy and reduces variance on a 2D classification problem.

Conclusion: Superposed parameterised quantum circuits represent a hardware-efficient and versatile solution for quantum machine learning, enabling the learning of complex decision boundaries through enhanced expressivity and scalability.

Abstract: Quantum machine learning has shown promise for high-dimensional data
analysis, yet many existing approaches rely on linear unitary operations and
shared trainable parameters across outputs. These constraints limit
expressivity and scalability relative to the multi-layered, non-linear
architectures of classical deep networks. We introduce superposed parameterised
quantum circuits to overcome these limitations. By combining flip-flop quantum
random-access memory with repeat-until-success protocols, a superposed
parameterised quantum circuit embeds an exponential number of parameterised
sub-models in a single circuit and induces polynomial activation functions
through amplitude transformations and post-selection. We provide an analytic
description of the architecture, showing how multiple parameter sets are
trained in parallel while non-linear amplitude transformations broaden
representational power beyond conventional quantum kernels. Numerical
experiments underscore these advantages: on a 1D step-function regression a
two-qubit superposed parameterised quantum circuit cuts the mean-squared error
by three orders of magnitude versus a parameter-matched variational baseline;
on a 2D star-shaped two-dimensional classification task, introducing a
quadratic activation lifts accuracy to 81.4% and reduces run-to-run variance
three-fold. These results position superposed parameterised quantum circuits as
a hardware-efficient route toward deeper, more versatile parameterised quantum
circuits capable of learning complex decision boundaries.

</details>


### [439] [Solving excited states for long-range interacting trapped ions with neural networks](https://arxiv.org/abs/2506.08594)
*Yixuan Ma,Chang Liu,Weikang Li,Shun-Yao Zhang,L. -M. Duan,Yukai Wu,Dong-Ling Deng*

Main category: quant-ph

TL;DR: The study introduces the Neural Quantum Excited-State (NQES) algorithm to efficiently compute multiple excited states in quantum many-body systems, overcoming exponential scaling challenges.


<details>
  <summary>Details</summary>
Motivation: Computing excited states in strongly interacting quantum many-body systems is crucial but challenging due to the exponential growth of Hilbert space with system size. The aim is to address this complexity using neural networks.

Method: The NQES algorithm employs neural networks to compute low-lying excited states without requiring explicit orthogonalization and is applicable across various dimensions and interaction types.

Result: The algorithm successfully computes excited states and observables for models including the Haldane-Shastry model, trapped-ion systems, and large systems with long-range interactions, providing insights into gap scaling and spatial correlations.

Conclusion: The NQES algorithm is scalable and efficient, enabling computation of excited states for complex quantum systems, supporting applications in quantum benchmarking and physical processes like photoisomerization.

Abstract: The computation of excited states in strongly interacting quantum many-body
systems is of fundamental importance. Yet, it is notoriously challenging due to
the exponential scaling of the Hilbert space dimension with the system size.
Here, we introduce a neural network-based algorithm that can simultaneously
output multiple low-lying excited states of a quantum many-body spin system in
an accurate and efficient fashion. This algorithm, dubbed the neural quantum
excited-state (NQES) algorithm, requires no explicit orthogonalization of the
states and is generally applicable to higher dimensions. We demonstrate,
through concrete examples including the Haldane-Shastry model with all-to-all
interactions, that the NQES algorithm is capable of efficiently computing
multiple excited states and their related observable expectations. In addition,
we apply the NQES algorithm to two classes of long-range interacting
trapped-ion systems in a two-dimensional Wigner crystal. For non-decaying
all-to-all interactions with alternating signs, our computed low-lying excited
states bear spatial correlation patterns similar to those of the ground states,
which closely match recent experimental observations that the
quasi-adiabatically prepared state accurately reproduces analytical
ground-state correlations. For a system of up to 300 ions with power-law
decaying antiferromagnetic interactions, we successfully uncover its gap
scaling and correlation features. Our results establish a scalable and
efficient algorithm for computing excited states of interacting quantum
many-body systems, which holds potential applications ranging from benchmarking
quantum devices to photoisomerization.

</details>


### [440] [Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions](https://arxiv.org/abs/2506.08448)
*Hyakka Nakada,Shu Tanaka*

Main category: quant-ph

TL;DR: The paper presents a method to use Quantum Annealing (QA) for solving complex machine learning problems by converting them into QUBO formulations using rectified linear unit bases.


<details>
  <summary>Details</summary>
Motivation: QA is efficient for QUBO problems, but methods to quadratize the complex, nonlinear functions involved in machine learning are underdeveloped.

Method: Utilize rectified linear unit bases for universal approximation and convert target functions into equivalent quadratic-polynomial representations, enabling their use in QA.

Result: Presented numerical and analytical validation of the quadratization approach; developed a black-box optimization scheme integrating ML surrogate regressors with QA.

Conclusion: The proposed approach extends QA's applicability to difficult machine learning problems through effective function quadratization, offering a new optimization design.

Abstract: Quantum Annealing (QA) can efficiently solve combinatorial optimization
problems whose objective functions are represented by Quadratic Unconstrained
Binary Optimization (QUBO) formulations. For broader applicability of QA,
quadratization methods are used to transform higher-order problems into QUBOs.
However, quadratization methods for complex problems involving Machine Learning
(ML) remain largely unknown. In these problems, strong nonlinearity and dense
interactions prevent conventional methods from being applied. Therefore, we
model target functions by the sum of rectified linear unit bases, which not
only have the ability of universal approximation, but also have an equivalent
quadratic-polynomial representation. In this study, the proof of concept is
verified both numerically and analytically. In addition, by combining QA with
the proposed quadratization, we design a new black-box optimization scheme, in
which ML surrogate regressors are inputted to QA after the quadratization
process.

</details>


### [441] [The interplay of robustness and generalization in quantum machine learning](https://arxiv.org/abs/2506.08455)
*Julian Berberich,Tobias Fellner,Christian Holm*

Main category: quant-ph

TL;DR: This paper investigates the intersection of adversarial robustness and generalization in variational quantum models using Lipschitz bounds, proposing a regularization approach for robust and generalizable models, showcased in time series analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand and address the interplay between adversarial robustness and generalization in variational quantum models, essential for their reliable application in tasks like supervised learning.

Method: The method uses Lipschitz bounds to quantify robustness and generalization, applied within a regularization-based training framework that incorporates trainable data encoding strategies.

Result: The theoretical insights reveal the dependence of robustness and generalization on model parameters, and practical tests on time series analysis demonstrate the efficacy of the proposed approach.

Conclusion: The study highlights the value of leveraging Lipschitz bounds and trainable data encoding to develop variational quantum models that are both robust and generalizable.

Abstract: While adversarial robustness and generalization have individually received
substantial attention in the recent literature on quantum machine learning,
their interplay is much less explored. In this chapter, we address this
interplay for variational quantum models, which were recently proposed as
function approximators in supervised learning. We discuss recent results
quantifying both robustness and generalization via Lipschitz bounds, which
explicitly depend on model parameters. Thus, they give rise to a
regularization-based training approach for robust and generalizable quantum
models, highlighting the importance of trainable data encoding strategies. The
practical implications of the theoretical results are demonstrated with an
application to time series analysis.

</details>


### [442] [Quantum Adiabatic Generation of Human-Like Passwords](https://arxiv.org/abs/2506.08917)
*Sascha Mücke,Raoul Heese,Thore Gerlach,David Biesner,Loong Kuan Lee,Nico Piatkowski*

Main category: quant-ph

TL;DR: The paper explores the use of adiabatic quantum computing for password generation, successfully producing realistic human-like passwords such as "Tunas200992."


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess whether Quantum Computing (QC) can mitigate the resource-heavy demands of Generative AI, particularly in the context of generating short, realistic semantic structures like passwords.

Method: The researchers used adiabatic quantum computing with encodings based on Quadratic Unconstrained Binary Optimization (QUBO) and Unit-Disk Maximum Independent Set (UD-MIS) problems to estimate token distributions from data. Quantum states were adiabatically prepared, and passwords were generated via quantum measurements.

Result: Quantum-generated passwords using the QuEra Aquila 256-qubit neutral atom quantum computer showed human-like characteristics, exemplified by samples like "Tunas200992" and "teedem28iglove."

Conclusion: While large-scale generative NLP tasks remain infeasible, quantum computing shows promise in smaller tasks such as generating realistic passwords, demonstrating potential utility in security-related applications.

Abstract: Generative Artificial Intelligence (GenAI) for Natural Language Processing
(NLP) is the predominant AI technology to date. An important perspective for
Quantum Computing (QC) is the question whether QC has the potential to reduce
the vast resource requirements for training and operating GenAI models. While
large-scale generative NLP tasks are currently out of reach for practical
quantum computers, the generation of short semantic structures such as
passwords is not. Generating passwords that mimic real user behavior has many
applications, for example to test an authentication system against realistic
threat models. Classical password generation via deep learning have recently
been investigated with significant progress in their ability to generate novel,
realistic password candidates. In the present work we investigate the utility
of adiabatic quantum computers for this task. More precisely, we study
different encodings of token strings and propose novel approaches based on the
Quadratic Unconstrained Binary Optimization (QUBO) and the Unit-Disk Maximum
Independent Set (UD-MIS) problems. Our approach allows us to estimate the token
distribution from data and adiabatically prepare a quantum state from which we
eventually sample the generated passwords via measurements. Our results show
that relatively small samples of 128 passwords, generated on the QuEra Aquila
256-qubit neutral atom quantum computer, contain human-like passwords such as
"Tunas200992" or "teedem28iglove".

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [443] [Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy](https://arxiv.org/abs/2506.08073)
*Yu Liu,Utkarsh Pratiush,Kamyar Barakati,Hiroshi Funakubo,Ching-Che Lin,Jaegyu Kim,Lane W. Martin,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: This paper introduces a kernel-learning approach to analyze microstructural rules governing ferroelectric polarization switching, enabling high-throughput insights and applications in complex design spaces.


<details>
  <summary>Details</summary>
Motivation: Understanding the dependence of ferroelectric polarization switching on microstructural features is crucial for the performance of various materials and devices, but is challenging due to the complexity and impracticality of manual or grid-based methods.

Method: The authors developed a multi-objective kernel-learning framework applied to automated piezoresponse force microscopy (PFM) experiments to infer key microstructural rules for polarization switching.

Result: The framework uncovers relationships between domain-wall configurations and local switching behaviors, and translates abstract metrics into interpretable physical descriptors, enabling mechanistic analyses.

Conclusion: This approach facilitates both active learning and mechanistic insight, offering a generalizable tool for exploring complex design spaces beyond ferroelectric systems.

Abstract: Ferroelectric polarization switching underpins the functional performance of
a wide range of materials and devices, yet its dependence on complex local
microstructural features renders systematic exploration by manual or grid-based
spectroscopic measurements impractical. Here, we introduce a multi-objective
kernel-learning workflow that infers the microstructural rules governing
switching behavior directly from high-resolution imaging data. Applied to
automated piezoresponse force microscopy (PFM) experiments, our framework
efficiently identifies the key relationships between domain-wall configurations
and local switching kinetics, revealing how specific wall geometries and defect
distributions modulate polarization reversal. Post-experiment analysis projects
abstract reward functions, such as switching ease and domain symmetry, onto
physically interpretable descriptors including domain configuration and
proximity to boundaries. This enables not only high-throughput active learning,
but also mechanistic insight into the microstructural control of switching
phenomena. While demonstrated for ferroelectric domain switching, our approach
provides a powerful, generalizable tool for navigating complex,
non-differentiable design spaces, from structure-property correlations in
molecular discovery to combinatorial optimization across diverse imaging
modalities.

</details>


### [444] [Mic-hackathon 2024: Hackathon on Machine Learning for Electron and Scanning Probe Microscopy](https://arxiv.org/abs/2506.08423)
*Utkarsh Pratiush,Austin Houston,Kamyar Barakati,Aditya Raghavan,Dasol Yoon,Harikrishnan KP,Zhaslan Baraissov,Desheng Ma,Samuel S. Welborn,Mikolaj Jakowski,Shawn-Patrick Barhorst,Alexander J. Pattison,Panayotis Manganaris,Sita Sirisha Madugula,Sai Venkata Gayathri Ayyagari,Vishal Kennedy,Ralph Bulanadi,Michelle Wang,Kieran J. Pang,Ian Addison-Smith,Willy Menacho,Horacio V. Guzman,Alexander Kiefer,Nicholas Furth,Nikola L. Kolev,Mikhail Petrov,Viktoriia Liu,Sergey Ilyev,Srikar Rairao,Tommaso Rodani,Ivan Pinto-Huguet,Xuli Chen,Josep Cruañes,Marta Torrens,Jovan Pomar,Fanzhi Su,Pawan Vedanti,Zhiheng Lyu,Xingzhi Wang,Lehan Yao,Amir Taqieddin,Forrest Laskowski,Xiangyu Yin,Yu-Tsun Shao,Benjamin Fein-Ashley,Yi Jiang,Vineet Kumar,Himanshu Mishra,Yogesh Paul,Adib Bazgir,Rama chandra Praneeth Madugula,Yuwen Zhang,Pravan Omprakash,Jian Huang,Eric Montufar-Morales,Vivek Chawla,Harshit Sethi,Jie Huang,Lauri Kurki,Grace Guinan,Addison Salvador,Arman Ter-Petrosyan,Madeline Van Winkle,Steven R. Spurgeon,Ganesh Narasimha,Zijie Wu,Richard Liu,Yongtao Liu,Boris Slautin,Andrew R Lupini,Rama Vasudevan,Gerd Duscher,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: This paper discusses how hackathons bridge the gap between microscopy and machine learning (ML) communities, enabling solutions like benchmark datasets and digital twins to enhance research and workflow standardization.


<details>
  <summary>Details</summary>
Motivation: Advanced microscopy generates extensive data crucial for understanding materials at nanoscale, but challenges like inconsistent data formats and lack of standardized tools hinder efficient analysis and impact.

Method: The paper highlights organizing hackathons to facilitate collaboration between ML and microscopy experts, producing benchmark datasets, digital twins, and publicly available code for community use.

Result: The hackathon resulted in the creation of datasets, digital twins of microscopes, and publicly shared resources to standardize and optimize workflows using ML in microscopy.

Conclusion: Hackathons serve as an effective bridge between microscopy and ML, accelerating innovation, improving research capabilities, and preparing professionals for multidisciplinary roles.

Abstract: Microscopy is a primary source of information on materials structure and
functionality at nanometer and atomic scales. The data generated is often
well-structured, enriched with metadata and sample histories, though not always
consistent in detail or format. The adoption of Data Management Plans (DMPs) by
major funding agencies promotes preservation and access. However, deriving
insights remains difficult due to the lack of standardized code ecosystems,
benchmarks, and integration strategies. As a result, data usage is inefficient
and analysis time is extensive. In addition to post-acquisition analysis, new
APIs from major microscope manufacturers enable real-time, ML-based analytics
for automated decision-making and ML-agent-controlled microscope operation.
Yet, a gap remains between the ML and microscopy communities, limiting the
impact of these methods on physics, materials discovery, and optimization.
Hackathons help bridge this divide by fostering collaboration between ML
researchers and microscopy experts. They encourage the development of novel
solutions that apply ML to microscopy, while preparing a future workforce for
instrumentation, materials science, and applied ML. This hackathon produced
benchmark datasets and digital twins of microscopes to support community growth
and standardized workflows. All related code is available at GitHub:
https://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [445] [DEKC: Data-Enable Control for Tethered Space Robot Deployment in the Presence of Uncertainty via Koopman Operator Theory](https://arxiv.org/abs/2506.08319)
*Ao Jin,Qinyi Wang,Sijie Wen,Ya Liu,Ganghui Shen,Panfeng Huang,Fan Zhang*

Main category: eess.SY

TL;DR: This paper presents DEKC, a framework for deploying tethered space robots in uncertain environments using Koopman theory and deep learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying tethered space robots in environments with unknown uncertainties by accurately modeling and mitigating these uncertainties.

Method: The DEKC framework consists of an offline training phase that uses Koopman theory with deep neural networks to model uncertainties and an online phase where this proxy model compensates uncertainties, augmented by a receding-horizon update scheme.

Result: Extensive numerical simulations validate the high accuracy and effectiveness of the proposed DEKC framework, and its implementation is publicly available.

Conclusion: The DEKC framework effectively models and addresses unknown uncertainties, offering a reliable solution for tethered space robot deployment.

Abstract: This work focuses the deployment of tethered space robot in the presence of
unknown uncertainty. A data-enable framework called DEKC which contains offline
training part and online execution part is proposed to deploy tethered space
robot in the presence of uncertainty. The main idea of this work is modeling
the unknown uncertainty as a dynamical system, which enables high accuracy and
convergence of capturing uncertainty. The core part of proposed framework is a
proxy model of uncertainty, which is derived from data-driven Koopman theory
and is separated with controller design. In the offline stage, the lifting
functions associated with Koopman operator are parameterized with deep neural
networks. Then by solving an optimization problem, the lifting functions are
learned from sampling data. In the online execution stage, the proxy model
cooperates the learned lifting functions obtained in the offline phase to
capture the unknown uncertainty. Then the output of proxy model is compensated
to the baseline controller such that the effect of uncertainty can be
attenuated or even eliminated. Furthermore, considering some scenarios in which
the performance of proxy model may weaken, a receding-horizon scheme is
proposed to update the proxy model online. Finally, the extensive numerical
simulations demonstrate the effectiveness of our proposed framework. The
implementation of proposed DEKC framework is publicly available at
https://github.com/NPU-RCIR/DEKC.git.

</details>


### [446] [Efficient Learning of Vehicle Controller Parameters via Multi-Fidelity Bayesian Optimization: From Simulation to Experiment](https://arxiv.org/abs/2506.08719)
*Yongpeng Zhao,Maik Pfefferkorn,Maximilian Templer,Rolf Findeisen*

Main category: eess.SY

TL;DR: The paper introduces a multi-fidelity Bayesian optimization method that minimizes the need for real-world experiments and manual effort in tuning vehicle controllers.


<details>
  <summary>Details</summary>
Motivation: Parameter tuning for vehicle controllers in automotive development is expensive and time-intensive, primarily due to reliance on extensive real-world testing.

Method: The authors utilized a multi-fidelity Bayesian optimization approach leveraging both low-fidelity simulation data and few real-world experiments via an auto-regressive multi-fidelity Gaussian process model.

Result: The study demonstrated efficient learning of optimal controller parameters, achieving high-quality performance with minimal real-world experiments, validated through both simulations and experiments.

Conclusion: The proposed method reduces the need for expensive field testing and manual tuning, presenting a scalable and practical solution for industrial vehicle controller optimization.

Abstract: Parameter tuning for vehicle controllers remains a costly and time-intensive
challenge in automotive development. Traditional approaches rely on extensive
real-world testing, making the process inefficient. We propose a multi-fidelity
Bayesian optimization approach that efficiently learns optimal controller
parameters by leveraging both low-fidelity simulation data and a very limited
number of real-world experiments. Our approach significantly reduces the need
for manual tuning and expensive field testing while maintaining the standard
two-stage development workflow used in industry. The core contribution is the
integration of an auto-regressive multi-fidelity Gaussian process model into
Bayesian optimization, enabling knowledge transfer between different fidelity
levels without requiring additional low-fidelity evaluations during real-world
testing. We validate our approach through both simulation studies and realworld
experiments. The results demonstrate that our method achieves high-quality
controller performance with only very few real-world experiments, highlighting
its potential as a practical and scalable solution for intelligent vehicle
control tuning in industrial applications.

</details>


### [447] [Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning](https://arxiv.org/abs/2506.08029)
*Jiayu Li,Masood Mortazavi,Ning Yan,Yihong Ma,Reza Zafarani*

Main category: eess.SY

TL;DR: DCIDA is a framework for inverse design in distributed circuits. It employs a Transformer-based policy network to achieve optimal designs with reduced errors for complex transfer functions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in real-world circuit design, such as non-differentiable evaluation procedures, changing topologies, and near-continuous placement spaces.

Method: The paper introduces DCIDA, a framework that learns a near-optimal design sampling policy using a Transformer-based network and conditional distributions. It utilizes an injective map to transform sampled designs into physical representations.

Result: DCIDA significantly outperforms state-of-the-art methods in reducing design errors, especially for complex transfer functions.

Conclusion: DCIDA offers a robust approach to circuit design by successfully handling complex design factors and achieving better performance than existing methods.

Abstract: The goal of inverse design in distributed circuits is to generate
near-optimal designs that meet a desirable transfer function specification.
Existing design exploration methods use some combination of strategies
involving artificial grids, differentiable evaluation procedures, and specific
template topologies. However, real-world design practices often require
non-differentiable evaluation procedures, varying topologies, and
near-continuous placement spaces. In this paper, we propose DCIDA, a design
exploration framework that learns a near-optimal design sampling policy for a
target transfer function. DCIDA decides all design factors in a compound
single-step action by sampling from a set of jointly-trained conditional
distributions generated by the policy. Utilizing an injective interdependent
``map", DCIDA transforms raw sampled design ``actions" into uniquely equivalent
physical representations, enabling the framework to learn the conditional
dependencies among joint ``raw'' design decisions. Our experiments demonstrate
DCIDA's Transformer-based policy network achieves significant reductions in
design error compared to state-of-the-art approaches, with significantly better
fit in cases involving more complex transfer functions.

</details>


### [448] [Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases](https://arxiv.org/abs/2506.08033)
*Axel TahmasebiMoradi,Vincent Ren,Benjamin Le-Creurer,Chetra Mang*

Main category: eess.SY

TL;DR: The paper introduces a convolutional neural network (CNN) and a multi-layer perceptron (MLP) to create a surrogate model for radiative heat transfer in a 2D domain, focusing on adapting inputs and optimizing performance.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational cost of numerical simulations for radiative heat transfer solutions.

Method: The authors employed CNN and MLP models, trained using two datasets generated by the ICARUS2D solver, and optimized their hyperparameters using Optuna.

Result: CNN delivers higher accuracy, robustness, and stability compared to MLP, while both architectures significantly outperform the classical solver in speed with acceptable error margins.

Conclusion: CNN is shown to be superior to MLP and classical solvers for radiative heat transfer simulation approximations, demonstrating effective performance with reduced computational costs.

Abstract: Aiming to reduce the computational cost of numerical simulations, a
convolutional neural network (CNN) and a multi-layer perceptron (MLP) are
introduced to build a surrogate model to approximate radiative heat transfer
solutions in a 2-D walled domain with participative gases. The originality of
this work lays in the adaptation of the inputs of the problem (gas and wall
properties) in order to fit with the CNN architecture, more commonly used for
image processing. Two precision datasets have been created with the classical
solver, ICARUS2D, that uses the discrete transfer radiation method with the
statistical narrow bands model. The performance of the CNN architecture is
compared to a more classical MLP architecture in terms of speed and accuracy.
Thanks to Optuna, all results are obtained using the optimized hyper parameters
networks. The results show a significant speedup with industrially acceptable
relative errors compared to the classical solver for both architectures.
Additionally, the CNN outperforms the MLP in terms of precision and is more
robust and stable to changes in hyper-parameters. A performance analysis on the
dataset size of the samples have also been carried out to gain a deeper
understanding of the model behavior.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [449] [The World of AI: A Novel Approach to AI Literacy for First-year Engineering Students](https://arxiv.org/abs/2506.08041)
*Siddharth Siddharth,Brainerd Prince,Amol Harsh,Shreyas Ramachandran*

Main category: cs.CY

TL;DR: A novel interdisciplinary AI course for first-year engineering students, focusing on foundational knowledge, societal implications, and practical applications of AI.


<details>
  <summary>Details</summary>
Motivation: To address the lack of foundational AI knowledge and appreciation of its societal implications among first-year engineering students.

Method: An interdisciplinary course with three co-delivered modules: planetary (AI and environment), societal impact (bias/privacy), and workplace (job adaptation).

Result: Students showed improved awareness of AI's environmental impact, better solutions for AI fairness, and a more nuanced understanding of AI's societal transformation.

Conclusion: The course succeeded in combining technical and societal perspectives, enhancing students' comprehension and perception of AI's broad impact.

Abstract: This work presents a novel course titled The World of AI designed for
first-year undergraduate engineering students with little to no prior exposure
to AI. The central problem addressed by this course is that engineering
students often lack foundational knowledge of AI and its broader societal
implications at the outset of their academic journeys. We believe the way to
address this gap is to design and deliver an interdisciplinary course that can
a) be accessed by first-year undergraduate engineering students across any
domain, b) enable them to understand the basic workings of AI systems sans
mathematics, and c) make them appreciate AI's far-reaching implications on our
lives. The course was divided into three modules co-delivered by faculty from
both engineering and humanities. The planetary module explored AI's dual role
as both a catalyst for sustainability and a contributor to environmental
challenges. The societal impact module focused on AI biases and concerns around
privacy and fairness. Lastly, the workplace module highlighted AI-driven job
displacement, emphasizing the importance of adaptation. The novelty of this
course lies in its interdisciplinary curriculum design and pedagogical
approach, which combines technical instruction with societal discourse. Results
revealed that students' comprehension of AI challenges improved across diverse
metrics like (a) increased awareness of AI's environmental impact, and (b)
efficient corrective solutions for AI fairness. Furthermore, it also indicated
the evolution in students' perception of AI's transformative impact on our
lives.

</details>


### [450] [Evaluation of Machine Learning Models in Student Academic Performance Prediction](https://arxiv.org/abs/2506.08047)
*A. G. R. Sandeepa,Sanka Mohottala*

Main category: cs.CY

TL;DR: This paper explores using machine learning, particularly multi-layer perceptron classifier (MLPC), for predicting students' academic performance and demonstrates its high accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: To leverage machine learning for predicting students' academic performance using diverse student data and improve model transparency.

Method: Standard machine learning techniques, including MLPC, were implemented with feature selection and explainable AI methods to achieve accurate predictions and demystify model predictions.

Result: MLPC achieved an 86.46% maximum accuracy on the test set and demonstrated strong potential over other models, emphasizing feature selection's contributions.

Conclusion: Neural networks like MLPC are promising and data-efficient for academic performance prediction; incorporating explainable methods can enhance model transparency and trustworthiness.

Abstract: This research investigates the use of machine learning methods to forecast
students' academic performance in a school setting. Students' data with
behavioral, academic, and demographic details were used in implementations with
standard classical machine learning models including multi-layer perceptron
classifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across
all implementations. Under 10-fold cross validation, MLPC obtained 79.58%
average accuracy for test set while for train set, it was 99.65%. MLP's better
performance over other machine learning models strongly suggest the potential
use of neural networks as data-efficient models. Feature selection approach
played a crucial role in improving the performance and multiple evaluation
approaches were used in order to compare with existing literature. Explainable
machine learning methods were utilized to demystify the black box models and to
validate the feature selection approach.

</details>


### [451] [Surgeons Awareness, Expectations, and Involvement with Artificial Intelligence: a Survey Pre and Post the GPT Era](https://arxiv.org/abs/2506.08258)
*Lorenzo Arboit,Dennis N. Schneider,Toby Collins,Daniel A. Hashimoto,Silvana Perretta,Bernard Dallemagne,Jacques Marescaux,EAES Working Group,Nicolas Padoy,Pietro Mascagni*

Main category: cs.CY

TL;DR: The paper assesses surgeons' evolving perspectives on AI in surgery through global surveys, identifying increased awareness, ethical concerns, and infrastructural challenges between 2021 and 2024.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand how surgeons perceive and engage with AI technologies amidst the rising role of generative AI and surgical data advancements.

Method: Two global cross-sectional surveys were conducted in 2021 and 2024, analyzing surgeons' awareness, expectations, ethical concerns, and barriers to AI adoption in surgery.

Result: AI course awareness increased, ethical concerns gained importance, and hospital management roles for AI were more recognized. However, familiarity with AI concepts remained limited, and infrastructural challenges persisted.

Conclusion: Surgeons are optimistic about AI's potential in surgery but highlight the need for education, ethical guidelines, and infrastructure development to foster AI integration.

Abstract: Artificial Intelligence (AI) is transforming medicine, with generative AI
models like ChatGPT reshaping perceptions of its potential. This study examines
surgeons' awareness, expectations, and involvement with AI in surgery through
comparative surveys conducted in 2021 and 2024. Two cross-sectional surveys
were distributed globally in 2021 and 2024, the first before an IRCAD webinar
and the second during the annual EAES meeting. The surveys assessed
demographics, AI awareness, expectations, involvement, and ethics (2024 only).
The surveys collected a total of 671 responses from 98 countries, 522 in 2021
and 149 in 2024. Awareness of AI courses rose from 14.5% in 2021 to 44.6% in
2024, while course attendance increased from 12.9% to 23%. Despite this,
familiarity with foundational AI concepts remained limited. Expectations for
AI's role shifted in 2024, with hospital management gaining relevance. Ethical
concerns gained prominence, with 87.2% of 2024 participants emphasizing
accountability and transparency. Infrastructure limitations remained the
primary obstacle to implementation. Interdisciplinary collaboration and
structured training were identified as critical for successful AI adoption.
Optimism about AI's transformative potential remained high, with 79.9% of
respondents believing AI would positively impact surgery and 96.6% willing to
integrate AI into their clinical practice. Surgeons' perceptions of AI are
evolving, driven by the rise of generative AI and advancements in surgical data
science. While enthusiasm for integration is strong, knowledge gaps and
infrastructural challenges persist. Addressing these through education, ethical
frameworks, and infrastructure development is essential.

</details>


### [452] [WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis](https://arxiv.org/abs/2506.08962)
*Liangliang Chen,Huiru Xie,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: The paper introduces an AI-driven smart tutor designed for homework feedback and assessment in an undergraduate circuit analysis course, showing promising results with student satisfaction.


<details>
  <summary>Details</summary>
Motivation: To improve personalized instruction and feedback for students while providing real-time insights to instructors on student difficulties.

Method: The tutor includes features like open-ended question answering, homework feedback generation, and collects interaction data for analysis, deployed on Microsoft Azure.

Result: 90.9% of students expressed satisfaction with the tutor; data analysis highlighted frequent problems and student questions, enabling targeted instruction.

Conclusion: The AI-enabled tutor presents an effective approach to enhancing learning and instruction, with plans to expand its applications and analyze broader data in the future.

Abstract: This research-to-practice work-in-progress (WIP) paper presents an AI-enabled
smart tutor designed to provide homework assessment and feedback for students
in an undergraduate circuit analysis course. We detail the tutor's design
philosophy and core components, including open-ended question answering and
homework feedback generation. The prompts are carefully crafted to optimize
responses across different problems. The smart tutor was deployed on the
Microsoft Azure platform and is currently in use in an undergraduate circuit
analysis course at the School of Electrical and Computer Engineering in a
large, public, research-intensive institution in the Southeastern United
States. Beyond offering personalized instruction and feedback, the tutor
collects student interaction data, which is summarized and shared with the
course instructor. To evaluate its effectiveness, we collected student
feedback, with 90.9% of responses indicating satisfaction with the tutor.
Additionally, we analyze a subset of collected data on preliminary circuit
analysis topics to assess tutor usage frequency for each problem and identify
frequently asked questions. These insights help instructors gain real-time
awareness of student difficulties, enabling more targeted classroom
instruction. In future work, we will release a full analysis once the complete
dataset is available after the Spring 2025 semester. We also explore the
potential applications of this smart tutor across a broader range of
engineering disciplines by developing improved prompts, diagram-recognition
methods, and database management strategies, which remain ongoing areas of
research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [453] [Guidelines for Gaze-based Neural Preliminary Diagnosis](https://arxiv.org/abs/2506.08517)
*Mayar Elfares,Salma Younis,Pascal Reisert,Ralf Küsters,Tobias Renner,Andreas Bulling*

Main category: cs.HC

TL;DR: The paper addresses the use of eye tracking as a more objective method for diagnosing neural disorders, systematizing existing knowledge to improve its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for diagnosing neural disorders are often cumbersome, subjective, and time-intensive, creating a need for more objective and efficient diagnostic techniques.

Method: The paper outlines a systematic review and systematisation of existing eye-tracking findings across multiple fields to address contradictions and standardize protocols.

Result: The paper consolidates agreed-upon findings to highlight potential applications of gaze-based diagnostics in neural disorder diagnosis.

Conclusion: Eye tracking shows promise as a preliminary diagnostic tool for neural disorders, but further protocol standardization and research are necessary to enhance its application and reliability.

Abstract: Neural disorders refer to any condition affecting the nervous system and that
influence how individuals perceive and interact with the world. Traditional
neural diagnoses rely on cumbersome, time-consuming, or subjective methods,
such as clinical interviews, behavioural observations, or medical imaging. Eye
tracking is an attractive alternative because analysing eye movements, such as
fixations and saccades, can provide more objective insights into brain function
and cognitive processing by capturing non-verbal and unconscious responses.
Despite its potential, existing gaze-based studies presented seemingly
contradictory findings. They are dispersed across diverse fields, requiring
further research to standardise protocols and expand their application,
particularly as a preliminary indicator of neural processes for differential
diagnosis. Therefore, this paper outlines the main agreed-upon findings and
provides a systematisation of knowledge and key guidelines towards advancing
gaze-based neural preliminary diagnosis.

</details>


### [454] [Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration](https://arxiv.org/abs/2506.08805)
*Stina Klein,Pooja Prajod,Katharina Weitz,Matteo Lavit Nicora,Dimitra Tsovaltzi,Elisabeth André*

Main category: cs.HC

TL;DR: The study explores the use of avatars to improve human-robot collaboration (HRC) by conducting a focus group with manufacturing employees, revealing insights into their potential roles and challenges in industrial settings.


<details>
  <summary>Details</summary>
Motivation: To address reduced social interactions caused by collaborative robots (cobots) in industrial environments and explore avatars as a solution to enhance worker well-being and engagement.

Method: A qualitative focus group study was conducted with German manufacturing employees, combining a scripted HRC demo in a lab setting for better context and subsequent discussion.

Result: Key findings highlight the potential roles of avatars, necessary improvements to their behavior, and practical considerations for deploying them in industrial work cells, emphasizing personalized communication and task assistance.

Conclusion: While the study is limited in generalizability, it establishes an initial framework for recognizing the potential of context-aware avatar interaction in industrial setups.

Abstract: The integration of collaborative robots (cobots) in industrial settings
raises concerns about worker well-being, particularly due to reduced social
interactions. Avatars - designed to facilitate worker interactions and
engagement - are promising solutions to enhance the human-robot collaboration
(HRC) experience. However, real-world perspectives on avatar-supported HRC
remain unexplored. To address this gap, we conducted a focus group study with
employees from a German manufacturing company that uses cobots. Before the
discussion, participants engaged with a scripted, industry-like HRC demo in a
lab setting. This qualitative approach provided valuable insights into the
avatar's potential roles, improvements to its behavior, and practical
considerations for deploying them in industrial workcells. Our findings also
emphasize the importance of personalized communication and task assistance.
Although our study's limitations restrict its generalizability, it serves as an
initial step in recognizing the potential of adaptive, context-aware avatar
interactions in real-world industrial environments.

</details>


### [455] [Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams](https://arxiv.org/abs/2506.08892)
*Tauhid Tanjim,Jonathan St. George,Kevin Ching,Hee Rin Lee,Angelique Taylor*

Main category: cs.HC

TL;DR: The paper studies how robots can improve collaboration in human teams by using multimodal communication in time-sensitive scenarios like medical training.


<details>
  <summary>Details</summary>
Motivation: To address the gap in knowledge on how robots can effectively communicate with human action teams using multimodal cues in critical environments.

Method: An experimental in-lab study with a robotic crash cart providing verbal and visual cues in a medical training scenario.

Result: Verbal cues for object search and visual cues for task reminders reduced workload and increased perceived ease of use and usefulness compared to robots without feedback.

Conclusion: The study emphasizes the value of multimodal robot communication to improve team collaboration, suggesting further research for best integration practices in time-sensitive applications.

Abstract: The human-robot interaction (HRI) field has recognized the importance of
enabling robots to interact with teams. Human teams rely on effective
communication for successful collaboration in time-sensitive environments.
Robots can play a role in enhancing team coordination through real-time
assistance. Despite significant progress in human-robot teaming research, there
remains an essential gap in how robots can effectively communicate with action
teams using multimodal interaction cues in time-sensitive environments. This
study addresses this knowledge gap in an experimental in-lab study to
investigate how multimodal robot communication in action teams affects workload
and human perception of robots. We explore team collaboration in a medical
training scenario where a robotic crash cart (RCC) provides verbal and
non-verbal cues to help users remember to perform iterative tasks and search
for supplies. Our findings show that verbal cues for object search tasks and
visual cues for task reminders reduce team workload and increase perceived ease
of use and perceived usefulness more effectively than a robot with no feedback.
Our work contributes to multimodal interaction research in the HRI field,
highlighting the need for more human-robot teaming research to understand best
practices for integrating collaborative robots in time-sensitive environments
such as in hospitals, search and rescue, and manufacturing applications.

</details>


### [456] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)
*Kazuki Kawamura,Jun Rekimoto*

Main category: cs.HC

TL;DR: SakugaFlow integrates AI illustration tools and learning with a scaffolded four-stage process using diffusion-based image generation and large-language-model tutoring.


<details>
  <summary>Details</summary>
Motivation: To provide novices with a structured learning environment that combines creative exploration and skill acquisition while creating illustrations.

Method: The four-stage pipeline offers real-time feedback, non-linear revision, branching alternatives, intermediate outputs exposure, and embedded pedagogical dialogues.

Result: SakugaFlow shifts AI-driven tools from mere image generation to a platform promoting learning and creativity enhancement.

Conclusion: This approach transforms black-box AI generators into transparent, educational systems that merge art creation and skill development effectively.

Abstract: While current AI illustration tools can generate high-quality images from
text prompts, they rarely reveal the step-by-step procedure that human artists
follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based
image generation with a large-language-model tutor. At each stage, novices
receive real-time feedback on anatomy, perspective, and composition, revise any
step non-linearly, and branch alternative versions. By exposing intermediate
outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box
generator into a scaffolded learning environment that supports both creative
exploration and skills acquisition.

</details>


### [457] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Main category: cs.HC

TL;DR: The paper introduces MOSAIC-F, a framework that combines multimodal data and AI to provide personalized feedback for student activities, specifically tested in oral presentations.


<details>
  <summary>Details</summary>
Motivation: Current feedback methods often lack personalization and integration of multimodal insights to enhance learning outcomes.

Method: MOSAIC-F operates via four steps: peer/professor assessments, multimodal data collection, AI-generated feedback, and self-assessment through feedback visualization.

Result: The framework proved effective in delivering personalized and actionable feedback that improved oral presentation skills.

Conclusion: MOSAIC-F successfully integrates human evaluations and AI insights, providing more accurate and personalized feedback to improve student learning processes.

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


### [458] [Stop Misusing t-SNE and UMAP for Visual Analytics](https://arxiv.org/abs/2506.08725)
*Hyeon Jeon,Jeongin Park,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: The paper investigates the widespread misuse of t-SNE and UMAP in visual analytics and offers insights and solutions to encourage their appropriate use.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address and correct the growing misunderstanding and misuse of t-SNE and UMAP projections, particularly regarding inter-cluster relationships.

Method: The authors performed a literature review of 114 papers and conducted an interview study to uncover both explicit practices and implicit practitioner motivations for using these dimensionality reduction techniques.

Result: They found that misuse primarily arises due to a lack of discourse around the appropriate use of t-SNE and UMAP in visual analytics.

Conclusion: The study concludes by suggesting future research directions and actionable steps to ensure more informed and reasonable use of these dimensionality reduction methods.

Abstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly
common. For example, although t-SNE and UMAP projections often do not
faithfully reflect true distances between clusters, practitioners frequently
use them to investigate inter-cluster relationships. In this paper, we bring
this issue to the surface and comprehensively investigate why such misuse
occurs and how to prevent it. We conduct a literature review of 114 papers to
verify the prevalence of the misuse and analyze the reasonings behind it. We
then execute an interview study to uncover practitioners' implicit motivations
for using these techniques -- rationales often undisclosed in the literature.
Our findings indicate that misuse of t-SNE and UMAP primarily stems from
limited discourse on their appropriate use in visual analytics. We conclude by
proposing future directions and concrete action items to promote more
reasonable use of DR.

</details>


### [459] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: This paper develops a keyword spotting (KWS) system on a microcontroller with an integrated Neural Processing Unit (NPU) for efficient real-time voice interaction.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and low-power voice interaction on resource-constrained embedded devices.

Method: Combining MFCC feature extraction with a CNN classifier, optimized using Quantization Aware Training for size reduction with minimal loss in accuracy.

Result: Achieved a 59x speedup in inference time using the NPU over CPU-only execution, with 97.06% accuracy and a model size of 30.58 KB.

Conclusion: The proposed system demonstrates the feasibility of implementing accurate and efficient voice interfaces on embedded platforms with constrained resources.

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [460] [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: This paper introduces Exp4Fuse, a novel framework that enhances sparse retrieval by utilizing zero-shot LLM-based query expansion and combining ranking lists through reciprocal rank fusion.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations posed by computationally intensive methods in query expansion, and to improve sparse information retrieval through efficient use of zero-shot LLM-based techniques.

Method: Exp4Fuse employs two parallel retrieval methods: one using the original query and another using LLM-augmented query output. These are combined with a reciprocal rank fusion technique to generate improved ranking lists.

Result: Extensive evaluations show Exp4Fuse outperforms existing methods on MS MARCO datasets and several low-resource datasets, achieving state-of-the-art results in some benchmarks.

Conclusion: Exp4Fuse demonstrates significant advancements in improving sparse query expansion and retrieval performance with efficient zero-shot LLM-based methodologies.

Abstract: Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.

</details>


### [461] [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
*Abdellah Ghassel,Ian Robinson,Gabriel Tanase,Hal Cooper,Bryan Thompson,Zhen Han,Vassilis N. Ioannidis,Soji Adeshina,Huzefa Rangwala*

Main category: cs.IR

TL;DR: The paper introduces Hierarchical Lexical Graph (HLG) and novel retrieval methods to enhance Retrieval-Augmented Generation (RAG) in addressing semantically distant documents for multi-hop summarization tasks.


<details>
  <summary>Details</summary>
Motivation: Improving Retrieval-Augmented Generation (RAG) systems to better handle complex queries requiring answers found across semantically distant documents.

Method: Proposes a three-tier index (HLG) that traces propositions to sources, clusters into latent topics, and links entities/relations, accompanied by two specialized retrievers: StatementGraphRAG and TopicGraphRAG for precise and exploratory queries, respectively.

Result: Experiments show significant improvements (23.1% average relative) over traditional chunk-based RAG in retrieval recall and correctness across five datasets. A new synthetic dataset pipeline for evaluation is also introduced.

Conclusion: Hierarchical Lexical Graph (HLG) and its associated retrievers provide a robust framework to enhance multi-hop question answering and summarization, with demonstrated performance gains.

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in
external evidence, yet it still falters when answers must be pieced together
across semantically distant documents. We close this gap with the Hierarchical
Lexical Graph (HLG), a three-tier index that (i) traces every atomic
proposition to its source, (ii) clusters propositions into latent topics, and
(iii) links entities and relations to expose cross-document paths. On top of
HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,
which performs fine-grained entity-aware beam search over propositions for
high-precision factoid questions, and TopicGraphRAG, which selects coarse
topics before expanding along entity links to supply broad yet relevant context
for exploratory queries. Additionally, existing benchmarks lack the complexity
required to rigorously evaluate multi-hop summarization systems, often focusing
on single-document queries or limited datasets. To address this, we introduce a
synthetic dataset generation pipeline that curates realistic, multi-document
question-answer pairs, enabling robust evaluation of multi-hop retrieval
systems. Extensive experiments across five datasets demonstrate that our
methods outperform naive chunk-based RAG achieving an average relative
improvement of 23.1% in retrieval recall and correctness. Open-source Python
library is available at https://github.com/awslabs/graphrag-toolkit.

</details>


### [462] [Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems](https://arxiv.org/abs/2506.08743)
*Michael Färber,David Lamprecht,Yuni Susanti*

Main category: cs.IR

TL;DR: The paper explores integrating RDF knowledge graphs (KGs) with Graph Neural Networks (GNNs) to enhance recommender systems by leveraging semantic information.


<details>
  <summary>Details</summary>
Motivation: Despite the availability of over a thousand RDF KGs under the W3C standard, their semantic richness remains underutilized in existing GNN-based recommender systems.

Method: The study integrates RDF KGs with GNNs, incorporating RDF object properties (topological information) and RDF datatype properties (content information). It evaluates different GNN models, focusing on semantic feature initialization and graph structure heterogeneity.

Result: Experimental results on multi-million-node RDF graphs demonstrate that leveraging RDF KGs' semantics improves the performance of recommender systems while advancing GNN-based approaches for the Linked Open Data cloud.

Conclusion: The integration of RDF KGs and GNNs significantly enhances recommendation tasks, providing a solid foundation for future research in this area.

Abstract: Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation

</details>


### [463] [Multimodal Representation Alignment for Cross-modal Information Retrieval](https://arxiv.org/abs/2506.08774)
*Fan Xu,Luis A. Leiva*

Main category: cs.IR

TL;DR: This paper explores the alignment of visual and textual features for multimodal retrieval to enhance cross-modal applications.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand and improve the alignment between visual and textual representations to solve challenges in multimodal information retrieval across real-world applications.

Method: The study investigates the geometric relationships in embeddings from various models and aligns them using both standard similarity metrics like cosine similarity and learned ones through neural networks.

Result: Cosine similarity emerged as the best-performing metric for feature alignment, while the Wasserstein distance highlighted the modality gap. Common architectures like MLPs were found inadequate for modeling complex image-text interactions.

Conclusion: The findings provide insights and guidelines for researchers aiming to optimize multimodal feature alignment, emphasizing the need for advanced architectures in real-world use cases.

Abstract: Different machine learning models can represent the same underlying concept
in different ways. This variability is particularly valuable for in-the-wild
multimodal retrieval, where the objective is to identify the corresponding
representation in one modality given another modality as input. This challenge
can be effectively framed as a feature alignment problem. For example, given a
sentence encoded by a language model, retrieve the most semantically aligned
image based on features produced by an image encoder, or vice versa. In this
work, we first investigate the geometric relationships between visual and
textual embeddings derived from both vision-language models and combined
unimodal models. We then align these representations using four standard
similarity metrics as well as two learned ones, implemented via neural
networks. Our findings indicate that the Wasserstein distance can serve as an
informative measure of the modality gap, while cosine similarity consistently
outperforms alternative metrics in feature alignment tasks. Furthermore, we
observe that conventional architectures such as multilayer perceptrons are
insufficient for capturing the complex interactions between image and text
representations. Our study offers novel insights and practical considerations
for researchers working in multimodal information retrieval, particularly in
real-world, cross-modal applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [464] [A System for Accurate Tracking and Video Recordings of Rodent Eye Movements using Convolutional Neural Networks for Biomedical Image Segmentation](https://arxiv.org/abs/2506.08183)
*Isha Puri,David Cox*

Main category: eess.IV

TL;DR: This paper introduces a novel convolutional neural network-based method for accurately identifying pupil and corneal reflections in rodents' eyes, addressing specific challenges like small size and surrounding hair.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in video-based eye tracking, existing methods focus on human eyes and fail to address the unique challenges presented by rodent eyes, such as small size, hair abundance, and variability in eye parameters.

Method: The paper develops a convolutional neural network architecture specifically aimed at biomedical image segmentation for pupil and corneal reflection identification in rodent eyes. The model is designed to be trainable to adapt to varying eye parameters.

Result: The method demonstrates highly accurate identification in rodent gaze tracking, setting a benchmark for practical applications in neuroscience and vision science.

Conclusion: This work provides a state-of-the-art, flexible, and robust solution for rodent eye tracking, significantly advancing research potential in the fields of neuroscience and vision science.

Abstract: Research in neuroscience and vision science relies heavily on careful
measurements of animal subject's gaze direction. Rodents are the most widely
studied animal subjects for such research because of their economic advantage
and hardiness. Recently, video based eye trackers that use image processing
techniques have become a popular option for gaze tracking because they are easy
to use and are completely noninvasive. Although significant progress has been
made in improving the accuracy and robustness of eye tracking algorithms,
unfortunately, almost all of the techniques have focused on human eyes, which
does not account for the unique characteristics of the rodent eye images, e.g.,
variability in eye parameters, abundance of surrounding hair, and their small
size. To overcome these unique challenges, this work presents a flexible,
robust, and highly accurate model for pupil and corneal reflection
identification in rodent gaze determination that can be incrementally trained
to account for variability in eye parameters encountered in the field. To the
best of our knowledge, this is the first paper that demonstrates a highly
accurate and practical biomedical image segmentation based convolutional neural
network architecture for pupil and corneal reflection identification in eye
images. This new method, in conjunction with our automated infrared videobased
eye recording system, offers the state of the art technology in eye tracking
for neuroscience and vision science research for rodents.

</details>


### [465] [Snap-and-tune: combining deep learning and test-time optimization for high-fidelity cardiovascular volumetric meshing](https://arxiv.org/abs/2506.08280)
*Daniel H. Pak,Shubh Thaker,Kyle Baylous,Xiaoran Zhang,Danny Bluestein,James S. Duncan*

Main category: eess.IV

TL;DR: This paper proposes a novel volumetric meshing method for medical images using deep learning combined with test-time optimization for improved spatial accuracy and mesh quality.


<details>
  <summary>Details</summary>
Motivation: There are ongoing limitations in current deep learning-based volumetric meshing approaches, such as reduced mesh quality in high-curvature areas and unrealistic inter-part distances.

Method: The authors introduce a 'snap-and-tune' strategy combining fast DL-based initial shape fitting with test-time optimization for detailed sample-specific mesh corrections.

Result: The method significantly improves spatial accuracy and mesh quality, is fully automated, and does not require additional training labels.

Conclusion: The approach enhances volumetric meshing for medical images, demonstrating its effectiveness by successful application in solid mechanics simulations across two software platforms.

Abstract: High-quality volumetric meshing from medical images is a key bottleneck for
physics-based simulations in personalized medicine. For volumetric meshing of
complex medical structures, recent studies have often utilized deep learning
(DL)-based template deformation approaches to enable fast test-time generation
with high spatial accuracy. However, these approaches still exhibit
limitations, such as limited flexibility at high-curvature areas and
unrealistic inter-part distances. In this study, we introduce a simple yet
effective snap-and-tune strategy that sequentially applies DL and test-time
optimization, which combines fast initial shape fitting with more detailed
sample-specific mesh corrections. Our method provides significant improvements
in both spatial accuracy and mesh quality, while being fully automated and
requiring no additional training labels. Finally, we demonstrate the
versatility and usefulness of our newly generated meshes via solid mechanics
simulations in two different software platforms. Our code is available at
https://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh.

</details>


### [466] [Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models](https://arxiv.org/abs/2506.08520)
*Srinivasan Kidambi,Pravin Nair*

Main category: eess.IV

TL;DR: The paper introduces PnP-Nystra, a plug-and-play module replacing multi-head self-attention (MHSA) to reduce computational complexity in image and video restoration tasks.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of multi-head self-attention (MHSA) limits its efficiency in time-sensitive and resource-constrained environments.

Method: The authors propose PnP-Nystra, a Nyström-based linear approximation of attention, which acts as a drop-in replacement for MHSA, eliminating the need to retrain pre-trained models.

Result: Experiments on tasks like denoising and super-resolution show PnP-Nystra achieves a 2-5x speed-up on GPUs and CPUs, with minimal PSNR drops (up to 1.5 dB).

Conclusion: PnP-Nystra provides an efficient, training-free linear attention module for image and video restoration models, enabling faster inference with acceptable accuracy trade-offs.

Abstract: Multi-head self-attention (MHSA) has become a core component in modern
computer vision models. However, its quadratic complexity with respect to input
length poses a significant computational bottleneck in real-time and resource
constrained environments. We propose PnP-Nystra, a Nystr\"om based linear
approximation of self-attention, developed as a plug-and-play (PnP) module that
can be integrated into the pre-trained image and video restoration models
without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables
efficient acceleration in various window-based transformer architectures,
including SwinIR, Uformer, and RVRT. Our experiments across diverse image and
video restoration tasks, including denoising, deblurring, and super-resolution,
demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU
and a 2-5x speed-up on CPU inference. Despite these significant gains, the
method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To
the best of our knowledge, we are the first to demonstrate a linear attention
functioning as a training-free substitute for MHSA in restoration models.

</details>


### [467] [DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View](https://arxiv.org/abs/2506.08534)
*Donglian Li,Hui Guo,Minglang Chen,Huizhen Chen,Jialing Chen,Bocheng Liang,Pengchen Liang,Ying Tan*

Main category: eess.IV

TL;DR: The paper introduces DCD, a deep learning model for precise segmentation of fetal heart anatomy in ultrasound images, overcoming challenges like noise and anatomical variability.


<details>
  <summary>Details</summary>
Motivation: To address difficulties in segmenting fetal echocardiography images, such as noise, boundary ambiguity, and variability, for better diagnosis of congenital heart disease (CHD).

Method: The model integrates Dense Atrous Spatial Pyramid Pooling (Dense ASPP) for multi-scale feature extraction and a Convolutional Block Attention Module (CBAM) for adaptive feature representation.

Result: DCD demonstrates precise and robust segmentation of key anatomical structures in the fetal apical four-chamber (A4C) ultrasound view.

Conclusion: The proposed DCD model improves the accuracy of prenatal cardiac assessment, reducing the workload on sonographers and advancing the diagnosis of CHD.

Abstract: Accurate segmentation of anatomical structures in the apical four-chamber
(A4C) view of fetal echocardiography is essential for early diagnosis and
prenatal evaluation of congenital heart disease (CHD). However, precise
segmentation remains challenging due to ultrasound artifacts, speckle noise,
anatomical variability, and boundary ambiguity across different gestational
stages. To reduce the workload of sonographers and enhance segmentation
accuracy, we propose DCD, an advanced deep learning-based model for automatic
segmentation of key anatomical structures in the fetal A4C view. Our model
incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,
enabling superior multi-scale feature extraction, and a Convolutional Block
Attention Module (CBAM) to enhance adaptive feature representation. By
effectively capturing both local and global contextual information, DCD
achieves precise and robust segmentation, contributing to improved prenatal
cardiac assessment.

</details>


### [468] [Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification](https://arxiv.org/abs/2506.08623)
*Rinat Prochii,Elizaveta Dakhova,Pavel Birulin,Maxim Sharaev*

Main category: eess.IV

TL;DR: The paper presents a biologically inspired deep learning framework for classifying 16 fetal structures from second-trimester ultrasound images, addressing challenges like low image quality and class imbalance.


<details>
  <summary>Details</summary>
Motivation: The work aims to enhance the accuracy of fetal anatomy classification from ultrasound images, addressing challenges like low image quality, intra-class variability, and class imbalance.

Method: The authors propose a modular deep learning ensemble framework inspired by biological vision systems. It uses two complementary branches for coarse and fine features, combining their outputs for prediction. The models EfficientNet-B0 and EfficientNet-B6 are utilized with LDAM-Focal loss.

Result: The ensemble achieved accuracy > 0.75 for 90% of organs and > 0.85 for 75% of organs, showcasing performance competitive with more complex models across fewer categories.

Conclusion: The biologically inspired modular stacking approach is effective for scalable and robust classification of fetal anatomy in complex clinical scenarios.

Abstract: Accurate classification of second-trimester fetal ultrasound images remains
challenging due to low image quality, high intra-class variability, and
significant class imbalance. In this work, we introduce a simple yet powerful,
biologically inspired deep learning ensemble framework that-unlike prior
studies focused on only a handful of anatomical targets-simultaneously
distinguishes 16 fetal structures. Drawing on the hierarchical, modular
organization of biological vision systems, our model stacks two complementary
branches (a "shallow" path for coarse, low-resolution cues and a "detailed"
path for fine, high-resolution features), concatenating their outputs for final
prediction. To our knowledge, no existing method has addressed such a large
number of classes with a comparably lightweight architecture. We trained and
evaluated on 5,298 routinely acquired clinical images (annotated by three
experts and reconciled via Dawid-Skene), reflecting real-world noise and
variability rather than a "cleaned" dataset. Despite this complexity, our
ensemble (EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss) identifies
90% of organs with accuracy > 0.75 and 75% of organs with accuracy >
0.85-performance competitive with more elaborate models applied to far fewer
categories. These results demonstrate that biologically inspired modular
stacking can yield robust, scalable fetal anatomy recognition in challenging
clinical settings.

</details>


### [469] [MAMBO: High-Resolution Generative Approach for Mammography Images](https://arxiv.org/abs/2506.08677)
*Milica Škipina,Nikola Jovišić,Nicola Dall'Asen,Vanja Švenda,Anil Osman Tur,Slobodan Ilić,Elisa Ricci,Dubravko Ćulibrk*

Main category: eess.IV

TL;DR: The paper introduces MAMBO, a diffusion-based method for generating high-resolution mammograms to aid AI in breast cancer detection.


<details>
  <summary>Details</summary>
Motivation: Training AI models for breast cancer detection requires diverse mammogram datasets, which are limited by privacy and ethical concerns.

Method: MAMBO combines local and global diffusion models to generate high-resolution (3840x3840 pixels) artificial mammograms with fine-grained details.

Result: MAMBO demonstrates strong performance in generating realistic mammograms, super-resolution tasks, and anomaly detection.

Conclusion: The approach can enhance AI-based mammography for better diagnosis and early detection of breast lesions.

Abstract: Mammography is the gold standard for the detection and diagnosis of breast
cancer. This procedure can be significantly enhanced with Artificial
Intelligence (AI)-based software, which assists radiologists in identifying
abnormalities. However, training AI systems requires large and diverse
datasets, which are often difficult to obtain due to privacy and ethical
constraints. To address this issue, the paper introduces MAMmography ensemBle
mOdel (MAMBO), a novel patch-based diffusion approach designed to generate
full-resolution mammograms. Diffusion models have shown breakthrough results in
realistic image generation, yet few studies have focused on mammograms, and
none have successfully generated high-resolution outputs required to capture
fine-grained features of small lesions. To achieve this, MAMBO integrates
separate diffusion models to capture both local and global (image-level)
contexts. The contextual information is then fed into the final patch-based
model, significantly aiding the noise removal process. This thoughtful design
enables MAMBO to generate highly realistic mammograms of up to 3840x3840
pixels. Importantly, this approach can be used to enhance the training of
classification models and extended to anomaly detection. Experiments, both
numerical and radiologist validation, assess MAMBO's capabilities in image
generation, super-resolution, and anomaly detection, highlighting its potential
to enhance mammography analysis for more accurate diagnoses and earlier lesion
detection.

</details>


### [470] [Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment](https://arxiv.org/abs/2506.08716)
*Maximilian Tschuchnig,Lukas Lamminger,Philipp Steininger,Michael Gadermayr*

Main category: eess.IV

TL;DR: This study improves synthetic CT (sCT) generation using multimodal learning, combining CBCT and CT data to reduce artifacts and improve image quality.


<details>
  <summary>Details</summary>
Motivation: CBCT is widely used in intraoperative imaging for its speed and low radiation but suffers from artifacts. Enhancing CBCT quality by generating high-quality synthetic CT can improve clinical outcomes.

Method: The paper proposes a multimodal learning framework that integrates intraoperative CBCT with preoperative CT for enhanced sCT generation. It validates its approach using real-world and synthetic datasets while analyzing factors like alignment and quality.

Result: Multimodal learning for sCT consistently outperforms unimodal methods, especially in cases with well-aligned but low-quality CBCT-CT data.

Conclusion: The approach is robust and reproducible in real-world clinical settings, showing significant potential for improving CBCT imaging quality through sCT generation.

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for real-time
intraoperative imaging due to its low radiation dose and high acquisition
speed. However, despite its high resolution, CBCT suffers from significant
artifacts and thereby lower visual quality, compared to conventional Computed
Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT
(sCT) generation, translating CBCT volumes into the CT domain. In this work, we
enhance sCT generation through multimodal learning, integrating intraoperative
CBCT with preoperative CT. Beyond validation on two real-world datasets, we use
a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT
quality affect sCT quality. The results demonstrate that multimodal sCT
consistently outperform unimodal baselines, with the most significant gains
observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate
that these findings are highly reproducible in real-world clinical datasets.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [471] [TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses](https://arxiv.org/abs/2506.08381)
*He Yang,Fei Ren,Hai-Sui Yu,Xueyu Geng,Pei-Zhi Zhuang*

Main category: physics.geo-ph

TL;DR: The paper introduces a new machine learning approach, TS-PIELM, which significantly improves the accuracy and efficiency of neural networks for soil consolidation analyses.


<details>
  <summary>Details</summary>
Motivation: Conventional PINNs lack sufficient accuracy and efficiency to serve as competitive tools for soil consolidation analysis.

Method: The TS-PIELM divides the consolidation process into time intervals, uses an extreme learning machine (ELM) with fixed input weights, and replaces gradient descent with a linear system solution for faster training.

Result: The TS-PIELM framework achieved efficiency improvements of over 1000 times and accuracy improvements of over 100 times compared to PINN in one-dimensional test cases.

Conclusion: This proposed TS-PIELM demonstrates the viability of physics-informed machine learning for computational geotechnical applications, showing substantial superiority over existing methods.

Abstract: Accuracy and efficiency of the conventional physics-informed neural network
(PINN) need to be improved before it can be a competitive alternative for soil
consolidation analyses. This paper aims to overcome these limitations by
proposing a highly accurate and efficient physics-informed machine learning
(PIML) approach, termed time-stepping physics-informed extreme learning machine
(TS-PIELM). In the TS-PIELM framework the consolidation process is divided into
numerous time intervals, which helps overcome the limitation of PIELM in
solving differential equations with sharp gradients. To accelerate network
training, the solution is approximated by a single-layer feedforward extreme
learning machine (ELM), rather than using a fully connected neural network in
PINN. The input layer weights of the ELM network are generated randomly and
fixed during the training process. Subsequently, the output layer weights are
directly computed by solving a system of linear equations, which significantly
enhances the training efficiency compared to the time-consuming gradient
descent method in PINN. Finally, the superior performance of TS-PIELM is
demonstrated by solving three typical Terzaghi consolidation problems. Compared
to PINN, results show that the computational efficiency and accuracy of the
novel TS-PIELM framework are improved by more than 1000 times and 100 times for
one-dimensional cases, respectively. This paper provides compelling evidence
that PIML can be a powerful tool for computational geotechnics.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [472] [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/abs/2506.08507)
*Kuo Yang,Xingjie Yang,Linhui Yu,Qing Xu,Yan Fang,Xu Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.MA

TL;DR: The paper introduces MasHost, an RL-driven framework for designing autonomous multi-agent systems, achieving superior performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing multi-agent systems that rely on manually crafted or semi-autonomous mechanisms, introducing biases and resisting truly autonomous operation.

Method: Proposes MasHost, framing Mas construction as a graph search problem using reinforcement learning with a new strategy called Hierarchical Relative Policy Optimization (HRPO) for multi-objective optimization.

Result: Extensive experiments across six benchmarks demonstrate MasHost's superiority over existing methods in terms of effectiveness, efficiency, and structure rationality.

Conclusion: MasHost advances Mas design by enabling fully autonomous and query-adaptive systems, pushing the boundaries of existing approaches with a novel RL-driven framework.

Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently
emerged as a powerful paradigm for tackling complex real-world tasks. However,
existing Mas construction methods typically rely on manually crafted
interaction mechanisms or heuristic rules, introducing human biases and
constraining the autonomous ability. Even with recent advances in adaptive Mas
construction, existing systems largely remain within the paradigm of
semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement
Learning (RL)-based framework for autonomous and query-adaptive Mas design. By
formulating Mas construction as a graph search problem, our proposed MasHost
jointly samples agent roles and their interactions through a unified
probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives
pursued in prior works, we introduce component rationality as an additional and
novel design principle in Mas. To achieve this multi-objective optimization, we
propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy
that collaboratively integrates group-relative advantages and action-wise
rewards. To our knowledge, our proposed MasHost is the first RL-driven
framework for autonomous Mas graph construction. Extensive experiments on six
benchmarks demonstrate that MasHost consistently outperforms most competitive
baselines, validating its effectiveness, efficiency, and structure rationality.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [473] [Dynamic Diffusion Schrödinger Bridge in Astrophysical Observational Inversions](https://arxiv.org/abs/2506.08065)
*Ye Zhu,Duo Xu,Zhiwei Deng,Jonathon C. Tan,Olga Russakovsky*

Main category: astro-ph.IM

TL;DR: This paper introduces the Astro-DSB model, a variant of Diffusion Schrödinger Bridge (DSB), to address astrophysical dynamics in star formation, yielding improved interpretability, efficiency, and prediction performance.


<details>
  <summary>Details</summary>
Motivation: The research aims to enhance interpretability and predictive capabilities in modeling dynamical astrophysical systems, such as Giant Molecular Clouds, using advanced generative methodologies like the DSB framework.

Method: The authors leverage the Diffusion Schrödinger Bridge model with a pairwise domain assumption, applied to both simulated and real astrophysical data to assess prediction performance and interpretability.

Result: Astro-DSB outperforms conventional astrostatistical methods and discriminative pixel-to-pixel modeling, especially in Out-Of-Distribution scenarios, improving learning efficiency and predictive insights.

Conclusion: The paper highlights the potential of diffusion-based generative models to align machine learning approaches with physical systems, offering new research avenues in physics-aware generative modeling.

Abstract: We study Diffusion Schr\"odinger Bridge (DSB) models in the context of
dynamical astrophysical systems, specifically tackling observational inverse
prediction tasks within Giant Molecular Clouds (GMCs) for star formation. We
introduce the Astro-DSB model, a variant of DSB with the pairwise domain
assumption tailored for astrophysical dynamics. By investigating its learning
process and prediction performance in both physically simulated data and in
real observations (the Taurus B213 data), we present two main takeaways. First,
from the astrophysical perspective, our proposed paired DSB method improves
interpretability, learning efficiency, and prediction performance over
conventional astrostatistical and other machine learning methods. Second, from
the generative modeling perspective, probabilistic generative modeling reveals
improvements over discriminative pixel-to-pixel modeling in Out-Of-Distribution
(OOD) testing cases of physical simulations with unseen initial conditions and
different dominant physical processes. Our study expands research into
diffusion models beyond the traditional visual synthesis application and
provides evidence of the models' learning abilities beyond pure data
statistics, paving a path for future physics-aware generative models which can
align dynamics between machine learning and real (astro)physical systems.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [474] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph is an AI-powered framework that streamlines computational chemistry and materials science workflows using graph neural networks and large language models (LLMs), automating tasks like molecular structure generation and thermochemistry calculations, while enabling smaller LLMs to handle complex tasks via task decomposition.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in running atomistic simulations, including the complexity of computational methods, diverse software ecosystems, and the need for significant expert intervention.

Method: The authors introduce ChemGraph, which integrates graph neural networks for efficient simulations and LLMs for language understanding, task planning, and scientific reasoning, enabling natural and interactive task execution.

Result: ChemGraph was evaluated on 13 benchmark tasks, showing that smaller LLMs perform well on simpler workflows while task decomposition allows them to handle complex scenarios effectively, sometimes matching larger LLMs like GPT-4o.

Conclusion: ChemGraph makes atomistic simulations in computational chemistry and material sciences more intuitive and accessible by leveraging AI technologies, optimizing both simpler tasks and complex workflows through effective task management.

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [475] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Main category: q-bio.BM

TL;DR: This paper introduces a model for retrieving proteins with similar structures and semantics using a CLIP-style framework, leveraging a large dataset of protein-caption pairs.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenges in functional interpretation of protein structures derived from methods like cryo-EM and by the progress in vision-language models (VLMs).

Method: The paper proposes a CLIP-style contrastive learning framework that aligns 3D protein structures with functional annotations. A dataset of 200,000 protein-caption pairs is used to train the model.

Result: The model demonstrates promising zero-shot retrieval performance in both in-domain and cross-database tasks on datasets like PDB and EMDB.

Conclusion: This work highlights the potential of multimodal foundation models in advancing structure-function understanding in protein biology.

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [476] [Enabling stratified sampling in high dimensions via nonlinear dimensionality reduction](https://arxiv.org/abs/2506.08921)
*Gianluca Geraci,Daniele E. Schiavazzi,Andrea Zanoni*

Main category: math.NA

TL;DR: The paper introduces a stratified sampling method for propagating uncertainty in high-dimensional computational models by using nonlinear dimensionality reduction.


<details>
  <summary>Details</summary>
Motivation: Current stratified sampling methods face challenges in high-dimensional inputs, limiting their efficiency for variance reduction in complex computational models.

Method: The authors propose stratifying with respect to a uniform distribution over the unit interval and then mapping that stratification back to the original high-dimensional space using nonlinear dimensionality reduction techniques.

Result: The proposed method demonstrates effectiveness in high-dimensional contexts and enhances variance reduction when applied in multifidelity Monte Carlo estimators.

Conclusion: By leveraging nonlinear dimensionality reduction, the method overcomes the challenges of stratification in high-dimensional spaces, enabling more efficient uncertainty propagation in computationally expensive models.

Abstract: We consider the problem of propagating the uncertainty from a possibly large
number of random inputs through a computationally expensive model. Stratified
sampling is a well-known variance reduction strategy, but its application, thus
far, has focused on models with a limited number of inputs due to the
challenges of creating uniform partitions in high dimensions. To overcome these
challenges, we perform stratification with respect to the uniform distribution
defined over the unit interval, and then derive the corresponding strata in the
original space using nonlinear dimensionality reduction. We show that our
approach is effective in high dimensions and can be used to further reduce the
variance of multifidelity Monte Carlo estimators.

</details>


### [477] [Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification](https://arxiv.org/abs/2506.08761)
*Matthias Beckmann,Robert Beinert,Jonas Bresch*

Main category: math.NA

TL;DR: This paper discusses the Radon cumulative distribution transforms (R-CDT), focusing on its extensions to make it robust against affine and non-affine transformations in image classification tasks, demonstrated through theoretical and numerical analysis.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness of the R-CDT feature extractor under affine and non-affine transformations, addressing challenges in real-world tasks where transformations or deformations are common.

Method: The paper introduces the max-normalized and mean-normalized versions of the R-CDT, performing sensitivity analyses and leveraging Wasserstein distance metrics to ensure stability and robustness of image separability against affine, non-affine deformations, and noise.

Result: The study demonstrates that the max-normalized R-CDT ensures separability under affine transformations, while the mean-normalized R-CDT extends robustness to non-affine deformations and impulsive noise. Both are validated through numerical experiments.

Conclusion: The novel R-CDT variants established in this paper enhance image classification performance in challenging scenarios, showing robust feature extraction properties adaptable to diverse deformation types.

Abstract: The Radon cumulative distribution transform (R-CDT), is an easy-to-compute
feature extractor that facilitates image classification tasks especially in the
small data regime. It is closely related to the sliced Wasserstein distance and
provably guaranties the linear separability of image classes that emerge from
translations or scalings. In many real-world applications, like the recognition
of watermarks in filigranology, however, the data is subject to general affine
transformations originating from the measurement process. To overcome this
issue, we recently introduced the so-called max-normalized R-CDT that only
requires elementary operations and guaranties the separability under arbitrary
affine transformations. The aim of this paper is to continue our study of the
max-normalized R-CDT especially with respect to its robustness against
non-affine image deformations. Our sensitivity analysis shows that its
separability properties are stable provided the Wasserstein-infinity distance
between the samples can be controlled. Since the Wasserstein-infinity distance
only allows small local image deformations, we moreover introduce a
mean-normalized version of the R-CDT. In this case, robustness relates to the
Wasserstein-2 distance and also covers image deformations caused by impulsive
noise for instance. Our theoretical results are supported by numerical
experiments showing the effectiveness of our novel feature extractors as well
as their robustness against local non-affine deformations and impulsive noise.

</details>


### [478] [Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation](https://arxiv.org/abs/2506.08535)
*Ronald Katende*

Main category: math.NA

TL;DR: The paper introduces the D-decomposition, a non-orthogonal matrix factorization method with advantages in controlling rank, sparsity, and conditioning for improved reconstruction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to design a matrix factorization method that offers enhanced reconstruction accuracy and robustness under sparsity and noise, surpassing limitations of existing methods like LU and SVD.

Method: The method involves defining the D-decomposition variationally and computing it via alternating minimization using a regularized Frobenius loss, with computational complexity per update of O(n²k).

Result: Benchmarks demonstrate superior reconstruction accuracy compared to SVD, CUR, and nonnegative matrix factorization across datasets like MovieLens, MNIST, Olivetti Faces, and gene expression matrices, especially under sparsity and noise conditions.

Conclusion: D-decomposition provides significant benefits in reconstruction metrics and robustness, proving effective for practical applications with sparse and noisy data.

Abstract: We introduce the $D$-decomposition, a non-orthogonal matrix factorization of
the form $A \approx P D Q$, where $P \in \mathbb{R}^{n \times k}$, $D \in
\mathbb{R}^{k \times k}$, and $Q \in \mathbb{R}^{k \times n}$. The
decomposition is defined variationally by minimizing a regularized Frobenius
loss, allowing control over rank, sparsity, and conditioning. Unlike algebraic
factorizations such as LU or SVD, it is computed by alternating minimization.
We establish existence and perturbation stability of the solution and show that
each update has complexity $\mathcal{O}(n^2k)$. Benchmarks against truncated
SVD, CUR, and nonnegative matrix factorization show improved reconstruction
accuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices,
particularly under sparsity and noise.

</details>


### [479] [sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation](https://arxiv.org/abs/2506.08670)
*Renjie Xu,Chong Wu,Maolin Che,Zhuoheng Ran,Yimin Wei,Hong Yan*

Main category: math.NA

TL;DR: The paper introduces sparseGeoHOPCA, a framework for sparse high-dimensional tensor decomposition, improving computational efficiency and scalability while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing methods for sparse higher-order principal component analysis (SHOPCA), such as high computational cost and inefficiencies in high-dimensional, unbalanced data scenarios.

Method: The method unfolds the input tensor along each mode and reformulates sparse optimization subproblems as geometric binary linear problems, bypassing the need for explicit covariance estimation and iterative deflation. It also theoretically establishes equivalences and derives error bounds.

Result: SparseGeoHOPCA achieves computational efficiency with complexity scaling linearly with tensor size, accurately recovers sparse supports in simulations, maintains classification performance under high compression, and performs well in ImageNet image reconstruction tasks.

Conclusion: SparseGeoHOPCA provides a scalable, interpretable, and efficient solution for high-dimensional tensor decomposition, offering theoretical guarantees and practical robustness.

Abstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order
principal component analysis (SHOPCA) that introduces a geometric perspective
to high-dimensional tensor decomposition. By unfolding the input tensor along
each mode and reformulating the resulting subproblems as structured binary
linear optimization problems, our method transforms the original nonconvex
sparse objective into a tractable geometric form. This eliminates the need for
explicit covariance estimation and iterative deflation, enabling significant
gains in both computational efficiency and interpretability, particularly in
high-dimensional and unbalanced data scenarios. We theoretically establish the
equivalence between the geometric subproblems and the original SHOPCA
formulation, and derive worst-case approximation error bounds based on
classical PCA residuals, providing data-dependent performance guarantees. The
proposed algorithm achieves a total computational complexity of
$O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with
tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately
recovers sparse supports in synthetic settings, preserves classification
performance under 10$\times$ compression, and achieves high-quality image
reconstruction on ImageNet, highlighting its robustness and versatility.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [480] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: The paper presents a neural simulator for realistic and real-time soft tissue deformation by using Kelvinlet-based priors for physics-informed learning.


<details>
  <summary>Details</summary>
Motivation: Soft tissue simulation is vital for improving surgical robotics and medical training, where achieving both real-time performance and accuracy is challenging.

Method: The approach integrates Kelvinlet-based priors with neural simulators and uses FEM simulations for enhanced accuracy and consistency in soft tissue modeling.

Result: The method achieves accurate real-time soft tissue deformations and demonstrates effective surgical maneuvers for laparoscopic tissue grasping simulations.

Conclusion: Kelvinlet-augmented learning is a promising strategy for real-time, accurate, and physics-consistent soft tissue simulations in medical applications.

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


### [481] [A Real-time 3D Desktop Display](https://arxiv.org/abs/2506.08064)
*Livio Tenze,Enrique Canessa*

Main category: cs.GR

TL;DR: The paper introduces an extended version of the altiro3D library for real-time 3D video stream synthesis and rendering using AI-based depth extraction from 2D images.


<details>
  <summary>Details</summary>
Motivation: To create realistic 3D experiences using easily accessible 2D webcam images or video streams, addressing the demand for interactive, glass-free holographic displays.

Method: The method leverages the MiDaS CNN for depth map extraction from single 2D images. AI techniques enhance performance, and a multi-platform GUI simplifies user interactions and screen region selection. Output is rendered to 3D light-field devices like Looking Glass Portrait.

Result: The extended altiro3D library can now process various input types, including static images, video streams, and desktop screen portions. It renders these into light-field 3D formats in real time.

Conclusion: The extended altiro3D library offers a viable, real-time solution for creating immersive 3D holographic outputs from 2D sources, with an intuitive user interface and applications in diverse desktop environments.

Abstract: A new extended version of the altiro3D C++ Library -- initially developed to
get glass-free holographic displays starting from 2D images -- is here
introduced aiming to deal with 3D video streams from either 2D webcam images or
flat video files. These streams are processed in real-time to synthesize
light-fields (in Native format) and feed realistic 3D experiences. The core
function needed to recreate multiviews consists on the use of MiDaS
Convolutional Neural Network (CNN), which allows to extract a depth map from a
single 2D image. Artificial Intelligence (AI) computing techniques are applied
to improve the overall performance of the extended altiro3D Library. Thus,
altiro3D can now treat standard images, video streams or screen portions of a
Desktop where other apps may be also running (like web browsers, video chats,
etc) and render them into 3D. To achieve the latter, a screen region need to be
selected in order to feed the output directly into a light-field 3D device such
as Looking Glass (LG) Portrait. In order to simplify the acquisition of a
Desktop screen area by the user, a multi-platform Graphical User Interface has
been also implemented. Sources available at:
https://github.com/canessae/altiro3D/releases/tag/2.0.0

</details>


### [482] [Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos](https://arxiv.org/abs/2506.08334)
*Weikun Peng,Jun Lv,Cewu Lu,Manolis Savva*

Main category: cs.GR

TL;DR: The paper addresses the limitation of reconstructing articulated objects using casually captured RGBD video, introducing a new coarse-to-fine framework for joint parameter inference and part segmentation.


<details>
  <summary>Details</summary>
Motivation: Understanding kinematic structures and reconstructing articulated objects is essential for applications in robotics and embodied AI but is currently limited by the need for carefully captured data.

Method: The proposed approach utilizes a coarse-to-fine framework that processes dynamic RGBD videos to infer joint parameters of articulated objects and segment their movable parts.

Result: The method was tested on a synthetic dataset of 784 videos and demonstrated significant performance improvements over existing methods in reconstructing both synthetic and real articulated objects.

Conclusion: Casually captured dynamic RGBD videos can be effectively used for reconstructing articulated objects, making the process scalable and generalizable while overcoming challenges of simultaneous object-camera movement and occlusions.

Abstract: Articulated objects are prevalent in daily life. Understanding their
kinematic structure and reconstructing them have numerous applications in
embodied AI and robotics. However, current methods require carefully captured
data for training or inference, preventing practical, scalable, and
generalizable reconstruction of articulated objects. We focus on reconstruction
of an articulated object from a casually captured RGBD video shot with a
hand-held camera. A casually captured video of an interaction with an
articulated object is easy to acquire at scale using smartphones. However, this
setting is quite challenging, as the object and camera move simultaneously and
there are significant occlusions as the person interacts with the object. To
tackle these challenges, we introduce a coarse-to-fine framework that infers
joint parameters and segments movable parts of the object from a dynamic RGBD
video. To evaluate our method under this new setting, we build a 20$\times$
larger synthetic dataset of 784 videos containing 284 objects across 11
categories. We compare our approach with existing methods that also take video
as input. Experiments show that our method can reconstruct synthetic and real
articulated objects across different categories from dynamic RGBD videos,
outperforming existing methods significantly.

</details>


### [483] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Akşit*

Main category: cs.GR

TL;DR: The paper introduces a novel technique to model amplitude and phase properties of light in 3D scenes for holographic displays using complex-valued Gaussian primitives without relying on intensity-based intermediaries.


<details>
  <summary>Details</summary>
Motivation: To improve physically plausible rendering, particularly in holographic displays, by directly modeling light waves without computationally expensive hologram re-optimization.

Method: The authors reformulate 3D Gaussian splatting using complex-valued Gaussian primitives optimized from RGBD multi-view images to represent holographic scenes in 3D.

Result: The proposed method achieves speed improvements of 30x-10,000x while maintaining comparable image quality compared to state-of-the-art methods.

Conclusion: This work represents a significant step towards computationally efficient and geometrically aligned physically plausible holographic scene representations.

Abstract: Modeling the full properties of light, including both amplitude and phase, in
3D representations is crucial for advancing physically plausible rendering,
particularly in holographic displays. To support these features, we propose a
novel representation that optimizes 3D scenes without relying on
intensity-based intermediaries. We reformulate 3D Gaussian splatting with
complex-valued Gaussian primitives, expanding support for rendering with light
waves. By leveraging RGBD multi-view images, our method directly optimizes
complex-valued Gaussians as a 3D holographic scene representation. This
eliminates the need for computationally expensive hologram re-optimization.
Compared with state-of-the-art methods, our method achieves 30x-10,000x speed
improvements while maintaining on-par image quality, representing a first step
towards geometrically aligned, physically plausible holographic scene
representations.

</details>


### [484] [Fine-Grained Spatially Varying Material Selection in Images](https://arxiv.org/abs/2506.09023)
*Julia Guerrero-Viu,Michael Fischer,Iliyan Georgiev,Elena Garces,Diego Gutierrez,Belen Masia,Valentin Deschaintre*

Main category: cs.GR

TL;DR: A new method for robust image material selection using vision transformers (ViT) and a novel dataset (DuMaS) offering texture and subtexture annotations.


<details>
  <summary>Details</summary>
Motivation: To improve robustness in selecting materials in images for editing purposes, overcoming lighting and reflectance challenges.

Method: Introduced a multi-resolution strategy leveraging vision transformer (ViT) features and proposed the DuMaS dataset with detailed annotations for texture and subtexture analysis.

Result: Achieved finer and more stable selection results compared to previous techniques.

Conclusion: The approach enhances material selection robustness and granularity, facilitating advanced image editing capabilities.

Abstract: Selection is the first step in many image editing processes, enabling faster
and simpler modifications of all pixels sharing a common modality. In this
work, we present a method for material selection in images, robust to lighting
and reflectance variations, which can be used for downstream editing tasks. We
rely on vision transformer (ViT) models and leverage their features for
selection, proposing a multi-resolution processing strategy that yields finer
and more stable selection results than prior methods. Furthermore, we enable
selection at two levels: texture and subtexture, leveraging a new two-level
material selection (DuMaS) dataset which includes dense annotations for over
800,000 synthetic images, both on the texture and subtexture levels.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [485] [Confidence Boosts Trust-Based Resilience in Cooperative Multi-Robot Systems](https://arxiv.org/abs/2506.08807)
*Luca Ballotta,Áron Vékássy,Stephanie Gil,Michal Yemini*

Main category: eess.SP

TL;DR: The paper proposes a protocol to enhance the resilience of multi-robot systems against cyberattacks by using trustworthiness indications from the physical communication channel.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of multi-robot systems to cyberattacks that exploit wireless communication, posing risks to safety and performance.

Method: A resilient protocol is developed using a parameter λt to quantify the confidence in the legitimacy of nearby robots based on physical channel indications.

Result: The protocol ensures resilient coordination even with many malicious robots, offering a tradeoff between optimal coordination and task execution speed.

Conclusion: The proposed protocol is effective for resilient multi-robot operations, as demonstrated in experiments with autonomous car platoons under malicious conditions.

Abstract: Wireless communication-based multi-robot systems open the door to
cyberattacks that can disrupt safety and performance of collaborative robots.
The physical channel supporting inter-robot communication offers an attractive
opportunity to decouple the detection of malicious robots from task-relevant
data exchange between legitimate robots. Yet, trustworthiness indications
coming from physical channels are uncertain and must be handled with this in
mind. In this paper, we propose a resilient protocol for multi-robot operation
wherein a parameter {\lambda}t accounts for how confident a robot is about the
legitimacy of nearby robots that the physical channel indicates. Analytical
results prove that our protocol achieves resilient coordination with
arbitrarily many malicious robots under mild assumptions. Tuning {\lambda}t
allows a designer to trade between near-optimal inter-robot coordination and
quick task execution; see Fig. 1. This is a fundamental performance tradeoff
and must be carefully evaluated based on the task at hand. The effectiveness of
our approach is numerically verified with experiments involving platoons of
autonomous cars where some vehicles are maliciously spoofed.

</details>
