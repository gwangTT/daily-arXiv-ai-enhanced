<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 69]
- [cs.AR](#cs.AR) [Total: 8]
- [cs.CL](#cs.CL) [Total: 111]
- [cs.CV](#cs.CV) [Total: 161]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.LG](#cs.LG) [Total: 167]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 32]
- [cs.SE](#cs.SE) [Total: 25]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 8]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cs.IT](#cs.IT) [Total: 3]
- [hep-ex](#hep-ex) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [eess.SY](#eess.SY) [Total: 4]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.DL](#cs.DL) [Total: 5]
- [cs.CY](#cs.CY) [Total: 20]
- [math.NA](#math.NA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 12]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 3]
- [cs.MS](#cs.MS) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [cs.HC](#cs.HC) [Total: 14]
- [eess.IV](#eess.IV) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 6]
- [cs.CR](#cs.CR) [Total: 18]
- [econ.GN](#econ.GN) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [math.HO](#math.HO) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [stat.ME](#stat.ME) [Total: 5]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [eess.AS](#eess.AS) [Total: 7]
- [cs.IR](#cs.IR) [Total: 9]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [math.LO](#math.LO) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Online parameter estimation for the Crazyflie quadcopter through an EM algorithm](https://arxiv.org/abs/2601.17009)
*Yanhua Zhao*

Main category: cs.AI

TL;DR: The paper examines the impact of random noise on quadcopter systems, estimating the states using an extended Kalman filter and analyzing parameter estimation through expectation maximization.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the reliability and functionality of drones in challenging environments by addressing their sensitivity to noise and improving state and parameter estimation methods.

Method: Random noise is introduced into the quadcopter system, the states are estimated using the extended Kalman filter, a linear quadratic Gaussian controller is implemented, and the expectation maximization algorithm is used for parameter estimation.

Result: The findings reveal that online parameter estimation shows a slightly larger range of convergence values compared to offline parameter estimation.

Conclusion: The research demonstrates the feasibility of using these methodologies to improve quadcopter operations, particularly in noisy conditions.

Abstract: Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.

</details>


### [2] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu,Dhari Gandhi,Himanshu Joshi,Ahmad Rezaie Mianroodi,Sedef Akinli Kocak,Dhanesh Ramachandran*

Main category: cs.AI

TL;DR: The paper discusses the unique interpretability and AI safety challenges in agentic systems created by LLMs, highlighting the limitations of current methods and proposing directions for new analytical approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the AI safety challenges and interpretability gaps in agentic systems built on LLMs, especially given their autonomous and complex decision-making nature.

Method: The paper evaluates the limitations of existing static-model-based interpretability techniques in the context of agentic systems and proposes future directions for developing tailored methods for these systems.

Result: The study identifies critical gaps in current interpretability methods for agentic systems and outlines required advances to support safe and accountable AI deployment.

Conclusion: Advances in interpretability methods tailored to agentic systems are essential for embedding oversight, ensuring AI safety and accountability throughout their lifecycle.

Abstract: Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [3] [Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction](https://arxiv.org/abs/2601.17188)
*Swapn Shah,Wlodek Zadrozny*

Main category: cs.AI

TL;DR: The paper investigates unification of symbolic reasoning and neural networks through Tensor Logic, validated via three experiments on reasoning and predictions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unifying symbolic reasoning (reliability and interpretability) with neural networks (scalability and learning capabilities).

Method: Using Tensor Logic, experiments include (1) tensor-based logical rule computation in genealogy graphs, (2) embedding space reasoning with neural networks for zero-shot inference, and (3) evaluation on FB15k-237 knowledge graph.

Result: Key results include proof of equivalence between recursive rules and tensor contractions, successful zero-shot compositional inference using neural networks, and accurate multi-hop link prediction on a large-scale knowledge graph.

Conclusion: Tensor Logic effectively bridges symbolic reasoning and neural approaches, providing a mathematically sound and empirically validated framework for unified AI systems.

Abstract: The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework through three experiments. First, we demonstrate the equivalence between recursive Datalog rules and iterative tensor contractions by computing the transitive closure of a biblical genealogy graph containing 1,972 individuals and 1,727 parent-child relationships, converging in 74 iterations to discover 33,945 ancestor relationships. Second, we implement reasoning in embedding space by training a neural network with learnable transformation matrices, demonstrating successful zero-shot compositional inference on held-out queries. Third, we validate the Tensor Logic superposition construction on FB15k-237, a large-scale knowledge graph with 14,541 entities and 237 relations. Using Domingos's relation matrix formulation $R_r = E^\top A_r E$, we achieve MRR of 0.3068 on standard link prediction and MRR of 0.3346 on a compositional reasoning benchmark where direct edges are removed during training, demonstrating that matrix composition enables multi-hop inference without direct training examples.

</details>


### [4] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi,Tomohisa Seki,Hiromasa Ito,Toru Takiguchi,Kazuhiko Ohe,Yoshimasa Kawazoe*

Main category: cs.AI

TL;DR: Researchers developed a generative simulator model for predicting patient trajectories using over 200 million clinical records. The model provides realistic future timelines, matching observed patient data.


<details>
  <summary>Details</summary>
Motivation: Simulating patient trajectories is important for personalized treatment planning and virtual clinical trials, but is challenging due to complex influences.

Method: The study uses a generative simulator model trained on large-scale clinical records to predict fine-grained future patient timelines.

Result: The model demonstrates high accuracy in mimicking real patient data, maintaining event occurrence rates and providing reliable future event probabilities.

Conclusion: Real-world clinical data is highly valuable for scalable in silico modeling of patient care, enabling transformative advances in personalized medicine.

Abstract: Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [5] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu,Linglong Kong,Jian Pei*

Main category: cs.AI

TL;DR: The abstract analyzes multi-agent systems' performance under fixed inference budgets and identifies bottlenecks resulting from architecture limitations. It presents a theoretical framework predicting when synergy, saturation, or collapse occurs in agent systems.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand and predict the behaviors of multi-agent systems with budget constraints, focusing on limitations like context windows, communication losses, and shared agent failures.

Method: The paper introduces a theoretical framework based on defined parameters such as agent compute-performance scaling $β$, communication fidelity $γ(m)$, shared-error correlation $ρ$, and context window size $W$. Mathematical analyses, including phase transitions and closed-form risk derivations, were conducted.

Result: The paper predicts when amplification or degradation occurs in agent systems and provides explicit compute allocation rules, thresholds, and designs that optimize performance.

Conclusion: The theory clarifies key bottlenecks and phase behaviors in multi-agent systems and validates these through synthetic simulations and comparisons to large-scale studies.

Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [6] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao,Hongteng Xu*

Main category: cs.AI

TL;DR: TheoremForge introduces a cost-effective formal data synthesis pipeline to improve formal mathematics data workflows, achieving superior performance and lower costs.


<details>
  <summary>Details</summary>
Motivation: The research addresses the high cost and inefficiency of formal mathematics workflows, aiming to overcome the lack of scalable, open-source data corpora.

Method: The proposed solution, TheoremForge, decomposes formalization into five sub-tasks and employs a Decoupled Extraction Strategy to maximize the value of failed workflows.

Result: TheoremForge achieves a 12.6% Verified Rate, surpassing the 8.6% baseline, at a low cost of $0.481 per successful trajectory, and increases data yield by 1.6×.

Conclusion: TheoremForge establishes itself as a scalable and efficient framework for synthesizing formal mathematics data, offering a pathway for future expert-model training.

Abstract: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [7] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin,Ren-Hao Deng,Yao-Ting Hsieh,En-Ming Huang,Shih-Hao Hung*

Main category: cs.AI

TL;DR: This paper introduces EvolVE, a novel framework that overcomes challenges in Verilog's design cycle using evolutionary strategies and Structured Testbench Generation (STG). Achieving state-of-the-art results on benchmarks, it significantly optimizes Power, Performance, and Area (PPA) metrics in hardware design.


<details>
  <summary>Details</summary>
Motivation: The labor-intensive nature of Verilog's design process and the limitations of Large Language Models (LLMs) in capturing hardware systems' logic prompted the exploration of automated methods to enhance functional correctness and optimization.

Method: EvolVE uses multiple evolutionary strategies, primarily Monte Carlo Tree Search (MCTS) and Idea-Guided Refinement (IGR), paired with Structured Testbench Generation (STG). The IC-RTL benchmark was also introduced to assess optimization strategies on industry-scale problems.

Result: EvolVE achieved 98.1% accuracy on VerilogEval v2 and 92% on RTLLM v2. On IC-RTL industry benchmarks, it surpassed reference designs, improving Huffman Coding PPA by 66% and geometric mean PPA across all problems by 17%.

Conclusion: EvolVE establishes itself as a robust framework for chip design, demonstrating state-of-the-art accuracy and optimization on both standard and industry-scale benchmarks.

Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [8] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: The paper formalizes AGI as a semantic predicate tied to tasks, resources, and performance, showing theoretical limitations in its existence, robustness, self-verification, and universality.


<details>
  <summary>Details</summary>
Motivation: To determine if claims about AGI's existence, robustness, or self-verification can be supported by a coherent theoretical framework.

Method: AGI is formalized axiomatically using task families, distributions, performance metrics, and resource constraints. Mathematical proofs and arguments (e.g., Rice-style and Gödel--Tarski approaches) are used.

Result: Key findings include: AGI's generality being relational to task distributions, lack of universal robustness due to task distribution perturbations, finite-resource constraints limiting generalization, and AGI's self-certification being theoretically impossible.

Conclusion: Claims on universal or self-verifying AGI are undefined without explicit formalization, and empirical AI progress does not ensure achievement of such self-certifying intelligence.

Abstract: We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [9] [Are We Evaluating the Edit Locality of LLM Model Editing Properly?](https://arxiv.org/abs/2601.17343)
*Wei Liu,Haomei Xu,Hongkai Liu,Zhiying Deng,Ruixuan Li,Heng Huang,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: The paper addresses issues in evaluating specificity for model editing in large language models and proposes an improved evaluation protocol.


<details>
  <summary>Details</summary>
Motivation: To ensure effective model editing by balancing knowledge injection efficacy with specificity, avoiding preservation of non-target knowledge.

Method: Analyze limitations of current specificity metrics and propose a new evaluation protocol that adjusts strictness and avoids biases.

Result: Find improved metrics derived from the proposed protocol to enhance sensitivity and correlation with specificity regularizers, enabling better evaluation of knowledge preservation.

Conclusion: The new protocol significantly advances specificity evaluation for model editing, allowing fine-grained comparison of methods.

Abstract: Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.

</details>


### [10] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu,Changyong Qi,Tong Liu,Bohao Zhang,Anna He,Bingqian Jiang,Longwei Zheng,Xiaoqing Gu*

Main category: cs.AI

TL;DR: This research introduces a Multi-Agent Learning Path Planning framework (MALPP) using large language models (LLMs) to enhance adaptive learning paths in higher education, outperforming existing models in quality and consistency.


<details>
  <summary>Details</summary>
Motivation: Current learning path systems lack transparency, adaptability, and learner-centered explainability, limiting their effectiveness in personalized education.

Method: A framework (MALPP) utilizing three task-specific intelligent agents, powered by LLMs, collaborates via role- and rule-based mechanisms. It is grounded in cognitive theories to generate and refine tailored learning paths with interpretable feedback.

Result: MALPP, tested on the MOOCCubeX dataset with seven LLMs, significantly improved learning path quality, knowledge consistency, and cognitive load alignment compared to baselines. Ablation studies confirm the robustness of its collaborative design.

Conclusion: The study provides an advanced, explainable, and scalable solution for personalized education, integrating trustworthy AI to enhance learner-centered adaptive instruction.

Abstract: The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [11] [Auditing Disability Representation in Vision-Language Models](https://arxiv.org/abs/2601.17348)
*Srikant Panda,Sourabh Singh Yadav,Palkesh Malviya*

Main category: cs.AI

TL;DR: The paper investigates how vision-language models (VLMs) behave when describing disability-centric images, showing biases and interpretation inaccuracies. It proposes a benchmark and methods to improve model fidelity.


<details>
  <summary>Details</summary>
Motivation: To understand and address the biases and inaccuracies of VLMs in describing socially sensitive topics, particularly in disability contexts, where inappropriate interpretations and affective biases occur.

Method: The authors developed a benchmark using paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) to evaluate 15 VLMs across 9 disability categories. They utilized text-based metrics and an LLM-as-judge framework validated by individuals with lived disability experience.

Result: The findings reveal that disability-context introduction degrades interpretive fidelity, causing speculative inference, biased sentiment, and deficit framing. These effects are amplified by race and gender factors.

Conclusion: Targeted prompting and preference fine-tuning can improve VLM accuracy and reduce biased interpretive shifts. There is a need to better align VLMs when dealing with disability contexts to achieve fair and factual representations.

Abstract: Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.

</details>


### [12] [A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models](https://arxiv.org/abs/2601.17426)
*Zhengqing Zang,Yuqi Ding,Yanmei Gu,Changkai Song,Zhengkai Yang,Guoping Du,Junbo Zhao,Haobo Wang*

Main category: cs.AI

TL;DR: The paper examines whether large language models (LLMs) evolve their logical reasoning frameworks like human logic and explores their syllogistic reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand if and how large language models progress from intuition-driven inference to formal logical frameworks, mimicking the historical evolution of human reasoning.

Method: The researchers test the syllogistic reasoning of state-of-the-art (SOTA) LLMs using a specially designed dataset, with a focus on traditional and modern logic frameworks.

Result: Key findings are that larger models tend toward modern logic, deliberative thinking accelerates logical reasoning shifts, and the base model significantly impacts results.

Conclusion: The paper advances understanding of LLM logical reasoning, offering insights into their evolution and capacity for formal reasoning systems.

Abstract: Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.

</details>


### [13] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst,Tawab Safi,Joseph Edell,Vashisht Ganesh,Karime Maamari*

Main category: cs.AI

TL;DR: Lattice is a framework for conversational AI guardrails that adapt and improve over time, surpassing current static rule methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of static rule-based guardrails for conversational AI, which are unable to adapt to new threats or deployment contexts.

Method: Lattice operates in two stages: (1) construction phase using labeled examples and iterative optimization; (2) continuous improvement phase employing risk assessment, adversarial testing, and closed-loop optimization.

Result: On the ProsocialDialog dataset, Lattice achieves 91% F1 score, significantly outperforming keyword baselines (+43pp), LlamaGuard (+25pp), and NeMo (+4pp). In addition, continuous improvement delivers a 7pp F1 score increase on cross-domain data.

Conclusion: Lattice successfully demonstrates that guardrails for conversational AI can be effectively self-constructed and continuously enhanced via iterative optimization and autonomous improvement mechanisms.

Abstract: Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [14] [Cognitive Platform Engineering for Autonomous Cloud Operations](https://arxiv.org/abs/2601.17542)
*Vinoth Punniyamoorthy,Nitin Saksena,Srivenkateswara Reddy Sankiti,Nachiappan Chockalingam,Aswathnarayan Muthukrishnan Kirubakaran,Shiva Kumar Reddy Carimireddy,Durgaraman Maruthavanan*

Main category: cs.AI

TL;DR: The paper introduces 'Cognitive Platform Engineering,' a next-gen concept for self-managing cloud-native systems, demonstrating improvements in resolving issues efficiently and sustainably through a smart, integrated architecture.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of traditional DevOps practices, which struggle with the increasing scale, telemetry data volume, and dynamic configurations of cloud-native systems.

Method: They propose a four-layer architecture combining data collection, intelligent inference, policy-driven orchestration, and human interaction to form a continuous feedback loop. A prototype utilizing Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection validates this approach.

Result: The prototype demonstrated enhanced mean time to resolution, greater resource efficiency, and ensured compliance, showcasing the benefits of embedding intelligence for adaptive and resilient systems.

Conclusion: The findings highlight the potential of intelligent platform engineering in fostering resilient, self-adjusting cloud platforms. The paper also opens pathways for future research in reinforcement learning, explainable governance, and sustainable self-maintenance of cloud systems.

Abstract: Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.

</details>


### [15] [JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research](https://arxiv.org/abs/2601.17564)
*Aadam,Monu Verma,Mohamed Abdel-Mottaleb*

Main category: cs.AI

TL;DR: The paper introduces JaxARC, a new high-performance RL environment for the Abstraction and Reasoning Corpus (ARC) implemented in JAX, enabling massive computational speedup and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address computational bottlenecks in existing Gymnasium-based RL environments for ARC, which limit the scalability and efficiency of experiments on human-like inductive reasoning tasks.

Method: The authors implemented a functional, stateless architecture for JaxARC in JAX, enabling massive parallelism and faster processing speeds for reinforcement learning in ARC tasks.

Result: JaxARC achieves a speedup of 38-5,439x over Gymnasium environments at comparable batch sizes and supports a peak throughput of 790M steps per second. It also supports multiple ARC datasets and flexible configurations, making large-scale research practical.

Conclusion: JaxARC offers a substantial improvement in computational efficiency and scalability for RL research on ARC, making previously infeasible large-scale experiments possible. It is open-source and available on GitHub.

Abstract: The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.

</details>


### [16] [Discovery of Feasible 3D Printing Configurations for Metal Alloys via AI-driven Adaptive Experimental Design](https://arxiv.org/abs/2601.17587)
*Azza Fadhel,Nathaniel W. Zuckschwerdt,Aryan Deshwal,Susmita Bose,Amit Bandyopadhyay,Jana Doppa*

Main category: cs.AI

TL;DR: Configuring parameters for metal 3D printing is resource-intensive. This paper uses AI-driven methods and domain knowledge to optimize experiments, dramatically improving output quality of GRCop-42 alloy for aerospace applications.


<details>
  <summary>Details</summary>
Motivation: The trial-and-error method for additive manufacturing is highly inefficient due to large configuration spaces and high resource costs. The paper aims to offer a faster and cost-effective approach to find feasible parameter configurations.

Method: This paper utilizes AI-driven adaptive experimental design combined with domain knowledge, employing surrogate modeling to select input configurations for validation effectively.

Result: The proposed methodology achieved defect-free outputs of GRCop-42 alloy in three months, significantly faster and more efficient than manual experimentation methods.

Conclusion: The paper introduced a novel, effective approach for GRCop-42 fabrication using accessible infrared laser platforms, reducing costs and decentralizing production for aerospace applications.

Abstract: Configuring the parameters of additive manufacturing processes for metal alloys is a challenging problem due to complex relationships between input parameters (e.g., laser power, scan speed) and quality of printed outputs. The standard trial-and-error approach to find feasible parameter configurations is highly inefficient because validating each configuration is expensive in terms of resources (physical and human labor) and the configuration space is very large. This paper combines the general principles of AI-driven adaptive experimental design with domain knowledge to address the challenging problem of discovering feasible configurations. The key idea is to build a surrogate model from past experiments to intelligently select a small batch of input configurations for validation in each iteration. To demonstrate the effectiveness of this methodology, we deploy it for Directed Energy Deposition process to print GRCop--42, a high-performance copper--chromium--niobium alloy developed by NASA for aerospace applications. Within three months, our approach yielded multiple defect-free outputs across a range of laser powers dramatically reducing time to result and resource expenditure compared to several months of manual experimentation by domain scientists with no success. By enabling high-quality GRCop--42 fabrication on readily available infrared laser platforms for the first time, we democratize access to this critical alloy, paving the way for cost-effective, decentralized production for aerospace applications.

</details>


### [17] [Intelligence Requires Grounding But Not Embodiment](https://arxiv.org/abs/2601.17588)
*Marcus Ma,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: The paper argues that intelligence does not require physical embodiment but instead requires grounding, achieved through motivation, predictive ability, understanding causality, and learning from experience.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve the ongoing debate about whether embodiment is necessary for intelligence by investigating the role of grounding and proposing a framework to define intelligence.

Method: The authors define intelligence through four key properties and use a thought experiment of a grounded, non-embodied LLM agent in a digital environment to demonstrate their claims.

Result: The argument is presented that grounding is sufficient for intelligence and that embodiment is not necessary, supported by theoretical reasoning and the presented thought experiment.

Conclusion: Grounding, rather than embodiment, is argued to be the essential requirement for intelligence, providing a new perspective on how intelligent systems can be designed.

Abstract: Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.

</details>


### [18] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang,Liting Huang,Guanghao Wu,Preslav Nakov,Heng Ji,Usman Naseem*

Main category: cs.AI

TL;DR: The paper introduces Health-ORSC-Bench, a benchmark to assess safe and effective response quality in healthcare-focused LLMs, tackling issues of over-refusal and unsafe compliance.


<details>
  <summary>Details</summary>
Motivation: Improve safety alignment of LLMs in the healthcare domain to ensure models avoid over-refusing benign queries or complying with harmful ones.

Method: Developed Health-ORSC-Bench, a benchmark with 31,920 benign prompts across seven health categories, and evaluated 30 LLMs, varying in size and intent ambiguity.

Result: Found that safety-optimized models often over-refuse up to 80% of borderline queries, while domain-specific models trade safety for utility. Larger models had higher over-refusal tendencies compared to smaller ones.

Conclusion: Current LLMs face challenges in balancing refusal and compliance in healthcare contexts, highlighting the need for nuanced calibration frameworks like Health-ORSC-Bench.

Abstract: Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [19] [Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems](https://arxiv.org/abs/2601.17744)
*Amjad Fatmi*

Main category: cs.AI

TL;DR: Faramesh introduces a protocol-agnostic execution control system for autonomous agents, ensuring authorization for agent-driven actions before execution.


<details>
  <summary>Details</summary>
Motivation: To address the lack of mandatory checkpoints in agent stacks for controlling real-world actions triggered by autonomous agents.

Method: Faramesh implements an Action Authorization Boundary (AAB), canonicalizes agent's intent, evaluates actions based on policies/state, and provides auditable decision artifacts (PERMIT/DEFER/DENY).

Result: The system ensures deterministic, enforceable governance for autonomous execution, independent of frameworks, agents, or transport protocols.

Conclusion: Faramesh provides predictable control for agent-driven actions, enhancing governance and auditability in autonomous systems.

Abstract: Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.

</details>


### [20] [DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories](https://arxiv.org/abs/2601.17678)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: This paper introduces DIML, a framework for recovering unknown mechanisms based on observed behaviors of self-interested agents, leveraging likelihood-based modeling of multi-agent learning dynamics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of recovering unstructured incentive mechanisms from the behavioral data of agents, as existing approaches primarily focus on structured mechanisms or forward optimization rather than inference from observational settings.

Method: The authors propose DIML, which uses a likelihood-based framework involving differentiable modeling of multi-agent learning dynamics. It incorporates counterfactual payoffs for predicting observed actions and ensures identifiability and statistical consistency under specific conditions.

Result: DIML successfully recovers incentive differences in various simulated environments, including neural mechanisms and large-scale games. It demonstrates competitive performance compared to oracle methods in small environments and maintains scalability in larger setups.

Conclusion: DIML offers a robust framework for inverse mechanism learning, supporting identification, counterfactual prediction, and scalability across diverse environments. This makes it valuable for understanding and replicating incentive systems in observational settings.

Abstract: We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.

</details>


### [21] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

Main category: cs.AI

TL;DR: Success conditioning is proven to solve a trust-region optimization problem by maximizing policy improvement under a $χ^2$ divergence constraint.


<details>
  <summary>Details</summary>
Motivation: The paper aims to clarify the optimization problem solved by success conditioning techniques used in policy improvement across various applications.

Method: A theoretical analysis was conducted to establish a formal connection between success conditioning and trust-region optimization, including metrics like relative policy improvement and action-influence.

Result: Success conditioning is identified as a conservative improvement operator that ensures safety from performance degradation or distribution shift, while amplifying improvement and revealing its constraints when failing.

Conclusion: Success conditioning is a principled method for policy improvement with predictable outcomes and identifiable limitations, especially in cases involving return thresholding trade-offs.

Abstract: A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [22] [SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL](https://arxiv.org/abs/2601.17699)
*Harper Hua,Zhen Han,Zhengyuan Shen,Jeremy Lee,Patrick Guan,Qi Zhu,Sullam Jeoung,Yueyan Chen,Yunfei Bai,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.AI

TL;DR: SQL-Trail, a multi-turn RL framework, improves Text-to-SQL by using iterative reasoning, achieving up to 18x efficiency gains and outperforming larger systems.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the significant performance gap between AI systems and human experts in challenging Text-to-SQL benchmarks, such as BIRD-SQL, caused by the limitations of single-pass paradigms.

Method: The paper presents SQL-Trail, a multi-turn reinforcement learning framework, which utilizes interaction with the environment and execution feedback with mechanisms like adaptive turn-budget allocation and composite reward panels.

Result: SQL-Trail demonstrates a new state of the art across Text-to-SQL benchmarks, achieving substantial data efficiency (up to 18x improvement) and outperforming larger proprietary models by an average of 5%.

Conclusion: Iterative, agentic Text-to-SQL generation workflows like SQL-Trail enable robust performance, closing the gap in challenging benchmarks and showcasing the scalability of collaborative RL frameworks.

Abstract: While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.

</details>


### [23] [The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data](https://arxiv.org/abs/2601.17717)
*Kaituo Zhang,Mingzhi Hu,Hoang Anh Duy Le,Fariha Kabir Torsha,Zhimeng Jiang,Minh Khai Bui,Chia-Yuan Chang,Yu-Neng Chuang,Zhen Xiong,Ying Lin,Guanchu Wang,Na Zou*

Main category: cs.AI

TL;DR: The paper introduces the "LLM Data Auditor framework" to evaluate the quality and trustworthiness of synthetic data generated by LLMs across six modalities, highlighting the shortcomings in current evaluation practices and proposing improvements.


<details>
  <summary>Details</summary>
Motivation: LLMs have transformed data generation but face challenges in ensuring the quality of synthetic data. There is a lack of focus on intrinsic evaluation metrics and a unified approach across different data modalities.

Method: The authors propose the LLM Data Auditor framework, which categorizes and systematically evaluates intrinsic properties (quality and trustworthiness) of synthetic data generated for six modalities and analyzes current generation methods.

Result: The findings revealed significant deficiencies in existing evaluation methodologies, demonstrating the need for intrinsic rather than solely extrinsic evaluations.

Conclusion: The paper emphasizes the importance of intrinsic evaluation metrics for LLM-generated data, provides concrete suggestions to improve current practices, and outlines practical applications of synthetic data.

Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.

</details>


### [24] [EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents](https://arxiv.org/abs/2601.17722)
*Ying Mo,Yu Bai,Dapeng Sun,Yuqian Shi,Yukai Miao,Li Chen,Dan Li*

Main category: cs.AI

TL;DR: The paper introduces EntWorld, a benchmark designed to evaluate the performance of multimodal language models in complex enterprise workflows, revealing significant gaps in current capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for large language models focus on consumer scenarios and fail to evaluate the challenges of enterprise systems, such as complex business workflows and high-density interfaces.

Method: The authors developed EntWorld, a benchmark with 1,756 tasks across six enterprise domains. It uses schema-driven task generation and SQL-based deterministic verification for precise state validation.

Result: EntWorld reveals that state-of-the-art models like GPT-4.1 perform poorly (47.61% success rate) when compared to human capabilities in enterprise-specific scenarios.

Conclusion: Current generalist agents fall short in handling enterprise-specific challenges, emphasizing the need for domain-specific digital agents. EntWorld offers a platform to push advancements in this area.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.

</details>


### [25] [ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents](https://arxiv.org/abs/2601.17735)
*Kyungho Kim,Geon Lee,Juyeon Kim,Dongwon Choi,Shinhwan Kang,Kijung Shin*

Main category: cs.AI

TL;DR: This paper introduces ReFuGe, a framework for creating relational features to improve prediction tasks on relational databases by utilizing large language model agents in an iterative feedback loop.


<details>
  <summary>Details</summary>
Motivation: The aim is to improve prediction tasks on relational databases by generating informative features, addressing challenges like reasoning over complex schemas and handling a large feature space.

Method: The proposed ReFuGe framework uses three large language model agents: a schema selection agent, a feature generation agent, and a feature filtering agent, operating iteratively to refine features.

Result: ReFuGe significantly enhances performance on prediction tasks in benchmarks of relational databases.

Conclusion: ReFuGe offers a novel and effective methodology for prediction tasks in relational databases, making use of iterative feature generation and filtering through language model agents.

Abstract: Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.

</details>


### [26] [HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis](https://arxiv.org/abs/2601.17767)
*Rajan Das Gupta,Xiaobin Wu,Xun Liu,Jiaqi He*

Main category: cs.AI

TL;DR: A hybrid ensemble framework using CNN, LSTM and classical machine learning models significantly improves cardiovascular disease prediction accuracy across two datasets.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of traditional predictive models in generalizing across heterogeneous datasets for CVD diagnosis.

Method: The paper proposes combining CNN, LSTM, KNN, and XGB in an ensemble hybrid framework using a voting mechanism.

Result: Achieved 82.30% accuracy in Dataset I and 97.10% in Dataset II, with improved precision, recall, and F1-score.

Conclusion: Hybrid AI frameworks are robust for CVD predictions and support early diagnosis, aligning with sustainable healthcare goals.

Abstract: Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.

</details>


### [27] [Neuro-Symbolic Verification on Instruction Following of LLMs](https://arxiv.org/abs/2601.17789)
*Yiming Su,Kunzhao Xu,Yanjie Gao,Fan Yang,Cheng Li,Mao Yang,Tianyin Xu*

Main category: cs.AI

TL;DR: This paper introduces NSVIF, a neuro-symbolic framework for ensuring LLMs follow their instructions, and evaluates it using the VIFBENCH benchmark.


<details>
  <summary>Details</summary>
Motivation: LLMs may fail to follow instructions, leading to task failures and system incidents in extended workflows.

Method: NSVIF uses a neuro-symbolic approach, modeling instructions as constraints and verifying outputs by logical reasoning and semantic analysis.

Result: NSVIF outperforms LLM-based verification methods and provides interpretable feedback. Its feedback improves LLMs' performance without needing post-training.

Conclusion: NSVIF offers a significant improvement in verifying LLM outputs and enhances instruction-following capabilities without modifying the underlying model.

Abstract: A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.

</details>


### [28] [MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing](https://arxiv.org/abs/2601.17814)
*Haoxuan Ma,Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: The paper introduces MMR-Bench, a benchmark for studying and improving query-level model selection in multimodal language models (MLLMs) under cost constraints to enhance routing and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models currently face challenges such as varied architecture and efficiency, making it difficult for a single model to excel across different workloads. Moreover, effective query-level model selection for multimodal queries remains unresolved due to issues like computation costs and multimodal input fusion.

Method: The authors developed MMR-Bench, a benchmark designed to evaluate multimodal model routing. It uses a controlled setup to support variable computation budgets and includes tasks like OCR, general VQA, and multimodal reasoning. It offers comparative tools like reference models, oracle bounds, and routing policies.

Result: Experiments show that incorporating multimodal cues in routing improves efficiency, achieving higher accuracy than the best single model while reducing computational cost by 33%. Additionally, routing policies show the ability to generalize to new datasets and scenarios without retraining.

Conclusion: MMR-Bench helps in understanding and optimizing multimodal routing strategies, allowing adaptive and cost-efficient deployment of MLLMs. It establishes a baseline for evaluating effective model selection policies in multimodal contexts.

Abstract: Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.

</details>


### [29] [RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance](https://arxiv.org/abs/2601.17826)
*Siyuan Yang,Xihan Bian,Jiayin Tang*

Main category: cs.AI

TL;DR: RegGuard is an AI system that automates the interpretation of regulatory texts for pharmaceutical companies, mitigating errors and costs associated with compliance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by pharmaceutical companies in keeping up with complex and frequent regulatory changes, which are costly, error-prone, and labor-intensive.

Method: RegGuard employs two novel components: HiSACC for semantically segmenting long documents and ReLACE, a cross-encoder for improving query ranking relevance. It also incorporates features for auditability and traceability.

Result: Evaluations in real-world enterprise settings show that RegGuard improves answer quality, specifically in relevance, groundedness, and contextual focus, while reducing hallucination risk.

Conclusion: The system is highly effective for automating regulatory compliance in dynamic domains, with its architecture ensuring adaptability, security, and relevance.

Abstract: The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.

</details>


### [30] [AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito](https://arxiv.org/abs/2601.18381)
*Yinghan Hou,Zongyou Yang*

Main category: cs.AI

TL;DR: This paper presents an AI agent framework to transition legacy finite difference codes into the Devito environment using a hybrid LangGraph architecture combining RAG and large language models.


<details>
  <summary>Details</summary>
Motivation: Transforming legacy codebases like Fortran into advanced computational frameworks like Devito while maintaining performance and adaptability.

Method: A hybrid LangGraph architecture integrates Retrieval-Augmented Generation (RAG), large language models, and reverse engineering workflows. The process involves knowledge graph construction, multi-stage retrieval pipelines, and Pydantic-based code synthesis.

Result: The framework enables precise and reliable code transformation with validation via static analysis, G-Eval, and multi-stage workflows, facilitated by reinforcement learning-based feedback optimization.

Conclusion: This system demonstrates effective adaptation and translation of legacy code into dynamic capabilities, advancing computational applications in domains like seismic wave simulations and CFD.

Abstract: To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.

</details>


### [31] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma,Yang Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.AI

TL;DR: The paper proposes Information Gain Fine-Tuning (IGFT) for medical conversational AI, enabling models to conduct effective patient interviews and generate HPIs without relying on human-annotated conversations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to train models capable of comprehensive patient interviews without requiring pre-collected expert-annotated dialogue, thus reducing reliance on expensive datasets.

Method: IGFT employs RL with Group Relative Policy Optimization (GRPO) and an information gain reward function to train models using self-generated conversations with simulated patients.

Result: Fine-tuned models achieved improved F1 scores (DeepSeek-R1-Distill-Qwen-7B: 10.9% on Avey, 12.9% on MIMIC; Llama-3.1-8B-Instruct: 11.2% on MIMIC) and surpassed both OpenAI's and medical domain-specific baselines such as HuatuoGPT.

Conclusion: The IGFT framework enables efficient training of conversational AI for multi-turn patient dialogues, outperforming established medical AI baselines and supporting generalization across different datasets.

Abstract: We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [32] [When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents](https://arxiv.org/abs/2601.17887)
*Jiahe Guo,Xiangran Guo,Yulin Hu,Zimo Long,Xingyu Sui,Xuda Zhi,Yongbo Huang,Hao He,Weixiang Zhao,Yanyan Zhao,Bing Qin*

Main category: cs.AI

TL;DR: The paper explores the safety risks of long-term memory in LLM-based personalized agents, revealing a phenomenon called intent legitimation, where benign personal memories lead to harmful query legitimization. The researchers introduce a benchmark (PS-Bench) and propose a method to mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: To assess and address safety risks in personalized LLM agents, specifically focusing on how long-term memory can inadvertently legitimize harmful intents.

Method: The study uses PS-Bench to analyze intent legitimation across multiple LLM frameworks and proposes a detection-reflection method to reduce its impact.

Result: Personalization caused an increase of 15.8%-243.7% in attack success rates across tested scenarios, evidencing safety degradation due to memory's influence.

Conclusion: The research highlights intent legitimation as a critical safety issue in personalized LLM agents and underscores the importance of evaluating safety in systems with long-term personal context.

Abstract: Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.

</details>


### [33] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu,Yinhe Long,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: The paper introduces UniCog, a framework for analyzing the cognition of large language models (LLMs) through a latent mind space, and demonstrates improved reasoning performance using insights from this analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand and interpret cognitive processes in large language models, as existing methods are limited in explaining how these models engage cognitive abilities during reasoning.

Method: UniCog is a latent variable model that encodes diverse abilities from dense activations into sparse, disentangled latent dimensions, allowing analysis of cognition in LLMs.

Result: Through analysis of six advanced LLMs, UniCog reveals a Pareto principle in LLM cognition and identifies reasoning failures as anomalous patterns in latent activations. The framework also improves reasoning performance by up to 7.5% on benchmarks.

Conclusion: The study provides a new paradigm for analyzing LLM reasoning dynamics, enabling better understanding of cognitive abilities and enhancing LLM performance with latent-based strategies.

Abstract: A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [34] [Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation](https://arxiv.org/abs/2601.17915)
*Saurabh Jha,Rohan Arora,Bhavya,Noah Zheutlin,Paulina Toro Isaza,Laura Shwartz,Yu Deng,Daby Sow,Ruchi Mahindru,Ruchir Puri*

Main category: cs.AI

TL;DR: The paper introduces EoG (Explanations over Graphs), a disaggregated framework for LLM agents to address challenges in open-ended investigations involving dynamic environments and complex evidence mining, outperforming ReAct baselines.


<details>
  <summary>Details</summary>
Motivation: LLM agents struggle with open-ended investigations due to limitations in bounded context windows, hidden dependency structures, and brittleness in reasoning mechanisms like ReAct. There was a need for a reliable framework that addresses these challenges.

Method: The paper proposes EoG, where an LLM handles localized evidence mining and labeling while a deterministic controller manages traversal, state, and belief propagation using a dependency graph to arrive at minimal explanatory frontiers.

Result: EoG demonstrated higher accuracy and consistency on the ITBench diagnostics task, including a 7x average improvement in Majority-at-k entity F1 compared to ReAct-style agents.

Conclusion: The EoG framework enhances LLM performance in environments requiring iterative evidence gathering and reasoning, with improved reliability and accuracy over existing methodologies like ReAct.

Abstract: LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.
  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.

</details>


### [35] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen,Audrey Wang,Stanley Yin,Hanyang Jiang,Dong Zhang*

Main category: cs.AI

TL;DR: The paper focuses on the AI challenges inherent in self-driving laboratories (SDLs), providing a framework to connect SDL pipelines with established AI principles and proposing benchmarks and a taxonomy for evaluating SDL systems.


<details>
  <summary>Details</summary>
Motivation: To address AI problems in real self-driving laboratories, specifically focusing on autonomy challenges related to experiment design under practical constraints and limitations.

Method: The study frames SDL autonomy as an agent-environment interaction problem, reviews AI methods like Bayesian optimization, active learning, reinforcement learning, and tool-using agents, and proposes a taxonomy for SDL capabilities along with benchmark templates.

Result: The paper synthesizes methodologies and metrics for evaluating SDL systems, categorizing them by decision horizon, uncertainties, constraints, and human involvement while identifying key challenges.

Conclusion: SDLs offer a critical platform for advancing agentic AI, and further research is required to overcome open challenges like safe exploration, multi-modal representation, and reproducibility benchmarks.

Abstract: Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [36] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

Main category: cs.AI

TL;DR: The paper proposes a skill-graph-driven framework for continually learning agents in real-time settings, demonstrated in Dark Souls III.


<details>
  <summary>Details</summary>
Motivation: To create agents that can continually learn and expand their competence over time without having to retrain from scratch or losing previously learned behaviors.

Method: The authors use a hierarchical skill graph framework, decomposing complex control into smaller skills (e.g., camera control, movement) trained progressively in a curriculum, enabling selective fine-tuning when the environment changes.

Result: The system demonstrated efficient learning and adaptability; fine-tuning just two skills allowed the agent to recover performance under shifting environmental conditions within a limited interaction budget.

Conclusion: Skill-graph curricula combined with selective fine-tuning offer a practical approach to building continually learning agents in complex environments, promoting efficiency and adaptability.

Abstract: Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [37] [LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting](https://arxiv.org/abs/2601.17942)
*Yu-Jie Yang,Hung-Fu Chang,Po-An Chen*

Main category: cs.AI

TL;DR: The paper proposes methods for improving Text-to-SQL performance, including the SSEV pipeline and ReCAPAgent-SQL framework.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of ambiguity, schema linking, limited SQL generalization, and domain understanding in Text-to-SQL tasks.

Method: Developed a SSEV (Single-Agent Self-Refinement with Ensemble Voting) pipeline and introduced ReCAPAgent-SQL using specialized agents and iterative refinement.

Result: SSEV achieved high benchmarking scores, and ReCAPAgent-SQL improved performance for real-world Text-to-SQL enterprise tasks.

Conclusion: Their approaches significantly improve practical Text-to-SQL systems by making them scalable and efficient for enterprise use.

Abstract: Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.

</details>


### [38] [Sentipolis: Emotion-Aware Agents for Social Simulations](https://arxiv.org/abs/2601.18027)
*Chiyuan Fu,Lyuhao Chen,Yunze Xiao,Weihao Xuan,Carlos Busso,Mona Diab*

Main category: cs.AI

TL;DR: This paper introduces Sentipolis, a framework that improves emotional state modeling and continuity in LLM agents for social simulations.


<details>
  <summary>Details</summary>
Motivation: To address the problem of emotional amnesia and weak emotional continuity in LLM agents used for social simulations.

Method: The framework integrates a Pleasure-Arousal-Dominance (PAD) model, dual-speed emotion dynamics, and links emotion with memory across interactions.

Result: Sentipolis leads to better emotional continuity, enhances communication, shows believability gains (depending on model capacity), but emotion-awareness can slightly decrease adherence to social norms.

Conclusion: Sentipolis allows agents to exhibit more human-like emotional and social behaviors, supporting studies of cumulative social dynamics despite some trade-offs in smaller models.

Abstract: LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.

</details>


### [39] [Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing](https://arxiv.org/abs/2601.18061)
*Kiana Jafari,Paul Ulrich Nikolaus Rust,Duncan Eddy,Robbie Fraser,Nina Vasan,Darja Djordjevic,Akanksha Dadlani,Max Lamparth,Eugenia Kim,Mykel Kochenderfer*

Main category: cs.AI

TL;DR: This paper examines poor inter-rater reliability among psychiatrists evaluating AI-generated responses, revealing systematic disagreements on safety-critical items like suicide prevention.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to evaluate the validity of learning from human feedback in high-stakes scenarios like mental health, where expert consensus is critical.

Method: Three certified psychiatrists evaluated AI-generated mental health responses using a calibrated rubric, accompanied by qualitative interviews to understand disagreement causes.

Result: Inter-rater reliability was poor ($ICC$: $0.087$–$0.295$). Disagreement was highest on safety-critical items, particularly on suicide response mechanisms, influenced by differing professional philosophies.

Conclusion: Expert disagreement is a structured sociotechnical phenomenon, not measurement error. AI systems should adopt alignment methods to learn from differing expert perspectives rather than aggregating for consensus-based ground truth.

Abstract: Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $α= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.

</details>


### [40] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye,Yiwen Duan,Yonghong Yu,Victor Ma,Yang Gao,Xing Chen*

Main category: cs.AI

TL;DR: OurBench introduces a benchmark to test SQL reasoning and debugging with realistic errors in complex SQL queries. Current LLMs show low performance, highlighting limitations.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty in generating fully correct SQL code in one attempt and evaluate the capability of LLMs for enterprise-level SQL debugging.

Method: The authors created OurBench by systematically injecting bugs into large-scale SQL to mimic real-world debugging scenarios and designed an execution-free evaluation framework.

Result: Performance evaluation showed that the top-performing LLM achieved only around 36% and 32% accuracy on syntactic and semantic error benchmarks, respectively, with most models scoring below 20%.

Conclusion: There remains a significant gap in enterprise SQL debugging capabilities with current LLMs, requiring further research and innovation to improve performance on complex SQL tasks.

Abstract: SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [41] [Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters](https://arxiv.org/abs/2601.18123)
*Muhammad Ibrahim Khan,Bivin Pradeep,James Brusey*

Main category: cs.AI

TL;DR: The paper discusses deadline-aware control of water heating systems, introducing efficient Gymnasium models and comparing methods like bang-bang control, Monte Carlo Tree Search, and Proximal Policy Optimization for energy savings.


<details>
  <summary>Details</summary>
Motivation: Current domestic water heaters usually prioritize fast heating over energy efficiency, ignoring predictable demand patterns and ambient thermal losses. The paper aims to address this inefficiency by designing a control method to meet deadlines while minimizing energy use.

Method: The authors create a Gymnasium environment simulating a digital immersion water heater with thermal dynamics and discrete actions. They test a bang-bang baseline, Monte Carlo Tree Search planning, and Proximal Policy Optimization on varying target temperatures, time horizons, and initial conditions.

Result: Proximal Policy Optimization significantly reduced energy consumption, showing a savings of 26%-69% compared to traditional bang-bang methods and was more efficient than Monte Carlo Tree Search, especially over longer heating horizons.

Conclusion: Learned deadline-aware control improves energy efficiency for domestic water heaters. PPO demonstrates superior performance in reducing energy usage without any additional inference cost post-training, outperforming both planners and baseline methods.

Abstract: Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.

</details>


### [42] [RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents](https://arxiv.org/abs/2601.18130)
*Jize Wang,Han Wu,Zhiyuan You,Yiming Song,Yijun Wang,Zifei Shan,Yining Li,Songyang Zhang,Xinyi Le,Cailian Chen,Xinping Guan,Dacheng Tao*

Main category: cs.AI

TL;DR: RouteMoA is an efficient mixture-of-agents framework that dynamically routes tasks to reduce costs and latency while enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Existing Mixture-of-Agents (MoA) systems are effective but incur high costs and latency due to their dense model dependency and lack of dynamic routing for model selection.

Method: RouteMoA employs a lightweight scorer for initial candidate screening, a mixture of judges for refined assessment using existing model outputs, and a model ranking mechanism to balance performance, cost, and latency.

Result: RouteMoA improved task performance while reducing cost by 89.8% and latency by 63.6% when compared to traditional MoA methods, particularly in large-scale model pools.

Conclusion: RouteMoA optimizes LLM performance with reduced computational costs and latency through its dynamic and efficient routing framework, offering a scalable approach for large model pools.

Abstract: Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.

</details>


### [43] [RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening](https://arxiv.org/abs/2601.18132)
*Xi Chen,Hongru Zhou,Huahui Yi,Shiyu Feng,Hanyu Zhou,Tiancheng He,Mingke You,Li Wang,Qiankun Li,Kun Wang,Weili Fu,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: RareAlert is a new model for early risk screening of rare diseases using primary-visit information, validated on a dataset of 158,666 cases and achieving superior performance compared to existing models.


<details>
  <summary>Details</summary>
Motivation: The study addresses the critical issue of delayed or missed diagnosis in rare diseases by proposing targeted universal screening as current primary care processes fail to reliably triage rare disease risk at initial encounters.

Method: RareAlert integrates rare disease risk predictions from ten large language models (LLMs), aligns their reasoning, and calibrates the outputs into a single machine learning model. This model was developed using a dataset (RareBench) covering 158,666 cases, representing over 33 disease categories and 7,000 rare conditions.

Result: RareAlert achieved an AUC of 0.917 in identifying rare diseases on an independent test set, outperforming state-of-the-art LLMs and machine learning ensembles.

Conclusion: RareAlert offers accurate, scalable, and privacy-preserving rare disease risk screening, demonstrating the potential of calibrated LLM reasoning in uncertain clinical scenarios.

Abstract: Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.

</details>


### [44] [DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints](https://arxiv.org/abs/2601.18137)
*Yinger Zhang,Shutong Jiang,Renhao Li,Jianhong Tu,Yang Su,Lianghao Deng,Xudong Guo,Chenxu Lv,Junyang Lin*

Main category: cs.AI

TL;DR: The paper introduces DeepPlanning, a benchmark for assessing long-horizon planning in agents, focusing on realistic tasks like multi-day travel and shopping with complex constraints.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are insufficient for testing long-horizon planning and fail to address both global optimization and active information gathering, which are essential for real-world scenarios.

Method: DeepPlanning is proposed as a benchmark that evaluates tasks requiring proactive information acquisition, local reasoning with fine-grained constraints, and global optimization over extended scenarios.

Result: Evaluation on DeepPlanning reveals significant challenges for current large language models (LLMs) in handling long-horizon planning tasks, emphasizing the need for better reasoning methods and tool integration.

Conclusion: DeepPlanning highlights the current limitations of LLMs in long-horizon planning and underscores areas for improvement, while providing an open-source foundation for future research developments.

Abstract: While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.

</details>


### [45] [GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models](https://arxiv.org/abs/2601.18197)
*Shaokang Wang,Pei Fu,Ruoceng Zhang,Shaojie Zhang,Xiuwen Xi,Jiahui Yang,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.AI

TL;DR: This paper proposes GAIA, a training framework with iterative critic capabilities for improving the performance of GUI agents by reducing erroneous actions. Positive and negative action feedback train a critic model, refining agent accuracy iteratively.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of irreversibility in GUI agent operations where a single error can cause significant deviations, and improving the agents' reliability and performance.

Method: The GAIA framework involves training an Intuitive Critic Model (ICM) using feedback from a base agent. This model evaluates intended actions and guides agent performance through iterative refinement, ultimately creating a self-improving cycle using augmented data.

Result: Experiments on various datasets show that the proposed method improves test-time performance of both open-source and closed-source models. Continuous data recycling further enhances accuracy and results.

Conclusion: ICM and the GAIA framework effectively overcome critical challenges in GUI agent operation by introducing iterative refinement, showcasing the potential for improving agents' reliability and scalability in real tasks.

Abstract: While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.

</details>


### [46] [SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback](https://arxiv.org/abs/2601.18202)
*Fangyuan Xu,Rujun Han,Yanfei Chen,Zifeng Wang,I-Hung Hsu,Jun Yan,Vishy Tirumalashetty,Eunsol Choi,Tomas Pfister,Chen-Yu Lee*

Main category: cs.AI

TL;DR: The paper introduces SAGE, a pipeline for automatically generating high-quality, difficulty-controlled question-answer pairs to enhance deep search agents.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of creating datasets for deep search agents, as human annotations for this task are prohibitively expensive and time-consuming.

Method: The proposed method, SAGE, combines a data generator that produces QA pairs and a search agent that refines them through iterative feedback loops to meet specified difficulty levels.

Result: SAGE improves the correctness and complexity of QA pairs, achieving up to 23% better performance on deep search benchmarks. Agents trained on its data can also adapt to dynamic search environments like Google Search without extra training.

Conclusion: The pipeline effectively generates high-quality data for training deep search agents, enhancing their ability to reason over multiple documents.

Abstract: Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.

</details>


### [47] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: The paper explores how reinforcement learning (RL) environments and modeling choices influence cross-domain generalization for generalist LLM agents, introducing low-overhead techniques and insights towards improving out-of-domain robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge where generalist LLM agents, typically trained in narrow environments, struggle in broader unseen domains during deployment.

Method: The authors analyze properties of RL environments, such as state information richness and planning complexity, and introduce randomization techniques to increase robustness. They also study modeling strategies like SFT warmup and step-by-step thinking during RL training.

Result: Findings show environment properties like state information richness strongly enhance cross-domain generalization. Additionally, specific modeling choices affect stability and performance in unseen domains.

Conclusion: Increasing state information richness and leveraging effective modeling practices can significantly improve cross-domain robustness for generalist LLM agents, even without narrowing domains during training.

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [48] [ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants](https://arxiv.org/abs/2601.18225)
*Pei Wang,Yanan Wu,Xiaoshuai Song,Weixun Wang,Gengru Chen,Zhongwen Li,Kezhong Yan,Ken Deng,Qi Liu,Shuaibing Zhao,Shaopan Xiong,Xuepeng Liu,Xuefeng Chen,Wanxi Deng,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: The paper introduces 'ShopSimulator,' a challenging Chinese e-commerce simulation environment, evaluating LLMs' performance in personalized shopping tasks. It identifies performance limitations and explores training improvements using supervised fine-tuning (SFT) and reinforcement learning (RL).


<details>
  <summary>Details</summary>
Motivation: Enhance e-commerce shopping agents by addressing gaps in their ability to interpret user preferences, handle multi-turn dialogues, and accurately retrieve/discriminate between highly similar products.

Method: Develop and employ 'ShopSimulator,' a unified, large-scale Chinese shopping simulation environment, to evaluate and train LLMs on personalization and product search tasks.

Result: LLMs achieved less than 40% success rate in scenarios, showing struggles in deep search, product selection, and personalization cues balance. SFT and RL training significantly improved their performance.

Conclusion: ShopSimulator reveals critical shortcomings in LLMs' shopping agent capabilities, and tailored training methods like SFT and RL are effective in addressing these weaknesses.

Abstract: Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.

</details>


### [49] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li,Shijun Yang,Weizhen Qi,Silei Zhao,Rui Hua,Mingzhu Song,Xiaojian Yang,Chao Peng*

Main category: cs.AI

TL;DR: The paper proposes a paradigm called In-Situ Self-Evolving to enable agent systems to adapt to open-ended task environments by synthesizing and optimizing tools based on continuous task feedback instead of static training.


<details>
  <summary>Details</summary>
Motivation: Conventional agent systems face challenges in dynamic environments due to reliance on static training and tools, which limit adaptability and capability expansion.

Method: They propose a self-evolving system (Yunjue Agent) that synthesizes, optimizes, and evolves tools using sequential task feedback without ground-truth labels, supported by a Parallel Batch Evolution strategy.

Result: Experiments across five benchmarks show significant performance improvements over baselines and demonstrate knowledge transfer to novel domains, along with the introduction of a metric for monitoring evolution convergence.

Conclusion: The self-evolving paradigm enhances resilient intelligence in dynamic environments, with the codebase open-sourced for further research development.

Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [50] [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282)
*Lei Wei,Jinpeng Ou,Xiao Peng,Bin Wang*

Main category: cs.AI

TL;DR: TAFC is a framework enhancing function calling in large language models by improving reasoning accuracy and transparency at parameter levels.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs lack explicit reasoning transparency, especially for functions with interdependent parameters, limiting their effectiveness and interpretability.

Method: TAFC introduces a universal "think" parameter for reasoning clarity and employs dynamic and granular optimization based on complexity scoring to enhance reasoning quality.

Result: Tests on ToolBench reveal TAFC's improved parameter generation accuracy, reasoning coherence, and better debugging interpretability.

Conclusion: TAFC enhances functionality in LLM-powered autonomous agents without architectural changes, improving reasoning accuracy and interpretability at the parameter level.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal "think" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.

</details>


### [51] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

Main category: cs.AI

TL;DR: The paper introduces Climate RADAR, an innovative AI-based system to enhance disaster communication by shifting focus from mere alerts to actionable recommendations, resulting in better disaster preparedness and equitable outcomes.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional early warning systems, which often fail to trigger prompt protective actions, causing preventable damages and inequities.

Method: The authors developed Climate RADAR, a generative AI system integrating diverse data sources like meteorological and social information into a risk index. It uses large language models with guardrails to provide personalized and actionable recommendations across citizen, volunteer, and municipal platforms.

Result: The system demonstrated improved execution of protective actions, reduced response delays, and increased trust and usability through simulations, user studies, and a municipal pilot program.

Conclusion: Climate RADAR revolutionizes early warning systems by combining predictive analytics, behavioral science, and responsible AI, ensuring more people-centered and equitable disaster resilience solutions.

Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


### [52] [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353)
*Tuhin Chakrabarty,Paramveer S. Dhillon*

Main category: cs.AI

TL;DR: The paper examines the competition between MFA expert writers and AI in emulating famous authors' styles, revealing preferences for AI writing among lay judges and the impact on experts' self-perception.


<details>
  <summary>Details</summary>
Motivation: To explore how well Generative AI can emulate human creative writing styles and examine its implications on human writers and the discourse around AI's creative capabilities.

Method: Behavioral experiments were conducted, where MFA expert writers and three large language models (LLMs) competed to replicate the styles of 50 acclaimed authors. Judges (both expert and lay) performed blind pairwise evaluations under different training conditions.

Result: Expert judges preferred human writing in 82.7% of cases with in-context prompting; however, 62% preferred AI writing after fine-tuning. Lay judges consistently favored AI writing.

Conclusion: Generative AI's ability to emulate human-level creative writing challenges assumptions about its creative limitations, while raising concerns about its impact on human writers' confidence and the identity of creative labor.

Abstract: Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes "good writing." These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.

</details>


### [53] [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383)
*Zhenyuan Guo,Tong Chen,Wenlong Meng,Chen Gong,Xin Yu,Chengkun Wei,Wenzhi Chen*

Main category: cs.AI

TL;DR: Large Reasoning Models (LRMs) are computationally heavy due to reasoning traces, so this paper introduces a method called DynTS to enhance efficiency by identifying and retaining only critical tokens.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiency in LRMs caused by the computational and memory overhead of reasoning traces in problem-solving.

Method: Using attention maps to identify decision-critical tokens in reasoning traces and retaining only their key-value cache states while evicting redundant entries during inference.

Result: Reduced computational and memory overhead, ensuring more efficient operations in Large Reasoning Models without sacrificing performance.

Conclusion: Selective retention of critical elements during reasoning processes can significantly enhance the efficiency of LRMs, maintaining accuracy while reducing resource usage.

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.

</details>


### [54] [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467)
*Yuhang Zhou,Kai Zheng,Qiguang Chen,Mengkang Hu,Qingfeng Sun,Can Xu,Jingjing Chen*

Main category: cs.AI

TL;DR: The paper introduces a solution to train research agents using offline reinforcement learning with a curated set of resources, avoiding expensive online RL.


<details>
  <summary>Details</summary>
Motivation: The high cost of online reinforcement learning (RL) due to extensive API calls motivates the exploration of more efficient alternatives for training research agents.

Method: The authors present DeepForge, a task synthesis framework, and provide curated datasets of QA pairs, SFT trajectories, and DPO pairs for effective offline training.

Result: Through offline training, they developed OffSeeker (8B), which performs competitively on benchmarks compared to larger models trained with online RL.

Conclusion: Expensive online RL is not necessary for building powerful research agents; offline training using the proposed resources and tools is a viable alternative.

Abstract: Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.

</details>


### [55] [AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security](https://arxiv.org/abs/2601.18491)
*Dongrui Liu,Qihan Ren,Chen Qian,Shuai Shao,Yuejin Xie,Yu Li,Zhonghao Yang,Haoyu Luo,Peng Wang,Qingyu Liu,Binxin Hu,Ling Tang,Jilin Mei,Dadi Guo,Leitao Yuan,Junyao Yang,Guanxu Chen,Qihao Lin,Yi Yu,Bo Zhang,Jiaxuan Guo,Jie Zhang,Wenqi Shao,Huiqi Deng,Zhiheng Xi,Wenjie Wang,Wenxuan Wang,Wen Shen,Zhikai Chen,Haoyu Xie,Jialing Tao,Juntao Dai,Jiaming Ji,Zhongjie Ba,Linfeng Zhang,Yong Liu,Quanshi Zhang,Lei Zhu,Zhihua Wei,Hui Xue,Chaochao Lu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: The paper introduces a taxonomy for AI agent risks and presents AgentDoG, a framework for safety and security verification, which outperforms others in diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Address the lack of risk awareness, diagnosis transparency, and comprehensive safety guardrails for autonomous AI agents.

Method: Develop a three-dimensional taxonomy of risks (source, failure mode, consequence), propose ATBench benchmark, and framework called AgentDoG for risk monitoring and diagnosis.

Result: AgentDoG models achieve leading performance in identifying and managing risks in diverse agentic scenarios, supported by publicly released models and datasets.

Conclusion: AgentDoG’s contextual monitoring improves AI safety by diagnosing root causes of risks transparently, enhancing alignment and usability in complex interactions.

Abstract: The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.

</details>


### [56] [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496)
*Zihan wang,Hao Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yiqun Zhang,Jinghao Lin,Haihua Yang,Xiaozhong Ji*

Main category: cs.AI

TL;DR: The paper introduces DeepMed, a model designed to improve medical reasoning by addressing gaps in traditional DeepReasoning (DR) approaches, such as inadequate contextual reasoning and noisy tool usage. It combines specialized data synthesis, training adjustments, and inference monitoring to outperform other models, achieving a 9.79% improvement on average across seven medical benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the limitations of traditional DR models in the medical field, specifically their inability to handle clinical-context reasoning and the adverse effects of excessive, unregulated tool usage.

Method: DeepMed leverages multi-hop medical search QA data synthesis, introduces a difficulty-aware training framework to manage tool-call growth, and incorporates monitored inference to validate hypotheses within controlled steps.

Result: On seven medical benchmarks, DeepMed demonstrates an average performance improvement of 9.79%, surpassing both its base model and larger DR and medical reasoning models.

Conclusion: By addressing key challenges in medical reasoning, DeepMed proves to be a more effective and reliable solution, setting new benchmarks for leveraging DR models in the medical domain.

Abstract: Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.

</details>


### [57] [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: The study develops MOSAIC, a modular assessment framework to analyze the instruction compliance capabilities of LLMs, revealing significant variations in performance based on constraints and model-specific weaknesses.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks often fail to reflect real-world applications or segregate compliance from task success in LLMs.

Method: A modular framework called MOSAIC, leveraging dynamically generated datasets with multiple generation constraints to assess LLM performance.

Result: Analysis of five LLMs reveals varying compliance levels influenced by constraint type, number, and instruction positioning, along with identifying positional biases and interaction effects.

Conclusion: This granular evaluation approach is vital for diagnosing failures and advancing reliable LLM development for systems requiring stringent instruction adherence.

Abstract: Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.

</details>


### [58] [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588)
*Xianzhe Meng,Qiangsheng Zeng,Ling Luo,Qinghan Yang,Jiarui Hao,Wenbo Wu,Qinyu Wang,Rui Yin,Lin Qi,Renzhi Lu*

Main category: cs.AI

TL;DR: This paper examines how stabilizing training dynamics impacts generation in large language models, finding that it reduces entropy and leads to degenerated outputs, hence showing stability may not ensure generative quality.


<details>
  <summary>Details</summary>
Motivation: To analyze the relationship between training stability and generative expressivity in large language models, as stability is considered essential for optimization.

Method: The authors theoretically connect stable parameter trajectories to low-entropy solutions via forward KL divergence minimization and empirically validate this using a controlled feedback-based training framework.

Result: Stable training dynamics result in reduced generative entropy and repetitive outputs, demonstrating degeneration despite smooth loss convergence.

Conclusion: Optimization stability alone is not sufficient to ensure high-quality generative expressivity in language models, as stability may induce systematic degeneration.

Abstract: Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.

</details>


### [59] [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595)
*Joseph Cotnareanu,Didier Chetelat,Yingxue Zhang,Mark Coates*

Main category: cs.AI

TL;DR: The paper introduces a method that combines logic solvers and large language models (LLMs) for reasoning problems involving missing commonsense information, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of large language models (LLMs) in handling problems requiring both formal reasoning and implied commonsense knowledge.

Method: The method iteratively uses feedback from a logic solver to identify and incorporate missing commonsense relations, which are provided by LLMs, into logic problems via search procedures.

Result: The proposed method achieves significant improvements in reasoning abilities on datasets with missing commonsense information, compared to existing techniques.

Conclusion: By balancing the strengths of symbolic logic solvers and neural models, such as LLMs, the paper demonstrates an effective way to handle tasks requiring both logic and commonsense reasoning.

Abstract: Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.

</details>


### [60] [PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression](https://arxiv.org/abs/2601.18608)
*Fabian Fumagalli,R. Teal Witter,Christopher Musco*

Main category: cs.AI

TL;DR: This paper enhances the KernelSHAP algorithm for approximating Shapley values by employing higher-degree polynomial approximations (PolySHAP), improving accuracy. It also establishes theoretical justification for paired sampling in KernelSHAP.


<details>
  <summary>Details</summary>
Motivation: Shapley values are crucial for explainable AI but are computationally expensive to calculate exactly. KernelSHAP approximates them efficiently, though it currently relies on linear game approximations.

Method: PolySHAP extends KernelSHAP by using higher-degree polynomials to approximate the game, capturing non-linear feature interactions. The study also connects paired sampling to second-order PolySHAP, proving its effectiveness theoretically.

Result: PolySHAP provides improved Shapley value estimates for benchmark datasets and validates paired sampling's equivalence to second-order PolySHAP with theoretical backing.

Conclusion: PolySHAP improves Shapley value estimation accuracy by capturing feature interactions and justifies the practical success of paired sampling in KernelSHAP.

Abstract: Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires $2^d$ game evaluations for a model with $d$ features. Lundberg and Lee's KernelSHAP algorithm has emerged as a leading method for avoiding this exponential cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets.
  In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent.
  Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modification to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic.

</details>


### [61] [Emergence of Phonemic, Syntactic, and Semantic Representations in Artificial Neural Networks](https://arxiv.org/abs/2601.18617)
*Pierre Orhan,Pablo Diego-Simón,Emmnanuel Chemla,Yair Lakretz,Yves Boubenec,Jean-Rémi King*

Main category: cs.AI

TL;DR: The paper studies how artificial neural networks develop phonemic, lexical, and syntactic representations during training, identifying similarities and differences compared to human language acquisition.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a unifying computational framework explaining the neural representations underlying children's language acquisition.

Method: The study investigates artificial neural networks' training to observe the emergence of phonemic, lexical, and syntactic representations and compares their developmental trajectory to children's language acquisition.

Result: Artificial models show a sequential learning process for phonemic, lexical, and syntactic structures, requiring significantly more data to achieve similar developmental milestones compared to humans.

Conclusion: The research unveils conditions under which language acquisition stages spontaneously emerge in artificial models, paving the way for understanding underlying computational processes.

Abstract: During language acquisition, children successively learn to categorize phonemes, identify words, and combine them with syntax to form new meaning. While the development of this behavior is well characterized, we still lack a unifying computational framework to explain its underlying neural representations. Here, we investigate whether and when phonemic, lexical, and syntactic representations emerge in the activations of artificial neural networks during their training. Our results show that both speech- and text-based models follow a sequence of learning stages: during training, their neural activations successively build subspaces, where the geometry of the neural activations represents phonemic, lexical, and syntactic structure. While this developmental trajectory qualitatively relates to children's, it is quantitatively different: These algorithms indeed require two to four orders of magnitude more data for these neural representations to emerge. Together, these results show conditions under which major stages of language acquisition spontaneously emerge, and hence delineate a promising path to understand the computations underpinning language acquisition.

</details>


### [62] [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630)
*Abeer Badawi,Md Tahmid Rahman Laskar,Elahe Rahimi,Sheri Grach,Lindsay Bertrand,Lames Danok,Frank Rudzicz,Jimmy Huang,Elham Dolatabadi*

Main category: cs.AI

TL;DR: This paper assesses the potential of Large Language Models (LLMs) for mental health support through a novel evaluation framework, revealing strengths in cognitive support but limitations in emotional resonance.


<details>
  <summary>Details</summary>
Motivation: To address the global mental health crisis and the shortage of qualified therapists by exploring the use of LLMs as scalable and potentially reliable tools for therapeutic support.

Method: The authors developed a human-grounded evaluation methodology, using a dataset of 500 mental health conversations to evaluate responses from nine LLMs. Experts rated responses on a 5-point Likert scale across a 6 attribute rubric that measures cognitive and affective dimensions.

Result: LLMs performed strongly in providing safe, coherent, and clinically accurate information (cognitive support) but struggled with affective alignment, particularly in emotional sensitivity and resonance. Closed-source models like GPT-4 showed better balance, while open-source models exhibited more variability.

Conclusion: The study highlights the need for evaluation frameworks that incorporate relational sensitivity with clinical rigor and suggests human-in-the-loop protocols to ensure both therapeutic sensitivity and responsible LLM design in mental health applications.

Abstract: The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.

</details>


### [63] [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631)
*Mingyang Song,Haoyu Sun,Jiawei Gu,Linjie Li,Luxin Xu,Ranjay Krishna,Yu Cheng*

Main category: cs.AI

TL;DR: AdaReasoner is a multimodal model focusing on learning general tool-use reasoning to enhance visual reasoning tasks, leading to state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To enable multimodal large language models to effectively and adaptively use tools, especially under new tasks or with new tools, for improved visual reasoning.

Method: AdaReasoner employs a data curation pipeline for long-horizon tool interactions, the Tool-GRPO reinforcement learning algorithm for tool usage optimization, and an adaptive learning mechanism for dynamic tool regulation.

Result: AdaReasoner autonomously adopts beneficial tools, suppresses irrelevant ones, and dynamically adjusts tool usage, achieving state-of-the-art results across benchmarks and surpassing proprietary systems like GPT-5 in tasks such as VSP and Jigsaw.

Conclusion: This framework demonstrates the potential of generalizable and adaptive tool use in multimodal reasoning, achieving significant performance improvements and generalization across tasks.

Abstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.

</details>


### [64] [FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory](https://arxiv.org/abs/2601.18642)
*Lei Wei,Xu Dong,Xiao Peng,Niantao Xie,Bin Wang*

Main category: cs.AI

TL;DR: Large language models lack mechanisms for selective forgetting, leading to issues in memory retention. The paper proposes FadeMem, a biologically-inspired memory architecture that uses adaptive forgetting to achieve balance.


<details>
  <summary>Details</summary>
Motivation: Address critical memory issues in large language models that either forget entirely or retain all information, causing inefficiency.

Method: FadeMem employs a dual-layer memory hierarchy with adaptive exponential decay functions based on semantic relevance, access frequency, and time patterns to manage selective forgetting.

Result: FadeMem achieved better multi-hop reasoning and retrieval while reducing storage usage by 45% across several benchmark datasets, validating its approach.

Conclusion: Biologically-inspired forgetting mechanisms can enhance memory efficiency and reasoning capabilities in autonomous AI agents.

Abstract: Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.

</details>


### [65] [TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent](https://arxiv.org/abs/2601.18700)
*Xingyu Sui,Yanyan Zhao,Yulin Hu,Jiahe Guo,Weixiang Zhao,Bing Qin*

Main category: cs.AI

TL;DR: This paper introduces TEA-Bench, the first benchmark for assessing the integration of external tools into emotional support conversation (ESC) systems, aiming to improve factual grounding and reduce hallucinations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing ESC systems, which overly focus on affective support in text-only settings and ignore the impact of external tools for factual grounding.

Method: The authors introduce TEA-Bench, an interactive benchmark with emotional scenarios, a tool environment, and process-level metrics, alongside a tool-enhanced dialogue dataset (TEA-Dialog). They evaluate nine large language models (LLMs) on their ability to use tools effectively within this setup.

Result: Tool augmentation generally enhances emotional support and reduces hallucination. However, strong LLMs benefit significantly, while weak ones see only slight improvements. Fine-tuned models improve within the training data but fail to generalize effectively.

Conclusion: Tool integration is critical for creating reliable ESC systems, but benefits vary based on model capacity. Effective tool use is essential for grounding and delivering trustworthy emotional support.

Abstract: Emotional Support Conversation requires not only affective expression but also grounded instrumental support to provide trustworthy guidance. However, existing ESC systems and benchmarks largely focus on affective support in text-only settings, overlooking how external tools can enable factual grounding and reduce hallucination in multi-turn emotional support. We introduce TEA-Bench, the first interactive benchmark for evaluating tool-augmented agents in ESC, featuring realistic emotional scenarios, an MCP-style tool environment, and process-level metrics that jointly assess the quality and factual grounding of emotional support. Experiments on nine LLMs show that tool augmentation generally improves emotional support quality and reduces hallucination, but the gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. We further release TEA-Dialog, a dataset of tool-enhanced ESC dialogues, and find that supervised fine-tuning improves in-distribution support but generalizes poorly. Our results underscore the importance of tool use in building reliable emotional support agents.

</details>


### [66] [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706)
*Zhichao Yang,Sepehr Janghorbani,Dongxu Zhang,Jun Han,Qian Qian,Andrew Ressler,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: The paper presents Health-SCORE, a framework for scalable and efficient rubric-based training and evaluation for open-ended LLM responses in healthcare.


<details>
  <summary>Details</summary>
Motivation: Creating high-quality rubrics for LLM evaluation in domains such as healthcare is time-intensive and costly, limiting scalability.

Method: Health-SCORE is introduced as a scalable framework that reduces the cost of rubric development, provides structured reinforcement learning supervision, and enables in-context learning through prompts.

Result: Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly reducing development costs.

Conclusion: Health-SCORE makes rubric-based evaluation and training for healthcare LLM responses scalable and effective without compromising quality.

Abstract: Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.

</details>


### [67] [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716)
*Naeyma N. Islam,Thomas R. Caulfield*

Main category: cs.AI

TL;DR: The paper introduces an AI-assisted approach to design molecular glues targeting amyloid beta-42 (Abeta-42) for Alzheimer's disease treatment using the ubiquitin-proteasome system.


<details>
  <summary>Details</summary>
Motivation: Given the role of intracellular Abeta-42 in driving Alzheimer's disease progression, addressing its targeted degradation is essential and currently limited.

Method: The study employs structure-based modeling, ADMET screening, docking, and a Ligase-Conditioned Junction Tree Variational Autoencoder to design E3 ligase-specific molecular glues.

Result: The designed generative model successfully produced novel, chemically valid molecules tailored to degrade Abeta-42 by targeting specific E3 ligases.

Conclusion: This innovative AI-driven approach has the potential to transform UPS-targeted therapy design for Alzheimer's disease and other neurodegenerative conditions.

Abstract: Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.

</details>


### [68] [Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems](https://arxiv.org/abs/2601.18735)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Jing Yang,Jiawei Yao,Jian Wang,Guanlong Qu,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: Agora introduces a market-based framework for economically sustainable coordination in multi-agent, vision-language systems, outperforming existing strategies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the high economic costs and inefficiency of scaling vision-language multi-agent systems, caused by uncoordinated approaches.

Method: Agora converts epistemic uncertainty into tradable assets and employs a market-aware broker to guide agents towards cost-efficient, coordinated decision-making.

Result: Agora achieves superior performance (e.g., +8.5% accuracy on MMMU) while significantly reducing costs by over 3x compared to baseline strategies.

Conclusion: Market-based coordination offers a scalable and cost-efficient methodology for multi-agent visual intelligence systems, proving its superiority over traditional approaches.

Abstract: Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.

</details>


### [69] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu,Xingang Guo,Lingzhi Yuan,Haoqiang Kang,Hongyu Zhao,Lianhui Qin,Furong Huang,Bin Hu,Tianyi Zhou*

Main category: cs.AI

TL;DR: The paper introduces TSRBench, a benchmark assessing time series reasoning across 14 domains and 4 dimensions, highlighting challenges in generalist models' reasoning and prediction capabilities.


<details>
  <summary>Details</summary>
Motivation: Time series reasoning is crucial for solving real-world problems, yet it is absent from generalist model benchmarks. The paper aims to address this gap.

Method: They developed TSRBench featuring 4125 problems across 14 domains and evaluated 30 models on 15 diverse tasks spanning perception to decision-making.

Result: Results reveal gaps in current models' predictive and fusion capabilities, including decoupling between reasoning and forecasting, and scaling law limitations.

Conclusion: TSRBench provides a framework to assess and evolve generalist models' time series reasoning to address critical application challenges.

Abstract: Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [70] [SPADE: A SIMD Posit-enabled compute engine for Accelerating DNN Efficiency](https://arxiv.org/abs/2601.17279)
*Sonu Kumar,Lavanya Vinnakota,Mukul Lokhande,Santosh Kumar Vishvakarma,Adam Teman*

Main category: cs.AR

TL;DR: The paper introduces SPADE, a multi-precision SIMD Posit-based MAC architecture, which optimizes energy and hardware efficiency while maintaining numerical accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the increasing need for efficient edge-AI systems that support various numerical formats with precision, energy efficiency, and compact hardware, while leveraging the advantages of Posit arithmetic.

Method: Developed SPADE, a multi-precision MAC architecture that hierarchically reuses Posit-specific submodules and supports Posit (8,0), Posit (16,1), and Posit (32,2) without replicating the entire datapath.

Result: FPGA implementation demonstrated significant resource reductions (45.13% LUT and 80% slice for Posit 8,0; 28.44% improvement for Posit 16,1; 17.47% for Posit 32,2). ASIC achieved high performance and low power (1.38 GHz at 6.1 mW).

Conclusion: SPADE effectively balances hardware efficiency and numerical robustness, showcasing its applicability in AI inference tasks (e.g., MNIST, CIFAR-10/100) with competitive accuracy and efficiency.

Abstract: The growing demand for edge-AI systems requires arithmetic units that balance numerical precision, energy efficiency, and compact hardware while supporting diverse formats. Posit arithmetic offers advantages over floating- and fixed-point representations through its tapered precision, wide dynamic range, and improved numerical robustness. This work presents SPADE, a unified multi-precision SIMD Posit-based multiplyaccumulate (MAC) architecture supporting Posit (8,0), Posit (16,1), and Posit (32,2) within a single framework. Unlike prior single-precision or floating/fixed-point SIMD MACs, SPADE introduces a regime-aware, lane-fused SIMD Posit datapath that hierarchically reuses Posit-specific submodules (LOD, complementor, shifter, and multiplier) across 8/16/32-bit precisions without datapath replication. FPGA implementation on a Xilinx Virtex-7 shows 45.13% LUT and 80% slice reduction for Posit (8,0), and up to 28.44% and 17.47% improvement for Posit (16,1) and Posit (32,2) over prior work, with only 6.9% LUT and 14.9% register overhead for multi-precision support. ASIC results across TSMC nodes achieve 1.38 GHz at 6.1 mW (28 nm). Evaluation on MNIST, CIFAR-10/100, and alphabet datasets confirms competitive inference accuracy.

</details>


### [71] [Athena: Synergizing Data Prefetching and Off-Chip Prediction via Online Reinforcement Learning](https://arxiv.org/abs/2601.17615)
*Rahul Bera,Zhenrong Lang,Caroline Hengartner,Konstantinos Kanellopoulos,Rakesh Kumar,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.AR

TL;DR: Athena, a reinforcement learning-based framework, coordinates prefetchers and off-chip predictors to optimize memory latency performance in high-performance processors.


<details>
  <summary>Details</summary>
Motivation: To address the complementary benefits yet inefficiencies in current prefetching and off-chip predictor techniques to optimize memory access latencies.

Method: Model coordination of prefetchers and off-chip predictor (OCP) as a reinforcement learning problem, with Athena autonomously learning a policy using system-level features and rewards.

Result: Athena demonstrated superior performance compared to state-of-the-art coordination policies across diverse workloads and system configurations, while maintaining modest storage overhead.

Conclusion: Athena achieves efficient autonomous coordination of prefetchers and OCPs, advancing memory latency optimization and reducing inefficiencies.

Abstract: Prefetching and off-chip prediction are two techniques proposed to hide long memory access latencies in high-performance processors. In this work, we demonstrate that: (1) prefetching and off-chip prediction often provide complementary performance benefits, yet (2) naively combining them often fails to realize their full performance potential, and (3) existing prefetcher control policies leave significant room for performance improvement behind.
  Our goal is to design a holistic framework that can autonomously learn to coordinate an off-chip predictor with multiple prefetchers employed at various cache levels. To this end, we propose a new technique called Athena, which models the coordination between prefetchers and off-chip predictor (OCP) as a reinforcement learning (RL) problem. Athena acts as the RL agent that observes multiple system-level features (e.g., prefetcher/OCP accuracy, bandwidth usage) over an epoch of program execution, and uses them as state information to select a coordination action (i.e., enabling the prefetcher and/or OCP, and adjusting prefetcher aggressiveness). At the end of every epoch, Athena receives a numerical reward that measures the change in multiple system-level metrics (e.g., number of cycles taken to execute an epoch). Athena uses this reward to autonomously and continuously learn a policy to coordinate prefetchers with OCP.
  Our extensive evaluation using a diverse set of memory-intensive workloads shows that Athena consistently outperforms prior state-of-the-art coordination policies across a wide range of system configurations with various combinations of underlying prefetchers, OCPs, and main memory bandwidths, while incurring only modest storage overhead. Athena is freely available at https://github.com/CMU-SAFARI/Athena.

</details>


### [72] [Conduit: Programmer-Transparent Near-Data Processing Using Multiple Compute-Capable Resources in Solid State Drives](https://arxiv.org/abs/2601.17633)
*Rakesh Nadig,Vamanan Arulchelvan,Mayank Kabra,Harshita Gupta,Rahul Bera,Nika Mansouri Ghiasi,Nanditha Rao,Qingcai Jiang,Andreas Kosmas Kakolyris,Yu Liang,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.AR

TL;DR: Conduit is a framework that optimizes computation in SSDs, leveraging multiple SSD resources to enhance efficiency and reduce energy consumption.


<details>
  <summary>Details</summary>
Motivation: Existing SSD-based NDP techniques operate in isolation and do not exploit the full computational potential of SSDs, leading to inefficiencies and limited adaptability.

Method: Conduit uses a custom compiler to vectorize application code for SIMD operations and embeds metadata to guide runtime decisions, relying on a cost function to select optimal SSD resources.

Result: Conduit outperforms prior methods by 1.8x in efficiency and reduces energy consumption by 46%, as demonstrated using an SSD simulator.

Conclusion: Conduit leverages heterogeneous SSD resources effectively for computation, enhancing performance and energy efficiency in data-intensive workloads.

Abstract: Solid-state drives (SSDs) are well suited for near-data processing (NDP) because they: (1) store large application datasets, and (2) support three NDP paradigms: in-storage processing (ISP), processing using DRAM in the SSD (PuD-SSD), and in-flash processing (IFP). A large body of prior SSD-based NDP techniques operate in isolation, mapping computations to only one or two NDP paradigms (i.e., ISP, PuD-SSD, or IFP) within the SSD. These techniques (1) are tailored to specific workloads or kernels, (2) do not exploit the full computational potential of an SSD, and (3) lack programmer-transparency. While several prior works propose techniques to partition computation between the host and near-memory accelerators, adapting these techniques to SSDs has limited benefits because they (1) ignore the heterogeneity of the SSD resources, and (2) make offloading decisions based on limited factors such as bandwidth utilization, or data movement cost. We propose Conduit, a general-purpose, programmer-transparent NDP framework for SSDs that leverages multiple SSD computation resources. At compile time, Conduit executes a custom compiler (e.g., LLVM) pass that (i) vectorizes suitable application code segments into SIMD operations that align with the SSD's page layout, and (ii) embeds metadata (e.g., operation type, operand sizes) into the vectorized instructions to guide runtime offloading decisions. At runtime, within the SSD, Conduit performs instruction-granularity offloading by evaluating six key features, and uses a cost function to select the most suitable SSD resource. We evaluate Conduit and two prior NDP offloading techniques using an in-house event-driven SSD simulator on six data-intensive workloads. Conduit outperforms the best-performing prior offloading policy by 1.8x and reduces energy consumption by 46%.

</details>


### [73] [Late Breaking Results: Boosting Efficient Dual-Issue Execution on Lightweight RISC-V Cores](https://arxiv.org/abs/2601.17940)
*Luca Colagrande,Luca Benini*

Main category: cs.AR

TL;DR: The paper introduces COPIFTv2, a lightweight, open-source system improving communication and synchronization in RISC-V cores, achieving up to 1.49x speedup and 1.47x energy efficiency gain over its predecessor.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the performance and energy efficiency of lightweight processing elements in ML accelerators by addressing the limitations of the complex and error-prone programming model (COPIFT) used in dual-issue execution.

Method: The paper proposes COPIFTv2, which integrates lightweight queues for direct communication and synchronization between integer and FP threads in lightweight RISC-V cores, eliminating the tiling and software pipelining steps of the prior model.

Result: COPIFTv2 achieves up to a 1.49x speedup, 1.47x energy efficiency gain over COPIFT, and a peak IPC of 1.81, all while simplifying programmability.

Conclusion: COPIFTv2 significantly improves the efficiency, energy use, and ease of programming in dual-issue execution for lightweight cores, making it a valuable advancement for ML accelerators.

Abstract: Large-scale ML accelerators rely on large numbers of PEs, imposing strict bounds on the area and energy budget of each PE. Prior work demonstrates that limited dual-issue capabilities can be efficiently integrated into a lightweight in-order open-source RISC-V core (Snitch), with a geomean IPC boost of 1.6x and a geomean energy efficiency gain of 1.3x, obtained by concurrently executing integer and FP instructions. Unfortunately, this required a complex and error-prone low level programming model (COPIFT). We introduce COPIFTv2 which augments Snitch with lightweight queues enabling direct, fine-grained communication and synchronization between integer and FP threads. By eliminating the tiling and software pipelining steps of COPIFT, we can remove much of its complexity and software overheads. As a result, COPIFTv2 achieves up to a 1.49x speedup and a 1.47x energy-efficiency gain over COPIFT, and a peak IPC of 1.81. Overall, COPIFTv2 significantly enhances the efficiency and programmability of dual-issue execution on lightweight cores. Our implementation is fully open source and performance experiments are reproducible using free software.

</details>


### [74] [Memory-Efficient FPGA Implementation of Stochastic Simulated Annealing](https://arxiv.org/abs/2601.18007)
*Duckgyu Shin,Naoya Onizawa,Warren J. Gross,Takahiro Hanyu*

Main category: cs.AR

TL;DR: The paper introduces the hardware-aware stochastic simulated annealing (HA-SSA) algorithm for FPGA implementations, improving memory efficiency and computation speed on optimization problems.


<details>
  <summary>Details</summary>
Motivation: To resolve the challenge of increased computation time and inefficiency of memory usage in simulated annealing for combinatorial optimization problems, particularly for FPGA implementations.

Method: The researchers developed the HA-SSA algorithm—a variation of SSA—to optimize memory usage without compromising computation speed. They evaluated it on maximum cut problems and implemented it on an FPGA platform.

Result: HA-SSA accelerates convergence up to 114-times faster compared to conventional SA and improves memory efficiency by 6-times compared to SSA while maintaining solution quality.

Conclusion: HA-SSA proves effective for solving combinatorial optimization problems on FPGA, offering both superior speed and memory efficiency without degrading solution quality.

Abstract: Simulated annealing (SA) is a well-known algorithm for solving combinatorial optimization problems. However, the computation time of SA increases rapidly, as the size of the problem grows. Recently, a stochastic simulated annealing (SSA) algorithm that converges faster than conventional SA has been reported. In this paper, we present a hardware-aware SSA (HA- SSA) algorithm for memory-efficient FPGA implementations. HA-SSA can reduce the memory usage of storing intermediate results while maintaining the computing speed of SSA. For evaluation purposes, the proposed algorithm is compared with the conventional SSA and SA approaches on maximum cut combinatorial optimization problems. HA-SSA achieves a convergence speed that is up to 114-times faster than that of the conventional SA algorithm depending on the maximum cut problem selected from the G-set which is a dataset of the maximum cut problems. HA-SSA is implemented on a field-programmable gate array (FPGA) (Xilinx Kintex-7), and it achieves up to 6-times the memory efficiency of conventional SSA while maintaining high solution quality for optimization problems.

</details>


### [75] [CIM-Tuner: Balancing the Compute and Storage Capacity of SRAM-CIM Accelerator via Hardware-mapping Co-exploration](https://arxiv.org/abs/2601.18070)
*Jinwu Chen,Yuhui Shi,He Wang,Zhe Jiang,Jun Yang,Xin Si,Zhenhua Zhu*

Main category: cs.AR

TL;DR: The paper introduces CIM-Tuner, a tool for optimizing hardware and mapping strategies for SRAM Computing-In-Memory (CIM) accelerators, achieving significant gains in energy efficiency and throughput.


<details>
  <summary>Details</summary>
Motivation: High energy efficiency and throughput in SRAM CIM accelerators are hampered by varying designs and suboptimal mapping strategies, causing performance degradation.

Method: Introducing CIM-Tuner, a hardware-mapping co-exploration tool employing matrix abstraction and two-level mapping strategies for optimized resource balancing across CIM designs.

Result: CIM-Tuner achieves 1.58× higher energy efficiency and 2.11× higher throughput compared to prior mappings. Comparable improvements are observed with current state-of-the-art CIM accelerators.

Conclusion: CIM-Tuner effectively optimizes hardware balancing and mapping for SRAM-CIM accelerators under area constraints, showcasing its universality, accuracy, and practicality. It is open-sourced for broad use.

Abstract: As an emerging type of AI computing accelerator, SRAM Computing-In-Memory (CIM) accelerators feature high energy efficiency and throughput. However, various CIM designs and under-explored mapping strategies impede the full exploration of compute and storage balancing in SRAM-CIM accelerator, potentially leading to significant performance degradation. To address this issue, we propose CIM-Tuner, an automatic tool for hardware balancing and optimal mapping strategy under area constraint via hardware-mapping co-exploration. It ensures universality across various CIM designs through a matrix abstraction of CIM macros and a generalized accelerator template. For efficient mapping with different hardware configurations, it employs fine-grained two-level strategies comprising accelerator-level scheduling and macro-level tiling. Compared to prior CIM mapping, CIM-Tuner's extended strategy space achieves 1.58$\times$ higher energy efficiency and 2.11$\times$ higher throughput. Applied to SOTA CIM accelerators with identical area budget, CIM-Tuner also delivers comparable improvements. The simulation accuracy is silicon-verified and CIM-Tuner tool is open-sourced at https://github.com/champloo2878/CIM-Tuner.git.

</details>


### [76] [RTeAAL Sim: Using Tensor Algebra to Represent and Accelerate RTL Simulation (Extended Version)](https://arxiv.org/abs/2601.18140)
*Yan Zhu,Boru Chen,Christopher W. Fletcher,Nandeeka Nayak*

Main category: cs.AR

TL;DR: This paper presents RTeAAL Sim, a method that redefines RTL simulation as a sparse tensor algebra problem to improve performance and reduce compilation overhead.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies in RTL simulation such as long compilation times and frontend CPU bottlenecking caused by instruction-cache pressure.

Method: The paper proposes reimagining RTL simulations as sparse tensor algebra problems, representing circuits as tensors and simulations as sparse tensor algebra kernels to leverage algebraic optimizations.

Result: The prototype tensor-based simulator demonstrated reduced compilation overhead and frontend pressure and achieved performance comparable to Verilator across multiple CPUs and ISAs.

Conclusion: RTeAAL Sim successfully reformulates RTL simulation, enabling optimizations and competitive performance, overcoming traditional RTL simulation bottlenecks.

Abstract: RTL simulation on CPUs remains a persistent bottleneck in hardware design. State-of-the-art simulators embed the circuit directly into the simulation binary, resulting in long compilation times and execution that is fundamentally CPU frontend-bound, with severe instruction-cache pressure.
  This work proposes RTeAAL Sim, which reformulates RTL simulation as a sparse tensor algebra problem. By representing RTL circuits as tensors and simulation as a sparse tensor algebra kernel, RTeAAL Sim decouples simulation behavior from binary size and makes RTL simulation amenable to well-studied tensor algebra optimizations. We demonstrate that a prototype of our tensor-based simulator, even with a subset of these optimizations, already mitigates the compilation overhead and frontend pressure and achieves performance competitive with the highly optimized Verilator simulator across multiple CPUs and ISAs.

</details>


### [77] [Lifecycle Cost-Effectiveness Modeling for Redundancy-Enhanced Multi-Chiplet Architectures](https://arxiv.org/abs/2601.18159)
*Zizhen Liu,Fangzhiyi Wang,Mengdi Wang,Jing Ye,Hayden Kwok-Hay So,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: The paper introduces a Lifecycle Cost Effectiveness (LCE) framework to optimize costs for multi-chiplet architectures by factoring redundancy and lifetime computations.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of economic efficiency in multi-chiplet architectures by integrating redundancy strategies and operational lifetime analysis.

Method: Developed a comprehensive LCE framework with redundancy-aware cost modeling, reliability-driven lifetime estimation, and analysis of redundancy configurations.

Result: The framework showcased the importance of module and chiplet-level redundancy co-optimization through trade-off and optimization studies.

Conclusion: The proposed approach enables cost-efficient designs for multi-chiplet architectures, contributing to long-term economic benefits and operational scalability.

Abstract: The growing demand for compute-intensive applications has made multi-chiplet architectures a promising alternative to monolithic designs, offering improved scalability and manufacturing flexibility. However, effectively managing the economic effectiveness remains challenging. Existing cost models either overlook the amortization of compute value over a chip's operational lifetime or fail to evaluate how redundancy strategies, which are widely adopted to enhance yield and fault tolerance, impact long-term cost efficiency. This paper presents a comprehensive cost-effectiveness framework for multi-chiplet architectures, introducing a novel Lifecycle Cost Effectiveness (LCE) metric that evaluates amortized compute costs by jointly optimizing manufacturing expenses and operational lifetime. Our approach uniquely integrates: (1) redundancy-aware cost modeling spanning both intra- and inter-chiplet levels, (2) reliability-driven lifetime estimation, and (3) quantitative analysis of how redundancy configurations on overall economic effectiveness. Extensive trade-off and multi-objective optimization studies demonstrate the effectiveness of the model and reveal essential co-optimization strategies between module and chiplet-level redundancy to achieve cost-efficient multi-chiplet architecture designs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [78] [Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle](https://arxiv.org/abs/2601.16986)
*Zihan Wang,Cheng Tang,Lei Gong,Cheng Li,Chao Wang,teng wang,Wenqi Lou,Xuehai Zhou*

Main category: cs.CL

TL;DR: Crystal-KV is a framework to manage Key-Value (KV) cache in Chain-of-Thought reasoning tasks for large language models, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought reasoning in LLMs requires excessive memory during inference due to lengthy think-stage sequences stored in the KV cache. The motivation is to address this inefficiency while maintaining task accuracy.

Method: The paper introduces Crystal-KV, which employs the 'answer-first principle' to prioritize reasoning relevant to final answers. It utilizes an attention-based Least Recently Frequently Used algorithm for intelligent cache entry eviction and adaptive budget allocation algorithms for optimal resource utilization.

Result: Crystal-KV improves KV cache compression and optimizes throughput. It enhances response time, maintains answer accuracy, and often improves it for CoT reasoning tasks.

Conclusion: Crystal-KV establishes a revolutionary framework for improving KV cache management in reasoning-heavy tasks. It effectively balances memory usage, computational efficiency, and reasoning accuracy.

Abstract: Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.

</details>


### [79] [Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions](https://arxiv.org/abs/2601.16987)
*Shunyang Luo,Peibei Cao,Zhihui Zhu,Kehua Feng,Zhihua Wang,Keyan Ding*

Main category: cs.CL

TL;DR: The paper introduces Pairwise Maximum Discrepancy Competition (PMDC), a new method for evaluating reward models' generalization abilities in large language models, especially in unseen and dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing reward model evaluations, which rely on static datasets and do not adequately test generalization in open-world or shifting distributions.

Method: The method involves a framework called PMDC, where highly contentious test cases are dynamically selected based on the disagreement between two reward models. These cases are then judged by an oracle, and results are aggregated using a Bradley-Terry model to produce rankings and win-rates.

Result: The application of PMDC on 10 reward models revealed significant rank changes compared to conventional metrics and uncovered systematic generalization issues.

Conclusion: PMDC showcases improved assessment of model generalization, providing deeper insights into model performance and guiding improvements in reward modeling.

Abstract: Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.

</details>


### [80] [Uncertainty Quantification for Named Entity Recognition via Full-Sequence and Subsequence Conformal Prediction](https://arxiv.org/abs/2601.16999)
*Matthew Singer,Srijan Sengupta,Karl Pazdernik*

Main category: cs.CL

TL;DR: The paper introduces a framework for making Named Entity Recognition (NER) models uncertainty-aware by generating prediction sets with guaranteed correctness at a specified confidence level, improving reliability and reducing downstream cascading errors.


<details>
  <summary>Details</summary>
Motivation: Current NER models do not provide uncertainty estimates, which can lead to errors propagating in downstream applications.

Method: The paper adapts NER models to produce confidence-based prediction sets using conformal prediction and designed efficient nonconformity scoring functions to ensure well-calibrated sets.

Result: Experiments on four NER models and three datasets demonstrate the framework's broad applicability, validity, and efficiency.

Conclusion: The method provides a way to make NER predictions more robust and reliable, similar to confidence intervals in classical statistics, by introducing formal guarantees about prediction quality.

Abstract: Named Entity Recognition (NER) serves as a foundational component in many natural language processing (NLP) pipelines. However, current NER models typically output a single predicted label sequence without any accompanying measure of uncertainty, leaving downstream applications vulnerable to cascading errors. In this paper, we introduce a general framework for adapting sequence-labeling-based NER models to produce uncertainty-aware prediction sets. These prediction sets are collections of full-sentence labelings that are guaranteed to contain the correct labeling with a user-specified confidence level. This approach serves a role analogous to confidence intervals in classical statistics by providing formal guarantees about the reliability of model predictions. Our method builds on conformal prediction, which offers finite-sample coverage guarantees under minimal assumptions. We design efficient nonconformity scoring functions to construct efficient, well-calibrated prediction sets that support both unconditional and class-conditional coverage. This framework accounts for heterogeneity across sentence length, language, entity type, and number of entities within a sentence. Empirical experiments on four NER models across three benchmark datasets demonstrate the broad applicability, validity, and efficiency of the proposed methods.

</details>


### [81] [RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection](https://arxiv.org/abs/2601.17002)
*Ziyang Zhou,Ziqi Liu,Yan Wang,Yiming Lin,Yangbin Chen*

Main category: cs.CL

TL;DR: The paper introduces RAM-SD, a sarcasm detection framework that outperforms previous models and provides interpretable reasoning processes.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge in sarcasm detection, stemming from the nuanced and diverse linguistic cues, external knowledge, and contextual understanding required.

Method: The proposed Retrieval-Augmented Multi-Agent (RAM-SD) framework operates in four stages: contextual retrieval, meta-planning for classification and reasoning plan selection, multi-view analysis by specialized agents, and integration into a final interpretable judgment.

Result: RAM-SD outperforms the GPT-4o+CoC baseline by 7.01 Macro-F1 points and achieves state-of-the-art results on four standard benchmarks.

Conclusion: RAM-SD not only sets a performance benchmark but also enhances the understanding of sarcasm detection processes by providing interpretable reasoning traces.

Abstract: Sarcasm detection remains a significant challenge due to its reliance on nuanced contextual understanding, world knowledge, and multi-faceted linguistic cues that vary substantially across different sarcastic expressions. Existing approaches, from fine-tuned transformers to large language models, apply a uniform reasoning strategy to all inputs, struggling to address the diverse analytical demands of sarcasm. These demands range from modeling contextual expectation violations to requiring external knowledge grounding or recognizing specific rhetorical patterns. To address this limitation, we introduce RAM-SD, a Retrieval-Augmented Multi-Agent framework for Sarcasm Detection. The framework operates through four stages: (1) contextual retrieval grounds the query in both sarcastic and non-sarcastic exemplars; (2) a meta-planner classifies the sarcasm type and selects an optimal reasoning plan from a predefined set; (3) an ensemble of specialized agents performs complementary, multi-view analysis; and (4) an integrator synthesizes these analyses into a final, interpretable judgment with a natural language explanation. Evaluated on four standard benchmarks, RAM-SD achieves a state-of-the-art Macro-F1 of 77.74%, outperforming the strong GPT-4o+CoC baseline by 7.01 points. Our framework not only sets a new performance benchmark but also provides transparent and interpretable reasoning traces, illuminating the cognitive processes behind sarcasm comprehension.

</details>


### [82] [From Emotion to Expression: Theoretical Foundations and Resources for Fear Speech](https://arxiv.org/abs/2601.17132)
*Vigneshwaran Shankaran,Gabriella Lapesa,Claudia Wagner*

Main category: cs.CL

TL;DR: This paper explores the concept of fear speech, its theoretical foundation, and its distinct nature from hate speech, offering a taxonomy and methodology for further study.


<details>
  <summary>Details</summary>
Motivation: The study addresses the growing presence and impact of fear speech, filling the gap in computational linguistics by focusing on it as a unique form of communication, distinct from hate speech.

Method: The paper compares theories from multiple disciplines including Psychology, Political Science, Communication Science, and Linguistics, reviews existing definitions, surveys related datasets, and proposes a consolidated taxonomy for fear speech.

Result: It provides a theoretical framework and practical guidance for identifying and studying fear speech, emphasizing its complexity and differences from hate speech.

Conclusion: The work lays a foundation for better understanding fear speech, aiding in the development of datasets and methodologies, and advancing research in this fragmented area.

Abstract: Few forces rival fear in their ability to mobilize societies, distort communication, and reshape collective behavior. In computational linguistics, fear is primarily studied as an emotion, but not as a distinct form of speech. Fear speech content is widespread and growing, and often outperforms hate-speech content in reach and engagement because it appears "civiler" and evades moderation. Yet the computational study of fear speech remains fragmented and under-resourced. This can be understood by recognizing that fear speech is a phenomenon shaped by contributions from multiple disciplines. In this paper, we bridge cross-disciplinary perspectives by comparing theories of fear from Psychology, Political science, Communication science, and Linguistics. Building on this, we review existing definitions. We follow up with a survey of datasets from related research areas and propose a taxonomy that consolidates different dimensions of fear for studying fear speech. By reviewing current datasets and defining core concepts, our work offers both theoretical and practical guidance for creating datasets and advancing fear speech research.

</details>


### [83] [Dynamic Role Assignment for Multi-Agent Debate](https://arxiv.org/abs/2601.17152)
*Miao Zhang,Junsik Kim,Siyuan Xiang,Jian Gao,Cheng Cao*

Main category: cs.CL

TL;DR: Proposes a dynamic role assignment framework with a Meta-Debate system for agent selection in multi-agent debate setups, showing performance improvements over uniform and random model assignments.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems employ fixed role assignments without leveraging agent specializations to determine role suitability, limiting their problem-solving efficiency.

Method: Introduced a Meta-Debate system with two stages—proposal (role-tailored arguments by candidates) and peer review (scoring arguments with role-specific criteria) to dynamically select agents for roles.

Result: Dynamic role assignment outperformed uniform role assignments by up to 74.8% and random role assignments by up to 29.7% across LLM problem-solving benchmarks.

Conclusion: Dynamic role assignment enhances the efficiency of multi-agent systems by enabling capability-aware role selection, setting a benchmark for flexible and specialized agent deployment.

Abstract: Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.

</details>


### [84] [Interpretability of the Intent Detection Problem: A New Approach](https://arxiv.org/abs/2601.17156)
*Eduardo Sanchez-Karhunen,Jose F. Quesada-Moreno,Miguel A. Gutiérrez-Naranjo*

Main category: cs.CL

TL;DR: The paper analyzes how RNN architectures solve intent detection tasks using dynamical systems theory, showing how balanced and imbalanced datasets affect hidden state clusters for intents.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how Recurrent Neural Networks operate internally to solve intent detection tasks, which have widespread applications in business and beyond.

Method: The authors interpret sentences as trajectories in the RNN hidden state space and perform analysis on two datasets: the balanced SNIPS and the imbalanced ATIS, observing geometric clusters and distortions.

Result: The network achieves geometric clustering for intents on balanced datasets, while class imbalance in datasets leads to distorted clusters for low-frequency intents, affecting performance.

Conclusion: The study provides a geometric explanation of RNN behavior and connects dataset properties directly to their computational solutions, advancing understanding of RNN dynamics.

Abstract: Intent detection, a fundamental text classification task, aims to identify and label the semantics of user queries, playing a vital role in numerous business applications. Despite the dominance of deep learning techniques in this field, the internal mechanisms enabling Recurrent Neural Networks (RNNs) to solve intent detection tasks are poorly understood. In this work, we apply dynamical systems theory to analyze how RNN architectures address this problem, using both the balanced SNIPS and the imbalanced ATIS datasets. By interpreting sentences as trajectories in the hidden state space, we first show that on the balanced SNIPS dataset, the network learns an ideal solution: the state space, constrained to a low-dimensional manifold, is partitioned into distinct clusters corresponding to each intent. The application of this framework to the imbalanced ATIS dataset then reveals how this ideal geometric solution is distorted by class imbalance, causing the clusters for low-frequency intents to degrade. Our framework decouples geometric separation from readout alignment, providing a novel, mechanistic explanation for real world performance disparities. These findings provide new insights into RNN dynamics, offering a geometric interpretation of how dataset properties directly shape a network's computational solution.

</details>


### [85] [Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text](https://arxiv.org/abs/2601.17172)
*Tunazzina Islam*

Main category: cs.CL

TL;DR: This paper analyzes how large language models (LLMs) generate demographic-conditioned targeted messages, revealing biases influenced by age and gender.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by concerns about the biases and fairness of automated communication when LLMs create personalized, persuasive content at scale.

Method: Using a controlled evaluation framework, the authors test three major LLMs (GPT-4o, Llama-3.3, and Mistral-Large 2.1) in two settings: Standalone Generation and Context-Rich Generation, assessing lexical, stylistic, and persuasive dimensions.

Result: The analysis highlights demographic biases, such as age- and gender-specific messaging patterns, with male and youth-targeted messages focusing on innovation and agency, while female and senior-targeted ones emphasize care and tradition.

Conclusion: Demographic stereotypes emerge and intensify in LLM outputs, necessitating bias-aware generation systems and transparent auditing for fair and sensitive applications.

Abstract: Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.

</details>


### [86] [Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content](https://arxiv.org/abs/2601.17173)
*Parth Bhalerao,Diola Dsouza,Ruiwen Guan,Oana Ignat*

Main category: cs.CL

TL;DR: Introduction of MentorQA, a mentorship-focused multilingual QA dataset with nearly 9,000 QA pairs from long-form videos in multiple languages. Multi-Agent architectures outperform others, especially for complex topics, and human judgment alignment challenges automated evaluations.


<details>
  <summary>Details</summary>
Motivation: To address mentorship-focused QA needs in applications like education and guidance, where clarity, reflection, and learning value go beyond simple factual correctness.

Method: Creation of the MentorQA dataset and evaluation framework consisting of mentorship-focused dimensions applied across QA architectures (Single-Agent, Dual-Agent, RAG, Multi-Agent). Empirical testing and analysis of automated and human evaluation techniques.

Result: Multi-Agent architectures provide superior mentorship responses for complex and low-resource languages. Automated LLM evaluations exhibit inconsistent alignment with human judgments.

Conclusion: Mentorship-focused QA is established as a unique research area, offering new multilingual frameworks and benchmarks to enhance agent-based QA systems' effectiveness in educational AI settings.

Abstract: Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at https://github.com/AIM-SCU/MentorQA.

</details>


### [87] [Systematicity between Forms and Meanings across Languages Supports Efficient Communication](https://arxiv.org/abs/2601.17181)
*Doreen Osmelak,Yang Xu,Michael Hahn,Kate McCurdy*

Main category: cs.CL

TL;DR: This paper investigates how grammatical meanings are expressed in verbs and pronouns across languages, demonstrating how competing pressures influence linguistic structures.


<details>
  <summary>Details</summary>
Motivation: To explore how linguistic forms accommodate systematic relations and optimize for efficient communication while balancing simplicity and accuracy.

Method: A novel complexity measure, based on learnability, is introduced to analyze meaning-to-form mappings in verbs and pronouns of diverse languages.

Result: The model reveals fine-grained regularities in language structures, distinguishing between plausible and implausible linguistic systems.

Conclusion: The findings extend efficient communication theory, linking it to systematicity in natural language through new complexity measures.

Abstract: Languages vary widely in how meanings map to word forms. These mappings have been found to support efficient communication; however, this theory does not account for systematic relations within word forms. We examine how a restricted set of grammatical meanings (e.g. person, number) are expressed on verbs and pronouns across typologically diverse languages. Consistent with prior work, we find that verb and pronoun forms are shaped by competing communicative pressures for simplicity (minimizing the inventory of grammatical distinctions) and accuracy (enabling recovery of intended meanings). Crucially, our proposed model uses a novel measure of complexity (inverse of simplicity) based on the learnability of meaning-to-form mappings. This innovation captures fine-grained regularities in linguistic form, allowing better discrimination between attested and unattested systems, and establishes a new connection from efficient communication theory to systematicity in natural language.

</details>


### [88] [Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding](https://arxiv.org/abs/2601.17197)
*Seyyed Saeid Cheshmi,Hahnemann Ortiz,James Mooney,Dongyeop Kang*

Main category: cs.CL

TL;DR: The paper addresses challenges in figurative language for vision-language models (VLMs), presenting a framework to improve multimodal reasoning with substantial performance and generalization boosts.


<details>
  <summary>Details</summary>
Motivation: Figurative language, which includes sarcasm, humor, and metaphor, poses challenges for VLMs due to its reliance on subtle meanings and incongruities, especially in multimodal settings.

Method: The paper proposes a three-step framework focusing on interpretation of figurative language, transparent reasoning traces, and generalization across figurative styles. Experiments applied these elements across various figurative styles.

Result: Results show reasoning traces improve figurative language understanding, learned reasoning transfers well across styles, and joint training across styles produces superior generalized VLMs.

Conclusion: Lightweight VLMs equipped with verifiable reasoning mechanisms achieve effective multimodal figurative language tasks and excel in cross-style generalization while providing inspectable reasoning traces.

Abstract: Vision-language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative language, such as sarcasm, humor, and metaphor, remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to others, especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization while providing inspectable reasoning traces for multimodal tasks. The code and implementation are available at https://github.com/scheshmi/CrossStyle-MMR.

</details>


### [89] [Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis](https://arxiv.org/abs/2601.17203)
*Scott Friedman,Sonja Schmer-Galunder,Anthony Chen,Jeffrey Rye*

Main category: cs.CL

TL;DR: The paper introduces a method to quantify and analyze gender bias in word embeddings to study gender gaps across various domains using large-scale data.


<details>
  <summary>Details</summary>
Motivation: To address the racial and gender biases present in NLP models' training data, while utilizing these biases to study real-world cultural gender gaps.

Method: Quantifies gender bias in word embeddings and correlates these biases with statistical gender gap metrics using Twitter data from 51 U.S. regions and 99 countries.

Result: Word embedding biases correlate with international and U.S.-based gender gap statistics, providing insights into cultural regularities.

Conclusion: Gender biases in word embeddings can serve as tools for understanding statistical gender gaps across diverse regions and domains.

Abstract: Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.

</details>


### [90] [DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.17212)
*Saadat Hasan Khan,Spencer Hong,Jingyu Wu,Kevin Lybarger,Youbing Yin,Erin Babinsky,Daben Liu*

Main category: cs.CL

TL;DR: DF-RAG introduces diversity in retrieval for complex QA tasks, achieving 4-10% better F1 than vanilla RAG.


<details>
  <summary>Details</summary>
Motivation: RAG struggles with reasoning-intensive QA due to redundant content in retrieval methods, reducing information recall.

Method: DF-RAG uses a diversity-focused retrieval approach based on Maximal Marginal Relevance to balance relevance and dissimilarity dynamically.

Result: DF-RAG achieves 4-10% F1 improvement over vanilla RAG and captures 91.3% of Oracle ceiling gains.

Conclusion: DF-RAG is an effective enhancement for reasoning-intensive QA tasks, showcasing significant performance gains.

Abstract: Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.

</details>


### [91] [Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning](https://arxiv.org/abs/2601.17223)
*Massimiliano Pronesti,Anya Belz,Yufang Hou*

Main category: cs.CL

TL;DR: The paper introduces Verifiable Process Reward Models (VPRMs), a framework where rule-based verifiers assess intermediate reasoning steps in reinforcement learning. This method outperforms existing models in logical coherence and evidence grounding, with up to 20% higher F1 scores.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the issues of bias, opacity, and reward hacking in existing methods of process supervision for large language models, using deterministic and verifiable rewards as a solution.

Method: VPRMs use deterministic, rule-based verifiers to assess intermediate reasoning steps in complex tasks like medical evidence synthesis, leveraging guideline-defined criteria and programmed verification processes.

Result: VPRMs led to reasoning that aligns closely with domain rules, achieving superior coherence between step-level decisions and outcomes. They outperformed existing models, achieving gains like up to 20% higher F1 scores and better logical consistency.

Conclusion: VPRMs demonstrate a novel and effective framework for improving reasoning processes in reinforcement learning, showing significant advantages over current state-of-the-art models in verifiability and performance.

Abstract: Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.

</details>


### [92] [Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation](https://arxiv.org/abs/2601.17226)
*David Y. Liu,Xanthe Muston,Aditya Joshi,Sebastian Sequoiah-Grayson*

Main category: cs.CL

TL;DR: This paper explores reinforcement learning as an alternative to supervised fine-tuning in automatic story generation, leveraging theories of narrative equilibrium and incorporating human-aligned evaluation by large language models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of using limited ground truths in training and evaluation of automatic story generation while improving the diversity and alignment of generated stories with human narrative conventions.

Method: Reinforcement learning (d-RLAIF) was applied post-training, leveraging Todorov’s Theory of Narrative Equilibrium. Large language models were used as judges to align with human annotators, alongside evaluation on human-written datasets like TimeTravel.

Result: The post-training reinforcement learning method produced more diverse stories that aligned better with human narrative conventions compared to supervised fine-tuning.

Conclusion: Reinforcement learning represents a promising alternative to supervised fine-tuning for subjective tasks like automatic story generation, achieving improved diversity and linguistic grounding.

Abstract: Despite the subjective nature of storytelling, past works on automatic story generation (ASG) have relied on limited ground truths for training and evaluation. In this work, we explore reinforcement learning (d-RLAIF) as a post-training alternative to supervised fine-tuning (SFT). We first apply Todorov's Theory of Narrative Equilibrium to establish principles that define desirable ASG qualities. We prompt 7B and 14B LLM-as-judge models with our principles to test alignment with human annotators and provide reward signals during d-RLAIF. We use Gemini-3-Flash to evaluate the output of our post-trained models and compare them to human-written stories from the TimeTravel dataset. We show that d-RLAIF offers a viable alternative to supervised fine-tuning (SFT)--producing stories that are more diverse and aligned with human narrative conventions. Our paper demonstrates the promise of reinforcement learning for linguistically grounded post-training for subjective tasks such as ASG.

</details>


### [93] [CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval](https://arxiv.org/abs/2601.17230)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: The paper presents CaseFacts, a legal fact-checking benchmark designed for layperson claims against U.S. Supreme Court precedents, emphasizing temporal validity and semantic complexity.


<details>
  <summary>Details</summary>
Motivation: Automated fact-checking has largely ignored evolving and technically complex domains like law, which is important for verifying legal claims accurately.

Method: The benchmark consists of categorizing 6,294 legal claims as Supported, Refuted, or Overruled, using a pipeline involving LLMs and a semantic similarity heuristic to verify claims.

Result: State-of-the-art LLMs struggle with the task as augmenting retrieval from the web produces noisy results, performing worse than closed-book methods.

Conclusion: CaseFacts is introduced as a novel dataset to address the challenging task of legal fact verification, aiming to advance this critical area of research.

Abstract: Automated Fact-Checking has largely focused on verifying general knowledge against static corpora, overlooking high-stakes domains like law where truth is evolving and technically complex. We introduce CaseFacts, a benchmark for verifying colloquial legal claims against U.S. Supreme Court precedents. Unlike existing resources that map formal texts to formal texts, CaseFacts challenges systems to bridge the semantic gap between layperson assertions and technical jurisprudence while accounting for temporal validity. The dataset consists of 6,294 claims categorized as Supported, Refuted, or Overruled. We construct this benchmark using a multi-stage pipeline that leverages Large Language Models (LLMs) to synthesize claims from expert case summaries, employing a novel semantic similarity heuristic to efficiently identify and verify complex legal overrulings. Experiments with state-of-the-art LLMs reveal that the task remains challenging; notably, augmenting models with unrestricted web search degrades performance compared to closed-book baselines due to the retrieval of noisy, non-authoritative precedents. We release CaseFacts to spur research into legal fact verification systems.

</details>


### [94] [Frame-Guided Synthetic Claim Generation for Automatic Fact-Checking Using High-Volume Tabular Data](https://arxiv.org/abs/2601.17232)
*Jacob Devasier,Akshith Putta,Qing Wang,Alankrit Moses,Chengkai Li*

Main category: cs.CL

TL;DR: The paper introduces a large-scale multilingual dataset for fact-checking against structured real-world data tables to bridge a critical research gap.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for automated fact-checking focus on small curated tables, neglecting the challenge posed by massive real-world data. This creates a gap in research.

Method: The authors created a dataset of 78,503 synthetic claims grounded in 434 large OECD tables using a frame-guided methodology. This methodology programmatically selects key data points using six semantic frames in multiple languages, ensuring realistic claim creation.

Result: Models were unable to leverage memorized facts, requiring genuine retrieval and reasoning due to the challenging nature of the benchmark. A baseline SQL-generation system highlights evidence retrieval as a significant bottleneck.

Conclusion: This dataset is valuable for progressing research in automated fact-checking against large-scale structured real-world data, emphasizing retrieval and reasoning challenges.

Abstract: Automated fact-checking benchmarks have largely ignored the challenge of verifying claims against real-world, high-volume structured data, instead focusing on small, curated tables. We introduce a new large-scale, multilingual dataset to address this critical gap. It contains 78,503 synthetic claims grounded in 434 complex OECD tables, which average over 500K rows each. We propose a novel, frame-guided methodology where algorithms programmatically select significant data points based on six semantic frames to generate realistic claims in English, Chinese, Spanish, and Hindi. Crucially, we demonstrate through knowledge-probing experiments that LLMs have not memorized these facts, forcing systems to perform genuine retrieval and reasoning rather than relying on parameterized knowledge. We provide a baseline SQL-generation system and show that our benchmark is highly challenging. Our analysis identifies evidence retrieval as the primary bottleneck, with models struggling to find the correct data in massive tables. This dataset provides a critical new resource for advancing research on this unsolved, real-world problem.

</details>


### [95] [PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues](https://arxiv.org/abs/2601.17277)
*Mohammad Rifqi Farhansyah,Hanif Muhammad Zhafran,Farid Adilazuarda,Shamsuddeen Hassan Muhammad,Maryam Ibrahim Mukhtar,Nedjma Ousidhoum,Genta Indra Winata,Ayu Purwarianti,Alham Fikri Aji*

Main category: cs.CL

TL;DR: The paper introduces PingPong, a benchmark for natural multi-party code-switching dialogues, highlighting its complexity and proposing specific downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to accurately reflect the complexity of code-switching in real-world multilingual communication, creating a need for datasets that better capture this phenomenon.

Method: The authors developed PingPong, a benchmark dataset featuring authentic multi-party, code-switched dialogues across five language combinations, incorporating human-generated conversations with diverse structural characteristics.

Result: Analysis demonstrated PingPong's superior naturalness and structural diversity compared to machine-generated dialogues. Evaluations showed existing language models struggle with code-switched inputs.

Conclusion: PingPong addresses the limitations of current benchmarks by offering a realistic and diverse dataset, highlighting the need for improved NLP systems to handle real-world multilingual dialogue complexities.

Abstract: Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.

</details>


### [96] [Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering](https://arxiv.org/abs/2601.17284)
*Yaokun Liu,Yifan Liu,Phoebe Mbuvi,Zelin Li,Ruichen Yao,Gawon Lim,Dong Wang*

Main category: cs.CL

TL;DR: This paper addresses ambiguity in medical question answering by linking it to aleatoric uncertainty and introduces a novel clarify-before-answer framework featuring an AU-Probe module to enhance safety and accuracy.


<details>
  <summary>Details</summary>
Motivation: To tackle the critical issue of ambiguous user queries in medical QA systems, which poses safety risks and reduces answer reliability.

Method: Developed CV-MedBench for testing input ambiguity and introduced AU-Probe, a module that detects ambiguity from LLM activation states, allowing for clarification before answering.

Result: Experiments with four open LLMs show an average accuracy improvement of 9.48% using the proposed framework compared to existing baselines.

Conclusion: The proposed framework provides a practical, robust, and efficient solution to improve safety and accuracy in medical QA applications.

Abstract: The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided "Clarify-Before-Answer" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at https://github.com/yaokunliu/AU-Med.git, and the CV-MedBench dataset is released on Hugging Face at https://huggingface.co/datasets/yaokunl/CV-MedBench.

</details>


### [97] [Meta-Judging with Large Language Models: Concepts, Methods, and Challenges](https://arxiv.org/abs/2601.17312)
*Hugo Silva,Mateus Mendes,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: This paper surveys recent advances in using LLMs as meta-evaluators, highlighting a promising paradigm to enhance automated evaluation stability and trustworthiness, while addressing current challenges.


<details>
  <summary>Details</summary>
Motivation: The limitations of LLMs serving as evaluators, including biases, prompt sensitivity, and unreliable rationales, have motivated the need for a more robust evaluation paradigm.

Method: The paper introduces a systematic framework for meta-judging across six perspectives: conceptual foundations, mechanisms, training methods, evaluation approaches, limitations, and future advancements.

Result: It summarizes the progress and potential of LLM-as-a-Meta-Judge and provides insights into overcoming limitations of current evaluation approaches, aiming for improved reliability and trustworthiness.

Conclusion: LLM-as-a-Meta-Judge represents a promising shift in automated evaluation, though challenges such as cost, sensitivity, and shared biases must still be addressed to fully realize its potential.

Abstract: Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.

</details>


### [98] [The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents](https://arxiv.org/abs/2601.17344)
*Chen Chen,Kim Young Il,Yuan Yang,Wenhao Su,Yilin Zhang,Xueluan Gong,Qian Wang,Yongsen Zheng,Ziyao Liu,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: This paper addresses the issue of intrinsic value misalignment in autonomous large language model agents, proposing a new framework, IMPRESS, to evaluate and mitigate related risks.


<details>
  <summary>Details</summary>
Motivation: Autonomous LLM agents exhibit risks of deviating from human values and ethical norms in benign settings, a problem currently underexplored.

Method: The authors formalized the Loss-of-Control risk and Intrinsic Value Misalignment, developed IMPRESS framework with quality-controlled, realistic scenario benchmarks, and tested 21 LLM agents.

Result: They observed intrinsic value misalignment across models consistently, influenced by framing/contextualization, but showed vulnerability in safety strategies like guardrails.

Conclusion: Intrinsic value misalignment is a prevalent challenge to LLM safety, and IMPRESS provides a systematic tool to assess and mitigate these risks, enabling safer LLM deployments.

Abstract: Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.

</details>


### [99] [Do readers prefer AI-generated Italian short stories?](https://arxiv.org/abs/2601.17363)
*Michael Farrell*

Main category: cs.CL

TL;DR: This study examines if readers prefer AI-generated Italian short stories over those by a renowned human author, finding a slight preference for AI stories, though differences were minor.


<details>
  <summary>Details</summary>
Motivation: To explore whether AI-generated stories can compete with or surpass human-authored fiction in terms of reader preference, and to challenge assumptions about literary tastes.

Method: 20 participants read and rated three stories (two AI-generated and one human-written) in a blind setup. Demographic and reading habit data were also collected for analysis.

Result: AI-generated texts received slightly higher average ratings and were more frequently preferred than the human-authored story. No significant links were found between preferences and demographic or reading habit data.

Conclusion: Readers may slightly favor AI-generated fiction, challenging the need for heavy editing of synthetic texts in literary applications and raising questions about perceptions of human vs. AI authorship.

Abstract: This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were modest. No statistically significant associations were found between text preference and demographic or reading-habit variables. These findings challenge assumptions about reader preference for human-authored fiction and raise questions about the necessity of synthetic-text editing in literary contexts.

</details>


### [100] [Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws](https://arxiv.org/abs/2601.17364)
*Mohammed Fasha,Bassam Hammo,Bilal Sowan,Husam Barham,Esam Nsour*

Main category: cs.CL

TL;DR: The paper explores fine-tuning large language models for Arabic question-answering in legal contexts using Jordanian law as a case study.


<details>
  <summary>Details</summary>
Motivation: To enhance large language models' capabilities in addressing Arabic legal question-answering through domain-specific adaptation.

Method: Two Llama-3.1 models are fine-tuned with parameter-efficient fine-tuning (PEFT), LoRA adapters, and 4-bit quantized models using legal question-answer data.

Result: Improved legal reasoning and accuracy in Arabic legal question-answering tasks, with resource-efficient training methods achieving better performance than base models.

Conclusion: Fine-tuning techniques, including LoRA and quantization, successfully adapt large language models to the Arabic legal domain, showcasing their utility for domain-specific tasks.

Abstract: This study uses Jordanian law as a case study to explore the fine-tuning of the Llama-3.1 large language model for Arabic question-answering. Two versions of the model - Llama-3.1-8B-bnb-4bit and Llama-3.1-8B-Instruct-bnb-4bit - were fine-tuned using parameter-efficient fine-tuning (PEFT) with LoRA adapters and 4-bit quantized models, leveraging the Unsloth framework for accelerated and resource-efficient training. A custom dataset of 6000 legal question-answer pairs was curated from Jordanian laws and formatted into structured prompts. Performance was evaluated using the BLEU and the ROUGE metrics to compare the fine-tuned models to their respective base versions. Results demonstrated improved legal reasoning and accuracy while achieving resource efficiency through quantization and optimized fine-tuning strategies. This work underscores the potential of adapting large language models for Arabic legal domains and highlights effective techniques for fine-tuning domain-specific tasks.

</details>


### [101] [Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings](https://arxiv.org/abs/2601.18788)
*Mumin Jia,Jairo Diaz-Rodriguez*

Main category: cs.CL

TL;DR: Embed-KCPD introduces a training-free text segmentation method, leveraging embeddings and penalized KCPD objectives, supported by theoretical guarantees and strong empirical performance.


<details>
  <summary>Details</summary>
Motivation: Boundary labels for text segmentation are expensive, subjective, and often domain-dependent, creating a need for unsupervised and scalable solutions.

Method: Sentences are represented as embedding vectors. Embed-KCPD identifies boundaries by minimizing penalized KCPD objectives, validated through dependence-aware theory and LLM-based simulations.

Result: The method often outperforms strong unsupervised baselines on standard benchmarks and is validated on real-world data such as Taylor Swift's tweets.

Conclusion: Embed-KCPD is theoretically solid, performs reliably in simulations, and is practical and effective for text segmentation tasks.

Abstract: Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.

</details>


### [102] [Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers](https://arxiv.org/abs/2601.17367)
*Zecheng Tang,Quantong Qiu,Yi Yang,Zhiyi Hong,Haiya Xiang,Kebin Liu,Qingqing Dang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: The paper introduces Elastic Attention, a method for dynamically adjusting the sparsity of attention mechanisms in LLMs to improve scalability in long-context tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the quadratic complexity bottleneck of standard attention mechanisms in long-context scenarios and improve scalability in LLMs.

Method: The method involves integrating an Attention Router into existing pretrained models to dynamically assign attention heads to different computation modes based on input sparsity sensitivity.

Result: Elastic Attention allows models to achieve both strong task performance and efficient inference, demonstrated through experiments on three benchmarks with LLMs.

Conclusion: Elastic Attention successfully addresses scalability challenges in long-context scenarios by dynamically optimizing attention sparsity, proving effective and efficient with modest training resources.

Abstract: The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.

</details>


### [103] [WarrantScore: Modeling Warrants between Claims and Evidence for Substantiation Evaluation in Peer Reviews](https://arxiv.org/abs/2601.17377)
*Kiyotada Mori,Shohei Tanaka,Tosho Hirasawa,Tadashi Kozuno,Koichiro Yoshino,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: The paper proposes an advanced evaluation metric to assess logical inference in scientific reviews, aiming to address resource shortages in peer-review processes.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge posed by the growing demand for peer reviews due to the increase in submitted papers and the lack of sufficient human resources.

Method: The proposed method extracts claims and evidence from scientific reviews and evaluates the logical inference between them to assess substantiation levels.

Result: Experimental results demonstrate the new metric's superior correlation with human scores compared to existing methods.

Conclusion: This approach could enhance the efficiency of the peer-review process, potentially addressing human resource challenges in academia.

Abstract: The scientific peer-review process is facing a shortage of human resources due to the rapid growth in the number of submitted papers. The use of language models to reduce the human cost of peer review has been actively explored as a potential solution to this challenge. A method has been proposed to evaluate the level of substantiation in scientific reviews in a manner that is interpretable by humans. This method extracts the core components of an argument, claims and evidence, and assesses the level of substantiation based on the proportion of claims supported by evidence. The level of substantiation refers to the extent to which claims are based on objective facts. However, when assessing the level of substantiation, simply detecting the presence or absence of supporting evidence for a claim is insufficient; it is also necessary to accurately assess the logical inference between a claim and its evidence. We propose a new evaluation metric for scientific review comments that assesses the logical inference between claims and evidence. Experimental results show that the proposed method achieves a higher correlation with human scores than conventional methods, indicating its potential to better support the efficiency of the peer-review process.

</details>


### [104] [Revisiting Modality Invariance in a Multilingual Speech-Text Model via Neuron-Level Analysis](https://arxiv.org/abs/2601.17387)
*Toshiki Nakai,Varsha Suresh,Vera Demberg*

Main category: cs.CL

TL;DR: The paper probes whether multilingual speech-text foundation models, like SeamlessM4T v2, internally represent spoken and written forms of language consistently, revealing incomplete modality invariance.


<details>
  <summary>Details</summary>
Motivation: To explore whether multilingual speech-text models truly encode language information uniformly across modalities (speech vs. text) and understand how these distinctions manifest internally.

Method: The authors used three approaches: average-precision ranking to identify selective neurons, median-replacement interventions during inference to study their causal functions, and analysis of activation-magnitude inequality to examine encoding across languages and modalities.

Result: Findings indicate incomplete modality invariance; while encoder representations move towards language-agnosticism, they hinder decoding of original languages, particularly in adapting speech to text. Sharp modality-selective patterns and higher neural activation concentration across certain conditions were observed.

Conclusion: The study highlights challenges in achieving true modality invariance in multilingual foundation models, with insights into inefficiencies and potential weaknesses arising from localized or heavily concentrated neural encoding patterns.

Abstract: Multilingual speech-text foundation models aim to process language uniformly across both modality and language, yet it remains unclear whether they internally represent the same language consistently when it is spoken versus written. We investigate this question in SeamlessM4T v2 through three complementary analyses that probe where language and modality information is encoded, how selective neurons causally influence decoding, and how concentrated this influence is across the network. We identify language- and modality-selective neurons using average-precision ranking, investigate their functional role via median-replacement interventions at inference time, and analyze activation-magnitude inequality across languages and modalities. Across experiments, we find evidence of incomplete modality invariance. Although encoder representations become increasingly language-agnostic, this compression makes it more difficult for the shared decoder to recover the language of origin when constructing modality-agnostic representations, particularly when adapting from speech to text. We further observe sharply localized modality-selective structure in cross-attention key and value projections. Finally, speech-conditioned decoding and non-dominant scripts exhibit higher activation concentration, indicating heavier reliance on a small subset of neurons, which may underlie increased brittleness across modalities and languages.

</details>


### [105] [CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing](https://arxiv.org/abs/2601.17397)
*Yucheng Hu,Wei Zhou,Juesi Xiao*

Main category: cs.CL

TL;DR: The paper introduces CLM-Bench, a culturally aware benchmark for multilingual knowledge editing in LLMs. It highlights the shortcomings of translation-based benchmarks and demonstrates significant cross-lingual misalignments in current LLMs.


<details>
  <summary>Details</summary>
Motivation: To address biased evaluation frameworks in Multilingual Knowledge Editing (MKE) caused by mechanically translated benchmarks that fail to account for cultural context and knowledge distribution.

Method: The authors propose CLM-Bench, a benchmark built using a native Chinese-first approach, curating culturally aligned CounterFact pairs and conducting experiments on LLMs like Llama-3 and Qwen2. They analyze cross-lingual misalignment through geometric representation studies.

Result: The study reveals that cross-lingual misalignment exists between edits in Chinese and English, with edit vectors being nearly orthogonal. Mixed-lingual editing shows linear additivity; however, current methods fail in effective cross-lingual transfer.

Conclusion: Culturally native benchmarks like CLM-Bench are essential for meaningful MKE evaluations, as the paper challenges existing methods' assumptions about cross-lingual transfer.

Abstract: Knowledge Editing (KE) has emerged as a promising paradigm for updating facts in Large Language Models (LLMs) without retraining. However, progress in Multilingual Knowledge Editing (MKE) is currently hindered by biased evaluation frameworks. We observe that existing MKE benchmarks are typically constructed by mechanically translating English-centric datasets into target languages (e.g., English-to-Chinese). This approach introduces translation artifacts and neglects culturally specific entities native to the target language, failing to reflect the true knowledge distribution of LLMs. To address this, we propose CLM-Bench, a culture-aware benchmark constructed using a native Chinese-first methodology. We curate 1,010 high-quality CounterFact pairs rooted in Chinese cultural contexts and align them with English counterparts. Using CLM-Bench, we conduct extensive experiments on representative LLMs (e.g., Llama-3, Qwen2) and reveal a significant Cross-lingual Misalignment: edits in one language function independently and fail to propagate to the other. We further provide a geometric explanation via layer-wise representation analysis, demonstrating that edit vectors for Chinese and English are nearly orthogonal -- residing in disjoint subspaces -- while mixed-lingual editing exhibits linear additivity of these vectors. Our findings challenge the effectiveness of current methods in cross-lingual transfer and underscore the importance of culturally native benchmarks.

</details>


### [106] [Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning](https://arxiv.org/abs/2601.17421)
*Jaehui Hwang,Dongyoon Han,Sangdoo Yun,Byeongho Heo*

Main category: cs.CL

TL;DR: The paper examines how certain discourse-like tokens, such as "wait" and "therefore," in large language models (LLMs) correlate with reasoning abilities across different training strategies and model scales.


<details>
  <summary>Details</summary>
Motivation: To understand how tokens like "wait" and "therefore" reflect reasoning processes and how their usage varies across different training and model scales.

Method: Analyzing token-level probabilities in correlation with reasoning correctness across various training strategies and scales of LLMs.

Result: Specific tokens correlate strongly with reasoning correctness; their role fluctuates with training strategies but remains consistent across model sizes. Models fine-tuned on small datasets acquire reasoning signals but utilize them only partially.

Conclusion: Token-level signals provide a systematic method to understand reasoning dynamics in LLMs, shedding light on their reasoning processes.

Abstract: The emergence of discourse-like tokens such as "wait" and "therefore" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the "wait" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.

</details>


### [107] [Clustering-driven Memory Compression for On-device Large Language Models](https://arxiv.org/abs/2601.17443)
*Ondrej Bohdal,Pramit Saha,Umberto Michieli,Mete Ozay,Taha Ceritli*

Main category: cs.CL

TL;DR: This paper proposes a clustering-based method to compress user-specific memories in LLMs, helping achieve better performance and compact memory representations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of exhausting context limits in on-device LLMs for personalized generation, caused by concatenating user-specific memories.

Method: The authors introduce a clustering-based strategy where memories are grouped by similarity and merged within clusters to preserve coherence and reduce redundancy.

Result: Experiments show reduced memory tokens and improved generation quality compared to averaging and direct concatenation methods.

Conclusion: The clustering-driven compression balances context efficiency and personalization quality, leading to compact representations and enhanced performance within limited context budgets.

Abstract: Large language models (LLMs) often rely on user-specific memories distilled from past interactions to enable personalized generation. A common practice is to concatenate these memories with the input prompt, but this approach quickly exhausts the limited context available in on-device LLMs. Compressing memories by averaging can mitigate context growth, yet it frequently harms performance due to semantic conflicts across heterogeneous memories. In this work, we introduce a clustering-based memory compression strategy that balances context efficiency and personalization quality. Our method groups memories by similarity and merges them within clusters prior to concatenation, thereby preserving coherence while reducing redundancy. Experiments demonstrate that our approach substantially lowers the number of memory tokens while outperforming baseline strategies such as naive averaging or direct concatenation. Furthermore, for a fixed context budget, clustering-driven merging yields more compact memory representations and consistently enhances generation quality.

</details>


### [108] [Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes](https://arxiv.org/abs/2601.17530)
*Gautam Siddharth Kashyap,Harsh Joshi,Niharika Jain,Ebad Shabbir,Jiechao Gao,Nipun Joshi,Usman Naseem*

Main category: cs.CL

TL;DR: The paper introduces ConLLM, a hybrid framework for detecting multimodal deepfakes, addressing challenges like modality fragmentation and shallow inter-modal reasoning.


<details>
  <summary>Details</summary>
Motivation: Deepfake technology threatens social and political stability, and current detection methods are limited in handling diverse modalities and semantic inconsistencies.

Method: ConLLM uses a two-stage architecture with Pre-Trained Models (PTMs) for embedding extractions, contrastive learning to align these embeddings, and LLM-based reasoning for detecting fine-grained semantic inconsistencies.

Result: ConLLM significantly enhances performance across audio, video, and audio-visual modalities with notable improvements in metrics, such as reducing audio deepfake EER by 50%, improving video accuracy by 8%, and increasing audio-visual task accuracy by 9%.

Conclusion: ConLLM demonstrates robust effectiveness in multimodal deepfake detection with consistent improvements across various modalities, addressing core limitations in previous methods.

Abstract: The rapid rise of deepfake technology poses a severe threat to social and political stability by enabling hyper-realistic synthetic media capable of manipulating public perception. However, existing detection methods struggle with two core limitations: (1) modality fragmentation, which leads to poor generalization across diverse and adversarial deepfake modalities; and (2) shallow inter-modal reasoning, resulting in limited detection of fine-grained semantic inconsistencies. To address these, we propose ConLLM (Contrastive Learning with Large Language Models), a hybrid framework for robust multimodal deepfake detection. ConLLM employs a two-stage architecture: stage 1 uses Pre-Trained Models (PTMs) to extract modality-specific embeddings; stage 2 aligns these embeddings via contrastive learning to mitigate modality fragmentation, and refines them using LLM-based reasoning to address shallow inter-modal reasoning by capturing semantic inconsistencies. ConLLM demonstrates strong performance across audio, video, and audio-visual modalities. It reduces audio deepfake EER by up to 50%, improves video accuracy by up to 8%, and achieves approximately 9% accuracy gains in audio-visual tasks. Ablation studies confirm that PTM-based embeddings contribute 9%-10% consistent improvements across modalities.

</details>


### [109] [Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection](https://arxiv.org/abs/2601.17532)
*Zhipeng Song,Yizhi Zhou,Xiangyu Kong,Jiulong Jiao,Xinrui Bao,Xu You,Xueqing Shi,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: The paper introduces "Information Gain Pruning (IGP)", which optimizes the selection of external evidence for retrieval-augmented generation, improving cost-efficiency and quality of answers.


<details>
  <summary>Details</summary>
Motivation: Current methods of retrieval-augmented generation struggle to effectively select the most relevant retrieved passages for injecting in limited context scenarios. Traditional retrieval metrics poorly correlate with QA quality and may hinder performance with multi-passage injection.

Method: The authors propose Information Gain Pruning (IGP), a module that reranks and prunes retrieved evidence using a generator-aligned utility signal, eliminating weak or harmful passages without altering the budget interface.

Result: IGP significantly improves QA performance across five benchmarks with various retrievers and generators, boosting F1 scores by 12–20% while reducing input token usage by 76–79% compared to baseline methods.

Conclusion: The proposed IGP method enhances retrieval-augmented QA systems by simultaneously increasing answer quality and reducing computational costs, thus demonstrating its effectiveness and efficiency.

Abstract: Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.

</details>


### [110] [Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations](https://arxiv.org/abs/2601.17569)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: This paper introduces $P^3$, a framework that achieves personalization for large language models (LLMs) without exposing private data, significantly improving performance and privacy efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the trade-off between exposing private data to server-side LLMs for personalization and relying on less capable local models, aiming for high-quality personalization with improved privacy.

Method: A large server-side model generates partial drafts based on the user query, and a small client-side model (with access to private profiles) modifies these drafts iteratively until completion, ensuring personalization without full profile exposure.

Result: $P^3$ outperforms personalized and non-personalized baselines by a margin of 7.4% to 9%, recovers up to 95.7% of the utility when private profiles are exposed to server-side models, and adds minimal privacy leakage compared to fully non-personalized setups.

Conclusion: The $P^3$ framework offers a practical solution for personalized generation, achieving consistent improvements in performance and maintaining robust privacy while being efficient for edge deployment.

Abstract: Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.

</details>


### [111] [Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models](https://arxiv.org/abs/2601.17585)
*Matija Luka Kukić,Marko Čuljak,David Dukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: This paper explores sequence repetition (SR) as an alternative method to enable bidirectional context in decoder-only language models for sequence labeling tasks, showing improvements in effectiveness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Decoder-only models are inherently autoregressive and lack bidirectional context, which limits their effectiveness in sequence labeling tasks that demand such context.

Method: The authors propose and evaluate sequence repetition (SR) as a technique to enhance bidirectional capabilities in decoder-only models without removing causal masking.

Result: Fine-tuning experiments show that SR improves token-level embedding, surpassing encoder models and unmasked decoders. Increasing repetitions further supports performance, and intermediate-layer embeddings prove efficient and effective.

Conclusion: SR mitigates decoder-only model limitations, making them more efficient and adaptable for token-level tasks, broadening their usability beyond autoregressive applications.

Abstract: Modern language models (LMs) are trained in an autoregressive manner, conditioned only on the prefix. In contrast, sequence labeling (SL) tasks assign labels to each individual input token, naturally benefiting from bidirectional context. This discrepancy has historically led SL to rely on inherently bidirectional encoder-only models. However, the rapid development of decoder-only models has raised the question of whether they can be adapted to SL. While causal mask removal has emerged as a viable technique for adapting decoder-only models to leverage the full context for SL, it requires considerable changes to the base model functionality. In this work, we explore sequence repetition (SR) as a less invasive alternative for enabling bidirectionality in decoder-only models. Through fine-tuning experiments, we show that SR inherently makes decoders bidirectional, improving the quality of token-level embeddings and surpassing encoders and unmasked decoders. Contrary to earlier claims, we find that increasing the number of repetitions does not degrade SL performance. Finally, we demonstrate that embeddings from intermediate layers are highly effective for SR, comparable to those from final layers, while being significantly more efficient to compute. Our findings underscore that SR alleviates the structural limitations of decoders, enabling more efficient and adaptable LMs and broadening their applicability to other token-level tasks.

</details>


### [112] [From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs](https://arxiv.org/abs/2601.17593)
*Tianjun Zhong,Linyang He,Nima Mesgarani*

Main category: cs.CL

TL;DR: The paper introduces Reasoning DAG Probing, a framework to investigate whether graph-structured reasoning is encoded in the hidden states of Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To explore whether multi-step reasoning in LLMs, traditionally viewed as linear, actually maps onto a more complex graph structure, such as directed acyclic graphs (DAGs), where intermediate conclusions involve multiple dependencies and reuse.

Method: The authors train lightweight probes to analyze hidden states in LLMs and to predict graph-theoretic properties like node depth and pairwise node distance. They evaluate how reasoning DAG structures emerge throughout model layers and through controlled tests.

Result: The study finds that reasoning DAG geometry is encoded in intermediate layers of LLMs, with recoverability influenced by node depth and model scale.

Conclusion: LLMs exhibit measurable internal graph structures during reasoning, going beyond a sequential process and reflecting meaningful representations of graph-structured reasoning tasks.

Abstract: Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question.
  In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.

</details>


### [113] [Learning to Ideate for Machine Learning Engineering Agents](https://arxiv.org/abs/2601.17596)
*Yunxiang Zhang,Kang Zhou,Zhichao Xu,Kiran Ramnath,Yun Zhou,Sangmin Woo,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: The paper introduces MLE-Ideator, a framework separating ideation and implementation for improving machine learning engineering efficiency, showing significant performance boosts in training-free and reinforcement learning setups.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning engineering agents struggle to iteratively optimize algorithms effectively, necessitating new approaches for strategic ideation and implementation.

Method: The method involves a dual-agent framework where implementation and ideation are separated. An implementation agent can seek strategic help from an Ideator, which can be further trained using reinforcement learning.

Result: The framework outperformed implementation-only agents on MLE-Bench under a training-free setup and showed an 11.5% improvement with a RL-trained Ideator, surpassing Claude Sonnet 3.5.

Conclusion: MLE-Ideator demonstrates the potential of separating ideation and implementation to strategically train AI systems for scientific discovery.

Abstract: Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a training-free setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery.

</details>


### [114] [What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization](https://arxiv.org/abs/2601.17609)
*Sara Rezaeimanesh,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: This paper introduces LoID (Logit-Informed Distributions), a deterministic method leveraging large language models (LLMs) to extract robust priors for Bayesian logistic regression, significantly improving model generalization on out-of-distribution tabular datasets under covariate shift.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenges of training models in important domains like medicine and finance, where labeled data is limited and models often fail to generalize to unseen real-world data. By tapping into the extensive knowledge embedded in LLMs, the authors aim to enhance model performance in such scenarios.

Method: LoID probes LLMs by constructing sentences that assess confidence in opposing semantic directions (e.g., positive vs. negative impact), using this information to extract consistent, reliable priors based on the strength of the LLM’s beliefs. These priors improve Bayesian logistic regression under covariate shift.

Result: LoID demonstrated improved generalization on 10 real-world datasets under synthetic out-of-distribution conditions. It closed up to 59% of the performance gap relative to an oracle model and outperformed alternative approaches such as AutoElicit and LLMProcesses on 8 out of 10 datasets.

Conclusion: LoID provides a computationally efficient and reproducible method to integrate LLM knowledge into Bayesian inference, significantly enhancing performance in domains with small or restricted datasets affected by covariate shifts.

Abstract: In domains like medicine and finance, large-scale labeled data is costly and often unavailable, leading to models trained on small datasets that struggle to generalize to real-world populations. Large language models contain extensive knowledge from years of research across these domains. We propose LoID (Logit-Informed Distributions), a deterministic method for extracting informative prior distributions for Bayesian logistic regression by directly accessing their token-level predictions. Rather than relying on generated text, we probe the model's confidence in opposing semantic directions (positive vs. negative impact) through carefully constructed sentences. By measuring how consistently the LLM favors one direction across diverse phrasings, we extract the strength and reliability of the model's belief about each feature's influence. We evaluate LoID on ten real-world tabular datasets under synthetic out-of-distribution (OOD) settings characterized by covariate shift, where the training data represents only a subset of the population. We compare our approach against (1) standard uninformative priors, (2) AutoElicit, a recent method that prompts LLMs to generate priors via text completions, (3) LLMProcesses, a method that uses LLMs to generate numerical predictions through in-context learning and (4) an oracle-style upper bound derived from fitting logistic regression on the full dataset. We assess performance using Area Under the Curve (AUC). Across datasets, LoID significantly improves performance over logistic regression trained on OOD data, recovering up to \textbf{59\%} of the performance gap relative to the oracle model. LoID outperforms AutoElicit and LLMProcessesc on 8 out of 10 datasets, while providing a reproducible and computationally efficient mechanism for integrating LLM knowledge into Bayesian inference.

</details>


### [115] [Beyond the Rabbit Hole: Mapping the Relational Harms of QAnon Radicalization](https://arxiv.org/abs/2601.17658)
*Bich Ngoc,Doan,Giuseppe Russo,Gianmarco De Francisci Morales,Robert West*

Main category: cs.CL

TL;DR: This paper investigates the personal and emotional toll on families and friends of QAnon conspiracy believers by analyzing narratives from the r/QAnonCasualties community.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked personal and emotional toll on loved ones caused by conspiracy radicalization, which is underrepresented in computational research.

Method: The authors use a mixed-methods approach, employing BERTopic for topic modeling to track radicalization paths, LDA-based graphical modeling to identify six 'radicalization personas,' and LLM-assisted emotion detection with regression modeling to connect personas to specific emotional impacts.

Result: Six archetypal radicalization personas are uncovered, alongside their emotional toll on narrators. These personas help predict emotional harms like anger, disgust, fear, and sadness among those close to QAnon adherents.

Conclusion: The paper provides a framework for understanding the interpersonal and relational impacts of radicalization, aiding researchers and practitioners in addressing its emotional toll.

Abstract: The rise of conspiracy theories has created far-reaching societal harm in the public discourse by eroding trust and fueling polarization. Beyond this public impact lies a deeply personal toll on the friends and families of conspiracy believers, a dimension often overlooked in large-scale computational research. This study fills this gap by systematically mapping radicalization journeys and quantifying the associated emotional toll inflicted on loved ones. We use the prominent case of QAnon as a case study, analyzing 12747 narratives from the r/QAnonCasualties support community through a novel mixed-methods approach. First, we use topic modeling (BERTopic) to map the radicalization trajectories, identifying key pre-existing conditions, triggers, and post-radicalization characteristics. From this, we apply an LDA-based graphical model to uncover six recurring archetypes of QAnon adherents, which we term "radicalization personas." Finally, using LLM-assisted emotion detection and regression modeling, we link these personas to the specific emotional toll reported by narrators. Our findings reveal that these personas are not just descriptive; they are powerful predictors of the specific emotional harms experienced by narrators. Radicalization perceived as a deliberate ideological choice is associated with narrator anger and disgust, while those marked by personal and cognitive collapse are linked to fear and sadness. This work provides the first empirical framework for understanding radicalization as a relational phenomenon, offering a vital roadmap for researchers and practitioners to navigate its interpersonal fallout.

</details>


### [116] [UrduLM: A Resource-Efficient Monolingual Urdu Language Model](https://arxiv.org/abs/2601.17664)
*Syed Muhammad Ali,Hammad Sajid,Zainab Haider,Ali Muhammad Asad,Haya Fatima,Abdul Samad*

Main category: cs.CL

TL;DR: The paper introduces UrduLM, a pretrained monolingual Urdu language model addressing limitations in multilingual models for Urdu NLP, achieving efficient tokenization and competitive few-shot performance.


<details>
  <summary>Details</summary>
Motivation: There is a lack of dedicated transformer-based language models and curated corpora for the Urdu language, causing issues such as poor performance and cultural inaccuracies in multilingual models.

Method: The authors curated a 33GB Urdu corpus, developed a custom BPE tokenizer, and pretrained a 100M-parameter decoder-only model, optimizing for low-resource settings.

Result: UrduLM demonstrates competitive performance in few-shot evaluations, achieving 66.6% sentiment classification accuracy and BLEU scores above 30 in grammar correction tasks.

Conclusion: UrduLM sets a baseline for Urdu NLP research with openly released resources, providing a scalable framework for other underrepresented languages.

Abstract: Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.

</details>


### [117] [Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning](https://arxiv.org/abs/2601.17671)
*Chunxu Zhao,Xin Huang,Xue Han,Shujian Huang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: This paper introduces PASMR, a method to enhance LLMs' multilingual math reasoning by aligning reasoning abilities using a pivot language and self-feedback mechanism.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) show reasoning decline in multilingual settings, especially for low-resource languages, due to inconsistent reasoning alignment across languages.

Method: PASMR uses a pivot language for reasoning alignment by translating questions into the pivot language and supervising its reasoning process in the target language via self-feedback from pivot language reasoning answers.

Result: Experimental results demonstrate improved question understanding and reasoning capabilities across languages, leading to better task performance.

Conclusion: The PASMR method successfully enhances multilingual reasoning abilities in LLMs without relying on external correct answers or reward models.

Abstract: Despite the impressive reasoning abilities demonstrated by large language models (LLMs), empirical evidence indicates that they are not language agnostic as expected, leading to performance declines in multilingual settings, especially for low-resource languages. We attribute the decline to the model's inconsistent multilingual understanding and reasoning alignment. To address this, we present Pivot-Aligned Self-Feedback Multilingual Reasoning (PASMR), aiming to improve the alignment of multilingual math reasoning abilities in LLMs. This approach designates the model's primary language as the pivot language. During training, the model first translates questions into the pivot language to facilitate better alignment of reasoning patterns. The reasoning process in the target language is then supervised by the pivot language's reasoning answers, thereby establishing a cross-lingual self-feedback mechanism without relying on external correct answers or reward models. Extensive experimental results demonstrate that our method enhances both the model's understanding of questions and its reasoning capabilities, leading to notable task improvements.

</details>


### [118] [S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference](https://arxiv.org/abs/2601.17702)
*Qingsen Ma,Dianyun Wang,Yaoye Wang,Lechen Ning,Sujie Zhu,Xiaohang Zhang,Jiaming Lyu,Linhao Ren,Zhenbo Xu,Zhaofeng He*

Main category: cs.CL

TL;DR: This paper introduces S3-Attention, a framework to improve long-context language model inference focusing on reducing memory and noise inefficiency.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inefficiencies in memory handling and noise during long-context inference in large language models, particularly with key-value caching and external retrieval methods.

Method: The proposed S3-Attention leverages attention-aligned endogenous retrieval using sparse autoencoders to create top-k sparse features and an inverted index on tokens during streaming scans, eliminating the need for full key-value caching and optimizing memory usage.

Result: S3-Hybrid implementation shows comparable performance to full-context inference on LongBench evaluation while enhancing robustness in information-heavy scenarios.

Conclusion: S3-Hybrid presents a promising alternative for memory-efficient long-context inference but needs further optimization to reduce latency compared to conventional methods.

Abstract: Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.
  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.
  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.

</details>


### [119] [Distance-to-Distance Ratio: A Similarity Measure for Sentences Based on Rate of Change in LLM Embeddings](https://arxiv.org/abs/2601.17705)
*Abdullah Qureshi,Kenneth Rice,Alexander Wolpert*

Main category: cs.CL

TL;DR: The paper introduces Distance-to-Distance Ratio (DDR), a new similarity measure for LLM embeddings that aligns with human text similarity perception.


<details>
  <summary>Details</summary>
Motivation: Existing similarity measures for text embeddings may not adequately reflect human judgments of semantic similarity, motivating the need for a more perceptually aligned approach.

Method: DDR is inspired by Lipschitz continuity and evaluates semantic context influence by measuring changes in similarity between embeddings before and after context application. A series of perturbations (word substitutions) are used in experiments to test its efficacy.

Result: DDR consistently outperforms other similarity metrics, providing better discrimination between semantically similar and dissimilar texts, even under minimal controlled edits.

Conclusion: DDR provides a more nuanced and human-perception-aligned assessment of text similarity than traditional metrics, showcasing its potential as a superior tool for evaluating sentence embeddings.

Abstract: A measure of similarity between text embeddings can be considered adequate only if it adheres to the human perception of similarity between texts. In this paper, we introduce the distance-to-distance ratio (DDR), a novel measure of similarity between LLM sentence embeddings. Inspired by Lipschitz continuity, DDR measures the rate of change in similarity between the pre-context word embeddings and the similarity between post-context LLM embeddings, thus measuring the semantic influence of context. We evaluate the performance of DDR in experiments designed as a series of perturbations applied to sentences drawn from a sentence dataset. For each sentence, we generate variants by replacing one, two, or three words with either synonyms, which constitute semantically similar text, or randomly chosen words, which constitute semantically dissimilar text. We compare the performance of DDR with other prevailing similarity metrics and demonstrate that DDR consistently provides finer discrimination between semantically similar and dissimilar texts, even under minimal, controlled edits.

</details>


### [120] [A Computational Approach to Visual Metonymy](https://arxiv.org/abs/2601.17706)
*Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.CL

TL;DR: The paper introduces a computational pipeline to study visual metonymy and creates the ViMET dataset for evaluating multimodal language models. Findings indicate current models struggle with indirect visual references.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of research and computational approaches to studying visual metonymy, which involves interpreting indirect visual references in images.

Method: The researchers developed a pipeline using semiotic theory combined with large language models and text-to-image models to generate visual metonymy data. They also created a dataset of multiple-choice questions to evaluate reasoning abilities in multimodal models.

Result: The study found that humans vastly outperform state-of-the-art vision-language models in interpreting indirect visual references, scoring 86.9% compared to models' 65.9%.

Conclusion: The findings expose significant shortcomings in current vision-language models' cognitive reasoning capabilities regarding visual metonymy, driving the need for advancements in this area.

Abstract: Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET.

</details>


### [121] [Unsupervised Elicitation of Moral Values from Language Models](https://arxiv.org/abs/2601.17728)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei*

Main category: cs.CL

TL;DR: This paper investigates the intrinsic moral reasoning capabilities of pre-trained language models (LMs) through an unsupervised method called Internal Coherence Maximization (ICM). ICM outperforms existing baselines in moral judgments and reduces social biases, suggesting a scalable approach for AI alignment without needing ground-truth moral data.


<details>
  <summary>Details</summary>
Motivation: To align AI systems with human values, the study explores whether pretrained LMs have latent moral reasoning that can be accessed without human supervision, addressing the complexities of constructing moral ground truth data.

Method: The study employs the Internal Coherence Maximization (ICM) algorithm on pretrained LMs to evaluate their ability to make moral judgments, generalize across moral frameworks, and reduce social bias using benchmark datasets.

Result: ICM surpassed both pretrained and chatbot baseline performances on benchmarks, reduced social bias errors significantly, and showed considerable gains in Justice and Commonsense moral frameworks.

Conclusion: Pretrained LMs have latent moral reasoning skills that can be surfaced using unsupervised methods like ICM, highlighting a promising approach for scalable AI alignment with human values.

Abstract: As AI systems become pervasive, grounding their behavior in human values is critical. Prior work suggests that language models (LMs) exhibit limited inherent moral reasoning, leading to calls for explicit moral teaching. However, constructing ground truth data for moral evaluation is difficult given plural frameworks and pervasive biases. We investigate unsupervised elicitation as an alternative, asking whether pretrained (base) LMs possess intrinsic moral reasoning capability that can be surfaced without human supervision. Using the Internal Coherence Maximization (ICM) algorithm across three benchmark datasets and four LMs, we test whether ICM can reliably label moral judgments, generalize across moral frameworks, and mitigate social bias. Results show that ICM outperforms all pre-trained and chatbot baselines on the Norm Bank and ETHICS benchmarks, while fine-tuning on ICM labels performs on par with or surpasses those of human labels. Across theoretically motivated moral frameworks, ICM yields its largest relative gains on Justice and Commonsense morality. Furthermore, although chatbot LMs exhibit social bias failure rates comparable to their pretrained ones, ICM reduces such errors by more than half, with the largest improvements in race, socioeconomic status, and politics. These findings suggest that pretrained LMs possess latent moral reasoning capacities that can be elicited through unsupervised methods like ICM, providing a scalable path for AI alignment.

</details>


### [122] [Hylog: A Hybrid Approach to Logging Text Production in Non-alphabetic Scripts](https://arxiv.org/abs/2601.17753)
*Roberto Crotti,Giovanni Denaro,Zhiqiang Du,Ricardo Muñoz Martín*

Main category: cs.CL

TL;DR: This paper introduces Hylog, a hybrid logging system designed to capture both keylogging data and rendered text to improve cognitive research on non-alphabetic scripts.


<details>
  <summary>Details</summary>
Motivation: There’s a methodological gap in text production research, as traditional keyloggers fail to capture transformations done by Input Method Editors (IMEs), crucial for non-alphabetic scripts.

Method: Hylog integrates analytical keylogging with ecological text logging using modular plug-ins and a hybridizer module to synchronize keyboard output with on-screen text.

Result: In a proof-of-concept study, Hylog captured data from simplified Chinese typing, including keypress intervals and other measurements that traditional tools miss.

Conclusion: Hylog not only validates its feasibility but also presents opportunities to broaden cognitive research on multilingual text production and integrate with other IME systems.

Abstract: Research keyloggers are essential for cognitive studies of text production, yet most fail to capture the on-screen transformations performed by Input Method Editors (IMEs) for non-alphabetic scripts. To address this methodological gap, we present Hylog, a novel hybrid logging system that combines analytical keylogging with ecological text logging for a more complete and finer-grained analysis. Our modular, open-source system uses plug-ins for standard applications (Microsoft Word, Google Chrome) to capture both keyboard output and rendered text, which a hybridizer module then synchronizes into a dual trace. To validate the system's technical feasibility and demonstrate its analytical capabilities, we conducted a proof-of-concept study where two volunteers translated a text into simplified Chinese. Hylog successfully captured keypresses and temporal intervals between Latin letters, Chinese characters, and IME confirmations -- some measurements invisible to traditional keyloggers. The resulting data enable the formulation of new, testable hypotheses about the cognitive restrictions and affordances at different linguistic layers in IME-mediated typing. Our plug-in architecture enables extension to other IME systems and fosters more inclusive multilingual text-production research.

</details>


### [123] [ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation](https://arxiv.org/abs/2601.17755)
*Jinyoung Park,Sanghyeok Lee,Omar Zia Khan,Hyunwoo J. Kim,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: ProGraph-R1 improves reasoning and quality in multi-hop question answering by addressing limitations in existing RL-based GraphRAG methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of RL-based GraphRAG frameworks that neglect graph structure in retrieval and rely on sparse rewards.

Method: ProGraph-R1 uses a structure-aware hypergraph retrieval mechanism and progress-based step-wise policy optimization to improve graph traversal.

Result: Experiments show that ProGraph-R1 enhances reasoning accuracy and generation quality on multi-hop QA benchmarks.

Conclusion: ProGraph-R1 demonstrates the effectiveness of integrating graph structure and dense learning signals for better reasoning in GraphRAG systems.

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.

</details>


### [124] [Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali](https://arxiv.org/abs/2601.17764)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Jui Saha Pritha,Abdullah Al Noman,Abir Ahmed,Golam Md Mohiuddin,Tze Hui Liew*

Main category: cs.CL

TL;DR: The paper analyzes gender bias in Bengali, revealing significant deficiencies in English-centric bias frameworks and emphasizing the need for localized, context-sensitive NLP solutions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on gender bias in underrepresented languages, particularly Bengali, and evaluate current gender bias detection methods in such contexts.

Method: The study employed techniques like lexicon-based mining, computational classification models, translation comparison, GPT-based bias creation, and field investigations involving rural and low-income Bengali-speaking areas.

Result: The findings demonstrate that English-centric methods inadequately address Bengali-specific gender bias due to linguistic and socio-cultural disparities. The field studies provided culturally relevant insights on bias.

Conclusion: The research highlights the need for context-sensitive and community-driven methodologies to mitigate gender bias in Bengali and other underrepresented languages, laying groundwork for inclusive NLP systems.

Abstract: Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.

</details>


### [125] [DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning](https://arxiv.org/abs/2601.17777)
*Xiaoyu Liu,Xiaoyu Guan,Di Liang,Xianjie Wu*

Main category: cs.CL

TL;DR: This paper addresses the problem of conflicting objectives in supervised fine-tuning (SFT) for LLMs by proposing a parameter isolation strategy, improving cross-task performance.


<details>
  <summary>Details</summary>
Motivation: SFT on LLMs suffers from the 'seesaw effect,' where optimizing for one task impacts others negatively due to parameter conflicts across diverse tasks.

Method: The authors propose independently fine-tuning LLMs to identify core parameter regions, merge overlapping tasks, organize disjoint tasks in stages, and freeze core parameters to prevent interference.

Result: Experiments demonstrate that the parameter isolation method reduces task conflicts and achieves consistent performance improvements over existing multi-stage and multi-task tuning approaches.

Conclusion: Disentangling task-specific parameter regions is an effective strategy for mitigating cross-task interference and improving SFT on LLMs.

Abstract: Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the "seesaw effect": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.

</details>


### [126] [Controlling Reading Ease with Gaze-Guided Text Generation](https://arxiv.org/abs/2601.17781)
*Andreas Säuberli,Darja Jepifanova,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: The paper proposes using human gaze prediction models to guide language model outputs for generating texts with controllable reading ease.


<details>
  <summary>Details</summary>
Motivation: To improve text accessibility and adapt texts for personalized purposes such as language learning or cognitive accessibility, leveraging eye movement data for determining reading ease is explored.

Method: The method uses a model predicting human gaze patterns to guide language model text generation, aiming for desired reading behaviors. This was tested through an eye-tracking experiment with native and non-native English speakers.

Result: The generated texts successfully achieved controllable readability levels, as validated through reading times, perceived difficulty ratings, and statistical analysis revealing lexical processing as the key factor.

Conclusion: The method effectively manipulates text readability and holds potential for applications like simplifying content or creating individualized language education materials.

Abstract: The way our eyes move while reading can tell us about the cognitive effort required to process the text. In the present study, we use this fact to generate texts with controllable reading ease. Our method employs a model that predicts human gaze patterns to steer language model outputs towards eliciting certain reading behaviors. We evaluate the approach in an eye-tracking experiment with native and non-native speakers of English. The results demonstrate that the method is effective at making the generated texts easier or harder to read, measured both in terms of reading times and perceived difficulty of the texts. A statistical analysis reveals that the changes in reading behavior are mostly due to features that affect lexical processing. Possible applications of our approach include text simplification for information accessibility and generation of personalized educational material for language learning.

</details>


### [127] [Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations](https://arxiv.org/abs/2601.17786)
*Yixin Liu,Kehan Yan,Shiyuan Li,Qingfeng Chen,Shirui Pan*

Main category: cs.CL

TL;DR: The paper introduces $MCA^2$, a multi-view framework for text anomaly detection (TAD) leveraging multiple pretrained language models for improved effectiveness and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current TAD methods rely heavily on single embedding models which limit adaptability across datasets and anomaly types.

Method: $MCA^2$ combines embeddings from multiple pretrained language models, using multi-view reconstruction, a contrastive collaboration module for view interaction, and an adaptive allocation module for contribution weight assignment.

Result: The proposed method demonstrated strong effectiveness over 10 benchmark datasets compared to other approaches.

Conclusion: $MCA^2$ enhances TAD by utilizing multi-view perspectives and adaptive mechanisms, proving advantageous across diverse datasets and anomalies.

Abstract: Text anomaly detection (TAD) plays a critical role in various language-driven real-world applications, including harmful content moderation, phishing detection, and spam review filtering. While two-step "embedding-detector" TAD methods have shown state-of-the-art performance, their effectiveness is often limited by the use of a single embedding model and the lack of adaptability across diverse datasets and anomaly types. To address these limitations, we propose to exploit the embeddings from multiple pretrained language models and integrate them into $MCA^2$, a multi-view TAD framework. $MCA^2$ adopts a multi-view reconstruction model to effectively extract normal textual patterns from multiple embedding perspectives. To exploit inter-view complementarity, a contrastive collaboration module is designed to leverage and strengthen the interactions across different views. Moreover, an adaptive allocation module is developed to automatically assign the contribution weight of each view, thereby improving the adaptability to diverse datasets. Extensive experiments on 10 benchmark datasets verify the effectiveness of $MCA^2$ against strong baselines. The source code of $MCA^2$ is available at https://github.com/yankehan/MCA2.

</details>


### [128] [DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation](https://arxiv.org/abs/2601.17823)
*Pranav Kasela,Marco Braga,Alessandro Ghiotto,Andrea Pilzer,Marco Viviani,Alessandro Raganato*

Main category: cs.CL

TL;DR: DIETA is a decoder-only Transformer model with 0.5 billion parameters for Italian-English machine translation, trained on a large parallel corpus. It performs competitively and resources are publicly released.


<details>
  <summary>Details</summary>
Motivation: To create a specialized and efficient Italian-English machine translation model that performs well on diverse domains while offering publicly available resources for further research.

Method: Collected and curated a parallel corpus of 207 million sentence pairs and 352 million back-translated data. Developed and evaluated the DIETA model, designed as a 0.5B parameter decoder-only Transformer.

Result: The DIETA model achieved competitive performance on Italian-English benchmarks, ranking in the second quartile of a leaderboard and surpassing most sub-3B models on four out of five test suites.

Conclusion: DIETA demonstrates the effectiveness of a smaller-scale model targeted at Italian-English translation. The publicly available resources provide a foundation for future research and development in this field.

Abstract: In this paper, we present DIETA, a small, decoder-only Transformer model with 0.5 billion parameters, specifically designed and trained for Italian-English machine translation. We collect and curate a large parallel corpus consisting of approximately 207 million Italian-English sentence pairs across diverse domains, including parliamentary proceedings, legal texts, web-crawled content, subtitles, news, literature and 352 million back-translated data using pretrained models. Additionally, we create and release a new small-scale evaluation set, consisting of 450 sentences, based on 2025 WikiNews articles, enabling assessment of translation quality on contemporary text. Comprehensive evaluations show that DIETA achieves competitive performance on multiple Italian-English benchmarks, consistently ranking in the second quartile of a 32-system leaderboard and outperforming most other sub-3B models on four out of five test suites. The training script, trained models, curated corpus, and newly introduced evaluation set are made publicly available, facilitating further research and development in specialized Italian-English machine translation. https://github.com/pkasela/DIETA-Machine-Translation

</details>


### [129] [Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents](https://arxiv.org/abs/2601.17829)
*Dan Greenstein,Zohar Karnin,Chen Amiraz,Oren Somekh*

Main category: cs.CL

TL;DR: The paper focuses on enhancing synthetic data generation for function-calling agents by optimizing diversity metrics without handcrafted rules, and achieves notable accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of obtaining high-quality, diverse training data for function-calling agents, specifically focusing on linguistic diversity and argument coverage.

Method: It proposes a method to generate synthetic datasets by optimizing diversity metrics across queries and arguments, avoiding reliance on manually crafted rules.

Result: The technique outperforms baselines in diversity and maintains comparable correctness. Models trained on the generated data achieve better performance in out-of-distribution tests, notably a 7.4% accuracy increase on the BFCL benchmark.

Conclusion: Optimizing diversity in synthetic dataset generation improves model capabilities for function-calling agents, providing robust training data for various use cases.

Abstract: The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \texttt{city\_name}, \texttt{stock\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.

</details>


### [130] [EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy](https://arxiv.org/abs/2601.17842)
*Lanqing Du,Yunong Li,YuJie Long,Shihong Chen*

Main category: cs.CL

TL;DR: This paper introduces the EFT-CoT framework for MHQA that leverages a multi-agent and Emotion-Focused Therapy (EFT) approach to design a counseling system exhibiting high empathy and professionalism.


<details>
  <summary>Details</summary>
Motivation: Current MHQA approaches lack emphasis on embodied experiences and primary emotions in Cognitive Behavioral Therapy (CBT) interventions, creating a gap this paper aims to address.

Method: The proposed method develops an Emotion-Focused Therapy-based Multi-Agent Chain-of-Thought (EFT-CoT) framework, which uses three-stage reasoning: embodied perception, cognitive exploration, and narrative intervention. It also introduces the "EFT-Instruct" dataset and a fine-tuned EFT-LLM model.

Result: The EFT-LLM model outperforms humans and established models in empathy and structured counseling capabilities. Ablation studies validate the necessity of its multi-agent framework.

Conclusion: EFT-CoT offers a novel data-driven counseling model that demonstrates superior performance in interpretability, psychological reasoning, and empathetic interaction for mental health applications.

Abstract: Leveraging Large Language Models (LLMs) for Mental Health Question Answering (MHQA) is promising for mitigating resource shortages. However, existing Cognitive Behavioral Therapy (CBT)-based approaches predominantly favor a "top-down" rational restructuring, often neglecting clients' embodied experiences and primary emotion processing. To address this, we propose an Emotion-Focused Therapy (EFT)-based Multi-Agent Chain-of-Thought framework (EFT-CoT). Adopting a "bottom-up" trajectory, it deconstructs the intervention into a three-stage reasoning flow: "Embodied Perception - Cognitive Exploration - Narrative Intervention." Utilizing eight specialized agents, the system explicitly executes critical components such as somatic awareness mapping, adaptive assessment, core belief extraction, and narrative restructuring. We further constructed "EFT-Instruct," a high-quality dataset via Chain-of-Thought distillation of approximately 67,000 authentic texts, and fine-tuned a specialized model, EFT-LLM. Experimental evaluations demonstrate that EFT-LLM outperforms strong baselines and human responses across metrics like empathy depth and structural professionalism. Ablation studies confirm the necessity of the multi-agent mechanism. The model exhibits superior psychological reasoning, offering an effective pathway for interpretable, high-empathy counseling systems.

</details>


### [131] [D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models](https://arxiv.org/abs/2601.17865)
*Jia Gu,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: The study explores how predictive probabilities in LLMs align with task-level targets, identifying two model types with different behavior and offering guidance for model selection in practical applications.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the fine-grained sampling probabilities of large language models faithfully align with task requirements and to understand the trade-offs between diversity and stability in their outputs.

Method: Controlled distribution-sampling simulations were conducted to classify LLMs as D-models or E-models based on their sampling behaviors. These models were also evaluated on downstream tasks (e.g., code generation, recommendation) and analyzed for internal mechanisms.

Result: D-models showed high variability and poor alignment with task-level distributions, while E-models demonstrated greater stability and alignment. There are systematic trade-offs between diversity and stability that affect task performance.

Conclusion: The findings clarify the probabilistic sampling behavior of LLMs and offer actionable insights on choosing between D-models and E-models depending on application needs, improving the reliability of tasks like recommendation and search.

Abstract: The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.

</details>


### [132] [On the Emergence and Test-Time Use of Structural Information in Large Language Models](https://arxiv.org/abs/2601.17869)
*Michelle Chao Chen,Moritz Miller,Bernhard Schölkopf,Siyuan Guo*

Main category: cs.CL

TL;DR: The paper investigates how language models learn and utilize abstract structural information and identifies limitations in their compositional generation abilities at test-time.


<details>
  <summary>Details</summary>
Motivation: Understanding abstract structures in data is crucial for scientific discovery and flexible test-time generation. The study aims to explore how language models grasp and employ this structural understanding.

Method: A natural language dataset based on linguistic structural transformations is designed, enabling controlled experiments for studying structural information learning and usage.

Result: The study finds that learning structural information is linked to complex reasoning tasks, but test-time compositional generation capabilities of language models are still constrained.

Conclusion: While language models exhibit some ability to learn structural information, their compositional generation at test-time remains a challenge, requiring further research to enhance their performance.

Abstract: Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.

</details>


### [133] [Self-Manager: Parallel Agent Loop for Long-form Deep Research](https://arxiv.org/abs/2601.17879)
*Yilong Xu,Zhi Zheng,Xiang Long,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: This paper introduces Self-Manager, a framework for asynchronous and concurrent task execution to improve scalability and adaptability over conventional single-agent loops.


<details>
  <summary>Details</summary>
Motivation: Existing agents struggle with mutual interference and scalability issues due to linear context accumulation and blocking execution paradigms.

Method: The proposed Self-Manager framework uses a parallel agent loop with multiple subthreads, each maintaining isolated contexts, controlled iteratively via Thread Control Blocks.

Result: Benchmarks on DeepResearch Bench show Self-Manager outperforming single-agent loop baselines across all metrics, with additional analytical experiments validating its design choices.

Conclusion: Self-Manager provides improved contextual capacity, efficiency, and generalization, addressing scalability issues in multi-agent task execution frameworks.

Abstract: Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.

</details>


### [134] [Assessment of Generative Named Entity Recognition in the Era of Large Language Models](https://arxiv.org/abs/2601.17898)
*Qi Zhan,Yile Wang,Hui Huang*

Main category: cs.CL

TL;DR: This paper evaluates generative Named Entity Recognition (NER) capabilities in open-source Large Language Models (LLMs), finding competitive results with parameter-efficient fine-tuning, structured formats, and instruction tuning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the shift of NER tasks from traditional sequence labeling to a generative paradigm enabled by LLMs, addressing research questions on output formats, memorization, and preserving general capabilities.

Method: Experiments are conducted on eight LLMs of differing scales and four standard NER datasets, testing performance with fine-tuning, structured output formats, and instruction tuning.

Result: Open-source LLMs show competitive performance to traditional models and outperform closed-source LLMs like GPT-3. Generative ability and instruction-following drive NER capability rather than memorization, and instruction tuning improves general entity understanding.

Conclusion: Generative NER with LLMs is a viable alternative to traditional methods, providing robust performance and user-friendly implementations without sacrificing general capabilities.

Abstract: Named entity recognition (NER) is evolving from a sequence labeling task into a generative paradigm with the rise of large language models (LLMs). We conduct a systematic evaluation of open-source LLMs on both flat and nested NER tasks. We investigate several research questions including the performance gap between generative NER and traditional NER models, the impact of output formats, whether LLMs rely on memorization, and the preservation of general capabilities after fine-tuning. Through experiments across eight LLMs of varying scales and four standard NER datasets, we find that: (1) With parameter-efficient fine-tuning and structured formats like inline bracketed or XML, open-source LLMs achieve performance competitive with traditional encoder-based models and surpass closed-source LLMs like GPT-3; (2) The NER capability of LLMs stems from instruction-following and generative power, not mere memorization of entity-label pairs; and (3) Applying NER instruction tuning has minimal impact on general capabilities of LLMs, even improving performance on datasets like DROP due to enhanced entity understanding. These findings demonstrate that generative NER with LLMs is a promising, user-friendly alternative to traditional methods. We release the data and code at https://github.com/szu-tera/LLMs4NER.

</details>


### [135] [ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation](https://arxiv.org/abs/2601.17921)
*Yi Zhao,Qinghua Yao,Xinyuan song,Wei Zhu*

Main category: cs.CL

TL;DR: This paper introduces ShapLoRA, a framework aimed at improving rank allocation methods in LoRA for parameter-efficient fine-tuning of large language models, surpassing the limitations of previous approaches.


<details>
  <summary>Details</summary>
Motivation: Previous rank allocation methods for LoRA rely on inexplicable and unreliable importance measures, reducing their effectiveness and explainability in optimizing model performance.

Method: The authors propose ShapLoRA, which introduces Shapley sensitivity as an importance measure by integrating sensitivity-based measures with Shapley Value-inspired coalition ideas. They further optimize the workflow by using a separate validation set for calculations and establishing allocation-retraining procedures.

Result: Experimental results show that ShapLoRA outperforms recent baseline methods on various challenging tasks, despite having a comparable number of tunable parameters.

Conclusion: The ShapLoRA framework enhances the explainability and effectiveness of rank allocation in LoRA fine-tuning, pushing forward research in parameter-efficient model optimization.

Abstract: Low-rank adaption (LoRA) is a representative method in the field of parameter-efficient fine-tuning (PEFT), and is key to Democratizating the modern large language models (LLMs). The vanilla LoRA is implemented with uniform ranks, and the recent literature have found that properly allocating ranks on the LLM backbones results in performance boosts. However, the previous rank allocation methods have limitations since they rely on inexplanable and unreliable importance measures for the LoRA ranks. To address the above issues, we propose the ShapLoRA framework. Inspired by the explanable attribution measure Shapley Value, we combine the sensitivity-based measures with the idea of coalitions in the collaborative games among LoRA ranks, and propose a more explainable importance measure called Shapley sensitivity. In addition, we optimize the workflow of the existing works by: (a) calculating Shapley sensitivity on a separate validation set; (b) Setting up the allocating-retraining procedures for fair comparisons. We have conducted experiments on various challenging tasks, and the experimental results demonstrate that our ShapLoRA method can outperform the recent baselines with comparable tunable parameters.\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research.

</details>


### [136] [A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models](https://arxiv.org/abs/2601.17952)
*Michail Mamalakis,Tiago Azevedo,Cristian Cosentino,Chiara D'Ercoli,Subati Abulikemu,Zhongtian Sun,Richard Bethlehem,Pietro Lio*

Main category: cs.CL

TL;DR: This paper addresses the challenge of interpretability in large language models for clinical applications like Alzheimer's disease diagnosis by proposing a unified framework that integrates attributional and mechanistic perspectives.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the interpretability of LLMs, particularly in sensitive clinical settings, by addressing the issues of variability in attribution methods and the lack of direct alignment in mechanistic interpretability.

Method: The proposed method introduces a monosemantic embedding space at the LLM layer level and optimizes for reduced inter-method variability, combining attributional and mechanistic interpretability techniques.

Result: The approach produces stable, input-level importance scores and highlights key features through a decompressed representation of the LLM layer, enhancing trustworthy clinical predictions.

Conclusion: The newly developed framework paves the way for safer and more reliable applications of LLMs in diagnosing and understanding neurodegenerative diseases.

Abstract: Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.

</details>


### [137] [LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction](https://arxiv.org/abs/2601.17971)
*Junior Cedric Tonga,Chen Cecilia Liu,Iryna Gurevych,Fajri Koto*

Main category: cs.CL

TL;DR: This paper proposes using large language models (LLMs) to develop a Cultural Commonsense Knowledge Graph (CCKG) through an iterative, prompt-based framework, enabling structured cultural knowledge extraction.


<details>
  <summary>Details</summary>
Motivation: To systematically extract and organize cultural knowledge from LLMs, which currently encode implicit and unstructured cultural commonsense, thus improving interpretability and practical use in NLP tasks.

Method: An iterative, prompt-based framework uses LLMs as cultural archives to elicit culture-specific entities, relations, and practices, organizing these into multi-step inferential chains across languages. Evaluations are based on cultural relevance, correctness, and path coherence.

Result: The generated cultural knowledge graphs are more effective in English compared to non-English contexts, showcasing uneven cultural encoding in LLMs. Augmenting smaller LLMs with CCKG boosted cultural reasoning and story generation, particularly with English chains.

Conclusion: LLMs hold promise as tools for cultural understanding but exhibit limitations, with chain-structured cultural knowledge proving to be a practical approach for culturally grounded NLP applications.

Abstract: Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.

</details>


### [138] [SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets](https://arxiv.org/abs/2601.17982)
*Kshitij Mishra,Nils Lukas,Salem Lahlou*

Main category: cs.CL

TL;DR: The paper proposes a reinforcement learning framework, SD-E$^2$, to enhance reasoning in small language models (SLMs) by incentivizing semantic diversity in solution strategies.


<details>
  <summary>Details</summary>
Motivation: Current small language models (SLMs) face limitations in reasoning abilities due to prohibitive exploration costs under limited compute budgets.

Method: SD-E$^2$ combines semantic diversity rewards, based on embedding space dissimilarity, with outcome correctness and efficiency through z-score-normalized multi-objective optimization.

Result: SD-E$^2$ achieves significant improvements on GSM8K (+27.4 points), MedMCQA, and the AIME benchmark compared to baseline models, while discovering diverse strategic solutions.

Conclusion: Rewarding semantic novelty enables compute-efficient reasoning development in SLMs and offers an alternative approach to resource optimization by focusing on reasoning structure.

Abstract: Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.

</details>


### [139] [AI-based approach to burnout identification from textual data](https://arxiv.org/abs/2601.17993)
*Marina Zavertiaeva,Petr Parshakov,Mikhail Usanin,Aleksei Smirnov,Sofia Paklina,Anastasiia Kibardina*

Main category: cs.CL

TL;DR: This paper proposes an NLP-based method using a fine-tuned RuBERT model for burnout detection in text.


<details>
  <summary>Details</summary>
Motivation: To create a scalable tool that detects burnout from textual data, addressing the need for monitoring burnout language in high-stress occupations.

Method: The RuBERT sentiment analysis model was fine-tuned with synthetic sentences (via ChatGPT) and real user comments from Russian YouTube videos related to burnout.

Result: The fine-tuned model assigns probabilities of burnout presence in textual inputs and shows potential for assessing extensive text data.

Conclusion: The methodology provides a promising avenue for burnout monitoring through text analysis, using AI in organizational and healthcare settings.

Abstract: This study introduces an AI-based methodology that utilizes natural language processing (NLP) to detect burnout from textual data. The approach relies on a RuBERT model originally trained for sentiment analysis and subsequently fine-tuned for burnout detection using two data sources: synthetic sentences generated with ChatGPT and user comments collected from Russian YouTube videos about burnout. The resulting model assigns a burnout probability to input texts and can be applied to process large volumes of written communication for monitoring burnout-related language signals in high-stress work environments.

</details>


### [140] [PEAR: Pairwise Evaluation for Automatic Relative Scoring in Machine Translation](https://arxiv.org/abs/2601.18006)
*Lorenzo Proietti,Roman Grundkiewicz,Matt Post*

Main category: cs.CL

TL;DR: PEAR introduces a supervised Quality Estimation metric for machine translation evaluation using pairwise graded comparison, outperforming other models despite fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To improve machine translation evaluation through a more precise, efficient, and less redundant metric system.

Method: PEAR employs a graded pairwise comparison algorithm trained with human judgment data and includes regularization for consistency in scoring order reversal.

Result: PEAR outperformed competing QE baselines and larger metrics in benchmark tests, demonstrating efficiency and accuracy using fewer parameters.

Conclusion: PEAR provides an advanced and effective evaluation framework for MT application and decoding, balancing scoring quality and computational efficiency.

Abstract: We present PEAR (Pairwise Evaluation for Automatic Relative Scoring), a supervised Quality Estimation (QE) metric family that reframes reference-free Machine Translation (MT) evaluation as a graded pairwise comparison. Given a source segment and two candidate translations, PEAR predicts the direction and magnitude of their quality difference. The metrics are trained using pairwise supervision derived from differences in human judgments, with an additional regularization term that encourages sign inversion under candidate order reversal. On the WMT24 meta-evaluation benchmark, PEAR outperforms strictly matched single-candidate QE baselines trained with the same data and backbones, isolating the benefit of the proposed pairwise formulation. Despite using substantially fewer parameters than recent large metrics, PEAR surpasses far larger QE models and reference-based metrics. Our analysis further indicates that PEAR yields a less redundant evaluation signal relative to other top metrics. Finally, we show that PEAR is an effective utility function for Minimum Bayes Risk (MBR) decoding, reducing pairwise scoring cost at negligible impact.

</details>


### [141] [Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems](https://arxiv.org/abs/2601.18012)
*Hendrika Maclean,Mert Can Cakmak,Muzakkiruddin Ahmed Mohammed,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: This paper examines large language models' capabilities for cent-accurate application of payroll rules, using diverse models and prompt designs.


<details>
  <summary>Details</summary>
Motivation: To explore whether large language models can reliably understand and execute high-stakes payroll calculations.

Method: Experiments were conducted on tiered datasets (basic to complex), through varying prompt designs, and across multiple model families like GPT, Claude, Gemini, etc.

Result: Findings indicate scenarios where careful prompting works efficiently and cases requiring explicit computation for cent-accurate results.

Conclusion: This study provides a reproducible framework and guidance for deploying LLMs in contexts demanding both precision and auditability.

Abstract: Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.

</details>


### [142] [A System for Name and Address Parsing with Large Language Models](https://arxiv.org/abs/2601.18014)
*Adeeba Tarannum,Muzakkiruddin Ahmed Mohammed,Mert Can Cakmak,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: The paper presents a validation-centered framework for transforming unstructured person and address text into structured data using generative prompting and deterministic validation, achieving high accuracy and scalability without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of reliably converting unstructured text into structured data, especially under noisy or multilingual conditions where traditional methods or neural models fall short.

Method: The framework employs input normalization, structured prompting, constrained decoding, and strict rule-based validation, combined under fixed experimental conditions to ensure reproducibility.

Result: The method achieves high field-level accuracy, strong adherence to the schema, and stable confidence calibration on heterogeneous real-world address data.

Conclusion: Using deterministic validation alongside generative prompting provides an interpretable and scalable approach to structured data extraction, serving as a practical alternative to training-intensive or domain-specific models.

Abstract: Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.

</details>


### [143] [CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data](https://arxiv.org/abs/2601.18026)
*Pedro Ortiz Suarez,Laurie Burchell,Catherine Arnett,Rafael Mosquera-Gómez,Sara Hincapie-Monsalve,Thom Vaughan,Damian Stewart,Malte Ostendorff,Idris Abdulmumin,Vukosi Marivate,Shamsuddeen Hassan Muhammad,Atnafu Lambebo Tonja,Hend Al-Khalifa,Nadia Ghezaiel Hammouda,Verrah Otiende,Tack Hwa Wong,Jakhongir Saydaliev,Melika Nobakhtian,Muhammad Ravi Shulthan Habibi,Chalamalasetti Kranti,Carol Muchemi,Khang Nguyen,Faisal Muhammad Adam,Luis Frentzen Salim,Reem Alqifari,Cynthia Amol,Joseph Marvin Imperial,Ilker Kesen,Ahmad Mustafid,Pavel Stepachev,Leshem Choshen,David Anugraha,Hamada Nayel,Seid Muhie Yimam,Vallerie Alexandra Putra,My Chiffon Nguyen,Azmine Toushik Wasi,Gouthami Vadithya,Rob van der Goot,Lanwenn ar C'horr,Karan Dua,Andrew Yates,Mithil Bangera,Yeshil Bangera,Hitesh Laxmichand Patel,Shu Okabe,Fenal Ashokbhai Ilasariya,Dmitry Gaynullin,Genta Indra Winata,Yiyuan Li,Juan Pablo Martínez,Amit Agarwal,Ikhlasul Akmal Hanif,Raia Abu Ahmad,Esther Adenuga,Filbert Aurelian Tjiaranata,Weerayut Buaphet,Michael Anugraha,Sowmya Vajjala,Benjamin Rice,Azril Hafizi Amirudin,Jesujoba O. Alabi,Srikant Panda,Yassine Toughrai,Bruhan Kyomuhendo,Daniel Ruffinelli,Akshata A,Manuel Goulão,Ej Zhou,Ingrid Gabriela Franco Ramirez,Cristina Aggazzotti,Konstantin Dobler,Jun Kevin,Quentin Pagès,Nicholas Andrews,Nuhu Ibrahim,Mattes Ruckdeschel,Amr Keleg,Mike Zhang,Casper Muziri,Saron Samuel,Sotaro Takeshita,Kun Kerdthaisong,Luca Foppiano,Rasul Dent,Tommaso Green,Ahmad Mustapha Wali,Kamohelo Makaaka,Vicky Feliren,Inshirah Idris,Hande Celikkanat,Abdulhamid Abubakar,Jean Maillard,Benoît Sagot,Thibault Clérice,Kenton Murray,Sarah Luger*

Main category: cs.CL

TL;DR: The paper introduces CommonLID, a human-annotated language identification (LID) benchmark for web data covering 109 languages, emphasizing under-served ones.


<details>
  <summary>Details</summary>
Motivation: To improve language identification (LID) accuracy, especially for under-served languages in noisy web domains.

Method: The authors developed CommonLID, a human-annotated benchmark for web data, covering 109 languages. They tested eight LID models using CommonLID alongside other evaluation sets.

Result: CommonLID revealed that existing LID evaluations overestimate accuracy for many languages in the web domain.

Conclusion: CommonLID is a valuable resource for enhancing multilingual corpora and improving LID for under-represented languages in challenging web contexts.

Abstract: Language identification (LID) is a fundamental step in curating multilingual corpora. However, LID models still perform poorly for many languages, especially on the noisy and heterogeneous web data often used to train multilingual language models. In this paper, we introduce CommonLID, a community-driven, human-annotated LID benchmark for the web domain, covering 109 languages. Many of the included languages have been previously under-served, making CommonLID a key resource for developing more representative high-quality text corpora. We show CommonLID's value by using it, alongside five other common evaluation sets, to test eight popular LID models. We analyse our results to situate our contribution and to provide an overview of the state of the art. In particular, we highlight that existing evaluations overestimate LID accuracy for many languages in the web domain. We make CommonLID and the code used to create it available under an open, permissive license.

</details>


### [144] [Addressing LLM Diversity by Infusing Random Concepts](https://arxiv.org/abs/2601.18053)
*Pulin Agrawal,Prasoon Goyal*

Main category: cs.CL

TL;DR: The study investigates improving LLM output diversity using random concept pre-prompting, with a systematic evaluation protocol showing promising results.


<details>
  <summary>Details</summary>
Motivation: The research aims to enhance the diversity of outputs generated by large language models (LLMs) addressing their known limitation of generating repetitive or non-diverse outputs.

Method: A systematic evaluation protocol was developed to test diversity by prepending random, unrelated words or sentences to prompts (e.g., asking LLMs to "Name 10 Hollywood actors") and analyzing the resulting changes in diversity.

Result: Experiments revealed that adding random, unrelated concepts to prompts significantly increased the diversity in outputs produced by various LLMs.

Conclusion: The infusion of randomness in prompts shows potential for improving LLM diversity and inspires further investigation into other applications and systematic benchmarking of LLM diversity.

Abstract: Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form "Name 10 Hollywood actors", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.

</details>


### [145] [Neurocomputational Mechanisms of Syntactic Transfer in Bilingual Sentence Production](https://arxiv.org/abs/2601.18056)
*Ahmet Yavuz Uluslu,Elliot Murphy*

Main category: cs.CL

TL;DR: This paper advocates for integrating oscillatory signatures into bilingual production error studies and presents the ROSE neural model as a way to explain cross-linguistic influence (CLI) tied to specific oscillatory patterns in L2 planning.


<details>
  <summary>Details</summary>
Motivation: To explore how oscillatory signatures and the ROSE model can provide deeper insights and constraints for theories of bilingualism, especially in explaining syntactic transfer and morphosyntactic failures.

Method: The authors review bilingual production errors, timing signatures, and oscillatory patterns while utilizing the ROSE neural model as a theoretical framework to analyze cross-linguistic influence and functional inhibition/competition.

Result: The study highlights the potential of oscillatory failure modes to account for CLI and morphosyntactic challenges while linking with specific L2 sentence planning neural dynamics.

Conclusion: Incorporating oscillatory signatures and ROSE in bilingual studies enables better modeling of CLI, offering novel biomarkers for language dysfunction and refining bilingualism theories.

Abstract: We discuss the benefits of incorporating into the study of bilingual production errors and their traditionally documented timing signatures (e.g., event-related potentials) certain types of oscillatory signatures, which can offer new implementational-level constraints for theories of bilingualism. We argue that a recent neural model of language, ROSE, can offer a neurocomputational account of syntactic transfer in bilingual production, capturing some of its formal properties and the scope of morphosyntactic sequencing failure modes. We take as a case study cross-linguistic influence (CLI) and attendant theories of functional inhibition/competition, and present these as being driven by specific oscillatory failure modes during L2 sentence planning. We argue that modeling CLI in this way not only offers the kind of linking hypothesis ROSE was built to encourage, but also licenses the exploration of more spatiotemporally complex biomarkers of language dysfunction than more commonly discussed neural signatures.

</details>


### [146] [Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models](https://arxiv.org/abs/2601.18065)
*Aryan Roy,Zekun Wang,Christopher J. MacLellan*

Main category: cs.CL

TL;DR: The paper evaluates whether vision-language models (VLMs) exhibit greater sensitivity to linguistic concreteness compared to text-only large language models (LLMs) using text-only prompts.


<details>
  <summary>Details</summary>
Motivation: Investigate whether multimodal training in VLMs leads to more human-like linguistic concreteness sensitivity compared to text-only LLMs, aiming to understand the impact of perceptual grounding.

Method: The study involves controlled comparison of matched Llama text backbones and Llama Vision models across scales, considering multiple concreteness levels: output behavior, embedding geometry, attention dynamics, and token-level concreteness ratings compared to human norms.

Result: VLMs outperform LLMs in capturing linguistic concreteness, showing clearer representation structures, better alignment to human judgments, and distinct attention patterns aligned with increased grounding.

Conclusion: Multimodal training in VLMs enhances linguistic concreteness sensitivity, reinforcing the role of perceptual grounding in aligning models more closely with human cognitive patterns.

Abstract: Do vision--language models (VLMs) develop more human-like sensitivity to linguistic concreteness than text-only large language models (LLMs) when both are evaluated with text-only prompts? We study this question with a controlled comparison between matched Llama text backbones and their Llama Vision counterparts across multiple model scales, treating multimodal pretraining as an ablation on perceptual grounding rather than access to images at inference. We measure concreteness effects at three complementary levels: (i) output behavior, by relating question-level concreteness to QA accuracy; (ii) embedding geometry, by testing whether representations organize along a concreteness axis; and (iii) attention dynamics, by quantifying context reliance via attention-entropy measures. In addition, we elicit token-level concreteness ratings from models and evaluate alignment to human norm distributions, testing whether multimodal training yields more human-consistent judgments. Across benchmarks and scales, VLMs show larger gains on more concrete inputs, exhibit clearer concreteness-structured representations, produce ratings that better match human norms, and display systematically different attention patterns consistent with increased grounding.

</details>


### [147] [Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents](https://arxiv.org/abs/2601.18077)
*Mahesh Ramesh,Kaousheik Jayakumar,Aswinkumar Ramkumar,Pavan Thodima,Aniket Rege*

Main category: cs.CL

TL;DR: The paper evaluates reasoning and cooperative abilities of 17 state-of-the-art LLM agents in Hanabi, measuring cognitive performance across different setup approaches and achieving significant improvements through RL and supervised finetuning.


<details>
  <summary>Details</summary>
Motivation: To address challenges in cooperative reasoning under incomplete information by exploring LLMs' effectiveness in games requiring theory-of-mind reasoning and strategic communication.

Method: Benchmarking LLM agents using varying reasoning contexts (explicit card details, Bayesian deductions, and deep memory tracking) with supervised and reinforcement learning finetuning on newly released annotated datasets.

Result: Strong reasoning models perform competitively but trail humans and specialized agents. RL and supervised finetuning improve Hanabi performance significantly and show generalization to other reasoning tasks.

Conclusion: Improved LLM reasoning using fine-tuned models, releasing datasets to support cooperative reasoning advancements applicable across diverse benchmarks beyond Hanabi.

Abstract: Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.

</details>


### [148] [CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations](https://arxiv.org/abs/2601.18102)
*Stephanie Fong,Zimu Wang,Guilherme C. Oliveira,Xiangyu Zhao,Yiwen Jiang,Jiahe Liu,Beau-Luke Colton,Scott Woods,Martha E. Shenton,Barnaby Nelson,Zongyuan Ge,Dominic Dwyer*

Main category: cs.CL

TL;DR: This paper presents CHiRPE, an NLP pipeline designed for predicting psychosis risk from clinical interviews with novel explainable AI methods tailored for clinician usability. It achieved over 90% accuracy and its unique explanation approach was preferred by clinical experts.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in explainable AI tailored for medical NLP tools, allowing clinical reasoning alignment and input from clinicians for better interpretability.

Method: CHiRPE integrates semi-structured clinical interviews with symptom-domain mapping, large language model summarization, and BERT-based classification. It also introduces novel SHAP explanation formats co-developed with clinicians.

Result: CHiRPE achieved over 90% accuracy with three BERT variants and outperformed baseline models. The explanation formats were preferred by 28 clinical experts, highlighting concept-guided hybrid graph-and-text summaries.

Conclusion: Clinically-guided development can produce accurate and interpretable NLP models. Future work includes real-world testing across international clinical sites.

Abstract: The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.

</details>


### [149] [GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health](https://arxiv.org/abs/2601.18106)
*Jiatan Huang,Zheyuan Zhang,Tianyi Ma,Mingchen Li,Yaning Zheng,Yanfang Ye,Chuxu Zhang*

Main category: cs.CL

TL;DR: The paper introduces GLEN-Bench, a graph-language benchmark for nutritional health assessment, designed to address gaps in personalized dietary interventions considering real-world constraints.


<details>
  <summary>Details</summary>
Motivation: The study aims to fill gaps in current dietary guidance systems which neglect socioeconomic, comorbidity, and food-access factors, and to improve explainability and benchmark evaluation in nutritional health interventions.

Method: A knowledge graph combining multiple datasets (NHANES, FNDDS, USDA) is created, and the benchmark includes three tasks: risk detection, personalized food recommendation, and graph-based question answering. Computational models like graph neural networks and large language models are evaluated.

Result: GLEN-Bench identifies health-risk dietary patterns, offering insights for interventions, and establishes baselines for evaluating hybrid graph-language approaches.

Conclusion: GLEN-Bench provides a systematic way to address practical dietary guidance while integrating real-world constraints and enhancing explainability, paving the way for more effective interventions.

Abstract: Nutritional interventions are important for managing chronic health conditions, but current computational methods provide limited support for personalized dietary guidance. We identify three key gaps: (1) dietary pattern studies often ignore real-world constraints such as socioeconomic status, comorbidities, and limited food access; (2) recommendation systems rarely explain why a particular food helps a given patient; and (3) no unified benchmark evaluates methods across the connected tasks needed for nutritional interventions. We introduce GLEN-Bench, the first comprehensive graph-language based benchmark for nutritional health assessment. We combine NHANES health records, FNDDS food composition data, and USDA food-access metrics to build a knowledge graph that links demographics, health conditions, dietary behaviors, poverty-related constraints, and nutrient needs. We test the benchmark using opioid use disorder, where models must detect subtle nutritional differences across disease stages. GLEN-Bench includes three linked tasks: risk detection identifies at-risk individuals from dietary and socioeconomic patterns; recommendation suggests personalized foods that meet clinical needs within resource constraints; and question answering provides graph-grounded, natural-language explanations to facilitate comprehension. We evaluate these graph-language approaches, including graph neural networks, large language models, and hybrid architectures, to establish solid baselines and identify practical design choices. Our analysis identifies clear dietary patterns linked to health risks, providing insights that can guide practical interventions.

</details>


### [150] [FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning](https://arxiv.org/abs/2601.18116)
*Lin Sun,Linglin Zhang,Jingang Huang,Change Jia,Zhengwei Cheng,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: FABLE proposes a retrieval framework leveraging hierarchical semantic structures with efficiency and accuracy improvements over traditional RAG systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses long-context LLM limitations such as inefficiency, poor scalability, and semantic issues, aiming to enhance structured retrieval.

Method: The method involves constructing hierarchical forest-based indexes enhanced by LLMs with bi-path strategies to optimize performance and scalability.

Result: FABLE outperformed state-of-the-art RAG systems, achieving similar accuracy to full-context LLM inference while significantly reducing token usage.

Conclusion: Structured retrieval frameworks like FABLE are essential alongside long-context LLMs to overcome their limitations and improve efficiency and accuracy.

Abstract: The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.
  We present \textbf{FABLE}, a \textbf{F}orest-based \textbf{A}daptive \textbf{B}i-path \textbf{L}LM-\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.
  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.

</details>


### [151] [Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models](https://arxiv.org/abs/2601.18129)
*Kunat Pipatanakul,Pittawat Taveekitworachai*

Main category: cs.CL

TL;DR: This paper introduces Typhoon S, a minimal and open post-training approach for creating sovereign Large Language Models (LLMs) tailored for specific regional or domain-specific needs.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by institutions operating under constrained resources who require control over model weights, training data, and deployment in their local language and cultural context.

Method: The Typhoon S method combines supervised fine-tuning, on-policy distillation, and small-scale reinforcement fine-tuning. They use InK-GRPO, an extended GRPO loss enriched with next-word prediction loss, tested on Thai as a case study.

Result: Their method demonstrated strong performance in instruction-tuning base models tailored for Thai legal reasoning and cultural knowledge, even with limited data and compute resources.

Conclusion: A well-designed post-training process can enable the development of high-quality regional LLMs without relying on massive compute and instruction data, offering a feasible roadmap for sovereign applications.

Abstract: Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.

</details>


### [152] [Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models](https://arxiv.org/abs/2601.18162)
*Ani Harutyunyan,Sachin Kumar*

Main category: cs.CL

TL;DR: This paper benchmarks three model families on GoEmotions for multi-label emotion recognition and finds BERT outperforms others overall, though logistic regression excels in Micro-F1.


<details>
  <summary>Details</summary>
Motivation: To address challenges in fine-grained emotion recognition, specifically label overlap and class imbalance, by evaluating diverse model families and their handling of multi-label classification on the GoEmotions dataset.

Method: Benchmarks used three models: a TF-IDF-based logistic regression system with binary relevance, a BiLSTM with attention, and a fine-tuned BERT for multi-label classification, with experiments leveraging inverse-frequency class weights to account for imbalance.

Result: Logistic regression had the highest Micro-F1 score at 0.51, while BERT surpassed prior work's results with top performance in Macro-F1 (0.49), Hamming Loss (0.036), and Subset Accuracy (0.36).

Conclusion: Frequent emotions are better predicted using lexical cues, as seen with logistic regression, but BERT's contextual understanding boosts accuracy for rarer, ambiguous emotions, offering a more balanced performance.

Abstract: Fine-grained emotion recognition is a challenging multi-label NLP task due to label overlap and class imbalance. In this work, we benchmark three modeling families on the GoEmotions dataset: a TF-IDF-based logistic regression system trained with binary relevance, a BiLSTM with attention, and a BERT model fine-tuned for multi-label classification. Experiments follow the official train/validation/test split, and imbalance is mitigated using inverse-frequency class weights. Across several metrics, namely Micro-F1, Macro-F1, Hamming Loss, and Subset Accuracy, we observe that logistic regression attains the highest Micro-F1 of 0.51, while BERT achieves the best overall balance surpassing the official paper's reported results, reaching Macro-F1 0.49, Hamming Loss 0.036, and Subset Accuracy 0.36. This suggests that frequent emotions often rely on surface lexical cues, whereas contextual representations improve performance on rarer emotions and more ambiguous examples.

</details>


### [153] [MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2601.18204)
*Juexiang Ye,Xue Li,Xinyu Yang,Chengkai Huang,Lanshun Nie,Lina Yao,Dechen Zhan*

Main category: cs.CL

TL;DR: The paper presents MemWeaver, a memory framework for language model agents, improving reasoning and memory efficiency during long interactions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current memory approaches for agents, such as temporal conflicts, brittle reasoning, and lack of evidence traceability in long-horizon interactions.

Method: Introducing MemWeaver, a framework with three memory components: temporally grounded graph memory, experience memory, and passage memory, combined with a dual-channel retrieval strategy for efficient and accurate reasoning.

Result: MemWeaver enhances multi-hop and temporal reasoning accuracy and reduces context lengths by over 95% on the LoCoMo benchmark.

Conclusion: MemWeaver provides a more efficient and accurate memory system for long-horizon interactions in language model agents.

Abstract: Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\% compared to long-context baselines.

</details>


### [154] [TechING: Towards Real World Technical Image Understanding via VLMs](https://arxiv.org/abs/2601.18238)
*Tafazzul Nadeem,Bhavik Shangari,Manish Rai,Gagan Raj Gupta,Ashutosh Modi*

Main category: cs.CL

TL;DR: The paper introduces a synthetically generated dataset to train Vision-Language Models (VLMs) for better understanding hand-drawn technical diagrams. It fine-tunes the Llama 3.2 11B model, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Professionals face difficulty editing hand-drawn technical diagrams as modern VLMs struggle to interpret these properly, yet generating large datasets of real hand-drawn images is challenging.

Method: The approach involves creating a synthetic corpus mimicking real-world hand-drawn diagrams, introducing self-supervision tasks, and fine-tuning the Llama 3.2 11B-instruct model on this corpus.

Result: The fine-tuned Llama-VL-TUG model improves ROUGE-L by 2.14x and reduces compilation errors across 7 of 8 diagram types in human evaluation, achieving a 6.97x boost in F1 score.

Conclusion: Synthetic data can effectively train VLMs for technical diagram understanding, outperforming baselines and enhancing practical usability.

Abstract: Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.

</details>


### [155] [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253)
*Peng Sun,Xiangyu Zhang,Duan Wu*

Main category: cs.CL

TL;DR: BoRP is a framework for evaluating user satisfaction in conversational AI using LLM latent space properties, achieving better performance and reduced cost compared to generative methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reliable metrics in A/B testing for open-ended conversational AI due to sparse explicit feedback and ambiguous implicit metrics.

Method: BoRP employs polarization-index-based bootstrapping for rubric generation and Partial Least Squares (PLS) to map hidden states to scores.

Result: BoRP significantly outperforms generative baselines in alignment with human evaluations and reduces inference costs drastically.

Conclusion: BoRP provides a scalable, efficient, and accurate solution for satisfaction evaluation, enhancing A/B testing and monitoring for conversational AI.

Abstract: Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.

</details>


### [156] [Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue](https://arxiv.org/abs/2601.18281)
*Yuhang Jia,Pei Liu,Haoqin Sun,Jiaming Zhou,Xuxin Cheng,Cao Liu,Ke Zeng,Xunliang Cai,Yong Qin*

Main category: cs.CL

TL;DR: The paper introduces ReEmpathy, an end-to-end Spoken Language Model (SLM) that leverages reflective reasoning for better empathetic dialogue generation, addressing current limitations of rigid supervised signals.


<details>
  <summary>Details</summary>
Motivation: To improve the empathetic capabilities of Spoken Language Models for empathetic dialogue, moving beyond rigid and limited supervised signals that fail to capture complex empathy in emotional expression.

Method: The researchers developed EmpathyEval, a natural-language-based evaluation model, and subsequently proposed ReEmpathy, an SLM that incorporates an Empathetic Self-Reflective Alternating Inference mechanism to interleave spoken response generation with reflective reasoning.

Result: Experiments show that ReEmpathy enhances empathetic dialogue quality and enables effective reflective reasoning, making it more emotionally intelligent.

Conclusion: ReEmpathy demonstrates substantial improvements in generating empathy-aware spoken dialogues, offering a valuable step toward enriching human-computer interactions with emotional intelligence.

Abstract: End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single "correct" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.

</details>


### [157] [U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents](https://arxiv.org/abs/2601.18285)
*Jin Su,Runnan Fang,Yeqiu Li,Xiaobin Wang,Shihao Cai,Pengjun Xie,Ningyu Zhang,Fajie Yuan*

Main category: cs.CL

TL;DR: U-Fold enhances context management in large language models, excelling in realistic, user-centric tasks and long, noisy dialogues by improving intent-aware summaries and task-relevant logs.


<details>
  <summary>Details</summary>
Motivation: Existing context-folding methods fail to handle realistic multi-turn, user-centric dialogues due to loss of essential details and inability to track evolving intent.

Method: U-Fold dynamically creates intent-aware dialogue summaries and a task-relevant tool log, retaining the full dialogue and tool-call history for effective context management.

Result: U-Fold outperforms prior methods in context-inflated and multi-turn settings, achieving a 71.4% win rate against ReAct and improving performance by up to 27% over other baselines.

Conclusion: U-Fold offers a significant improvement in context management, enabling large language models to handle complex, user-centric tasks more effectively.

Abstract: Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $τ$-bench, $τ^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.

</details>


### [158] [Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.18296)
*Zhaoyan Gong,Zhiqiang Liu,Songze Li,Xiaoke Guo,Yuanxiang Liu,Xinle Deng,Zhizhen Liu,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: The paper introduces Temp-R1, an autonomous agent for answering complex temporal knowledge graph questions using reinforcement learning and reverse curriculum learning.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing TKGQA methods that rely on fixed workflows and expensive APIs, aiming to enhance flexibility, scalability, and temporal reasoning capabilities.

Method: Temp-R1 employs reinforcement learning, extended action spaces to reduce cognitive overload, and reverse curriculum learning to improve reasoning by starting with difficult questions.

Result: Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA datasets, improving 19.8% over strong baselines notably for complex questions.

Conclusion: Temp-R1 establishes a new standard for autonomous agents in temporal knowledge graph reasoning and demonstrates scalable and effective approaches for TKGQA.

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.

</details>


### [159] [Suppressing Final Layer Hidden State Jumps in Transformer Pretraining](https://arxiv.org/abs/2601.18302)
*Keigo Shibata,Kazuki Yano,Ryosuke Takahashi,Jaesung Lee,Wataru Ikeda,Jun Suzuki*

Main category: cs.CL

TL;DR: The paper focuses on Transformer language models' behavior, particularly the large angular distance change in the final layers. It introduces a metric for characterizing this property, proposes a regularizer to suppress these jumps, and demonstrates improved task performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address an observed undesirable behavior in pre-trained Transformer language models, where middle layers show slight angular changes while the final layers exhibit disproportionately large jumps. This is seen as suboptimal utilization of model capacity.

Method: The paper introduces a quantitative metric to measure the jump strength around the final layer. It then proposes the "jump-suppressing regularizer" (JREG) during pre-training to distribute capability usage more evenly across middle layers.

Result: Empirical evaluations on Llama-based models trained with the proposed JREG method show improved task performance compared to baseline models, without modifying the model architecture.

Conclusion: The paper concludes that suppressing large angular jumps in final Transformer layers improves task performance, indicating better layer utilization and balancing of the model's internal behavior.

Abstract: This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.

</details>


### [160] [Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM](https://arxiv.org/abs/2601.18306)
*Everlyn Asiko Chimoto,Mostafa Elhoushi,Bruce A. Bassett*

Main category: cs.CL

TL;DR: This paper evaluates multilingual calibration sets for post-training quantization in Large Language Models (LLMs), finding that non-English and multilingual sets significantly improve model perplexity over English-only baselines.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of research into the impact of multilingual calibration sets on the quantization performance of LLMs, especially since existing methods predominantly use English-only data, which might not be optimal for multilingual models.

Method: The authors systematically evaluate eight different calibration settings, including single-language and multilingual mixes, using two quantizers (GPTQ, AWQ) on datasets covering 10 languages. They analyze how calibration data affects perplexity in popular LLMs like Llama3.1 8B and Qwen2.5 7B.

Result: Non-English and multilingual calibration sets consistently enhance perplexity performance compared to English-only baselines. Multilingual mixes achieve the best results, reducing perplexity by up to 3.52 points. The study also finds that aligning calibration sets with specific evaluation languages yields the largest improvements in individual cases.

Conclusion: Static English-only calibration is not optimal for multilingual LLMs. Tailoring calibration data by including linguistic diversity and aligning it with evaluation languages is crucial for robust quantization, while certain language-quantizer combinations may still fail due to activation range differences across languages.

Abstract: Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.

</details>


### [161] [MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization](https://arxiv.org/abs/2601.18320)
*Jinwei Lu,Yuanfeng Song,Chen Zhang,Raymond Chi-Wing Wong*

Main category: cs.CL

TL;DR: This paper introduces MultiVis-Agent, a reliable multi-modal visualization framework using a logic rule-guided multi-agent system.


<details>
  <summary>Details</summary>
Motivation: Existing systems for visualization generation struggle with multi-modal inputs and iterative processes, and face reliability concerns when using language models (LLMs).

Method: A four-layer logic rule-enhanced multi-agent framework ensures reliability while allowing flexibility, with a focus on mathematical constraints to guide reasoning.

Result: MultiVis-Agent achieved a 75.63% visualization score, 99.58% task completion rate, and 94.56% code success rate on the MultiVis-Bench benchmark, surpassing baselines in performance.

Conclusion: The proposed MultiVis-Agent addresses complex and reliability-focused challenges in visualization tasks, outperforming existing approaches and advancing the field of automated visualization.

Abstract: Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.

</details>


### [162] [Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare](https://arxiv.org/abs/2601.18334)
*Clément Christophe,Wadood Mohammed Abdul,Prateek Munjal,Tathagata Raha,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.CL

TL;DR: This paper investigates the risks of sycophancy in LLMs within clinical workflows and proposes a novel Adjusted Sycophancy Score to evaluate and address alignment bias.


<details>
  <summary>Details</summary>
Motivation: With LLMs being integrated into clinical workflows, their tendency to prioritize user agreement over factual accuracy poses risks to patient safety, necessitating reliable evaluations of model behavior.

Method: The paper introduces the Adjusted Sycophancy Score as a new metric, grounded in medical MCQA with verifiable truths, and conducts scaling analyses on Qwen-3 and Llama-3 while analyzing reasoning-optimized models' vulnerabilities.

Result: Findings highlight that high benchmark performance is not indicative of clinical reliability and reveal a scaling trajectory for resilience but also vulnerabilities in models rationalizing incorrect suggestions.

Conclusion: Simplified reasoning structures may better counteract sycophancy in expert-driven scenarios, and current evaluation metrics are insufficient for ensuring clinical reliability of LLMs.

Abstract: As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or "confusability". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized "Thinking" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.

</details>


### [163] [When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs](https://arxiv.org/abs/2601.18350)
*Junyi Zou*

Main category: cs.CL

TL;DR: The paper discusses enhancing large language models (LLMs) in safety-critical medical domains using a two-stage LoRA pipeline combining domain-specific training and fine-tuning, achieving measurable improvements in medical data tasks.


<details>
  <summary>Details</summary>
Motivation: Despite their capabilities, LLMs struggle with precision in medical terminology and safety-critical tasks. The study aims to improve model performance and reliability in these specific areas.

Method: The method involves two-stage LoRA pipeline: (1) domain-adaptive pre-training (DAPT) to equip the model with medical knowledge, (2) supervised fine-tuning (SFT) to align with medical question-answering tasks, followed by Weighted Adapter Merging to combine the training stages effectively.

Result: The merged model showed improved metrics on a medical validation dataset achieving a BLEU-4 score of 16.38, ROUGE-1 of 20.42, ROUGE-2 of 4.60, and ROUGE-L of 11.54.

Conclusion: The proposed methodology successfully enhances LLMs’ ability to perform in safety-critical medical contexts by balancing instruction-following precision with domain knowledge through novel weighted adapter merging techniques.

Abstract: Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.

</details>


### [164] [Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning](https://arxiv.org/abs/2601.18352)
*Manjie Xu,Isabella Yin,Xinyi Tu,Chi Zhang,Yixin Zhu*

Main category: cs.CL

TL;DR: The paper investigates the phenomenon of 'Semantic Inertia' in language models, where they struggle to override pre-trained knowledge when confronted with contradictory, context-based rules. It uses the game 'Baba Is You' for analysis and demonstrates methods to address this limitation.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenge that language models cannot easily override their pre-trained associations (e.g., 'Lava is dangerous') when context introduces contradictory rules, which is essential for accurate reasoning in dynamic environments.

Method: The authors utilize the game 'Baba Is You' for controlled experiments, introduce the idea of representing dynamics as executable code rather than descriptive text, and propose a fine-tuning method called Code-Grounded Vistas (LCV) to train models on counterfactual examples.

Result: Representing rules as executable code and employing the LCV training method improves contextual reasoning, proving more effective and efficient than inference-time search methods. Larger models initially exhibit inverse scaling, but this approach corrects such limitations.

Conclusion: Scaling in language models does not universally lead to better performance. Instead, the representation method (code versus text) fundamentally alters the models' ability to adjust to changing contexts. This has significant implications for applications requiring dynamic reasoning.

Abstract: LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., "Lava is Dangerous") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting "Lava is Safe"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.

</details>


### [165] [CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes](https://arxiv.org/abs/2601.18374)
*Rodrigo Silva,José Evans,José Isidro,Miguel Marques,Afonso Fonseca,Ricardo Morais,João Canavilhas,Arian Pasquali,Purificação Silvano,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,Ricardo Campos*

Main category: cs.CL

TL;DR: City council minutes are transformed into searchable and accessible data using the CitiLink platform leveraging NLP and IR technologies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the accessibility and transparency of municipal meeting minutes that are currently lengthy and hard to navigate for citizens and journalists.

Method: The system uses large language models (LLMs) to extract metadata, discussed topics, and voting outcomes, then indexes the data in a searchable format with BM25 ranking and faceted filtering.

Result: The system was successfully applied to 120 minutes from six Portuguese municipalities, and user testing with municipal personnel provided insights on system usability. Additionally, Gemini's system effectively extracted relevant information.

Conclusion: CitiLink improves public engagement with local government by making municipal meeting minutes more accessible and transparent through structured data transformation and user-friendly search interfaces.

Abstract: City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.

</details>


### [166] [Hierarchical Text Classification with LLM-Refined Taxonomies](https://arxiv.org/abs/2601.18375)
*Jonas Golde,Nicolaas Jedema,Ravi Krishnan,Phong Le*

Main category: cs.CL

TL;DR: The paper introduces TaxMorph framework to refine taxonomies for hierarchical text classification using large language models, improving performance.


<details>
  <summary>Details</summary>
Motivation: Hierarchical text classification often encounters issues due to ambiguities in real-world taxonomies, making it hard for language models to establish clear decision boundaries.

Method: TaxMorph refines taxonomies using operations like renaming, merging, splitting, and reordering, leveraging large language models to align hierarchies with language model semantics.

Result: Refined taxonomies using TaxMorph outperform human-curated ones across benchmarks, with improvements up to +2.9pp in F1 scores.

Conclusion: LLM-refined taxonomies better match models’ inductive biases, leading to improved HTC performance by aligning to model learning patterns despite reduced separability in embedding space.

Abstract: Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.

</details>


### [167] [Corpus-Based Approaches to Igbo Diacritic Restoration](https://arxiv.org/abs/2601.18380)
*Ignatius Ezeani*

Main category: cs.CL

TL;DR: This paper develops a framework for generating datasets for diacritic restoration in Igbo language using n-gram, classification, and embedding models.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the lack of NLP tools and resources for low-resourced languages, particularly for diacritic ambiguity in the Igbo language.

Method: Three methods were researched: n-gram models utilizing prior word sequences, classification models using context windows around a word, and embedding models comparing similarity scores of word vectors.

Result: A flexible framework was developed for dataset generation to support diacritic restoration in the Igbo language, addressing a critical gap in NLP efforts for low-resourced languages.

Conclusion: This research contributes to NLP by providing tools and methodologies to handle diacritic disambiguation for the Igbo language, paving the way for better resource support in low-resourced languages.

Abstract: With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.
  In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.

</details>


### [168] [Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction](https://arxiv.org/abs/2601.18395)
*Mikel Zubillaga,Oscar Sainz,Oier Lopez de Lacalle,Eneko Agirre*

Main category: cs.CL

TL;DR: This paper proposes "ThinkTwice," which improves document-level information extraction (DocIE) by leveraging LLM's ability to generate multiple candidate outputs and selecting the best one. It outperforms previous methods.


<details>
  <summary>Details</summary>
Motivation: The current methods for document-level information extraction often rely on greedy decoding, which limits output variability and potentially overlooks better solutions.

Method: The paper introduces ThinkTwice, a framework with unsupervised and supervised selection methods. Multiple candidate templates are generated by the LLM using sampling, and the most suitable one is chosen via agreement exploitation (unsupervised) or a reward model trained on labeled DocIE data (supervised). A novel rejection-sampling method is also used to create silver reasoning data.

Result: Experiments show that ThinkTwice, in both unsupervised and supervised setups, outperforms standard greedy baselines and existing state-of-the-art methods.

Conclusion: The study concludes that embracing output variability through sampling and implementing strategic selection methods can significantly enhance the performance of DocIE tasks.

Abstract: Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.

</details>


### [169] [Pisets: A Robust Speech Recognition System for Lectures and Interviews](https://arxiv.org/abs/2601.18415)
*Ivan Bondarenko,Daniil Grebenkin,Oleg Sedukhin,Mikhail Klementev,Roman Derunets,Lyudmila Budneva*

Main category: cs.CL

TL;DR: The paper introduces "Pisets," a speech-to-text system leveraging advanced techniques to improve transcription accuracy, building on the Whisper model.


<details>
  <summary>Details</summary>
Motivation: To address errors and hallucinations associated with the Whisper model for transcription, providing robust and accurate speech recognition.

Method: The system uses a three-component architecture: Wav2Vec2 for initial recognition, AST for false positive filtering, and Whisper for final transcription. It incorporates curriculum learning, diverse Russian-language corpora, and uncertainty modeling techniques.

Result: "Pisets" demonstrates improved audio transcription across varying acoustic conditions, outperforming WhisperX and standard Whisper models.

Conclusion: The approaches ensure efficient and reliable transcription for long audio, and the system's source code is publicly available for further use.

Abstract: This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.

</details>


### [170] [Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.18468)
*Daniel B. Hier,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: Large language models store biomedical facts unevenly. Fine-tuning enhances recall and latent knowledge predicts faster learning, but generalization remains limited.


<details>
  <summary>Details</summary>
Motivation: Understanding the uneven distribution and accessibility of biomedical facts within large language models, aiming to enhance their factual learning and generalization capabilities.

Method: Fine-tuning Llama 3.1 8B Instruct with ontology mapping data followed by analysis using stochastic decoding and Cox proportional hazards models over 20 epochs.

Result: Baseline recall of facts was improved from 2.8% to 71.9%, latent knowledge influenced faster fact acquisition, and generalization to unseen facts remained low at 5.8%. Reinforcement during training helped resist degradation.

Conclusion: Latent knowledge significantly impacts factual learning speed and highlights limited generalization to unseen ontology facts; reinforcement during training is key to retaining learned facts.

Abstract: Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.

</details>


### [171] [Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs](https://arxiv.org/abs/2601.18483)
*Arya Labroo,Ivaxi Sheth,Vyas Raina,Amaani Ahmed,Mario Fritz*

Main category: cs.CL

TL;DR: The paper investigates fine-grained concept control in LLMs, introducing a framework that evaluates single- and multi-attribute settings.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation for fine-grained and multi-attribute control in language generation tasks using LLMs.

Method: Developed an evaluation framework to assess LLM performance in single and dual-concept control, focusing on distinct linguistic attributes.

Result: Found performance drops in dual-concept control due to LLMs struggling with compositionality, even for independent concepts.

Conclusion: The study highlights a limitation in naive prompting-based approaches for multi-concept control and presents a tool to assess future improvements.

Abstract: Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.

</details>


### [172] [Demographic Probing of Large Language Models Lacks Construct Validity](https://arxiv.org/abs/2601.18486)
*Manuel Tonneau,Neil K. R. Seghal,Niyati Malhotra,Victor Orozco-Olvera,Ana María Muñoz Boudet,Lakshmi Subramanian,Sharath Chandra Guntuku,Valentin Hofmann*

Main category: cs.CL

TL;DR: Demographic probing in LLMs shows inconsistencies in how models respond to demographic cues, with unstable estimates of disparities across race and gender cues.


<details>
  <summary>Details</summary>
Motivation: The paper analyzes whether current methods for demographic probing in LLMs are valid for capturing demographic-conditioned behaviors in realistic advice-seeking contexts.

Method: The authors tested demographic probing by analyzing overlapping and distinct behavior changes when multiple cues of the same demographic group (race and gender) were used.

Result: Findings revealed instability in model behavior responses across cues, weak differentiation between demographic groups within the same cue, and the influence of linguistic confounders.

Conclusion: The study suggests current demographic probing methods lack construct validity. It recommends using multiple demographic cues and controlling for confounders to achieve more reliable insights into LLMs' demographic effects.

Abstract: Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.

</details>


### [173] [Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research](https://arxiv.org/abs/2601.18512)
*Antonio Garzon-Vico,Krithika Sharon Komalapati,Arsalan Shahid,Jan Rosier*

Main category: cs.CL

TL;DR: The paper develops a framework to simulate real CEOs' decision-making using large language models, validating their effectiveness through comparisons with human behavior.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in gaining direct access to executives for organizational research by creating credible simulations.

Method: The study constructs virtual CEO personas based on real communications and Moral Foundations Theory, validating their reliability and fidelity in three phases.

Result: Virtual personas approximate human moral judgments, providing reliable tools for organizational studies.

Conclusion: LLM-based personas can supplement organizational research, particularly when access to real leaders is limited.

Abstract: This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.

</details>


### [174] [GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback](https://arxiv.org/abs/2601.18517)
*James Sungarda,Hongkai Liu,Zilong Zhou,Tien-Hsuan Wu,Johnson Chun-Sing Cheung,Ben Kao*

Main category: cs.CL

TL;DR: The paper introduces SWITCH, a chatbot for social work training that simulates realistic client interaction, provides skill assessment, and incorporates a system for Motivational Interviewing progression.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address constraints in social work training by providing scalable, consistent feedback and easing reliance on field resources like instructors and real clients.

Method: SWITCH integrates a client simulation model with static and dynamic fields, a counseling skill classification module, and an MI progression system. It employs in-context learning and a fine-tuned BERT model for improving classification accuracy.

Result: Experiments show that the fine-tuned BERT classifier and in-context learning approach significantly outperform baselines in classifying counseling skills.

Conclusion: SWITCH enhances social work training by offering a scalable, cost-effective, and efficient tool that aligns with field education while enabling supervisors to focus on mentorship.

Abstract: Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.

</details>


### [175] [Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models](https://arxiv.org/abs/2601.18527)
*Francesco Maria Molfese,Momchil Hardalov,Rexhina Blloshmi,Bill Byrne,Adrià de Gispert*

Main category: cs.CL

TL;DR: The paper investigates fine-tuning strategies for Long-Context Language Models (LCLMs), and their impact on performance and robustness under KV-cache compression, revealing significant in-domain improvements but mixed out-of-domain results.


<details>
  <summary>Details</summary>
Motivation: To explore whether fine-tuning strategies can enhance long-context performance in LCLMs and improve robustness under KV-cache compression.

Method: The study evaluates different training strategies for LCLMs to improve their ability to identify and utilize relevant information across contexts, and measures the performance in both in-domain and out-of-domain tasks as well as under KV-cache compression.

Result: The study achieved significant performance gains in-domain (up to +20 points) and demonstrated task-specific variances in out-of-domain performance, with LCLMs performing better in finance tasks and retrieval-augmented generation stronger in multiple-choice tasks. Moderate robustness gains under KV-cache compression were observed.

Conclusion: Fine-tuning can substantially improve LCLM in-domain performance, but out-of-domain generalization and KV-cache robustness gains are task-dependent, requiring further research for broader applicability.

Abstract: With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.

</details>


### [176] [From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation](https://arxiv.org/abs/2601.18533)
*Yuxin Jiang,Yufei Wang,Qiyuan Zhang,Xingshan Zeng,Liangyou Li,Jierun Chen,Chaofan Tao,Haoli Bai,Lifeng Shang*

Main category: cs.CL

TL;DR: The paper proposes RLVRR, a reinforcement learning approach to handle open-ended generation tasks by introducing reference-based reward chains.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and reward hacking in reinforcement learning with verifiable rewards when applied to open-ended generation tasks.

Method: The RLVRR uses a reward chain extracted from high-quality references to decompose rewards into content and style dimensions, enabling efficient learning and verification.

Result: RLVRR outperforms supervised fine-tuning on over 10 benchmarks, combines reasoning with open-ended tasks, and generalizes effectively while maintaining diversity.

Conclusion: RLVRR is a principled and efficient method for aligning general-purpose language generation tasks with verifiable reinforcement learning.

Abstract: Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.

</details>


### [177] [Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features](https://arxiv.org/abs/2601.18536)
*Abishek Stephen,Jindřich Libovický*

Main category: cs.CL

TL;DR: The paper introduces a novel metric to evaluate subword segmentation's morphological plausibility using morpho-syntactic features instead of gold segmentation data.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of inconsistent or unavailable gold segmentation data, which limits subword segmentation evaluation across various languages.

Method: The proposed metric uses morpho-syntactic features from resources like Universal Dependencies or UniMorph and employs IBM Model 1 for probabilistic alignment with subwords.

Result: Experiments validate that the metric correlates well with traditional morpheme boundary recall and has wider applicability across languages.

Conclusion: This metric provides a reliable and broadly applicable solution for evaluating subword segmentation by leveraging existing morpho-syntactic resources.

Abstract: We present a novel metric for the evaluation of the morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.

</details>


### [178] [Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection](https://arxiv.org/abs/2601.18552)
*Devansh Srivastav,David Pape,Lea Schönherr*

Main category: cs.CL

TL;DR: The paper analyzes hidden intentions in LLMs, categorizing and evaluating their detectability under real-world settings, while emphasizing the urgent need for frameworks to address these behaviors.


<details>
  <summary>Details</summary>
Motivation: To address concerns about hidden, goal-directed behaviors in LLMs that can shape user beliefs and actions, whether arising from design flaws or adversarial intentions.

Method: Introduces a taxonomy of hidden intentions, demonstrates induction in controlled models, assesses detection methods, and performs stress tests on real-world scenarios. Includes a qualitative case study to showcase manifestation in deployed LLMs.

Result: Detection methods fail in open-world settings due to challenges with false positives and false negatives, highlighting auditing limitations without strong detection frameworks.

Conclusion: The framework provides a foundation to detect and prevent hidden intentions in LLMs, urging the development of robust governance mechanisms and adaptative strategies for emerging threats.

Abstract: LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.

</details>


### [179] [One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization](https://arxiv.org/abs/2601.18572)
*Franziska Weeber,Vera Neplenbroek,Jan Batzner,Sebastian Padó*

Main category: cs.CL

TL;DR: This paper evaluates the effects of sociodemographic persona cues on LLM responses, finding significant variation and recommending the use of multiple cues for robust personalization research.


<details>
  <summary>Details</summary>
Motivation: To address the risks of biases and unfair outcomes caused by sociodemographic personalization in LLMs and the limited exploration of persona cues in this context.

Method: Six persona cues were compared across seven LLMs in four tasks to study the sensitivity of models to variations in prompts and ensure external validity.

Result: The persona cues are highly correlated but induce significant response variance across personas, highlighting the need to assess multiple cues.

Conclusion: Relying on a single persona cue is cautioned against in personalization research, and broader testing with multiple externally valid cues is recommended for unbiased studies.

Abstract: Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.

</details>


### [180] [From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection](https://arxiv.org/abs/2601.18582)
*Yuan Cao,Feixiang Liu,Xinyue Wang,Yihan Zhu,Hui Xu,Zheng Wang,Qiang Qiu*

Main category: cs.CL

TL;DR: The paper proposes viewing personality detection as a ranking task instead of classification and introduces reinforcement learning methods to improve accuracy and address challenges in personality trait analysis.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies in existing classification approaches for analyzing personality traits, such as dependency on expert-crafted knowledge and difficulty in classifying complex personalities.

Method: The method utilizes supervised fine-tuning (SFT) for initializing ranking capabilities and Group Relative Policy Optimization (GRPO) with a tailored reward function to train LLMs for optimal personality trait ranking.

Result: Experimental results confirm that the proposed method achieves state-of-the-art performance across multiple benchmarks for personality detection tasks.

Conclusion: The reinforcement learning-based ranking approach significantly improves personality detection capabilities, providing a solution to inherent challenges in personality assessment and enabling more autonomous and accurate pattern learning.

Abstract: Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.

</details>


### [181] [Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning](https://arxiv.org/abs/2601.18722)
*Lintang Sutawika,Gokul Swamy,Zhiwei Steven Wu,Graham Neubig*

Main category: cs.CL

TL;DR: The paper introduces SP3F, a framework to boost multilingual reasoning in large language models without relying on data in the target languages, yielding significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the issue of poor performance in reasoning tasks by large language models when interacting in languages with limited training data.

Method: The method involves a two-stage process: first, supervise fine-tuning (SFT) on translated English question-answer pairs, followed by reinforcement learning (RL) with privileged pairwise feedback using self-play to refine model responses.

Result: SP3F improves the model significantly across various tasks and languages, outperforming even fully post-trained models with less training data.

Conclusion: SP3F provides a promising solution for enhancing multilingual reasoning capabilities in large language models without needing target language-specific data.

Abstract: When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than
  of the training data across the single-language, multilingual, and generalization to unseen language settings.

</details>


### [182] [HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences](https://arxiv.org/abs/2601.18724)
*Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: This paper investigates the issue of hallucinated citations ('HalluCitation'), identifying their prevalence in recent ACL-related conferences and emphasizing their growing impact on scientific credibility.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the growing concern of hallucinated citations ('HalluCitation') which threaten scientific reliability and the credibility of prestigious conferences.

Method: The authors analyzed all papers presented at ACL, NAACL, and EMNLP from 2024 to 2025, focusing on main conference, Findings, and workshop papers to identify instances of HalluCitations.

Result: The analysis found nearly 300 papers with HalluCitations, especially at EMNLP 2025, where more than half of the identified cases were discovered.

Conclusion: The increasing prevalence of HalluCitations compromises scientific reliability and negatively impacts the reputation of conferences, warranting immediate attention and remedies.

Abstract: Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as "HalluCitation" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.

</details>


### [183] [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730)
*Henry Bell,Caroline Zhang,Mohammed Mobasserul Haque,Dhaval Potdar,Samia Zaman,Brandon Fain*

Main category: cs.CL

TL;DR: This paper introduces \textsc{reflect}, a framework for aligning large language models (LLMs) with constitutional principles during inference, without requiring training or additional data.


<details>
  <summary>Details</summary>
Motivation: Motivated by challenges in existing techniques (such as RLHF), which are computationally expensive, require engineering, tuning, and costly human feedback, the authors seek an efficient and plug-and-play system for aligning LLMs with value-driven principles.

Method: The proposed method, \textsc{reflect}, achieves alignment entirely during inference by combining constitution-conditioned responses with post-generation self-evaluation, self-critique, and final revision, avoiding the necessity of extra model training.

Result: \textsc{reflect} enhances conformance to principles including those not originally emphasized during model fine-tuning. It reduces rare but critical principle violations, sustains factual accuracy, and offers transparent reasoning traces.

Conclusion: The framework proves effective in improving LLM safety and robustness, and generates training data that aids traditional parameter fine-tuning for future scalability and efficiency.

Abstract: The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.

</details>


### [184] [One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment](https://arxiv.org/abs/2601.18731)
*Hongru Cai,Yongqi Li,Tiezheng Yu,Fengbin Zhu,Wenjie Wang,Fuli Feng,Wenjie Li*

Main category: cs.CL

TL;DR: This paper introduces Meta Reward Modeling (MRM) to improve personalized alignment of Large Language Models (LLMs) despite limited feedback and unseen users through meta-learning techniques.


<details>
  <summary>Details</summary>
Motivation: The scarcity of user feedback and need for efficient adaptation to new users are major challenges in creating personalized reward models for LLMs. This necessitates a shift in approach toward learning the adaptation process itself.

Method: The paper reformulates personalized reward modeling as a meta-learning problem, using Model-Agnostic Meta-Learning (MAML) to initialize user-specific reward models for fast adaptation with limited feedback. It incorporates Robust Personalization Objective (RPO) for handling difficult user preferences.

Result: MRM demonstrated enhanced few-shot personalization, improved robustness in addressing hard-to-learn user preferences, and consistently outperformed baseline models in experiments conducted on personalized preference datasets.

Conclusion: MRM provides a promising approach for personalized alignment in LLMs, addressing user preference adaptation under constrained settings and significantly improving alignment performance.

Abstract: Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.

</details>


### [185] [Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory](https://arxiv.org/abs/2601.18771)
*Yanming Liu,Xinyue Peng,Zixuan Yan,Yanxin Shen,Wenjie Xu,Yuefeng Huang,Xinyi Wang,Jiannan Cao,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: This paper introduces Dep-Search, a dependency-aware search framework that enhances large language models' (LLMs) complex reasoning by integrating structured reasoning, retrieval, and memory.


<details>
  <summary>Details</summary>
Motivation: Current search-augmented LLM frameworks heavily rely on implicit natural language reasoning, causing inefficiencies in managing dependencies, reusing knowledge, and learning optimal search strategies.

Method: Dep-Search integrates explicit control mechanisms for question decomposition, retrieval, memory access, and summarization to address gaps in existing search-based frameworks.

Result: Extensive experiments on seven question answering datasets demonstrate significant improvements in LLMs’ multi-hop reasoning capabilities over existing approaches.

Conclusion: Dep-Search effectively advances LLM performance in complex reasoning by systematically addressing fundamental limitations of prior frameworks through its dependency-aware approach.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.

</details>


### [186] [MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts](https://arxiv.org/abs/2601.18790)
*Etienne Lanzeray,Stephane Meilliez,Malo Ruelle,Damien Sileo*

Main category: cs.CL

TL;DR: The paper investigates how optimization of large language models (LLMs) for reasoning might compromise safety, introducing a benchmark called MortalMATH to assess behavior in life-threatening scenarios.


<details>
  <summary>Details</summary>
Motivation: To explore whether the focus on achieving high reasoning and task accuracy in LLMs inadvertently neglects safety considerations in critical contexts.

Method: Developed MortalMATH, a benchmark of 150 scenarios where users require help with math while describing life-threatening emergencies, and tested models like Llama-3.1, Qwen-3-32b, and GPT-5-nano.

Result: Generalist models like Llama-3.1 prioritize safety by refusing to perform tasks, but specialized reasoning models disregard emergencies, maintaining over 95% task completion despite life-critical cues. Computational delays exacerbate risks.

Conclusion: The pursuit of high reasoning accuracy in LLM training might compromise safety mechanisms, indicating the need for balanced prioritization of safety alongside task performance.

Abstract: Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a "tunnel vision" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.

</details>


### [187] [Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets](https://arxiv.org/abs/2601.18791)
*Iaroslav Chelombitko,Mika Hämäläinen,Aleksey Komissarov*

Main category: cs.CL

TL;DR: The paper introduces a methodology for large-scale cross-linguistic comparison across 242 Latin and Cyrillic-script languages using Byte-Pair Encoding (BPE). It explores vocabulary overlap, lexical divergence, and language similarity at scale.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address a lack of unified frameworks for simultaneously analyzing linguistic similarities and differences across diverse languages.

Method: The study employs Byte-Pair Encoding (BPE) to construct rank-based subword vectors from Wikipedia lexicons. The analysis evaluates vocabulary similarity, lexical divergence, and segmentation accuracy with phylogenetic correlations.

Result: BPE segmentation aligns with morpheme boundaries significantly better than a random baseline. BPE vocabulary similarity exhibits strong correlation with genetic language relatedness. Romance languages show highest internal clustering compared to cross-family pairs.

Conclusion: The proposed BPE-based approach provides new macro-linguistic insights into typologically diverse languages, demonstrating its value for systematic linguistic comparison.

Abstract: We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.

</details>


### [188] [ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models](https://arxiv.org/abs/2601.18796)
*Brian Ondov,Chia-Hsuan Chang,Yujia Zhou,Mauro Giuffrè,Hua Xu*

Main category: cs.CL

TL;DR: The research develops a method to align large language models (LLMs) to clinical trial embeddings, creating the ctELM model for describing, comparing, and generating clinical trials.


<details>
  <summary>Details</summary>
Motivation: To improve transparency in text embeddings and unlock generative applications by aligning LLMs to embedding spaces, specifically for clinical trials.

Method: The researchers created a domain-agnostic ELM architecture, training framework, and expert-validated synthetic dataset. They trained task-specialized models and analyzed training regimes.

Result: Their model, ctELM, effectively interprets, compares, and generates clinical trial data, responding to concept vectors such as age and sex of study subjects.

Conclusion: The work enhances the potential of LLM embedding alignment in biomedical fields, providing tools for improved interpretability and generative capabilities in clinical trial analysis.

Abstract: Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [189] [Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility](https://arxiv.org/abs/2601.17027)
*Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu*

Main category: cs.CV

TL;DR: This paper studies the synthesis of scientifically rigorous images for multimodal reasoning, highlighting the shortcomings of current models, and proposes a framework (ImgCoder) for improving their scientific precision.


<details>
  <summary>Details</summary>
Motivation: Existing models for Text-to-Image generation often create visually plausible but scientifically incorrect outputs, limiting their use for scientific reasoning.

Method: The authors systematically study scientific image synthesis, propose ImgCoder (a logic-driven framework), and introduce SciGenBench, a benchmark for evaluating scientific correctness in generated images.

Result: Systematic errors in pixel-based models were revealed, while ImgCoder showed improved structural precision. Fine-tuning large multimodal models on science-verified synthetic images demonstrated consistent reasoning improvements.

Conclusion: High-fidelity scientific image synthesis is critical for achieving significant advancements in multimodal reasoning capabilities, validating the approach as a promising avenue for future development.

Abstract: While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.

</details>


### [190] [Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection](https://arxiv.org/abs/2601.17031)
*Yunhao Xu,Fuquan Zong,Yexuan Xing,Chulong Zhang,Guang Yang,Shilong Yang,Xiaokun Liang,Juan Yu*

Main category: cs.CV

TL;DR: This paper introduces a dual-augmentation framework combining spatial manifold expansion and semantic object injection for effective medical image segmentation with limited data.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation faces challenges in achieving high performance with limited and high-quality annotated data, particularly for complex pathologies like meningiomas.

Method: The framework uses Implicit Neural Representations (INR) for continuous velocity field modeling and a lesion injection module that blends synthetic and real-world pathologies.

Result: The proposed framework significantly boosts the performance, data efficiency, and robustness of models like nnU-Net and U-Mamba on a hybrid dataset.

Conclusion: A novel approach for high-performance medical image segmentation with limited annotations is established, highlighting its potential for improving data efficiency and model robustness.

Abstract: The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.

</details>


### [191] [Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images](https://arxiv.org/abs/2601.17032)
*Wilkie Delgado-Font,Miriela Escobedo-Nicot,Manuel González-Hidalgo,Silena Herold-Garcia,Antoni Jaume-i-Capó,Arnau Mir*

Main category: cs.CV

TL;DR: The paper proposes an automated image analysis method for monitoring red blood cell (RBC) deformation, aiming to improve diagnostics for sickle cell anemia.


<details>
  <summary>Details</summary>
Motivation: To address challenges in manual RBC observation for diseases like sickle cell anemia, including time consumption, high error rates, and the requirement of specialists.

Method: The paper uses peripheral blood smear image analysis where RBCs are segmented using a Chan-Vese active contour model. Shape descriptors (CSF, ESF) and elliptical adjustments are applied to classify RBCs into normal, elongated, or deformed.

Result: The proposed method achieves superior performance compared to state-of-the-art methods, with F-measure values of 0.97 for normal and 0.95 for elongated RBCs.

Conclusion: The study's automated method demonstrates its effectiveness in aiding clinical diagnostics and treatment for sickle cell anemia.

Abstract: Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.

</details>


### [192] [AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs](https://arxiv.org/abs/2601.17037)
*Aahana Basappa,Pranay Goel,Anusri Karra,Anish Karra,Asa Gilmore,Kevin Zhu*

Main category: cs.CV

TL;DR: The paper examines visual reasoning weaknesses in multimodal large language models (MLLMs) and image generation models (IGMs) through a novel benchmark, revealing shared and unique failure modes.


<details>
  <summary>Details</summary>
Motivation: To identify and compare visual reasoning limitations across image-to-text and text-to-image tasks, and understand gaps in vision-language models.

Method: The authors developed AMVICC, a novel benchmark adapting MMVP questions into explicit and implicit prompts, and tested it across 11 MLLMs and 3 IGMs in various visual reasoning categories.

Result: Both MLLMs and IGMs exhibited failures in basic visual reasoning tasks. IGMs showed significant struggles in fine-grained visual manipulation, especially in explicit prompts.

Conclusion: The study highlights shared and model-specific failure modes in visual reasoning tasks and establishes a framework for evaluating cross-modal alignment and improving vision-language modeling.

Abstract: We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.

</details>


### [193] [Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification](https://arxiv.org/abs/2601.17038)
*Obai Alashram,Nejad Alagha,Mahmoud AlKakuri,Zeeshan Swaveel,Abigail Copiaco*

Main category: cs.CV

TL;DR: This study develops a hybrid vision-based pipeline for classifying construction debris, achieving up to 99.5% accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the critical need for sustainable waste management and resource recovery by improving debris classification in the construction industry.

Method: A hybrid approach combining deep feature extraction using the Xception network and classical machine learning classifiers such as SVM, kNN, and Bagged Trees was employed. A dataset of 1,800 real-world images was used to test the methodology.

Result: The proposed system achieved state-of-the-art accuracy of up to 99.5%, outperforming more complex end-to-end deep learning models.

Conclusion: The hybrid pipeline effectively identifies construction debris, proving robust for real-world conditions and offering prospects for integration with automation and robotics.

Abstract: The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.

</details>


### [194] [MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation](https://arxiv.org/abs/2601.17039)
*Junhyuk Heo,Beomkyu Choi,Hyunjin Shin,Darongsae Kwon*

Main category: cs.CV

TL;DR: This paper presents the MANGO dataset, a global resource for mangrove detection utilizing Sentinel-2 imagery to improve monitoring and conservation.


<details>
  <summary>Details</summary>
Motivation: Effective global mangrove conservation requires more reliable monitoring systems, which are hindered by existing datasets' limitations in coverage, accessibility, and representational quality.

Method: This study created the MANGO dataset by acquiring Sentinel-2 imagery from 2020 for mangrove regions globally and aligning observations with annual masks using a targeted detection approach.

Result: The MANGO dataset includes 42,703 labeled image-mask pairs across 124 countries, serving as a standardized resource for mangrove monitoring.

Conclusion: MANGO establishes a benchmark for scalable, reliable global mangrove monitoring and supports advancements in semantic segmentation and conservation efforts.

Abstract: Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.

</details>


### [195] [FP-THD: Full page transcription of historical documents](https://arxiv.org/abs/2601.17040)
*H Neji,J Nogueras-Iso,J Lacasta,MÁ Latre,FJ García-Marco*

Main category: cs.CV

TL;DR: The paper proposes a pipeline for transcribing historical Latin documents from the XV and XVI centuries while preserving their unique characters and symbols.


<details>
  <summary>Details</summary>
Motivation: Maintain the accuracy and original style of historical Latin documents during digitization.

Method: Combine layout analysis and OCR models in a pipeline to process historical document text lines and digitize them.

Result: The pipeline demonstrated efficient transcription on diverse datasets, effectively handling handwritten, printed, and multi-language texts.

Conclusion: The proposed pipeline facilitates historical document transcription while preserving textual integrity and style.

Abstract: The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.

</details>


### [196] [Arabic Sign Language Recognition using Multimodal Approach](https://arxiv.org/abs/2601.17041)
*Ghadeer Alanazi,Abir Benabid*

Main category: cs.CV

TL;DR: The paper presents a multimodal system combining Leap Motion and RGB camera data for recognizing Arabic Sign Language, achieving 78% accuracy with potential for further development.


<details>
  <summary>Details</summary>
Motivation: Existing recognition systems for Arabic Sign Language face challenges due to reliance on single sensors, leading to issues like improper tracking of complex hand orientations and inaccurate 3D movement recognition.

Method: The study employs a multimodal system with Leap Motion data analyzed via a custom dense neural network and RGB data processed through a fine-tuned VGG16 model. Feature representations from both modalities are fused and classified using SoftMax activation.

Result: The system demonstrated an accuracy of 78% on a custom dataset of 18 Arabic Sign Language words, successfully recognizing 13 of them.

Conclusion: The research highlights the potential of multimodal systems for Arabic Sign Language recognition but suggests further optimization and expanded datasets to improve performance.

Abstract: Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.

</details>


### [197] [Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective](https://arxiv.org/abs/2601.17042)
*Tianyuan Liu,Libin Hou,Linyuan Wang,Bin Yan*

Main category: cs.CV

TL;DR: The paper proposes Decoupled Membership-Subspace Attention (DMSA) to improve efficiency and interpretability in visual modeling within transformer architectures.


<details>
  <summary>Details</summary>
Motivation: Current designs tightly couple "membership matrix" and "subspaces U" in MCR2, leading to inefficient redundant coding under incorrect token projection.

Method: The authors decouple functional relationships between "membership matrix" and "subspaces U" in MCR2, optimizing their derivation, and use gradient unrolling to develop the sparse linear attention operator DMSA.

Result: Experimental results show DMSA integrated into Token Statistics Transformer (DMST) achieves faster coding reduction and improves ImageNet-1K top-1 accuracy by up to 1.45%, with significant gains in efficiency and interpretability.

Conclusion: Decoupling of "membership matrix" and "subspace matrix U" and deriving DMSA add both computational efficiency and interpretability to models, outperforming traditional approaches in visual tasks.

Abstract: Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between "membership matrix" and "subspace matrix U" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the "membership matrix" and "subspaces U" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.

</details>


### [198] [Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning](https://arxiv.org/abs/2601.17046)
*Matan Leibovich,Mai Tan,Adria Marcos-Morales,Sreyas Mohan,Peter A. Crozier,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: A novel deep learning approach for extracting 3D atomic-level information from noisy TEM images using semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: To develop a robust methodology for accurately estimating 3D atomic depths from noisy TEM images, addressing current challenges in noise-corrupted data analysis.

Method: The authors formulated depth estimation as a semantic segmentation problem, used a deep convolutional neural network trained with simulated noisy data to generate depth segmentation maps.

Result: Results showed that the method accurately estimated atomic column depths in CeO2 nanoparticles, exhibiting robustness and calibration even with noise.

Conclusion: The proposed approach is effective and reliable for extracting atomic-level depth information from TEM data, offering robustness against noise interference.

Abstract: We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.

</details>


### [199] [A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities](https://arxiv.org/abs/2601.17047)
*Yuanjie Gu,Yiqun Wang,Chaohui Yu,Ang Xuan,Fan Wang,Zhi Lu,Biqin Dong*

Main category: cs.CV

TL;DR: The paper introduces 'Noisomics' to decode imaging noise using the Contrastive Pre-trained (CoP) model, achieving superior performance with minimal data.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of characterizing imaging noise and disentangling signals from complex factors without requiring extensive supervised datasets.

Method: The CoP model employs contrastive learning and synthetic noise genome to distinguish semantic signals from stochastic perturbations, overcoming scaling laws with minimal training data.

Result: With only 100 training samples, CoP outperformed models trained on 100,000 samples, showing better generalization and reducing estimation errors significantly across diverse datasets.

Conclusion: By redefining imaging noise as an informational resource, the framework offers a robust, efficient alternative for precise imaging diagnostics without relying on device-specific calibration.

Abstract: Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce "Noisomics", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.

</details>


### [200] [SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis](https://arxiv.org/abs/2601.17048)
*Jing Jie Tan,Rupert Schreiner,Matthias Hausladen,Ali Asgharzade,Simon Edler,Julian Bartsch,Michael Bachmann,Andreas Schels,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: The paper presents SiMiC, a deep learning model for silicon microstructure characterization using CNNs with attention mechanisms, reducing manual labor and improving accuracy in SEM image analysis.


<details>
  <summary>Details</summary>
Motivation: To address limitations in manual evaluation of silicon microstructures using SEM, which is laborious and inconsistent, affecting quality and throughput.

Method: The authors developed a specialized dataset of silicon field-emitter tip images and trained a customized attention-integrated CNN for morphological feature extraction and dimensional prediction.

Result: SiMiC outperformed classical techniques in accuracy and interpretability for feature analysis, enabling effective multi-class classification and dimensional predictions.

Conclusion: SiMiC bridges the gap between microstructure characterization and field-emission performance, setting a foundation for further data-driven advancements in optimized device designs.

Abstract: Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC

</details>


### [201] [Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support](https://arxiv.org/abs/2601.17049)
*Christina Garcia,Nhat Tan Le,Taihei Fujioka,Umang Dobhal,Milyun Ni'ma Shoumi,Thanh Nha Nguyen,Sozo Inoue*

Main category: cs.CV

TL;DR: This paper summarizes the 'Recognize the Unseen' challenge aimed at detecting unusual behavior in developmental disabilities facilities using pose data.


<details>
  <summary>Details</summary>
Motivation: To address the need for automated unusual behavior detection in individuals with developmental disabilities, utilizing non-invasive data methods.

Method: A pose data competition was organized with teams analyzing skeleton keypoints from videos and evaluated using LOSO and macro-averaged F1 scoring.

Result: The competition showcased challenges in modeling rare behaviors in noisy data, with 40 teams applying diverse ML and DL approaches.

Conclusion: The study advances understanding of behavior recognition, emphasizing the importance of temporal and contextual nuances for responsible AI in healthcare.

Abstract: This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.

</details>


### [202] [Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence](https://arxiv.org/abs/2601.17050)
*Hongjun An,Yiliang Song,Jiawei Shao,Zhe Sun,Xuelong Li*

Main category: cs.CV

TL;DR: The paper introduces SP-VLM, a framework combining single-pixel sensing with vision-language models, offering privacy-conscious monitoring in sensitive areas like restrooms and changing rooms while preserving individuals' anonymity.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of monitoring adverse social interactions, such as bullying and harassment, in privacy-sensitive spaces where traditional surveillance methods are not feasible due to ethical and legal constraints.

Method: The authors propose SP-VLM, which captures low-dimensional single-pixel signals to suppress identity detection while analyzing behavioral patterns through vision-language integration. They validate its ability to perform anomaly detection and activity understanding under severely degraded visual input.

Result: SP-VLM inherently protects identity by making advanced face recognition systems ineffective at a specific sampling rate, while still enabling accurate behavioral detection and analysis, such as people counting and anomaly recognition.

Conclusion: SP-VLM provides a viable solution for safety monitoring that balances timely intervention with upholding privacy, suggesting a new path forward for ethical surveillance in sensitive environments.

Abstract: Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.

</details>


### [203] [Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults](https://arxiv.org/abs/2601.17053)
*Shuhao Que,Dieuwke van Dartel,Ilse Heeringa,Han Hegeman,Miriam Vollenbroek-Hutten,Ying Wang*

Main category: cs.CV

TL;DR: This paper focuses on developing a human activity recognition system tailored for older adults in hip fracture rehabilitation, addressing the limitations of existing wearable trackers with synthetic data guidance to improve activity classification.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve physical activity quantification for older adults during hip fracture rehabilitation, combating long-term functional decline and addressing the unreliability of current wearable systems in this age group.

Method: The study utilized accelerometers on the lower back and thigh, simulated free-living conditions, and employed synthetic data-guided feature intervention modeling evaluated through cross-validation.

Result: The developed system achieved high reliability in activity recognition (F1-scores: walking 0.896, standing 0.927, sitting 0.997, lying down 0.937, postural transfers 0.816), particularly improving detection of postural transfers.

Conclusion: Preliminary results indicate the potential for robust activity recognition in older adults. Further validation in actual hip fracture patients is needed for real-world application.

Abstract: Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.

</details>


### [204] [Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring](https://arxiv.org/abs/2601.17056)
*Zahra Vaseqi,James Clark*

Main category: cs.CV

TL;DR: The paper addresses challenges in egocentric video action recognition amidst domain shifts by introducing Ego4OOD, a benchmark emphasizing covariate diversity while controlling concept shift. It demonstrates the efficacy of a binary training objective, achieving competitive results with simpler models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve egocentric video action recognition by tackling large intra-class variability, feature distribution imbalances, and action-environment correlations, while providing a reliable evaluation framework through controlled benchmarks.

Method: The authors propose Ego4OOD, a benchmark emphasizing covariate diversity and low concept shift. They employ a one-vs-all binary training objective to decompose action recognition tasks and analyze performance using a covariate shift metric.

Result: A lightweight two-layer fully connected network, guided by the proposed binary objective, achieves competitive performance with state-of-the-art methods on Argo1M and Ego4OOD benchmarks.

Conclusion: The paper underscores the value of controlled benchmarks like Ego4OOD for studying domain generalization, highlighting the relationship between covariate shift and performance. The proposed approach offers effective recognition with minimal resource requirements.

Abstract: Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.

</details>


### [205] [Feature-Space Generative Models for One-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.17905)
*Jack Foster,Kirill Paramonov,Mete Ozay,Umberto Michieli*

Main category: cs.CV

TL;DR: The paper introduces Gen1S, an approach to improve few-shot class-incremental learning (FSCIL) with only one sample per class, focusing on generalization without additional training or model changes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of recognizing novel classes in FSCIL with limited data (1-shot per class) while avoiding further training or altering the base model.

Method: The method maps embeddings into a residual space using class prototypes and employs generative models (VAE/diffusion) to learn the multi-modal distribution of residuals as structural priors for novel class recognition.

Result: Gen1S demonstrates consistent improvements in recognizing novel classes compared to state-of-the-art methods across various benchmarks and architectures.

Conclusion: The structural similarity between base and novel class embeddings, coupled with generative modeling, provides effective generalization to novel classes in FSCIL scenarios.

Abstract: Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.

</details>


### [206] [A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing](https://arxiv.org/abs/2601.17062)
*Robert M. Belcher,Brendan C. Degryse,Leonard R. Kosta,Christopher J. Lowrance*

Main category: cs.CV

TL;DR: The paper introduces a computer vision system using YOLOv8 and IoU analysis for automated bullet hole detection and differentiation during rifle zeroing, with 97.0% accuracy in detection and 88.8% accuracy in iteration tracking.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome delays and human errors during rifle zeroing caused by the need for physical inspection and range safety protocols.

Method: The paper proposes a method combining YOLOv8 for small-object detection, IoU analysis for differentiation across images, and a novel data augmentation technique to simulate firing sequences. Additionally, it uses ORB-based perspective correction for standardizing target orientation.

Result: The system achieves 97.0% mean average precision for bullet hole detection and 88.8% accuracy in assigning bullet holes to firing iterations.

Conclusion: The framework provides an efficient, automated solution for rifle zeroing while offering potential applications in other domains requiring temporal object differentiation.

Abstract: Adjusting rifle sights, a process commonly called "zeroing," requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.

</details>


### [207] [A Mechanistic View on Video Generation as World Models: State and Dynamics](https://arxiv.org/abs/2601.17067)
*Luozhou Wang,Zhifei Chen,Yihua Du,Dongyu Yan,Wenhang Ge,Guibao Shen,Xinli Xu,Leyi Wu,Man Chen,Tianshuo Xu,Peiran Ren,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CV

TL;DR: This paper introduces a taxonomy for video generation models based on state construction and dynamics modeling, aiming to evolve models from visual fidelity to functional world simulators.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the gap between modern 'stateless' video generation and classic state-centric theories, emphasizing the need for models with physical coherence and causal reasoning capabilities.

Method: The paper develops a taxonomy focusing on State Construction (implicit and explicit paradigms) and Dynamics Modeling (knowledge integration and architectural reformulation), advocating for new evaluation metrics beyond visual fidelity.

Result: It identifies critical challenges in enhancing physical persistence and causality in video models, proposing directions like data-driven memory, compressed fidelity, latent decoupling, and reasoning-prior integration.

Conclusion: Advancing these models can transform them into robust world simulators capable of functional and causal reasoning, surpassing current visual plausibility benchmarks.

Abstract: Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.

</details>


### [208] [Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing](https://arxiv.org/abs/2601.18252)
*Chao Wang,Xuanying Li,Cheng Dai,Jinglei Feng,Yuxiang Luo,Yuqi Ouyang,Hao Qin*

Main category: cs.CV

TL;DR: The paper introduces Co-PLNet, a collaborative framework for wireframe parsing that improves accuracy, robustness, and real-time efficiency by exchanging spatial cues between line and junction tasks.


<details>
  <summary>Details</summary>
Motivation: Existing wireframe parsing methods struggle with mismatches and robustness by separately predicting lines and junctions and reconciling them later.

Method: Co-PLNet utilizes a Point-Line Prompt Encoder (PLP-Encoder) to convert spatial cues into compact maps and a Cross-Guidance Line Decoder (CGL-Decoder) for refining predictions with sparse attention.

Result: Experiments demonstrated consistent accuracy and robustness improvements on datasets like Wireframe and YorkUrban, along with favorable real-time performance.

Conclusion: The proposed framework successfully enhances structured geometry perception with improved efficiency and task consistency.

Abstract: Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.

</details>


### [209] [Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances](https://arxiv.org/abs/2601.17071)
*Jisui Huang,Andreas Alpers,Ke Chen,Na Lei*

Main category: cs.CV

TL;DR: The paper proposes an efficient image segmentation method using a two-level clustering process involving superpixels and object-level segments, based on distributional optimal transport (OT).


<details>
  <summary>Details</summary>
Motivation: To address the challenge of image segmentation in the presence of strong inhomogeneities and improve accuracy while maintaining efficiency.

Method: A two-level clustering approach: pixels are grouped into superpixels via a least-squares assignment problem related to optimal transport, and superpixels are merged into segments using 2-Wasserstein distance between distributions.

Result: Enhanced segmentation accuracy on complex images with high computational efficiency.

Conclusion: The framework's use of distributional OT distances improves segmentation performance and unifies the clustering process mathematically.

Abstract: We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.

</details>


### [210] [GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars](https://arxiv.org/abs/2601.17088)
*Rui-Yang Ju,Jen-Shiun Chiang*

Main category: cs.CV

TL;DR: The paper introduces GlassesGB, a framework that allows users to customize 3D eyewear for VR head avatars, bridging the gap between 2D design and 3D rendering.


<details>
  <summary>Details</summary>
Motivation: Current virtual try-on (VTON) systems lack support for fine-grained, user-driven customization, and existing methods like GlassesGAN are confined to 2D image generation rather than 3D avatar adaptation.

Method: The authors integrate GlassesGAN for personalized 2D eyewear design with 3D Gaussian Blendshapes for head reconstruction to create GlassesGB, enabling customizable 3D eyewear generation.

Result: GlassesGB enables the generation of personalized and customizable 3D eyewear that can be rendered on head avatars for VR applications. The implementation code is provided for public access.

Conclusion: The proposed GlassesGB framework overcomes limitations of previous methods by facilitating fine-grained and personalized eyewear customization in a 3D context suitable for VR scenarios.

Abstract: Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.

</details>


### [211] [GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing](https://arxiv.org/abs/2601.17089)
*Qigan Sun,Chaoning Zhang,Jianwei Zhang,Xudong Wang,Jiehui Xie,Pengcheng Zheng,Haoyu Wang,Sungyoung Lee,Chi-lok Andy Tai,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: The paper proposes a parameter-efficient fine-tuning method called Guided Region-Aware Sparse Prompting (GRASP) to improve visual question answering on remote sensing (RS) images.


<details>
  <summary>Details</summary>
Motivation: Challenges arise in leveraging multimodal large language models (MLLMs) for remote sensing images due to issues like overfitting, sparse target distributions, and complex semantics.

Method: The GRASP approach uses spatially structured soft prompts linked to visual token grids and employs a question-guided sparse fusion mechanism to focus on task-relevant regions while filtering noise.

Result: Experiments on multiple RSVQA benchmarks demonstrate that GRASP delivers competitive performance compared to existing methods with greater parameter efficiency.

Conclusion: GRASP effectively addresses the challenges of visual question answering for RS images by enabling the focus on relevant regions and maintaining efficiency.

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.

</details>


### [212] [LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation](https://arxiv.org/abs/2601.17095)
*Xusheng Du,Athiwat Kongkaeo,Ye Zhang,Haoran Xie*

Main category: cs.CV

TL;DR: The paper introduces an automatic framework for generating consistent multi-level architectural designs using generative AI models to address limitations of manual processes and lack of training data.


<details>
  <summary>Details</summary>
Motivation: Manual LoD modeling processes in architecture are inefficient and inconsistent. The lack of quality paired data for training AI models limits generative AI's application in creating multi-level architectural designs.

Method: The authors develop an AI-based framework that progressively extracts and simplifies architectural models into multiple Levels of Detail (LoD) while preserving geometric consistency and structure using computer vision and generative AI techniques.

Result: The framework achieves significant geometric consistency across LoD transitions, evidenced by high SSIM values (0.7319 for LoD3-LoD2, 0.7532 for LoD2-LoD1) and manageable geometric deviations (Hausdorff distances of 25.1% and 61.0% of image diagonal).

Conclusion: This framework proves effective in generating geometrically consistent and hierarchically coherent multi-LoD representations, setting a foundation for AI-driven architectural modeling.

Abstract: For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.

</details>


### [213] [Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals](https://arxiv.org/abs/2601.17103)
*Pascaline André,Charles Heitz,Evangelia Christodoulou,Annika Reinke,Carole H. Sudre,Michela Antonelli,Patrick Godau,M. Jorge Cardoso,Antoine Gilson,Sophie Tezenas du Montcel,Gaël Varoquaux,Lena Maier-Hein,Olivier Colliot*

Main category: cs.CV

TL;DR: The study examines how confidence intervals (CIs) behave in medical imaging AI validation, conducting a large-scale analysis to provide insights for creating more reliable and precise methods in reporting performance uncertainty.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gap in understanding and evaluating the behavior of confidence intervals (CIs) in medical imaging AI, which is critical for ensuring accurate performance validation and clinical applicability.

Method: The study involved analyzing data across 24 segmentation and classification tasks, using 19 models per task group, diverse performance metrics, multiple aggregation strategies, and several CI methods to measure their reliability and precision.

Result: The study uncovered five key findings, including the varying sample size requirements for CIs, the impact of performance metrics and aggregation strategies on reliability, the influence of machine learning problems, and the varying reliability of CI methods depending on the use case.

Conclusion: The findings offer a foundation for creating comprehensive guidelines for reporting performance uncertainty in medical imaging AI, thus fostering more reliable clinical translation and validation.

Abstract: Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.

</details>


### [214] [StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors](https://arxiv.org/abs/2601.17107)
*Qinkai Yu,Chong Zhang,Gaojie Jin,Tianjin Huang,Wei Zhou,Wenhui Li,Xiaobo Jin,Bo Huang,Yitian Zhao,Guang Yang,Gregory Y. H. Lip,Yalin Zheng,Aline Villavicencio,Yanda Meng*

Main category: cs.CV

TL;DR: StealthMark is proposed as a novel method to stealthily embed ownership verification in medical segmentation models without hindering their performance, using model uncertainty modulation and QR code-based watermarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of annotating medical data for AI training, which is costly, limited, and faces privacy issues. There's a need for robust protection mechanisms for medical segmentation models, which are valuable intellectual property.

Method: StealthMark introduces a method that modulates model uncertainty without altering segmentation outputs. Model-agnostic explanation methods like LIME are used to extract attributions, embedding a distinctive watermark designed as a QR code for ownership verification.

Result: Experiments on four medical imaging datasets and five segmentation models demonstrated StealthMark's effectiveness, achieving over 95% ASR with negligible drops in performance metrics like Dice and AUC scores.

Conclusion: StealthMark provides an effective and practical solution for watermarking medical segmentation models, ensuring ownership verification without sacrificing model performance. It outperforms existing backdoor-based methods and is suited for real-world deployment.

Abstract: Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.

</details>


### [215] [iFSQ: Improving FSQ for Image Generation with 1 Line of Code](https://arxiv.org/abs/2601.17124)
*Bin Lin,Zongjian Li,Yuwei Niu,Kaixiong Gong,Yunyang Ge,Yunlong Lin,Mingzhe Zheng,JianWei Zhang,Miles Yang,Zhao Zhong,Liefeng Bo,Li Yuan*

Main category: cs.CV

TL;DR: This paper introduces iFSQ, a simple enhancement of Finite Scalar Quantization (FSQ) for image generation, aiming to unify discrete and continuous approaches while ensuring optimal bin utilization and reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: The discrepancy between VQ-VAEs and VAEs creates challenges for unified image generation modeling and fair benchmarking. The authors aim to bridge this gap while addressing the shortcomings of the original FSQ model.

Method: The authors propose iFSQ by replacing FSQ's activation function with a distribution-matching mapping, ensuring a uniform prior. This modification resolves issues of activation collapse and achieves better quantization and reconstruction.

Result: iFSQ demonstrates that an optimal balance between discrete and continuous representations is at 4 bits per dimension. Additionally, AR models converge faster initially, while diffusion models deliver higher performance limits.

Conclusion: The work simplifies FSQ, unifies modeling approaches, and highlights trade-offs in AR and diffusion models. It also enhances AR models with REPA, introducing LlamaGen-REPA for practical improvements.

Abstract: The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ

</details>


### [216] [Scaling medical imaging report generation with multimodal reinforcement learning](https://arxiv.org/abs/2601.17151)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Yu Gu,Ying Jin,Sam Preston,Yanbo Xu,Sid Kiblawi,Wen-wai Yim,Tim Ossowski,Tristan Naumann,Mu Wei,Hoifung Poon*

Main category: cs.CV

TL;DR: Universal Report Generation (UniRG), a framework for medical imaging report generation, uses reinforcement learning to improve generalization and sets new state-of-the-art performance in chest X-ray report generation.


<details>
  <summary>Details</summary>
Motivation: To address the persistent gaps in multimodal understanding and reasoning of frontier models, particularly in generating medical imaging reports that involve high-value and complex verticals like biomedicine.

Method: UniRG leverages reinforcement learning to directly optimize evaluation metrics relevant to end applications, improving upon supervised fine-tuning approaches in a generalizable framework.

Result: UniRG-CXR, trained on publicly available chest X-ray data, achieves new state-of-the-art results on the ReXrank benchmark, significantly outperforming previous methods.

Conclusion: The study demonstrates that reinforcement learning can be effectively employed in medical imaging report generation, producing robust improvements and generalized performance across varying clinical settings.

Abstract: Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.

</details>


### [217] [LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction](https://arxiv.org/abs/2601.17185)
*Shima Salehi,Atharva Agashe,Andrew J. McFarland,Joshua Peeples*

Main category: cs.CV

TL;DR: The paper introduces a novel few-shot 3D reconstruction method combining frequency regularization and releasing a multispectral greenhouse dataset alongside an open-source benchmarking package.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of 3D Gaussian Splatting models in handling sparse-view conditions and lack of dataset/tools for standardized few-shot reconstruction evaluation.

Method: Integrates global and local frequency regularization to stabilize geometry and preserve fine details, supported by a diverse multispectral greenhouse dataset and benchmarking protocols.

Result: The proposed method produces sharper, stable, and spectrally consistent reconstructions, validated through experiments on both the new and standard datasets.

Conclusion: The method and dataset advance few-shot 3D reconstruction, providing a reliable tool for enhanced spectral and geometric precision in sparse-view settings.

Abstract: We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available

</details>


### [218] [Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments](https://arxiv.org/abs/2601.17194)
*Cheyu Lin,Katherine A. Flanigan,Sirajum Munir*

Main category: cs.CV

TL;DR: The paper introduces the DUET dataset and a kinesics recognition framework for analyzing social interactions in built environments, addressing methodological gaps with privacy-preserving solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of consistent and privacy-preserving methods to measure socially meaningful interactions in built environments, limiting design evaluation and alignment with social capital theory.

Method: The authors present the DUET dataset, containing 12 dyadic kinesic interactions using multiple sensing modalities, and develop a recognition framework to infer communicative functions based on skeletal motion and transfer-learning techniques.

Result: DUET enables benchmarking of human activity recognition models, demonstrating challenges of communicative-function recognition and structured clustering of kinesic functions with effective generalization.

Conclusion: The work provides a novel approach for privacy-preserving social interaction analysis using DUET, enabling consistency in evaluating social interaction in civil and built-environment research.

Abstract: Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize "interaction" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.

</details>


### [219] [Structural Complexity of Brain MRI reveals age-associated patterns](https://arxiv.org/abs/2601.17211)
*Anzhe Cheng,Italo Ivo Lima Dias Pinto,Paul Bogdan*

Main category: cs.CV

TL;DR: This paper introduces a refined method for analyzing the structural complexity of brain MRI data, showing that structural complexity decreases with age and demonstrating its predictive power for estimating biological age.


<details>
  <summary>Details</summary>
Motivation: The authors aim to develop a reliable and robust framework for multiscale analysis of 3D imaging data, addressing instability issues in traditional methods and exploring its application in the context of aging.

Method: They propose a sliding-window coarse-graining scheme to improve the stability and robustness of structural complexity analysis of volumetric brain MRI data across progressively larger spatial scales.

Result: The analysis of large MRI datasets revealed that structural complexity decreases systematically with age, particularly at coarser scales, suggesting aging-related changes in brain organization.

Conclusion: Structural complexity is a valuable tool for multiscale 3D imaging analysis, offering insights into brain aging and potential for predicting biological age from MRI data.

Abstract: We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.

</details>


### [220] [Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction](https://arxiv.org/abs/2601.17216)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.CV

TL;DR: The paper presents a semantic V2X framework for real-time collision prediction in Intelligent Transportation Systems (ITS), showing significant communication reduction while improving predictive accuracy through semantic embeddings.


<details>
  <summary>Details</summary>
Motivation: Current ITS collision prediction methods rely on transmitting raw video or high-dimensional sensory data, which is inefficient under vehicular communication constraints.

Method: The authors propose using RSU-mounted cameras to generate spatiotemporal semantic embeddings of future traffic frames using the Video Joint Embedding Predictive Architecture (V-JEPA), and transmitting these embeddings via V2X links to vehicles for collision prediction.

Result: The framework demonstrates a 10% F1-score improvement in collision prediction and reduces transmission requirements by four orders of magnitude compared to raw video.

Conclusion: Semantic V2X communication effectively reduces bandwidth requirements while enhancing predictive accuracy, enabling real-time collision prediction in ITS.

Abstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.

</details>


### [221] [Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification](https://arxiv.org/abs/2601.17228)
*Tengyue Zhang,Ruiwen Ding,Luoting Zhuang,Yuxiao Wu,Erika F. Rodriguez,William Hsu*

Main category: cs.CV

TL;DR: This paper presents a semi-supervised domain adaptation (SSDA) framework using a latent diffusion model to preserve tissue structure and introduce target-domain traits, improving generalization in computational pathology.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the issue of deep learning models failing to generalize across domains in computational pathology due to domain shifts, using a method that avoids the limitations of existing solutions such as distortion of tissue structures.

Method: The paper proposes an SSDA framework that employs a latent diffusion model trained on unlabeled data from source and target domains, generating synthetic images that preserve both tissue structures and target-domain characteristics through conditioning on foundation model features, cohort identity, and preparation method.

Result: The method demonstrated improved domain generalization specifically in lung adenocarcinoma prognostication, increasing the weighted F1 score from 0.611 to 0.706 and macro F1 score from 0.641 to 0.716 on the target cohort, without affecting the source-cohort performance.

Conclusion: The target-aware diffusion-based synthetic data augmentation effectively enhances domain generalization, showing promise in addressing computational pathology challenges like domain shift.

Abstract: Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.

</details>


### [222] [C-RADIOv4 (Tech Report)](https://arxiv.org/abs/2601.17237)
*Mike Ranzinger,Greg Heinrich,Collin McCarthy,Jan Kautz,Andrew Tao,Bryan Catanzaro,Pavlo Molchanov*

Main category: cs.CV

TL;DR: C-RADIOv4 models integrate capabilities of multiple teacher models to deliver improved performance and efficiency in vision tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance vision backbones by uniting the strengths of multiple teacher models into a single student model, improving task performance while maintaining computational efficiency.

Method: It utilizes multi-teacher distillation from SigLIP2, DINOv3, and SAM3 to design C-RADIOv4 models, offering updated capabilities and performance improvements over its predecessor models.

Result: C-RADIOv4 achieves better core metrics, supports any resolution, reintroduces ViTDet for high-resolution efficiency, and offers a permissive license.

Conclusion: The C-RADIOv4 family demonstrates robust enhancements and expanded capabilities, making it a strong choice for high-resolution and efficient vision tasks.

Abstract: By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.

</details>


### [223] [Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization](https://arxiv.org/abs/2601.17254)
*Takato Yasuno*

Main category: cs.CV

TL;DR: The paper introduces an open-source system for bridge damage detection employing advanced machine learning techniques while safeguarding regional privacy.


<details>
  <summary>Details</summary>
Motivation: Efficiently monitor civil infrastructure in Japan with a focus on visual inspections every five years while addressing regional privacy concerns and optimizing damage detection.

Method: The system uses SAM3 for rebar corrosion detection, DBSCAN for auto-completion of missed regions, Gaussian blur for privacy protection, and preprocessing techniques to enhance OCR accuracy, all implemented on GPU for speed.

Result: Achieved 1.7-second image processing, effective damage detection, and regional privacy protection using PyTorch, OpenCV, pytesseract, SAM3, and scikit-learn.

Conclusion: The system provides a privacy-preserving, efficient, and accurate solution for automated bridge damage inspection and repair assessment.

Abstract: In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.

</details>


### [224] [FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding](https://arxiv.org/abs/2601.17258)
*João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: The paper introduces FineVAU, a benchmark for evaluating Video Anomaly Understanding (VAU) using fine-grained and human-aligned metrics.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing evaluation methods in Video Anomaly Understanding, including their inability to assess fine-grained visual elements or their misalignment with human perception.

Method: The authors propose FineVAU with two main components: FVScore, a novel metric aligned with human perception for evaluating anomalies, and FineW3, a comprehensive dataset enriched with fine-grained visual annotations.

Result: Human evaluation demonstrates that FVScore aligns better with human perception of video anomalies than existing metrics. Experiments reveal weaknesses in LVLMs for detecting spatial and temporal anomalies.

Conclusion: FineVAU improves VAU evaluation by introducing human-aligned benchmarks and highlighting LVLM limitations in anomaly detection.

Abstract: Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.

</details>


### [225] [Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling](https://arxiv.org/abs/2601.17259)
*Angad Singh Ahuja,Aarush Ram Anandh*

Main category: cs.CV

TL;DR: The paper addresses the issue of precise color control in text-to-image diffusion systems using a training-free, inference-time method incorporating advanced loss functions and ROI-based approaches.


<details>
  <summary>Details</summary>
Motivation: To solve the persistent issue of color control in diffusion systems, particularly for workflows requiring user-specified explicit color adherence.

Method: An inference-time method combining ROI-inpainting, latent nudging with gradient guidance, and mechanisms like CVaR-style loss to manage pixelwise errors, all without the need for additional training.

Result: The proposed method significantly improves color adherence while overcoming limitations of mean-only baselines, ensuring stable guidance through denoising steps.

Conclusion: The method effectively integrates into Stable Diffusion pipelines, providing a practical solution for design-oriented workflows where precise color adherence is a critical requirement.

Abstract: Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.

</details>


### [226] [Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales](https://arxiv.org/abs/2601.17271)
*Kun Huang,Fang-Lue Zhang,Neil Dodgson*

Main category: cs.CV

TL;DR: The paper presents Cross360, a cross-attention-based method combining local tangent patch and global equirectangular features to improve 360° depth estimation.


<details>
  <summary>Details</summary>
Motivation: Current challenges in 360° depth estimation arise from balancing global and local consistency due to distorted spherical representations and boundary discrepancies in feature extraction.

Method: Cross360 utilizes a Cross Projection Feature Alignment module leveraging cross-attention for aligning local tangent features with equirectangular representations and a Progressive Feature Aggregation module for refining multi-scale features.

Result: Cross360 outperforms existing methods across benchmark datasets, accurately estimating depth with improved global consistency.

Conclusion: Cross360 effectively tackles the challenges of 360° depth estimation, integrating local and global features for higher precision and consistency, and its approach is validated on several datasets.

Abstract: 360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.

</details>


### [227] [Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing](https://arxiv.org/abs/2601.17288)
*Jin Bai,Huiyao Zhang,Qi Wen,Shengyang Li,Xiaolin Tian,Atta ur Rahman*

Main category: cs.CV

TL;DR: Fluxamba is a lightweight, topology-aware model for segmenting geological features with high efficiency, achieving state-of-the-art results on key benchmarks.


<details>
  <summary>Details</summary>
Motivation: The segmentation of geological linear features is challenging due to their complex, curvilinear topologies, and conventional State Space Models fail to adequately capture this due to their rigid dependencies on axis-aligned scanning trajectories.

Method: Fluxamba introduces the Structural Flux Block (SFB) with Anisotropic Structural Gate (ASG) and Prior-Modulated Flow (PMF) for dynamic feature aggregation along curvilinear geometries. It also includes a Hierarchical Spatial Regulator (HSR) and High-Fidelity Focus Unit (HFFU) for enhancing feature alignment and signal-to-noise ratio.

Result: Fluxamba achieves state-of-the-art segmentation performance, including an F1-score of 89.22% and mIoU of 89.87% on the LROC-Lineament dataset, and runs in real-time (24 FPS) while being highly lightweight (3.4M parameters, 6.3G FLOPs).

Conclusion: Fluxamba successfully balances high segmentation performance and computational efficiency, making it suitable for onboard deployment in resource-constrained environments.

Abstract: The precise segmentation of geological linear features, spanning from planetary lineaments to terrestrial fractures, demands capturing long-range dependencies across complex anisotropic topologies. Although State Space Models (SSMs) offer near-linear computational complexity, their dependence on rigid, axis-aligned scanning trajectories induces a fundamental topological mismatch with curvilinear targets, resulting in fragmented context and feature erosion. To bridge this gap, we propose Fluxamba, a lightweight architecture that introduces a topology-aware feature rectification framework. Central to our design is the Structural Flux Block (SFB), which orchestrates an anisotropic information flux by integrating an Anisotropic Structural Gate (ASG) with a Prior-Modulated Flow (PMF). This mechanism decouples feature orientation from spatial location, dynamically gating context aggregation along the target's intrinsic geometry rather than rigid paths. Furthermore, to mitigate serialization-induced noise in low-contrast environments, we incorporate a Hierarchical Spatial Regulator (HSR) for multi-scale semantic alignment and a High-Fidelity Focus Unit (HFFU) to explicitly maximize the signal-to-noise ratio of faint features. Extensive experiments on diverse geological benchmarks (LROC-Lineament, LineaMapper, and GeoCrack) demonstrate that Fluxamba establishes a new state-of-the-art. Notably, on the challenging LROC-Lineament dataset, it achieves an F1-score of 89.22% and mIoU of 89.87%. Achieving a real-time inference speed of over 24 FPS with only 3.4M parameters and 6.3G FLOPs, Fluxamba reduces computational costs by up to two orders of magnitude compared to heavy-weight baselines, thereby establishing a new Pareto frontier between segmentation fidelity and onboard deployment feasibility.

</details>


### [228] [PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation](https://arxiv.org/abs/2601.17885)
*Qingyu Fan,Zhaoxiang Li,Yi Lu,Wang Chen,Qiu Shen,Xiao-xiao Long,Yinghao Cai,Tao Lu,Shuo Wang,Xun Cao*

Main category: cs.CV

TL;DR: The paper introduces PEAfowl, a novel multi-view vision-language-action policy for stable bimanual manipulation in cluttered scenes, addressing issues with depth perception and instruction grounding.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with stable performance under occlusions, viewpoint changes, and poor generalization due to weak 3D spatial understanding and coarse language grounding.

Method: PEAfowl enhances spatial reasoning using depth distributions, 3D lifting, and local cross-view aggregation. For instruction grounding, it uses a text-aware readout over CLIP features and distills depth information from a pretrained teacher during training.

Result: PEAfowl showed a 23% improvement in success rate over baselines in simulated settings and demonstrated reliable sim-to-real transfer in real-robot experiments.

Conclusion: PEAfowl effectively addresses challenges in perception and instruction grounding, significantly improving policy performance and generalizability for bimanual manipulation tasks.

Abstract: Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.

</details>


### [229] [Dynamic Meta-Ensemble Framework for Efficient and Accurate Deep Learning in Plant Leaf Disease Detection on Resource-Constrained Edge Devices](https://arxiv.org/abs/2601.17290)
*Weloday Fikadu Moges,Jianmei Su,Amin Waqas*

Main category: cs.CV

TL;DR: This paper introduces the Dynamic Meta-Ensemble Framework (DMEF) for plant disease detection, achieving high accuracy with efficient resource usage on edge devices.


<details>
  <summary>Details</summary>
Motivation: Edge devices used in agriculture face computational and energy limitations, making it difficult to deploy accurate AI-based plant disease detection solutions.

Method: DMEF combines three lightweight neural networks (MobileNetV2, NASNetMobile, InceptionV3) using an adaptive weighting mechanism during training to balance accuracy and efficiency.

Result: The framework achieved classification accuracies of 99.53% and 96.61% on potato and maize disease datasets, surpassing existing models by up to 6.3%.

Conclusion: DMEF is effective in resource-constrained environments, enabling scalable and accurate plant disease detection for edge-based agricultural applications.

Abstract: Deploying deep learning models for plant disease detection on edge devices such as IoT sensors, smartphones, and embedded systems is severely constrained by limited computational resources and energy budgets. To address this challenge, we introduce a novel Dynamic Meta-Ensemble Framework (DMEF) for high-accuracy plant disease diagnosis under resource constraints. DMEF employs an adaptive weighting mechanism that dynamically combines the predictions of three lightweight convolutional neural networks (MobileNetV2, NASNetMobile, and InceptionV3) by optimizing a trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). During training, the ensemble weights are updated iteratively, favoring models exhibiting high performance and low complexity. Extensive experiments on benchmark datasets for potato and maize diseases demonstrate state-of-the-art classification accuracies of 99.53% and 96.61%, respectively, surpassing standalone models and static ensembles by 2.1% and 6.3%. With computationally efficient inference latency (<75ms) and a compact footprint (<1 million parameters), DMEF shows strong potential for edge-based agricultural monitoring, suggesting viability for scalable crop disease management. This bridges the gap between high-accuracy AI and practical field applications.

</details>


### [230] [Masked Depth Modeling for Spatial Perception](https://arxiv.org/abs/2601.17895)
*Bin Tan,Changjiang Sun,Xiage Qin,Hanat Adai,Zelin Fu,Tianxiang Zhou,Han Zhang,Yinghao Xu,Xing Zhu,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: This paper introduces LingBot-Depth, a model for refining depth maps using RGB-D data overcoming hardware and environmental limitations, while demonstrating enhanced depth precision and modality alignment.


<details>
  <summary>Details</summary>
Motivation: Despite the potential of RGB-D cameras, capturing accurate pixel-aligned metric depth remains hindered by hardware limitations and challenging environments, motivating the need for improved depth completion methods.

Method: LingBot-Depth employs masked depth modeling, leveraging visual context for depth refinement, combined with automated data pipeline curation for scalable and efficient training.

Result: LingBot-Depth outperformed leading RGB-D cameras in depth precision and pixel coverage, showing superiority across tasks with aligned latent representations of RGB-depth modalities.

Conclusion: LingBot-Depth provides a robust solution to depth completion challenges and releases valuable resources for the community, contributing significantly to spatial perception advancements.

Abstract: Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as "masked" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.

</details>


### [231] [ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17315)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: This paper proposes ClinNet, a novel framework for knee osteoarthritis (KOA) grading using radiographic images, addressing the challenges of subtle differences, annotation uncertainty, and ordinal progression of the disease.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and trustworthiness of KOA grading while acknowledging the uncertainties in expert annotations and the ordinal nature of disease progression.

Method: ClinNet combines a Bilateral Asymmetry Encoder for modeling structural discrepancies, a Diagnostic Memory Bank for maintaining robust feature representations, and an Evidential Ordinal Head based on the Normal-Inverse-Gamma distribution to estimate KL grades and uncertainty.

Result: ClinNet achieves superior performance with a Quadratic Weighted Kappa of 0.892 and an Accuracy of 0.768, significantly outperforming existing methods. Its uncertainty estimates effectively identify out-of-distribution samples and misdiagnoses.

Conclusion: ClinNet demonstrates high accuracy and trustworthiness in KOA grading, offering a promising solution for safe clinical deployment.

Abstract: Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.

</details>


### [232] [SkyReels-V3 Technique Report](https://arxiv.org/abs/2601.17323)
*Debang Li,Zhengcong Fei,Tuanhui Li,Yikun Dou,Zheng Chen,Jiangping Yang,Mingyuan Fan,Jingtao Xu,Jiahua Wang,Baoxuan Gu,Mingshan Chang,Yuqiang Xie,Binjie Mao,Youqiang Zhang,Nuo Pang,Hao Zhang,Yuzhe Jin,Zhiheng Xu,Dixuan Lin,Guibin Chen,Yahui Zhou*

Main category: cs.CV

TL;DR: SkyReels-V3 is a conditional video generation model designed for multimodal in-context learning, supporting reference-to-video synthesis, video extension, and audio-guided video generation.


<details>
  <summary>Details</summary>
Motivation: To advance building world models through multimodal contextual inference, improving video generation capabilities in fidelity, coherence, and adherence.

Method: The model employs diffusion Transformers and a multimodal learning framework, with processes like hybrid optimization strategies and intelligent cinematographic pattern simulations.

Result: SkyReels-V3 demonstrates state-of-the-art visual quality, instruction adherence, and robust multimodal video generation capabilities.

Conclusion: SkyReels-V3 represents significant progress in multimodal video generation, offering versatile generative functionalities within a unified architecture that rivals leading systems.

Abstract: Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.
  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.

</details>


### [233] [Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning](https://arxiv.org/abs/2601.18714)
*Judith Vilella-Cantos,Mauro Martini,Marcello Chiaberge,Mónica Ballesta,David Valiente*

Main category: cs.CV

TL;DR: The paper introduces MinkUNeXt-VINE, a lightweight deep-learning method enhancing place recognition in vineyards using sparse LiDAR and efficient processing.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with place recognition in unstructured agricultural environments without landmarks.

Method: MinkUNeXt-VINE combines pre-processing, Matryoshka Representation Learning multi-loss, and sparse LiDAR to ensure efficiency and performance.

Result: The method achieves robust place recognition in vineyard environments using low-cost sensors and demonstrates efficiency across datasets.

Conclusion: MinkUNeXt-VINE improves vineyard place recognition with low-dimensional outputs suitable for real-time applications, proving its capability for robust and cost-effective localization in agriculture.

Abstract: Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.

</details>


### [234] [SymbolSight: Minimizing Inter-Symbol Interference for Reading with Prosthetic Vision](https://arxiv.org/abs/2601.17326)
*Jasmine Lesner,Michael Beyeler*

Main category: cs.CV

TL;DR: The paper introduces SymbolSight, a computational framework to optimize visual symbols for prosthetic vision, reducing temporal interference in sequential letter reading.


<details>
  <summary>Details</summary>
Motivation: To address the problem of low spatial resolution and temporal persistence in retinal prostheses, which hinder reading ability by causing interference between sequentially presented letters.

Method: SymbolSight uses simulated prosthetic vision, a neural proxy observer, and language-specific bigram statistics to map symbols to letters, minimizing confusion between adjacent letters.

Result: The optimized symbol sets reduced confusion by a median factor of 22 compared to native alphabets in simulations across Arabic, Bulgarian, and English.

Conclusion: Standard alphabets are suboptimal for low-bandwidth prosthetic vision; optimized visual encodings can enhance performance and guide future clinical evaluations.

Abstract: Retinal prostheses restore limited visual perception, but low spatial resolution and temporal persistence make reading difficult. In sequential letter presentation, the afterimage of one symbol can interfere with perception of the next, leading to systematic recognition errors. Rather than relying on future hardware improvements, we investigate whether optimizing the visual symbols themselves can mitigate this temporal interference. We present SymbolSight, a computational framework that selects symbol-to-letter mappings to minimize confusion among frequently adjacent letters. Using simulated prosthetic vision (SPV) and a neural proxy observer, we estimate pairwise symbol confusability and optimize assignments using language-specific bigram statistics. Across simulations in Arabic, Bulgarian, and English, the resulting heterogeneous symbol sets reduced predicted confusion by a median factor of 22 relative to native alphabets. These results suggest that standard typography is poorly matched to serial, low-bandwidth prosthetic vision and demonstrate how computational modeling can efficiently narrow the design space of visual encodings to generate high-potential candidates for future psychophysical and clinical evaluation.

</details>


### [235] [Learning with Geometric Priors in U-Net Variants for Polyp Segmentation](https://arxiv.org/abs/2601.17331)
*Fabian Vazquez,Jose A. Nuñez,Diego Adame,Alissen Moreno,Augustin Zhan,Huimin Li,Jinghao Yang,Haoteng Tang,Bin Fu,Pengfei Gu*

Main category: cs.CV

TL;DR: The paper proposed a Geometric Prior-guided Module (GPM) to enhance polyp segmentation by incorporating depth maps with geometric cues into U-Net-based architectures.


<details>
  <summary>Details</summary>
Motivation: Polyp segmentation is crucial for early colorectal cancer detection and diagnosis, but existing methods struggle with geometric and structural cues in challenging colonoscopy scenes.

Method: The approach fine-tunes the Visual Geometry Grounded Transformer (VGGT) to estimate depth maps tailored to polyp images and utilizes GPM to integrate these maps into U-Net architectures with spatial and channel attention mechanisms.

Result: Experiments on five public polyp datasets showed consistent improvement in segmentation performance over three baseline methods.

Conclusion: The proposed GPM module enhances U-Net-based models, making them robust for polyp segmentation with geometric priors, offering a plug-and-play solution for diverse architectures.

Abstract: Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg

</details>


### [236] [AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17336)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: The paper proposes AGE-Net, a ConvNeXt-based framework for automated KL grading in knee radiographs, addressing challenges like structural changes and grade ambiguity. It achieves strong performance with a QWK of 0.9017 and introduces techniques like SSF, AGR, and DFR.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenges in automated KL grading of knee radiographs due to subtle structural changes, long anatomical relationships, and grade ambiguities.

Method: The authors propose AGE-Net, which combines ConvNeXt architecture with Spectral-Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). It also uses a Normal-Inverse-Gamma evidential head and ordinal ranking constraints.

Result: AGE-Net demonstrates superior performance with a QWK score of 0.9017 and an MSE of 0.2349 on the KL dataset. It outperforms conventional CNNs and shows robustness in ablation studies.

Conclusion: AGE-Net significantly enhances KL grading accuracy and addresses challenges like subtle anatomy changes and uncertainties in grading through advanced architectural techniques.

Abstract: Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.

</details>


### [237] [TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution](https://arxiv.org/abs/2601.17340)
*Haodong He,Xin Zhan,Yancheng Bai,Rui Lan,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: This paper addresses the challenges of real-world text image super-resolution by constructing a high-quality dataset (Real-Texts) and proposing a method (TEXTS-Diff) that excels in both background and text region restoration, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing datasets, which lack text image data and quality background information, and to improve text legibility and overall quality in degraded images.

Method: The study introduces the Real-Texts dataset, encompassing diverse real-world text instances, and proposes TEXTS-Diff, a diffusion model focusing on understanding textual visuals and preserving both fidelity and textual detail.

Result: Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance in text restoration accuracy and generalization, handling complex visual scenarios effectively.

Conclusion: The combination of the Real-Texts dataset and TEXTS-Diff model results in enhanced super-resolution capabilities for real-world degraded text images, ensuring robust restoration of text and background.

Abstract: Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.

</details>


### [238] [STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2601.17342)
*Tong Wang,Xiaodong Zhang,Guanzhou Chen,Jiaqi Wang,Chenxi Liu,Xiaoliang Tan,Wenchao Guo,Xuyang Li,Xuanrui Wang,Zifan Wang*

Main category: cs.CV

TL;DR: STARS addresses the challenge of missing modality data in multimodal remote sensing using an alignment and sampling framework to enhance semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: To address performance decline in multimodal remote sensing caused by missing modality data, which traditional fusion models struggle with due to feature collapse and generalization issues.

Method: STARS uses an asymmetric alignment mechanism (with bidirectional translation and stop-gradient) and a Pixel-level Semantic sampling Alignment strategy (class-balanced pixel sampling and cross-modality semantic alignment loss).

Result: The framework improves robustness in multimodal remote sensing and enhances minority-class recognition despite missing data inputs.

Conclusion: The two-fold framework of STARS effectively tackles missing modality challenges in multimodal inputs and provides a reliable solution for advanced semantic segmentation.

Abstract: Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.

</details>


### [239] [Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective](https://arxiv.org/abs/2601.17349)
*Hailong Yan,Shice Liu,Xiangtao Zhang,Lujian Yao,Fengxiang Yang,Jinwei Chen,Bo Li*

Main category: cs.CV

TL;DR: This paper proposes a new lightweight method for low-light image enhancement using YUV color space and achieves state-of-the-art performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Enhancing low-light images on mobile devices requires balancing visual quality and model compactness, an area where previous methods are limited by ignoring channel-specific and cross-channel interactions.

Method: The authors conducted a frequency-domain analysis, confirmed the benefits of YUV color space, and designed a novel restoration paradigm leveraging three modules tailored to channel-specific degradations and interactions.

Result: The proposed model achieved superior visual quality on benchmarks with significantly fewer parameters than existing methods.

Conclusion: The paper establishes a new lightweight paradigm for low-light image enhancement using YUV color space, offering both high visual quality and model efficiency.

Abstract: In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.

</details>


### [240] [NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields](https://arxiv.org/abs/2601.17350)
*Xianliang Huang,Zhizhou Zhong,Shuhang Chen,Yi Xu,Juhong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: NeRF-MIR is proposed to restore 3D scenes from corrupted images using neural rendering, achieving strong masked image restoration results via new strategies and loss functions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Neural Radiance Fields (NeRF) in handling corrupted images, which can degrade 3D scene restoration quality.

Method: NeRF-MIR introduces a Patch-based Entropy for Ray Emitting (PERE) strategy for ray distribution, uses Progressively Iterative Restoration (PIRE) for self-training, and implements a dynamically-weighted loss function to improve masked image restoration performance.

Result: Constructed datasets and real-world tests demonstrate that NeRF-MIR significantly outperforms other approaches in masked image restoration tasks.

Conclusion: The proposed NeRF-MIR method successfully expands the capability of NeRF for masked image restoration, offering a powerful solution to corruption in captured 3D scenes.

Abstract: Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \textbf{P}atch-based \textbf{E}ntropy for \textbf{R}ay \textbf{E}mitting (\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \textbf{P}rogressively \textbf{I}terative \textbf{RE}storation (\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.

</details>


### [241] [HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data](https://arxiv.org/abs/2601.17352)
*M. L. Mamud,Piyoosh Jaysaval,Frederick D Day-Lewis,M. K. Mudunuru*

Main category: cs.CV

TL;DR: The paper introduces HyDeMiC, a CNN-based model for hyperspectral mineral classification, demonstrating strong performance under noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional mineral classification methods face challenges with noisy data, sensor limitations, and high-dimensional hyperspectral imaging data, necessitating a robust classification approach.

Method: A CNN model named HyDeMiC was trained using laboratory-measured hyperspectral data from the USGS library, simulating realistic field conditions by incorporating HSI sensor response functions and various noise levels.

Result: HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets while maintaining strong performance in moderate noise conditions.

Conclusion: HyDeMiC is a robust solution for hyperspectral mineral classification, suitable for real-world noisy field conditions, overcoming limitations of traditional methods.

Abstract: Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.

</details>


### [242] [PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling](https://arxiv.org/abs/2601.17354)
*Wenzhi Guo,Guangchi Fang,Shu Yang,Bing Wang*

Main category: cs.CV

TL;DR: PocketGS introduces a mobile-friendly approach for efficient and high-quality real-time 3D scene modeling using constrained resources.


<details>
  <summary>Details</summary>
Motivation: Recent methods for 3D modeling, such as 3D Gaussian Splatting, struggle to work efficiently on mobile devices due to training limitations like memory and time constraints.

Method: PocketGS introduces three operators (G, I, and T) to address mobile device limitations: G creates geometry-faithful priors, I injects local surface statistics, and T stabilizes mobile backpropagation with cached intermediates.

Result: PocketGS successfully achieves high-quality 3D scene modeling, outperforming workstation-based methods under mobile constraints.

Conclusion: PocketGS delivers efficient, compact, and high-fidelity 3D scene reconstruction, enabling on-device modeling and rendering workflows.

Abstract: Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.

</details>


### [243] [UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation](https://arxiv.org/abs/2601.17366)
*Chengbo Ding,Fenghe Tang,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: UCAD is a semi-supervised framework for medical image segmentation, improving consistency and accuracy by using contour-aware semantics and uncertainty-guided methods.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised segmentation methods face challenges such as boundary distortions and semantic inconsistency due to reliance on rectangular displacements, especially in medical images.

Method: UCAD introduces anatomically coherent superpixel-based displacements aligned with boundaries and employs uncertainty-guided selection and a dynamic uncertainty-weighted consistency loss for stabilization and regularization.

Result: Extensive experiments show UCAD significantly improves segmentation accuracy compared to other state-of-the-art semi-supervised methods under limited annotations.

Conclusion: UCAD effectively addresses segmentation challenges by integrating anatomical considerations and uncertainty mechanisms, providing a robust solution for semi-supervised medical image segmentation.

Abstract: Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.

</details>


### [244] [Physical Prompt Injection Attacks on Large Vision-Language Models](https://arxiv.org/abs/2601.17383)
*Chen Ling,Kai Hu,Hangcheng Liu,Xingshuo Han,Tianwei Zhang,Changhai Ou*

Main category: cs.CV

TL;DR: The paper introduces Physical Prompt Injection Attack (PPIA), which embeds malicious instructions into physical objects to exploit vulnerabilities in Large Vision-Language Models (LVLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the security vulnerabilities of LVLMs in real-world scenarios, particularly addressing limitations of existing prompt injection attacks which assume unrealistic access to models or user inputs.

Method: The proposed PPIA operates as a black-box, query-agnostic attack embedding typographic instructions into physical objects. The approach combines offline selection of effective visual prompts and strategic placement guided by spatiotemporal attention for maximum perceptibility and influence.

Result: PPIA was tested on 10 advanced LVLMs in simulated and real-world tasks like question answering, navigation, and planning, achieving up to 98% attack success rate with robustness under varying conditions (distance, viewpoint, lighting).

Conclusion: PPIA highlights the seriousness of vulnerabilities in LVLMs, demonstrating a new attack vector that doesn't require direct access to models or inputs, urging consideration of these risks in practical deployments.

Abstract: Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.

</details>


### [245] [ONRW: Optimizing inversion noise for high-quality and robust watermark](https://arxiv.org/abs/2601.17388)
*Xuan Ding,Xiu Yan,Chuanlong Xie,Yao Zhu*

Main category: cs.CV

TL;DR: This paper introduces a diffusion model-based watermarking framework that ensures high image quality and robust watermarking against image corruptions.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based watermarking techniques struggle with robustness during image corruptions, calling for a more practical and reliable solution.

Method: The framework is based on the diffusion model, utilizing a null-text optimization process to convert images into latent inversion noise, followed by iterative denoising with self-attention constraints and pseudo-mask strategies to maintain both quality and semantic integrity.

Result: Experimental results show that the approach outperforms the stable signature method by an average of 10% across 12 image transformations on COCO datasets, proving robustness and effectiveness.

Conclusion: The proposed method effectively balances watermark quality and robustness, showcasing significant improvements over existing systems and reinforcing its practical applicability.

Abstract: Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.

</details>


### [246] [SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition](https://arxiv.org/abs/2601.17391)
*Rui Fan,Weidong Hao*

Main category: cs.CV

TL;DR: This paper advances event camera action recognition (EAR) by enhancing spatiotemporal multi-view representation methods through translation-invariant event conversion, dynamic fusion of motion features, and bio-inspired temporal augmentation, achieving superior accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve methods for event-based action recognition and overcome limitations in current spatiotemporal multi-view representation techniques, particularly regarding spatial binning and fusion architectures.

Method: The authors propose three innovations: translation-invariant dense conversion of sparse events, a dynamic dual-branch fusion model for integrating motion features, and a bio-inspired temporal warping augmentation technique to simulate real-world action speed variations.

Result: On three datasets—HARDVS, DailyDVS-200, and THU-EACT-50-CHL—the proposed approach outperformed existing methods in Top-1 accuracy by up to +10.7%, while achieving 30.1% reduction in parameters and 35.7% lower computations.

Conclusion: The proposed methods significantly improve EAR accuracy and efficiency, establishing a novel framework for event-based representation learning and action recognition.

Abstract: Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.

</details>


### [247] [ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs](https://arxiv.org/abs/2601.17399)
*Rui Fang,Jian Li,Wei Chen,Bin Hu,Ying-Cong Chen,Xin Tang,Liang Diao*

Main category: cs.CV

TL;DR: The paper presents ReLE, a scalable evaluation system for assessing large language models' (LLMs) performance across domains, addressing challenges like benchmark saturation and computational costs.


<details>
  <summary>Details</summary>
Motivation: Benchmark limitations and high computational expenses hinder proper evaluation of the evolving abilities of Chinese language LLMs.

Method: ReLE diagnoses model performance with a Domain × Capability orthogonal matrix, hybrid scoring to address reasoning task false positives, and a variance-aware scheduler to cut evaluation costs dramatically.

Result: 304 LLMs were evaluated, showing that rankings are highly sensitive to evaluation weightings and confirming model specialization rather than general superiority.

Conclusion: ReLE supplements static benchmarks by serving as a dynamic tool for diagnosing model evolution and capability anisotropy in LLMs.

Abstract: Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\% compared to full-pass evaluations while maintaining a ranking correlation of $ρ=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.

</details>


### [248] [HAAF: Hierarchical Adaptation and Alignment of Foundation Models for Few-Shot Pathology Anomaly Detection](https://arxiv.org/abs/2601.17405)
*Chunze Yang,Wenjie Zhao,Yue Tang,Junbo Lu,Jiusong Ge,Qidong Liu,Zeyu Gao,Chen Li*

Main category: cs.CV

TL;DR: This paper addresses challenges in precision pathology by proposing the Hierarchical Adaptation and Alignment Framework (HAAF) for Vision-Language (V-L) models, improving performance in detecting fine-grained abnormalities in low-resource scenarios.


<details>
  <summary>Details</summary>
Motivation: The key motivation is to tackle the granularity mismatch issue in Vision-Language (V-L) models, which struggle to detect subtle morphological abnormalities within specific Regions of Interest (ROIs) critical for precision pathology diagnoses.

Method: The authors introduce the Hierarchical Adaptation and Alignment Framework (HAAF), which includes the Cross-Level Scaled Alignment (CLSA) mechanism to link visual and textual information and a dual-branch inference strategy to enhance few-shot setting stability.

Result: HAAF demonstrates superior performance compared to state-of-the-art methods on four benchmarks and scales effectively with domain-specific backbones (such as CONCH) in low-resource scenarios.

Conclusion: HAAF bridges the gap in V-L model adaptation for fine-grained diagnostic tasks by introducing novel mechanisms for better alignment and stability, achieving significant advancements in precision pathology applications.

Abstract: Precision pathology relies on detecting fine-grained morphological abnormalities within specific Regions of Interest (ROIs), as these local, texture-rich cues - rather than global slide contexts - drive expert diagnostic reasoning. While Vision-Language (V-L) models promise data efficiency by leveraging semantic priors, adapting them faces a critical Granularity Mismatch, where generic representations fail to resolve such subtle defects. Current adaptation methods often treat modalities as independent streams, failing to ground semantic prompts in ROI-specific visual contexts. To bridge this gap, we propose the Hierarchical Adaptation and Alignment Framework (HAAF). At its core is a novel Cross-Level Scaled Alignment (CLSA) mechanism that enforces a sequential calibration order: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide the visual encoder to spotlight anomalies. Additionally, a dual-branch inference strategy integrates semantic scores with geometric prototypes to ensure stability in few-shot settings. Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.

</details>


### [249] [Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity](https://arxiv.org/abs/2601.17408)
*Harsharaj Pathak,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: This paper addresses Source-Free Domain Adaptation (SFDA) by improving neighborhood consistency through the concept of neighborhood signature and achieves adaptation with a single loss term.


<details>
  <summary>Details</summary>
Motivation: The need to adapt models from a labeled source domain to an unlabeled target domain without accessing source data while tackling errors from misleading neighborhood information.

Method: Utilizing neighborhood signature to refine clusters and mitigate noisy neighbors, employing a single loss term to optimize similarity and dissimilarity in target domain predictions.

Result: The proposed method outperforms existing approaches on the VisDA dataset and achieves competitive results on other benchmarks.

Conclusion: The study contributes an effective SFDA approach that simplifies adaptation and improves performance by leveraging neighborhood signature and a tailored loss term.

Abstract: Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.

</details>


### [250] [Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase](https://arxiv.org/abs/2601.17414)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: This paper introduces a cost-effective cloud-enabled IoT system using Firebase for synchronized monitoring and control, proven efficient and scalable through experiments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional systems in real-time data accessibility, remote control, and cloud integration, driven by the rising demand for efficient IoT solutions.

Method: The system employs an ESP32 microcontroller, Firebase Realtime Database, DHT22 and HC-SR04 sensors, and cloud-based interfaces to enable real-time monitoring and control, tested through experiments.

Result: The system achieved a 99.2% data transmission success rate, control latency under 1.5 seconds, and demonstrated scalability at a cost of $32.50.

Conclusion: The solution offers an effective, scalable, and accessible IoT system framework for a broad range of applications, integrating cloud functionality without complex infrastructure.

Abstract: The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.

</details>


### [251] [CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction](https://arxiv.org/abs/2601.17420)
*Shiu-hong Kao,Chak Ho Huang,Huaiqian Liu,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.CV

TL;DR: This paper introduces CoT-Seg, a training-free framework combining chain-of-thought reasoning with self-correction for complex reasoning segmentation.


<details>
  <summary>Details</summary>
Motivation: Address challenges in reasoning segmentation for complicated queries and out-of-domain images by adopting step-by-step reasoning and self-correction akin to human thought processes.

Method: CoT-Seg uses pre-trained MLLMs to decompose queries, extract fine-grained semantics, identify objects, and refine segmentation masks iteratively through self-evaluation and correction without fine-tuning.

Result: CoT-Seg demonstrates reliable and robust segmentation performance, especially in ambiguous and error-prone scenarios, and leverages retrieval-augmented reasoning for external knowledge.

Conclusion: Combining chain-of-thought reasoning with self-correction significantly enhances vision-language driven segmentation; validated by introducing a new challenging dataset ReasonSeg-Hard.

Abstract: Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.

</details>


### [252] [Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography](https://arxiv.org/abs/2601.17429)
*Mehdi Yousefzadeh,Siavash Shirzadeh Barough,Ashkan Fakharifar,Yashar Tayyarazad,Narges Eghbali,Mohaddeseh Mozaffari,Hoda Taeb,Negar Sadat Rafiee Tabatabaee,Parsa Esfahanian,Ghazaleh Sadeghi Gohar,Amineh Safavirad,Saeideh Mazloomzadeh,Ehsan khalilipur,Armin Elahifar,Majid Maleki*

Main category: cs.CV

TL;DR: This paper addresses challenges in vessel segmentation for X-ray coronary angiography (XCA), proposing robust methods including classical filtering enhancements and deep learning models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges such as low contrast, motion artifacts, and domain shifts in robust coronary vessel segmentation for improved quantitative analysis.

Method: The authors utilize a combination of classical vessel filters with per-image tuning via Support Vector Regression (SVR) and deep learning models (U-Net, FPN, and Swin Transformer), with refined supervision strategies.

Result: SVR tuning improved Dice scores of classical filters, while merged-label FPN deep models achieved high Dice (0.931±0.006) scores in internal evaluations, though external test performance dropped. Vessel-type labeling accuracy also reached high percentages (98.5% for RCA, for example).

Conclusion: Per-image tuning strengthens classical methods, while merged-label FPN models enhance segmentation robustness and domain transfer adaptability in coronary vessel analysis.

Abstract: X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.

</details>


### [253] [ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation](https://arxiv.org/abs/2601.17468)
*Chia-Ming Lee,Yu-Fan Lin,Jing-Hui Jung,Yu-Jou Hsiao,Chih-Chung Hsu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: ReflexSplit addresses challenges in Single Image Reflection Separation (SIRS) by introducing three innovative mechanisms for better feature consistency, layer disentanglement, and progressive training, achieving state-of-the-art robustness and results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for SIRS struggle with transmission-reflection confusion caused by nonlinear mixing and inadequate coordination across scales, especially in deep decoder layers.

Method: ReflexSplit utilizes a dual-stream framework emphasizing Cross-scale Gated Fusion (CrGF), Layer Fusion-Separation Blocks (LFSB), and curriculum training to enhance feature aggregation, layer disentanglement, and gradual learning.

Result: Through comprehensive experiments on synthetic and real-world benchmarks, ReflexSplit showcases superior perceptual quality, improved generalization, and state-of-the-art performance.

Conclusion: ReflexSplit provides a robust and innovative solution to SIRS challenges, demonstrating effectiveness in disentangling images with greater precision and applicability across diverse benchmarks.

Abstract: Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.

</details>


### [254] [PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors](https://arxiv.org/abs/2601.17470)
*Chia-Ming Lee,Yu-Fan Lin,Yu-Jou Hsiao,Jing-Hui Jung,Yu-Lun Liu,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: Proposes PhaSR, a method for shadow removal that handles diverse lighting conditions using dual-level alignment of illumination and reflectance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in shadow removal, especially under varying and complex lighting conditions, by disentangling illumination from intrinsic reflectance.

Method: Introduces two modules: Physically Aligned Normalization (PAN) for illumination correction and Geometric-Semantic Rectification Attention (GSRA) for cross-modal alignment of depth geometry and semantic embeddings.

Result: Achieves competitive performance under diverse illumination conditions, outperforming traditional methods in multi-source lighting setups with lower complexity.

Conclusion: PhaSR demonstrates strong shadow removal capabilities and generalizes well to complex ambient lighting scenarios, providing a robust and efficient solution.

Abstract: Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.

</details>


### [255] [BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation](https://arxiv.org/abs/2601.17504)
*Yan Zhou,Zhen Huang,Yingqiu Li,Yue Ouyang,Suncheng Xiang,Zehua Wang*

Main category: cs.CV

TL;DR: The paper introduces BMDS-Net, a framework prioritizing clinical robustness and trustworthiness in brain tumor segmentation from MRI, addressing challenges like missing modalities and lack of confidence calibration.


<details>
  <summary>Details</summary>
Motivation: Brain tumor segmentation models face challenges with missing modalities in clinical imaging and lack adequate confidence calibration, making them risky for real-world medical applications.

Method: BMDS-Net combines a robust backbone with zero-init multimodal contextual fusion, residual-gated decoder mechanisms, and a Bayesian fine-tuning strategy for probabilistic predictions and uncertainty mapping.

Result: BMDS-Net maintains competitive segmentation accuracy and demonstrates superior stability and robustness under missing-modality scenarios compared to baseline models.

Conclusion: BMDS-Net advances brain tumor segmentation by providing reliable predictions and uncertainty maps, making it more suitable for clinical application compared to existing methods.

Abstract: Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.

</details>


### [256] [FMIR, a foundation model-based Image Registration Framework for Robust Image Registration](https://arxiv.org/abs/2601.17529)
*Fengting Zhang,Yue He,Qinghao Liu,Yaonan Wang,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: This paper introduces FMIR, a deep learning framework enhancing generalization in medical image registration using a foundation model-based feature encoder.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for medical image registration struggle to generalize beyond the training domain due to limited medical dataset scales.

Method: FMIR uses a foundation model-based feature encoder for anatomical structure extraction, a general registration head, and employs a channel regularization strategy. It achieves robust performance using a single dataset.

Result: FMIR achieves state-of-the-art performance in-domain and maintains robustness in out-of-domain image registration.

Conclusion: FMIR addresses the generalization issue in medical image registration and paves the way for developing foundational medical imaging models with limited data resources.

Abstract: Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.

</details>


### [257] [Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries](https://arxiv.org/abs/2601.17535)
*Kevin Robbins,Xiaotong Liu,Yu Wu,Le Sun,Grady McPeak,Abby Stylianou,Robert Pless*

Main category: cs.CV

TL;DR: The paper explores predicting the effectiveness of Vision-Language Models (VLMs) like CLIP using text and synthetic image-based evaluations without requiring labeled data.


<details>
  <summary>Details</summary>
Motivation: To address the problem that non-experts have no easy way to evaluate whether a VLM like CLIP is suitable for their particular domain or task.

Method: The authors combine text-only comparisons with synthetic image generation approaches to predict zero-shot accuracy and evaluate VLMs' performance for specific tasks.

Result: Generated synthetic images significantly improve prediction accuracy over text-only methods, and provide users with visual feedback on the evaluation process.

Conclusion: The image-based evaluation approach allows users, even without labeled data, to predict if a VLM will perform effectively for their specific application, enhancing usability for non-expert users.

Abstract: Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.

</details>


### [258] [OTI: A Model-free and Visually Interpretable Measure of Image Attackability](https://arxiv.org/abs/2601.17536)
*Jiaming Liang,Haowei Liu,Chi-Man Pun*

Main category: cs.CV

TL;DR: The paper introduces a model-free method, Object Texture Intensity (OTI), to evaluate image attackability effectively and interpretable.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations in existing attackability measures, such as model dependency and lack of visual interpretability, creating the need for a more accessible and interpretable evaluation method.

Method: Proposes Object Texture Intensity (OTI), a model-free measure that quantifies image attackability through the texture intensity of an image's semantic object.

Result: OTI demonstrates effectiveness, computational efficiency, and provides visual interpretability in experiments focused on adversarial machine learning.

Conclusion: OTI is a practical and interpretable alternative for assessing image attackability while overcoming the limitations of prior methods.

Abstract: Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.

</details>


### [259] [Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper](https://arxiv.org/abs/2601.17555)
*Justin Downes,Sam Saltwick,Anthony Chen*

Main category: cs.CV

TL;DR: This study introduces a saliency-based approach to satellite image compression, optimizing storage and bandwidth by focusing only on significant image regions.


<details>
  <summary>Details</summary>
Motivation: Satellite imagery generates vast amounts of data daily, causing storage and bandwidth challenges. Most tasks focus on specific image regions, and current methods don't prioritize these areas during compression.

Method: Saliency maps are employed to guide imagery preprocessing by applying variable-sized smoothing kernels to different regions based on importance. This approach integrates with traditional lossy compression standards to achieve adaptive compression within single images.

Result: The proposed method tailors compression rates across the image by prioritizing key regions, enhancing storage and encoding efficiency without losing task-relevant data.

Conclusion: Saliency-based preprocessing is a promising tool for optimized satellite image compression, effectively balancing between storage efficiency and data integrity for downstream applications.

Abstract: The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.

</details>


### [260] [Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning](https://arxiv.org/abs/2601.17566)
*Qi Li,Xinchao Wang*

Main category: cs.CV

TL;DR: The paper introduces Sponge Tool Attack (STA), a methodology that disrupts reasoning tasks of large language models through stealthy input prompt manipulation, causing inefficiencies.


<details>
  <summary>Details</summary>
Motivation: Explores the unexplored vulnerabilities in agentic reasoning augmented LLMs due to potential attacks in the tool-calling process.

Method: Proposes STA, which rewrites input prompts in a multi-agent collaborative framework while maintaining high semantic fidelity and preserving original task intent.

Result: STA converts efficient reasoning tasks into verbose trajectories, inducing computational overhead, and demonstrates effectiveness across various models, tools, and datasets.

Conclusion: STA reveals a significant security vulnerability in high utility reasoning frameworks, necessitating countermeasures against malignant prompt manipulations.

Abstract: Enabling large language models (LLMs) to solve complex reasoning tasks is a key step toward artificial general intelligence. Recent work augments LLMs with external tools to enable agentic reasoning, achieving high utility and efficiency in a plug-and-play manner. However, the inherent vulnerabilities of such methods to malicious manipulation of the tool-calling process remain largely unexplored. In this work, we identify a tool-specific attack surface and propose Sponge Tool Attack (STA), which disrupts agentic reasoning solely by rewriting the input prompt under a strict query-only access assumption. Without any modification on the underlying model or the external tools, STA converts originally concise and efficient reasoning trajectories into unnecessarily verbose and convoluted ones before arriving at the final answer. This results in substantial computational overhead while remaining stealthy by preserving the original task semantics and user intent. To achieve this, we design STA as an iterative, multi-agent collaborative framework with explicit rewritten policy control, and generates benign-looking prompt rewrites from the original one with high semantic fidelity. Extensive experiments across 6 models (including both open-source models and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate the effectiveness of STA.

</details>


### [261] [Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization](https://arxiv.org/abs/2601.17586)
*Sebastian Doerrich,Francesco Di Salvo,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: Deep learning models often fail to generalize across medical domains and demographics due to data scarcity. This paper introduces Stylizing ViT, a Vision Transformer encoder leveraging self- and cross-attention for domain-robust data augmentation, yielding superior results without artifacts.


<details>
  <summary>Details</summary>
Motivation: Address issues of limited generalizability in medical image analysis due to heterogeneity and domain shifts, and overcome shortcomings of traditional and stylistic augmentation techniques.

Method: Proposed a Vision Transformer encoder (Stylizing ViT) utilizing weight-shared attention blocks for self- and cross-attention to ensure anatomical consistency and perform style transfer effectively.

Result: Achieves up to +13% accuracy improvement compared to state-of-the-art methods while avoiding artifacts, with additional 17% boost in inference performance using test-time augmentation.

Conclusion: Stylizing ViT enhances domain generalization in medical image classification tasks and provides robust data augmentation without sacrificing image quality; code is openly available.

Abstract: Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .

</details>


### [262] [SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation](https://arxiv.org/abs/2601.17657)
*Taewan Cho,Taeryang Kim,Andrew Jaeyong Choi*

Main category: cs.CV

TL;DR: The paper introduces SPACE-CLIP, a method to extract geometric structures directly from CLIP's vision encoder, bypassing textual prompts, achieving better performance on spatial understanding.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with perceiving geometric structures, and current methods using textual prompts are indirect and inefficient.

Method: Employs a dual-pathway decoder: semantic pathway with FiLM for high-level feature interpretation and a structural pathway for fine-grained spatial detail extraction, fused to combine semantic context and geometry.

Result: Outperforms previous CLIP-based methods in geometry-related tasks, as demonstrated on the KITTI benchmark.

Conclusion: SPACE-CLIP provides an efficient, effective way to utilize large vision models for spatial tasks and can be integrated into embodied AI systems like vision-language-action models.

Abstract: Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip

</details>


### [263] [Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting](https://arxiv.org/abs/2601.17666)
*Xinyue Pan,Yuhao Chen,Fengqing Zhu*

Main category: cs.CV

TL;DR: The paper introduces Prompt Grafting (PG), a novel framework for generating realistic multi-food images by controlling food separation and showing improved object presence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of object entanglement in multi-food image generation, which is crucial for applications like dietary assessment and recipe visualization.

Method: The paper proposes Prompt Grafting (PG), a two-stage process involving explicit spatial cues in text prompts followed by implicit layout guidance to control food separation during image generation.

Result: Experiments show that PG significantly enhances the accuracy of multi-food image generation across two food datasets, demonstrating better object presence and controllable separation.

Conclusion: Prompt Grafting enables reliable compositional food image generation through user-controlled food arrangement, offering improvements for applications requiring multi-food representations.

Abstract: Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.

</details>


### [264] [Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing](https://arxiv.org/abs/2601.17673)
*Weiyu Zhang,Yuan Hu,Yong Li,Yu Liu*

Main category: cs.CV

TL;DR: This paper addresses the spatial reversal issue in unified multimodal remote sensing models by introducing a novel solution, Uni-RS, for improving spatial faithfulness in text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Unified multimodal models in remote sensing often struggle with accurately representing core spatial relations during text-to-image generation, a fundamental semantic aspect in the field.

Method: Uni-RS employs Spatial-Layout Planning, Spatial-Aware Query Supervision, and Image-Caption Spatial Layout Variation to improve spatial consistency. It separates geometric planning from synthesis, biases queries to spatial relations, and incorporates systematic transformations for robustness.

Result: Experiments demonstrate that Uni-RS significantly enhances spatial faithfulness in text-to-image generation while maintaining high performance in multimodal comprehension tasks like image captioning, visual grounding, and VQA.

Conclusion: Uni-RS successfully addresses the spatial symmetry challenges in multimodal remote sensing, representing a significant step forward for unified models in this domain.

Abstract: Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.

</details>


### [265] [StyleDecoupler: Generalizable Artistic Style Disentanglement](https://arxiv.org/abs/2601.17697)
*Zexi Jia,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: The paper introduces StyleDecoupler, a framework to separate artistic style from content using an information-theoretic approach, alongside a large-scale artistic dataset called WeART for benchmarking.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to solve the challenge of disentangling artistic style from semantic content in visual data, which has significant applications in art analysis and style-specific tasks.

Method: The proposed method, StyleDecoupler, uses uni-modal and multi-modal vision models. It minimizes mutual information between these models' embeddings to isolate artistic style. This approach works as a plug-and-play module on frozen vision-language models without needing fine-tuning.

Result: StyleDecoupler achieved state-of-the-art performance in style retrieval tasks on both the WeART and WikiART datasets. Moreover, it demonstrated new capabilities like mapping style relationships and evaluating generative models.

Conclusion: The study concludes that StyleDecoupler is effective for isolating artistic style from content and has practical applications in style retrieval and analysis. The introduction of the WeART dataset further supports the evaluation and study of artistic styles.

Abstract: Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.

</details>


### [266] [An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays](https://arxiv.org/abs/2601.17703)
*Nikhil Kadivar,Guansheng Li,Jianlu Zheng,John M. Higgins,Ming Dao,George Em Karniadakis,Mengjia Xu*

Main category: cs.CV

TL;DR: This paper introduces an automated deep learning framework for analyzing red blood cell dynamics by quantifying morphological transitions in time-lapse images, achieving high accuracy despite limited labeled data.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in tracking dynamic morphological changes in sickle cells under dense and overlapping conditions, which make manual annotation and analysis difficult.

Method: An integrated framework combining AI-assisted manual annotation, nnU-Net for segmentation, classification, and instance counting, coupled with a watershed algorithm to handle cell overlap.

Result: The framework improves segmentation accuracy and robustness, doubling experimental throughput in densely packed cell suspensions while identifying drug-dependent behaviors and mechanobiological trends.

Conclusion: This approach offers a high-throughput, scalable, and reliable tool for studying cellular dynamics and assessing drug efficacy in microphysiological contexts.

Abstract: Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.

</details>


### [267] [Advancing Structured Priors for Sparse-Voxel Surface Reconstruction](https://arxiv.org/abs/2601.17720)
*Ting-Hsun Chi,Chu-Rong Chen,Chi-Tun Hsu,Hsuan-Ting Lin,Sheng-Yu Huang,Cheng Sun,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: The paper presents a method combining 3D Gaussian Splatting and sparse-voxel rasterization to enhance surface reconstruction for radiance fields, achieving better accuracy and convergence.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of 3D Gaussian Splatting (poor surface fidelity) and sparse-voxel rasterization (slow convergence and underutilization of scene structure) for more effective surface reconstruction.

Method: The authors propose a novel voxel initialization method that strategically places voxels with appropriate levels of detail and introduce depth geometry supervision for per-ray depth regularization.

Result: The proposed method improves geometric accuracy, recovers fine structures, completes surfaces more effectively, and converges faster as demonstrated on standard benchmarks.

Conclusion: By combining the strengths of 3D Gaussian Splatting and sparse-voxel rasterization, the method achieves better surface fidelity and reconstruction efficiency in radiance fields.

Abstract: Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.

</details>


### [268] [Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study](https://arxiv.org/abs/2601.17723)
*Tayyab Nasir,Daochang Liu,Ajmal Mian*

Main category: cs.CV

TL;DR: This paper systematically analyzes implicit neural representation (INR) methods for arbitrary-scale image super-resolution (ASSR) by benchmarking, exploring training recipes, and proposing improvements in objective design.


<details>
  <summary>Details</summary>
Motivation: A lack of empirical studies to evaluate existing INR methods and training configurations in ASSR prompts a need for deeper analysis to benchmark performance, reveal gains, identify saturation limits, and explore promising directions.

Method: Comparative analysis across existing INR techniques and training recipes, introducing a unified code repository and examining effects of configurations and a novel loss function that balances intensity variations with edge and texture preservation.

Result: The findings reveal marginal improvements with newer INR methods, strong correlations between model performance and training configurations, better texture fidelity from a proposed loss function, and predictable scaling gains via complexity and data diversity.

Conclusion: Modern INR methods show limited improvement over older approaches. Training specifics and objective engineering significantly impact outcomes, emphasizing scaling laws for growth and optimization strategies for enhanced quality.

Abstract: Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.

</details>


### [269] [Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles](https://arxiv.org/abs/2601.17733)
*Junran Lu,Yuanqi Li,Hengji Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: This paper proposes a novel way to represent and generate Boundary Representations (B-Reps) for CAD with particle-based modeling that decouples rigid topological hierarchies, improving context awareness and geometric accuracy.


<details>
  <summary>Details</summary>
Motivation: Generative modeling for Boundary Representations in CAD is challenging due to their complex combination of topology and geometry across various orders of geometric entities. Existing methods struggle with context awareness and error recovery due to reliance on hierarchical cascades.

Method: The authors propose a new approach by reformulating B-Reps into sets of compositional particles, encoding each topological entity using shared latent representations at interfaces. This allows unified and context-aware generation of topology and geometry using a multi-modal flow matching framework.

Result: The proposed method achieves high-fidelity CAD models with better validity and editability. It also successfully handles tasks like unconditional generation, single-view 3D reconstruction, point cloud-based reconstruction, and in-painting.

Conclusion: The particle-based representation for B-Rep modeling enhances geometric coupling, unifies the representation of different topological entities, and enables joint topology-geometry generation with downstream capabilities, outperforming state-of-the-art techniques.

Abstract: Boundary Representation (B-Rep) is the widely adopted standard
  in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.
  We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.

</details>


### [270] [The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation](https://arxiv.org/abs/2601.17737)
*Chenyu Mu,Xin He,Qu Yang,Wanshun Chen,Jiadi Yao,Huang Liu,Zihao Yi,Bo Zhao,Xingyu Chen,Ruotian Ma,Fanghua Ye,Erkun Yang,Cheng Deng,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CV

TL;DR: The paper introduces an end-to-end framework for generating coherent cinematic videos from dialogue, addressing challenges with long-form narrative generation.


<details>
  <summary>Details</summary>
Motivation: Current video generation models excel at creating visuals from text prompts but struggle with generating coherent long-form narratives, particularly from high-level, dialogue-based concepts.

Method: The authors propose a new framework featuring two agents: ScripterAgent, which transforms dialogue into detailed scripts, and DirectorAgent, which generates coherent cinematic videos from these scripts using a continuous cross-scene strategy. To train and evaluate this system, they developed ScriptBench, a large-scale benchmark with multimodal annotations.

Result: The proposed framework improves script-faithfulness and temporal coherence across video models. It employs a new Visual-Script Alignment (VSA) metric and CriticAgent for evaluation.

Conclusion: The study addresses the semantic gap in automated filmmaking, offering insights into balancing visual quality and script adherence for future video generation advancements.

Abstract: Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.

</details>


### [271] [Learning Sewing Patterns via Latent Flow Matching of Implicit Fields](https://arxiv.org/abs/2601.17740)
*Cong Cao,Ren Li,Corentin Dumery,Hao Li*

Main category: cs.CV

TL;DR: The paper introduces a novel method for modeling and generating sewing patterns using implicit representations and latent space encoding, enabling advanced applications such as pattern estimation, completion, and refitting.


<details>
  <summary>Details</summary>
Motivation: Current tools struggle with accurately modeling sewing patterns due to high variability in panel geometry and seam arrangements, which limits their utility for fashion design and simulations.

Method: The method uses signed and unsigned distance fields to encode panel boundaries and seam endpoints into differentiable latent spaces, employs latent flow matching to model panel combinations, and predicts seam relations to reconstruct sewing patterns.

Result: Experiments demonstrate improved accuracy in estimating sewing patterns from images compared to existing methods, and the approach supports advanced tasks like pattern completion and refitting.

Conclusion: This technique enhances sewing pattern modeling and generation capabilities, highlighting its practical potential in digital fashion design and related applications.

Abstract: Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.

</details>


### [272] [Frequency-aware Neural Representation for Videos](https://arxiv.org/abs/2601.17741)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: FaNeRV, a frequency-aware neural representation for videos, addresses INR spectral bias by decoupling low- and high-frequency components for improved video reconstruction and compression.


<details>
  <summary>Details</summary>
Motivation: Existing INR-based video compression methods often suffer from spectral bias, causing over-smoothed reconstructions and poor rate-distortion performance. Improving these components motivates this research.

Method: FaNeRV employs multi-resolution supervision for progressive structure-texture learning, a dynamic high-frequency injection for challenging regions, and a frequency-decomposed network module for better spectral modeling.

Result: FaNeRV achieves significantly better results than state-of-the-art INR methods and performs competitively against traditional video compression codecs in rate-distortion tasks.

Conclusion: The proposed FaNeRV framework effectively enhances video compression by mitigating spectral bias and integrating frequency-aware mechanisms, leading to improved video reconstructions.

Abstract: Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.

</details>


### [273] [Video Compression with Hierarchical Temporal Neural Representation](https://arxiv.org/abs/2601.17743)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: This paper proposes TeNeRV, a video compression method using hierarchical neural representations, enhancing efficiency through temporal coherence and adaptive modulation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve video compression techniques by addressing limitations in capturing complex temporal dependencies in existing neural representation methods.

Method: TeNeRV employs Inter-Frame Feature Fusion for local coherence and GoP-Adaptive Modulation to adaptively learn group-specific priors across video segments.

Result: Experiments show a consistent improvement in rate-distortion performance over other INR-based methods.

Conclusion: TeNeRV provides a robust and efficient solution for video compression, effectively capturing both short- and long-term dependencies.

Abstract: Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.

</details>


### [274] [Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection](https://arxiv.org/abs/2601.17747)
*Kaixuan Jiang,Chen Wu,Zhenghui Zhao,Chengxi Han*

Main category: cs.CV

TL;DR: This paper introduces UniCD, an integrated change detection framework that handles supervised, weakly-supervised, and unsupervised tasks collaboratively, achieving remarkable performance in change detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of expensive pixel-level change labels in multi-temporal remote sensing tasks and to create an adaptable solution for diverse annotation availabilities.

Method: The proposed method, UniCD, utilizes a shared encoder and multi-branch collaborative learning mechanism. It enables deep integration of heterogeneous supervision signals with specific branches for supervised tasks, weakly-supervised change regularization (CRR), and unsupervised semantic prior-driven change inference (SPCI).

Result: UniCD shows optimal performance in all three scenarios, achieving significant accuracy improvements of 12.72% and 12.37% on weakly-supervised and unsupervised tasks, respectively, on the LEVIR-CD dataset.

Conclusion: UniCD effectively addresses diverse annotation settings in change detection and establishes a new state-of-the-art performance in weakly-supervised and unsupervised scenarios.

Abstract: Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.

</details>


### [275] [MV-S2V: Multi-View Subject-Consistent Video Generation](https://arxiv.org/abs/2601.17756)
*Ziyang Song,Xinyu Gong,Bangya Liu,Zelin Zhao*

Main category: cs.CV

TL;DR: The paper introduces Multi-View Subject-to-Video Generation (MV-S2V) to address limitations in previous S2V methods, achieving better 3D subject consistency and high-quality video synthesis using multiple viewpoints.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of existing S2V methods being constrained to single-view references, enabling better subject control and achieving improved video generation.

Method: A synthetic data curation pipeline with real-world data integration for training, paired with the Temporally Shifted RoPE (TS-RoPE) technique to distinguish subjects and views effectively.

Result: Improved 3D subject consistency and high-quality visual outputs showcased in multi-view video generation.

Conclusion: The introduced MV-S2V framework successfully advances subject-driven video generation, setting a benchmark for using multi-view references in video synthesis.

Abstract: Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href="https://szy-young.github.io/mv-s2v">this URL</a>

</details>


### [276] [Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation](https://arxiv.org/abs/2601.17791)
*Rabin Dulal,Wenfeng Jia,Lihong Zheng,Jane Quinn*

Main category: cs.CV

TL;DR: The study presents a cost-effective, non-contact cattle live weight estimation method using 3D reconstruction from multi-view images with SAM 3D-based fusion and ensemble regression models, achieving consistent results for farm implementation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional live weight estimation methods that require manual handling of cattle and impact productivity.

Method: The research uses 3D reconstruction with multi-view RGB images combined with SAM 3D-based agreement-guided fusion and ensemble regression models.

Result: The approach outperformed other 3D generation methods and showed reliable performance using classical ensemble models (R$^2$ = 0.69 ± 0.10, MAPE = 2.22 ± 0.56%) in farm conditions.

Conclusion: Improving 3D reconstruction quality is key to scalable implementation, rather than focusing on increasing model complexity for producing large datasets.

Abstract: Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\pm$ 0.10, MAPE = 2.22 $\pm$ 0.56 \%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.

</details>


### [277] [ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning](https://arxiv.org/abs/2601.17818)
*Wen Luo,Peng Chen,Xiaotao Huang,LiQun Huang*

Main category: cs.CV

TL;DR: The paper proposes a new framework, ViTCoP, for improving computational efficiency in Large Vision-Language Models (LVLMs) by selectively pruning redundant visual tokens while preserving essential information.


<details>
  <summary>Details</summary>
Motivation: Current visual token pruning methods in LVLMs either prune essential visual information too early or result in significant redundancy in selected tokens, leading to inefficiency.

Method: A framework named ViTCoP that performs redundancy filtering in the vision encoder and step-wise co-pruning within large language models based on hierarchical features, using a new token saliency metric (L2 norm of K-vectors) to maintain compatibility with acceleration methods.

Result: ViTCoP achieves state-of-the-art results on image and video understanding tasks, reduces inference latency, and decreases GPU memory usage efficiently, especially excelling under extreme pruning rates.

Conclusion: ViTCoP significantly improves computational efficiency and performance in LVLMs by addressing existing method limitations and offering a robust strategy for critical token selection.

Abstract: Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.

</details>


### [278] [VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training](https://arxiv.org/abs/2601.17830)
*Mengmeng Wang,Dengyang Jiang,Liuzhuozheng Li,Yucheng Lin,Guojiang Shen,Xiangjie Kong,Yong Liu,Guang Dai,Jingdong Wang*

Main category: cs.CV

TL;DR: The paper introduces a novel lightweight framework, \namex, for efficient training of denoising-based diffusion transformers by integrating pre-trained VAE features without external dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion transformers face slow training convergence, and current solutions introduce heavy computational overhead due to reliance on external encoders or dual models.

Method: The method uses pre-trained VAE features aligned with intermediate latent features through a projection layer with a feature alignment loss, eliminating the need for external representation encoding.

Result: The framework improves generation quality, speeds up training convergence, matches or surpasses current efficiency methods, and adds only 4% extra GFLOPs without external model costs.

Conclusion: \namex provides a computationally efficient and high-performing approach for denoising-based diffusion transformers, overcoming limitations of previous methods and ensuring faster training with minimal computational increase.

Abstract: Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.

</details>


### [279] [Geometry-Grounded Gaussian Splatting](https://arxiv.org/abs/2601.17835)
*Baowen Zhang,Chenxing Jiang,Heng Li,Shaojie Shen,Ping Tan*

Main category: cs.CV

TL;DR: This paper introduces a theoretical framework for Gaussian Splatting, enabling improved shape reconstruction using Gaussian primitives as stochastic solids.


<details>
  <summary>Details</summary>
Motivation: The need for improved shape extraction from Gaussian primitives, as existing methods have limitations in multi-view consistency and sensitivity to floaters.

Method: Developing a theoretical foundation that interprets Gaussian primitives as stochastic solids, allowing for their direct use in geometric representations and efficient depth map rendering for shape extraction.

Result: The method achieves superior shape reconstruction results on public datasets compared to other Gaussian Splatting-based techniques.

Conclusion: The approach offers a principled and efficient solution for geometry extraction, addressing existing limitations in Gaussian Splatting-based shape reconstruction.

Abstract: Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.

</details>


### [280] [SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction](https://arxiv.org/abs/2601.17857)
*Lan Yang,Minghan Yang,Ke Li,Honggang Zhang,Kaiyue Pang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: SynMind improves fMRI image reconstruction accuracy by integrating sentence-level semantic descriptions with visual priors, surpassing state-of-the-art methods and aligning better with human perception.


<details>
  <summary>Details</summary>
Motivation: Current fMRI-based image reconstruction methods suffer from semantic misalignment despite achieving high visual quality. This study aims to enhance semantic accuracy in reconstructed images to ensure objects are correctly identified.

Method: The paper introduces SynMind, which parses fMRI signals into sentence-level semantic descriptions using grounded visual language models (VLMs). These descriptions are integrated with visual priors within a pretrained diffusion model for reconstruction.

Result: SynMind significantly outperforms existing methods on quantitative metrics and human evaluations. It achieves better semantic alignment, utilizes broader brain regions, and operates efficiently on less computational resources.

Conclusion: SynMind addresses the semantic misalignment in fMRI decoding by incorporating explicit textual semantics, providing more accurate image reconstructions consistent with human perception and improving computational efficiency.

Abstract: Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.

</details>


### [281] [Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment](https://arxiv.org/abs/2601.17862)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: The paper proposes a lightweight framework using quantum-enhanced collaborative learning to improve the generalization of medical image AI models across different domains, addressing domain shifts without relying on multi-center labeled data.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of medical image AI models losing effectiveness in real-world multi-center deployment due to domain shifts, which limits their clinical utility.

Method: The framework employs a MobileNetV2-based domain-invariant encoder with three components: (1) simulation of imaging shifts, (2) domain-adversarial training with gradient reversal, and (3) a quantum feature enhancement layer leveraging parameterized quantum circuits. Also, test-time adaptation is integrated.

Result: Experiments show that the proposed method significantly outperforms traditional models on unseen domains, achieving lower performance variance and higher metrics like AUC and sensitivity.

Conclusion: This study underscores the potential of lightweight quantum-enhanced methods for domain generalization in medical imaging AI, offering a pathway for computationally efficient hybrid quantum-classical systems.

Abstract: Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.

</details>


### [282] [MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance](https://arxiv.org/abs/2601.17866)
*Yoonwoo Jeong,Cheng Sun,Yu-Chiang Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: The paper introduces MV-SAM, a multi-view segmentation framework that ensures 3D consistency using reconstructed pointmaps derived from unposed images. MV-SAM adapts the Segment Anything Model (SAM) by leveraging pixel-point correspondences in the 3D space.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing segmentation models like SAM, which lack 3D awareness and produce inconsistent results across views without costly per-scene optimization.

Method: MV-SAM utilizes pointmaps (3D points reconstructed from images) to lift image embeddings into 3D space. By aligning 2D interactions with 3D geometry, it integrates 3D prompt embeddings for consistent mask generation without requiring explicit 3D networks or annotated 3D data.

Result: The method performs well across multiple benchmarks like NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV, outperforming SAM2-Video and matching performance with optimization-heavy baselines.

Conclusion: MV-SAM establishes a framework for efficient and consistent multi-view segmentation by achieving 3D awareness without relying on annotated 3D data or networks. This demonstrates its scalability and generalization across tasks.

Abstract: Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.

</details>


### [283] [VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding](https://arxiv.org/abs/2601.17868)
*Zhihao He,Tieyuan Chen,Kangyu Wang,Ziran Qin,Yang Shao,Chaofan Gan,Shijie Li,Zuxuan Wu,Weiyao Lin*

Main category: cs.CV

TL;DR: VidLaDA improves video understanding by utilizing a diffusion-based approach with bidirectional attention and speeds up processing with MARS-Cache, achieving performance comparable to state-of-the-art autoregressive models.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and causal masking biases in autoregressive Video LLMs for better global spatiotemporal understanding in video processing.

Method: The paper introduces VidLaDA, a Video LLM based on Diffusion Language Models employing bidirectional attention, combined with MARS-Cache for accelerated inference through asynchronous cache refreshing and frame-wise chunk attention.

Result: VidLaDA demonstrates superior performance against diffusion baselines and rivals state-of-the-art models, achieving over 12x inference speedup with MARS-Cache while maintaining reasoning accuracy.

Conclusion: VidLaDA, with its novel diffusion-based approach and MARS-Cache framework, successfully improves video understanding efficiency and inference speed, offering a promising alternative to autoregressive models.

Abstract: Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.

</details>


### [284] [Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran](https://arxiv.org/abs/2601.17880)
*Muhammad Umar Salman,Mohammad Areeb Qazi,Mohammed Talha Alam*

Main category: cs.CV

TL;DR: The paper introduces Quran MD, a multimodal dataset of the Quran, including text, linguistic details, and audio at verse and word levels, aimed at advancing computational Quranic studies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create a comprehensive dataset for studying and analyzing the Quran by integrating text, translation, transliteration, and recitation audio data to bridge linguistic and multimodal dimensions.

Method: Quran MD provides structured data containing original Arabic text, English translation, phonetic transliteration, and verse-level and word-level audio from 32 reciters, enabling detailed linguistic and phonological analysis.

Result: The developed dataset supports applications like natural language processing, speech recognition, Quranic text-to-speech synthesis, and linguistic analysis, offering multimodal compatibility with rich oral and textual traditions.

Conclusion: The dataset enables both academic research and practical use cases in Quranic studies and computational tasks, making it accessible for community and research purposes via Hugging Face platform.

Abstract: We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset

</details>


### [285] [Revisiting 3D Reconstruction Kernels as Low-Pass Filters](https://arxiv.org/abs/2601.17900)
*Shengjun Zhang,Min Chen,Yibo Wei,Mingyu Dong,Yueqi Duan*

Main category: cs.CV

TL;DR: The paper introduces Jinc and modulated kernels for 3D reconstruction, addressing low-pass property challenges and improving rendering performance.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in 3D reconstruction caused by overlap of high and low-frequency components due to unideal low-pass kernels.

Method: The authors propose Jinc kernel with an ideal low-pass filtering property and modulated kernels for balancing spatial efficiency and fidelity.

Result: Experimental results show that the proposed kernels achieve superior rendering performance in 3D reconstruction.

Conclusion: Jinc and modulated kernels effectively address spectral challenges in 3D reconstruction, balancing spatial and frequency-domain fidelity.

Abstract: 3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.

</details>


### [286] [Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models](https://arxiv.org/abs/2601.17918)
*Dain Kim,Jiwoo Lee,Jaehoon Yun,Yong Hoe Koo,Qingyu Chen,Hyunjae Kim,Jaewoo Kang*

Main category: cs.CV

TL;DR: The paper evaluates Direct Preference Optimization (DPO) methods within medical Large Vision-Language Models (LVLMs), identifying their shortcomings, and introduces a novel preference construction strategy improving visual question-answering tasks.


<details>
  <summary>Details</summary>
Motivation: Despite the potential of LVLMs in medical applications, their reliability and alignment are often inadequate, particularly in high-stakes scenarios like healthcare, necessitating exploration of optimization techniques like DPO.

Method: The study rigorously evaluates nine DPO formulations across two medical LVLMs, analyzing their performance, limitations, and introducing a novel targeted preference construction strategy.

Result: The research uncovers significant limitations in current DPO methods, including inconsistent benefits and unresolved visual misinterpretation errors. The proposed strategy improves performance by 3.6% over the best DPO baseline for visual question-answering tasks.

Conclusion: While current DPO strategies show variability in medical LVLMs, the proposed enhancements address key visual misinterpretation errors, improving performance. The study also provides an open framework for further research and development.

Abstract: Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.

</details>


### [287] [RemEdit: Efficient Diffusion Editing with Riemannian Geometry](https://arxiv.org/abs/2601.17927)
*Eashan Adhikarla,Brian D. Davison*

Main category: cs.CV

TL;DR: RemEdit improves image editing using diffusion-based methods with enhanced fidelity and inference speed, addressing a key trade-off.


<details>
  <summary>Details</summary>
Motivation: Balancing semantic fidelity with inference speed is a major challenge in image generation and editing.

Method: The paper utilizes Riemannian manifold for latent space navigation, dual-SLERP blending, vision-language models for better prompts, and attention pruning for effectively speeding up the process.

Result: RemEdit surpasses prior state-of-the-art frameworks in editing quality while maintaining real-time speed under 50% pruning.

Conclusion: RemEdit sets a new standard by offering highly efficient, accurate, and real-time image editing capabilities.

Abstract: Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.

</details>


### [288] [From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images](https://arxiv.org/abs/2601.17934)
*Vi Vu,Thanh-Huy Nguyen,Tien-Thinh Nguyen,Ba-Thinh Lam,Hoang-Thien Nguyen,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: SC-SAM combines a specialist model (U-Net) and a generalist model (SAM) to improve medical image segmentation with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Adapting foundation models like SAM for medical imaging faces challenges such as domain shift, limited labels, and PEFT's inability to leverage unlabeled data.

Method: The SC-SAM framework introduces cooperation between U-Net and SAM, where U-Net provides prompts and pseudo-labels, and SAM regularizes U-Net in a co-training loop.

Result: SC-SAM achieved state-of-the-art performance in prostate MRI and polyp segmentation benchmarks, surpassing other advanced models like MedSAM.

Conclusion: Specialist-generalist collaboration in SC-SAM efficiently exploits unlabeled data, enhancing label-efficient segmentation for medical images.

Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.

</details>


### [289] [DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation](https://arxiv.org/abs/2601.17939)
*Chengkun Sun,Jinqian Pan,Renjie Liang,Zhengkang Fan,Xin Miao,Jiang Bian,Jie Xu*

Main category: cs.CV

TL;DR: This paper introduces Deformable Transposed Convolution (DTC) for improving upsampling and feature reconstruction in medical image segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Conventional upsampling methods in medical image segmentation, such as transposed convolution and linear interpolation, are limited by fixed sampling positions, leading to artifacts and loss of detail.

Method: The authors propose Deformable Transposed Convolution (DTC), a novel upsampling technique that learns dynamic coordinates for sampling positions to generate high-resolution feature maps.

Result: Experiments on 3D (BTCV15) and 2D datasets (ISIC18, BUSI) show that DTC improves feature reconstruction, detail recovery, and can be seamlessly integrated into existing segmentation models.

Conclusion: DTC provides a robust enhancement to upsampling methods, addressing structural information loss and boosting performance in medical image segmentation.

Abstract: In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.

</details>


### [290] [FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos](https://arxiv.org/abs/2601.17947)
*Bora Yimenicioglu,Vishal Manikanden*

Main category: cs.CV

TL;DR: FlowMorph is a self-supervised framework for studying RBC mechanics from microfluidic videos, achieving high silhouette IoU and accurate predictions of mechanical properties.


<details>
  <summary>Details</summary>
Motivation: Investigate RBC deformability as a biomarker for diseases using efficient microfluidic assays, overcoming challenges of supervised segmentation and limited physics encoding.

Method: FlowMorph employs parametric contours, physics-informed modeling, and optimization of RBC behavior without labels, using differentiable dynamics and multiple physics-based constraints.

Result: FlowMorph achieves high IoU (0.905), improves area conservation, predicts RBC dynamics with AUC of 0.863, and estimates Young’s modulus with 0.118 MPa error.

Conclusion: FlowMorph improves RBC mechanics analysis through physics-consistent methods, enabling better deformability predictions and robust performance across experimental variations.

Abstract: Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.
  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\sim 1.5\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.

</details>


### [291] [UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders](https://arxiv.org/abs/2601.17950)
*Matthew Walmer,Saksham Suri,Anirud Aggarwal,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: The paper introduces UPLiFT, a method for efficient feature upsampling that achieves competitive performance with lower costs.


<details>
  <summary>Details</summary>
Motivation: To explore efficient alternatives for task-agnostic feature upsampling, overcoming scaling issues in cross-attention-based methods.

Method: UPLiFT architecture uses an iterative upsampling approach with a Local Attender operator to stabilize features during upsampling.

Result: Achieves state-of-the-art performance in feature upsampling with lower inference costs; competes in generative tasks like VAE upsampling.

Conclusion: UPLiFT delivers a versatile and efficient solution for feature upsampling, offering competitive results with better efficiency.

Abstract: The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.

</details>


### [292] [Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors](https://arxiv.org/abs/2601.17977)
*Jinchen Gu,Nan Zhao,Lei Qiu,Lu Zhang*

Main category: cs.CV

TL;DR: This paper proposes a Domain-Knowledge-Guided Hybrid Mixture-of-Experts (DKGH-MoE) model, which unites data-driven learning with domain expertise for improved performance and interpretability, particularly in the medical domain.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of Mixture-of-Experts models in specialized fields like medicine, where small datasets hinder effective learning. Additionally, the aim is to incorporate rich expert clinical knowledge, such as physician gaze patterns, that traditional models fail to learn.

Method: The method involves designing DKGH-MoE, a hybrid framework that integrates two components: a data-driven MoE for extracting novel features from imaging data and a domain-expert-guided MoE that incorporates clinical cues from experts (e.g., eye-gaze) to focus on diagnostically relevant regions.

Result: DKGH-MoE demonstrates both improved modeling performance and better interpretability by effectively unifying clinical expertise with data-driven techniques.

Conclusion: Combining domain-specific expert insights with data-driven strategies in DKGH-MoE creates a balanced and interpretable model, suitable for robust learning in specialized domains such as medicine.

Abstract: Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.

</details>


### [293] [MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images](https://arxiv.org/abs/2601.18001)
*Aqsa Yousaf,Sint Sint Win,Megan Coffee,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: This paper introduces MorphXAI, an AI framework integrating parasite detection with detailed morphological analysis for enhanced interpretability, supported by a clinician-annotated dataset.


<details>
  <summary>Details</summary>
Motivation: Clinicians rely on manual inspection and expert knowledge to diagnose parasitic infections. Current AI approaches lack interpretability, limiting their practical use in identifying key characteristics clinicians need for diagnosis.

Method: MorphXAI combines parasite detection tasks with morphological analysis, utilizing a clinician-annotated dataset to train models on clinically relevant attributes (e.g., shape or developmental stage).

Result: The framework improves detection accuracy compared to baselines and offers biologically meaningful, interpretable insights by analyzing key morphology traits.

Conclusion: MorphXAI enhances both detection precision and interpretability, setting a new standard for automated parasitic analysis and supporting clinicians in resource-restrained settings.

Abstract: Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.

</details>


### [294] [Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection](https://arxiv.org/abs/2601.18008)
*Asiegbu Miracle Kanu-Asiegbu,Nitin Jotwani,Xiaoxiao Du*

Main category: cs.CV

TL;DR: The paper introduces Strip-Fusion, a spatial-temporal fusion network designed for robust multispectral pedestrian detection, addressing issues like misaligned inputs and challenging conditions.


<details>
  <summary>Details</summary>
Motivation: To enhance pedestrian detection in multispectral modalities (visible and thermal) by addressing challenges like temporal neglect, misalignment in image pairs, and difficult detection conditions such as varying lighting and occlusion.

Method: The authors proposed a Strip-Fusion network that employs temporally adaptive convolutions for dynamic spatial-temporal feature fusion. They introduced a Kullback-Leibler divergence loss to manage modality imbalance and a novel post-processing algorithm to mitigate false positives.

Result: The approach showed competitive performance on KAIST and CVC-14 benchmarks, outperforming prior methods, particularly under challenging conditions like heavy occlusion and misaligned inputs.

Conclusion: Strip-Fusion effectively improves multispectral pedestrian detection by integrating spatial-temporal features, addressing misalignment, and minimizing detection challenges.

Abstract: Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.

</details>


### [295] [Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation](https://arxiv.org/abs/2601.18045)
*Zhuangzhi Gao,Feixiang Zhou,He Zhao,Xiuju Chen,Xiaoxin Li,Qinkai Yu,Yitian Zhao,Alena Shantsila,Gregory Y. H. Lip,Eduard Shantsila,Yalin Zheng*

Main category: cs.CV

TL;DR: The paper introduces PIs-Regressor and Topology SegNet for curvilinear structure segmentation in medical images, enhancing topological integration and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Segmenting curvilinear structures is crucial for clinical image analysis, and existing methods poorly generalize due to reliance on handcrafted loss functions for topology encoding.

Method: The paper integrates persistence image (PI) representations into the network architecture via the PIs-Regressor module and Topology SegNet for robust segmentation.

Result: Experimental evaluations on three curvilinear benchmarks show improved accuracy and topological fidelity, even in challenging imaging conditions.

Conclusion: The proposed framework effectively incorporates topological information, enhances segmentation robustness, and can be integrated with other topology-based approaches.

Abstract: Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.

</details>


### [296] [Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling](https://arxiv.org/abs/2601.18049)
*Yunfei Qiu,Qiqiong Ma,Tianhua Lv,Li Fang,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: The paper proposes a new semi-supervised framework for hyperspectral image classification, combining spatial prior information and dynamic learning mechanisms to tackle challenges like boundary label diffusion and pseudo-label instability.


<details>
  <summary>Details</summary>
Motivation: Current semi-supervised HSI classification faces limitations due to high annotation costs, limited sample availability, boundary label diffusion, and pseudo-label instability.

Method: The proposed framework integrates an EASLP module for edge-aware superpixel label propagation and a DREPL framework consisting of DHP and ATSC strategies for pseudo-label stability and hierarchical sample utilization.

Result: Experimental evaluations on four benchmark datasets demonstrate improved classification performance, spatial-temporal consistency, and enhanced pseudo-label stability.

Conclusion: The integration of spatial and dynamic learning mechanisms in the framework significantly enhances classification robustness and efficiency for HSI data.

Abstract: Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.

</details>


### [297] [Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification](https://arxiv.org/abs/2601.18088)
*Jianshu Chao,Tianhua Lv,Qiqiong Ma,Yunfei Qiu,Li Fang,Huifang Shen,Wei Yao*

Main category: cs.CV

TL;DR: The paper proposes a self-supervised cross-domain transfer framework for hyperspectral data that avoids reliance on source domain labels and performs well with limited target domain samples.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised methods for hyperspectral representation struggle with cross-domain applications due to reliance on source domain labels and vulnerability to distribution shifts, resulting in poor target domain generalization.

Method: The framework uses a Spatial-Spectral Transformer (S2Former) during self-supervised pre-training to capture spectral-spatial representations with mutual guidance between spatial and spectral branches. It incorporates a Frequency Domain Constraint (FDC) to enhance detail recognition. During fine-tuning, Diffusion-Aligned Fine-tuning (DAFT) is introduced for robust transfer learning using a teacher-student structure.

Result: Experiments on four hyperspectral datasets show stable classification performance and strong cross-domain adaptability under resource-limited conditions.

Conclusion: The proposed framework achieves efficient cross-domain transfer and generalization for hyperspectral classification, even with few target domain labels.

Abstract: Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.

</details>


### [298] [Text-Pass Filter: An Efficient Scene Text Detector](https://arxiv.org/abs/2601.18098)
*Chuang Yang,Haozhao Ma,Xu Han,Yuan Yuan,Qi Wang*

Main category: cs.CV

TL;DR: This paper introduces Text-Pass Filter (TPF) for detecting arbitrary-shaped texts without relying on the shrink-mask strategy, overcoming issues like background-foreground confusion and loss of text margin features.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing text detection methods caused by shrink-mask strategies, such as feature loss and poor differentiation of foreground and background.

Method: The authors propose a Text-Pass Filter (TPF) inspired by a band-pass filter, creating unique feature-filter pairs for each text. They also develop a Reinforcement Ensemble Unit (REU) and Foreground Prior Unit (FPU) to enhance field recognition, feature consistency, and discrimination of foreground versus background.

Result: The experiments validate the effectiveness of TPF, REU, and FPU, highlighting improved text recognition, especially in separating adhesive texts and addressing large aspect ratio challenges.

Conclusion: TPF offers a promising method for real-time arbitrary-shaped text detection by avoiding complex post-processing, enhancing feature filtering, and improving text separation and recognition accuracy.

Abstract: To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.

</details>


### [299] [Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs](https://arxiv.org/abs/2601.18099)
*Akbar Saadat*

Main category: cs.CV

TL;DR: The paper introduces a zero-training computational framework for real-time realization of a Gaussian model, achieving high accuracy in estimating synthetic blur values and defocus filter applications.


<details>
  <summary>Details</summary>
Motivation: To enable real-time application of a Gaussian model framework with no training dataset, particularly for images that are partial blurred versions of each other.

Method: The framework discretely calculates analytic expressions of defocused images from sharper ones, filters multiple solutions using neighboring point similarity measures, and determines the best matches for Gaussian kernel standard deviations.

Result: The proposed framework achieves a mean absolute error below 1.7% in synthetic blur estimation, with less than 2% discrepancy in reconstructed blurred image intensities compared to actual values.

Conclusion: The paper demonstrates the effectiveness of zero-training framework in effectively and accurately estimating and analyzing image blur, ensuring real-time applicability in relevant contexts.

Abstract: Following the earlier verification for Gaussian model in \cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\%$, obtained by applying the extracted defocus filters to less blurred images.

</details>


### [300] [Spatial-Conditioned Reasoning in Long-Egocentric Videos](https://arxiv.org/abs/2601.18100)
*James Tribble,Hao Wang,Si-En Hong,Chaoyi Zhou,Ashish Bastola,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: This paper explores how spatial signals impact the reasoning abilities of vision-language models (VLMs) in long-horizon egocentric video navigation, introducing a new dataset annotation and testing depth-enhanced input.


<details>
  <summary>Details</summary>
Motivation: Vision-language models struggle with spatial reasoning in long egocentric video sequences due to viewpoint inconsistencies and lack of geometric context.

Method: The study introduces Sanpo-D, a detailed re-annotation of the Google Sanpo dataset, and evaluates VLMs with and without depth-augmented inputs on navigation-related spatial tasks.

Result: The analysis shows a balance between general-purpose and spatially specialized accuracy, with depth-enhanced inputs improving safety-critical task performance like detecting pedestrians and obstructions.

Conclusion: Incorporating spatial and depth information enhances visual navigation capabilities of VLMs, particularly for tasks that require precise spatial awareness.

Abstract: Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.

</details>


### [301] [LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment](https://arxiv.org/abs/2601.18118)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: This paper presents LungCRCT, a novel latent causal representation learning framework, enhancing lung cancer analysis, early detection, and enabling causal intervention simulations.


<details>
  <summary>Details</summary>
Motivation: Lung cancer in its early stages is frequently silent and symptoms resemble other respiratory diseases, leading to missed opportunities for early detection which impacts survival rates.

Method: The research introduces LungCRCT, leveraging advanced graph autoencoder-based causal discovery algorithms with distance correlation disentanglement and entropy-based image reconstruction refinement.

Result: LungCRCT demonstrates both effective causal intervention analysis capabilities for lung cancer treatments and robust performance in malignant tumor classification, achieving an AUC score of 93.91%.

Conclusion: The LungCRCT framework allows for significant enhancements in lung cancer treatment analysis and detection, supporting both practical application and future advancements in medical AI research.

Abstract: Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.

</details>


### [302] [Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection](https://arxiv.org/abs/2601.18135)
*Jiahao Lyu,Minghua Zhao,Xuewen Huang,Yifei Chen,Shuangli Du,Jing Hu,Cheng Shi,Zhiyong Lv*

Main category: cs.CV

TL;DR: FoGA introduces a lightweight model for video anomaly detection tailored to edge devices, achieving high accuracy and efficiency with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To address video anomaly detection limitations in resource-constrained edge devices by reducing model size and improving temporal prediction accuracy.

Method: FoGA employs a Unet-based framework for feature extraction on consecutive frames, uses gated context aggregation for dynamic feature fusion, introduces forward consistency loss, and integrates hybrid anomaly measurement strategies.

Result: FoGA outperforms state-of-the-art methods in video anomaly detection with up to 155 FPS while maintaining efficiency and accuracy.

Conclusion: FoGA provides a balanced approach that enhances video anomaly detection accuracy and efficiency, making it suitable for real-time edge device applications.

Abstract: As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.

</details>


### [303] [Agentic Very Long Video Understanding](https://arxiv.org/abs/2601.18157)
*Aniket Rege,Arka Sadhu,Yuliang Li,Kejie Li,Ramya Korlakai Vinayak,Yuning Chai,Yong Jae Lee,Hyo Jin Kim*

Main category: cs.CV

TL;DR: The paper introduces EGAgent, a framework for better long-term contextual understanding of egocentric videos in wearable devices, using entity scene graphs and hybrid search capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of long-horizon video understanding for always-on personal AI assistants, which require interpreting continuous, lengthy egocentric video and audio streams.

Method: The paper proposes EGAgent, which creates entity scene graphs to represent relationships among entities over time, enabling structured and coherent reasoning. It integrates tools for hybrid visual and audio search for comprehensive understanding in long-term contexts.

Result: The proposed method demonstrates state-of-the-art performance on the EgoLifeQA dataset (57.5%) and shows competitive results on the Video-MME (Long) dataset (74.1%) for tasks involving complex, longitudinal video understanding.

Conclusion: EGAgent offers a novel way to enable detailed, temporally coherent reasoning for long-horizon video streams, showing promise for use with AI assistants and wearable devices.

Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.

</details>


### [304] [TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration](https://arxiv.org/abs/2601.18168)
*Zehua Liu,Shihao Zou,Jincai Huang,Yanfang Zhang,Chao Tong,Weixin Si*

Main category: cs.CV

TL;DR: The paper introduces a novel coarse-to-fine vessel registration method to improve accuracy and feasibility for TACE procedures.


<details>
  <summary>Details</summary>
Motivation: Transarterial chemoembolization (TACE) for liver malignancies is challenging due to the need for precise intra-operative vascular navigation and anatomical variability.

Method: The study developed a global alignment module (SA-PnP) for structure-aware 2D-3D vessel correspondence and TempDiffReg, a temporal diffusion model for iterative vessel deformation.

Result: The proposed method delivered 66.7% lower mean squared error (MSE) and 17.7% lower mean absolute error (MAE) compared to existing approaches.

Conclusion: This method enhances vascular registration accuracy, aiding clinicians in safely and effectively performing TACE procedures, improving surgical outcomes.

Abstract: Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\% lower MSE and 17.7\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}

</details>


### [305] [YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection](https://arxiv.org/abs/2601.18172)
*Lin Huang,Yujuan Tan,Weisheng Li,Shitai Shan,Liu Liu,Bo Liu,Linlin Shen,Jing Yu,Yue Niu*

Main category: cs.CV

TL;DR: The paper proposes YOLO-DS, an enhanced object detection method that uses a novel Dual-Statistic Synergy Operator (DSO) to improve performance by modeling channel-wise statistics.


<details>
  <summary>Details</summary>
Motivation: Currently, YOLO detectors fail to explicitly address heterogeneous object responses within shared feature channels, which restricts their performance capabilities.

Method: The study introduces the Dual-Statistic Synergy Operator (DSO), which captures channel-wise mean and peak-to-mean difference. Additionally, two lightweight gating modules (DSG and MSG) are devised for feature selection and weighting.

Result: On the MS-COCO benchmark, YOLO-DS outperforms YOLOv8 by achieving AP gains of 1.1%-1.7% across five model sizes, with minimal inference latency increase.

Conclusion: YOLO-DS effectively enhances heterogeneity discrimination and efficiency, proving to be superior through experiments and studies.

Abstract: One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.

</details>


### [306] [\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation](https://arxiv.org/abs/2601.18188)
*Weiye Zhu,Zekai Zhang,Xiangchen Wang,Hewei Pan,Teng Wang,Tiantian Geng,Rongtao Xu,Feng Zheng*

Main category: cs.CV

TL;DR: This paper presents NaVIDA, a framework enhancing Vision-and-Language Navigation (VLN) by integrating action-grounded visual dynamics and adaptive execution, achieving superior performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing VLN methods lack explicit modeling of vision-action causality, leading to unstable behaviors, weak generalization, and cumulative errors.

Method: NaVIDA integrates inverse-dynamics supervision, hierarchical probabilistic action chunking (HPAC), and an entropy-guided mechanism to address VLN challenges. It learns causal relationships between actions and visual changes and adaptively controls execution.

Result: NaVIDA outperforms previous state-of-the-art methods in navigation performance while utilizing fewer parameters (3B vs. 8B). Real-world robot tests validate its feasibility.

Conclusion: NaVIDA demonstrates improved robustness, generalization, and performance in VLN tasks, showcasing its practical utility in real-world applications.

Abstract: Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \textsc{NaVIDA} (\textbf{Nav}igation with \textbf{I}nverse \textbf{D}ynamics \textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.

</details>


### [307] [Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval](https://arxiv.org/abs/2601.18190)
*Yifan Li,Shiying Wang,Jianqiang Huang*

Main category: cs.CV

TL;DR: This paper introduces MPS-CLIP, a parameter-efficient framework focusing on fine-grained alignment for Remote Sensing Image-Text Retrieval, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Global alignment methods in retrieval overlook multi-scale semantic details of overhead images and adapting large models via full fine-tuning is computationally costly. The paper aims to address these inefficiencies.

Method: MPS-CLIP employs a framework combining keyword-guided fine-grained alignment, using LLM to extract semantic keywords, SamGeo for sub-perspectives, a Gated Global Attention adapter, and a Multi-Perspective Representation module. It optimizes hybrid objectives for precise semantic matching.

Result: MPS-CLIP achieves state-of-the-art performance in RSICD and RSITMD benchmarks with significant improvements in mean Recall (35.18% and 48.40%).

Conclusion: MPS-CLIP successfully shifts from coarse global matching to fine-grained alignment, improving retrieval accuracy while reducing computational costs associated with adapting large models.

Abstract: Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.

</details>


### [308] [MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models](https://arxiv.org/abs/2601.18192)
*Tian-Yi Zhou,Xuan-Hao Liu,Bao-Liang Lu,Wei-Long Zheng*

Main category: cs.CV

TL;DR: The paper introduces MindCine, a framework for reconstructing videos from EEG signals using multimodal and large-scale EEG models to address challenges like single modality reliance and data scarcity.


<details>
  <summary>Details</summary>
Motivation: To enable dynamic and accurate human visual perception reconstruction from non-invasive EEG signals, overcoming single modality and data scarcity challenges.

Method: The method integrates multimodal joint learning with beyond-text modalities, uses a pre-trained large-scale EEG model for semantic decoding, and employs a Seq2Seq model with causal attention for perceptual decoding.

Result: Experiments demonstrate that the proposed MindCine outperforms current state-of-the-art methods, showcasing improved reconstruction quality both qualitatively and quantitatively.

Conclusion: The study confirms the complementary benefits of multimodal data and the utility of large-scale EEG models, making significant advancements in EEG-to-video reconstruction with limited data.

Abstract: Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.

</details>


### [309] [QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding](https://arxiv.org/abs/2601.18195)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Kaiwei Zhang,Jun Jia,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: This paper introduces QualiRAG, a training-free framework for visual quality assessment (VQA) using a retrieval-augmented generation (RAG) approach. It relies on latent perceptual knowledge within large multimodal models without requiring task-specific training.


<details>
  <summary>Details</summary>
Motivation: The authors aim to create a solution for visual quality assessment that avoids the bias and labor-intensive nature of supervised fine-tuning and reinforcement learning methods, which rely heavily on curated datasets.

Method: QualiRAG dynamically generates and retrieves auxiliary knowledge from large multimodal models by structuring questions into four types of knowledge: visual metadata, subject localization, global quality summaries, and local quality descriptions. It then uses relevance-aware retrieval to provide evidence-based reasoning.

Result: QualiRAG demonstrates significant improvements over existing open-source general-purpose and VQA-finetuned multimodal models in visual quality understanding. It also performs competitively in visual quality comparison tasks without task-specific training.

Conclusion: The study confirms that QualiRAG is a robust, training-free method for visual quality assessment, leveraging large multimodal models and retrieval-augmented generation for comprehensive quality understanding tasks.

Abstract: Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.

</details>


### [310] [HomoFM: Deep Homography Estimation with Flow Matching](https://arxiv.org/abs/2601.18222)
*Mengfan He,Liangzheng Sun,Chunyu Li,Ziyang Meng*

Main category: cs.CV

TL;DR: The paper introduces HomoFM, a novel homography estimation framework using flow matching techniques and improved domain adaptation strategies, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for homography estimation struggle with capturing complex geometric transformations and generalization across domains.

Method: The paper uses flow matching from generative modeling and velocity field learning to improve homography estimation. It also incorporates gradient reversal layers for domain-invariant feature learning.

Result: HomoFM outperforms state-of-the-art methods in accuracy and robustness, as shown through extensive experiments.

Conclusion: The proposed HomoFM framework offers a more robust and accurate solution for homography estimation, addressing challenges of complex transformations and domain shifts.

Abstract: Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.

</details>


### [311] [Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach](https://arxiv.org/abs/2601.18228)
*Sahil Naik,Soham Bagayatkar,Pavankumar Singh*

Main category: cs.CV

TL;DR: This paper proposes an efficient facial emotion recognition model based on EfficientNetB2 tailored for real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges such as low image quality, lighting variations, pose changes, background distractions, noisy labels, class imbalance, and high computational cost in facial emotion recognition using datasets like FER-2013.

Method: A lightweight EfficientNetB2-based model trained with a two-stage warm-up and fine-tuning strategy, incorporating techniques like AdamW optimization, label smoothing, clipped class weights, dropout, mixed-precision training, and data augmentation.

Result: The model achieves a test accuracy of 68.78% on FER-2013 with significantly fewer parameters (nearly ten times less) compared to VGG16-based models.

Conclusion: The proposed method demonstrates stable training and strong generalization, making it suitable for real-time and edge-based applications requiring efficient emotion detection.

Abstract: Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.

</details>


### [312] [V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering](https://arxiv.org/abs/2601.18240)
*Mengyuan Jin,Zehui Liao,Yong Xia*

Main category: cs.CV

TL;DR: The paper introduces V-Loop, a framework for detecting hallucinations in medical visual question answering by using a bidirectional reasoning process to verify factual correctness of responses.


<details>
  <summary>Details</summary>
Motivation: To address hallucinations in medical VQA outputs, which pose high risks in medical scenarios and are not effectively handled by current indirect uncertainty-based detection approaches.

Method: The method employs V-Loop, which forms a visually grounded logical loop using bidirectional reasoning. It cross-verifies the primary answer by generating a verification question and ensuring visual attention consistency, flagging hallucinated answers when the logical loop fails to close.

Result: The proposed V-Loop framework consistently surpasses existing methods in detecting hallucinations, demonstrates efficiency, and complements uncertainty-based approaches when integrated.

Conclusion: V-Loop offers a reliable, efficient, and plug-and-play solution for improving factual correctness in medical VQA, addressing a key limitation in current hallucination detection techniques.

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.

</details>


### [313] [Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation](https://arxiv.org/abs/2601.18242)
*Zerui Kang,Yishen Lim,Zhouyou Gu,Seung-Woo Ko,Tony Q. S. Quek,Jihong Park*

Main category: cs.CV

TL;DR: This paper presents a vision-language-model (VLM) guided framework to accelerate and stabilize RF material parameter estimation in differentiable ray tracing (DRT) engines, achieving faster convergence with high accuracy and fewer measurements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of gradient-based inverse ray tracing for RF material parameter estimation, which is sensitive to initialization and computationally expensive under limited measurements, crucial for electromagnetic digital twins in 6G systems.

Method: The paper uses a VLM to parse scene images, infer material properties, provide quantitative priors for conductivity initialization, and optimize transmitter/receiver placement. These priors enable improved gradient-based refinement within a differentiable RT engine.

Result: The method achieved 2-4x faster convergence, 10-100x lower final error, and sub-0.1% mean relative error with reduced receiver setups in indoor scenes simulated in NVIDIA Sionna.

Conclusion: Semantic priors from vision-language models effectively enhance physics-based optimization, providing a fast and reliable solution for RF material parameter estimation in electromagnetic digital twins for 6G applications.

Abstract: Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\times$ faster convergence and 10-100$\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.

</details>


### [314] [A multimodal vision foundation model for generalizable knee pathology](https://arxiv.org/abs/2601.18250)
*Kang Yu,Dingyu Wang,Zimu Yuan,Nan Zhou,Jiajun Liu,Jiaxin Liu,Shanggui Liu,Yaoyan Zheng,Huishu Yuan,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: OrthoFoundation, a multimodal vision foundation model for musculoskeletal pathology, achieves SOTA performance across 14 tasks with reduced labeled data and superior generalization.


<details>
  <summary>Details</summary>
Motivation: Musculoskeletal disorders are a major global disability driver, emphasizing the need for accurate AI-based medical imaging interpretation, given flaws in existing task-specific supervised models.

Method: A self-supervised contrastive learning approach using Dinov3 backbone, trained on a dataset of 1.2 million unlabeled knee X-ray and MRI images.

Result: State-of-the-art performance on 14 tasks, superior knee-specific accuracy, exceptional cross-anatomy generalization, and notable label efficiency.

Conclusion: OrthoFoundation addresses the limitations of conventional AI models by enabling robust joint-agnostic semantic understanding, reducing annotation requirements, and improving diagnostic precision for musculoskeletal imaging.

Abstract: Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.

</details>


### [315] [Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images](https://arxiv.org/abs/2601.18260)
*Eytan Kats,Kai Geissler,Daniel Mensing,Jochen G. Hirsch,Stefan Heldman,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: The paper introduces a learning-based framework that uses depth cameras to predict the 3D positions and shapes of organs based on 2D body surface depth images, aiming to improve patient scanning processes.


<details>
  <summary>Details</summary>
Motivation: Current patient positioning methods can be suboptimal, and automated solutions leveraging depth information could enhance scanning efficiency and accuracy, especially for radiology workflows.

Method: The authors employ a convolutional neural network trained on a large dataset of full-body MRI scans, where synthetic depth images and corresponding anatomical segmentations are used to predict multiple organs' 3D localization and shape.

Result: The proposed framework accurately localizes various anatomical structures from depth images without requiring surface reconstruction.

Conclusion: Integrating depth sensors into radiology procedures could automate patient positioning, thereby optimizing the scanning process and improving the patient experience.

Abstract: Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.

</details>


### [316] [Revisiting Aerial Scene Classification on the AID Benchmark](https://arxiv.org/abs/2601.18263)
*Subhajeet Das,Susmita Ghosh,Abhiroop Chatterjee*

Main category: cs.CV

TL;DR: The paper reviews machine learning approaches to aerial image classification and introduces Aerial-Y-Net, achieving high accuracy on the AID dataset.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the complexities in aerial image classification, a critical task for urban planning and environmental preservation.

Method: The study surveys existing machine learning techniques and develops Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion.

Result: Aerial-Y-Net achieves 91.72% accuracy on the AID dataset, surpassing baseline models.

Conclusion: The proposed Aerial-Y-Net proves effective in addressing challenges in aerial image classification, marking improvement over prior techniques.

Abstract: Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.

</details>


### [317] [Contextual Range-View Projection for 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.18301)
*Seyedali Mousavi,Seyedhamidreza Mousavi,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: The paper proposes new techniques to optimize the range-view projection of 3D LiDAR point clouds, improving semantic encoding and object representation in 2D models.


<details>
  <summary>Details</summary>
Motivation: Existing depth-based point selection in range-view projection for LiDAR data leads to loss of contextual information, negatively impacting semantic and structural representation.

Method: Introduced Centerness-Aware Projection (CAP) to prioritize central instance points and Class-Weighted-Aware Projection (CWAP) to prioritize object classes based on user-defined weights.

Result: CAP achieved up to a 3.1% improvement in mean Intersection-over-Union (mIoU) over the baseline on the SemanticKITTI dataset. CWAP improved performance for targeted classes without impacting others.

Conclusion: Incorporating centerness and class-based priorities in range projection mitigates information loss, enhancing the effectiveness of LiDAR point cloud processing for semantic segmentation.

Abstract: Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \textit{Centerness-Aware Projection (CAP)} and \textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes

</details>


### [318] [SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis](https://arxiv.org/abs/2601.18305)
*Xuan Wang,Siyuan Su,Quantong Fu,Yongxiang Hu,Yangfan Zhou*

Main category: cs.CV

TL;DR: The paper proposes SwipeGen, an automated pipeline for synthesizing human-like swipe interactions in GUI agents, and introduces GUISwiper, a GUI agent with enhanced swipe execution, achieving significant improvements over existing baselines.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the emerging bottleneck in GUI agents related to step execution capabilities, particularly in handling swipe interactions which are overly simplified in current systems.

Method: The authors decompose human swipe gestures into quantifiable dimensions and propose SwipeGen, a pipeline to synthesize human-like swipes. They also release a benchmark for evaluating swipe executions and develop GUISwiper by leveraging synthesized swipe data.

Result: GUISwiper achieves a 69.07% swipe execution accuracy, marking a 214% improvement over prior visual language model (VLM) baselines.

Conclusion: SwipeGen effectively enhances the swipe execution abilities of GUI agents through synthesized human-like swipes, validating its utility with improved performance in GUISwiper.

Abstract: With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.

</details>


### [319] [A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification](https://arxiv.org/abs/2601.18330)
*Muhammad Ali Shah,Muhammad Mansoor Alam,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: This paper introduces the Efficient Densely Swin Hybrid (EDSH) framework for accurate brain tumor MRI analysis, using advanced hybrid architectures to achieve high accuracy and sensitivity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve brain tumor detection using MRI analysis by addressing challenges in capturing both local texture patterns and global contextual dependencies.

Method: The study proposes a hybrid framework combining DenseNet and Swin Transformer branches, with two experimental setups: BFS for glioma analysis and hierarchical DenseNet-Swint for meningioma/pituitary tumor analysis.

Result: The framework achieves 98.50% accuracy and recall on a large-scale MRI test dataset (40,260 images), outperforming standalone models.

Conclusion: The EDSH framework provides significant advancements in brain tumor MRI diagnosis, showcasing effectiveness in learning complex tumor characteristics while reducing false negatives.

Abstract: This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.

</details>


### [320] [PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction](https://arxiv.org/abs/2601.18336)
*Isaac Deutsch,Nicolas Moënne-Loccoz,Gavriel State,Zan Gojcic*

Main category: cs.CV

TL;DR: This paper introduces a module for multi-view 3D reconstruction that corrects for photometric inconsistencies using interpretable and physically-based transformations.


<details>
  <summary>Details</summary>
Motivation: To address the sensitivity of multi-view 3D reconstruction methods to photometric inconsistencies caused by optical and ISP variations, which existing methods fail to handle robustly.

Method: The authors propose the Physically-Plausible ISP (PPISP) module, which disentangles camera intrinsic effects via physically-grounded transformations and predicts ISP parameters for novel views using a PPISP controller.

Result: The PPISP module achieves state-of-the-art performance on benchmarks, enhancing both the quality and fairness of novel view reconstructions without needing ground-truth images.

Conclusion: PPISP provides a physically interpretable solution for mitigating ISP inconsistencies, improving multi-view 3D reconstructions, and facilitates better control and metadata integration.

Abstract: Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp

</details>


### [321] [Beyond Rigid: Benchmarking Non-Rigid Video Editing](https://arxiv.org/abs/2601.18340)
*Bingzheng Qu,Kehai Chen,Xuefeng Bai,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: The paper introduces NRVBench, a benchmark for evaluating non-rigid video editing, including a dataset, a novel metric, and the VM-Edit baseline.


<details>
  <summary>Details</summary>
Motivation: The research addresses challenges in text-driven video editing, specifically generating coherent non-rigid deformations without physical distortion or temporal flicker.

Method: NRVBench includes a curated dataset, a new evaluation metric (NRVE-Acc) using Vision-Language Models, and VM-Edit, a baseline utilizing dual-region denoising for structure-aware control.

Result: NRVBench demonstrates current methods struggle with physical plausibility, but their method achieves superior performance on metrics.

Conclusion: NRVBench can become a standard platform for advancing physics-aware video editing.

Abstract: Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.

</details>


### [322] [Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception](https://arxiv.org/abs/2601.18346)
*Sijing Wu,Yunhao Li,Zicheng Zhang,Qi Jia,Xinyue Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces Q-Bench-Portrait, a benchmark designed to assess the capabilities of multimodal large language models (MLLMs) in perceiving and evaluating portrait images.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs perform well on generic image benchmarks but lack focus on portrait images, which have unique structural and perceptual features.

Method: The authors created Q-Bench-Portrait, featuring 2,765 image-question-answer triplets, and included diverse portrait sources, multiple quality dimensions, and question formats to assess global and local-level understanding of portrait image quality.

Result: They evaluated 25 MLLMs (20 open-source, 5 closed-source), finding that while models showed some portrait-perception ability, their performance was subpar and far from human-level accuracy.

Conclusion: This benchmark aims to encourage further advancements in improving the portrait image perception abilities of MLLMs, for both general-purpose and specialized applications.

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.

</details>


### [323] [OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI](https://arxiv.org/abs/2601.18368)
*Caterina Fuster-Barceló,Claudia Castrillón,Laura Rodrigo-Muñoz,Victor Manuel Vega-Suárez,Nicolás Pérez-Fernández,Gorka Bastarrika,Arrate Muñoz-Barrutia*

Main category: cs.CV

TL;DR: OREHAS, a deep-learning-based pipeline, automates volumetric quantification of endolymphatic hydrops (EH) from 3D MRI, reducing manual intervention while improving reliability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of manual and less accurate methods for quantifying endolymphatic hydrops (EH) in the auditory system through automated, consistent, and realistic volumetric measurement techniques.

Method: An automated workflow combining slice classification, inner ear localization, and segmentation to compute EH volume ratios from MRI scans, trained with minimal manual annotations.

Result: OREHAS achieved high accuracy with Dice scores of 0.90 (SPACE-MRC) and 0.75 (REAL-IR). It outperformed clinical software by delivering more physiologically realistic and reliable measurements, closely matching expert annotations.

Conclusion: OREHAS offers a scalable, robust, and consistent approach for EH quantification from standard MRI scans, reducing operator dependency and enabling large-scale clinical and research applications.

Abstract: We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.
  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.
  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.

</details>


### [324] [Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues](https://arxiv.org/abs/2601.18372)
*Christos Petrou,Harris Partaourides,Athanasios Balomenos,Yannis Kopsinis,Sotirios Chatzis*

Main category: cs.CV

TL;DR: This paper introduces a gaze prediction framework using head-mounted display motion signals combined with visual saliency cues, offering a solution in VR without relying on direct eye tracking.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of direct eye tracking, which may be unavailable due to hardware constraints or privacy concerns, and improve gaze-based interaction in VR.

Method: They fuse HMD motion signals with visual saliency features (using UniSal encoder) and process these through time-series prediction modules like TSMixer and LSTM.

Result: The proposed method outperforms baseline approaches in predicting gaze direction when tested on the EHTask dataset and deployed in commercial VR systems.

Conclusion: Predictive techniques using fused visual and motion data improve gaze prediction, reducing lag and enhancing VR interaction, especially in scenarios without direct eye tracking.

Abstract: Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.

</details>


### [325] [Estimation of geometric transformation matrices using grid-shaped pilot signals](https://arxiv.org/abs/2601.18385)
*Rinka Kawano,Masaki Kawamura*

Main category: cs.CV

TL;DR: This paper presents a watermarking method using a grid-shaped pilot signal for synchronizing watermarks in images distorted by cropping and other geometric transformations.


<details>
  <summary>Details</summary>
Motivation: The issue in current watermarking systems is their lack of robustness against image cropping, which disrupts synchronization, making it difficult to accurately extract the watermark.

Method: The proposed method embeds a grid-shaped pilot signal with distinct horizontal and vertical values. By analyzing distortions in grid patterns caused by transformations such as scaling, rotation, and cropping, it estimates transformation matrices using techniques including the Radon transform.

Result: Simulations indicate the method accurately estimates geometric transformations with low error, even under complex composite attacks such as cropping combined with other distortions.

Conclusion: This proposed approach improves watermark synchronization robustness, providing effective watermark extraction for images subjected to cropping and other distortive operations.

Abstract: Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.

</details>


### [326] [ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks](https://arxiv.org/abs/2601.18386)
*Gabriel Lee Jun Rong,Christos Korgialas,Dion Jia Xu Ho,Pai Chet Ng,Xiaoxiao Miao,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: The paper presents the ARMOR framework, which combines multiple adversarial attack methods through collaborative agents and achieves better adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing attack frameworks lack flexibility, semantic understanding, and adaptation capabilities, which limits their effectiveness.

Method: ARMOR integrates Vision Language Models (VLM)-powered agents and Large Language Models (LLM) to optimize the orchestration and adaptation of adversarial attacks in real-time.

Result: ARMOR improves transferability, effectively fools diverse target models, and optimally selects or blends attacks using evaluation metrics.

Conclusion: The ARMOR framework enhances attack strategies through adaptive and semantically-aware collaborations, showcasing its potential on benchmark datasets.

Abstract: Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.

</details>


### [327] [Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space](https://arxiv.org/abs/2601.18392)
*Moritz Rempe,Lukas T. Rotkopf,Marco Schlimbach,Helmut Becker,Fabian Hörst,Johannes Haubold,Philipp Dammann,Kevin Kröninger,Jens Kleesiek*

Main category: cs.CV

TL;DR: The paper introduces kViT, a complex-valued Vision Transformer that enables direct classification on MRI k-Space data, addressing inefficiencies of current methods.


<details>
  <summary>Details</summary>
Motivation: Standard deep learning methods for MRI rely on reconstructed image data, discarding critical phase information and requiring computationally costly transformations, which are not well-suited for global frequency-domain information.

Method: The authors propose kViT, a Vision Transformer tailored for k-Space with a radial patching strategy that aligns with MRI physics, allowing direct classification of frequency-domain data.

Result: kViT achieves competitive classification performance compared to state-of-the-art image-domain models and demonstrates significantly improved robustness to high acceleration factors, with lower computational costs by up to 68× in VRAM usage.

Conclusion: kViT provides a novel, resource-efficient approach to analyze MRI k-Space data directly, enabling advancements in computational efficiency and robustness in medical imaging applications.

Abstract: Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.

</details>


### [328] [Larger than memory image processing](https://arxiv.org/abs/2601.18407)
*Jon Sporring,David Stansby*

Main category: cs.CV

TL;DR: This paper presents a framework for analyzing petascale image datasets by optimizing I/O, leveraging slice-based streaming, and introducing a domain-specific language (DSL) for operational efficiency, significantly enhancing data handling for large-scale tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of analyzing petascale datasets like electron-microscopy volumes and human-organ atlases, which are larger than available memory, and optimize performance constrained by I/O.

Method: The study introduces a slice-based streaming architecture for data processing, formalizes sweep-based execution and overlap-aware tiling to reduce redundant disk I/O, and develops a domain-specific language (DSL) to enable efficient algorithm representation and runtime/memory optimization.

Result: The proposed approach yields near-linear I/O scans, predictable memory use, and better throughput for large image analysis without requiring full-volume in-memory processing. It is compatible with existing segmentation and morphology tools.

Conclusion: Structuring analysis as streaming passes and optimizing disk and memory usage provides a significant performance boost for extremely large-scale image datasets, beyond the limitations of memory-residency approaches.

Abstract: This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.

</details>


### [329] [Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings](https://arxiv.org/abs/2601.18414)
*Aura Loredana Dan*

Main category: cs.CV

TL;DR: This paper compares deep learning models (MobileNet, EfficientNet, VGG16) for recognizing emotions in children's drawings, focusing on classification, robustness, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve early detection of affective states in children with autism spectrum disorder (ASD) using a less intrusive, more objective method through analysis of children's drawings.

Method: Three deep learning models (MobileNet, EfficientNet, VGG16) are evaluated with transfer learning on a labeled dataset of children's drawings to classify emotions.

Result: The research identifies trade-offs between lighter and deeper architectures in terms of performance, robustness, and computational efficiency for mobile and real-time applications.

Conclusion: Lightweight and deep learning models offer complementary benefits for analyzing children's drawings in affective computing, with applications in ASD assessment.

Abstract: Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.

</details>


### [330] [On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics](https://arxiv.org/abs/2601.18448)
*Lloyd Austin Courtenay*

Main category: cs.CV

TL;DR: The paper examines issues caused by current preprocessing methods in geometric morphometrics (GMM) when integrating with machine learning (ML), offering solutions for reduced dependency and contamination in predictive models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address a preprocessing flaw in GMM, where Generalized Procrustes Analysis (GPA) used before training/testing data splits introduces statistical dependencies, negatively impacting downstream ML analyses.

Method: The authors conducted controlled 2D and 3D simulations to assess GPA's impact under different conditions (sample sizes, landmark densities, and allometric patterns). They introduced a novel alignment method that aligns test specimens to the training set during preprocessing to mitigate dependency issues.

Result: The simulations showed a strong relationship between sample size and RMSE scaling. The paper analytically derived scaling slopes in Procrustes tangent space and demonstrated the importance of accounting for spatial autocorrelation among landmarks in ML models.

Conclusion: The research identifies preprocessing flaws in GMM for ML and demonstrates the impact of GPA-induced contamination on results. It provides solutions and guidelines for robust preprocessing to enhance ML-GMM integration.

Abstract: Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust "diagonal" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.

</details>


### [331] [3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control](https://arxiv.org/abs/2601.18451)
*Xuanmeng Sha,Liyun Zhang,Tomohiro Mashita,Naoya Chiba,Yuki Uranishi*

Main category: cs.CV

TL;DR: The paper presents 3DGesPolicy, a framework addressing incoherent and unstable gesture generation by modeling holistic continuous trajectories, achieving natural and synchronized co-speech gestures.


<details>
  <summary>Details</summary>
Motivation: To resolve issues of semantic incoherence and spatial instability in existing co-speech motion generation methods, particularly when integrating body motion and facial expressions.

Method: 3DGesPolicy, a continuous trajectory control framework using diffusion policy, models frame-to-frame holistic actions and integrates a Gesture-Audio-Phoneme fusion module for refined multi-modal alignment.

Result: The approach achieves spatially and semantically coherent movements, aligning gestures closely with speech, surpassing state-of-the-art methods on the BEAT2 dataset.

Conclusion: 3DGesPolicy ensures realistic, expressive, and synchronized co-speech gestures by addressing both semantic coherence and spatial alignment challenges.

Abstract: Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.

</details>


### [332] [Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System](https://arxiv.org/abs/2601.18464)
*Wenbin Wei,Suyuan Yao,Cheng Huang,Xiangyu Gao*

Main category: cs.CV

TL;DR: This paper introduces Fair-Eye Net, an AI system designed to enhance glaucoma detection and follow-up by integrating multimodal data. It addresses equity, reliability, and efficiency in clinical glaucoma care.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the early detection and longitudinal care of glaucoma, addressing subjectivity, fragmented care, access disparities, and the lack of tools for equitable real-world usage.

Method: Fair-Eye Net combines fundus photos, OCT metrics, visual field indices, and demographic data using a dual-stream heterogeneous fusion architecture with an uncertainty-aware hierarchical gating strategy for selective prediction and referral.

Result: The system achieved an AUC of 0.912, reduced racial false-negativity disparities by 73.4%, demonstrated stable cross-domain performance, and provided early risk alerts with high sensitivity and specificity.

Conclusion: Fair-Eye Net effectively integrates multimodal data, emphasizes fairness and clinical reliability, and demonstrates potential for deployment in advancing equitable global eye health care.

Abstract: Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.

</details>


### [333] [DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment](https://arxiv.org/abs/2601.18493)
*Sara Tehrani,Yonghao Xu,Leif Haglund,Amanda Berg,Michael Felsberg*

Main category: cs.CV

TL;DR: The paper introduces DisasterInsight, a multimodal benchmark improving disaster response via satellite imagery analysis. It proposes DI-Chat, a vision-language model fine-tuned for disaster-specific tasks, achieving better results in some metrics but highlighting challenges in building-function classification.


<details>
  <summary>Details</summary>
Motivation: The work aims to create a realistic benchmark for vision-language models to address critical shortcomings in current disaster response analysis using satellite data, such as lack of functional understanding and instruction robustness.

Method: The authors restructured the xBD dataset to include 112K building-centered instances, allowing for diverse disaster analysis tasks. They also developed DI-Chat by fine-tuning models with Low-Rank Adaptation (LoRA) on disaster-specific training data.

Result: DI-Chat demonstrated improved performance for disaster-specific tasks like damage-level and disaster-type classifications and structured report generation while maintaining significant gaps in building-function classification.

Conclusion: DisasterInsight bridges the gap in evaluating multimodal reasoning in disaster imagery, though challenges remain. DI-Chat shows improvement in specialized tasks, marking progress in vision-language model adaptability for disaster response.

Abstract: Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.

</details>


### [334] [From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation](https://arxiv.org/abs/2601.18532)
*Devon Levy,Bar Assayag,Laura Gaspar,Ilan Shimshoni,Bella Specktor-Fadida*

Main category: cs.CV

TL;DR: This paper introduces a novel active learning method for segmentation annotations using foundation-model embeddings and spatial diversity considerations, achieving higher accuracy with low data.


<details>
  <summary>Details</summary>
Motivation: Manual labeling for segmentation annotations is time-consuming and expertise-dependent, crucial for disease monitoring, necessitating improved sample selection methodologies for efficiency.

Method: The study combines foundation-model embeddings and clustering for diverse cold-start sampling, followed by an uncertainty-driven active learning approach incorporating spatial diversity.

Result: On multiple datasets (CheXmask, Montgomery, SynthStrip), the novel sample selection strategies yielded significant improvements in Dice scores and Hausdorff distances compared to baseline methods.

Conclusion: The framework is effective in low-data scenarios, improving segmentation accuracy while being interpretable and intuitive with visualizable feature-space distributions.

Abstract: Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.

</details>


### [335] [GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning](https://arxiv.org/abs/2601.18543)
*Kaixun Jiang,Yuzheng Wang,Junjie Zhou,Pandeng Li,Zhihang Liu,Chen-Wei Xie,Zhaoyu Chen,Yun Zheng,Wenqiang Zhang*

Main category: cs.CV

TL;DR: GenAgent unifies visual understanding and image generation through a multimodal agentic model that iteratively refines multimodal outputs via reasoning and tool invocation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of unified models that face high training costs and inefficiencies in balancing visual understanding and generation.

Method: Introduced an agentic framework that separates visual understanding (handled by the multimodal model) and generation (managed through external image generation models), along with a two-stage training strategy combining supervised fine-tuning and reinforcement learning.

Result: GenAgent improves image generation performance on benchmarks (GenEval++ by +23.6%, WISE by +14%), exhibits cross-tool generalization, and achieves adaptive and scalable reasoning at test time.

Conclusion: GenAgent offers a novel and efficient multimodal framework that balances understanding and generation, achieving better adaptability, performance, and reasoning capabilities.

Abstract: We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{https://github.com/deep-kaixun/GenAgent}{this url}.

</details>


### [336] [REMAC: Reference-Based Martian Asymmetrical Image Compression](https://arxiv.org/abs/2601.18547)
*Qing Ding,Mai Xu,Shengxi Li,Xin Deng,Xin Zou*

Main category: cs.CV

TL;DR: The paper proposes a new Martian image compression method, REMAC, which improves on efficiency and performance by leveraging inter- and intra-image similarities while reducing computational complexity for Mars-to-Earth communication.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies in existing Martian image compression methods, which fail to account for computational constraints on Mars and overlook inter-image similarities.

Method: REMAC is a reference-based asymmetrical approach that shifts computational tasks to the resource-rich decoder, utilizing inter-image similarities with a reference-guided entropy module and ref-decoder, along with intra-image similarities via a multi-scale architecture.

Result: Experimental results show that REMAC reduces encoder complexity by 43.51% while achieving a BD-PSNR gain of 0.2664 dB compared to state-of-the-art methods.

Conclusion: REMAC improves Martian image compression by balancing computational tasks, leveraging visual similarities, and providing significant performance gains for space communication systems.

Abstract: To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \textit{intra-} and \textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.

</details>


### [337] [Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray](https://arxiv.org/abs/2601.18555)
*Roberto Di Via,Vito Paolo Pastore,Francesca Odone,Siôn Glyn-Jones,Irina Voiculescu*

Main category: cs.CV

TL;DR: The paper validates the use of MRI for assessing FemoroAcetabular Impingement (FAI), showing diagnostic equivalence with traditional X-rays using heatmap regression architectures.


<details>
  <summary>Details</summary>
Motivation: To enhance FAI diagnosis by demonstrating that MRI, traditionally used for 3D views, can achieve equivalent diagnostic accuracy as X-rays, thus integrating automated FAI assessment into MRI workflows.

Method: The study applies heatmap regression architectures for cross-modality validation in a matched-cohort study involving 89 patients with paired MRI and X-rays.

Result: MRI achieves equivalent localization and diagnostic accuracy for cam-type FAI, demonstrating feasibility for coronal views and potential for volumetric analysis.

Conclusion: MRI can be effectively integrated into routine workflows for automated FAI assessment, potentially replacing traditional X-ray measurements, with code released for broader adoption.

Abstract: Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions

</details>


### [338] [Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis](https://arxiv.org/abs/2601.18556)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: The paper introduces SDA-QEC, a novel framework combining simplified data augmentation and quantum-enhanced classification for medical imaging tasks addressing class imbalance in datasets.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in medical datasets often leads to biased models and low recall rates for minority classes, compromising diagnostic accuracy and increasing misdiagnosis risk.

Method: The method combines simplified diffusion-based augmentation for generating synthetic samples and a MobileNetV2 embedded with a quantum feature layer for improved feature discrimination.

Result: SDA-QEC demonstrated outstanding performance on coronary angiography classification with 98.33% accuracy, 98.78% AUC, and balanced specificity and sensitivity, outperforming traditional models like ResNet18 and DenseNet121.

Conclusion: The research showcases the effectiveness of integrating generative data augmentation and quantum-enhanced modeling to create robust and balanced medical AI systems for high-risk diagnostic scenarios.

Abstract: In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.

</details>


### [339] [AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging](https://arxiv.org/abs/2601.18560)
*Li Fang,Tianyu Li,Yanghong Lin,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: This paper introduces an AI-powered Satellite Edge Computing method to enable faster and autonomous hyperspectral image classification by satellites using a resource-efficient, non-deep learning framework integrated with a few-shot learning strategy.


<details>
  <summary>Details</summary>
Motivation: There is a pressing need to overcome the bottleneck of slow satellite downlink transmission speeds in critical applications like disaster monitoring and emergency mapping, where immediate response is essential.

Method: The paper proposes a lightweight non-deep learning framework utilizing few-shot learning and a two-stage pixel-wise label propagation scheme to perform hyperspectral image classification. The method avoids reliance on spatial information and deep networks while utilizing spectral features for labeling.

Result: The method enables satellites to autonomously classify hyperspectral images despite degraded image quality due to misaligned pixels and noise. A graph clustering algorithm further refines classification by determining anchor labels.

Conclusion: The proposed method achieves efficient onboard hyperspectral image classification under the resource constraints of satellite platforms, demonstrating its potential for real-time and autonomous applications.

Abstract: As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.

</details>


### [340] [Self-Refining Video Sampling](https://arxiv.org/abs/2601.18577)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Saining Xie,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: This paper presents a self-refining method for video sampling that improves motion and physics realism without external verifiers or additional training.


<details>
  <summary>Details</summary>
Motivation: Modern video generators struggle with achieving physical realism in complex dynamics, and existing methods are computationally expensive and insufficient for fine-grained motion refinement.

Method: The method uses a pre-trained video generator as a self-refiner, performing iterative refinement based on its role as a denoising autoencoder. It introduces uncertainty-aware refinement to focus on self-consistency and avoid artifacts.

Result: The approach shows notable enhancements in motion coherence and alignment with physics, with over 70% human preference compared to other methods.

Conclusion: The proposed self-refining video sampling method effectively improves physical realism in video generation without the need for external resources or retraining.

Abstract: Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.

</details>


### [341] [GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization](https://arxiv.org/abs/2601.18585)
*Chenxi Liu,Selena Ling,Alec Jacobson*

Main category: cs.CV

TL;DR: GimmBO facilitates efficient adapter merging for diffusion-based image generation by using Preferential Bayesian Optimization, overcoming challenges in weight selection while offering better convergence and success rates.


<details>
  <summary>Details</summary>
Motivation: To address scalability and inefficiency issues in manual slider-based tuning for merging adapters in diffusion-based image generation, especially when the number of adapters increases.

Method: Development of GimmBO, which utilizes a Preferential Bayesian Optimization framework with a two-stage backend optimized for high-dimensional spaces, addressing sparsity and constrained weight ranges.

Result: Simulated user experiments and a user study show GimmBO achieves improved convergence, high success rates, and outperforms baseline methods like BO and line-search.

Conclusion: GimmBO enhances interactive exploration of adapter merging for image generation, providing an effective, scalable alternative to manual tuning methods while demonstrating flexibility in its framework.

Abstract: Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.

</details>


### [342] [AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment](https://arxiv.org/abs/2601.18589)
*KV Karthikeya,Ashok Kumar Das,Shantanu Pal,Vivekananda Bhat K,Arun Sekar Rajasekaran*

Main category: cs.CV

TL;DR: AGSP-DSA framework enhances multimodal data fusion using graphs, attention mechanisms, and achieves top performance in sentiment analysis, event recognition, and classification.


<details>
  <summary>Details</summary>
Motivation: To improve robust multimodal fusion across heterogeneous sources like text, audio, and images using adaptive graph-based methods.

Method: Dual graph construction for intra- and inter-modal relation learning, spectral graph filtering, multi-scale GCN embeddings, and semantic-aware attention mechanism.

Result: AGSP-DSA outperforms state-of-the-art methods, showing high accuracy, F1-score, and robustness across CMU-MOSEI, AVE, and MM-IMDB benchmark datasets.

Conclusion: AGSP-DSA is highly efficient and generalizable for multimodal tasks, proving effective even in scenarios with missing data.

Abstract: In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.

</details>


### [343] [EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery](https://arxiv.org/abs/2601.18597)
*Yu Xia,Chang Liu,Tianqi Xiang,Zhigang Tu*

Main category: cs.CV

TL;DR: EFSI-DETR addresses the challenges of small object detection in UAV imagery through innovative semantic feature enhancement and frequency-spatial guidance.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in limited feature representation and ineffective multi-scale fusion in UAV-based small object detection.

Method: EFSI-DETR incorporates a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet), Efficient Semantic Feature Concentrator (ESFC), and Fine-grained Feature Retention (FFR).

Result: Experiments on VisDrone and CODrone benchmarks show SOTA performance with improvements of 1.6% in AP, 5.8% in APs, and 188 FPS real-time inference speed.

Conclusion: EFSI-DETR offers a new, computationally efficient framework that significantly enhances small object detection capabilities in UAV datasets with high inference speed.

Abstract: Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.

</details>


### [344] [Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures](https://arxiv.org/abs/2601.18619)
*Jorge Quesada,Ghassan AlRegib*

Main category: cs.CV

TL;DR: This paper enhances self-supervised learning (SSL) for segmentation tasks involving small, sparse structures by integrating small-window cropping during pretraining, yielding up to a 13% improvement in accuracy.


<details>
  <summary>Details</summary>
Motivation: To address limitations of SSL in segmenting small, sparse, or irregular objects, which is often ineffective with current methods tuned for large, homogeneous regions.

Method: A scale-aware SSL method is devised by incorporating small-window cropping into the augmentation pipeline for pretraining, focusing on fine-scale structures.

Result: The proposed method delivers up to a 13% accuracy boost for fault segmentation and 5% for cell delineation, outperforming standard and state-of-the-art baselines under label constraints.

Conclusion: The value of SSL is highly dependent on the scale of target objects; the paper establishes a principle of adapting SSL methods to the size and sparsity of objects for optimal performance in scientific imaging.

Abstract: Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.

</details>


### [345] [Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation](https://arxiv.org/abs/2601.18623)
*Zihao Wang,Yuzhou Chen,Shaogang Ren*

Main category: cs.CV

TL;DR: This paper introduces a new approach for cross-modal image translation, aiming to address inefficiencies found in standard diffusion models by incorporating domain-shift dynamics into the generative process.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve cross-modal image translation, which is typically inefficient and prone to semantic drift due to the reliance on global linear domain transfer in standard diffusion models.

Method: The model incorporates spatially varying mixing fields at every reverse step and introduces a target-consistent restoration term into the drift. This approach replaces global alignment with local residual correction.

Result: The proposed framework improves structural fidelity and semantic consistency across medical imaging, remote sensing, and electroluminescence tasks while requiring fewer denoising steps.

Conclusion: By embedding domain-shift dynamics into the generative process, the method enhances efficiency, fidelity, and consistency in cross-modal image translation, shifting the focus from global to local correction.

Abstract: Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.

</details>


### [346] [CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search](https://arxiv.org/abs/2601.18625)
*Zequn Xie*

Main category: cs.CV

TL;DR: CONQUER is a two-stage framework improving text-based person search by tackling cross-modal discrepancies and vague queries. 


<details>
  <summary>Details</summary>
Motivation: Text-Based Person Search helps retrieve pedestrian images via natural language descriptions, crucial for public safety tasks. Challenges include cross-modal discrepancies and ambiguous user inputs.

Method: CONQUER combines robust training with multi-granularity encoding and context-guided matching, along with adaptive query refinement during inference via anchor selection and enrichment.

Result: Experiments on three benchmarks (CUHK-PEDES, ICFG-PEDES, RSTPReid) demonstrate superior Rank-1 accuracy and mAP, especially in cross-domain and incomplete-query cases.

Conclusion: CONQUER provides a practical, high-performing solution for real-world Text-Based Person Search deployment. Source code is publicly available for further exploration.

Abstract: Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.

</details>


### [347] [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/abs/2601.18633)
*Tong Shi,Melonie de Almeida,Daniela Ivanova,Nicolas Pugeault,Paul Henderson*

Main category: cs.CV

TL;DR: Splat-Portrait is a Gaussian-splatting-based method that improves the realism of 3D talking head generation by addressing 3D reconstruction and lip motion synthesis, without relying on motion-driven priors or landmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of previous 3D talking head methods that use heuristics and priors but fail to achieve realistic 3D avatar reconstructions.

Method: The paper proposes a technique that disentangles a single portrait into a 3D static representation with Gaussian Splatting and a predicted 2D background. It uses purely 2D reconstruction and score-distillation losses for training with no reliance on 3D supervision or landmarks.

Result: Experimental results show superior performance in talking head generation and novel view synthesis, delivering visually higher-quality outputs compared to prior methods.

Conclusion: Splat-Portrait successfully improves 3D head reconstruction and lip motion synthesis, pushing the frontiers of talking head generation while eliminating reliance on domain-specific heuristics. Code and resources are publicly available.

Abstract: Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.

</details>


### [348] [Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge](https://arxiv.org/abs/2601.18698)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: This paper evaluates geographic equity in text-to-video generation models by introducing a systematic framework (GAP) and a global benchmark (GEOATTRACTION-500). It finds that the Sora 2 model demonstrates a relatively balanced level of geographically grounded visual knowledge.


<details>
  <summary>Details</summary>
Motivation: Investigate whether text-to-video models encode geographically equitable visual knowledge and how well they synthesize global landmarks.

Method: Development of GAP, a framework for evaluating video models using metrics like structural alignment and vision-language judgments, validated by human evaluations, alongside the GEOATTRACTION-500 benchmark.

Result: The evaluation of the Sora 2 model revealed a consistent level of geographically grounded visual knowledge across regions, development levels, and cultural groups, with minimal bias based on popularity.

Conclusion: Contrary to assumptions, state-of-the-art text-to-video models exhibit unexpected geographic equity, suggesting a potential for global applications; continuous evaluation is crucial as these models advance.

Abstract: Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.

</details>


### [349] [SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification](https://arxiv.org/abs/2601.18739)
*Ignacio Antequera-Sánchez,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.CV

TL;DR: The paper introduces SeNeDiF-OOD, a hierarchical framework for OOD detection, designed to handle diverse OOD data with improved performance.


<details>
  <summary>Details</summary>
Motivation: Deploying AI in open-world environments requires robust OOD detection to manage diverse and unpredictable OOD inputs. Traditional single-stage detectors struggle with these challenges.

Method: The proposed method, SeNeDiF-OOD, utilizes Semantic Nested Dichotomy Fusion, organizing detection tasks hierarchically into binary fusion nodes, each targeting specific semantic levels.

Result: Extensive experimental evaluation on MonuMAI demonstrates the framework's superiority in filtering diverse OOD categories while maintaining strong in-distribution performance.

Conclusion: SeNeDiF-OOD is effective for OOD detection in challenging open-world settings, outperforming traditional baselines and offering robust solutions for real-world applications.

Abstract: Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [350] [Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs](https://arxiv.org/abs/2601.17136)
*Julian Bellavita,Matthew Rubino,Nakul Iyer,Andrew Chang,Aditya Devarakonda,Flavio Vella,Giulia Guidi*

Main category: cs.DC

TL;DR: The paper introduces distributed-memory parallel algorithms for Kernel K-means on multi-GPU systems, scaling clustering to datasets much larger than previously possible.


<details>
  <summary>Details</summary>
Motivation: K-means clustering is simple but fails for non-linear clusters. Kernel K-means addresses this but faces computational and memory challenges for large datasets, particularly on single GPUs.

Method: The authors design communication-efficient distributed linear algebra primitives and partitioning schemes for Kernel K-means on multi-GPU systems, including a highly scalable 1.5D algorithm.

Result: The 1.5D algorithm demonstrates scalability for million-scale datasets, achieving high efficiency (79.7%) on 256 GPUs and drastically reducing clustering time from hours to seconds.

Conclusion: Application-specific distributed algorithms can significantly improve performance, enabling large-scale Kernel K-means clustering on modern GPU systems.

Abstract: Clustering is an important tool in data analysis, with K-means being popular for its simplicity and versatility. However, it cannot handle non-linearly separable clusters. Kernel K-means addresses this limitation but requires a large kernel matrix, making it computationally and memory intensive. Prior work has accelerated Kernel K-means by formulating it using sparse linear algebra primitives and implementing it on a single GPU. However, that approach cannot run on datasets with more than approximately 80,000 samples due to limited GPU memory.
  In this work, we address this issue by presenting a suite of distributed-memory parallel algorithms for large-scale Kernel K-means clustering on multi-GPU systems. Our approach maps the most computationally expensive components of Kernel K-means onto communication-efficient distributed linear algebra primitives uniquely tailored for Kernel K-means, enabling highly scalable implementations that efficiently cluster million-scale datasets. Central to our work is the design of partitioning schemes that enable communication-efficient composition of the linear algebra primitives that appear in Kernel K-means.
  Our 1.5D algorithm consistently achieves the highest performance, enabling Kernel K-means to scale to data one to two orders of magnitude larger than previously practical. On 256 GPUs, it achieves a geometric mean weak scaling efficiency of $79.7\%$ and a geometric mean strong scaling speedup of $4.2\times$. Compared to our 1D algorithm, the 1.5D approach achieves up to a $3.6\times$ speedup on 256 GPUs and reduces clustering time from over an hour to under two seconds relative to a single-GPU sliding window implementation. Our results show that distributed algorithms designed with application-specific linear algebraic formulations can achieve substantial performance improvement.

</details>


### [351] [Push Down Optimization for Distributed Multi Cloud Data Integration](https://arxiv.org/abs/2601.17546)
*Ravi Kiran Kodali,Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Balakrishna Pothineni,Aswathnarayan Muthukrishnan Kirubakaran,Sumit Saha,Nachiappan Chockalingam*

Main category: cs.DC

TL;DR: This study assesses the feasibility of push down optimization in multi-cloud ETL pipelines, highlighting its benefits, challenges, and recommending strategies for improved scalability and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges faced by enterprises in optimizing ETL pipelines across multi-cloud environments, particularly focusing on latency, cost, and data transfer complexities.

Method: The paper evaluates localized push down, hybrid models, and data federation techniques, along with a case study comparing Redshift and BigQuery for performance analysis.

Result: The case study demonstrated measurable performance benefits, such as reduced runtime, lower data transfer volume, and improved cost efficiency when employing push down optimization in multi-cloud settings.

Conclusion: Push down optimization, with tailored strategies, offers significant advantages in multi-cloud ETL pipelines, improving both scalability and reliability, but demands careful orchestration to manage associated challenges.

Abstract: Enterprises increasingly adopt multi cloud architectures to take advantage of diverse database engines, regional availability, and cost models. In these environments, ETL pipelines must process large, distributed datasets while minimizing latency and transfer cost. Push down optimization, which executes transformation logic within database engines rather than within the ETL tool, has proven highly effective in single cloud systems. However, when applied across multiple clouds, it faces challenges related to data movement, heterogeneous SQL engines, orchestration complexity, and fragmented security controls. This paper examines the feasibility of push down optimization in multi cloud ETL pipelines and analyzes its benefits and limitations. It evaluates localized push down, hybrid models, and data federation techniques that reduce cross cloud traffic while improving performance. A case study across Redshift and BigQuery demonstrates measurable gains, including lower end to end runtime, reduced transfer volume, and improved cost efficiency. The study highlights practical strategies that organizations can adopt to improve ETL scalability and reliability in distributed cloud environments.

</details>


### [352] [A Unified Approach to Concurrent, Parallel Map-Reduce in R using Futures](https://arxiv.org/abs/2601.17578)
*Henrik Bengtsson*

Main category: cs.DC

TL;DR: The futurize package simplifies parallel computing in R by converting sequential map-reduce expressions into parallel ones using a single function, futurize(), which integrates with diverse map-reduce APIs.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the complexity and incompatibility of learning multiple parallel APIs in the R ecosystem for iterative map-reduce computations.

Method: The paper introduces the futurize package, which transpiles sequential code into parallel equivalents using a unified function, minimizing refactoring through integration with R's native pipe operator.

Result: The result is a streamlined approach to parallel computing in R, where developers declare tasks to parallelize with futurize() and end-users configure execution with plan(). It supports multiple frameworks and domain-specific tasks.

Conclusion: The futurize package provides a significant abstraction and unification for parallelizing map-reduce operations in R, reducing code complexity and enabling simpler user adoption of parallel computation.

Abstract: The R ecosystem offers a rich variety of map-reduce application programming interfaces (APIs) for iterative computations, yet parallelizing code across these diverse frameworks requires learning multiple, often incompatible, parallel APIs. The futurize package addresses this challenge by providing a single function, futurize(), which transpiles sequential map-reduce expressions into their parallel equivalents in the future ecosystem, which performs all the heavy lifting. By leveraging R's native pipe operator, users can parallelize existing code with minimal refactoring -- often by simply appending `|> futurize()' to an expression. The package supports classical map-reduce functions from base R, purrr, crossmap, foreach, plyr, BiocParallel, e.g., lapply(xs, fcn) |> futurize() and map(xs, fcn) |> futurize(), as well as a growing set of domain-specific packages, e.g., boot, caret, glmnet, lme4, mgcv, and tm. By abstracting away the underlying parallel machinery, and unifying handling of future options, the package enables developers to declare what to parallelize via futurize(), and end-users to choose how via plan(). This article describes the philosophy, design, and implementation of futurize, demonstrates its usage across various map-reduce paradigms, and discusses its role in simplifying parallel computing in R.

</details>


### [353] [On the Bandwidth Consumption of Blockchains](https://arxiv.org/abs/2601.18400)
*Andrei Lebedev,Vincent Gramoli*

Main category: cs.DC

TL;DR: This paper presents the first-ever comparative study on the bandwidth consumption of blockchain protocols.


<details>
  <summary>Details</summary>
Motivation: To address the lack of empirical studies regarding the bandwidth consumption of blockchains, despite their growing presence and associated costs.

Method: Measured and analyzed the network traffic of nodes from five blockchain protocols: Algorand, Aptos, Avalanche, Redbelly, and Solana, considering variations over time, node roles, and network sizes.

Result: Identified that transport protocol significantly impacts traffic, segregating node roles reduces bandwidth usage, and network size affects blockchain performance differently.

Conclusion: The findings provide an understanding of how blockchain protocols handle traffic, offering insights for optimizing their network efficiency.

Abstract: With the advent of blockchain technology, the number of proposals has boomed. The network traffic imposed by these blockchain proposals increases the cost of hosting nodes. Unfortunately, as of today, we are not aware of any comparative study of the bandwidth consumption of blockchains.
  In this paper, we propose the first empirical comparison of blockchain bandwidth consumption. To this end, we measure the network traffic of blockchain network nodes of five blockchain protocols: Algorand, Aptos, Avalanche, Redbelly and Solana. We study the variation over time, differentiate the receiving and sending traffic and analyze how this traffic varies with the number of nodes and validators.
  We conclude that the transport protocol is the main factor impacting the network traffic, segregating node roles helps reduce traffic and different blockchains are differently impacted by the network size.

</details>


### [354] [Lightspeed Data Compute for the Space Era](https://arxiv.org/abs/2601.17589)
*Thomas Sandholm,Bernardo A. Huberman,Klas Segeljakt,Paris Carbone*

Main category: cs.DC

TL;DR: SpaceCoMP is a data processing model for LEO satellite networks that processes data in orbit to overcome downlink bandwidth limitations and provides significant improvements in efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the issue of insufficient downlink bandwidth, which prevents much of Earth's satellite data from being received and utilized on the ground.

Method: The authors propose SpaceCoMP, a MapReduce-inspired framework for satellite mesh networks. This approach uses inter-satellite laser links to collect and process data cooperatively in orbit. The model incorporates a distance-aware routing protocol and a bipartite match scheduling strategy to optimize task allocation and reduce costs.

Result: Simulations with satellite constellations of 1,000-10,000 units demonstrated substantial improvements: 61-79% in map placement efficiency over baselines, 18-28% over greedy allocation methods, and 67-72% reduction in aggregation costs.

Conclusion: SpaceCoMP showcases that Low Earth Orbit satellite networks can be transformed from mere communication relays into efficient platforms for high-speed processing of Earth observation data.

Abstract: While thousands of satellites photograph Earth every day, most of that data never makes it to the ground because downlink bandwidth simply cannot keep up. Processing data in the Low Earth Orbit (LEO) zone offers promising capabilities to overcome this limitation. We propose SpaceCoMP, a MapReduce-inspired processing model for LEO satellite mesh networks. Ground stations submit queries over an area of interest; satellites collect sensor data, process it cooperatively at light-speed using inter-satellite laser links, and return only the results. Our compute model leverages space physics to accelerate computations on LEO megaconstellations. Our distance-aware routing protocol exploits orbital geometry. In addition, our bipartite match scheduling strategy places map and reduce tasks within orbital regions while minimizing aggregation costs. We have simulated constellations of 1,000-10,000 satellites showcasing 61-79% improvement in map placement efficiency over baselines, 18-28% over greedy allocation, and 67-72% reduction in aggregation cost. SpaceCoMP demonstrates that the orbital mesh is not merely useful as a communication relay, as seen today, but can provide the foundations for faster data processing above the skies.

</details>


### [355] [Scaling All-to-all Operations Across Emerging Many-Core Supercomputers](https://arxiv.org/abs/2601.17606)
*Shannon Kinkead,Jackson Wesley,Whit Schonbein,David DeBonis,Matthew G. F. Dosanjh,Amanda Bienz*

Main category: cs.DC

TL;DR: This paper introduces new all-to-all collective algorithms for MPI, delivering up to 3x speedup over traditional MPI methods on many-core systems.


<details>
  <summary>Details</summary>
Motivation: The need for efficient MPI collective operations arises from their critical role in FFTs, transpositions, and machine learning tasks on modern many-core systems.

Method: The authors develop and propose novel algorithms for MPI all-to-all communications and benchmark them against existing algorithms and system MPI.

Result: The proposed algorithms demonstrate up to 3x performance improvement over system MPI on Sapphire Rapids systems with 32 nodes.

Conclusion: The novel algorithms excel in improving MPI performance for all-to-all operations, particularly on many-core systems, proving their efficacy over traditional methods.

Abstract: Performant all-to-all collective operations in MPI are critical to fast Fourier transforms, transposition, and machine learning applications. There are many existing implementations for all-to-all exchanges on emerging systems, with the achieved performance dependent on many factors, including message size, process count, architecture, and parallel system partition. This paper presents novel all-to-all algorithms for emerging many-core systems. Further, the paper presents a performance analysis against existing algorithms and system MPI, with novel algorithms achieving up to 3x speedup over system MPI at 32 nodes of state-of-the-art Sapphire Rapids systems.

</details>


### [356] [An MLIR Lowering Pipeline for Stencils at Wafer-Scale](https://arxiv.org/abs/2601.17754)
*Nicolai Stawinoga,David Katz,Anton Lydike,Justs Zarins,Nick Brown,George Bisbas,Tobias Grosser*

Main category: cs.DC

TL;DR: This paper presents a compiler pipeline for transforming stencil-based kernels into optimized code for the Cerebras Wafer-Scale Engine (WSE), achieving performance comparably better than manual optimization without code changes.


<details>
  <summary>Details</summary>
Motivation: The Cerebras WSE, with its unique architecture, shows potential for HPC applications but requires significant re-implementation efforts for porting existing code due to its asynchronous programming model. Automating this process is critical for better utilization.

Method: A domain-specific compiler pipeline was developed to transform stencil-based kernels into optimized code for the WSE, enabling the automatic targeting of WSE's asynchronous execution model without manual changes to application code.

Result: The approach delivered performance comparable to or better than manually optimized code. Tests showed that it achieved 14x and 20x speed-ups on the WSE3 compared to 128 Nvidia A100 GPUs and 128 Cray-EX supercomputer nodes, respectively.

Conclusion: The work demonstrates that compiler optimizations using stencil domain-specific information can bridge the gap between mathematical problem abstraction and WSE's execution model, unlocking significant performance improvements with minimal effort.

Abstract: The Cerebras Wafer-Scale Engine (WSE) delivers performance at an unprecedented scale of over 900,000 compute units, all connected via a single-wafer on-chip interconnect. Initially designed for AI, the WSE architecture is also well-suited for High Performance Computing (HPC). However, its distributed asynchronous programming model diverges significantly from the simple sequential or bulk-synchronous programs that one would typically derive for a given mathematical program description. Targeting the WSE requires a bespoke re-implementation when porting existing code. The absence of WSE support in compilers such as MLIR, meant that there was little hope for automating this process.
  Stencils are ubiquitous in HPC, and in this paper we explore the hypothesis that domain specific information about stencils can be leveraged by the compiler to automatically target the WSE without requiring application-level code changes. We present a compiler pipeline that transforms stencil-based kernels into highly optimized CSL code for the WSE, bridging the semantic gap between the mathematical representation of the problem and the WSE's asynchronous execution model. Based upon five benchmarks across three HPC programming technologies, running on both the Cerebras WSE2 and WSE3, our approach delivers comparable, if not slightly better, performance than manually optimized code. Furthermore, without requiring any application level code changes, performance on the WSE3 is around 14 times faster than 128 Nvidia A100 GPUs and 20 times faster than 128 nodes of a CPU-based Cray-EX supercomputer when using our approach.

</details>


### [357] [Multi-core & GPU-based Balanced Butterfly Counting in Signed Bipartite Graphs](https://arxiv.org/abs/2601.17707)
*Mekala Kiran,Apurba Das,Suman Banerjee,Tathagata Ray*

Main category: cs.DC

TL;DR: The paper proposes scalable parallel algorithms for balanced butterfly counting in signed bipartite graphs, achieving significant speedups on GPUs and multicore CPUs.


<details>
  <summary>Details</summary>
Motivation: Balanced butterfly counting is vital for understanding higher-order structural properties in signed bipartite graphs, but existing methods are computationally expensive for large graphs.

Method: The study presents parallel implementations for multicore CPUs (M-BBC) using vertex-level parallelism and GPUs (G-BBC, G-BBC++) with tile-based approaches and dynamic scheduling to improve scalability and efficiency.

Result: Experimental results demonstrate M-BBC achieves up to 71.13x speedup while GPU algorithms achieve up to 13,320x speedup, significantly outperforming prior methods.

Conclusion: The proposed parallel algorithms offer a highly scalable and efficient solution for balanced butterfly counting, enabling high-performance analysis of massive bipartite graphs.

Abstract: Balanced butterfly counting, corresponding to counting balanced (2, 2)-bicliques, is a fundamental primitive in the analysis of signed bipartite graphs and provides a basis for studying higher-order structural properties such as clustering coefficients and community structure. Although prior work has proposed an efficient CPU-based serial method for counting balanced (2, k)-bicliques. The computational cost of balanced butterfly counting remains a major bottleneck on large-scale graphs. In this work, we present the highly parallel implementations for balanced butterfly counting for both multicore CPUs and GPUs. The proposed multi-core algorithm (M-BBC) employs fine-grained vertex-level parallelism to accelerate wedge-based counting while eliminating the generation of unbalanced substructures. To improve scalability, we develop a GPU-based method (G-BBC) that uses a tile-based parallel approach to effectively leverage shared memory while handling large vertex sets. We then present an improved variation, G-BBC++, which integrates dynamic scheduling to mitigate workload imbalance and maximize throughput. We conduct an experimental assessment of the proposed methods across 15 real-world datasets. Experimental results exhibit that M-BBC achieves speedups of up to 71.13x (average 38.13x) over the sequential baseline BB2K. The GPU-based algorithms deliver even greater improvements, achieving up to 13,320x speedup (average 2,600x) over BB2K and outperforming M-BBC by up to 186x (average 50x). These results indicate the substantial scalability and efficiency of our parallel algorithms and establish a robust foundation for high-performance signed motif analysis on massive bipartite graphs.

</details>


### [358] [CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation](https://arxiv.org/abs/2601.17774)
*Zizhao Zhang,Yihan Xue,Haotian Zhu,Sijia Li,Zhijun Wang,Yujie Xiao*

Main category: cs.DC

TL;DR: The paper presents CondenseGraph, a framework to reduce communication overhead in distributed GNN training by dynamically condensing graph data during transmission while maintaining training accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the communication bottleneck in distributed GNN training caused by the neighbor explosion problem requiring frequent data exchange across graph partitions.

Method: Developed a graph condensation mechanism to compress boundary node features into super nodes and introduced a gradient-based error feedback for information loss compensation.

Result: CondenseGraph reduces communication volume by 40-60%, lowers training time, and achieves accuracy comparable to full-precision methods on benchmark datasets.

Conclusion: CondenseGraph offers a scalable and efficient solution for distributed GNN training, balancing communication efficiency with model accuracy.

Abstract: Distributed Graph Neural Network (GNN) training suffers from substantial communication overhead due to the inherent neighborhood dependency in graph-structured data. This neighbor explosion problem requires workers to frequently exchange boundary node features across partitions, creating a communication bottleneck that severely limits training scalability. Existing approaches rely on static graph partitioning strategies that cannot adapt to dynamic network conditions. In this paper, we propose CondenseGraph, a novel communication-efficient framework for distributed GNN training. Our key innovation is an on-the-fly graph condensation mechanism that dynamically compresses boundary node features into compact super nodes before transmission. To compensate for the information loss introduced by compression, we develop a gradient-based error feedback mechanism that maintains convergence guarantees while reducing communication volume by 40-60%. Extensive experiments on four benchmark datasets demonstrate that CondenseGraph achieves comparable accuracy to full-precision baselines while significantly reducing communication costs and training time.

</details>


### [359] [A Universal Load Balancing Principle and Its Application to Large Language Model Serving](https://arxiv.org/abs/2601.17855)
*Zixi Chen,Tianci Bu,Chendong Song,Xin Lu,Yinyu Ye,Zijie Zhou*

Main category: cs.DC

TL;DR: The paper addresses load-balancing challenges in barrier-synchronized systems by proposing a universal principle to reduce imbalance, optimizing throughput, latency, and energy consumption in large language model serving.


<details>
  <summary>Details</summary>
Motivation: The bottleneck in large language model (LLM) serving due to barrier-synchronized workloads leads to persistent stragglers and high idle times, necessitating effective load-balancing strategies.

Method: The authors present a universal load-balancing principle based on finite-horizon integer-optimization, providing worst-case guarantees and theoretical improvements in imbalance reduction.

Result: Experiments demonstrate significant enhancements in throughput, latency, and energy efficiency for LLM serving, validating the theoretical framework.

Conclusion: The proposed framework offers a sustainable and theoretically grounded solution for load balancing in LLM serving and similar synchronization-gated workloads, with broad applicability.

Abstract: Load balancing-the allocation of work across parallel resources to reduce delay, energy and cost-is a pervasive challenge in science and engineering, from large-scale simulation and data processing to cloud and manufacturing operations. Motivated by the emerging bottleneck in large language model (LLM) serving, we study a particularly stringent regime of load balancing that arises in barrier-synchronized, stateful systems: work cannot be freely migrated and progress is gated by the slowest participant at each step, so heterogeneity and temporal drift in workloads create persistent stragglers and substantial idle time. LLM serving under data-parallel decoding provides a prominent modern instance: in production traces, barrier-induced idle can exceed 40% of compute time per decode step. Here we develop a universal load-balancing principle, which admits a step-wise finite-horizon integer-optimization formulation and yields worst-case guarantees: across LLM decode models and a broader class of non-decreasing workload drift processes, it reduces long-run imbalance by a factor that grows with batch size and system scale. Extensive experiments corroborate the theory, showing substantial improvements in throughput and latency together with reductions in energy consumption. These results provide a general, theoretically grounded framework for load balancing, with immediate implications for sustainable LLM serving and broad relevance to other synchronization-gated resource-allocation problems.

</details>


### [360] [An Initial Evaluation of Distributed Graph Algorithms using NWGraph and HPX](https://arxiv.org/abs/2601.18158)
*Karame Mohammadiporshokooh,Panagiotis Syskakis,Hartmut Kaiser*

Main category: cs.DC

TL;DR: The paper introduces a distributed implementation of NWGraph with HPX runtime for scalable graph analytics, aiming to reduce synchronization and improve performance. Initial evaluation shows mixed results relative to Boost Graph Library.


<details>
  <summary>Details</summary>
Motivation: Graphs are essential for modeling relationships across various domains, but their increasing scale necessitates efficient distributed solutions as single-node systems become insufficient.

Method: The authors integrate NWGraph library with HPX runtime, utilizing HPX's asynchronous many-task model to address bottlenecks like latency, irregular memory access, and synchronization issues.

Result: Breadth-First-Search (BFS) showed superior performance over the Boost Graph Library, whereas PageRank revealed challenges, failing to surpass BGL.

Conclusion: Asynchronous task-based runtimes show promise for distributed graph processing, though challenges remain, emphasizing the need for future optimization efforts.

Abstract: Graphs are central to modeling relationships in scientific computing, data analysis, and AI/ML, but their growing scale can exceed the memory and compute capacity of single nodes, requiring distributed solutions. Existing distributed graph framework, however, face fundamental challenges: graph algorithms are latency-bound, suffer from irregular memory access, and often impose synchronization costs that limit scalability and efficiency. In this work, we present a distributed implementation of the NWGraph library integrated with the HPX runtime system. By leveraging HPX's asynchronous many-task model, our approach aims to reduce synchronization overhead, improve load balance, and provide a foundation for distributed graph analytics. We evaluate this approach using two representative algorithms: Breadth-First-Search (BFS) and (PageRank). Our initial results show that BFS achieves better performance than the distributed Boost Graph Library (BGL), while PageRank remains more challenging, with current implementation not yet outperforming BGL. These findings highlight both the promise and the open challenges of applying asynchronous task-based runtimes to graph processing, and point to opportunities for future optimizations and extensions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [361] [TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation](https://arxiv.org/abs/2601.16984)
*Rahul Ghosh,Chun-Hao Liu,Gaurav Rele,Vidya Sagar Ravipati,Hazar Aouad*

Main category: cs.LG

TL;DR: This paper introduces TelcoAI, a Retrieval-Augmented Generation (RAG) system that effectively aids the understanding of complex 3GPP telecommunication documents using advanced multi-modal capabilities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in processing 3GPP telecommunication documents, which are difficult to manage due to their hierarchical, dense, and multi-modal content. Existing methods are insufficient in handling such complexity.

Method: The authors developed TelcoAI, employing techniques like section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams.

Result: TelcoAI achieved significant performance improvements, including 87% recall, 83% claim recall, and 92% faithfulness, representing a 16% improvement over current methods.

Conclusion: TelcoAI demonstrates that agentic and multi-modal reasoning can effectively handle complex technical documents, providing practical advancements for telecommunications research and engineering.

Abstract: The 3rd Generation Partnership Project (3GPP) produces complex technical specifications essential to global telecommunications, yet their hierarchical structure, dense formatting, and multi-modal content make them difficult to process. While Large Language Models (LLMs) show promise, existing approaches fall short in handling complex queries, visual information, and document interdependencies. We present TelcoAI, an agentic, multi-modal Retrieval-Augmented Generation (RAG) system tailored for 3GPP documentation. TelcoAI introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams. Evaluated on multiple benchmarks-including expert-curated queries-our system achieves $87\%$ recall, $83\%$ claim recall, and $92\%$ faithfulness, representing a $16\%$ improvement over state-of-the-art baselines. These results demonstrate the effectiveness of agentic and multi-modal reasoning in technical document understanding, advancing practical solutions for real-world telecommunications research and engineering.

</details>


### [362] [Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2601.16991)
*Longteng Zhang,Sen Wu,Shuai Hou,Zhengyu Qing,Zhuo Zheng,Danning Ke,Qihong Lin,Qiang Wang,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: SALR successfully merges Low-Rank Adaptation (LoRA) and sparse pruning to enable efficient runtime and storage in resource-constrained environments, achieving significant sparsity without sacrificing model performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency of adapting large pre-trained language models for downstream tasks due to high storage and computational costs.

Method: It proposes SALR, which combines sparse pruning and low-rank adaptation in a unified framework. This includes pruning only base weights, using a truncated SVD adapter for residual recovery, and optimizing hardware efficiency with fused adapters and bitmap-based encoding.

Result: SALR achieved 50% sparsity in large language models, maintained performance comparable to LoRA on benchmarks like GSM8K and MMLU, and reduced model size by 2× while enabling up to 1.7× inference speedup.

Conclusion: SALR proves to be an effective and efficient fine-tuning solution for large models that enhances computational performance and reduces storage demands without compromising accuracy.

Abstract: Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\times$, and delivers up to a $1.7\times$ inference speedup.

</details>


### [363] [A Dataset of Dengue Hospitalizations in Brazil (1999 to 2021) with Weekly Disaggregation from Monthly Counts](https://arxiv.org/abs/2601.16994)
*Lucas M. Morello,Matheus Lima Castro,Pedro Cesar M. G. Camargo,Liliane Moreira Nery,Darllan Collins da Cunha e Silva,Leopoldo Lusquino Filho*

Main category: cs.LG

TL;DR: This paper introduces a dataset enhancing the temporal granularity of dengue hospitalization data across Brazil, transforming monthly records into weekly series to better support AI-based epidemiological forecasting.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the temporal resolution of dengue hospitalization data to train more effective AI models for epidemiological forecasting.

Method: The approach involves disaggregating monthly dengue hospitalization data to weekly resolution using cubic spline interpolation, validated against high-resolution reference data from Sao Paulo. Explanatory variables are harmonized accordingly for multivariate compatibility.

Result: The authors successfully created weekly dengue hospitalization time series for the period 1999-2021. Statistical assessments demonstrated cubic spline interpolation's superior performance in disaggregation.

Conclusion: The dataset offers a valuable resource for multivariate time-series analysis, environmental health studies, and the development of machine learning models for epidemiological forecasting.

Abstract: This data paper describes and publicly releases this dataset (v1.0.0), published on Zenodo under DOI 10.5281/zenodo.18189192. Motivated by the need to increase the temporal granularity of originally monthly data to enable more effective training of AI models for epidemiological forecasting, the dataset harmonizes municipal-level dengue hospitalization time series across Brazil and disaggregates them to weekly resolution (epidemiological weeks) through an interpolation protocol with a correction step that preserves monthly totals. The statistical and temporal validity of this disaggregation was assessed using a high-resolution reference dataset from the state of Sao Paulo (2024), which simultaneously provides monthly and epidemiological-week counts, enabling a direct comparison of three strategies: linear interpolation, jittering, and cubic spline. Results indicated that cubic spline interpolation achieved the highest adherence to the reference data, and this strategy was therefore adopted to generate weekly series for the 1999 to 2021 period. In addition to hospitalization time series, the dataset includes a comprehensive set of explanatory variables commonly used in epidemiological and environmental modeling, such as demographic density, CH4, CO2, and NO2 emissions, poverty and urbanization indices, maximum temperature, mean monthly precipitation, minimum relative humidity, and municipal latitude and longitude, following the same temporal disaggregation scheme to ensure multivariate compatibility. The paper documents the datasets provenance, structure, formats, licenses, limitations, and quality metrics (MAE, RMSE, R2, KL, JSD, DTW, and the KS test), and provides usage recommendations for multivariate time-series analysis, environmental health studies, and the development of machine learning and deep learning models for outbreak forecasting.

</details>


### [364] [MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning](https://arxiv.org/abs/2601.17006)
*Xuchen Li,Jing Chen,Xuzhao Li,Hao Liang,Xiaohuan Zhou,Taifeng Wang,Wentao Zhang*

Main category: cs.LG

TL;DR: MathMixup is introduced as a novel method to synthesize diverse and difficulty-controlled mathematical reasoning problems, enhancing LLM performance on benchmarks with a 52.6% average score.


<details>
  <summary>Details</summary>
Motivation: The need for high-quality, difficulty-graded training data to effectively train LLMs in mathematical reasoning tasks, overcoming limitations in existing synthesis methods.

Method: Introduced MathMixup, a data synthesis paradigm combining hybrid and decomposed strategies, along with automated self-checking and manual screening to create difficulty-controlled datasets for curriculum learning.

Result: The curriculum strategy using MathMixup significantly improves LLMs' mathematical reasoning, with Qwen2.5-7B surpassing state-of-the-art performance on multiple benchmarks.

Conclusion: MathMixup demonstrates its effectiveness in generating high-quality math datasets and enhancing curriculum learning's impact on improving LLM mathematical reasoning abilities.

Abstract: In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.

</details>


### [365] [Analysis of voice recordings features for Classification of Parkinson's Disease](https://arxiv.org/abs/2601.17007)
*Beatriz Pérez-Sánchez,Noelia Sánchez-Maroño,Miguel A. Díaz-Freire*

Main category: cs.LG

TL;DR: The paper explores machine learning techniques to improve early Parkinson's disease diagnosis using vocal recordings, focusing on feature selection to optimize performance.


<details>
  <summary>Details</summary>
Motivation: Parkinson's disease is hard to diagnose early due to mild motor symptoms, and the high cost of clinical voice recording analysis necessitates efficient solutions.

Method: The study applies various machine learning models, including neural networks, alongside feature selection techniques to identify critical features in vocal recordings for disease classification.

Result: Machine learning methods, particularly neural networks, proved effective for Parkinson's diagnosis, and the number of vocal features needed was significantly reduced without compromising performance.

Conclusion: Machine learning combined with feature selection can enable efficient and accurate early diagnosis of Parkinson's disease, reducing computational and clinical burden.

Abstract: Parkinson's disease (PD) is a chronic neurodegenerative disease. Early diagnosis is essential to mitigate the progressive deterioration of patients' quality of life. The most characteristic motor symptoms are very mild in the early stages, making diagnosis difficult. Recent studies have shown that the use of patient voice recordings can aid in early diagnosis. Although the analysis of such recordings is costly from a clinical point of view, advances in machine learning techniques are making the processing of this type of data increasingly accurate and efficient. Vocal recordings contain many features, but it is not known whether all of them are relevant for diagnosing the disease.
  This paper proposes the use of different types of machine learning models combined with feature selection methods to detect the disease. The selection techniques allow to reduce the number of features used by the classifiers by determining which ones provide the most information about the problem. The results show that machine learning methods, in particular neural networks, are suitable for PD classification and that the number of features can be significantly reduced without affecting the performance of the models.

</details>


### [366] [Bayesian Robust Financial Trading with Adversarial Synthetic Market Data](https://arxiv.org/abs/2601.17008)
*Haochong Xia,Simin Li,Ruixiao Xu,Zhixia Zhang,Hongxiang Wang,Zhiqian Liu,Teng Yao Long,Molei Qin,Chuqiao Zong,Bo An*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian Robust Framework to improve algorithmic trading models by addressing robustness issues against market uncertainties and enhancing simulation environments.


<details>
  <summary>Details</summary>
Motivation: Algorithmic trading models struggle with real-world market regime changes, leading to performance degradation. Current models lack robustness and realistic training environments.

Method: The framework combines a macro-conditioned GAN-based generator for realistic data generation and a robust policy learning strategy. The trading process is modeled as a Bayesian Markov game with adversarial perturbations to improve policy robustness.

Result: Experiments on 9 financial instruments show superior performance compared to 9 state-of-the-art methods, including better handling of extreme events like COVID.

Conclusion: The proposed Bayesian Robust Framework provides a more reliable approach for trading under uncertain market conditions, enhancing both profitability and risk management.

Abstract: Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.

</details>


### [367] [Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study](https://arxiv.org/abs/2601.17010)
*Hudson Golino*

Main category: cs.LG

TL;DR: The paper introduces a method to optimize structural analysis of embeddings in psychological item pools, emphasizing the need to target specific embedding depths rather than treating embeddings as static representations.


<details>
  <summary>Details</summary>
Motivation: Current methods in psychological item pool analysis often treat embeddings as static, assuming uniform contribution across dimensions and failing to optimize embedding usage for better structural accuracy.

Method: The study employs Dynamic Exploratory Graph Analysis (DynEGA) to efficiently explore embedding dimensions using simulations of grandiose narcissism item pools across varied embedding depths and item sizes.

Result: The analysis finds competing optimization metrics—TEFI and NMI—in deep and shallow embedding ranges, showing the need for a balanced composite criterion to enhance structural recovery and fit. Optimal embedding depth is found to scale with item pool sizes.

Conclusion: Embedding analyses should transition from static assumptions to principled optimization approaches, recognizing embedding landscapes as dynamic and requiring targeted usage of specific embedding dimensions.

Abstract: Large language model (LLM) embeddings are increasingly used to estimate dimensional structure in psychological item pools prior to data collection, yet current applications treat embeddings as static, cross-sectional representations. This approach implicitly assumes uniform contribution across all embedding coordinates and overlooks the possibility that optimal structural information may be concentrated in specific regions of the embedding space. This study reframes embeddings as searchable landscapes and adapts Dynamic Exploratory Graph Analysis (DynEGA) to systematically traverse embedding coordinates, treating the dimension index as a pseudo-temporal ordering analogous to intensive longitudinal trajectories. A large-scale Monte Carlo simulation embedded items representing five dimensions of grandiose narcissism using OpenAI's text-embedding-3-small model, generating network estimations across systematically varied item pool sizes (3-40 items per dimension) and embedding depths (3-1,298 dimensions). Results reveal that Total Entropy Fit Index (TEFI) and Normalized Mutual Information (NMI) leads to competing optimization trajectories across the embedding landscape. TEFI achieves minima at deep embedding ranges (900--1,200 dimensions) where entropy-based organization is maximal but structural accuracy degrades, whereas NMI peaks at shallow depths where dimensional recovery is strongest but entropy-based fit remains suboptimal. Single-metric optimization produces structurally incoherent solutions, whereas a weighted composite criterion identifies embedding dimensions depth regions that jointly balance accuracy and organization. Optimal embedding depth scales systematically with item pool size. These findings establish embedding landscapes as non-uniform semantic spaces requiring principled optimization rather than default full-vector usage.

</details>


### [368] [FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices](https://arxiv.org/abs/2601.17063)
*Byeongju Kim,Jungwan Lee,Donghyeon Han,Hoi-Jun Yoo,Sangyeob Kim*

Main category: cs.LG

TL;DR: FlashMoE enables efficient inference of Mixture-of-Experts (MoE) models under limited RAM by offloading inactive experts to SSD, achieving substantial improvements in cache hit rates and processing speed.


<details>
  <summary>Details</summary>
Motivation: Accelerating on-device inference for extremely large Mixture-of-Experts (MoE) models by addressing memory constraints, which cannot be met by existing DRAM-offloading solutions.

Method: FlashMoE offloads inactive MoE model experts to SSD and employs a lightweight machine learning-based caching system combining recency and frequency signals to optimize expert reuse and reduce I/O operations.

Result: FlashMoE achieves up to a 51% higher cache hit rate compared to LRU and LFU policies and demonstrates up to 2.6x faster inference speed on a user-grade desktop platform compared to prior systems.

Conclusion: FlashMoE is a practical and efficient solution for running large MoE model inference on memory-constrained edge devices, leveraging SSD-based offloading and advanced caching methods to overcome limitations of existing systems.

Abstract: Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.

</details>


### [369] [Resonant Sparse Geometry Networks](https://arxiv.org/abs/2601.18064)
*Hasi Hays*

Main category: cs.LG

TL;DR: RSGN is a brain-inspired sparse neural network reducing computational complexity and using fewer parameters while surpassing Transformer performance in hierarchical classification and long-range dependency tasks.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in computational complexity with dense attention mechanisms like Transformers, and to explore biologically-inspired neural network designs.

Method: Developed RSGN, a sparse, hierarchical connectivity architecture leveraging hyperbolic space and Hebbian-inspired slow structural learning mechanisms.

Result: RSGN showed superior accuracy in hierarchical classification (23.8%) and long-range tasks (96.5%) using significantly fewer parameters compared to Transformers.

Conclusion: Sparse, geometrically-organized neural architectures inspired by biological computation principles can be efficient and effective alternatives to dense Transformer models.

Abstract: We introduce Resonant Sparse Geometry Networks (RSGN), a brain-inspired architecture with self-organizing sparse
  hierarchical input-dependent connectivity. Unlike Transformer architectures that employ dense attention mechanisms with
  O(n^2) computational complexity, RSGN embeds computational nodes in learned hyperbolic space where connection strength
  decays with geodesic distance, achieving dynamic sparsity that adapts to each input. The architecture operates on two
  distinct timescales: fast differentiable activation propagation optimized through gradient descent, and slow
  Hebbian-inspired structural learning for connectivity adaptation through local correlation rules. We provide rigorous
  mathematical analysis demonstrating that RSGN achieves O(n*k) computational complexity, where k << n represents the average
  active neighborhood size. Experimental evaluation on hierarchical classification and long-range dependency tasks
  demonstrates that RSGN achieves 96.5% accuracy on long-range dependency tasks while using approximately 15x fewer
  parameters than standard Transformers. On challenging hierarchical classification with 20 classes, RSGN achieves 23.8%
  accuracy (compared to 5% random baseline) with only 41,672 parameters, nearly 10x fewer than the Transformer baselines
  which require 403,348 parameters to achieve 30.1% accuracy. Our ablation studies confirm the contribution of each architectural
  component, with Hebbian learning providing consistent improvements. These results suggest that brain-inspired principles
  of sparse, geometrically-organized computation offer a promising direction toward more efficient and biologically plausible
  neural architectures.

</details>


### [370] [ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting](https://arxiv.org/abs/2601.17065)
*Haoxuan Li,He Chang,Yunshan Ma,Yi Bin,Yang Yang,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.LG

TL;DR: The paper introduces ThinkTank-ME, a multi-expert collaborative framework for Middle East event forecasting, addressing the limitations of single-model approaches.


<details>
  <summary>Details</summary>
Motivation: Existing single-model architectures struggle to capture the complexities of geopolitical nuances in regional contexts like the Middle East.

Method: The authors propose ThinkTank-ME, a framework that emulates collaborative expert analysis by incorporating multi-expert collaboration, and introduce POLECAT-FOR-ME as a benchmark for specialized Middle East event forecasting.

Result: Experimental results show that ThinkTank-ME outperforms single-model approaches in handling complex temporal geopolitical forecasting tasks.

Conclusion: Multi-expert collaboration significantly improves the ability to address diverse regional and geopolitical forecasting challenges.

Abstract: Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.

</details>


### [371] [From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic](https://arxiv.org/abs/2601.18702)
*Hansheng Ren*

Main category: cs.LG

TL;DR: This paper advocates for a shift from floating-point arithmetic to arbitrary precision arithmetic in AI systems to support accurate high-order causal reasoning.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address issues of hallucinations and logical failures observed in LLMs, attributing them to fundamental numerical precision limitations inherent in floating-point arithmetic.

Method: The paper introduces the Exactness Hypothesis and Halo Architecture, which utilizes Rational Arithmetic and an Exact Inference Unit (EIU) to ensure precise computation.

Result: Through empirical tests on Huginn-0125, the proposed model demonstrated indefinite numerical stability compared to baseline models that failed in chaotic systems.

Conclusion: Exact arithmetic is identified as a critical factor for logical accuracy in AGI systems, emphasizing its role in reducing logical uncertainty.

Abstract: Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the "hallucinations" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.

</details>


### [372] [Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis](https://arxiv.org/abs/2601.17073)
*Yifei Zhang,Meimei Liu,Zhengwu Zhang*

Main category: cs.LG

TL;DR: The paper proposes a framework, CM-JIVNet, to integrate structural and functional connectivity data for better understanding brain organization.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations in current connectome data analysis methods that struggle with high dimensionality, non-linearity, and disentangling shared versus modality-specific information across structural and functional brain connectivity.

Method: The authors developed CM-JIVNet, a probabilistic framework with a multi-head attention mechanism designed to extract both shared and modality-specific latent representations from paired SC-FC datasets.

Result: When tested on the Human Connectome Project Young Adult dataset, CM-JIVNet showed improved cross-modal data reconstruction and more accurate behavioral trait predictions compared to existing methods.

Conclusion: CM-JIVNet offers a scalable and interpretable solution for analyzing multimodal brain connectivity, advancing cross-modal understanding of behavioral and brain organization.

Abstract: Brain organization is increasingly characterized through multiple imaging modalities, most notably structural connectivity (SC) and functional connectivity (FC). Integrating these inherently distinct yet complementary data sources is essential for uncovering the cross-modal patterns that drive behavioral phenotypes. However, effective integration is hindered by the high dimensionality and non-linearity of connectome data, complex non-linear SC-FC coupling, and the challenge of disentangling shared information from modality-specific variations. To address these issues, we propose the Cross-Modal Joint-Individual Variational Network (CM-JIVNet), a unified probabilistic framework designed to learn factorized latent representations from paired SC-FC datasets. Our model utilizes a multi-head attention fusion module to capture non-linear cross-modal dependencies while isolating independent, modality-specific signals. Validated on Human Connectome Project Young Adult (HCP-YA) data, CM-JIVNet demonstrates superior performance in cross-modal reconstruction and behavioral trait prediction. By effectively disentangling joint and individual feature spaces, CM-JIVNet provides a robust, interpretable, and scalable solution for large-scale multimodal brain analysis.

</details>


### [373] [Multi-Agent Deep Reinforcement Learning Under Constrained Communications](https://arxiv.org/abs/2601.17069)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: The paper introduces DG-MAPPO, a distributed MARL framework that eliminates the need for centralized critics or global state information, enabling robust collaboration using local observations and multi-hop communication.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of centralized training with decentralized execution (CTDE) in MARL, including scalability, robustness, and generalization issues, and to handle dynamic environments and team changes more efficiently.

Method: The method involves a Distributed Graph Attention Network (D-GAT) for global state inference via multi-hop communication and a distributed framework (DG-MAPPO) where agents optimize policies and value functions using local observations and shared rewards.

Result: Experimental results show that the proposed DG-MAPPO outperforms strong CTDE baselines across various benchmarks, including StarCraftII, Google Research Football, and Multi-Agent Mujoco.

Conclusion: DG-MAPPO offers a scalable, robust, and fully distributed MARL solution that does not require centralized training or global observability, marking a significant advancement for peer-to-peer communication in MARL.

Abstract: Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.

</details>


### [374] [SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model](https://arxiv.org/abs/2601.18707)
*Jan Hagnberger,Mathias Niepert*

Main category: cs.LG

TL;DR: The paper introduces SMART, a mesh-free neural surrogate model for physical simulation, leveraging point-cloud geometry input to predict physical quantities efficiently and accurately, often outperforming mesh-based methods.


<details>
  <summary>Details</summary>
Motivation: Current simulation methods for complex geometries like car bodies either involve costly mesh-based calculations or less accurate mesh-free approaches. This gap motivates the need for an efficient, accurate, and mesh-free surrogate model to simplify computation without sacrificing performance.

Method: SMART uses a point-cloud representation of geometry, encoding simulation parameters and structural features into a shared latent space. A physics decoder attends to latent representation layers to map spatial queries to physical quantities, enabling cross-layer interaction between geometry and physical field updates.

Result: Extensive experiments demonstrate that SMART is competitive with or better than mesh-based methods, proving its utility for high-level physical simulations without relying on computation-intensive meshes.

Conclusion: SMART provides a more efficient and effective alternative to traditional simulation methods, showing strong potential for industry applications and reducing dependency on costly computational setups.

Abstract: Machine learning-based surrogate models have emerged as more efficient alternatives to numerical solvers for physical simulations over complex geometries, such as car bodies. Many existing models incorporate the simulation mesh as an additional input, thereby reducing prediction errors. However, generating a simulation mesh for new geometries is computationally costly. In contrast, mesh-free methods, which do not rely on the simulation mesh, typically incur higher errors. Motivated by these considerations, we introduce SMART, a neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry, without requiring access to the simulation mesh. The geometry and simulation parameters are encoded into a shared latent space that captures both structural and parametric characteristics of the physical field. A physics decoder then attends to the encoder's intermediate latent representations to map spatial queries to physical quantities. Through this cross-layer interaction, the model jointly updates latent geometric features and the evolving physical field. Extensive experiments show that SMART is competitive with and often outperforms existing methods that rely on the simulation mesh as input, demonstrating its capabilities for industry-level simulations.

</details>


### [375] [PhysE-Inv: A Physics-Encoded Inverse Modeling approach for Arctic Snow Depth Prediction](https://arxiv.org/abs/2601.17074)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: This paper introduces PhysE-Inv, a novel framework using physics-guided inference and advanced machine learning methods to improve Arctic snow depth estimation despite data sparsity and noise.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in accurately estimating Arctic snow depth due to sparse and noisy data, which limits the usability of current models.

Method: PhysE-Inv integrates a sophisticated sequential architecture (LSTM Encoder-Decoder with Multi-head Attention) combined with physics-guided contrastive learning and inversion techniques using a hydrostatic balance forward model.

Result: PhysE-Inv achieves a 20% error reduction compared to state-of-the-art baselines and demonstrates improved resilience to data sparsity and superior physical consistency.

Conclusion: The approach offers a significant advancement in noise-tolerant, interpretable inverse modeling, with potential applications in geospatial and cryospheric research domains.

Abstract: The accurate estimation of Arctic snow depth ($h_s$) remains a critical time-varying inverse problem due to the extreme scarcity and noise inherent in associated sea ice parameters. Existing process-based and data-driven models are either highly sensitive to sparse data or lack the physical interpretability required for climate-critical applications. To address this gap, we introduce PhysE-Inv, a novel framework that integrates a sophisticated sequential architecture, an LSTM Encoder-Decoder with Multi-head Attention and physics-guided contrastive learning, with physics-guided inference.Our core innovation lies in a surjective, physics-constrained inversion methodology. This methodology first leverages the hydrostatic balance forward model as a target-formulation proxy, enabling effective learning in the absence of direct $h_s$ ground truth; second, it uses reconstruction physics regularization over a latent space to dynamically discover hidden physical parameters from noisy, incomplete time-series input. Evaluated against state-of-the-art baselines, PhysE-Inv significantly improves prediction performance, reducing error by 20\% while demonstrating superior physical consistency and resilience to data sparsity compared to empirical methods. This approach pioneers a path for noise-tolerant, interpretable inverse modeling, with wide applicability in geospatial and cryospheric domains.

</details>


### [376] [Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation](https://arxiv.org/abs/2601.17133)
*Inderjeet Singh,Eleonore Vissol-Gaudin,Andikan Otung,Motoyoshi Sekiya*

Main category: cs.LG

TL;DR: The paper introduces KNEXA-FL, a decentralized framework enhancing collaboration among LLMs while ensuring data privacy, achieving improved performance on code generation tasks.


<details>
  <summary>Details</summary>
Motivation: To resolve the privacy and inefficiency challenges in fine-tuning LLMs for specialized domains, balancing collaboration and data sovereignty within decentralized settings.

Method: KNEXA-FL uses a Central Profiler/Matchmaker (CPM) and LinUCB contextual bandit algorithm to orchestrate optimal peer-to-peer collaborations among LLMs without aggregating raw data.

Result: KNEXA-FL achieves a ~50% improvement in Pass@1 on a code generation task compared to random peer-to-peer collaborations and demonstrates superior stability over centralized baselines.

Conclusion: The study underscores learning-based, decentralized orchestration as key to building reliable and effective federated AI systems.

Abstract: Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.

</details>


### [377] [E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning](https://arxiv.org/abs/2601.17076)
*Jiajun Chen,Yue Wu,Kai Huang,Wen Xi,Yangyang Wu,Xiaoye Miao,Mengying Zhu,Meng Xi,Guanjie Cheng*

Main category: cs.LG

TL;DR: A novel task called IMvMLCIL is proposed to handle missing views and dynamic class expansions in multi-view multi-label classification. The introduced framework, E2PL, incorporates efficient prompt learning and tensor decomposition techniques to enhance scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Modern web applications require effective classification systems to integrate diverse data sources, but face challenges like missing views and emerging classes.

Method: The paper introduces E2PL, combining task-tailored and missing-aware prompts for adaptability, along with efficient prototype tensorization to reduce complexity and dynamic contrastive learning for better dependency modeling.

Result: E2PL demonstrates superior performance compared to state-of-the-art methods on three benchmarks, excelling in both effectiveness and efficiency.

Conclusion: E2PL provides a scalable and robust solution to IMvMLCIL, proving its applicability in real-world web-scale scenarios.

Abstract: Multi-view multi-label classification (MvMLC) is indispensable for modern web applications aggregating information from diverse sources. However, real-world web-scale settings are rife with missing views and continuously emerging classes, which pose significant obstacles to robust learning. Prevailing methods are ill-equipped for this reality, as they either lack adaptability to new classes or incur exponential parameter growth when handling all possible missing-view patterns, severely limiting their scalability in web environments. To systematically address this gap, we formally introduce a novel task, termed \emph{incomplete multi-view multi-label class incremental learning} (IMvMLCIL), which requires models to simultaneously address heterogeneous missing views and dynamic class expansion. To tackle this task, we propose \textsf{E2PL}, an Effective and Efficient Prompt Learning framework for IMvMLCIL. \textsf{E2PL} unifies two novel prompt designs: \emph{task-tailored prompts} for class-incremental adaptation and \emph{missing-aware prompts} for the flexible integration of arbitrary view-missing scenarios. To fundamentally address the exponential parameter explosion inherent in missing-aware prompts, we devise an \emph{efficient prototype tensorization} module, which leverages atomic tensor decomposition to elegantly reduce the prompt parameter complexity from exponential to linear w.r.t. the number of views. We further incorporate a \emph{dynamic contrastive learning} strategy explicitly model the complex dependencies among diverse missing-view patterns, thus enhancing the model's robustness. Extensive experiments on three benchmarks demonstrate that \textsf{E2PL} consistently outperforms state-of-the-art methods in both effectiveness and efficiency. The codes and datasets are available at https://anonymous.4open.science/r/code-for-E2PL.

</details>


### [378] [Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach](https://arxiv.org/abs/2601.17303)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: The paper introduces a decentralized multi-agent swarm (DMAS) architecture for IIoT networks to enhance security, featuring distributed AI agents at edge gateways.


<details>
  <summary>Details</summary>
Motivation: To address latency and vulnerability issues in centralized security monitoring systems in expansive IIoT environments.

Method: The DMAS framework utilizes autonomous AI agents communicating via a peer-to-peer protocol to detect threats without relying on centralized cloud infrastructure. A consensus-based validation process enhances threat evaluation and containment.

Result: Results indicate 0.85ms response time, 97.3% accuracy in detecting threats, 87% zero-day attack detection accuracy, and a reduction in network bandwidth usage by 89% compared to cloud-based systems.

Conclusion: The DMAS architecture significantly enhances security, scalability, and efficiency for IIoT devices, addressing limitations of centralized and edge-computing models.

Abstract: As Industrial Internet of Things (IIoT) environments expand to include tens of thousands of connected devices. The centralization of security monitoring architectures creates serious latency issues that savvy attackers can exploit to compromise an entire manufacturing ecosystem. This paper outlines a new, decentralized multi-agent swarm (DMAS) architecture that includes autonomous artificial intelligence (AI) agents at each edge gateway, functioning as a distributed digital "immune system" for IIoT networks. Instead of using a traditional static firewall approach, the DMAS agents communicate via a lightweight peer-to-peer protocol to cooperatively detect anomalous behavior across the IIoT network without sending data to a cloud infrastructure. The authors also outline a consensus-based threat validation (CVT) process in which agents vote on the threat level of an identified threat, enabling instant quarantine of a compromised node or nodes. The authors conducted experiments on a testbed that simulated an innovative factory environment with 2000 IIoT devices and found that the DMAS demonstrated sub-millisecond response times (average of 0.85ms), 97.3% accuracy in detecting malicious activity under high load, and 87% accuracy in detecting zero-day attacks. All significantly higher than baseline values for both centralized and edge computing. Additionally, the proposed architecture can prevent real-time cascading failures in industrial control systems and reduce network bandwidth use by 89% compared to cloud-based solutions.

</details>


### [379] [SFO: Learning PDE Operators via Spectral Filtering](https://arxiv.org/abs/2601.17090)
*Noam Koren,Rafael Moschopoulos,Kira Radinsky,Elad Hazan*

Main category: cs.LG

TL;DR: Spectral Filtering Operator (SFO) is introduced as an efficient neural operator for solving PDEs, addressing challenges in nonlocal interactions through a spectral basis approach.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiency of neural operators in capturing long-range, nonlocal interactions in the solution maps of complex systems governed by PDEs.

Method: SFO uses the Universal Spectral Basis (USB), derived from the Hilbert matrix eigenmodes, to parameterize integral kernels and compactly approximate them using spectral coefficients.

Result: SFO achieves state-of-the-art accuracy, reducing error by up to 40% on benchmarks such as reaction-diffusion and 3D electromagnetics with fewer parameters compared to baselines.

Conclusion: SFO provides an efficient and accurate solution for neural operators dealing with PDEs, significantly improving performance while being computationally efficient.

Abstract: Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.

</details>


### [380] [CUROCKET: Optimizing ROCKET for GPU](https://arxiv.org/abs/2601.17091)
*Ole Stüven,Keno Moenck,Thorsten Schüppstuhl*

Main category: cs.LG

TL;DR: ROCKET, a TSC feature extraction algorithm, achieved computational and accuracy efficiency via random convolutional kernels. CUROCKET further optimizes it for GPU, achieving significant computational improvements.


<details>
  <summary>Details</summary>
Motivation: To enhance the computational efficiency of the ROCKET algorithm by leveraging GPU optimization for time series classification.

Method: Proposes CUROCKET, an algorithm efficiently utilizing GPU for ROCKET, tackling the inefficiency caused by inhomogeneous kernels in standard GPU applications.

Result: CUROCKET achieves up to 11 times higher computational efficiency per watt compared to ROCKET on CPU.

Conclusion: CUROCKET greatly enhances the efficiency of ROCKET by implementing GPU optimizations, making it a more practical and powerful tool for time series classification tasks.

Abstract: ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository https://github.com/oleeven/CUROCKET on github.

</details>


### [381] [The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations](https://arxiv.org/abs/2601.17093)
*Olha Sirikova,Alvin Chan*

Main category: cs.LG

TL;DR: The paper introduces the 'Triangle of Similarity,' a framework to comprehensively compare neural network representations. Using CNNs, Vision Transformers, and Vision-Language Models, it analyzes the static, functional, and sparsity representations across datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a comprehensive framework for comparing neural network representations in order to better understand and validate their architectures, an area where existing methods fall short.

Method: The proposed method, Triangle of Similarity, combines three perspectives: static representational similarity (e.g., CKA), functional similarity (e.g., Linear Mode Connectivity), and sparsity similarity (robustness under pruning). It is tested on CNNs, Vision Transformers, and Vision-Language Models using datasets like ImageNetV2 and CIFAR-10.

Result: Key findings include: (1) distinct clusters of representational similarity based on architectural family, (2) strong correlation between CKA self-similarity and task accuracy during pruning, and (3) pruning regularizes model representations, revealing shared computational cores.

Conclusion: The proposed framework provides a holistic and effective approach for comparing neural network representations, supporting better model selection and validation in scientific research.

Abstract: Comparing neural network representations is essential for understanding and validating models in scientific applications. Existing methods, however, often provide a limited view. We propose the Triangle of Similarity, a framework that combines three complementary perspectives: static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyzing a range of CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds, our initial findings suggest that: (1) architectural family is a primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; and (3) for some model pairs, pruning appears to regularize representations, exposing a shared computational core. This framework offers a more holistic approach for assessing whether models have converged on similar internal mechanisms, providing a useful tool for model selection and analysis in scientific research.

</details>


### [382] [Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training](https://arxiv.org/abs/2601.17654)
*Ruofan Wu,Jae-Won Chung,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: This paper introduces Kareus, a training system that optimizes both dynamic and static energy consumption in AI training by using kernel scheduling and frequency scaling, achieving significant energy and time savings.


<details>
  <summary>Details</summary>
Motivation: Address the growing gap between the computing demands of AI and the availability of energy through improved optimization techniques for training AI models.

Method: Kareus employs fine-grained kernel scheduling and frequency scaling to address both dynamic and static energy consumption. It uses a decomposition approach for the joint optimization problem and applies a multi-pass multi-objective optimization algorithm to develop execution schedules.

Result: Kareus achieved up to a 28.3% reduction in training energy at the same training time or reduced training time by up to 27.5% with the same energy consumption compared to the current state-of-the-art methods.

Conclusion: Kareus demonstrates that optimizing both dynamic and static energy consumption through fine-grained scheduling can significantly improve the efficiency of AI training processes, balancing time and energy tradeoffs effectively.

Abstract: The computing demand of AI is growing at an unprecedented rate, but energy supply is not keeping pace. As a result, energy has become an expensive, contended resource that requires explicit management and optimization. Although recent works have made significant progress in large model training optimization, they focus only on a single aspect of energy consumption: dynamic or static energy.
  We find that fine-grained kernel scheduling and frequency scaling jointly and interdependently impact both dynamic and static energy consumption. Based on this finding, we design Kareus, a training system that pushes the time--energy tradeoff frontier by optimizing both aspects. Kareus decomposes the intractable joint optimization problem into local, partition-based subproblems. It then uses a multi-pass multi-objective optimization algorithm to find execution schedules that push the time--energy tradeoff frontier. Compared to the state of the art, Kareus reduces training energy by up to 28.3% at the same training time, or reduces training time by up to 27.5% at the same energy consumption.

</details>


### [383] [Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation](https://arxiv.org/abs/2601.17094)
*Junichiro Niimi*

Main category: cs.LG

TL;DR: The paper introduces a principle separating world models from language models for better consistency and control in text generation, focusing on consumer review contexts.


<details>
  <summary>Details</summary>
Motivation: The research seeks to understand whether language models truly comprehend the world or only generate plausible text and proposes to separate linguistic skills from world knowledge.

Method: The approach uses a Deep Boltzmann Machine as the world model, an adapter for latent beliefs, and a frozen GPT-2 for linguistic abilities, tested on Amazon smartphone reviews.

Result: The system shows improved sentiment correlation, reduced perplexity, and semantic similarity in text generation, achieves valid configurations in domain-specific tasks, and handles causal interventions in text.

Conclusion: Integrating separate world models with language models enhances controllable and coherent text generation, supporting the proposed architectural principle.

Abstract: Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.

</details>


### [384] [MambaNet: Mamba-assisted Channel Estimation Neural Network With Attention Mechanism](https://arxiv.org/abs/2601.17108)
*Dianxin Luan,Chengsi Liang,Jie Huang,Zheng Lin,Kaitao Meng,John Thompson,Cheng-Xiang Wang*

Main category: cs.LG

TL;DR: The paper introduces a Mamba-assisted neural network framework with a self-attention mechanism for efficient channel estimation in OFDM systems with large subcarriers.


<details>
  <summary>Details</summary>
Motivation: To develop a low-complexity, efficient, and scalable solution for channel estimation in OFDM systems, especially for large-scale subcarrier setups.

Method: The framework integrates customized Mamba architecture and a bidirectional selective scan to capture long-distance dependencies and handle non-causal channel gains effectively, reducing space complexity compared to transformer-based networks.

Result: Simulation results on the 3GPP TS 36.101 channel show improved channel estimation performance with reduced tunable parameters compared to other neural network solutions.

Conclusion: The proposed framework improves channel estimation efficiently for large-scale systems with less complexity and better accuracy than existing methods.

Abstract: This paper proposes a Mamba-assisted neural network framework incorporating self-attention mechanism to achieve improved channel estimation with low complexity for orthogonal frequency-division multiplexing (OFDM) waveforms, particularly for configurations with a large number of subcarriers. With the integration of customized Mamba architecture, the proposed framework handles large-scale subcarrier channel estimation efficiently while capturing long-distance dependencies among these subcarriers effectively. Unlike conventional Mamba structure, this paper implements a bidirectional selective scan to improve channel estimation performance, because channel gains at different subcarriers are non-causal. Moreover, the proposed framework exhibits relatively lower space complexity than transformer-based neural networks. Simulation results tested on the 3GPP TS 36.101 channel demonstrate that compared to other baseline neural network solutions, the proposed method achieves improved channel estimation performance with a reduced number of tunable parameters.

</details>


### [385] [LLM-42: Enabling Determinism in LLM Inference with Verified Speculation](https://arxiv.org/abs/2601.17768)
*Raja Gond,Aditya K Kamath,Arkaprava Basu,Ramachandran Ramjee,Ashish Panwar*

Main category: cs.LG

TL;DR: The paper proposes LLM-42, a method to enable deterministic inference in large language models without sacrificing throughput.


<details>
  <summary>Details</summary>
Motivation: LLM inference often produces non-deterministic results due to GPU kernel behaviors and floating-point operations, creating challenges for applications requiring consistent outputs.

Method: LLM-42 uses a scheduling mechanism with speculative decoding, combining a non-deterministic fast path and a verify-rollback loop to enforce determinism by replaying and confirming token validity.

Result: LLM-42 achieves deterministic LLM inference while preserving throughput, mostly relying on existing kernel implementations and introducing overhead only where determinism is needed.

Conclusion: The proposed approach offers a practical solution to determinism in LLMs, achieving efficiency without requiring comprehensive kernel redesign or sacrificing overall inference performance.

Abstract: In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.
  Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.

</details>


### [386] [Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts](https://arxiv.org/abs/2601.17111)
*Xuan-Phi Nguyen,Shrey Pandit,Austin Xu,Caiming Xiong,Shafiq Joty*

Main category: cs.LG

TL;DR: The paper proposes Least-Loaded Expert Parallelism (LLEP), an algorithm to efficiently handle imbalances in Mixture-of-Experts (MoE) models and improve speed and memory usage.


<details>
  <summary>Details</summary>
Motivation: To address the issue of extreme imbalances in MoE models' routing, which causes failure during post-training/inference in systems relying on expert parallelism (EP).

Method: Introduced the LLEP algorithm, which dynamically reroutes workload from overloaded devices to underutilized ones, ensuring balanced memory-constrained operations that minimize latency.

Result: LLEP achieved up to 5x speedups and 4x peak memory reduction compared to standard EP, with ~1.9x faster throughput for models like gpt-oss-120b.

Conclusion: LLEP effectively resolves routing imbalances in MoE models, enhancing scalability and performance during post-training and inference while providing a framework for optimizing hardware usage.

Abstract: Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.

</details>


### [387] [Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning](https://arxiv.org/abs/2601.18626)
*Yingxiao Huo,Satya Prakash Dash,Radu Stoican,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: The paper introduces an efficient natural policy optimization using a rank-1 approximation to the Fisher Information Matrix for improved convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Natural gradients are computationally expensive because they involve inverting the Fisher Information Matrix, making it impractical for deep reinforcement learning.

Method: A novel natural policy optimization technique is developed, leveraging a rank-1 approximation for the inverse of the Fisher Information Matrix.

Result: The proposed method outperforms standard actor-critic and trust-region baselines in multiple environments.

Conclusion: Rank-1 approximations can offer computational efficiency and competitive performance, making them a practical alternative for reinforcement learning.

Abstract: Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.

</details>


### [388] [Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization](https://arxiv.org/abs/2601.17112)
*A. El Ichi,K. Jbilou*

Main category: cs.LG

TL;DR: A new tensor compression framework is introduced using the cproduct for low rank approximation in LLMs, focusing on efficient memory and computation usage.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges with high memory and computational demands, which limit their efficiency and scalability.

Method: The paper introduces a tensor compression framework leveraging the algebraic structure of the cproduct for computing low rank tensor approximations, considering embedding layers, attention projections, and feed-forward networks.

Result: The approach shows computationally efficient compression by exploiting multidimensional correlations beyond traditional methods like SVD.

Conclusion: This framework enhances computational efficiency for LLMs, addressing their memory footprint and computational limitation issues.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.

</details>


### [389] [How does Graph Structure Modulate Membership-Inference Risk for Graph Neural Networks?](https://arxiv.org/abs/2601.17130)
*Megha Khosla*

Main category: cs.LG

TL;DR: This research investigates privacy leakage in graph neural networks (GNNs) and proposes a detailed analysis of graph-specific membership inference (MI).


<details>
  <summary>Details</summary>
Motivation: While GNNs are beneficial for encoding complex relationships and improving tasks like node classification, their usage in sensitive fields raises privacy concerns. Current studies on MI are based on non-graph domains necessitating graph-specific investigation.

Method: The research formalizes MI for node-neighbourhood tuples, analyzing two factors: training graph construction and edge access during inference. The study adapts statistical exchangeability for DP-GNNs to evaluate auditability.

Result: Findings reveal that common methods like snowball sampling can reduce generalization, edge access influences MI more than performance gaps, and traditional statistical exchangeability breaks in graph models.

Conclusion: The paper uncovers limitations in traditional generalization gap measures for graph models, highlights the nuances of privacy risks in GNNs, and suggests key adjustments for developing safer, private systems.

Abstract: Graph neural networks (GNNs) have become the standard tool for encoding data and their complex relationships into continuous representations, improving prediction accuracy in several machine learning tasks like node classification and link prediction. However, their use in sensitive applications has raised concerns about the potential leakage of training data. Research on privacy leakage in GNNs has largely been shaped by findings from non-graph domains, such as images and tabular data. We emphasize the need of graph specific analysis and investigate the impact of graph structure on node level membership inference. We formalize MI over node-neighbourhood tuples and investigate two important dimensions: (i) training graph construction and (ii) inference-time edge access. Empirically, snowball's coverage bias often harms generalisation relative to random sampling, while enabling inter-train-test edges at inference improves test accuracy, shrinks the train-test gap, and yields the lowest membership advantage across most of the models and datasets. We further show that the generalisation gap empirically measured as the performance difference between the train and test nodes is an incomplete proxy for MI risk: access to edges dominates-MI can rise or fall independent of gap changes. Finally, we examine the auditability of differentially private GNNs, adapting the definition of statistical exchangeability of train-test data points for graph based models. We show that for node level tasks the inductive splits (random or snowball sampled) break exchangeability, limiting the applicability of standard bounds for membership advantage of differential private models.

</details>


### [390] [ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning](https://arxiv.org/abs/2601.17135)
*Jakob Karalus,Friedhelm Schwenker*

Main category: cs.LG

TL;DR: ConceptACT improves imitation learning efficiency by integrating semantic concept annotations during training, using a Transformer-based architecture.


<details>
  <summary>Details</summary>
Motivation: Current imitation learning methods overlook semantic knowledge humans have about tasks, relying solely on low-level sensorimotor data, which limits learning efficiency.

Method: ConceptACT incorporates semantic concept annotations (e.g., object properties, spatial relationships) during training. It uses a Transformer architecture with concept-aware cross-attention aligned with human annotations.

Result: Experiments on robotic tasks showed ConceptACT achieved faster convergence and greater sample efficiency. Architectural integration of semantic supervision significantly outperformed naive methods.

Conclusion: Properly integrated semantic annotations using attention mechanisms can serve as strong inductive biases, improving robot learning efficiency.

Abstract: Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.

</details>


### [391] [Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging](https://arxiv.org/abs/2601.17180)
*Inés Gonzalez-Pepe,Vinuyan Sivakolunthu,Jacob Fortin,Yohan Chatelain,Tristan Glatard*

Main category: cs.LG

TL;DR: This paper proposes methods to improve the efficiency of convolutional neural networks (CNNs) by identifying and skipping computations dominated by numerical noise.


<details>
  <summary>Details</summary>
Motivation: Increasing demands for efficiency in deep learning models, particularly in neuroimaging, due to the large computational requirements of modern architectures.

Method: The paper introduces Conservative & Aggressive NaNs, new pooling methods that replace numerically unstable values with NaNs, enabling subsequent layers to skip irrelevant computations without architectural changes.

Result: Evaluations show runtime improvements, average inference speedup of 1.67x for inputs with over 66% NaNs, reduction in convolution operations by up to 64.64%, and consistent efficiency gains across various models without measurable performance degradation.

Conclusion: Numerical uncertainty can effectively be leveraged to eliminate redundant computations, enhancing CNNs' runtime efficiency, especially in neuroimaging settings.

Abstract: Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.

</details>


### [392] [Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data](https://arxiv.org/abs/2601.17183)
*Farzam Asad,Junaid Saif Khan,Maria Tariq,Sundus Munir,Muhammad Adnan Khan*

Main category: cs.LG

TL;DR: The paper explores Federated Learning in healthcare using FedProx for heart disease prediction, achieving better accuracy than other approaches while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Privacy regulations prevent direct sharing of patient data across hospitals, necessitating collaborative yet privacy-preserving approaches like Federated Learning.

Method: Using the UCI Heart Disease dataset, researchers simulated non-IID scenarios across heterogeneous hospital clients and applied FedProx with a proximal parameter to optimize training.

Result: FedProx achieved 85% accuracy, outperforming centralized models (83.33%) and isolated models (78.45%) while maintaining data privacy.

Conclusion: FedProx effectively manages client drift in heterogeneous healthcare environments, offering practical insights for deploying federated learning in hospitals while upholding privacy standards.

Abstract: Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.

</details>


### [393] [Rethinking Benchmarks for Differentially Private Image Classification](https://arxiv.org/abs/2601.17189)
*Sabrina Mokhtari,Sara Kodeiri,Shubhankar Mohapatra,Florian Tramer,Gautam Kamath*

Main category: cs.LG

TL;DR: This paper revisits benchmarks for differentially private machine learning, proposing comprehensive benchmarks and a public leaderboard to assess and track progress.


<details>
  <summary>Details</summary>
Motivation: To improve the evaluation of differentially private image classification techniques by providing a wider set of benchmarks and encouraging community collaboration.

Method: The authors propose a comprehensive set of benchmarks for various settings and datasets, test established techniques, and create a public leaderboard for tracking progress.

Result: Benchmarks and a public leaderboard have been established, facilitating evaluation across diverse scenarios for differentially private machine learning.

Conclusion: A diverse set of benchmarks and a community-driven leaderboard provide valuable tools for advancing differentially private machine learning.

Abstract: We revisit benchmarks for differentially private image classification. We suggest a comprehensive set of benchmarks, allowing researchers to evaluate techniques for differentially private machine learning in a variety of settings, including with and without additional data, in convex settings, and on a variety of qualitatively different datasets. We further test established techniques on these benchmarks in order to see which ideas remain effective in different settings. Finally, we create a publicly available leader board for the community to track progress in differentially private machine learning.

</details>


### [394] [PUNCH: Physics-informed Uncertainty-aware Network for Coronary Hemodynamics](https://arxiv.org/abs/2601.17192)
*Sukirt Thakur,Marcus Roper,Yang Zhou,Reza Akbarian Bafghi,Brahmajee K. Nallamothu,C. Alberto Figueroa,Srinivas Paruchuri,Scott Burger,Maziar Raissi*

Main category: cs.LG

TL;DR: The paper presents a non-invasive method to estimate coronary flow reserve (CFR) from standard angiography using uncertainty-aware physics-informed neural networks.


<details>
  <summary>Details</summary>
Motivation: Coronary microvascular dysfunction (CMD) is widely prevalent but underdiagnosed due to invasive and inconsistent gold-standard diagnostic methods.

Method: The authors developed a framework combining physics-informed neural networks with variational inference to estimate coronary blood flow directly, without the need for ground-truth flow measurements.

Result: The method achieved high predictive accuracy and uncertainty resolution in both synthetic and clinical datasets. It demonstrated strong agreement with invasive diagnostic methods while being faster and non-invasive.

Conclusion: The approach leverages routine angiography to provide an accessible, scalable, and safer means of diagnosing CMD, offering a paradigm shift in patient-specific imaging analysis.

Abstract: Coronary microvascular dysfunction (CMD) affects millions worldwide yet remains underdiagnosed because gold-standard physiological measurements are invasive and variably reproducible. We introduce a non-invasive, uncertainty-aware framework for estimating coronary flow reserve (CFR) directly from standard angiography. The system integrates physics-informed neural networks with variational inference to infer coronary blood flow from first-principles models of contrast transport, without requiring ground-truth flow measurements. The pipeline runs in approximately three minutes per patient on a single GPU, with no population-level training.
  Using 1{,}000 synthetic spatiotemporal intensity maps (kymographs) with controlled noise and artifacts, the framework reliably identifies degraded data and outputs appropriately inflated uncertainty estimates, showing strong correspondence between predictive uncertainty and error (Pearson $r = 0.997$, Spearman $ρ= 0.998$). Clinical validation in 12 patients shows strong agreement between PUNCH-derived CFR and invasive bolus thermodilution (Pearson $r = 0.90$, $p = 6.3 \times 10^{-5}$). We focus on the LAD, the artery most commonly assessed in routine CMD testing. Probabilistic CFR estimates have confidence intervals narrower than the variability of repeated invasive measurements.
  By transforming routine angiography into quantitative, uncertainty-aware assessment, this approach enables scalable, safer, and more reproducible evaluation of coronary microvascular function. Because standard angiography is widely available globally, the framework could expand access to CMD diagnosis and establish a new paradigm for physics-informed, patient-specific inference from clinical imaging.

</details>


### [395] [Accelerated Sinkhorn Algorithms for Partial Optimal Transport](https://arxiv.org/abs/2601.17196)
*Nghia Thu Truong,Qui Phu Pham,Quang Nguyen,Dung Luong,Mai Tran*

Main category: cs.LG

TL;DR: POT tackles fraction of mass transport issues between distributions, making it suitable for scaling challenges. ASPOT, a new method, improves speed and effectiveness with a complexity of $\mathcal{O}(n^{7/3}\varepsilon^{-5/3})$.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of classical Sinkhorn methods for Partial Optimal Transport and improve scalability.

Method: Introduced ASPOT, combining alternating minimization with Nesterov-style acceleration, and explored impact of entropic parameter adjustment.

Result: ASPOT achieves faster rates and enhances the classical Sinkhorn method's performance. Real applications confirmed its efficiency.

Conclusion: ASPOT improves scalability and effectiveness for POT in unequal mass or outlier cases, validated by experiments.

Abstract: Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\mathcal{O}(n^{7/3}\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $γ$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.

</details>


### [396] [SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment](https://arxiv.org/abs/2601.17204)
*Yinkai Wang,Yan Zhou Chen,Xiaohui Chen,Li-Ping Liu,Soha Hassoun*

Main category: cs.LG

TL;DR: The paper introduces SpecBridge, a framework for improving small-molecule identification using implicit alignment of spectral data with molecular representations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in untargeted small-molecule identification where existing spectral libraries are incomplete and traditional methods face limitations.

Method: The method involves fine-tuning a self-supervised spectral encoder (DreaMS) to align with the frozen latent space of a pre-trained molecular model (ChemBERTa) and performing retrieval by cosine similarity using molecular embeddings.

Result: SpecBridge demonstrates a 20-25% increase in top-1 retrieval accuracy compared to existing neural methods across multiple datasets, with limited trainable parameters.

Conclusion: Aligning spectral data to existing foundation molecular models is a stable and effective alternative to building new architectures, enhancing performance with minimal parameter updates.

Abstract: Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at https://github.com/HassounLab/SpecBridge.

</details>


### [397] [NewPINNs: Physics-Informing Neural Networks Using Conventional Solvers for Partial Differential Equations](https://arxiv.org/abs/2601.17207)
*Maedeh Makki,Satish Chandran,Maziar Raissi,Adrien Grenier,Behzad Mohebbi*

Main category: cs.LG

TL;DR: NewPINNs, a novel framework, couples neural networks with numerical solvers to solve differential equations, focusing on solver-consistency rather than residual-based losses.


<details>
  <summary>Details</summary>
Motivation: To address common issues in physics-informed neural networks such as optimization pathologies, loss-weight sensitivity, and poor performance in challenging regimes.

Method: The framework integrates neural networks and numerical solvers. The network generates candidate solutions, which the solver evolves, minimizing discrepancies between predictions and solver-evolved states.

Result: Demonstrated effectiveness on various forward and inverse problems using finite volume, finite element, and spectral solvers.

Conclusion: NewPINNs improve performance and stability by delegating physics enforcement to numerical solvers, avoiding conventional loss engineering challenges.

Abstract: We introduce NewPINNs, a physics-informing learning framework that couples neural networks with conventional numerical solvers for solving differential equations. Rather than enforcing governing equations and boundary conditions through residual-based loss terms, NewPINNs integrates the solver directly into the training loop and defines learning objectives through solver-consistency. The neural network produces candidate solution states that are advanced by the numerical solver, and training minimizes the discrepancy between the network prediction and the solver-evolved state. This pull-push interaction enables the network to learn physically admissible solutions through repeated exposure to the solver's action, without requiring problem-specific loss engineering or explicit evaluation of differential equation residuals. By delegating the enforcement of physics, boundary conditions, and numerical stability to established numerical solvers, NewPINNs mitigates several well-known failure modes of standard physics-informed neural networks, including optimization pathologies, sensitivity to loss weighting, and poor performance in stiff or nonlinear regimes. We demonstrate the effectiveness of the proposed approach across multiple forward and inverse problems involving finite volume, finite element, and spectral solvers.

</details>


### [398] [JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers](https://arxiv.org/abs/2601.17215)
*Ruoqing Zheng,Chang Sun,Qibin Liu,Lauri Laatu,Arianna Cox,Benedikt Maier,Alexander Tapper,Jose G. F. Coutinho,Wayne Luk,Zhiqiang Que*

Main category: cs.LG

TL;DR: Introducing JetFormer, an encoder-only Transformer architecture for particle jet tagging, optimized for both high-accuracy offline systems and ultra-low-latency online trigger purposes.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable model capable of handling diverse jet tagging scenarios at the Large Hadron Collider, bridging both accuracy and deployability.

Method: Proposed an encoder-only Transformer model (JetFormer) to operate on variable-length particle features. Optimized computational efficiency using multi-objective hyperparameter search, pruning, and quantization.

Result: JetFormer achieved competitive accuracy against state-of-the-art models like ParT with significantly fewer computational demands. It outperformed models like MLPs and Interaction Networks on benchmark datasets. Compact versions suited for hardware deployment achieved sub-microsecond latency.

Conclusion: JetFormer unifies high performance and deployability, providing a practical solution for offline and online jet tagging at the LHC.

Abstract: We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at https://github.com/walkieq/JetFormer.

</details>


### [399] [Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning](https://arxiv.org/abs/2601.17224)
*Dmitrii Torbunov,Yihui Ren,Lijun Wu,Yimei Zhu*

Main category: cs.LG

TL;DR: This paper presents an extension of the Conditional Diffusion Model-based Inverse Problem Solver (CDI) to two-dimensional spatial data for enhanced uncertainty quantification in scientific inverse problems.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome challenges in uncertainty quantification for scientific inverse problems, particularly for spatial observations like 2D diffraction patterns, where standard regression methods fail to capture true uncertainty.

Method: The authors extend CDI to 2D spatial conditioning and validate its performance on CBED parameter inference using simulated diffraction data.

Result: The extended CDI model produces well-calibrated posterior distributions, distinguishing between identifiable and ambiguous parameters, outperforming standard regression methods in capturing uncertainty.

Conclusion: CDI's extension to spatial domains successfully enables more robust and accurate uncertainty quantification, crucial for scientific inverse problems.

Abstract: Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.

</details>


### [400] [A Constrained Optimization Perspective of Unrolled Transformers](https://arxiv.org/abs/2601.17257)
*Javier Porras-Valenzuela,Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: The paper presents a new way to train transformers by enforcing descent constraints, leading to better robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness and generalization of transformers, particularly under perturbations and out-of-distribution settings.

Method: A constrained optimization framework is introduced, enforcing layerwise descent constraints and employing a primal-dual training scheme.

Result: The approach results in improved robustness to perturbations, better out-of-distribution generalization, and preserved in-distribution performance.

Conclusion: By making transformers behave like optimization descent algorithms, they achieve superior performance in robustness and generalization across applications like video denoising and text classification.

Abstract: We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.

</details>


### [401] [The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment](https://arxiv.org/abs/2601.17260)
*Marco Pollanen*

Main category: cs.LG

TL;DR: This paper explores the impact of varying alignment pressure parameter $β$ in Direct Preference Optimization (DPO) on model behavior and performance, revealing non-monotonic effects and differences across architectures.


<details>
  <summary>Details</summary>
Motivation: Identify the optimal conditions in DPO training by analyzing the effect of the parameter $β$ on model performance and alignment, addressing instances where higher alignment pressure may not result in better behavior.

Method: The authors densely sweep over $β$ for three different 7B open-weight model families (Mistral, Llama, and Qwen) using a fixed DPO recipe, observing behavioral and performance shifts.

Result: They find sharp non-monotonic capability changes in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Notably, higher DPO preference margins can negatively correlate with reasoning capabilities, and training with high $β$ creates long-lasting performance deficits.

Conclusion: Capability evaluations across $β$ should account for its varying effects, rather than relying solely on preference margins or aggregate benchmarks, highlighting the intricacies of alignment tuning.

Abstract: Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $β$) yields progressively "better" behavior. We instead treat $β$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $β\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $β$ induces capability losses that persist even after $β$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $β$ landscape rather than reliance on margins or aggregate benchmarks.

</details>


### [402] [AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning](https://arxiv.org/abs/2601.17261)
*Wei Lin,Yining Jiang,Qingyu Song,Qiao Xiang,Hong Xu*

Main category: cs.LG

TL;DR: The paper introduces Activation-Guided Zeroth-Order optimization (AGZO), a novel ZO optimization method for fine-tuning large language models, leveraging activation-informed subspaces for improved efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing Zeroth-Order optimization methods which ignore valuable structural information during the forward pass, especially under memory-constrained scenarios for fine-tuning large language models.

Method: AGZO identifies the relationship between activation structures and gradients, builds activation-informed subspaces during the forward pass, and applies perturbations restricted to these low-rank subspaces for optimization.

Result: AGZO significantly outperforms state-of-the-art Zeroth-Order optimization baselines on various benchmarks, while achieving a performance closer to first-order fine-tuning methods and maintaining a similar memory footprint.

Conclusion: The approach effectively bridges the gap between Zeroth-Order and first-order fine-tuning methods by integrating activation-guided subspace, offering a more efficient and memory-conscious solution for optimizing large language models.

Abstract: Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.

</details>


### [403] [Unrolled Neural Networks for Constrained Optimization](https://arxiv.org/abs/2601.17274)
*Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: This paper introduces Constrained Dual Unrolling (CDU), a framework using coupled neural networks to solve constrained optimization problems with learnable counterparts to dual ascent algorithms.


<details>
  <summary>Details</summary>
Motivation: To develop efficient, learnable algorithms that solve constrained optimization problems, overcoming limitations of traditional dual ascent methods.

Method: The authors use two coupled neural networks (a primal network and a dual network) that emulate Lagrangian optimization dynamics under constraints. Training is designed as a nested optimization problem with alternating updates for the two networks.

Result: The CDU framework provides near-optimal and feasible solutions for problems like mixed-integer quadratic programs (MIQPs) and power allocation, showing strong generalization even with out-of-distribution data.

Conclusion: The proposed CDU framework is an effective method for solving constrained optimization problems, combining neural networks with constrained learning to achieve both performance and generalization.

Abstract: In this paper, we develop unrolled neural networks to solve constrained optimization problems, offering accelerated, learnable counterparts to dual ascent (DA) algorithms. Our framework, termed constrained dual unrolling (CDU), comprises two coupled neural networks that jointly approximate the saddle point of the Lagrangian. The primal network emulates an iterative optimizer that finds a stationary point of the Lagrangian for a given dual multiplier, sampled from an unknown distribution. The dual network generates trajectories towards the optimal multipliers across its layers while querying the primal network at each layer. Departing from standard unrolling, we induce DA dynamics by imposing primal-descent and dual-ascent constraints through constrained learning. We formulate training the two networks as a nested optimization problem and propose an alternating procedure that updates the primal and dual networks in turn, mitigating uncertainty in the multiplier distribution required for primal network training. We numerically evaluate the framework on mixed-integer quadratic programs (MIQPs) and power allocation in wireless networks. In both cases, our approach yields near-optimal near-feasible solutions and exhibits strong out-of-distribution (OOD) generalization.

</details>


### [404] [Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization](https://arxiv.org/abs/2601.17570)
*Hadi Salloum,Ali Jnadi,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.LG

TL;DR: The paper introduces the MC+QUBO method, combining Monte Carlo reinforcement learning with quantum-inspired optimization to enhance performance in sparse, complex environments.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo methods in reinforcement learning suffer from inefficiencies in dealing with sparse rewards, large state spaces, and correlated trajectories.

Method: The method involves reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and using quantum-inspired solvers for trajectory selection to improve policy evaluation.

Result: In experiments with a finite-horizon GridWorld, MC+QUBO showed improved convergence speed and policy quality compared to standard Monte Carlo approaches.

Conclusion: Quantum-inspired optimization techniques, like QUBO, can effectively enhance reinforcement learning methods in challenging environments.

Abstract: Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solving it with quantum-inspired samplers. Our method, MC+QUBO, integrates a combinatorial filtering step into standard MC policy evaluation: from each batch of trajectories, we select a subset that maximizes cumulative reward while promoting state-space coverage. This selection is encoded as a QUBO, where linear terms favor high-reward episodes and quadratic terms penalize redundancy. We explore both Simulated Quantum Annealing (SQA) and Simulated Bifurcation (SB) as black-box solvers within this framework. Experiments in a finite-horizon GridWorld demonstrate that MC+QUBO outperforms vanilla MC in convergence speed and final policy quality, highlighting the potential of quantum-inspired optimization as a decision-making subroutine in reinforcement learning.

</details>


### [405] [Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning](https://arxiv.org/abs/2601.17275)
*Lianlei Shan,Han Chen,Yixuan Wang,Zhenjie Liu,Wei Li*

Main category: cs.LG

TL;DR: The paper presents DeepLatent Reasoning (DLR), a novel reinforcement learning framework in continuous latent space, to enhance logical reasoning in Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Current LLMs underperform in complex reasoning tasks as their approach often relies on statistical patterns rather than logical deduction. Direct RL in discrete token spaces faces challenges like inefficiency, variability, and memory loss.

Method: The proposed DLR framework uses a lightweight assistant model to sample latent reasoning encodings, dual evaluations for filtering, contrastive learning for reasoning diversity, and a frozen main model for decoding, bypassing token-level generation and costly parameter updates.

Result: DLR achieves stable training convergence, supports longer reasoning processes, and enhances reasoning capabilities of LLMs while operating under the same computational budget.

Conclusion: DLR offers a scalable and efficient approach for integrating reinforcement learning with LLMs, improving their logical reasoning abilities while avoiding common pitfalls like catastrophic forgetting and inefficiencies.

Abstract: While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.

</details>


### [406] [Tabular Foundation Models are Strong Graph Anomaly Detectors](https://arxiv.org/abs/2601.17301)
*Yunhui Liu,Tieke He,Yongchao Liu,Can Yi,Hong Jin,Chuntao Hong*

Main category: cs.LG

TL;DR: This paper introduces TFM4GAD, a framework using tabular foundation models (TFMs) for generalized graph anomaly detection (GAD), addressing heterogeneity and generalization issues by leveraging synthetic pre-training and contextual learning.


<details>
  <summary>Details</summary>
Motivation: Existing GAD methods are dataset-specific, leading to high computational costs, poor scalability, and inadequate transferability across diverse datasets, necessitating a universal solution for GAD.

Method: The method adapts TFMs for GAD by transforming graph structures into augmented feature tables enriched with structural embeddings and features, enabling TFMs to process them effectively.

Result: TFM4GAD outperformed specialized, dataset-specific GAD models in extensive experiments, proving its effectiveness and generalization across diverse datasets and domains.

Conclusion: TFM4GAD provides a novel and practical methodology for re-purposing existing tabular foundation models as generalist tools for graph anomaly detection, addressing critical challenges of heterogeneity, scalability, and limited data.

Abstract: Graph anomaly detection (GAD), which aims to identify abnormal nodes that deviate from the majority, has become increasingly important in high-stakes Web domains. However, existing GAD methods follow a "one model per dataset" paradigm, leading to high computational costs, substantial data demands, and poor generalization when transferred to new datasets. This calls for a foundation model that enables a "one-for-all" GAD solution capable of detecting anomalies across diverse graphs without retraining. Yet, achieving this is challenging due to the large structural and feature heterogeneity across domains. In this paper, we propose TFM4GAD, a simple yet effective framework that adapts tabular foundation models (TFMs) for graph anomaly detection. Our key insight is that the core challenges of foundation GAD, handling heterogeneous features, generalizing across domains, and operating with scarce labels, are the exact problems that modern TFMs are designed to solve via synthetic pre-training and powerful in-context learning. The primary challenge thus becomes structural: TFMs are agnostic to graph topology. TFM4GAD bridges this gap by "flattening" the graph, constructing an augmented feature table that enriches raw node features with Laplacian embeddings, local and global structural characteristics, and anomaly-sensitive neighborhood aggregations. This augmented table is processed by a TFM in a fully in-context regime. Extensive experiments on multiple datasets with various TFM backbones reveal that TFM4GAD surprisingly achieves significant performance gains over specialized GAD models trained from scratch. Our work offers a new perspective and a practical paradigm for leveraging TFMs as powerful, generalist graph anomaly detectors.

</details>


### [407] [Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions](https://arxiv.org/abs/2601.18107)
*Pedram Agand,Mo Chen*

Main category: cs.LG

TL;DR: The paper introduces MoReBRAC, a model-based framework for Offline Reinforcement Learning (ORL) that improves policy learning by synthesizing high-confidence transitions using uncertainty-aware methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of distributional shifts in ORL, which restricts policy improvement due to reliance on overly conservative approaches.

Method: MoReBRAC employs a dual-recurrent world model to generate synthetic transitions using hierarchical uncertainty evaluation through VAE manifold detection, model sensitivity analysis, and Monte Carlo dropout.

Result: MoReBRAC demonstrates superior performance on D4RL Gym-MuJoCo benchmarks, particularly in scenarios involving "random" and "suboptimal" data regimes.

Conclusion: The framework advances ORL by utilizing synthesized high-confidence data, improving the learning process while managing distributional trade-offs.

Abstract: Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.

</details>


### [408] [Weighted Graph Clustering via Scale Contraction and Graph Structure Learning](https://arxiv.org/abs/2601.17307)
*Haobing Liu,Yinuo Zhang,Tingting Wang,Ruobing Jiang,Yanwei Yu*

Main category: cs.LG

TL;DR: This paper proposes a novel graph clustering network to address challenges in utilizing edge weights effectively by introducing graph contraction and noise mitigation mechanisms.


<details>
  <summary>Details</summary>
Motivation: To overcome two major challenges in leveraging edge weights for graph clustering, namely increased storage/training time and the negative impact of noisy edges.

Method: Developed a contractile edge-weight-aware graph clustering network with a cluster-oriented graph contraction module to reduce graph scale and an edge-weight-aware attention network to detect and weaken noisy connections.

Result: Extensive experiments on real-world datasets demonstrate the model's superior performance compared to baselines and its ability to significantly reduce training time and storage needs.

Conclusion: The proposed model effectively improves clustering performance by addressing the challenges associated with edge weights through graph contraction and noise mitigation mechanisms.

Abstract: Graph clustering aims to partition nodes into distinct clusters based on their similarity, thereby revealing relationships among nodes. Nevertheless, most existing methods do not fully utilize these edge weights. Leveraging edge weights in graph clustering tasks faces two critical challenges. (1) The introduction of edge weights may significantly increase storage space and training time, making it essential to reduce the graph scale while preserving nodes that are beneficial for the clustering task. (2) Edge weight information may inherently contain noise that negatively impacts clustering results. However, few studies can jointly optimize clustering and edge weights, which is crucial for mitigating the negative impact of noisy edges on clustering task. To address these challenges, we propose a contractile edge-weight-aware graph clustering network. Specifically, a cluster-oriented graph contraction module is designed to reduce the graph scale while preserving important nodes. An edge-weight-aware attention network is designed to identify and weaken noisy connections. In this way, we can more easily identify and mitigate the impact of noisy edges during the clustering process, thus enhancing clustering effectiveness. We conducted extensive experiments on three real-world weighted graph datasets. In particular, our model outperforms the best baseline, demonstrating its superior performance. Furthermore, experiments also show that the proposed graph contraction module can significantly reduce training time and storage space.

</details>


### [409] [PAR: Plausibility-aware Amortized Recourse Generation](https://arxiv.org/abs/2601.17309)
*Anagha Sabu,Vidhya S,Narayanan C Krishnan*

Main category: cs.LG

TL;DR: The paper introduces PAR, an approach for generating probable and effective algorithmic recourse by reformulating it as a Maximum A-Posteriori (MAP) problem. This method emphasizes plausibility, efficiency, and personalization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of generating realistic and actionable recommendations for individuals affected by unfavorable decisions by predictive models, focusing on generating plausible counterfactuals.

Method: PAR formulates recourse as a MAP inference problem to maximize counterfactual likelihood under constraints, using tractable probabilistic models for efficient evaluation and training. It incorporates a neighborhood-based conditioning mechanism for personalized recourse.

Result: Experiments on benchmark datasets demonstrate PAR's efficiency in generating realistic, sparse, and highly plausible counterfactuals, outperforming existing state-of-the-art methods.

Conclusion: PAR effectively advances algorithmic recourse by leveraging a probabilistic approach, balancing feasibility, plausibility, and personalization in generating actionable recommendations.

Abstract: Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.

</details>


### [410] [Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment](https://arxiv.org/abs/2601.17329)
*Tiejin Chen,Xiaoou Liu,Vishnu Nandam,Kuan-Ru Liou,Hua Wei*

Main category: cs.LG

TL;DR: The paper introduces Conformal Feedback Alignment (CFA), which addresses noisy and inconsistent preference labels in preference-based RL models by introducing a method based on conformal prediction for reliable preference weighting.


<details>
  <summary>Details</summary>
Motivation: Current methods like RLHF rely on noisy pairwise preferences and lack consideration for the reliability of answers being compared, leading to inefficiencies and reduced robustness in alignment.

Method: Introduces CFA, a method using conformal prediction sets to quantify the reliability of answers. These are used to weight preferences more effectively in training models such as DPO and PPO.

Result: CFA improves the robustness and data efficiency of alignment models through reliable answer-side uncertainty quantification, as demonstrated in experiments across various datasets.

Conclusion: Modeling answer-side uncertainty through CFA complements traditional preference-level weighting, offering a statistically grounded, robust, and efficient approach to preference-based alignment.

Abstract: Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.

</details>


### [411] [Thermodynamically Optimal Regularization under Information-Geometric Constraints](https://arxiv.org/abs/2601.17330)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: The paper presents a unifying theoretical framework that connects thermodynamic optimality, information geometry, and regularization. It introduces assumptions and derives a conditional optimality theorem showing a connection between the Fisher-Rao metric and optimal regularization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the theoretical heterogeneity of modern regularization techniques while considering the increasing energetic demands of training large machine learning models. It explores whether current algorithms approach fundamental efficiency limits.

Method: The authors propose a framework based on three assumptions: (1) intrinsic and parametrization-invariant measures of information for optimality, (2) modeling belief states using maximum-entropy distributions, and (3) quasi-static processes for optimality. They derive a conditional optimality theorem using these assumptions.

Result: The study identifies the Fisher--Rao metric as the unique measure on belief space related to thermodynamic optimality. Classical regularization approaches fail to guarantee thermodynamic efficiency, and novel geometric and thermodynamic foundations are provided.

Conclusion: The paper highlights the structural incapabilities of classical regularization methods regarding thermodynamic optimality and proposes a new notion of thermodynamic efficiency in learning, with experimentally testable predictions for future research.

Abstract: Modern machine learning relies on a collection of empirically successful but theoretically heterogeneous regularization techniques, such as weight decay, dropout, and exponential moving averages. At the same time, the rapidly increasing energetic cost of training large models raises the question of whether learning algorithms approach any fundamental efficiency bound. In this work, we propose a unifying theoretical framework connecting thermodynamic optimality, information geometry, and regularization.
  Under three explicit assumptions -- (A1) that optimality requires an intrinsic, parametrization-invariant measure of information, (A2) that belief states are modeled by maximum-entropy distributions under known constraints, and (A3) that optimal processes are quasi-static -- we prove a conditional optimality theorem. Specifically, the Fisher--Rao metric is the unique admissible geometry on belief space, and thermodynamically optimal regularization corresponds to minimizing squared Fisher--Rao distance to a reference state.
  We derive the induced geometries for Gaussian and circular belief models, yielding hyperbolic and von Mises manifolds, respectively, and show that classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality. We introduce a notion of thermodynamic efficiency of learning and propose experimentally testable predictions. This work provides a principled geometric and thermodynamic foundation for regularization in machine learning.

</details>


### [412] [Power-based Partial Attention: Bridging Linear-Complexity and Full Attention](https://arxiv.org/abs/2601.17334)
*Yufeng Huang*

Main category: cs.LG

TL;DR: The paper investigates the necessity of quadratic $O(L^2)$ attention in transformers and introduces power-based partial attention (PPA) with sub-quadratic complexity to explore its efficiency.


<details>
  <summary>Details</summary>
Motivation: To quantify the required complexity in attention mechanisms of transformers and determine if sub-quadratic attention can deliver comparable performance.

Method: The authors introduce PPA, an $O(L^{1+p})$ attention mechanism, and systematically evaluate transformer performance across varying attention complexities controlled by parameter $p$.

Result: Experiments reveal an S-curve-like behavior, transitioning performance from linear sliding-window attention to sub-quadratic performance levels near $0 < p < 1$, achieving parity with quadratic full attention.

Conclusion: Sub-quadratic attention mechanisms, specifically $0<p<1$, are sufficient for achieving comparable results to quadratic full attention, optimizing transformer complexity.

Abstract: It is widely accepted from transformer research that "attention is all we need", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \leq p \leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.

</details>


### [413] [Spectral Geometry for Deep Learning: Compression and Hallucination Detection via Random Matrix Theory](https://arxiv.org/abs/2601.17357)
*Davide Ettori*

Main category: cs.LG

TL;DR: The paper presents methods leveraging spectral geometry and random matrix theory to improve neural network reliability and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issues of unreliability and computational inefficiency in large language and deep neural networks.

Method: The methods include EigenTrack for detecting hallucinations/out-of-distribution behavior and RMT-KD for model compression using spectral analysis and knowledge distillation.

Result: The results demonstrate that spectral statistics offer robust signals for monitoring uncertainty and enabling effective model compression.

Conclusion: The study concludes that spectral geometry and random matrix theory provide practical tools for improving the reliability and performance of neural networks.

Abstract: Large language models and deep neural networks achieve strong performance but suffer from reliability issues and high computational cost. This thesis proposes a unified framework based on spectral geometry and random matrix theory to address both problems by analyzing the eigenvalue structure of hidden activations. The first contribution, EigenTrack, is a real-time method for detecting hallucinations and out-of-distribution behavior in language and vision-language models using spectral features and their temporal dynamics. The second contribution, RMT-KD, is a principled compression method that identifies informative spectral components and applies iterative knowledge distillation to produce compact and efficient models while preserving accuracy. Together, these results show that spectral statistics provide interpretable and robust signals for monitoring uncertainty and guiding compression in large-scale neural networks.

</details>


### [414] [Robust Privacy: Inference-Time Privacy through Certified Robustness](https://arxiv.org/abs/2601.17360)
*Jiankai Jin,Xiangzheng Zhang,Zhao Liu,Deyue Zhang,Quanchen Zou*

Main category: cs.LG

TL;DR: The paper introduces 'Robust Privacy' (RP), a method to enhance inference-time privacy by ensuring predictions are invariant within an input's neighborhood, thereby masking sensitive attributes and mitigating attacks.


<details>
  <summary>Details</summary>
Motivation: To prevent adversaries from inferring sensitive input attributes during inference time in machine learning systems, ensuring privacy and security.

Method: Proposed Robust Privacy (RP), which ensures predictions invariant within a radius around an input. Developed Attribute Privacy Enhancement (APE) to enhance privacy effects at the attribute level.

Result: Increased privacy by reducing the susceptibility to model inversion attacks (from 73% ASR to 4%) while managing partial performance degradation.

Conclusion: Robust Privacy effectively enhances privacy in machine learning by diminishing the relationship between sensitive inputs and outputs, while balancing performance impact.

Abstract: Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($σ=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.

</details>


### [415] [Diversified Scaling Inference in Time Series Foundation Models](https://arxiv.org/abs/2601.17376)
*Ruijin Hua,Zichuan Liu,Kun Zhang,Yiyuan Yang*

Main category: cs.LG

TL;DR: This paper explores inference-time compute scaling for Time Series Foundation Models (TSFMs), emphasizing diversified sampling to enhance performance and proposing a metric RobustMSE for assessment.


<details>
  <summary>Details</summary>
Motivation: To address the untapped potential of inference-time compute in TSFMs for optimizing performance without re-training.

Method: The study involves analyzing standard sampling flaws, introducing diverse sampling via time-series perturbations, and deriving theoretical trade-offs. Extensive experiments evaluate inference scaling and performance improvements.

Result: Diversified inference scaling notably enhances TSFM's performance across datasets, achieving efficiency without additional parameter updates. RobustMSE metric gauges TSFM performance under fixed computational budgets.

Conclusion: Inference design is crucial for TSFM optimization; diversified sampling enables scalable, compute-efficient performance improvements and reliability, eliminating the need for re-training.

Abstract: The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.

</details>


### [416] [GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems](https://arxiv.org/abs/2601.17396)
*Vashista Nobaub*

Main category: cs.LG

TL;DR: The paper introduces GO-OSC, a geometry-aware framework for analyzing degradation in oscillatory systems by focusing on geometric distortions of dynamics, enabling earlier and more stable detection of degradation.


<details>
  <summary>Details</summary>
Motivation: Existing diagnostics (energy-based or unconstrained learning) fail at early-stage degradation detection in oscillatory systems as they are structurally insensitive to geometric distortions like phase jitter and frequency drift.

Method: Introduces GO-OSC, a latent representation learning framework with canonicalization. It employs invariant geometric probes to detect degradations by targeting specific directions in latent space.

Result: Theoretical results prove geometric probes outperform energy-based methods under early-phase degradation. Experiments confirm earlier degradation detection, data efficiency, and robustness across conditions.

Conclusion: The work proposes a sensitive and robust framework for early-stage degradation detection in oscillatory systems, applicable across varying scenarios and conditions.

Abstract: Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.

</details>


### [417] [Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations](https://arxiv.org/abs/2601.17407)
*Prajwal Chauhan,Salah Eddine Choutri,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: The paper introduces D-SENO, a lightweight framework for solving various PDEs efficiently, with higher speed and competitive accuracy compared to existing models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inefficiency and high computational demands of current transformer-based models and neural operators in solving PDEs.

Method: The method leverages Dilated Squeeze-Excitation Neural Operator (D-SENO), using dilated convolutions to capture wide receptive fields and squeeze-and-excitation modules for adaptive channel scaling.

Result: D-SENO achieves training speeds up to 20 times faster than traditional models and provides competitive or superior accuracy in PDE benchmarks.

Conclusion: D-SENO offers a faster, lightweight, and accurate alternative to parameter-heavy frameworks, demonstrating advantages in multiple PDE applications.

Abstract: Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.

</details>


### [418] [Active Hypothesis Testing for Correlated Combinatorial Anomaly Detection](https://arxiv.org/abs/2601.17430)
*Zichuan Yang,Yiming Xing*

Main category: cs.LG

TL;DR: This paper introduces ECC-AHT, an adaptive algorithm designed for identifying anomalous subsets of streams under correlated noise in cyber-physical systems.


<details>
  <summary>Details</summary>
Motivation: The study is driven by the need to monitor and ensure security in cyber-physical systems, particularly under correlated noise challenges, which existing methods fail to address efficiently.

Method: The paper proposes ECC-AHT, an algorithm that uses adaptive measurements and maximizes Chernoff information between hypotheses, enabling active noise cancellation via differential sensing.

Result: ECC-AHT achieves optimal sample complexity guarantees and outperforms baseline methods in both synthetic and real-world environments.

Conclusion: The proposed ECC-AHT algorithm effectively identifies anomalies under correlated noise and provides a substantial improvement over existing techniques, validated through experiments.

Abstract: We study the problem of identifying an anomalous subset of streams under correlated noise, motivated by monitoring and security in cyber-physical systems. This problem can be viewed as a form of combinatorial pure exploration, where each stream plays the role of an arm and measurements must be allocated sequentially under uncertainty. Existing combinatorial bandit and hypothesis testing methods typically assume independent observations and fail to exploit correlation for efficient measurement design. We propose ECC-AHT, an adaptive algorithm that selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing. ECC-AHT achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments. The code is available on https://github.com/VincentdeCristo/ECC-AHT

</details>


### [419] [Data-driven Clustering and Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2601.17441)
*Ondrej Bohdal,Taha Ceritli,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: This paper addresses the problem of selecting representative task-specific adapters for on-device large language models, proposing a method called D2C for clustering and merging adapters to support multiple tasks under resource limits.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the impracticality of storing all task-specific adapters on resource-constrained devices like mobile phones, and to find an efficient way to generalize across multiple tasks with limited memory.

Method: The proposed method, D2C, uses minimal task-specific examples and an iterative optimization process to cluster task-specific adapters. These clusters are then merged to create multi-task adapters suitable for deployment within resource-constrained environments.

Result: Experimental results indicate that the D2C method successfully enhances performance while adhering to storage constraints, demonstrating its effectiveness for storing and utilizing multi-task adapters.

Conclusion: D2C offers a practical solution for optimizing and clustering task-specific adapters for large language models, enabling better generalization across tasks while meeting the memory restrictions of on-device deployments.

Abstract: On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.

</details>


### [420] [DREAM: Dual-Standard Semantic Homogeneity with Dynamic Optimization for Graph Learning with Label Noise](https://arxiv.org/abs/2601.17449)
*Yusheng Zhao,Jiaye Xie,Qixin Zhang,Weizhi Zhang,Xiao Luo,Zhiping Xiao,Philip S. Yu,Ming Zhang*

Main category: cs.LG

TL;DR: The paper introduces DREAM, a method to improve graph learning in noisy label conditions using a dual-standard semantic homogeneity framework.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of graph neural networks dealing with label noise, which affects real-world applications and is inadequately handled by current methods.

Method: Proposes the DREAM framework, which dynamically optimizes graph learning by evaluating node label reliability through a dual-standard relation assessment based on node proximity and topology.

Result: DREAM demonstrates superior performance across six datasets and three label noise scenarios compared to existing methods.

Conclusion: DREAM effectively tackles label noise in graph learning by leveraging relational and topological information, providing reliable optimization guidance.

Abstract: Graph neural networks (GNNs) have been widely used in various graph machine learning scenarios. Existing literature primarily assumes well-annotated training graphs, while the reliability of labels is not guaranteed in real-world scenarios. Recently, efforts have been made to address the problem of graph learning with label noise. However, existing methods often (i) struggle to distinguish between reliable and unreliable nodes, and (ii) overlook the relational information embedded in the graph topology. To tackle this problem, this paper proposes a novel method, Dual-Standard Semantic Homogeneity with Dynamic Optimization (DREAM), for reliable, relation-informed optimization on graphs with label noise. Specifically, we design a relation-informed dynamic optimization framework that iteratively reevaluates the reliability of each labeled node in the graph during the optimization process according to the relation of the target node and other nodes. To measure this relation comprehensively, we propose a dual-standard selection strategy that selects a set of anchor nodes based on both node proximity and graph topology. Subsequently, we compute the semantic homogeneity between the target node and the anchor nodes, which serves as guidance for optimization. We also provide a rigorous theoretical analysis to justify the design of DREAM. Extensive experiments are performed on six graph datasets across various domains under three types of graph label noise against competing baselines, and the results demonstrate the effectiveness of the proposed DREAM.

</details>


### [421] [Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping](https://arxiv.org/abs/2601.17467)
*Jianxiong Zhang,Bing Guo,Yuming Jiang,Haobo Wang,Bo An,Xuefeng Du*

Main category: cs.LG

TL;DR: The paper introduces ARS, a method to improve hallucination detection in large reasoning models by reshaping representations based on answer stability.


<details>
  <summary>Details</summary>
Motivation: Detecting hallucination in large reasoning models is challenging because traces are long, coherent, and detection methods often overfit to superficial patterns rather than ensuring answer validity.

Method: ARS generates counterfactual answers through perturbations in latent space and learns representations that group answer-agreeing states while separating answer-disagreeing ones, aiding hallucination risk detection.

Result: Experiments show ARS consistently improves hallucination detection in reasoning models and outperforms strong baseline detectors.

Conclusion: ARS offers a plug-and-play approach to enhance hallucination detection without requiring human annotations, improving robustness in detecting flawed reasoning traces.

Abstract: Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.

</details>


### [422] [Identifying and Correcting Label Noise for Robust GNNs via Influence Contradiction](https://arxiv.org/abs/2601.17469)
*Wei Ju,Wei Zhang,Siyu Yi,Zhengyang Mao,Yifan Wang,Jingyang Yuan,Zhiping Xiao,Ziyue Qiao,Ming Zhang*

Main category: cs.LG

TL;DR: The paper introduces ICGNN, a novel approach to tackle noisy labels in Graph Neural Networks (GNNs) using a noise detection and correction framework.


<details>
  <summary>Details</summary>
Motivation: Noisy labels, caused by annotation errors or inconsistencies, hinder the performance and robustness of GNNs, making it essential to address this issue for accurate learning on graph-structured data.

Method: The authors propose ICGNN with several steps: (1) use a noise indicator called the influence contradiction score (ICS) to detect label noise, (2) employ a Gaussian mixture model for precise noise detection, (3) correct noisy labels based on neighboring node predictions using a soft strategy, and (4) incorporate pseudo-labeling for unlabeled nodes to optimize the model.

Result: Experiments on benchmark datasets demonstrate the effectiveness and superiority of ICGNN in handling noisy labels compared to other approaches.

Conclusion: The proposed ICGNN method successfully improves GNN robustness by leveraging graph structure, identifying noisy labels, and incorporating additional pseudo-label supervision for enhanced learning.

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in learning from graph-structured data with various applications such as social analysis and bioinformatics. However, the presence of label noise in real scenarios poses a significant challenge in learning robust GNNs, and their effectiveness can be severely impacted when dealing with noisy labels on graphs, often stemming from annotation errors or inconsistencies. To address this, in this paper we propose a novel approach called ICGNN that harnesses the structure information of the graph to effectively alleviate the challenges posed by noisy labels. Specifically, we first design a novel noise indicator that measures the influence contradiction score (ICS) based on the graph diffusion matrix to quantify the credibility of nodes with clean labels, such that nodes with higher ICS values are more likely to be detected as having noisy labels. Then we leverage the Gaussian mixture model to precisely detect whether the label of a node is noisy or not. Additionally, we develop a soft strategy to combine the predictions from neighboring nodes on the graph to correct the detected noisy labels. At last, pseudo-labeling for abundant unlabeled nodes is incorporated to provide auxiliary supervision signals and guide the model optimization. Experiments on benchmark datasets show the superiority of our proposed approach.

</details>


### [423] [LeanTutor: Towards a Verified AI Mathematical Proof Tutor](https://arxiv.org/abs/2601.17473)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.LG

TL;DR: The paper introduces LeanTutor, an AI-based mathematical proof tutor combining LLMs and theorem provers for provably-correct tutoring.


<details>
  <summary>Details</summary>
Motivation: Large Language Models are effective for natural language communication but error-prone, while theorem provers like Lean ensure correctness but are hard to use for students. The paper aims to combine the strengths of both.

Method: The system LeanTutor integrates an autoformalizer/proof-checker, a next-step generator, and a natural language feedback generator. It utilizes a dataset, PeanoBench, for evaluation.

Result: A proof-of-concept system LeanTutor was developed, demonstrating the merging of LLMs and theorem provers, supported by the introduction of the PeanoBench dataset.

Conclusion: LeanTutor showcases how combining LLMs and theorem provers can enhance learning in mathematical proofs, balancing ease of communication with correctness.

Abstract: This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.

</details>


### [424] [Unintended Memorization of Sensitive Information in Fine-Tuned Language Models](https://arxiv.org/abs/2601.17480)
*Marton Szep,Jorge Marin Ruiz,Georgios Kaissis,Paulina Seidl,Rüdiger von Eisenhart-Rothe,Florian Hinterwimmer,Daniel Rueckert*

Main category: cs.LG

TL;DR: The study examines the risk of sensitive data leakage in fine-tuned Large Language Models (LLMs), identifies factors influencing PII memorization, and evaluates privacy-preserving methods.


<details>
  <summary>Details</summary>
Motivation: To address the risk that fine-tuned LLMs unintentionally memorize and expose sensitive Personally Identifiable Information (PII), compromising privacy and safety.

Method: The research uses synthetic and real-world datasets with controlled extraction probes to measure PII memorization behavior, alongside benchmarking privacy-preserving solutions such as differential privacy, machine unlearning, regularization, and preference alignment.

Result: Post-training methods consistently balance privacy and performance, while differential privacy effectively reduces leakage but can cause training instability.

Conclusion: Memorization of sensitive data in fine-tuned LLMs remains a challenge, necessitating scalable and robust privacy-preserving strategies.

Abstract: Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacy-utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.

</details>


### [425] [Automatic Stability and Recovery for Neural Network Training](https://arxiv.org/abs/2601.17483)
*Barak Or*

Main category: cs.LG

TL;DR: The paper introduces a framework to detect and recover from neural network training instabilities without altering the optimizer.


<details>
  <summary>Details</summary>
Motivation: To address the fragility and instability of neural network training, which often leads to divergence or performance degradation.

Method: A supervisory runtime stability framework using innovation signals from secondary measurements, enabling runtime safety and recovery without optimizer modifications.

Result: The framework can detect and recover destabilizing updates, with theoretical runtime safety guarantees, while incurring minimal computational overhead.

Conclusion: The proposed approach effectively stabilizes training processes and is compatible with various memory-constrained settings.

Abstract: Training modern neural networks is increasingly fragile, with rare but severe destabilizing updates often causing irreversible divergence or silent performance degradation. Existing optimization methods primarily rely on preventive mechanisms embedded within the optimizer, offering limited ability to detect and recover from instability once it occurs. We introduce a supervisory runtime stability framework that treats optimization as a controlled stochastic process. By isolating an innovation signal derived from secondary measurements, such as validation probes, the framework enables automatic detection and recovery from destabilizing updates without modifying the underlying optimizer. We provide theoretical runtime safety guarantees that formalize bounded degradation and recovery. Our implementation incurs minimal overhead and is compatible with memory-constrained training settings.

</details>


### [426] [SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving](https://arxiv.org/abs/2601.17489)
*Ashutosh Bajpai,Akshat Bhandari,Akshay Nambi,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: The paper presents SpatialMath, a framework enhancing multimodal language models with spatial comprehension for better performance in vision-intensive math problems.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with geometric problems requiring integration of visual comprehension and mathematical reasoning.

Method: SpatialMath uses a perception module to extract spatially-grounded representations and integrates them into symbolic reasoning chains.

Result: The framework achieves up to 10 percentage points improvement over baselines and demonstrates increased robustness in reasoning accuracy.

Conclusion: Structured visual representations enhance reasoning pipelines in multimodal language models, emphasizing the value of spatial comprehension for solving complex tasks.

Abstract: Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.

</details>


### [427] [PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems](https://arxiv.org/abs/2601.17495)
*Ruiyu Zhang,Lin Nie,Wai-Fung Lam,Qihao Wang,Xin Zhao*

Main category: cs.LG

TL;DR: The study introduces PEARL, a label-efficient method to align embeddings towards class prototypes for improved local neighborhood structure in similarity-based systems.


<details>
  <summary>Details</summary>
Motivation: In real-world systems using embeddings from pretrained models, nearest-neighbor retrieval often fails due to poor alignment with neighborhood structure. Labeled data is scarce, domain shifts occur, and retraining base encoders is difficult.

Method: PEARL (Prototype-Enhanced Aligned Representation Learning) uses limited supervision to align embeddings softly toward class prototypes, preserving dimensionality while reshaping local neighborhood geometry.

Result: PEARL shows substantial improvements in local neighborhood quality, achieving 25.7% gains over raw embeddings and over 21.1% relative improvements compared to strong unsupervised methods, particularly in cases of label scarcity.

Conclusion: PEARL bridges the gap between unsupervised and fully supervised post-processing methods, offering significant performance boosts in label-scarce scenarios commonly faced in similarity-based deployments.

Abstract: In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.

</details>


### [428] [One-Shot Federated Clustering of Non-Independent Completely Distributed Data](https://arxiv.org/abs/2601.17512)
*Yiqun Zhang,Shenghong Cai,Zihua Yang,Sen Feng,Yuzhu Ji,Haijun Zhang*

Main category: cs.LG

TL;DR: The paper introduces a novel framework called GOLD for addressing Federated Clustering (FC) challenges arising from Non-IID issues in distributed systems by enhancing local and global cluster distribution learning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve Federated Learning for distributed IoT systems by addressing clustering problems in Non-IID client environments, particularly dealing with fragmented cluster distributions.

Method: The proposed approach, GOLD (Global Oriented Local Distribution Learning), addresses FC challenges by exploring incomplete local cluster distributions, fusing global distributions, and enhancing local clusters under global guidance.

Result: Experiments demonstrate GOLD's effectiveness with significant tests, scalability evaluations, ablation studies, and qualitative analysis.

Conclusion: The GOLD framework overcomes clustering bottlenecks in Non-IID environments and offers robust performance improvements in privacy-preserving distributed systems.

Abstract: Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.

</details>


### [429] [Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment](https://arxiv.org/abs/2601.17563)
*Nathan Gavenski,Matteo Leonetti,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: The paper introduces UfO (Unsupervised Imitation Learning from Observation), improving upon state-of-the-art methods by bypassing supervision, achieving superior performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Current ILfO methods struggle with limitations like needing action supervision, assuming single optimal actions, and improperly applying teacher actions to environment states.

Method: UfO employs a two-stage process: approximating teacher actions from state transitions, followed by aligning agent trajectories with the teacher's for policy refinement.

Result: UfO outperforms the teacher and existing ILfO baselines across five environments and achieves reduced standard deviation, emphasizing better generalization to unseen scenarios.

Conclusion: UfO effectively overcomes the limitations of ILfO methods, demonstrating improved policy learning, performance, and robustness through unsupervised techniques.

Abstract: State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.

</details>


### [430] [Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout](https://arxiv.org/abs/2601.17602)
*Xuanzhou Chen*

Main category: cs.LG

TL;DR: This paper investigates the effects of overparameterization in Transformer models, focusing on angular similarity in high-dimensional embeddings and performance preservation during Bernoulli dropout.


<details>
  <summary>Details</summary>
Motivation: To understand how overparameterization impacts Transformer encoder-decoder embeddings and their stability under dropout, aiming to find thresholds for preserved model performance.

Method: The authors theoretically analyze embedding sparsity during Bernoulli dropout and empirically construct a Transformer augmented with Binary Erasure Channel (BEC). They measure performance on English-French translation tasks.

Result: The study identified the dropout thresholds at which validation accuracies and BLEU scores decline sharply, highlighting the sensitivity of the model to sparsity changes.

Conclusion: Transformer embeddings and performance are resilient to moderate coordinate dropout when sparsity is sufficiently large, but fail beyond certain thresholds, emphasizing the importance of embedding design.

Abstract: We study Transformer overparameterization through the lens of angular similarity in high-dimensional encoder-decoder embeddings. We apply Bernoulli dropout between the encoder and the decoder, varying the keep probability $p$ to identify a sparsity-dependent threshold above which the Top-1 prediction is preserved. Theoretically, we prove that, if the effective sparsity embeddings is sufficiently large, and thus decoder performance, remain stable under moderate coordinate dropout. Empirically, we implement the Bernoulli dropout by constructing a new Transformer model augmented with Binary Erasure Channel (BEC) and test its performance on an English-French translation task. Experimental results visualize the trends for validation accuracies and BLEU scores, both decline sharply at some threshold.

</details>


### [431] [A Thermodynamic Theory of Learning I: Irreversible Ensemble Transport and Epistemic Costs](https://arxiv.org/abs/2601.17607)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: This paper investigates how learning systems can generate structured insights despite classical information-theory constraints, proposing an epistemic perspective that incorporates entropy production and introduces the Epistemic Speed Limit (ESL).


<details>
  <summary>Details</summary>
Motivation: To address the paradox between deterministic transformation limits in information theory and the ability of learning systems to produce structured insights and abstractions.

Method: Proposed a framework based on transport processes in probability space, introducing an 'epistemic free-energy' concept and deriving ESL, a finite-time inequality to estimate minimal entropy production needed for a learning transformation.

Result: Developed an inequality (ESL) that lower-bounds the entropy production required for learning, dependent on Wasserstein distance between probability distributions while being independent of specific algorithms.

Conclusion: Learning is inherently irreversible over finite time, and achieving epistemic insight involves unavoidable entropy production, formalized with ESL to understand the constraints of learning processes.

Abstract: Learning systems acquire structured internal representations from data, yet classical information-theoretic results state that deterministic transformations do not increase information. This raises a fundamental question: how can learning produce abstraction and insight without violating information-theoretic limits?
  We argue that learning is inherently an irreversible process when performed over finite time, and that the realization of epistemic structure necessarily incurs entropy production. To formalize this perspective, we model learning as a transport process in the space of probability distributions over model configurations and introduce an epistemic free-energy framework.
  Within this framework, we define the free-energy drop as a bookkeeping quantity that records the total reduction of epistemic free energy along a learning trajectory. This reduction decomposes into a reversible component associated with potential improvement and an irreversible component corresponding to entropy production.
  We then derive the Epistemic Speed Limit (ESL), a finite-time inequality that lower-bounds the minimal entropy production required by any learning process to realize a given distributional transformation. This bound depends only on the Wasserstein distance between initial and final ensemble distributions and is independent of the specific learning algorithm.

</details>


### [432] [Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning](https://arxiv.org/abs/2601.17616)
*Fatema Siddika,Md Anwar Hossen,Tanwi Mallick,Ali Jannesari*

Main category: cs.LG

TL;DR: SETA framework mitigates catastrophic forgetting in Large Language Models by using modular subspaces for task-specific and shared knowledge.


<details>
  <summary>Details</summary>
Motivation: Address the plasticity-stability dilemma in continual learning for LLMs, aiming to prevent catastrophic forgetting when acquiring new tasks or capabilities.

Method: Introduce a modular architecture with a mixture of sparse experts, separating task-specific and shared knowledge, maintained via elastic weight anchoring, and enable efficient retrieval using a unified gating network.

Result: SETA outperformed state-of-the-art methods in continual learning across both domain-specific and general benchmarks.

Conclusion: SETA effectively resolves the plasticity-stability conflict, provides efficient continual learning, and preserves task-specific and shared capabilities in LLMs.

Abstract: Continual learning in Large Language Models (LLMs) is hindered by the plasticity-stability dilemma, where acquiring new capabilities often leads to catastrophic forgetting of previous knowledge. Existing methods typically treat parameters uniformly, failing to distinguish between specific task knowledge and shared capabilities. We introduce Mixture of Sparse Experts for Task-Agnostic Continual Learning, referred to as SETA, a framework that resolves the plasticity-stability conflict by decomposing the model into modular subspaces. Unlike standard updates, where tasks compete for the same parameters, SETA separates knowledge into unique experts, designed to isolate task-specific patterns, and shared experts, responsible for capturing common features. This structure is maintained through elastic weight anchoring, which protects critical shared knowledge and enables a unified gating network to automatically retrieve the correct expert combination for each task during inference. Extensive experiments across diverse domain-specific and general benchmarks demonstrate that SETA consistently outperforms state-of-the-art parameter-efficient fine-tuning-based continual learning methods.

</details>


### [433] [BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation](https://arxiv.org/abs/2601.17625)
*Yuhan Xie,Jinhan Liu,Xiaoyong Ni,Fei Tan,Icare Sakr,Thibault Collin,Shiqi Sun,Alejandro Rodriguez Guajardo,Demon Fanny,Charles-francois Vincent Latchoumane,Henri Lorach,Jocelyne Bloch,Gregoire Courtine,Mahsa Shoaran*

Main category: cs.LG

TL;DR: Transformer-based neural decoders showcase strong performance in BCIs but are computationally expensive. BrainDistill overcomes this by combining neural decoding with task-specific knowledge distillation, optimizing performance for implantable systems.


<details>
  <summary>Details</summary>
Motivation: The challenge is to develop high-performing BCI decoders with reduced computational demands suitable for power-constrained implantable systems, addressing the inefficiency of existing transformer-based decoders.

Method: The paper introduces BrainDistill, which includes an implantable neural decoder (IND) and a task-specific knowledge distillation (TSKD) framework, prioritizing relevant decoding features. Additionally, it implements a quantization-aware training scheme for efficient deployment.

Result: BrainDistill's IND outperforms previous decoders, especially with TSKD in few-shot calibration scenarios, and maintains performance even with quantization for power-efficient implantable BCI applications.

Conclusion: BrainDistill provides an effective solution to balance performance and power constraints in BCIs, enabling high-performance motor decoding in implantable systems through knowledge distillation and quantization techniques.

Abstract: Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.

</details>


### [434] [RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding](https://arxiv.org/abs/2601.17641)
*Hao Fang,Ryan A. Canfield,Tomohiro Ouchi,Beatrice Macagno,Eli Shlizerman,Amy L. Orsborn*

Main category: cs.LG

TL;DR: The paper introduces RPNT, a pretrained neural transformer model for decoding brain activity into behavior.


<details>
  <summary>Details</summary>
Motivation: To create a neural decoding model that can generalize across different brain sites, sessions, behaviors, and subjects.

Method: The authors designed RPNT with multidimensional rotary positional embedding (MRoPE), context-based attention, and robust self-supervised learning (SSL). It was pretrained on two datasets from monkey brain recordings.

Result: RPNT outperformed existing decoding models in generalizing to cross-session, cross-type, cross-subject, and cross-site tasks.

Conclusion: RPNT's robust design and pretraining enable higher generalization and performance in brain decoding tasks.

Abstract: Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.

</details>


### [435] [A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization](https://arxiv.org/abs/2601.17646)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: This paper focuses on the stability of empirical risk minimization (ERM) for set-valued outputs, introducing a new intrinsic stability notion and establishing related properties and bounds.


<details>
  <summary>Details</summary>
Motivation: ERM stability has been traditionally studied for single-valued outputs, but convex non-strict losses produce set-valued minimizers. The motivation is to develop a robust stability concept for such settings and address theoretical gaps in interpreting stability for set-valued minimizers.

Method: The authors introduce Painlevé-Kuratowski upper semicontinuity (PK-u.s.c.) as the stability concept for ERM solution correspondences and build a framework to analyze set-valued outputs. They examine Mosco-consistent perturbations, local boundedness, and quadratic growth to establish qualitative and quantitative results.

Result: They demonstrate that Mosco-consistent perturbations and locally bounded minimizers ensure PK-u.s.c., minimal-value continuity, and near-minimizer consistency. They also derive quantitative deviation bounds under quadratic growth.

Conclusion: The study establishes a new stability framework for ERM with set-valued minimizers, providing theoretical insights and precise conditions for both qualitative and quantitative stability.

Abstract: Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlevé-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.

</details>


### [436] [Time-Varying Causal Treatment for Quantifying the Causal Effect of Short-Term Variations on Arctic Sea Ice Dynamics](https://arxiv.org/abs/2601.17647)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: This paper introduces KGCM-VAE to quantify the causal relationship between sea ice thickness and sea surface height (SSH) for understanding polar climate change.


<details>
  <summary>Details</summary>
Motivation: Conventional models face challenges in estimating treatment effects in spatiotemporal data due to unobserved confounders and lack of physical constraints.

Method: The KGCM-VAE integrates velocity modulation, MMD balancing of covariate distributions in latent space, and causal adjacency-constrained decoding.

Result: Experimental evaluations show superior performance in PEHE and a 1.88% reduction in estimation error compared to benchmarks.

Conclusion: KGCM-VAE offers a reliable method to understand causal mechanisms in polar climate systems, advancing global sea-level rise predictions.

Abstract: Quantifying the causal relationship between ice melt and freshwater distribution is critical, as these complex interactions manifest as regional fluctuations in sea surface height (SSH). Leveraging SSH as a proxy for sea ice dynamics enables improved understanding of the feedback mechanisms driving polar climate change and global sea-level rise. However, conventional deep learning models often struggle with reliable treatment effect estimation in spatiotemporal settings due to unobserved confounders and the absence of physical constraints. To address these challenges, we propose the Knowledge-Guided Causal Model Variational Autoencoder (KGCM-VAE) to quantify causal mechanisms between sea ice thickness and SSH. The proposed framework integrates a velocity modulation scheme in which smoothed velocity signals are dynamically amplified via a sigmoid function governed by SSH transitions to generate physically grounded causal treatments. In addition, the model incorporates Maximum Mean Discrepancy (MMD) to balance treated and control covariate distributions in the latent space, along with a causal adjacency-constrained decoder to ensure alignment with established physical structures. Experimental results on both synthetic and real-world Arctic datasets demonstrate that KGCM-VAE achieves superior PEHE compared to state-of-the-art benchmarks. Ablation studies further confirm the effectiveness of the approach, showing that the joint application of MMD and causal adjacency constraints yields a 1.88\% reduction in estimation error.

</details>


### [437] [Entropic Risk-Aware Monte Carlo Tree Search](https://arxiv.org/abs/2601.17667)
*Pedro P. Santos,Jacopo Silvestrin,Alberto Sardinha,Francisco S. Melo*

Main category: cs.LG

TL;DR: The paper introduces a Monte Carlo tree search algorithm designed for risk-aware Markov decision processes measured by entropic risk. It demonstrates theoretical correctness and polynomial regret concentration, supported by comparisons with baseline methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving risk-aware Markov decision processes using a formal entropic risk measure approach in a computationally efficient and theoretically sound manner.

Method: Proposes a Monte Carlo tree search algorithm leveraging dynamic programming formulations along with upper confidence bound-based strategies for risk-aware MDPs with ERM objectives. Conducts non-asymptotic analysis for correctness and efficiency.

Result: The algorithm is demonstrated to provide optimal empirical ERM values with polynomial regret concentration. Experiments validate its performance against baseline methods.

Conclusion: The paper introduces an effective, theoretically grounded algorithm for risk-aware decision processes, validated through analysis and experiments.

Abstract: We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving \textit{risk-aware} Markov decision processes (MDPs) with \textit{entropic risk measure} (ERM) objectives. We provide a \textit{non-asymptotic} analysis of our proposed algorithm, showing that the algorithm: (i) is \textit{correct} in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys \textit{polynomial regret concentration}. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.

</details>


### [438] [Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction](https://arxiv.org/abs/2601.17668)
*Jang-Hyun Kim,Dongyoon Han,Sangdoo Yun*

Main category: cs.LG

TL;DR: The paper introduces a gating-based method to efficiently compress KV cache in frozen-weight LLMs with high compression ratios and negligible computational cost.


<details>
  <summary>Details</summary>
Motivation: Efficient KV cache management is essential for deploying large language models, but existing methods often sacrifice performance or computational efficiency.

Method: The method involves a gating-based eviction mechanism that uses lightweight sink-attention gating modules and a forward-pass algorithm for gate training, requiring no backpropagation. It works seamlessly in prefill and decoding stages.

Result: The approach achieves up to 70% KV cache evictions with negligible performance loss across multiple tasks and LLM families such as Qwen2.5-1M, Qwen3, and Gemma3.

Conclusion: Gating-based KV cache eviction method is both efficient and generalizable, offering high compression with near-lossless performance across diverse tasks.

Abstract: Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.

</details>


### [439] [$\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts](https://arxiv.org/abs/2601.17680)
*Shota Takashiro,Takeshi Kojima,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: The paper introduces $$-MoE, a model using infinite experts in continuous space, showing competitive computational efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of training MoE models with an increasing number of experts, while stabilizing training and maintaining efficiency.

Method: Introduce $$-MoE, which selects parts of FFN parameters in a continuous space for individual tokens, allowing infinite experts and computational efficiency.

Result: $$-MoE achieved comparable performance to GPT-2 Medium while using fewer parameters and offered improved accuracy (up to 2.5%) over traditional MoE.

Conclusion: $$-MoE enables flexible trade-offs between accuracy and speed, demonstrating the potential for computationally efficient language models with scalable expert configurations.

Abstract: The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.

</details>


### [440] [Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis](https://arxiv.org/abs/2601.17687)
*Hao Li,He Cao,Shenyao Peng,Zijing Liu,Bin Feng,Yu Wang,Zhiyuan Yan,Yonghong Tian,Yu Li,Li Yuan*

Main category: cs.LG

TL;DR: ChemCRAFT is a framework that enhances small language models for biochemistry by using agentic reinforcement learning to decouple chemical reasoning from knowledge storage, enabling high performance without privacy risks or high costs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of small language models (prone to hallucination and poor knowledge retention) and large cloud-based models (privacy risks and high costs) in the biochemistry domain.

Method: The method introduces ChemCRAFT, which uses agentic reinforcement learning and interactive sandboxes for precise chemical information retrieval, a new chemical tool trajectory dataset (ChemToolDataset), and a SMILES-GRPO dense reward function for agent calling.

Result: ChemCRAFT surpasses cloud-based LLMs in tasks like molecular structure analysis, optimization, and synthesis prediction while using a locally deployable model with minimal costs.

Conclusion: ChemCRAFT presents a cost-effective, privacy-preserving framework for AI-driven biochemistry, showing that scientific reasoning can be a learnable policy of tool orchestration rather than solely scale-dependent.

Abstract: Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model's ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.

</details>


### [441] [REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization](https://arxiv.org/abs/2601.17689)
*Shanu Saklani,Tushar M. Athawale,Nairita Pal,David Pugmire,Christopher R. Johnson,Soumya Dutta*

Main category: cs.LG

TL;DR: REV-INR, a regularized evidential implicit neural representation, enhances volumetric data reconstruction with uncertainty estimation, outperforming existing methods in quality and speed.


<details>
  <summary>Details</summary>
Motivation: Traditional INRs fail to provide uncertainty estimation in reconstructions, which can result in inaccurate data interpretation when raw data is unavailable.

Method: REV-INR incorporates regularized evidential learning to simultaneously predict data values and coordinate-level uncertainties during inference using a single forward pass.

Result: REV-INR outperforms established methods in reconstruction quality, uncertainty estimates, and inference speed, enabling more reliable and trustworthy volume visualizations.

Conclusion: REV-INR represents a significant advancement in volumetric data visualization by integrating robust uncertainty estimation directly into the reconstruction process.

Abstract: Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.

</details>


### [442] [FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices](https://arxiv.org/abs/2601.17713)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Yinfeng Cao*

Main category: cs.LG

TL;DR: This paper introduces FedCCA, a federated learning approach tackling data heterogeneity among IoT devices through client-specific adaptation and dynamic aggregation, demonstrating superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: Handling private data, such as IoT sensing data, demands privacy-preserving approaches like federated learning. Yet, data heterogeneity among IoT devices hampers model performance and convergence, motivating the development of a solution.

Method: The proposed FedCCA employs dynamic client selection and adaptive aggregation using a client-specific encoder. It integrates an attention-based global aggregation strategy for efficient multi-source knowledge transfer.

Result: FedCCA outperformed existing methods in experiments, demonstrating substantial model performance improvement and efficacy in tackling data heterogeneity issues.

Conclusion: FedCCA effectively addresses data heterogeneity in federated learning by leveraging client-specific knowledge and dynamic strategies, ensuring privacy preservation while improving model performance.

Abstract: With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [443] [Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games](https://arxiv.org/abs/2601.17716)
*Daniel M. Pedrozo,Telma W. de L. Soares,Bryan L. M. de Oliveira*

Main category: cs.LG

TL;DR: This paper introduces a multi-turn dialogue framework to evaluate how effectively LLMs gather information through structured yes/no questions in a knowledge graph environment.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with asking effective questions to resolve ambiguities, and existing benchmarks fail to adequately assess their information-seeking behavior using intermediate signals like Information Gain (IG).

Method: The authors propose a dialogue system with three LLM agents—one asking questions, one answering, and one updating the hypothesis space. They use IG grounded in Shannon entropy as a metric to evaluate the quality of questions. Experiments are conducted in a geographic game setting.

Result: Models with explicit reasoning (e.g., Chain-of-Thought) outperform others, achieving higher information gain per turn and solving tasks efficiently, particularly in partially-observable settings.

Conclusion: Explicit reasoning in LLMs leads to more effective ambiguity resolution, with larger models focusing on optimal questions and smaller models balancing this limitation with iterative exploration of options.

Abstract: Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.

</details>


### [444] [AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation](https://arxiv.org/abs/2601.17761)
*Dongjie Cheng,Ruifeng Yuan,Yongqi Li,Runyang You,Wenjie Wang,Liqiang Nie,Lei Zhang,Wenjie Li*

Main category: cs.LG

TL;DR: AR-Omni introduces a unified autoregressive model enabling multimodal text, image, and speech generation under a single Transformer decoder, resolving practical multimodal modeling challenges.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models rely heavily on expert components for multimodal generation, necessitating a unified and efficient system for training and inference.

Method: AR-Omni proposes a single autoregressive Transformer decoder supporting multimodal inputs and outputs, addressing issues like modality imbalance, visual fidelity, and stability-creativity trade-offs.

Result: The system demonstrates strong multimodal generation quality for text, images, and speech, maintaining real-time capabilities with a 0.88 real-time factor for speech.

Conclusion: AR-Omni represents a scalable and efficient approach to multimodal generation, eliminating the need for expert decoders and achieving competitive performance across modalities.

Abstract: Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of "Omni" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.

</details>


### [445] [Shortcut Learning in Binary Classifier Black Boxes: Applications to Voice Anti-Spoofing and Biometrics](https://arxiv.org/abs/2601.17782)
*Md Sahidullah,Hye-jin Shim,Rosa Gonzalez Hautamäki,Tomi H. Kinnunen*

Main category: cs.LG

TL;DR: This paper proposes a framework to analyze biases in binary classifiers using intervention and observational techniques. It evaluates classifier performance, focusing beyond error rates, with experiments conducted in audio anti-spoofing and speaker verification tasks.


<details>
  <summary>Details</summary>
Motivation: Deep-learning models often inherit biases from datasets, which can lead to unexpected and undesired model behavior. Addressing and understanding these biases is crucial for building robust AI systems.

Method: The study introduces a framework employing intervention and observational perspectives alongside a linear mixed-effects model for post-hoc analysis to uncover biases in classifier behavior.

Result: The proposed method provided insights into how biased datasets affect classifier behavior and demonstrated its effectiveness through experiments using both statistical models and deep neural networks.

Conclusion: This work advances understanding of bias in AI, contributes to explainable artificial intelligence, and has broad implications for addressing biases across domains.

Abstract: The widespread adoption of deep-learning models in data-driven applications has drawn attention to the potential risks associated with biased datasets and models. Neglected or hidden biases within datasets and models can lead to unexpected results. This study addresses the challenges of dataset bias and explores ``shortcut learning'' or ``Clever Hans effect'' in binary classifiers. We propose a novel framework for analyzing the black-box classifiers and for examining the impact of both training and test data on classifier scores. Our framework incorporates intervention and observational perspectives, employing a linear mixed-effects model for post-hoc analysis. By evaluating classifier performance beyond error rates, we aim to provide insights into biased datasets and offer a comprehensive understanding of their influence on classifier behavior. The effectiveness of our approach is demonstrated through experiments on audio anti-spoofing and speaker verification tasks using both statistical models and deep neural networks. The insights gained from this study have broader implications for tackling biases in other domains and advancing the field of explainable artificial intelligence.

</details>


### [446] [Robust Computational Extraction of Non-Enhancing Hypercellular Tumor Regions from Clinical Imaging Data](https://arxiv.org/abs/2601.17802)
*A. Brawanski,Th. Schaffer,F. Raab,K. -M. Schebesch,M. Schrey,Chr. Doenitz,A. M. Tomé,E. W. Lang*

Main category: cs.LG

TL;DR: A computational framework is introduced to identify non-enhancing hypercellular (NEH) tumor regions in brain MRI scans, validated with clinical markers, for improved precision oncology.


<details>
  <summary>Details</summary>
Motivation: The need for accurate identification of NEH tumor regions in neuro-imaging to enhance patient treatment and management.

Method: A computational model using diverse network architectures generates probability maps for NEH regions, using MRI data.

Result: The approach demonstrated robustness and biological relevance when validated against clinical markers like rCBV and ETRL.

Conclusion: The framework offers reliable non-invasive tumor compartment mapping, promoting its use as imaging biomarkers in precision oncology workflows.

Abstract: Accurate identification of non-enhancing hypercellular (NEH) tumor regions is an unmet need in neuro-oncological imaging, with significant implications for patient management and treatment planning. We present a robust computational framework that generates probability maps of NEH regions from routine MRI data, leveraging multiple network architectures to address the inherent variability and lack of clear imaging boundaries. Our approach was validated against independent clinical markers -- relative cerebral blood volume (rCBV) and enhancing tumor recurrence location (ETRL) -- demonstrating both methodological robustness and biological relevance. This framework enables reliable, non-invasive mapping of NEH tumor compartments, supporting their integration as imaging biomarkers in clinical workflows and advancing precision oncology for brain tumor patients.

</details>


### [447] [MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging](https://arxiv.org/abs/2601.17858)
*Jiapeng Wang,Changxin Tian,Kunlong Chen,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: MergeMix provides an efficient solution to optimize data mixtures for LLM training by repurposing model merging weights, significantly reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current methods for optimizing data mixtures in large language models require expensive computations and heuristic approaches, creating a need for a cost-effective, accurate solution.

Method: MergeMix uses model merging weights trained on minimal tokens as proxies to determine data mixture ratios, evaluating them against benchmarks instead of relying on full-scale training.

Result: Experiments on 8B and 16B parameter models show MergeMix achieves comparable or better performance than manual tuning while drastically cutting costs. It also demonstrates consistency and scalability.

Conclusion: MergeMix offers a scalable, low-cost, and automated way to optimize data mixtures for LLMs, outperforming traditional manual approaches.

Abstract: Optimizing data mixtures is essential for unlocking the full potential of large language models (LLMs), yet identifying the optimal composition remains computationally prohibitive due to reliance on heuristic trials or expensive proxy training. To address this, we introduce \textbf{MergeMix}, a novel approach that efficiently determines optimal data mixing ratios by repurposing model merging weights as a high-fidelity, low-cost performance proxy. By training domain-specific experts on minimal tokens and optimizing their merging weights against downstream benchmarks, MergeMix effectively optimizes the performance of data mixtures without incurring the cost of full-scale training. Extensive experiments on models with 8B and 16B parameters validate that MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs. Furthermore, MergeMix exhibits high rank consistency (Spearman $ρ> 0.9$) and strong cross-scale transferability, offering a scalable, automated solution for data mixture optimization.

</details>


### [448] [EEG Foundation Models: Progresses, Benchmarking, and Open Problems](https://arxiv.org/abs/2601.17883)
*Dingkun Liu,Yuheng Chen,Zhu Chen,Zhenyao Cui,Yaozhi Wen,Jiayu An,Jingwei Luo,Dongrui Wu*

Main category: cs.LG

TL;DR: The paper reviews and comparatively evaluates EEG foundation models for BCIs, introducing a unified framework and analyzing their performance across 13 datasets.


<details>
  <summary>Details</summary>
Motivation: To address the lack of fair and thorough comparisons between EEG foundation models, considering their inconsistent objectives and protocols in literature.

Method: The study organizes design choices into a unified taxonomy and evaluates 12 models across various datasets, focusing on generalization and calibration capabilities, while comparing linear probing and full-parameter fine-tuning.

Result: Linear probing often falls short, specialist models trained from scratch remain competitive, and larger foundation models don't always show better generalization under current conditions.

Conclusion: Existing practices around EEG foundation models require reconsideration as specialist models perform well and larger models don't guarantee better results in BCIs.

Abstract: Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.

</details>


### [449] [Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization](https://arxiv.org/abs/2601.17910)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: This paper introduces a formal axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across various scales, aiming for theoretical rigor and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing multi-teacher knowledge distillation relies on heuristic weighting, limiting robustness, efficiency, and safety. A rigorous framework is needed.

Method: The paper develops axiomatic structural conditions for adaptive weighting, analyzes operators' existence, stability, and robustness, and formulates safety-constrained distillation.

Result: Operators within the proposed framework are theoretically robust and characterized by convergence, adaptability, and safety under distribution shifts and constraints.

Conclusion: The framework enables principled analysis and optimization of adaptive knowledge distillation methods without reliance on specific heuristic implementations.

Abstract: Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.

</details>


### [450] [Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN](https://arxiv.org/abs/2601.17912)
*Qinyi Liu,Mohammad Khalil,Naman Goel*

Main category: cs.LG

TL;DR: The paper investigates the fairness properties of TabPFN, a foundation model for tabular data, and finds its predictive accuracy strong but fairness inconsistent under specific conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the fairness aspects of TabPFN, a foundation model utilizing causal reasoning during pre-training, as fairness implications have not been sufficiently studied.

Method: The authors conduct an empirical evaluation of TabPFN and its fine-tuned variants, assessing fairness, predictive performance, and robustness across different conditions, including distributional shifts.

Result: TabPFN achieves high predictive accuracy and robustness to spurious correlations but demonstrates inconsistent fairness improvements, especially under missing-not-at-random covariate shifts.

Conclusion: The causal pre-training in TabPFN improves prediction and robustness but falls short in algorithmic fairness, necessitating further interventions for fairness improvement in practical usage.

Abstract: Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, have not yet been explored in sufficient depth. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying such models in practice and the need for further fairness interventions.

</details>


### [451] [UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR](https://arxiv.org/abs/2601.17916)
*Jialu Tang,Tong Xia,Yuan Lu,Aaqib Saeed*

Main category: cs.LG

TL;DR: UniPACT is a framework that integrates numerical EHR data with ECG signals for clinical prognosis using LLMs, achieving high accuracy across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to process heterogeneous clinical data types like EHR and ECG simultaneously, limiting accurate clinical prognosis.

Method: UniPACT utilizes a structured prompting mechanism to convert EHR into text, combining it with raw ECG representations to enable unified analysis by LLMs.

Result: UniPACT achieved a state-of-the-art mean AUROC of 89.37% on the MDS-ED benchmark across diverse prognostic tasks, outperforming specialized methods.

Conclusion: UniPACT's multimodal approach bridges the modality gap between textual and signal-based data, enhancing robustness and scalability in clinical scenarios.

Abstract: Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.

</details>


### [452] [treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding](https://arxiv.org/abs/2601.17917)
*Zhongyu Xiao,Zhiwei Hao,Jianyuan Guo,Yong Luo,Jia Liu,Jie Xu,Han Hu*

Main category: cs.LG

TL;DR: The paper introduces Streaming-dLLM, a technique to make Diffusion Large Language Models (dLLMs) faster and more efficient without compromising quality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address bottlenecks in the block-wise diffusion process of dLLMs, such as inefficiencies in handling redundant suffix regions and applying fixed denoising schedules, which hamper decoding speed and performance.

Method: The authors propose a training-free framework that optimizes dLLM inference by pruning redundant mask tokens (spatial optimization) and using an early exit mechanism with dynamic confidence-aware strategies (temporal optimization).

Result: Streaming-dLLM achieves up to 68.2X faster decoding speeds compared to existing diffusion models, while preserving the quality of natural language generation.

Conclusion: The proposed Streaming-dLLM framework significantly enhances the efficiency of dLLMs through spatial and temporal strategies, proving its viability for faster natural language generation without quality compromise.

Abstract: Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.

</details>


### [453] [Dissipative Learning: A Framework for Viable Adaptive Systems](https://arxiv.org/abs/2601.17933)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: Learning is conceptualized as a dissipative process requiring forgetting and regularization as fundamental aspects. The BEDS framework models this process, emphasizing thermodynamic optimality and unification of existing methods.


<details>
  <summary>Details</summary>
Motivation: To provide a unified and principled perspective on learning by highlighting the intrinsic role of dissipation, forgetting, and regularization for adaptive systems.

Method: Proposed the BEDS (Bayesian Emergent Dissipative Structures) framework based on thermodynamics, information theory, and geometry. Introduced the Conditional Optimality Theorem emphasizing Fisher-Rao regularization as thermodynamically optimal.

Result: Identified Fisher-Rao regularization as the unique optimal strategy for minimal dissipation. Unified existing learning methods under the BEDS framework. Distinguished between crystallizable and maintainable problems in learning.

Conclusion: The study presents learning as maintenance of viable belief states with dissipation constraints, offering insights into overfitting, forgetting, and continual adaptation.

Abstract: We propose a perspective in which learning is an intrinsically dissipative process. Forgetting and regularization are not heuristic add-ons but structural requirements for adaptive systems. Drawing on information theory, thermodynamics, and information geometry, we introduce the BEDS (Bayesian Emergent Dissipative Structures) framework, modeling learning as the evolution of compressed belief states under dissipation constraints.
  A central contribution is the Conditional Optimality Theorem, showing that Fisher-Rao regularization measuring change via information divergence rather than Euclidean distance is the unique thermodynamically optimal regularization strategy, achieving minimal dissipation. Euclidean regularization is shown to be structurally suboptimal. The framework unifies existing methods (Ridge, SIGReg, EMA, SAC) as special cases of a single governing equation.
  Within this view, overfitting corresponds to over-crystallization, while catastrophic forgetting reflects insufficient dissipation control. The framework distinguishes BEDS-crystallizable problems, where beliefs converge to stable equilibria, from BEDS-maintainable problems, which require continual adaptation. It extends naturally to continual and multi-agent systems, where viability, stability under adaptation and finite resources replaces asymptotic optimality as the primary criterion. Overall, this work reframes learning as maintaining viable belief states under dissipation constraints, providing a principled lens on forgetting, regularization, and stability.

</details>


### [454] [FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering](https://arxiv.org/abs/2601.17935)
*Daniel Commey,Matilda Nkoom,Yousef Alsenani,Sena G. Hounsinou,Garth V. Crosby*

Main category: cs.LG

TL;DR: FedGraph-VASP introduces a federated graph learning framework allowing VASPs to detect cross-chain money laundering without exposing raw data, using privacy-preserving methods and securing interactions with post-quantum cryptography.


<details>
  <summary>Details</summary>
Motivation: VASPs struggle to balance regulatory compliance with user privacy when detecting cross-institutional money laundering. Existing methods either compromise privacy or miss critical laundering patterns.

Method: FedGraph-VASP uses a Boundary Embedding Exchange protocol and secures data with post-quantum cryptography techniques (Kyber-512 and AES-256-GCM). It enables collaborative anti-money laundering detection while protecting privacy.

Result: FedGraph-VASP outperformed FedSage+ on the Elliptic Bitcoin dataset (F1-score = 0.508 vs 0.453) but showed lower effectiveness in sparse Ethereum datasets. It demonstrated topology-dependent advantages and balanced privacy audits.

Conclusion: FedGraph-VASP highlights key trade-offs in privacy-preserving AML techniques, showing potential in connected graphs but requiring additional strategies for sparse modular setups.

Abstract: Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.

</details>


### [455] [Scaling Effects and Uncertainty Quantification in Neural Actor Critic Algorithms](https://arxiv.org/abs/2601.17954)
*Nikos Georgoudios,Konstantinos Spiliopoulos,Justin Sirignano*

Main category: cs.LG

TL;DR: The paper examines scaling methods in neural Actor Critic algorithms, showing improved statistical performance by adjusting network width scaling parameters.


<details>
  <summary>Details</summary>
Motivation: To address convergence properties and quantify uncertainty in neural Actor Critic methods.

Method: Analyzed general inverse polynomial scaling in network width; derived variance behavior and guidelines for hyperparameter selection.

Result: Variance improves with scaling; tuning scaling exponent optimizes robustness and convergence; numerical experiments confirm findings.

Conclusion: Proper scaling in network width ensures favorable statistical properties and provides a framework for hyperparameter tuning.

Abstract: We investigate the neural Actor Critic algorithm using shallow neural networks for both the Actor and Critic models. The focus of this work is twofold: first, to compare the convergence properties of the network outputs under various scaling schemes as the network width and the number of training steps tend to infinity; and second, to provide precise control of the approximation error associated with each scaling regime. Previous work has shown convergence to ordinary differential equations with random initial conditions under inverse square root scaling in the network width. In this work, we shift the focus from convergence speed alone to a more comprehensive statistical characterization of the algorithm's output, with the goal of quantifying uncertainty in neural Actor Critic methods. Specifically, we study a general inverse polynomial scaling in the network width, with an exponent treated as a tunable hyperparameter taking values strictly between one half and one. We derive an asymptotic expansion of the network outputs, interpreted as statistical estimators, in order to clarify their structure. To leading order, we show that the variance decays as a power of the network width, with an exponent equal to one half minus the scaling parameter, implying improved statistical robustness as the scaling parameter approaches one. Numerical experiments support this behavior and further suggest faster convergence for this choice of scaling. Finally, our analysis yields concrete guidelines for selecting algorithmic hyperparameters, including learning rates and exploration rates, as functions of the network width and the scaling parameter, ensuring provably favorable statistical behavior.

</details>


### [456] [TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors](https://arxiv.org/abs/2601.17958)
*Ido Andrew Atad,Itamar Zimerman,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: The paper introduces TensorLens, a transformative approach for representing the entire transformer model as a high-order, input-dependent linear operator, enabling unified analysis of attention, FFNs, activations, and residuals.


<details>
  <summary>Details</summary>
Motivation: Existing transformer analyses are limited as they primarily focus on individual components like attention heads or layers, failing to account for global model behavior. A unified representation encapsulating all blocks is needed.

Method: TensorLens is proposed, formulating the transformer as a high-order attention-interaction tensor, incorporating all aspects of model computation such as attention, FFNs, normalizations, and residual connections.

Result: Empirical validations demonstrate TensorLens provides richer and more theoretically grounded representations compared to prior methods, paving the way for enhanced interpretability and understanding.

Conclusion: TensorLens enables a more holistic and comprehensive representation of transformer models, offering a theoretical and practical foundation for advancements in model analysis and interpretability.

Abstract: Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.

</details>


### [457] [Federated learning for unpaired multimodal data through a homogeneous transformer model](https://arxiv.org/abs/2601.17986)
*Anders Eklund*

Main category: cs.LG

TL;DR: Training multimodal foundation models in federated settings with disjoint, unpaired data using public anchors and innovative alignment/aggregation methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of centralized training for multimodal models due to privacy concerns, lack of paired data, and decentralized environments.

Method: Proposed a framework using a public anchor set for semantic alignment, centered kernel alignment for privacy-preserving training, subspace-stabilized fine-tuning for large transformers, and precision weighted averaging for aggregating contributions.

Result: Achieves privacy-preserving, federated multimodal model training across decentralized nodes with disjoint data modalities and unpaired samples.

Conclusion: Provides a foundational method for federated learning of multimodal models without centralizing data or requiring aligned samples, ensuring privacy and scalability.

Abstract: Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.

</details>


### [458] [Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization](https://arxiv.org/abs/2601.17987)
*Ziwei Zheng,Huizhi Liang,Vaclav Snasel,Vito Latora,Panos Pardalos,Giuseppe Nicosia,Varun Ojha*

Main category: cs.LG

TL;DR: The paper explores relationships among convergence, pruning, and quantization for deep learning architectures, demonstrating that performance remains largely invariant across different models and datasets, while uncovering insights into learning dynamics and resilience.


<details>
  <summary>Details</summary>
Motivation: To systematically study and optimize deep learning architectures for reliable task-solving under constraints like pruning and quantization, especially in image classification.

Method: The authors implement a computational workflow with architecture design sweeps, evaluating convergence behavior, pruning sensitivity, and quantization robustness across various models and image classification tasks.

Result: Findings reveal invariant model performance, three distinct learning regimes (unstable, learning, overfitting), and insights like deeper architectures' higher resilience to pruning and quantization's stronger impact on harder datasets.

Conclusion: This research offers guidance for selecting compact and stable models that are resilient to pruning and robust under low-precision constraints in image classification tasks.

Abstract: Deep learning networks excel at classification, yet identifying minimal architectures that reliably solve a task remains challenging. We present a computational methodology for systematically exploring and analyzing the relationships among convergence, pruning, and quantization. The workflow first performs a structured design sweep across a large set of architectures, then evaluates convergence behavior, pruning sensitivity, and quantization robustness on representative models. Focusing on well-known image classification of increasing complexity, and across Deep Neural Networks, Convolutional Neural Networks, and Vision Transformers, our initial results show that, despite architectural diversity, performance is largely invariant and learning dynamics consistently exhibit three regimes: unstable, learning, and overfitting. We further characterize the minimal learnable parameters required for stable learning, uncover distinct convergence and pruning phases, and quantify the effect of reduced numeric precision on trainable parameters. Aligning with intuition, the results confirm that deeper architectures are more resilient to pruning than shallower ones, with parameter redundancy as high as 60%, and quantization impacts models with fewer learnable parameters more severely and has a larger effect on harder image datasets. These findings provide actionable guidance for selecting compact, stable models under pruning and low-precision constraints in image classification.

</details>


### [459] [Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning](https://arxiv.org/abs/2601.17995)
*Shudi Weng,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

TL;DR: The paper presents H-SecCoGC, a hierarchical secure aggregation scheme, which enhances model accuracy and privacy preservation in hierarchical federated learning (HFL) under unreliable communication.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge of maintaining model accuracy and preserving privacy in hierarchical federated learning (HFL) amidst unreliable communication and disrupted coordination among privacy noise.

Method: The authors propose H-SecCoGC, a secure aggregation scheme that incorporates coding strategies to enforce structured aggregation, ensure global model accuracy under privacy constraints, and mitigate the partial participation issue.

Result: Theoretical analyses and experimental results validate that H-SecCoGC outperforms existing methods in terms of robustness, privacy preservation, and learning efficiency under unreliable communication.

Conclusion: H-SecCoGC effectively addresses key challenges of unreliable communication in HFL, improving model accuracy, robustness, privacy preservation, and learning efficiency.

Abstract: Hierarchical federated learning (HFL) has emerged as an effective paradigm to enhance link quality between clients and the server. However, ensuring model accuracy while preserving privacy under unreliable communication remains a key challenge in HFL, as the coordination among privacy noise can be randomly disrupted. To address this limitation, we propose a robust hierarchical secure aggregation scheme, termed H-SecCoGC, which integrates coding strategies to enforce structured aggregation. The proposed scheme not only ensures accurate global model construction under varying levels of privacy, but also avoids the partial participation issue, thereby significantly improving robustness, privacy preservation, and learning efficiency. Both theoretical analyses and experimental results demonstrate the superiority of our scheme under unreliable communication across arbitrarily strong privacy guarantees

</details>


### [460] [Spelling Bee Embeddings for Language Modeling](https://arxiv.org/abs/2601.18030)
*Markus N. Rabe,Judith Clymo,Zheren Dong*

Main category: cs.LG

TL;DR: This paper proposes a modification to embedding layers by incorporating spelling information into token embeddings, resulting in improved performance across benchmarks and reduced compute and data requirements.


<details>
  <summary>Details</summary>
Motivation: To enhance model performance by integrating spelling information into token embeddings, addressing limitations in current embedding layers.

Method: Modify the embedding layer to embed tokens with their spelling information and conduct scaling studies on models with varying sizes to measure effectiveness.

Result: Models with the proposed embedding layer showed improvements in spelling and benchmark tasks, and required 8% less compute and data to achieve comparable test loss.

Conclusion: Embedding spelling information in token embeddings enhances model efficiency and performance.

Abstract: We introduce a simple modification to the embedding layer. The key change is to infuse token embeddings with information about their spelling. Models trained with these embeddings improve not only on spelling, but also across standard benchmarks. We conduct scaling studies for models with 40M to 800M parameters, which suggest that the improvements are equivalent to needing about 8% less compute and data to achieve the same test loss.

</details>


### [461] [Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity](https://arxiv.org/abs/2601.18032)
*Brijesh FNU,Viet Thanh Duy Nguyen,Ashima Sharma,Md Harun Rashid Molla,Chengyi Xu,Truong-Son Hy*

Main category: cs.LG

TL;DR: The paper focuses on developing a multimodal learning framework to address data scarcity in high-performance dielectric elastomers by using pretrained embeddings based on polymer corpora.


<details>
  <summary>Details</summary>
Motivation: Modern electronics demand soft and stretchable high-performing dielectric elastomers for human- and robot-interface applications, but current elastomers lack the balance between high dielectric constants and low Young's moduli.

Method: The paper curates a structured dataset of acrylate-based dielectric elastomers and employs pretrained multimodal models using graph- and sequence-based encoders for accurate predictions of key elastomer properties.

Result: The proposed framework enables accurate few-shot predictions of dielectric and mechanical properties, overcoming data scarcity, and demonstrates transferability to other polymer backbones.

Conclusion: This approach represents a paradigm shift in data-efficient discovery for soft high-k dielectric elastomers and provides a publicly accessible dataset and code for further advancements.

Abstract: Dielectric materials are critical building blocks for modern electronics such as sensors, actuators, and transistors. With the rapid recent advance in soft and stretchable electronics for emerging human- and robot-interfacing applications, there is a surging need for high-performance dielectric elastomers. However, it remains a grand challenge to develop soft elastomers that simultaneously possess high dielectric constants (k, related to energy storage capacity) and low Young's moduli (E, related to mechanical flexibility). While some new elastomer designs have been reported in individual (mostly one-off) studies, almost no structured dataset is currently available for dielectric elastomers that systematically encompasses their molecular sequence, dielectric, and mechanical properties. Within this context, we curate a compact, high-quality dataset of acrylate-based dielectric elastomers, one of the most widely explored elastomer backbones due to its versatile chemistry and molecular design flexibility, by screening and aggregating experimental results from the literature over the past 10 years. Building on this dataset, we propose a multimodal learning framework that leverages large-scale pretrained polymer representations from graph- and sequence-based encoders. These pretrained embeddings transfer rich chemical and structural knowledge from vast polymer corpora, enabling accurate few-shot prediction of both dielectric and mechanical properties from molecular sequences. Our results represent a new paradigm for transferring knowledge from pretrained multimodal models to overcome severe data scarcity, which can be readily translated to other polymer backbones (e.g., silicones, urethanes) and thus accelerate data-efficient discovery of soft high-k dielectric elastomers. Our source code and dataset are publicly available at https://github.com/HySonLab/Polymers

</details>


### [462] [Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming](https://arxiv.org/abs/2601.18076)
*Alexandra Chouldechova,A. Feder Cooper,Solon Barocas,Abhinav Palia,Dan Vann,Hanna Wallach*

Main category: cs.LG

TL;DR: The paper argues that attack success rate (ASR) comparisons, often used in AI red teaming, may not provide reliable insights due to issues like apples-to-oranges comparisons and measurement validity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to determine the conditions under which ASR can be meaningfully used to make reliable conclusions regarding AI system safety or attack methods.

Method: The paper uses a combination of social science measurement theory and inferential statistics, as well as conceptual, theoretical, and empirical frameworks, to analyze the efficacy of ASR-based comparisons. Jailbreaking is used as a key example.

Result: The authors identify problems with ASR comparisons, providing evidence for when such comparisons are invalid or misleading.

Conclusion: ASR comparisons are not always meaningful; reliable methods to evaluate these rates are necessary to accurately assess system and attack performance.

Abstract: We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.

</details>


### [463] [DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal](https://arxiv.org/abs/2601.18081)
*Peixuan Han,Yingjie Yu,Jingjun Xu,Jiaxuan You*

Main category: cs.LG

TL;DR: DRPG, a framework designed for automatic academic rebuttal generation, addresses challenges in academic communication and peer review by leveraging four distinct steps and achieving high accuracy and robust results.


<details>
  <summary>Details</summary>
Motivation: Provide automated, high-quality support for academic rebuttal generation and overcome limitations of existing off-the-shelf LLMs or pipelines in handling long-context understanding and producing persuasive responses.

Method: DRPG operates through four structured steps: (1) decomposing reviews into atomic concerns, (2) retrieving paper evidence, (3) planning rebuttal strategies, and (4) generating targeted responses with a Planner achieving over 98% accuracy.

Result: DRPG outperformed existing rebuttal pipelines and surpassed average human-level performance using an 8B model. Results highlight its planner's effectiveness in generating explainable and multi-perspective suggestions in complex settings.

Conclusion: DRPG significantly advances automated academic rebuttal generation, demonstrating its potential for high-quality, scalable academic communication and robust performance.

Abstract: Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.

</details>


### [464] [LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts](https://arxiv.org/abs/2601.18089)
*Venmugil Elango,Nidhi Bhatia,Roger Waleffe,Rasoul Shafipour,Tomer Asida,Abhinav Khattar,Nave Assaf,Maximilian Golub,Joey Guman,Tiyasa Mitra,Ritchie Zhao,Ritika Borkar,Ran Zilberstein,Mostofa Patwary,Mohammad Shoeybi,Bita Rouhani*

Main category: cs.LG

TL;DR: LatentMoE improves inference efficiency and accuracy per computational unit compared to typical MoE architectures.


<details>
  <summary>Details</summary>
Motivation: To evaluate and optimize MoE architectures to achieve better accuracy with reduced computational costs for diverse deployment needs.

Method: The study combines empirical and theoretical analysis, systematic design exploration, and large-scale training of LatentMoE architecture.

Result: LatentMoE consistently outperformed standard architectures in metrics like accuracy per FLOP and parameter, and has been adopted in large-scale models like Nemotron-3.

Conclusion: LatentMoE presents an optimized approach to MoE, addressing hardware-software challenges and improving performance in inference tasks.

Abstract: Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).

</details>


### [465] [From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models](https://arxiv.org/abs/2601.18091)
*Longwei Ding,Anhao Zhao,Fanghua Ye,Ziyang Chen,Xiaoyu Shen*

Main category: cs.LG

TL;DR: This paper explores pruning strategies for instruction-following and reasoning-augmented language models, finding that pruning methods should align with task type and model characteristics.


<details>
  <summary>Details</summary>
Motivation: Motivated by the cost of deploying large language models (LLMs), this paper aims to understand if pruning methods used for instruction-following LLMs are effective for reasoning-augmented LLMs.

Method: The study evaluates static depth, width, and dynamic pruning across 17 tasks, aligning calibration and recovery data with each model’s training distribution for stable pruning comparisons.

Result: Findings reveal differences: depth pruning is better for classification tasks, width pruning is more effective for generation/reasoning, and reasoning models prefer static pruning for preserving reasoning capabilities.

Conclusion: Pruning strategies need to be tailored to reasoning-augmented LLMs, as these models have distinct requirements for task performance and pruning impact.

Abstract: Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\textbf{LLM-instruct}$) and reasoning-augmented ($\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.

</details>


### [466] [AttenMIA: LLM Membership Inference Attack through Attention Signals](https://arxiv.org/abs/2601.18110)
*Pedram Zaree,Md Abdullah Al Mamun,Yue Dong,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: The paper introduces AttenMIA, a membership inference attack (MIA) framework leveraging self-attention patterns in transformer-based large language models (LLMs) to improve privacy attack accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the privacy and intellectual property concerns caused by LLMs' tendency to memorize training data, with specific focus on membership inference attacks which seek to identify whether a specific sample was part of the training dataset.

Method: The proposed AttenMIA method utilizes self-attention patterns inside the transformer models and combines them with perturbation-based divergence metrics to create a classifier for inferring membership. It examines attention head information across layers to identify dataset members.

Result: Attention-based features in AttenMIA outperform traditional MIA methods, achieving significant success in low false-positive metrics on benchmarks like WikiMIA-32. Attention signals were demonstrated to generalize well across datasets and models, and integration into data extraction frameworks also enhanced state-of-the-art training data extraction attacks.

Conclusion: Attention mechanisms in transformers unintentionally increase privacy risks, as demonstrated by AttenMIA's success in exploiting these signals for membership inference. These findings stress the urgency of developing better defenses to enhance privacy protection in LLMs.

Abstract: Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.

</details>


### [467] [Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting](https://arxiv.org/abs/2601.18111)
*Jean Kossaifi,Nikola Kovachki,Morteza Mardani,Daniel Leibovici,Suman Ravuri,Ira Shokar,Edoardo Calvello,Mohammad Shoaib Abbas,Peter Harrington,Ashay Subramaniam,Noah Brenowitz,Boris Bonev,Wonmin Byeon,Karsten Kreis,Dale Durran,Arash Vahdat,Mike Pritchard,Jan Kautz*

Main category: cs.LG

TL;DR: This paper proposes a scalable framework for learning atmospheric dynamics, achieving state-of-the-art probabilistic weather forecasting without requiring specialized architecture or training methods.


<details>
  <summary>Details</summary>
Motivation: The complexity and custom nature of current data-driven weather forecasting methods obscure the underlying factors that contribute to forecast accuracy.

Method: The authors introduce a framework combining a downsampled latent space with a history-conditioned local projector for high-resolution physics, compatible with multiple probabilistic estimators.

Result: The proposed framework outperforms existing techniques, such as the Integrated Forecasting System and GenCast, with improvements across most variables.

Conclusion: A general-purpose, scalable model can achieve state-of-the-art medium-range weather prediction, reducing the need for specialized strategies and fostering versatility in probabilistic frameworks.

Abstract: The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.

</details>


### [468] [Robust Learning of a Group DRO Neuron](https://arxiv.org/abs/2601.18115)
*Guyang Cao,Shuyao Li,Sushrut Karmalkar,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: The paper investigates learning a single neuron under squared loss amidst arbitrary label noise and group-level distributional shifts. It proposes a robust optimization framework and an efficient algorithm to address worst-case group distribution weights.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning robust neuron parameters under uncertain or heterogeneous group distributions, label noise, and distributional shifts.

Method: The authors present a Group Distributionally Robust Optimization framework using an objective that accounts for worst-case group weight combinations and optional divergence penalties. They developed a primal-dual algorithm for efficient computation, leveraging the dual extrapolation update.

Result: The algorithm guarantees constant-factor competitiveness with optimal neuron parameters under worst-case group weighting. Practical implementation shows efficacy on pre-training benchmarks in LLMs.

Conclusion: The paper demonstrates robust guarantees in learning neuron parameters amidst group-specific shifts and label noise, addressing nonconvexity challenges and optimizing performance for worst-case scenarios.

Abstract: We study the problem of learning a single neuron under standard squared loss in the presence of arbitrary label noise and group-level distributional shifts, for a broad family of covariate distributions. Our goal is to identify a ''best-fit'' neuron parameterized by $\mathbf{w}_*$ that performs well under the most challenging reweighting of the groups. Specifically, we address a Group Distributionally Robust Optimization problem: given sample access to $K$ distinct distributions $\mathcal p_{[1]},\dots,\mathcal p_{[K]}$, we seek to approximate $\mathbf{w}_*$ that minimizes the worst-case objective over convex combinations of group distributions $\boldsymbolλ \in Δ_K$, where the objective is $\sum_{i \in [K]}λ_{[i]}\,\mathbb E_{(\mathbf x,y)\sim\mathcal p_{[i]}}(σ(\mathbf w\cdot\mathbf x)-y)^2 - νd_f(\boldsymbolλ,\frac{1}{K}\mathbf1)$ and $d_f$ is an $f$-divergence that imposes (optional) penalty on deviations from uniform group weights, scaled by a parameter $ν\geq 0$. We develop a computationally efficient primal-dual algorithm that outputs a vector $\widehat{\mathbf w}$ that is constant-factor competitive with $\mathbf{w}_*$ under the worst-case group weighting. Our analytical framework directly confronts the inherent nonconvexity of the loss function, providing robust learning guarantees in the face of arbitrary label corruptions and group-specific distributional shifts. The implementation of the dual extrapolation update motivated by our algorithmic framework shows promise on LLM pre-training benchmarks.

</details>


### [469] [Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods](https://arxiv.org/abs/2601.18142)
*Mingxu Zhang,Huicheng Zhang,Jiaming Ji,Yaodong Yang,Ying Sun*

Main category: cs.LG

TL;DR: The paper introduces ADRC-Lagrangian methods to enhance safety in reinforcement learning by reducing oscillations and safety violations compared to classical approaches.


<details>
  <summary>Details</summary>
Motivation: Existing safe reinforcement learning methods, including PID and classical Lagrangian, suffer from oscillations and frequent safety violations due to parameter sensitivity and phase lag.

Method: The paper proposes ADRC-Lagrangian methods, leveraging Active Disturbance Rejection Control for robustness and reduced oscillations, within a unified framework that includes classical and PID methods as special cases.

Result: Experiments show significant improvements: a 74% reduction in safety violations, an 89% reduction in violation magnitudes, and a 67% reduction in average costs.

Conclusion: ADRC-Lagrangian methods demonstrate superior safety performance and effectiveness for reinforcement learning in complex environments, outperforming existing techniques.

Abstract: Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\%, establishing superior effectiveness for Safe RL in complex environments.

</details>


### [470] [FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning](https://arxiv.org/abs/2601.18150)
*Zhaopeng Qiu,Shuang Yu,Jingqi Zhang,Shuai Zhang,Xue Huang,Jingyi Yang,Junjie Lai*

Main category: cs.LG

TL;DR: This paper introduces an efficient FP8 rollout stack for reinforcement learning on LLMs, addressing challenges such as quantization, memory bottlenecks, and train-inference mismatch, achieving a 44% throughput gain while maintaining learning performance.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing inefficiencies in reinforcement learning for large language models caused by rollout bottlenecks, particularly due to memory and compute-intense operations with long output sequence lengths.

Method: The paper develops a practical FP8 rollout stack by enabling W8A8 linear layer rollout with blockwise FP8 quantization, recalibrating QKV scales to handle memory bottlenecks, and implementing importance-sampling-based correction techniques to address train-inference mismatches.

Result: The proposed method achieves up to 44% improvement in rollout throughput while maintaining learning behavior that is comparable to higher-precision BF16 baselines across dense and MoE models.

Conclusion: The work demonstrates the feasibility of accelerating reinforcement learning for LLMs using FP8 precision, showing practical gains in efficiency without compromising on learning stability or model performance.

Abstract: Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.

</details>


### [471] [Learning Fair Domain Adaptation with Virtual Label Distribution](https://arxiv.org/abs/2601.18171)
*Yuguang Zhang,Lijun Sheng,Jian Liang,Ran He*

Main category: cs.LG

TL;DR: This paper addresses the issue of category fairness in unsupervised domain adaptation (UDA) and proposes a method called VILL to improve performance disparities across categories.


<details>
  <summary>Details</summary>
Motivation: To tackle the overlooked issue of category fairness in UDA, where classifiers typically favor easier categories at the expense of harder ones.

Method: The authors propose Virtual Label-distribution-aware Learning (VILL), which uses an adaptive re-weighting strategy to prioritize harder categories and introduces a KL-divergence-based re-balancing strategy for improved decision boundaries.

Result: Experiments show that VILL significantly improves category fairness and can be easily integrated into existing UDA methods.

Conclusion: VILL is an effective framework enhancing worst-case performance and addressing category fairness in UDA, making it applicable as a plug-and-play solution for existing models.

Abstract: Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.

</details>


### [472] [Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients](https://arxiv.org/abs/2601.18189)
*Rui Wu,Yongjun Li*

Main category: cs.LG

TL;DR: Continuous optimization methods for causal discovery often yield dense matrices requiring thresholds to recover DAGs. This paper introduces AHOC and SPG-AHOC, offering finite-time exact DAG recovery without heuristic truncation.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods struggle to bridge continuous optimization and discrete graph structures, leading to arbitrary post-processing for achieving DAGs.

Method: The authors introduce the Hybrid-Order Acyclicity Constraint (AHOC) and optimize it through Smoothed Proximal Gradient (SPG-AHOC), leveraging proximal algorithms and the Manifold Identification Property.

Result: SPG-AHOC theoretically guarantees finite-time recovery of the exact DAG structure and empirically achieves state-of-the-art accuracy.

Conclusion: The proposed approach eliminates the need for post-hoc thresholding, ensuring accurate causal graph recovery in both theory and practice.

Abstract: Continuous optimization has significantly advanced causal discovery, yet existing methods (e.g., NOTEARS) generally guarantee only asymptotic convergence to a stationary point. This often yields dense weighted matrices that require arbitrary post-hoc thresholding to recover a DAG. This gap between continuous optimization and discrete graph structures remains a fundamental challenge. In this paper, we bridge this gap by proposing the Hybrid-Order Acyclicity Constraint (AHOC) and optimizing it via the Smoothed Proximal Gradient (SPG-AHOC). Leveraging the Manifold Identification Property of proximal algorithms, we provide a rigorous theoretical guarantee: the Finite-Time Oracle Property. We prove that under standard identifiability assumptions, SPG-AHOC recovers the exact DAG support (structure) in finite iterations, even when optimizing a smoothed approximation. This result eliminates structural ambiguity, as our algorithm returns graphs with exact zero entries without heuristic truncation. Empirically, SPG-AHOC achieves state-of-the-art accuracy and strongly corroborates the finite-time identification theory.

</details>


### [473] [HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models](https://arxiv.org/abs/2601.18200)
*Chenyu Zhang,Xinchen Lyu,Chenshan Ren,Shuhan Liu,Qimei Cui,Xiaofeng Tao*

Main category: cs.LG

TL;DR: The paper proposes HeterCSI, a novel framework to address challenges in pretraining wireless foundation models for CSI processing by improving generalization and scalability.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current CSI pretraining approaches which fail to generalize across scale and scenario dimensions due to destructive gradient interference and inefficient batching.

Method: It introduces HeterCSI, a framework using scale-aware adaptive batching, a double-masking mechanism, and reformulates heterogeneous batch construction for efficient pretraining.

Result: HeterCSI improves performance in CSI tasks significantly, reducing NMSE by 7.19 dB for reconstruction tasks and reduces training latency by 53% compared to similar models.

Conclusion: The proposed HeterCSI framework effectively balances training efficiency with robust generalization across diverse 6G applications, outperforming existing methods.

Abstract: Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining approaches either constrain inputs to fixed dimensions or isolate training by scale, limiting the generalization and scalability of wireless foundation models. In this paper, we propose HeterCSI, a channel-adaptive pretraining framework that reconciles training efficiency with robust cross-scenario generalization via a new understanding of gradient dynamics in heterogeneous CSI pretraining. Our key insight reveals that CSI scale heterogeneity primarily causes destructive gradient interference, while scenario diversity actually promotes constructive gradient alignment when properly managed. Specifically, we formulate heterogeneous CSI batch construction as a partitioning optimization problem that minimizes zero-padding overhead while preserving scenario diversity. To solve this, we develop a scale-aware adaptive batching strategy that aligns CSI samples of similar scales, and design a double-masking mechanism to isolate valid signals from padding artifacts. Extensive experiments on 12 datasets demonstrate that HeterCSI establishes a generalized foundation model without scenario-specific finetuning, achieving superior average performance over full-shot baselines. Compared to the state-of-the-art zero-shot benchmark WiFo, it reduces NMSE by 7.19 dB, 4.08 dB, and 5.27 dB for CSI reconstruction, time-domain, and frequency-domain prediction, respectively. The proposed HeterCSI framework also reduces training latency by 53% compared to existing approaches while improving generalization performance by 1.53 dB on average.

</details>


### [474] [PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR](https://arxiv.org/abs/2601.18207)
*James Burgess,Jan N. Hansen,Duo Peng,Yuhui Zhang,Alejandro Lozano,Min Woo Sun,Emma Lundberg,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: The paper introduces a method to train language model-based search agents for answering technical questions using a large biomedical paper corpus and a challenging factoid QA dataset, PaperSearchQA.


<details>
  <summary>Details</summary>
Motivation: Existing search agents trained via reinforcement learning have primarily been applied to general-domain QA, which limits their utility in technical domains such as science, engineering, and medicine.

Method: The authors propose a new framework where agents are trained to search and reason over a corpus of 16 million biomedical paper abstracts using RLVR techniques. They also present a factoid QA dataset called PaperSearchQA with 60,000 samples, benchmarks, and utilize the Search-R1 codebase.

Result: The trained search agents outperform non-RL retrieval baselines in answering technical questions and exhibit advanced behaviors such as reasoning, planning, and self-verification.

Conclusion: The developed methods and resources demonstrate the potential of RLVR search agents in scientific domains. The scalability of the approach allows its application to other scientific fields, paving the way toward advanced AI Scientist systems.

Abstract: Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.

</details>


### [475] [Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting](https://arxiv.org/abs/2601.18231)
*Trong Khiem Tran,Manh Cuong Dao,Phi Le Nguyen,Thao Nguyen Truong,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: The paper addresses how to adapt pre-trained models to unseen feature modalities, focusing on balancing feature alignment and target fitting for generalization.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to adapt models to new modalities for cross-disciplinary integration, requiring alignment of new features with relevant pre-trained model spaces.

Method: The authors propose a theoretical framework with a provable generalization bound that connects feature alignment and target fitting through the concept of feature-label distortion.

Result: The method outperforms state-of-the-art approaches across various benchmark datasets, highlighting its effectiveness.

Conclusion: The work provides actionable insights into optimizing the interaction between feature alignment and target fitting, enhancing generalization and practical algorithm design.

Abstract: Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.

</details>


### [476] [Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity](https://arxiv.org/abs/2601.18245)
*Santanu Das,Jatin Batra*

Main category: cs.LG

TL;DR: The paper introduces polynomial-time algorithms for robust phase retrieval with heavy-tailed noise and adversarial corruption of measurements and sensing vectors, significantly improving on previous methods.


<details>
  <summary>Details</summary>
Motivation: Phase retrieval is crucial for numerous applications such as optics and astrophysics. Robustness in such algorithms is vital due to noisy and corrupted measurements.

Method: This paper connects robust spectral initialization techniques with recent advances in robust PCA to create efficient algorithms for phase retrieval.

Result: The presented algorithms achieve robust phase retrieval under adversarial conditions with a near-linear (in n) sample complexity, contrasting the earlier exponential-time approaches.

Conclusion: The work establishes the feasibility of polynomial-time robust phase retrieval with significant improvements in sample complexity, impacting diverse applications requiring resilience to noisy and adversarial data.

Abstract: Phase retrieval is the classical problem of recovering a signal $x^* \in \mathbb{R}^n$ from its noisy phaseless measurements $y_i = \langle a_i, x^* \rangle^2 + ζ_i$ (where $ζ_i$ denotes noise, and $a_i$ is the sensing vector) for $i \in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.

</details>


### [477] [Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs](https://arxiv.org/abs/2601.18255)
*Fei Meng*

Main category: cs.LG

TL;DR: The paper explores continual learning in Large Language Models (LLMs), identifying challenges with Experience Replay (ER) and proposing Orthogonal Subspace Wake-up (OSW) to balance knowledge retention and new task learning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the key problem of balancing stability and plasticity in continual learning for LLMs, particularly in mitigating catastrophic forgetting and understanding the limitations of Experience Replay (ER).

Method: The method introduces Orthogonal Subspace Wake-up (OSW), which uses a wake-up phase to identify essential parameter subspaces for previous tasks and applies orthogonal updates for new tasks to preserve structural integrity.

Result: The proposed OSW approach successfully maintains performance on fragile, structured tasks like code generation while achieving high plasticity for learning new tasks, outperforming ER.

Conclusion: The study highlights the trade-offs in ER for LLMs and demonstrates that OSW can address these for structured domains, ensuring both retention of prior knowledge and adaptability to new tasks.

Abstract: Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief "wake-up" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded "safety guarantee" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.

</details>


### [478] [FGGM: Fisher-Guided Gradient Masking for Continual Learning](https://arxiv.org/abs/2601.18261)
*Chao-Hong Tan,Qian Chen,Wen Wang,Yukun Ma,Chong Zhang,Chong Deng,Qinglin Zhang,Xiangang Li,Jieping Ye*

Main category: cs.LG

TL;DR: The paper introduces Fisher-Guided Gradient Masking (FGGM) to combat catastrophic forgetting in large language models by using Fisher Information for parameter updates, showing notable improvements on benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in continuous learning of large language models, which hinders their ability to retain previous knowledge while adapting to new tasks.

Method: The proposed method, FGGM, leverages diagonal Fisher Information to determine critical parameters and employs adaptive binary masks to preserve these parameters without needing historical data.

Result: FGGM outperforms existing methods like supervised fine-tuning (SFT) and MIGU, with a 9.6% relative improvement in general capability retention and a 4.4% boost on TRACE tasks. Its effectiveness is further validated on code generation tasks.

Conclusion: FGGM effectively balances stability and plasticity in language models, mitigating catastrophic forgetting and demonstrating superior performance over magnitude-based approaches.

Abstract: Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.

</details>


### [479] [Neural Network Approximation: A View from Polytope Decomposition](https://arxiv.org/abs/2601.18264)
*ZeYu Li,ShiJun Zhang,TieYong Zeng,FengLei Fan*

Main category: cs.LG

TL;DR: The paper proposes a novel method for universal approximation using ReLU networks through polytope decomposition and kernel polynomial techniques, enhancing efficiency and flexibility compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: Existing universal approximation theories use rigid methods like dividing input space into tiny hypercubes, neglecting the local regularity of target functions. A better task-oriented approach is needed to improve efficiency and adaptivity.

Method: The authors develop a kernel polynomial method characterized by Totik-Ditzian-type continuity and polytopical domain decomposition. ReLU networks are used to approximate kernel polynomials within subdomains.

Result: Polytope decomposition improves approximation efficiency near singular points and offers flexibility across cases. The approach is extended to analytic functions for higher approximation rates.

Conclusion: The proposed approach outperforms traditional methods in adaptivity and efficiency, particularly in complex areas of target functions. It extends the scope of universal approximation theory for ReLU networks.

Abstract: Universal approximation theory offers a foundational framework to verify neural network expressiveness, enabling principled utilization in real-world applications. However, most existing theoretical constructions are established by uniformly dividing the input space into tiny hypercubes without considering the local regularity of the target function. In this work, we investigate the universal approximation capabilities of ReLU networks from a view of polytope decomposition, which offers a more realistic and task-oriented approach compared to current methods. To achieve this, we develop an explicit kernel polynomial method to derive an universal approximation of continuous functions, which is characterized not only by the refined Totik-Ditzian-type modulus of continuity, but also by polytopical domain decomposition. Then, a ReLU network is constructed to approximate the kernel polynomial in each subdomain separately. Furthermore, we find that polytope decomposition makes our approximation more efficient and flexible than existing methods in many cases, especially near singular points of the objective function. Lastly, we extend our approach to analytic functions to reach a higher approximation rate.

</details>


### [480] [What Do Learned Models Measure?](https://arxiv.org/abs/2601.18278)
*Indrė Žliobaitė*

Main category: cs.LG

TL;DR: The paper investigates the concept of measurement stability in machine learning models used as measurement tools, revealing limitations of standard evaluation methods and proposing an additional evaluative metric.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of traditional evaluation methods in ensuring reliable and stable measurements when machine learning models are used as measurement tools.

Method: The paper defines the concept of measurement stability, explores its lack of guarantee in current evaluation criteria, and demonstrates this issue through theoretical analysis and a real-world case study.

Result: It finds that models with similar predictive performance can yield systematically different measurement functions, illustrating failures in ensuring stable mappings due to distribution shifts.

Conclusion: The study highlights the need for a new evaluative dimension focused on measurement stability to complement traditional machine learning evaluation frameworks.

Abstract: In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.

</details>


### [481] [TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment](https://arxiv.org/abs/2601.18292)
*Zhewen Tan,Wenhan Yu,Jianfeng Si,Tongxin Liu,Kaiqi Guan,Huiyan Jin,Jiawen Tao,Xiaokun Yuan,Duohe Ma,Xiangzheng Zhang,Tong Yang,Lin Sun*

Main category: cs.LG

TL;DR: The paper introduces TriPlay-RL, a closed-loop framework for safety alignment of large language models, achieving significant improvements in adversarial, safety, and evaluative tasks without extensive manual annotations.


<details>
  <summary>Details</summary>
Motivation: To address the pressing concern of large language models generating toxic or harmful content and to create an efficient, scalable approach for improving their safety alignment.

Method: A closed-loop reinforcement learning framework (TriPlay-RL) involving iterative collaboration among attacker, defender, and evaluator roles, with minimal manual intervention.

Result: Experimental results indicate a 20%-50% improvement in adversarial attack effectiveness, 10%-30% enhancement in safety defense performance, and improved fine-grained evaluative abilities of the evaluator over iterations.

Conclusion: TriPlay-RL demonstrates a scalable, effective paradigm for improving LLM safety alignment through continuous, co-evolutionary collaboration, setting a benchmark for future innovations.

Abstract: In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.

</details>


### [482] [A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods](https://arxiv.org/abs/2601.18314)
*Lina Felsner,Sevgi G. Kafali,Hannah Eichhorn,Agnes A. J. Leth,Aidas Batvinskas,Andre Datchev,Fabian Klemm,Jan Aulich,Puntika Leepagorn,Ruben Klinger,Daniel Rueckert,Julia A. Schnabel*

Main category: cs.LG

TL;DR: This study details a hackathon for students to replicate results from three key MRI reconstruction papers and evaluates techniques to ensure reproducibility in research.


<details>
  <summary>Details</summary>
Motivation: To enhance scientific reproducibility and educate students, the hackathon was designed to replicate results from three significant MRI reconstruction methodologies.

Method: Participants attempted to replicate findings from papers focused on MoDL, HUMUS-Net, and a dynamic MRI method, while conducting additional experiments and discussing reproducible coding practices.

Result: The study successfully reproduced the outcomes of the target papers and identified essential practices for creating reproducible research code.

Conclusion: The hackathon showcased the potential of structured educational interventions to improve reproducibility and emphasized the importance of best practices in research coding.

Abstract: We report the design, protocol, and outcomes of a student reproducibility hackathon focused on replicating the results of three influential MRI reconstruction papers: (a) MoDL, an unrolled model-based network with learned denoising; (b) HUMUS-Net, a hybrid unrolled multiscale CNN+Transformer architecture; and (c) an untrained, physics-regularized dynamic MRI method that uses a quantitative MR model for early stopping. We describe the setup of the hackathon and present reproduction outcomes alongside additional experiments, and we detail fundamental practices for building reproducible codebases.

</details>


### [483] [Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals](https://arxiv.org/abs/2601.18326)
*Jie Li,Jing Li,Lu Lv,Zhanyu Ju,Fengkui Gong*

Main category: cs.LG

TL;DR: The paper proposes a multi-modal out-of-distribution detection algorithm for drone signals using Zadoff-Chu sequences and time-frequency images. It achieves significant performance improvements in remote identification tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the identification and classification of drone signals by overcoming limitations posed by unknown or non-standard communication protocols.

Method: Combining Zadoff-Chu sequences and time-frequency images into a multi-modal approach using feature extraction, interaction, and fusion methods with adaptive attention weights.

Result: The algorithm surpasses existing methods, improving remote identification metrics by 1.7% and out-of-distribution detection metrics by 7.5%, while showing robustness under diverse conditions.

Conclusion: The proposed multi-modal algorithm effectively integrates information for improved drone signal detection and classification under various operational conditions.

Abstract: We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.

</details>


### [484] [Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection](https://arxiv.org/abs/2601.18329)
*Chuhan Feng,Jing Li,Jie Li,Lu Lv,Fengkui Gong*

Main category: cs.LG

TL;DR: A novel algorithm for detecting drone signal out-of-distribution (OOD) using spatial-channel selection and gradient norm sensitivity.


<details>
  <summary>Details</summary>
Motivation: To improve detection of OOD samples in drone signals by enhancing discriminative power in time-frequency analysis.

Method: The algorithm adaptively weights image features spatially and across channels using inter-class metrics and introduces a gradient-norm metric fused with energy-based scores for joint inference.

Result: The algorithm demonstrates superior performance and robustness across signal-to-noise ratios (SNR) and different drone types in simulations.

Conclusion: The approach effectively captures OOD characteristics, providing robust and discriminative performance in drone signal analysis.

Abstract: We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.

</details>


### [485] [Structural Gender Bias in Credit Scoring: Proxy Leakage](https://arxiv.org/abs/2601.18342)
*Navya SD,Sreekanth D,SS Uma Sankari*

Main category: cs.LG

TL;DR: The paper audits the Taiwan Credit Default dataset, revealing persistent structural gender bias despite fairness interventions and the removal of explicit gender indicators.


<details>
  <summary>Details</summary>
Motivation: To address the persistent algorithmic bias in credit risk assessment, particularly focusing on structural gender bias in machine learning models.

Method: The study employs SHAP (SHapley Additive exPlanations) for interpretability and an adversarial inverse modeling framework to mathematically quantify implicit bias.

Result: It finds that despite fairness measures, non-sensitive features like Marital Status, Age, and Credit Limit still act as proxies for gender. Gender can be reconstructed with an ROC AUC score of 0.65 using non-sensitive features.

Conclusion: Traditional fairness audits are inadequate for addressing implicit bias in financial AI; the field should shift toward causal-aware modeling and structural accountability.

Abstract: As financial institutions increasingly adopt machine learning for credit risk assessment, the persistence of algorithmic bias remains a critical barrier to equitable financial inclusion. This study provides a comprehensive audit of structural gender bias within the Taiwan Credit Default dataset, specifically challenging the prevailing doctrine of "fairness through blindness." Despite the removal of explicit protected attributes and the application of industry standard fairness interventions, our results demonstrate that gendered predictive signals remain deeply embedded within non-sensitive features. Utilizing SHAP (SHapley Additive exPlanations), we identify that variables such as Marital Status, Age, and Credit Limit function as potent proxies for gender, allowing models to maintain discriminatory pathways while appearing statistically fair. To mathematically quantify this leakage, we employ an adversarial inverse modeling framework. Our findings reveal that the protected gender attribute can be reconstructed from purely non-sensitive financial features with an ROC AUC score of 0.65, demonstrating that traditional fairness audits are insufficient for detecting implicit structural bias. These results advocate for a shift from surface-level statistical parity toward causal-aware modeling and structural accountability in financial AI.

</details>


### [486] [Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning](https://arxiv.org/abs/2601.18356)
*Weiqin Yang,Haowen Xue,Qingyi Peng,Hexuan Hu,Qian Huang,Tingbo Zhang*

Main category: cs.LG

TL;DR: This paper addresses limitations of current medical vision-language models, introducing a framework that incorporates causal principles and retrieval mechanisms to improve clinical accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Current medical vision-language models often rely on statistical correlations, which makes them fragile, prone to hallucinations, and biased, limiting their utility in reliable clinical decision-making.

Method: The proposed method combines causal inference with multimodal retrieval, conditioning decisions on causal evidence (e.g., counterfactuals, external sources) instead of correlations, using causal graphs and relevant exemplars.

Result: The framework improves factual accuracy, robustness to data shifts, and interpretability when applied to radiology report generation, diagnosis prediction, and visual question answering.

Conclusion: Integrating causal retrieval with multimodal reasoning provides a scalable and trustworthy approach to enhance medical vision-language models for clinical use.

Abstract: Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.

</details>


### [487] [Estimating Dense-Packed Zone Height in Liquid-Liquid Separation: A Physics-Informed Neural Network Approach](https://arxiv.org/abs/2601.18399)
*Mehmet Velioglu,Song Zhai,Alexander Mitsos,Adel Mhamdi,Andreas Jupke,Manuel Dahmen*

Main category: cs.LG

TL;DR: The paper introduces a physics-informed neural network (PINN) to estimate dense-packed zone heights in gravity settlers using volume flow data, avoiding expensive and impractical direct measurements.


<details>
  <summary>Details</summary>
Motivation: To address challenges in measuring dense-packed zone heights in gravity settlers, which are critical in industrial settings, and to develop a cost-effective, efficient, and accurate estimation method.

Method: The approach involves pretraining a physics-informed neural network (PINN) on synthetic data and low-fidelity physics equations, finetuning it with experimental data, and using it for phase height estimation in a predictive model integrated into an Extended Kalman Filter framework.

Result: The two-stage trained PINN approach outperforms both mechanistic models and purely data-driven neural networks in accurately estimating phase heights while accounting for uncertainties.

Conclusion: The proposed PINN-based method proves to be a superior, efficient, and cost-effective solution for phase height estimation in gravity settlers, offering accurate predictions using minimal experimental data.

Abstract: Separating liquid-liquid dispersions in gravity settlers is critical in chemical, pharmaceutical, and recycling processes. The dense-packed zone height is an important performance and safety indicator but it is often expensive and impractical to measure due to optical limitations. We propose to estimate phase heights using only inexpensive volume flow measurements. To this end, a physics-informed neural network (PINN) is first pretrained on synthetic data and physics equations derived from a low-fidelity (approximate) mechanistic model to reduce the need for extensive experimental data. While the mechanistic model is used to generate synthetic training data, only volume balance equations are used in the PINN, since the integration of submodels describing droplet coalescence and sedimentation into the PINN would be computationally prohibitive. The pretrained PINN is then fine-tuned with scarce experimental data to capture the actual dynamics of the separator. We then employ the differentiable PINN as a predictive model in an Extended Kalman Filter inspired state estimation framework, enabling the phase heights to be tracked and updated from flow-rate measurements. We first test the two-stage trained PINN by forward simulation from a known initial state against the mechanistic model and a non-pretrained PINN. We then evaluate phase height estimation performance with the filter, comparing the two-stage trained PINN with a two-stage trained purely data-driven neural network. All model types are trained and evaluated using ensembles to account for model parameter uncertainty. In all evaluations, the two-stage trained PINN yields the most accurate phase-height estimates.

</details>


### [488] [Superlinear Multi-Step Attention](https://arxiv.org/abs/2601.18401)
*Yufeng Huang*

Main category: cs.LG

TL;DR: The paper introduces a scalable attention mechanism, 'Superlinear attention,' which improves efficiency for long sequences while maintaining random context access.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of high computational cost in standard attention mechanisms for processing long sequences without excluding any token positions structurally.

Method: A multi-step attention mechanism reformulating standard causal self-attention as a multi-step search with subquadratic complexity. Specifically, they employ a two-step implementation with span-search and restricted span-attention.

Result: Achieved decoding throughput of 114 tokens/sec for a 1M context length and 80 tokens/sec at 10M context lengths using a modified 30B hybrid MoE model. Initial validation on the NIAH task up to 256K context length demonstrated that routed span selection is trainable end-to-end.

Conclusion: The proposed Superlinear attention shows promise in achieving subquadratic complexity and scalability for long contexts, although broader quality evaluations across tasks are saved for further research.

Abstract: In this paper, we propose \textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \textbf{random context access} (a.k.a.\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.

</details>


### [489] [Frequency-Based Hyperparameter Selection in Games](https://arxiv.org/abs/2601.18409)
*Aniket Sanyal,Baraah A. M. Sidahmed,Rebekka Burkholz,Tatjana Chavdarova*

Main category: cs.LG

TL;DR: The paper introduces Modal LookAhead (MoLA), an adaptive hyperparameter selection method in smooth games, improving convergence and training efficiency through frequency-based analysis.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter tuning in smooth games is challenging due to rotational dynamics, invalidating classical tuning strategies. Effective methods for tuning in these scenarios are not well-explored.

Method: The authors analyze oscillatory dynamics using continuous-time trajectories and spectral analysis, and propose Modal LookAhead (MoLA), which adaptively selects hyperparameters based on frequency metrics.

Result: MoLA is shown to accelerate training in purely rotational and mixed regimes while offering convergence guarantees and minimal computational overhead.

Conclusion: Modal LookAhead demonstrates that frequency-based hyperparameter adaptation can enhance learning efficiency in smooth games, providing a principled solution to a significant tuning challenge.

Abstract: Learning in smooth games fundamentally differs from standard minimization due to rotational dynamics, which invalidate classical hyperparameter tuning strategies. Despite their practical importance, effective methods for tuning in games remain underexplored. A notable example is LookAhead (LA), which achieves strong empirical performance but introduces additional parameters that critically influence performance. We propose a principled approach to hyperparameter selection in games by leveraging frequency estimation of oscillatory dynamics. Specifically, we analyze oscillations both in continuous-time trajectories and through the spectrum of the discrete dynamics in the associated frequency-based space. Building on this analysis, we introduce \emph{Modal LookAhead (MoLA)}, an extension of LA that selects the hyperparameters adaptively to a given problem. We provide convergence guarantees and demonstrate in experiments that MoLA accelerates training in both purely rotational games and mixed regimes, all with minimal computational overhead.

</details>


### [490] [Gradient Regularized Natural Gradients](https://arxiv.org/abs/2601.18420)
*Satya Prakash Dash,Hossein Abdi,Wei Pan,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: The paper introduces Gradient-Regularized Natural Gradients (GRNG), a second-order optimization method combining natural gradients with gradient regularization to accelerate optimization and enhance generalizability in large-scale deep learning.


<details>
  <summary>Details</summary>
Motivation: To address the limited attention paid to the impact of gradient regularization on second-order optimizers and to improve both optimization speed and generalizability.

Method: Proposed GRNG framework includes a frequentist variant avoiding Fisher Information Matrix inversion with structured approximations and a Bayesian variant leveraging Regularized-Kalman formulation without FIM inversion, along with convergence guarantees.

Result: Empirical evidence shows GRNG outperforms first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia) on vision and language benchmarks, achieving better optimization speed and generalization.

Conclusion: Gradient regularization enhances the stability and robustness of natural gradient methods, making them effective for large-scale deep learning tasks.

Abstract: Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.

</details>


### [491] [GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level](https://arxiv.org/abs/2601.18447)
*Jinlong Hu,Jiacheng Liu*

Main category: cs.LG

TL;DR: This paper introduces GCFX, a generative model-level counterfactual explanation approach for deep graph learning models, combining advanced architecture and summarization algorithms to enhance understanding and trust in model predictions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparency in deep graph learning models, making their decisions easier to understand and trust by developing effective explanation techniques.

Method: The paper proposes GCFX, which uses deep graph generation with dual encoders, structure-aware taggers, Message Passing Neural Network decoders, and a global summarization algorithm to produce representative global counterfactual explanations.

Result: GCFX outperforms existing methods by achieving higher counterfactual validity and coverage while maintaining low explanation costs on synthetic and real-world datasets.

Conclusion: GCFX enhances the interpretability of deep graph learning models, improves their transparency, and fosters trust by providing comprehensive and practical global counterfactual explanations.

Abstract: Deep graph learning models have demonstrated remarkable capabilities in processing graph-structured data and have been widely applied across various fields. However, their complex internal architectures and lack of transparency make it difficult to explain their decisions, resulting in opaque models that users find hard to understand and trust. In this paper, we explore model-level explanation techniques for deep graph learning models, aiming to provide users with a comprehensive understanding of the models' overall decision-making processes and underlying mechanisms. Specifically, we address the problem of counterfactual explanations for deep graph learning models by introducing a generative model-level counterfactual explanation approach called GCFX, which is based on deep graph generation. This approach generates a set of high-quality counterfactual explanations that reflect the model's global predictive behavior by leveraging an enhanced deep graph generation framework and a global summarization algorithm. GCFX features an architecture that combines dual encoders, structure-aware taggers, and Message Passing Neural Network decoders, enabling it to accurately learn the true latent distribution of input data and generate high-quality, closely related counterfactual examples. Subsequently, a global counterfactual summarization algorithm selects the most representative and comprehensive explanations from numerous candidate counterfactuals, providing broad insights into the model's global predictive patterns. Experiments on a synthetic dataset and several real-world datasets demonstrate that GCFX outperforms existing methods in terms of counterfactual validity and coverage while maintaining low explanation costs, thereby offering crucial support for enhancing the practicality and trustworthiness of global counterfactual explanations.

</details>


### [492] [Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States](https://arxiv.org/abs/2601.18479)
*Kyoleen Kwak,Hyoseok Hwang*

Main category: cs.LG

TL;DR: The paper introduces a method called ASAP to reduce action oscillations in reinforcement learning using transition-induced similar states, improving control smoothness and performance.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning often suffers from high-frequency action oscillations, making it challenging to apply these methods in real-world scenarios.

Method: The authors define transition-induced similar states based on next-state distributions and propose the ASAP method, which aligns actions with transitions and penalizes high-frequency oscillations.

Result: Experiments in Gymnasium and Isaac-Lab demonstrate that the ASAP approach achieves smoother control and better policy performance compared to prior methods.

Conclusion: ASAP effectively mitigates action oscillations by leveraging system dynamics through transition-induced similar states, offering a practical reinforcement learning improvement.

Abstract: Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.

</details>


### [493] [Nearly Optimal Bayesian Inference for Structural Missingness](https://arxiv.org/abs/2601.18500)
*Chen Liang,Donghua Yang,Yutong Wang,Tianle Zhang,Shenghe Zhou,Zhiyu Liang,Hengtong Zhang,Hongzhi Wang,Ziqi Li,Xiyang Zhang,Zheng Liang,Yifei Li*

Main category: cs.LG

TL;DR: The paper addresses challenges in structural missingness in data, highlighting issues with traditional imputation methods and proposes a Bayesian solution that enables accurate predictions while maintaining uncertainty, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from problems with structural missingness in data, such as causal loops, MNAR scenarios, biased decisions from imputation methods, and the need for a more robust method to handle missingness effectively.

Method: The paper uses a Bayesian framework to decouple the learning of a missing-value posterior from label prediction, optimizing the predictive posterior distribution for better uncertainty management and prediction accuracy.

Result: The proposed method achieves state-of-the-art performances on 43 classification and 15 imputation benchmarks, with guarantees of near Bayes-optimality under their structural causal model (SCM) prior.

Conclusion: By leveraging the Bayesian approach, this method provides a robust solution to missing data challenges, enabling accurate prediction and proper uncertainty propagation, proving its effectiveness across multiple benchmarks.

Abstract: Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.

</details>


### [494] [Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark](https://arxiv.org/abs/2601.18509)
*Andro Sabashvili*

Main category: cs.LG

TL;DR: This paper reviews and benchmarks methods to adapt conformal prediction for time series data, considering the challenges introduced by temporal dependencies and data exchangeability.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of applying conformal prediction, a distribution-free framework, to time series forecasting, which violates its core assumption of data exchangeability.

Method: The paper critically surveys and benchmarks various algorithmic solutions including relaxing exchangeability assumptions, redefining data units, modeling prediction residual dynamics, and leveraging online learning for adaptation.

Result: The review synthesizes approaches to ensure practical applicability, computational efficiency, and coverage guarantees for real-world time series forecasting.

Conclusion: The paper concludes on the importance of adapting conformal prediction methods for reliable uncertainty quantification in time series forecasting while maintaining efficiency and theoretical rigor.

Abstract: Reliable uncertainty quantification is of critical importance in time series forecasting, yet traditional methods often rely on restrictive distributional assumptions. Conformal prediction (CP) has emerged as a promising distribution-free framework for generating prediction intervals with rigorous theoretical guarantees. However, applying CP to sequential data presents a primary challenge: the temporal dependencies inherent in time series fundamentally violate the core assumption of data exchangeability, upon which standard CP guarantees are built. This review critically examines the main categories of algorithmic solutions designed to address this conflict. We survey and benchmark methods that relax the exchangeability assumption, those that redefine the data unit to be a collection of independent time series, approaches that explicitly model the dynamics of the prediction residuals, and online learning algorithms that adapt to distribution shifts to maintain long-run coverage. By synthesizing these approaches, we highlight computational efficiency and practical performance on real-world data.

</details>


### [495] [Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates](https://arxiv.org/abs/2601.18510)
*Yibo Li,Zijie Lin,Ailin Deng,Xuan Zhang,Yufei He,Shuo Ji,Tri Cao,Bryan Hooi*

Main category: cs.LG

TL;DR: JitRL is a training-free framework for dynamic policy optimization in Large Language Models (LLMs), eliminating gradient updates and achieving state-of-the-art performance at significantly reduced costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of enabling continual adaptation in LLM agents post-deployment while avoiding high computational costs and catastrophic forgetting of RL methods.

Method: Developing JitRL, which utilizes a dynamic memory of experiences for real-time trajectory retrieval, estimating action advantages, and modulating LLM logits without requiring training or gradient updates.

Result: JitRL achieves state-of-the-art results on WebArena and Jericho benchmarks, surpassing computationally expensive fine-tuning methods like WebRL and reducing costs by over 30 times.

Conclusion: JitRL demonstrates an innovative, scalable approach for test-time policy optimization in LLMs, paving the way for more cost-effective and efficient continual learning solutions.

Abstract: While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.

</details>


### [496] [LipNeXt: Scaling up Lipschitz-based Certified Robustness to Billion-parameter Models](https://arxiv.org/abs/2601.18513)
*Kai Hu,Haoqi Hu,Matt Fredrikson*

Main category: cs.LG

TL;DR: LipNeXt introduces a constraint-free and convolution-free architecture, achieving state-of-the-art certified robustness and scalability in Lipschitz-based certification.


<details>
  <summary>Details</summary>
Motivation: To address limitations in scalability, training efficiency, and performance of Lipschitz-based certification methods, particularly on ImageNet.

Method: Utilizing manifold optimization on the orthogonal manifold and a Spatial Shift Module, the architecture combines orthogonal projections and spatial shifts with 1-Lipschitz nonlinearity and $L_2$ pooling.

Result: LipNeXt achieves state-of-the-art results in clean and certified robust accuracy on CIFAR-10/100, Tiny-ImageNet, and ImageNet, scaling to large models and improving robustness accuracy by up to +8% at ε=1.

Conclusion: Lipschitz-based certifications can leverage modern scaling techniques to achieve efficient, deterministic, and robust performance without constraints or loss of precision.

Abstract: Lipschitz-based certification offers efficient, deterministic robustness guarantees but has struggled to scale in model size, training efficiency, and ImageNet performance. We introduce \emph{LipNeXt}, the first \emph{constraint-free} and \emph{convolution-free} 1-Lipschitz architecture for certified robustness. LipNeXt is built using two techniques: (1) a manifold optimization procedure that updates parameters directly on the orthogonal manifold and (2) a \emph{Spatial Shift Module} to model spatial pattern without convolutions. The full network uses orthogonal projections, spatial shifts, a simple 1-Lipschitz $β$-Abs nonlinearity, and $L_2$ spatial pooling to maintain tight Lipschitz control while enabling expressive feature mixing. Across CIFAR-10/100 and Tiny-ImageNet, LipNeXt achieves state-of-the-art clean and certified robust accuracy (CRA), and on ImageNet it scales to 1-2B large models, improving CRA over prior Lipschitz models (e.g., up to $+8\%$ at $\varepsilon{=}1$) while retaining efficient, stable low-precision training. These results demonstrate that Lipschitz-based certification can benefit from modern scaling trends without sacrificing determinism or efficiency.

</details>


### [497] [Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning](https://arxiv.org/abs/2601.18521)
*Emna Boudabbous,Mohamed Karaa,Lokman Sboui,Julio Montecinos,Omar Alam*

Main category: cs.LG

TL;DR: The paper proposes a scalable, real-time city-scale urban bus delay prediction pipeline that combines multi-resolution features and deep learning, proving both effective and efficient.


<details>
  <summary>Details</summary>
Motivation: Urban bus systems need reliable delay predictions for enhancing passenger information, reducing waiting time, and supporting operational adjustments. Existing systems often lack scalability and generalization.

Method: A prediction pipeline introduced integrates multi-resolution feature engineering, dimensionality reduction, deep learning, and clustering for efficient, city-scale bus delay predictions.

Result: A global LSTM model with cluster-aware features emerged as optimal, outperforming transformers on the STM network data, achieving higher accuracy with significantly reduced parameters.

Conclusion: This pipeline is accurate, scalable, and suitable for real-time deployment across city-scale transit networks, and can be adapted for other networks efficiently.

Abstract: Urban bus transit agencies need reliable, network-wide delay predictions to provide accurate arrival information to passengers and support real-time operational control. Accurate predictions help passengers plan their trips, reduce waiting time, and allow operations staff to adjust headways, dispatch extra vehicles, and manage disruptions. Although real-time feeds such as GTFS-Realtime (GTFS-RT) are now widely available, most existing delay prediction systems handle only a few routes, depend on hand-crafted features, and offer little guidance on how to design a scalable, reusable architecture.
  We present a city-scale prediction pipeline that combines multi-resolution feature engineering, dimensionality reduction, and deep learning. The framework generates 1,683 spatiotemporal features by exploring 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, and compresses them into 83 components using Adaptive PCA while preserving 95% of the variance. To avoid the "giant cluster" problem that occurs when dense urban areas fall into a single H3 region, we introduce a hybrid H3+topology clustering method that yields 12 balanced route clusters (coefficient of variation 0.608) and enables efficient distributed training.
  We compare five model architectures on six months of bus operations from the Société de transport de Montréal (STM) network in Montréal. A global LSTM with cluster-aware features achieves the best trade-off between accuracy and efficiency, outperforming transformer models by 18 to 52% while using 275 times fewer parameters. We also report multi-level evaluation at the elementary segment, segment, and trip level with walk-forward validation and latency analysis, showing that the proposed pipeline is suitable for real-time, city-scale deployment and can be reused for other networks with limited adaptation.

</details>


### [498] [From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale](https://arxiv.org/abs/2601.18524)
*Yongqi Jin,Yecheng Wang,Jun-jie Wang,Rong Zhu,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: The paper introduces a semi-supervised machine learning framework to predict NMR chemical shifts using large-scale, unlabeled literature data combined with minimal labeled datasets, significantly improving prediction accuracy and robustness compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current NMR prediction methods require extensive labeled atom-assigned datasets, which are limited and time-consuming to compile. The aim is to leverage large-scale, weakly structured literature data to overcome these challenges.

Method: A semi-supervised framework integrates limited labeled data with millions of literature-derived, unassigned spectra. By treating chemical shift prediction as a permutation-invariant set supervision problem, the authors utilize a novel sorting-based loss function for stable training.

Result: The new models show superior accuracy and generalization on larger molecular datasets compared to existing methods. For the first time, systematic solvent effects were accounted for at scale in NMR shift predictions.

Conclusion: The study demonstrates that unlabeled literature-mined spectra can effectively train NMR chemical shift models. This suggests the potential of literature-derived weakly structured data in advancing data-centric AI for scientific applications.

Abstract: Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.

</details>


### [499] [Closing the Modality Gap Aligns Group-Wise Semantics](https://arxiv.org/abs/2601.18525)
*Eleonora Grassucci,Giordano Cicchetti,Emanuele Frasca,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: The paper introduces a method to address the modality gap in multimodal learning, showing that closing this gap enhances group-level tasks like clustering, while providing limited improvements for instance-wise tasks.


<details>
  <summary>Details</summary>
Motivation: Address the structural mismatch (modality gap) in latent spaces of multimodal learning which affects group-level tasks such as clustering.

Method: Introduce a novel approach to reduce the modality gap between different modalities, applicable to two-modal and general $n$-modal settings.

Result: The method demonstrates limited impact on instance-wise tasks like retrieval but significantly improves performance for group-level tasks.

Conclusion: The modality gap is crucial for tasks requiring semantic grouping, and addressing it can reshape understanding of multimodal learning spaces.

Abstract: In multimodal learning, CLIP has been recognized as the \textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.

</details>


### [500] [Information Hidden in Gradients of Regression with Target Noise](https://arxiv.org/abs/2601.18546)
*Arash Jamshidi,Katsiaryna Haitsiukevich,Kai Puolamäki*

Main category: cs.LG

TL;DR: The paper presents a method to approximate the Hessian from gradient information by introducing variance calibration using Gaussian noise, enabling effective second-order information recovery for applications like optimization and risk evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the problem of using incomplete gradient information to infer critical second-order information (e.g., Hessian) for optimization, diagnostics, and robustness, especially in modern machine learning settings where only gradients are available.

Method: A variance calibration approach is introduced, where Gaussian noise is injected to scale the target noise variance to match the batch size. This ensures the empirical gradient covariance approximates the Hessian. The method includes non-asymptotic operator-norm guarantees under sub-Gaussian inputs.

Result: The method reliably recovers the Hessian (or data covariance Σ) from just gradient information and proves robust under certain variance conditions ($\mathcal{O}(n)$). Theoretical guarantees and experimental results validate its accuracy and utility.

Conclusion: Accurate recovery of second-order information using gradient-only approaches is achievable via the proposed variance-calibrated method. This paves the way for enhancing applications like optimization preconditioning, adversarial risk estimation, and scalable gradient-only training systems.

Abstract: Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $Σ$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $Ω(1)$ factor. The proposed method is practical (a "set target-noise variance to $n$" rule) and robust (variance $\mathcal{O}(n)$ suffices to recover $Σ$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.

</details>


### [501] [An Unsupervised Tensor-Based Domain Alignment](https://arxiv.org/abs/2601.18564)
*Chong Hyun Lee,Kibae Lee,Hyun Hee Yim*

Main category: cs.LG

TL;DR: A tensor-based domain alignment algorithm is proposed, utilizing oblique manifold optimization and regularization to enhance tensor alignment and classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the need for improved domain adaptation methods with better optimization flexibility and robust performance.

Method: Utilizes iterative optimization on an oblique manifold with alignment matrices, alongside variance-preserving regularization terms.

Result: Increased domain adaptation conversion speed and significant improvement in classification accuracy.

Conclusion: The proposed method outperforms current state-of-the-art techniques and offers superior results for complex domain adaptation tasks.

Abstract: We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.

</details>


### [502] [K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents](https://arxiv.org/abs/2601.18580)
*Vincenzo De Paola,Mirco Mutti,Riccardo Zamboni,Marcello Restelli*

Main category: cs.LG

TL;DR: The paper introduces K-Myriad, an unsupervised method designed to leverage parallel policies in reinforcement learning to enhance exploration and training.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning often neglects diverse exploration strategies by using identical sampling distributions across multiple workers, limiting the benefits of parallelization.

Method: K-Myriad maximizes collective state entropy via a population of specialized exploration strategies, creating diverse policies that contribute to robust reinforcement learning initialization.

Result: Experimental results show that K-Myriad leads to diverse policy discovery and higher training efficiency in high-dimensional continuous control tasks with large-scale parallelization.

Conclusion: K-Myriad demonstrates its effectiveness in enabling collective exploration and providing a foundation for improved parallelization strategies in reinforcement learning.

Abstract: Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.

</details>


### [503] [Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning](https://arxiv.org/abs/2601.18586)
*Miguel Costa,Arthur Vandervoort,Carolin Schmidt,Morten W. Petersen,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: The paper introduces a decision-support framework using reinforcement learning to develop adaptive strategies for urban transportation in response to climate change impacts, demonstrated in Copenhagen.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges posed by climate change to urban transportation systems, including intensified hazards and deep uncertainty in long-term infrastructure investments.

Method: Coupling an integrated assessment model (IAM) with reinforcement learning (RL) to map climate change impacts, infrastructure disruptions, and societal costs into adaptive investment pathways.

Result: Tested in Copenhagen Municipality, the framework produced coordinated spatial-temporal strategies that improve robustness against climate change impacts compared to conventional methods.

Conclusion: The proposed framework helps design adaptive strategies for urban infrastructure under uncertainty, showcasing potential transferability to other cities and hazards.

Abstract: Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities.

</details>


### [504] [LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation](https://arxiv.org/abs/2601.18604)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: A new framework, LaCoGSEA, facilitates unsupervised pathway enrichment analysis combining deep learning and robust pathway statistics, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Pathway enrichment methods like GSEA are limited in unsupervised settings. Current alternative methods lack explicit gene-pathway modeling and rely on linear relationships. Generic AI interpretation tools fail to sufficiently explain results at the pathway-level in unsupervised transcriptomic analysis.

Method: The paper introduces LaCoGSEA, which integrates autoencoders for non-linear transcriptomic structure capture with a gene-latent correlation metric for differential expression. This generates dense gene rankings without prior labels.

Result: LaCoGSEA improves cancer subtype clustering, recovers biologically meaningful pathways more effectively than linear/XAI methods, and is robust across different protocols and dataset sizes.

Conclusion: LaCoGSEA sets a new standard for pathway enrichment analysis in unsupervised settings with improved clustering, pathway recovery, and experimental robustness.

Abstract: Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.
  Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.
  Availability and implementation: https://github.com/willyzzz/LaCoGSEA

</details>


### [505] [Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem](https://arxiv.org/abs/2601.18615)
*Ramiro Valdes Jara,Adam Meyers*

Main category: cs.LG

TL;DR: The study introduces a data-driven diffusion model to tackle the inverse problem in electrocardiographic imaging, showing better probabilistic reconstructions over deterministic methods.


<details>
  <summary>Details</summary>
Motivation: To address the non-unique, underdetermined nature of the ECGI inverse problem and eliminate the need for patient-specific mesh construction.

Method: A conditional diffusion framework learns probabilistic mappings from noisy signals to heart surface potentials while enabling geometry-free and data-driven reconstruction.

Result: The model outperforms deterministic baselines like CNNs, LSTMs, and transformers in reconstruction accuracy on a real ECGI dataset.

Conclusion: Diffusion models show promise as a geometry-free, data-driven tool for robust cardiac electrophysiology imaging with improved accuracy and probabilistic capabilities.

Abstract: This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.

</details>


### [506] [CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling](https://arxiv.org/abs/2601.18620)
*Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Ian Berlot-Attwell,Stéphane Aroca-Ouellette,Kaheer Suleman*

Main category: cs.LG

TL;DR: CASSANDRA is a neurosymbolic tool leveraging LLMs for effective world modeling and planning in complex domains, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Current planning in real-world businesses requires models capable of grasping rich semantics, causal relationships, and handling limited data effectively.

Method: CASSANDRA employs an LLM knowledge prior, combining LLM-synthesized code for deterministic modeling and LLM-guided learning for stochastic variables using probabilistic graphical structures.

Result: Experimental evaluations on coffee-shop and theme park simulators show improved transition predictions and planning performance compared to baselines.

Conclusion: CASSANDRA successfully integrates LLMs for improved modeling and planning efficiency in real-world domains.

Abstract: Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.

</details>


### [507] [Physics-Informed Uncertainty Enables Reliable AI-driven Design](https://arxiv.org/abs/2601.18638)
*Tingkai Xue,Chin Chun Ooi,Yang Jiang,Luu Trung Pham Duong,Pao-Hsiung Chiu,Weijiang Zhao,Nagarajan Raghavan,My Ha Dao*

Main category: cs.LG

TL;DR: The paper introduces a novel approach for inverse design in frequency-selective surfaces using physics-informed uncertainty to improve optimization success rates and reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: Current surrogate-assisted optimization methods often neglect uncertainty quantification, leading to poor performance when data is sparse.

Method: A new paradigm using Physics-Informed Uncertainty was introduced for predictive uncertainty, integrated into a multi-fidelity optimization workflow targeting frequency-selective surface design within the 20-30 GHz range.

Result: Success rate of finding performant designs increased from less than 10% to over 50%, while computational costs were cut by an order of magnitude compared to traditional methods.

Conclusion: Physics-informed uncertainty is shown to be an effective alternative for uncertainty quantification in machine-learning-driven inverse design, paving the way for autonomous systems in scientific discovery.

Abstract: Inverse design is a central goal in much of science and engineering, including frequency-selective surfaces (FSS) that are critical to microelectronics for telecommunications and optical metamaterials. Traditional surrogate-assisted optimization methods using deep learning can accelerate the design process but do not usually incorporate uncertainty quantification, leading to poorer optimization performance due to erroneous predictions in data-sparse regions. Here, we introduce and validate a fundamentally different paradigm of Physics-Informed Uncertainty, where the degree to which a model's prediction violates fundamental physical laws serves as a computationally-cheap and effective proxy for predictive uncertainty. By integrating physics-informed uncertainty into a multi-fidelity uncertainty-aware optimization workflow to design complex frequency-selective surfaces within the 20 - 30 GHz range, we increase the success rate of finding performant solutions from less than 10% to over 50%, while simultaneously reducing computational cost by an order of magnitude compared to the sole use of a high-fidelity solver. These results highlight the necessity of incorporating uncertainty quantification in machine-learning-driven inverse design for high-dimensional problems, and establish physics-informed uncertainty as a viable alternative to quantifying uncertainty in surrogate models for physical systems, thereby setting the stage for autonomous scientific discovery systems that can efficiently and robustly explore and evaluate candidate designs.

</details>


### [508] [TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning](https://arxiv.org/abs/2601.18640)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: The study introduces TwinPurify, a self-supervised framework for extracting meaningful tumor-specific transcriptomic signals from bulk RNA data using adjacent-normal profiles as background guidance.


<details>
  <summary>Details</summary>
Motivation: Most patient cohort studies rely on bulk transcriptomic data which suffers from variations in tumor purity, hindering downstream discoveries in tumor ecosystems.

Method: TwinPurify employs a representation learning approach based on the Barlow Twins, using internal cohort adjacent-normal profiles to disentangle tumor-specific signals without relying on external references.

Result: TwinPurify shows superior performance compared to standard methods such as auto-encoders in extracting tumor and immune signals, improving subtype classification, survival modeling, and identifying biologically relevant pathways.

Conclusion: The framework provides a reliable method for enhancing bulk transcriptomic analysis, thereby unlocking greater potential for molecular discovery in existing clinical datasets.

Abstract: Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.
  Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as "background" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.
  Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.

</details>


### [509] [FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning](https://arxiv.org/abs/2601.18650)
*Liheng Yu,Zhe Zhao,Yuxuan Wang,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: This paper introduces FaLW, a dynamic loss reweighting method for machine unlearning in long-tailed data scenarios, addressing key challenges of heterogeneous and skewed unlearning deviations.


<details>
  <summary>Details</summary>
Motivation: To tackle the overlooked issue in machine unlearning where data to be forgotten often follows a long-tailed distribution, leading to challenges in meeting data privacy demands effectively.

Method: Proposed FaLW, an instance-wise dynamic loss reweighting technique that calibrates unlearning intensity by assessing the predictive probability of each sample against unseen class data, applying a forgetting-aware reweighting mechanism.

Result: FaLW demonstrates superior performance in extensive experiments compared to existing unlearning methods in long-tailed data environments.

Conclusion: FaLW effectively addresses challenges in unlearning long-tailed data, maintaining high performance and advancing privacy-compliant machine learning solutions.

Abstract: Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \textit{Heterogeneous Unlearning Deviation} and \textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \textbf{Supplementary Material}.

</details>


### [510] [A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.18672)
*Spyros Rigas,Thanasis Papaioannou,Panagiotis Trakadas,Georgios Alexandridis*

Main category: cs.LG

TL;DR: This paper introduces a novel framework for improving Kolmogorov-Arnold Networks (KANs) training through curvature-based grid adaptation, achieving significant error reductions across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing adaptation strategies for KANs rely on input data density, neglecting geometric complexity of functions and training metrics.

Method: The authors propose Importance Density Functions (IDFs) for knot allocation governed by training dynamics, paired with curvature-based adaptation.

Result: The curvature-based method reduces errors by 25.3% on synthetic functions, 9.4% on the Feynman dataset, and 23.3% on PDE benchmarks, as statistically verified by Wilcoxon signed-rank tests.

Conclusion: Curvature-based adaptation is a robust and computationally efficient method for enhancing KAN training, outperforming traditional input-based baselines.

Abstract: Kolmogorov-Arnold Networks (KANs) have recently demonstrated promising potential in scientific machine learning, partly due to their capacity for grid adaptation during training. However, existing adaptation strategies rely solely on input data density, failing to account for the geometric complexity of the target function or metrics calculated during network training. In this work, we propose a generalized framework that treats knot allocation as a density estimation task governed by Importance Density Functions (IDFs), allowing training dynamics to determine grid resolution. We introduce a curvature-based adaptation strategy and evaluate it across synthetic function fitting, regression on a subset of the Feynman dataset and different instances of the Helmholtz PDE, demonstrating that it significantly outperforms the standard input-based baseline. Specifically, our method yields average relative error reductions of 25.3% on synthetic functions, 9.4% on the Feynman dataset, and 23.3% on the PDE benchmark. Statistical significance is confirmed via Wilcoxon signed-rank tests, establishing curvature-based adaptation as a robust and computationally efficient alternative for KAN training.

</details>


### [511] [Learning temporal embeddings from electronic health records of chronic kidney disease patients](https://arxiv.org/abs/2601.18675)
*Aditya Kumar,Mario A. Cypko,Oliver Amft*

Main category: cs.LG

TL;DR: This paper evaluates temporal embedding models (LSTM variants) on longitudinal health records and concludes that T-LSTM generates better generalizable representations than others, improving performance in clustering and prediction tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance representation learning in clinical data to improve generalization across tasks without compromising performance. Existing clinical models are task-specific and often lack transparency.

Method: The authors analyzed three LSTM variants (vanilla LSTM, attention-augmented LSTM, and time-aware T-LSTM) using the MIMIC-IV dataset for chronic kidney disease patients. They assessed embedding quality and predictive performance for CKD stage clustering and ICU mortality prediction.

Result: T-LSTM outperformed other architectures, achieving lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage clustering accuracy (0.74). Predictive models using embeddings were more accurate (0.82-0.83) than end-to-end models (0.72-0.75).

Conclusion: Learning temporal embeddings, particularly with T-LSTM models, offers superior generalization and predictive accuracy, highlighting the importance of choosing the right architecture for clinical applications.

Abstract: We investigate whether temporal embedding models trained on longitudinal electronic health records can learn clinically meaningful representations without compromising predictive performance, and how architectural choices affect embedding quality. Model-guided medicine requires representations that capture disease dynamics while remaining transparent and task agnostic, whereas most clinical prediction models are optimised for a single task. Representation learning facilitates learning embeddings that generalise across downstream tasks, and recurrent architectures are well-suited for modelling temporal structure in observational clinical data. Using the MIMIC-IV dataset, we study patients with chronic kidney disease (CKD) and compare three recurrent architectures: a vanilla LSTM, an attention-augmented LSTM, and a time-aware LSTM (T-LSTM). All models are trained both as embedding models and as direct end-to-end predictors. Embedding quality is evaluated via CKD stage clustering and in-ICU mortality prediction. The T-LSTM produces more structured embeddings, achieving a lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage classification accuracy (0.74) than the vanilla LSTM (DBI = 15.85, accuracy = 0.63) and attention-augmented LSTM (DBI = 20.72, accuracy = 0.67). For in-ICU mortality prediction, embedding models consistently outperform end-to-end predictors, improving accuracy from 0.72-0.75 to 0.82-0.83, which indicates that learning embeddings as an intermediate step is more effective than direct end-to-end learning.

</details>


### [512] [Quasi Monte Carlo methods enable extremely low-dimensional deep generative models](https://arxiv.org/abs/2601.18676)
*Miles Martinez,Alex H. Williams*

Main category: cs.LG

TL;DR: This paper introduces quasi-Monte Carlo latent variable models (QLVMs) designed for low-dimensional embeddings and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the need for highly interpretable, low-dimensional embeddings for high-dimensional datasets.

Method: QLVMs approximate marginal likelihood using randomized quasi-Monte Carlo integration rather than learning encoders or variational approximations.

Result: QLVMs outperform VAEs and IWAEs in low-dimensional cases, enabling visualization and tasks like clustering and density estimation.

Conclusion: QLVMs are compute-intensive but suitable for applications requiring interpretability and efficient latent space analysis.

Abstract: This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.

</details>


### [513] [Counterfactual Explanations on Robust Perceptual Geodesics](https://arxiv.org/abs/2601.18678)
*Eslam Zaher,Maciej Trzaskowski,Quan Nguyen,Fred Roosta*

Main category: cs.LG

TL;DR: The paper introduces PCG as a method for generating counterfactual explanations by leveraging perceptually aligned Riemannian metrics, leading to more meaningful and semantically-valid transformations.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanation methods often produce ambiguous or adversarial results due to improper distance metrics and geometries used for optimization. Current solutions typically suffer from semantic drift or off-manifold artifacts, highlighting the need for a geometrically-aligned methodology.

Method: The paper proposes Perceptual Counterfactual Geodesics (PCG), which utilizes a perceptually Riemannian metric derived from robust vision features. This method traces geodesics under this metric to produce meaningful counterfactual examples.

Result: The authors demonstrate the effectiveness of PCG through experiments on three vision datasets, showing better performance compared to baselines and unveiling hidden failure modes of standard metrics.

Conclusion: PCG provides a perceptually-grounded approach for crafting counterfactual explanations, avoiding pitfalls like semantic drift and adversarial shifts, and ensures smooth, meaningful transitions in the latent space.

Abstract: Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.

</details>


### [514] [ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule](https://arxiv.org/abs/2601.18681)
*Yilie Huang,Wenpin Tang,Xunyu Zhou*

Main category: cs.LG

TL;DR: The paper introduces Adaptive Reparameterized Time (ART) and its RL-based extension (ART-RL) to optimize time discretizations in score-based diffusion models. It improves sampling efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in uniform or manually-designed time grids for score-based diffusion models, especially under constraints on the number of time steps.

Method: The method involves time reparameterization (ART) that adjusts time steps non-uniformly. It formulates the problem as a reinforcement learning task (ART-RL) to learn optimal time schedules using actor-critic updates.

Result: Empirical results significantly improve generative quality (e.g., better Fréchet Inception Distance) across datasets like CIFAR-10, AFHQv2, FFHQ, and ImageNet without retraining.

Conclusion: Adaptive time discretization via ART and ART-RL enhances the efficiency and performance of diffusion-based generative models, providing a generalizable approach.

Abstract: We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fréchet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.

</details>


### [515] [Explainability Methods for Hardware Trojan Detection: A Systematic Comparison](https://arxiv.org/abs/2601.18696)
*Paul Whitten,Francis Wolff,Chris Papachristou*

Main category: cs.LG

TL;DR: The paper examines explainability methods for hardware trojan detection, comparing property-based, case-based, and model-agnostic approaches while improving classification quality using XGBoost.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need for accurate and interpretable hardware trojan detection to assist security engineers in validation and decision-making.

Method: Three explainability categories are compared: property-based analysis, case-based reasoning using k-nearest neighbors, and model-agnostic feature attribution methods (LIME, SHAP, gradient). XGBoost is implemented for classification.

Result: Property-based and case-based approaches demonstrate superior interpretability with circuit-level insights; XGBoost achieves significant improvements, including 46.15% precision and reduced false positives (0.25%). Gradient attribution runs faster but lacks detailed context.

Conclusion: Property-based and case-based reasoning are preferable in hardware trojan explainability as they align better with domain-specific requirements, improving interpretability and practical application for ML predictions in security engineering.

Abstract: Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient).
  Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like "high fanin complexity near outputs indicates potential triggers." Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation.
  XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights.
  This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.

</details>


### [516] [Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning](https://arxiv.org/abs/2601.18699)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: The paper investigates catastrophic forgetting in large language models (LLMs) during sequential fine-tuning, exploring its mechanisms and proposing correlations with task similarity and gradient alignment.


<details>
  <summary>Details</summary>
Motivation: Large language models often exhibit catastrophic forgetting when fine-tuned continuously on different tasks, but the underlying mechanisms of this phenomenon remain poorly understood.

Method: The analysis involves systematic experiments on transformer-based LLMs with varying scales (109B–400B parameters) and task sequences to identify causes of forgetting.

Result: Three mechanisms were identified: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. Severity correlates with task similarity (r = 0.87) and gradient alignment metrics. Severe disruption affects 15-23% of attention heads, particularly in lower layers.

Conclusion: The study provides mechanistic insights into catastrophic forgetting in LLMs, enabling development of strategies for better continual learning systems.

Abstract: Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.

</details>


### [517] [Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data](https://arxiv.org/abs/2601.18728)
*Willem Diepeveen,Oscar Leong*

Main category: cs.LG

TL;DR: The paper introduces Riemannian AmbientFlow, a framework to learn generative models and nonlinear data manifolds from noisy data, along with their theoretical and empirical validations.


<details>
  <summary>Details</summary>
Motivation: Many real-world applications have limited access to clean data samples, often encountering noisy or corrupted observations. Additionally, latent structures like data manifolds are crucial for scientific analysis, yet existing generative models provide limited support for extracting these geometries.

Method: The authors extend the AmbientFlow framework by integrating Riemannian geometry through normalizing flows. They employ pullback metrics and Riemannian Autoencoders to directly learn manifolds from corrupted data, ensuring geometric regularization.

Result: The method shows theoretical guarantees for recovering data distributions with minimal error while generating smooth manifold parametrizations. Experimentally, it performs well on both synthetic and real datasets (e.g., MNIST).

Conclusion: Riemannian AmbientFlow provides a robust way to extract underlying data manifolds and can serve as a prior for solving inverse problems, improving analysis and reliability in noisy data scenarios.

Abstract: Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.

</details>


### [518] [Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models](https://arxiv.org/abs/2601.18734)
*Siyan Zhao,Zhihui Xie,Mengchen Liu,Jing Huang,Guan Pang,Feiyu Chen,Aditya Grover*

Main category: cs.LG

TL;DR: The paper introduces On-Policy Self-Distillation (OPSD), where a single LLM acts as both teacher and student by utilizing privileged information for training, achieving better efficiency and performance in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Improving reasoning capabilities of smaller LLMs through effective training methods while addressing the limitations of traditional distillation approaches.

Method: Using OPSD, the model acts as both teacher and student by training from privileged information. The teacher LLM utilizes ground-truth solutions, and the student minimizes token divergence during rollouts.

Result: OPSD achieves significantly better token efficiency (4-8x) compared to reinforcement learning-based methods and outperforms typical off-policy distillation approaches in mathematical reasoning benchmarks.

Conclusion: OPSD is a practical and efficient framework for reasoning enhancement in LLMs, eliminating the need for a separate teacher model while effectively leveraging reasoning datasets.

Abstract: Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.

</details>


### [519] [Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift](https://arxiv.org/abs/2601.18736)
*Jake Lyon,Ehsan Saeedizade,Shamik Sengupta*

Main category: cs.LG

TL;DR: The paper explores lightweight machine learning models for IoT malware detection, revealing that tree-based models are effective but face challenges with evolving threats.


<details>
  <summary>Details</summary>
Motivation: The need to secure IoT devices, which are vulnerable to cyberattacks due to limited resources and varied network environments.

Method: Four supervised learning models (Random Forest, LightGBM, Logistic Regression, Multi-Layer Perceptron) were tested on the IoT-23 dataset for binary and multiclass classification tasks.

Result: Tree-based models showed high accuracy and generalization with limited training data but weakened over time as malware diversity increased.

Conclusion: Adaptive and resource-efficient ML models are crucial for safeguarding IoT systems in dynamic, real-world environments.

Abstract: The rapid expansion of the Internet of Things (IoT) in domains such as smart cities, transportation, and industrial systems has heightened the urgency of addressing their security vulnerabilities. IoT devices often operate under limited computational resources, lack robust physical safeguards, and are deployed in heterogeneous and dynamic networks, making them prime targets for cyberattacks and malware applications. Machine learning (ML) offers a promising approach to automated malware detection and classification, but practical deployment requires models that are both effective and lightweight. The goal of this study is to investigate the effectiveness of four supervised learning models (Random Forest, LightGBM, Logistic Regression, and a Multi-Layer Perceptron) for malware detection and classification using the IoT-23 dataset. We evaluate model performance in both binary and multiclass classification tasks, assess sensitivity to training data volume, and analyze temporal robustness to simulate deployment in evolving threat landscapes. Our results show that tree-based models achieve high accuracy and generalization, even with limited training data, while performance deteriorates over time as malware diversity increases. These findings underscore the importance of adaptive, resource-efficient ML models for securing IoT systems in real-world environments.

</details>


### [520] [Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback](https://arxiv.org/abs/2601.18751)
*Seyed Amir Hosseini,Maryam Abdolali,Amirhosein Tavakkoli,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.LG

TL;DR: The paper introduces TriTrust-PBRL (TTP), a system that can effectively handle adversarial annotators in preference-based reinforcement learning by jointly learning a reward model and trust parameters.


<details>
  <summary>Details</summary>
Motivation: Existing PBRL methods struggle with heterogeneous annotators, especially when faced with adversarial ones who intentionally give incorrect feedback. This research aims to address this gap.

Method: TTP jointly learns a shared reward model and expert-specific trust parameters. During training, the trust parameters classify annotators as trustworthy, noisy, or adversarial, helping the model invert adversarial preferences and recover useful data.

Result: Empirical evaluations show TTP outperforming existing methods in multiple scenarios, maintaining good performance even with adversarial corruption, and effectively distinguishing between different types of annotators.

Conclusion: TTP provides a robust solution for learning from mixed-quality annotators, handling adversarial feedback effectively, and integrating seamlessly with existing PBRL frameworks.

Abstract: Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.

</details>


### [521] [HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs](https://arxiv.org/abs/2601.18753)
*Xinyue Zeng,Junhong Lin,Yujun Yan,Feng Guo,Liang Shi,Jun Wu,Dawei Zhou*

Main category: cs.LG

TL;DR: The paper proposes a unified framework for detecting hallucinations in Large Language Models (LLMs), combining data-driven and reasoning-driven analysis, and introduces HalluGuard, a novel detection method achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs compromise reliability in critical domains like healthcare, law, and scientific discovery. Existing detection methods lack generalization and focus on single sources.

Method: The paper formulates the Hallucination Risk Bound framework to distinguish hallucination risk into data-driven and reasoning-driven categories. It also introduces HalluGuard, which uses NTK-based geometry and representations for unified detection.

Result: HalluGuard is tested on 10 benchmarks, 11 baselines, and 9 LLM models, delivering superior accuracy in detecting diverse LLM hallucinations.

Conclusion: The unified approach and HalluGuard demonstrate effectiveness in mitigating hallucinations, paving the way for reliable LLM applications in high-stakes contexts.

Abstract: The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.

</details>


### [522] [Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values](https://arxiv.org/abs/2601.18760)
*Henry Bell,Lara Neubauer da Costa Schertel,Bochu Ding,Brandon Fain*

Main category: cs.LG

TL;DR: This paper proposes Grounded Constitutional AI (GCAI) to align Large Language Models (LLMs) with a constitution derived from users' preferences and values.


<details>
  <summary>Details</summary>
Motivation: To fairly establish principles that align LLMs to human values, ensuring these principles represent stakeholder input and interaction preferences.

Method: The authors extend the Inverse Constitutional AI framework by incorporating human-provided reasons for preferences and statements of values to create a comprehensive constitution with general and contextual principles.

Result: The generated GCAI constitution is preferred by users over ICAI constitutions, demonstrating improved moral grounding, coherence, and pluralism.

Conclusion: GCAI provides a robust framework to align LLMs to human values by creating a constitution that is widely acceptable and morally grounded.

Abstract: A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.

</details>


### [523] [PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation](https://arxiv.org/abs/2601.18777)
*Abhishek Divekar,Anirban Majumder*

Main category: cs.LG

TL;DR: The paper introduces a framework (PRECISE) that combines minimal human annotations and LLM judgments to estimate sub-instance metrics for search systems, reducing annotation requirements significantly.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation of search and ranking systems needs extensive human annotations, but LLM-based automated judgments face bias issues. There’s a need to balance reliability and efficiency.

Method: They propose the PRECISE framework, which builds on Prediction-Powered Inference (PPI) and integrates minimal human annotations with LLM judgments for sub-instance-level metric estimation efficiently.

Result: PRECISE reduces annotation needs (100 human queries, 10,000 unlabeled examples), lowers computational complexity, and corrects LLM bias. It also improves variance estimation in Precision@K metrics.

Conclusion: The method demonstrates a novel way to use LLMs effectively while mitigating bias, offering significant improvements in evaluation efficiency without sacrificing reliability.

Abstract: Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.

</details>


### [524] [Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability](https://arxiv.org/abs/2601.18778)
*Shobhita Sundaram,John Quan,Ariel Kwiatkowski,Kartik Ahuja,Yann Ollivier,Julia Kempe*

Main category: cs.LG

TL;DR: This paper introduces SOAR, a framework enabling models to overcome learning plateaus by generating automated curriculum through meta-reinforcement learning (meta-RL).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge in reinforcement learning methods that stall when facing datasets with low initial success rates and lack of training signal. The paper aims to explore how pretrained language models can generate their own curriculum to improve performance on unsolved hard problems.

Method: The authors propose SOAR, a self-improvement framework using meta-RL. A teacher model generates synthetic problems for a student model and is rewarded when the student shows improvement on challenging problems. The framework emphasizes rewards based on actual progress rather than intrinsic proxies.

Result: The study demonstrates that (1) bi-level meta-RL can unlock learning with sparse binary rewards through latent model capacity, (2) grounded rewards outperform intrinsic schemes, preventing instability and diversity collapse, and (3) progress relies more on structured and well-posed problems rather than solution correctness.

Conclusion: The ability to generate effective stepping stones does not require solving hard problems initially, suggesting a potential path for breaking reasoning plateaus using pretrained model capacities without curated datasets.

Abstract: Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.

</details>


### [525] [POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration](https://arxiv.org/abs/2601.18779)
*Yuxiao Qu,Amrith Setlur,Virginia Smith,Ruslan Salakhutdinov,Aviral Kumar*

Main category: cs.LG

TL;DR: Existing reinforcement learning methods for large language models struggle on hard reasoning problems due to poor exploration, yielding zero reward and no learning. Proposed Privileged On-Policy Exploration (POPE) leverages oracle solutions for guided exploration, significantly improving performance on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: RL methods for training large language models (LLMs) often fail to solve hard reasoning problems due to insufficient exploration and ineffective optimization.

Method: POPE introduces privileged guidance using oracle solutions as prefixes during exploration, enabling RL to receive rewards on harder tasks and transfer learning back to unguided problems.

Result: POPE increases the solvability of reasoning problems and shows substantial performance improvements on challenging benchmarks.

Conclusion: Privileged On-Policy Exploration offers a promising solution for overcoming exploration challenges in RL training of LLMs, enabling better reasoning abilities in solving difficult problems.

Abstract: Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.

</details>


### [526] [Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic](https://arxiv.org/abs/2601.18783)
*Deepthi Pathare,Leo Laine,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: The paper develops a multi-objective reinforcement learning framework to balance safety, efficiency, and costs for autonomous highway driving in heavy-duty trucks, producing smooth Pareto-optimal policies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing conflicting objectives like safety, energy efficiency, and operational costs in decision-making for heavy-duty vehicles.

Method: The authors propose a Proximal Policy Optimization-based multi-objective reinforcement learning framework that explicitly captures trade-offs among objectives, creating a Pareto frontier of policies.

Result: The approach generates a continuous set of Pareto-optimal policies, which are smooth, interpretable, and allow transitions between policies without retraining.

Conclusion: The framework provides a robust decision-making strategy for autonomous trucking, enabling flexibility and adaptability in handling conflicting objectives efficiently.

Abstract: Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.

</details>


### [527] [Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes](https://arxiv.org/abs/2601.18795)
*Amrith Setlur,Zijian Wang,Andrew Cohen,Paria Rashidinejad,Sang Michael Xie*

Main category: cs.LG

TL;DR: The paper presents PrefixRL, a reinforcement learning (RL) method for large language models that enhances learning efficiency by reusing off-policy traces, achieving faster and better results on hard reasoning problems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve efficiency and overcome learning stalls in RL for large language models, particularly on complex problems where on-policy traces are rare and policy gradients vanish.

Method: PrefixRL reuses old compute in the form of off-policy traces, conditioning on prefixes of successful traces to stabilize optimization and modulate problem difficulty.

Result: PrefixRL reaches the same reward twice as fast and achieves triple the final reward compared to baseline methods. Its benefits transfer to new benchmarks and remain effective even with off-policy traces from different model families.

Conclusion: PrefixRL demonstrates a more sample-efficient, stable, and generalizable approach to RL for complex reasoning tasks, offering significant improvements over traditional methods.

Abstract: Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [528] [Constrained Multi-Objective Genetic Algorithm Variants for Design and Optimization of Tri-Band Microstrip Patch Antenna loaded CSRR for IoT Applications: A Comparative Case Study](https://arxiv.org/abs/2601.17513)
*Moahmed Hamza Boulaich,Said Ohamouddou,Mohammed Ali Ennasar,Abdelatif El Afia*

Main category: cs.NE

TL;DR: This paper proposes an automated multi-band antenna design framework using various multi-objective genetic algorithms and highlights superior performance with a proposed scalarization method.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in the design and optimization of multi-band antennas with conflicting objectives like minimizing return loss and achieving resonance at specific frequency bands.

Method: Five variants of MOGAs were implemented to optimize microstrip patch antennas loaded with CSRRs, utilizing a weighted-sum scalarization approach combined with domain-specific constraints.

Result: The scalarization approach achieved return loss values of -21.56 dB, -16.60 dB, and -27.69 dB, with respective gains of 1.96 dBi, 2.6 dB, and 3.99 dBi at 2.4 GHz, 3.6 GHz, and 5.2 GHz.

Conclusion: The proposed scalarization framework effectively balances multiple objectives simultaneously, providing superior performance compared to traditional multi-objective approaches.

Abstract: This paper presents an automated antenna design and optimization framework employing multi-objective genetic algorithms (MOGAs) to investigate various evolutionary optimization approaches, with a primary emphasis on multi-band frequency optimization. Five MOGA variants were implemented and compared: the Pareto genetic algorithm (PGA), non-dominated sorting genetic algorithm with niching (NSGA-I), non-dominated sorting genetic algorithm with elitism (NSGA-II), non-dominated sorting genetic algorithm using reference points (NSGA-III), and strength Pareto evolutionary algorithm (SPEA). These algorithms are employed to design and optimize microstrip patch antennas loaded with complementary split-ring resonators (CSRRs). A weighted-sum scalarization approach was adopted within a single-objective genetic algorithm framework enhanced with domain-specific constraint handling mechanisms. The optimization addresses the conflicting objectives of minimizing the return loss ($S_{11} < -10$~dB) and achieving multi-band resonance at 2.4~GHz, 3.6~GHz, and 5.2~GHz. The proposed method delivers a superior overall performance by aggregating these objectives into a unified fitness function encompassing $S_{11}$(2.4~GHz), $S_{11}$(3.6~GHz), and $S_{11}$(5.2~GHz). This approach effectively balances all three frequency bands simultaneously, rather than exploring trade-off solutions typical of traditional multi-objective approaches. The antenna was printed on a Rogers RT5880 substrate with a dielectric constant of 2.2 , loss tangent of 0.0009 , and thickness of 1.57~mm . Scalarization approach achieved return loss values of $-21.56$~dB, $-16.60$~dB, and $-27.69$~dB, with corresponding gains of 1.96~dBi, 2.6~dB, and 3.99~dBi at 2.4~GHz, 3.6~GHz, and 5.2~GHz, respectively.

</details>


### [529] [Deep Intrinsic Surprise-Regularized Control (DISRC): A Biologically Inspired Mechanism for Efficient Deep Q-Learning in Sparse Environments](https://arxiv.org/abs/2601.17598)
*Yash Kini,Shiv Davay,Shreya Polavarapu*

Main category: cs.NE

TL;DR: This paper introduces DISRC, an enhancement to DQN in reinforcement learning that dynamically scales learning updates based on surprise measures, improving performance in sparse-reward environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the instability and inefficiency of standard DQN agents caused by fixed learning rates and rigid learning update mechanisms, especially in sparse-reward settings where feedback is infrequent.

Method: The authors propose DISRC, which incorporates a LayerNorm-based encoder to calculate latent-space surprise scores for dynamically scaling Q-updates. Updates are scaled based on both Temporal Difference error and surprise intensity, allowing flexibility during exploration and stability with repeated experiences.

Result: In experiments on sparse-reward MiniGrid environments (DoorKey and LavaCrossing), DISRC demonstrated faster learning, lower reward variation, and higher reward scores compared to the standard DQN baseline.

Conclusion: DISRC shows improvements in efficiency and stability for learning in sparse-reward domains by leveraging surprise as an intrinsic signal, enabling better decision-making where conventional value-based methods struggle.

Abstract: Deep reinforcement learning (DRL) has driven major advances in autonomous control. Still, standard Deep Q-Network (DQN) agents tend to rely on fixed learning rates and uniform update scaling, even as updates are modulated by temporal-difference (TD) error. This rigidity destabilizes convergence, especially in sparse-reward settings where feedback is infrequent. We introduce Deep Intrinsic Surprise-Regularized Control (DISRC), a biologically inspired augmentation to DQN that dynamically scales Q-updates based on latent-space surprise. DISRC encodes states via a LayerNorm-based encoder and computes a deviation-based surprise score relative to a moving latent setpoint. Each update is then scaled in proportion to both TD error and surprise intensity, promoting plasticity during early exploration and stability as familiarity increases. We evaluate DISRC on two sparse-reward MiniGrid environments, which included MiniGrid-DoorKey-8x8 and MiniGrid-LavaCrossingS9N1, under identical settings as a vanilla DQN baseline. In DoorKey, DISRC reached the first successful episode (reward > 0.8) 33% faster than the vanilla DQN baseline (79 vs. 118 episodes), with lower reward standard deviation (0.25 vs. 0.34) and higher reward area under the curve (AUC: 596.42 vs. 534.90). These metrics reflect faster, more consistent learning - critical for sparse, delayed reward settings. In LavaCrossing, DISRC achieved a higher final reward (0.95 vs. 0.93) and the highest AUC of all agents (957.04), though it converged more gradually. These preliminary results establish DISRC as a novel mechanism for regulating learning intensity in off-policy agents, improving both efficiency and stability in sparse-reward domains. By treating surprise as an intrinsic learning signal, DISRC enables agents to modulate updates based on expectation violations, enhancing decision quality when conventional value-based methods fall short.

</details>


### [530] [Motif Diversity in Human Liver ChIP-seq Data Using MAP-Elites](https://arxiv.org/abs/2601.17808)
*Alejandro Medina,Mary Lauren Benton*

Main category: cs.NE

TL;DR: The paper proposes a novel approach to motif discovery using the MAP-Elites algorithm, emphasizing quality and diversity in regulatory sequence analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional motif discovery methods focus on finding a single dominant motif, missing the biological heterogeneity in regulatory sequence data. This paper aims to address this limitation by exploring multiple plausible motif explanations.

Method: The paper applies the MAP-Elites algorithm to evolve motifs based on position weight matrices (PWMs), optimizing likelihood-based fitness while preserving diversity across biologically meaningful dimensions.

Result: MAP-Elites successfully discovers multiple high-quality motif variants, comparable in fitness to the MEME tool, and reveals structured diversity that MEME does not capture.

Conclusion: MAP-Elites enhances motif discovery by recovering biologically diverse and high-quality motifs, addressing limitations of single-solution approaches like MEME.

Abstract: Motif discovery is a core problem in computational biology, traditionally formulated as a likelihood optimization task that returns a single dominant motif from a DNA sequence dataset. However, regulatory sequence data admit multiple plausible motif explanations, reflecting underlying biological heterogeneity. In this work, we frame motif discovery as a quality-diversity problem and apply the MAP-Elites algorithm to evolve position weight matrix motifs under a likelihood-based fitness objective while explicitly preserving diversity across biologically meaningful dimensions. We evaluate MAP-Elites using three complementary behavioral characterizations that capture trade-offs between motif specificity, compositional structure, coverage, and robustness. Experiments on human CTCF liver ChIP-seq data aligned to the human reference genome compare MAP-Elites against a standard motif discovery tool, MEME, under matched evaluation criteria across stratified dataset subsets. Results show that MAP-Elites recovers multiple high-quality motif variants with fitness comparable to MEME's strongest solutions while revealing structured diversity obscured by single-solution approaches.

</details>


### [531] [Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization](https://arxiv.org/abs/2601.17899)
*Junhao Qiu,Xin Chen,Liang Ge,Liyong Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.NE

TL;DR: The paper introduces a framework, E2OC, for improving Multi-Objective Evolutionary Algorithms (MOEAs) by co-evolving operator design strategies and executable codes through sequential decision-making.


<details>
  <summary>Details</summary>
Motivation: The reliance on expert design for neighborhood search operators in MOEAs and the limitations of recent automated heuristic design methods warrant a solution that explicitly explores interactions between multiple operators.

Method: The proposed E2OC framework formulates operator optimization as a Markov decision process, employs Monte Carlo Tree Search for strategy exploration, and integrates an operator rotation mechanism for effective configuration.

Result: Experimental evaluations indicate that E2OC surpasses state-of-the-art AHD methods and other co-design frameworks in terms of generalization and optimization across various tasks.

Conclusion: The E2OC framework demonstrates its ability to effectively optimize interdependent operators in MOEAs, offering a significant advancement in automated heuristic design.

Abstract: Neighborhood search operators are critical to the performance of Multi-Objective Evolutionary Algorithms (MOEAs) and rely heavily on expert design. Although recent LLM-based Automated Heuristic Design (AHD) methods have made notable progress, they primarily optimize individual heuristics or components independently, lacking explicit exploration and exploitation of dynamic coupling relationships between multiple operators. In this paper, multi-operator optimization in MOEAs is formulated as a Markov decision process, enabling the improvement of interdependent operators through sequential decision-making. To address this, we propose the Evolution of Operator Combination (E2OC) framework for MOEAs, which achieves the co-evolution of design strategies and executable codes. E2OC employs Monte Carlo Tree Search to progressively search combinations of operator design strategies and adopts an operator rotation mechanism to identify effective operator configurations while supporting the integration of mainstream AHD methods as the underlying designer. Experimental results across AHD tasks with varying objectives and problem scales show that E2OC consistently outperforms state-of-the-art AHD and other multi-heuristic co-design frameworks, demonstrating strong generalization and sustained optimization capability.

</details>


### [532] [TEFormer: Structured Bidirectional Temporal Enhancement Modeling in Spiking Transformers](https://arxiv.org/abs/2601.18274)
*Sicheng Shen,Mingyang Lv,Bing Han,Dongcheng Zhao,Guobin Shen,Feifei Zhao,Yi Zeng*

Main category: cs.NE

TL;DR: TEFormer, a new Spiking Transformer framework, introduces bidirectional temporal fusion for improved spatiotemporal modeling, outperforming baselines and proving robust across different encoding schemes.


<details>
  <summary>Details</summary>
Motivation: Existing Spiking Transformers lack an effective mechanism for temporal fusion, which limits their ability to fully leverage spatiotemporal dependencies.

Method: TEFormer incorporates a forward temporal fusion mechanism in the attention module and a backward gated recurrent structure in the MLP for bidirectional temporal modeling.

Result: TEFormer outperforms strong SNN and Spiking Transformer baselines in various datasets and maintains consistent performance across diverse neural encoding schemes.

Conclusion: TEFormer is an effective and general framework for temporal modeling, enhancing temporal consistency in Spiking Transformers and delivering reliable accuracy improvements.

Abstract: In recent years, Spiking Neural Networks (SNNs) have achieved remarkable progress, with Spiking Transformers emerging as a promising architecture for energy-efficient sequence modeling. However, existing Spiking Transformers still lack a principled mechanism for effective temporal fusion, limiting their ability to fully exploit spatiotemporal dependencies. Inspired by feedforward-feedback modulation in the human visual pathway, we propose TEFormer, the first Spiking Transformer framework that achieves bidirectional temporal fusion by decoupling temporal modeling across its core components. Specifically, TEFormer employs a lightweight and hyperparameter-free forward temporal fusion mechanism in the attention module, enabling fully parallel computation, while incorporating a backward gated recurrent structure in the MLP to aggregate temporal information in reverse order and reinforce temporal consistency. Extensive experiments across a wide range of benchmarks demonstrate that TEFormer consistently and significantly outperforms strong SNN and Spiking Transformer baselines under diverse datasets. Moreover, through the first systematic evaluation of Spiking Transformers under different neural encoding schemes, we show that the performance gains of TEFormer remain stable across encoding choices, indicating that the improved temporal modeling directly translates into reliable accuracy improvements across varied spiking representations. These results collectively establish TEFormer as an effective and general framework for temporal modeling in Spiking Transformers.

</details>


### [533] [Scaling Behaviors of Evolutionary Algorithms on GPUs: When Does Parallelism Pay Off?](https://arxiv.org/abs/2601.18446)
*Xinmeng Yu,Tao Jiang,Ran Cheng,Yaochu Jin,Kay Chen Tan*

Main category: cs.NE

TL;DR: The paper investigates the nuanced impacts of GPU acceleration on evolutionary algorithms (EAs) beyond raw speedup by conducting a detailed empirical study across multiple conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how GPU parallelism fundamentally affects EAs' behavior and to uncover its under-explored advantages beyond speed improvements.

Method: The authors conducted a comparative study of 16 EAs on 30 benchmark problems across different problem dimensionalities and population sizes, emphasizing fixed-time evaluation to analyze GPU and CPU executions.

Result: GPU parallelism shows heterogeneous impacts based on algorithmic structures. It enables large population sizes, unveils unique convergence dynamics, and demonstrates distinct scaling regimes.

Conclusion: GPU parallelism is essential for enhancing EA evaluation, comparison, and design, rather than being merely a technical detail of implementation.

Abstract: Evolutionary algorithms (EAs) are increasingly implemented on graphics processing units (GPUs) to leverage parallel processing capabilities for enhanced efficiency. However, existing studies largely emphasize the raw speedup obtained by porting individual algorithms from CPUs to GPUs. Consequently, these studies offer limited insight into when and why GPU parallelism fundamentally benefits EAs. To address this gap, we investigate how GPU parallelism alters the behavior of EAs beyond simple acceleration metrics. We conduct a systematic empirical study of 16 representative EAs on 30 benchmark problems. Specifically, we compare CPU and GPU executions across a wide range of problem dimensionalities and population sizes. Our results reveal that the impact of GPU acceleration is highly heterogeneous and depends strongly on algorithmic structure. We further demonstrate that conventional fixed-budget evaluation based on the number of function evaluations (FEs) is inadequate for GPU execution. In contrast, fixed-time evaluation uncovers performance characteristics that are unobservable under small or practically constrained FE budgets, particularly for adaptive and exploration-oriented algorithms. Moreover, we identify distinct scaling regimes in which GPU parallelism is beneficial, saturates, or degrades as problem dimensionality and population size increase. Crucially, we show that large populations enabled by GPUs not only improve hardware utilization but also reveal algorithm-specific convergence and diversity dynamics that are difficult to observe under CPU-constrained settings. Consequently, our findings indicate that GPU parallelism is not strictly an implementation detail, but a pivotal factor that influences how EAs should be evaluated, compared, and designed for modern computing platforms.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [534] [GreenServ: Energy-Efficient Context-Aware Dynamic Routing for Multi-Model LLM Inference](https://arxiv.org/abs/2601.17551)
*Thomas Ziller,Shashikant Ilager,Alessandro Tundo,Ezio Bartocci,Leonardo Mariani,Ivona Brandic*

Main category: cs.PF

TL;DR: This paper introduces GreenServ, a framework to optimize inference accuracy and energy efficiency of Large Language Models (LLMs) by using dynamic routing and adaptive policies.


<details>
  <summary>Details</summary>
Motivation: The need to reduce computational resource demands, especially energy consumption, during LLM inference while maintaining high accuracy.

Method: GreenServ uses lightweight contextual features from queries and employs a multi-armed bandit approach to dynamically route queries to the most efficient and accurate LLM from a heterogeneous pool.

Result: GreenServ improved accuracy by 22% and reduced energy consumption by 31% compared to random routing, with a peak accuracy of 75.7% using RouterBench.

Conclusion: GreenServ effectively enhances the efficiency and practicality of deploying LLMs by balancing accuracy and energy demands, demonstrating significant benefits over static or random inference strategies.

Abstract: Large language models (LLMs) demonstrate remarkable capabilities, but their broad deployment is limited by significant computational resource demands, particularly energy consumption during inference. Static, one-model-fits-all inference strategies are often inefficient, as they do not exploit the diverse range of available models or adapt to varying query requirements.
  This paper presents GreenServ, a dynamic, context-aware routing framework that optimizes the trade-off between inference accuracy and energy efficiency. GreenServ extracts lightweight contextual features from each query, including task type, semantic cluster, and text complexity, and routes queries to the most suitable model from a heterogeneous pool, based on observed accuracy and energy usage. We employ a multi-armed bandit approach to learn adaptive routing policies online. This approach operates under partial feedback, eliminates the need for extensive offline calibration, and streamlines the integration of new models into the inference pipeline.
  We evaluated GreenServ across five benchmark tasks and a pool of 16 contemporary open-access LLMs. Experimental results show that GreenServ consistently outperforms static (single-model) and random baselines. In particular, compared to random routing, GreenServ achieved a 22% increase in accuracy while reducing cumulative energy consumption by 31%. Finally, we evaluated GreenServ with RouterBench, achieving an average accuracy of 71.7% with a peak accuracy of 75.7%. All artifacts are open-source and available as an anonymous repository for review purposes here: https://anonymous.4open.science/r/llm-inference-router-EBEA/README.md

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [535] [Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop](https://arxiv.org/abs/2601.17670)
*Roberto Rossi,Steven D. Prestwich*

Main category: cs.PL

TL;DR: This paper introduces SyntAGM, a system that converts natural language problem descriptions into PyOPL mathematical models using a generative loop involving compilation and assessment.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve generative mathematical programming by automating the translation of natural language problem descriptions into formal models, leveraging compilers for accuracy and efficiency.

Method: The authors use PyOPL's AML compiler and introduce SyntAGM, which employs a generate–compile–assess–revise loop, integrates grammar awareness via PyOPL BNF grammar, and retrieves few-shot exemplars for model synthesis.

Result: SyntAGM achieves competitive accuracy compared to established baselines but with improved efficiency in token usage, cost, and latency.

Conclusion: SyntAGM demonstrates the feasibility and advantage of compiler-guided generative modeling systems in mathematical programming, suggesting potential for wider applications in formal model synthesis.

Abstract: This work investigates generative mathematical programming through the lens of Algebraic Modelling Languages (AMLs) and compiler-guided model synthesis. By leveraging PyOPL, an OPL-like AML compiler that provides detailed syntax diagnostics, we introduce SyntAGM, an end-to-end system that translates natural language problem descriptions into PyOPL models via a generate--compile--assess--revise loop. SyntAGM is grammar-aware thanks to in-context exposure to the PyOPL BNF grammar, and benefits from few-shot retrieval of literate PyOPL model exemplars. To obtain a valid PyOPL model that matches the problem description, SyntAGM mobilises compiler feedback and an LLM-based alignment judge. In a comparative study against established prompting baselines SyntAGM achieves competitive accuracy with superior token, cost, and latency profiles.

</details>


### [536] [Types for Grassroots Logic Programs](https://arxiv.org/abs/2601.17957)
*Ehud Shapiro*

Main category: cs.PL

TL;DR: This paper proposes a type system for Grassroots Logic Programs (GLP) to handle interactive partial computations using regular sets of moded paths.


<details>
  <summary>Details</summary>
Motivation: Programming complex communication modalities in GLP using AI alone proves cumbersome. A well-defined type system can enhance collaboration between humans and AI in these scenarios.

Method: The authors introduce moded paths to represent communication directionality in types. A program is deemed well-typed if its semantics satisfies specific covariance and contravariance conditions.

Result: The GLP type system was implemented in Dart by AI, showcasing the practical application of Typed GLP derived from mathematical specifications.

Conclusion: The proposed type system supports programming interactive modalities in GLP by fostering collaboration between humans and AI through structured design steps.

Abstract: Grassroots Logic Programs (GLP) is a concurrent logic programming language in which logic variables are partitioned into paired readers and writers. An assignment is produced at most once via a writer and consumed at most once via its paired reader, and may contain additional readers and/or writers. This enables the concise expression of rich multidirectional communication modalities.
  ``Logic Programs as Types for Logic Programs'' (LICS'91) defined types as regular sets of paths over derivable ground atoms. Here, we define types to be regular sets of moded paths, where a mode captures directionality of communication -- whether a subterm is consumed from or produced to the environment -- enabling the typing of interactive partial computations including those that eventually deadlock or fail, or never terminate. We provide a syntactic definition of well-typing and prove that a program is well-typed iff the path abstraction of its moded-atom semantics satisfies covariance and contravariance conditions with respect to its type.
  The GLP type system was implemented in Dart by AI, starting from a mathematical specification of Typed GLP (this paper), deriving from it an English spec (written by AI), and from the spec deriving Dart code (by AI). While GLP is naturally untyped, the motivation for Typed GLP comes from programming with AI: Asking AI to program complex communication modalities in GLP (and in general) and hoping for the best is a tenuous strategy. The emerging discipline we advocate and employ is for the human designer and AI to jointly develop and agree upon (1)~GLP types; (2)~GLP procedure type declarations; (3)~informal (English) descriptions of the procedures; and only then let AI attempt to write (4)~GLP code based on those.

</details>


### [537] [Handling Scope Checks (Extended Version)](https://arxiv.org/abs/2601.18793)
*Michael Lee,Ningning Xie,Oleg Kiselyov,Jeremy Yallop*

Main category: cs.PL

TL;DR: The paper explores the challenges in interacting metaprogramming and effect handlers, focusing on scope extrusion issues. It introduces a calculus ($λ_{<<op>>}$) to formalize dynamic checks, proposes a novel solution called the "Cause-for-Concern" check, and compares static and dynamic prevention methods.


<details>
  <summary>Details</summary>
Motivation: Effect handlers and metaprogramming exhibit issues like scope extrusion, an undesirable state causing ill-scoped code for programming language designers.

Method: Introduces a formal calculus ($λ_{<<op>>}$) for dynamic checks, proposes and proves the correctness of the "Cause-for-Concern" check, and extends the framework with refined classifiers for static scope extrusion prevention.

Result: Provides a formal approach to evaluate dynamic checks while demonstrating the correctness and advantages of the "Cause-for-Concern" check. Extends comparison between static and dynamic approaches.

Conclusion: This study advances understanding of scope extrusion by formalizing dynamic checks, presenting a proven and novel solution, and analyzing the interplay between static and dynamic methods for prevention.

Abstract: Metaprogramming and effect handlers interact in unexpected, and sometimes undesirable, ways. One example is scope extrusion: the generation of ill-scoped code. Scope extrusion can either be preemptively prevented, via static type systems, or retroactively detected, via dynamic checks. Static type systems exist in theory, but struggle with a range of implementation and usability problems in practice. In contrast, dynamic checks exist in practice (e.g. in MetaOCaml), but are understudied in theory. Designers of metalanguages are thus given little guidance regarding the design and implementation of checks. We present the first formal study of dynamic scope extrusion checks, introducing a calculus ($λ_{\langle\langle\text{op}\rangle\rangle}$) for describing and evaluating checks. Further, we introduce a novel dynamic check $\unicode{x2014}$ the "Cause-for-Concern" check $\unicode{x2014}$ which we prove correct, characterise without reference to its implementation, and argue combines the advantages of existing dynamic checks. Finally, we extend our framework with refined environment classifiers, which statically prevent scope extrusion, and compare their expressivity with the dynamic checks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [538] [Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics](https://arxiv.org/abs/2601.16985)
*Pierrick Lorang*

Main category: cs.RO

TL;DR: A neuro-symbolic framework is proposed to integrate hierarchical abstractions, planning, and reinforcement learning for rapid adaptation in robotics.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of slow adaptation, sample inefficiency, and catastrophic forgetting in existing autonomous systems when faced with unforeseen novelties in open-world environments.

Method: The paper introduces a neuro-symbolic framework that combines hierarchical abstractions, task and motion planning, symbolic goal-oriented learning, and a world model-based exploration to enhance adaptability.

Result: The approach was validated in robotic manipulation and autonomous driving, showing faster convergence, improved sample efficiency, and higher robustness than current hybrid methods.

Conclusion: The method demonstrates potential for real-world deployment by enhancing adaptability, efficiency, and robustness in robotics dealing with open-world environment changes.

Abstract: Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and world model-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.

</details>


### [539] [Advancing Improvisation in Human-Robot Construction Collaboration: Taxonomy and Research Roadmap](https://arxiv.org/abs/2601.17219)
*David Wireko Atibila,Vineet R. Kamat,Carol C. Menassa*

Main category: cs.RO

TL;DR: The paper explores the importance of human-robot collaboration in construction, proposes a taxonomy for collaboration levels, and analyzes current gaps and recommendations for improvement.


<details>
  <summary>Details</summary>
Motivation: The construction industry faces challenges like stagnant productivity, labor shortages, and safety concerns. Robotic automation could address these issues, but robots currently struggle to adapt to dynamic, unstructured environments.

Method: The researchers propose a six-level taxonomy for human-robot collaboration, developed through a systematic review of 214 articles (2010-2025). They also introduce a radar framework to measure HRC's evolution across key dimensions.

Result: The analysis finds research is mostly focused on lower collaboration levels, with significant gaps in experiential learning and collaborative improvisation techniques. Three major barriers to progress are identified.

Conclusion: Future research should focus on enhancing human-robot communication using technologies like Augmented/Virtual Reality, large language models, and cloud-based knowledge systems to achieve effective collaboration.

Abstract: The construction industry faces productivity stagnation, skilled labor shortages, and safety concerns. While robotic automation offers solutions, construction robots struggle to adapt to unstructured, dynamic sites. Central to this is improvisation, adapting to unexpected situations through creative problem-solving, which remains predominantly human. In construction's unpredictable environments, collaborative human-robot improvisation is essential for workflow continuity. This research develops a six-level taxonomy classifying human-robot collaboration (HRC) based on improvisation capabilities. Through systematic review of 214 articles (2010-2025), we categorize construction robotics across: Manual Work (Level 0), Human-Controlled Execution (Level 1), Adaptive Manipulation (Level 2), Imitation Learning (Level 3), Human-in-Loop BIM Workflow (Level 4), Cloud-Based Knowledge Integration (Level 5), and True Collaborative Improvisation (Level 6). Analysis reveals current research concentrates at lower levels, with critical gaps in experiential learning and limited progression toward collaborative improvisation. A five-dimensional radar framework illustrates progressive evolution of Planning, Cognitive Role, Physical Execution, Learning Capability, and Improvisation, demonstrating how complementary human-robot capabilities create team performance exceeding individual contributions. The research identifies three fundamental barriers: technical limitations in grounding and dialogic reasoning, conceptual gaps between human improvisation and robotics research, and methodological challenges. We recommend future research emphasizing improved human-robot communication via Augmented/Virtual Reality interfaces, large language model integration, and cloud-based knowledge systems to advance toward true collaborative improvisation.

</details>


### [540] [Hierarchical Informative Path Planning via Graph Guidance and Trajectory Optimization](https://arxiv.org/abs/2601.17227)
*Avraiem Iskandar,Shamak Dutta,Kevin Murrant,Yash Vardhan Pant,Stephen L. Smith*

Main category: cs.RO

TL;DR: This paper proposes a hierarchical framework for informative path planning (IPP) with travel budgets in cluttered environments, yielding faster and more efficient uncertainty reduction than existing methods.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to efficiently plan informative paths for agents in cluttered environments under travel budget constraints, addressing the limitations of current graph-based and continuous optimization methods.

Method: A hierarchical framework is developed, involving (i) global graph-based planning, (ii) budget allocation per segment using geometric/kernel bounds, and (iii) spline-based local refinement considering obstacles.

Result: The proposed method demonstrates reduced posterior uncertainty faster than graph-only and continuous methods by up to 9x and 20x against gradient-based and black-box solvers, respectively, in both synthetic and Arctic datasets.

Conclusion: Through a novel combination of global guidance and local refinement, the framework improves computational efficiency and uncertainty reduction for IPP in cluttered environments.

Abstract: We study informative path planning (IPP) with travel budgets in cluttered environments, where an agent collects measurements of a latent field modeled as a Gaussian process (GP) to reduce uncertainty at target locations. Graph-based solvers provide global guarantees but assume pre-selected measurement locations, while continuous trajectory optimization supports path-based sensing but is computationally intensive and sensitive to initialization in obstacle-dense settings. We propose a hierarchical framework with three stages: (i) graph-based global planning, (ii) segment-wise budget allocation using geometric and kernel bounds, and (iii) spline-based refinement of each segment with hard constraints and obstacle pruning. By combining global guidance with local refinement, our method achieves lower posterior uncertainty than graph-only and continuous baselines, while running faster than continuous-space solvers (up to 9x faster than gradient-based methods and 20x faster than black-box optimizers) across synthetic cluttered environments and Arctic datasets.

</details>


### [541] [Real-Time, Energy-Efficient, Sampling-Based Optimal Control via FPGA Acceleration](https://arxiv.org/abs/2601.17231)
*Tanmay Desai,Brian Plancher,R. Iris Bahar*

Main category: cs.RO

TL;DR: This paper presents an FPGA-optimized design for Model Predictive Path Integral Control (MPPI), achieving significant speedup and energy savings compared to GPU and CPU implementations for autonomous mobile robots.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of meeting energy and latency constraints on battery-constrained autonomous mobile robot platforms using embedded compute for robust planning and control.

Method: Develop an FPGA-optimized MPPI algorithm with fine-grained parallelism, deep pipelining, and parallelism across algorithmic stages, reducing synchronization bottlenecks.

Result: The proposed FPGA design provides a 3.1x to 7.5x speedup and 2.5x to 5.4x energy savings compared to embedded GPU and CPU implementations.

Conclusion: FPGA architectures are effective for achieving energy-efficient and high-performance control systems for edge robotics applications.

Abstract: Autonomous mobile robots (AMRs), used for search-and-rescue and remote exploration, require fast and robust planning and control schemes. Sampling-based approaches for Model Predictive Control, especially approaches based on the Model Predictive Path Integral Control (MPPI) algorithm, have recently proven both to be highly effective for such applications and to map naturally to GPUs for hardware acceleration. However, both GPU and CPU implementations of such algorithms can struggle to meet tight energy and latency budgets on battery-constrained AMR platforms that leverage embedded compute. To address this issue, we present an FPGA-optimized MPPI design that exposes fine-grained parallelism and eliminates synchronization bottlenecks via deep pipelining and parallelism across algorithmic stages. This results in an average 3.1x to 7.5x speedup over optimized implementations on an embedded GPU and CPU, respectively, while simultaneously achieving a 2.5x to 5.4x reduction in energy usage. These results demonstrate that FPGA architectures are a promising direction for energy-efficient and high-performance edge robotics.

</details>


### [542] [Quantifying Ergonomics in the Elevate Soft Robotic Suit](https://arxiv.org/abs/2601.17249)
*Peter Bryan,Rejin John Varghese,Dario Farina*

Main category: cs.RO

TL;DR: This paper evaluates the ergonomic and comfort performance of the Elevate soft robotic suit, designed to assist shoulder elevation, using motion-capture and force sensors.


<details>
  <summary>Details</summary>
Motivation: Challenges in creating data-driven, user-specific, and comfort-first soft robotic suits limit their wider adoption for rehabilitation and augmentation purposes.

Method: The authors conducted a quantitative assessment using motion-capture systems, force sensors, and pressure measurements to evaluate the shoulder assistance of the Elevate suit during lifting movements.

Result: The suit demonstrated no user discomfort with cable tensions up to 200N and applied shoulder pressures comparable to a human grasp (69.1-85.1kPa). Volumetric compression was less than 3% (torso) and 8% (upper arm).

Conclusion: Early validation of the Elevate suit proves its ergonomic design, setting the stage for further studies involving patient groups.

Abstract: Soft robotic suits have the potential to rehabilitate, assist, and augment the human body. The low weight, cost, and minimal form-factor of these devices make them ideal for daily use by both healthy and impaired individuals. However, challenges associated with data-driven, user-specific, and comfort-first design of human-robot interfaces using soft materials limit their widespread translation and adoption. In this work, we present the quantitative evaluation of ergonomics and comfort of the Elevate suit - a cable driven soft robotic suit that assists shoulder elevation. Using a motion-capture system and force sensors, we measured the suit's ergonomics during assisted shoulder elevation up to 70 degrees. Two 4-hour sessions were conducted with one subject, involving transmitting cable tensions of up to 200N with no discomfort reported. We estimated that the pressure applied to the shoulder during assisted movements was within the range seen in a human grasp (approximately 69.1-85.1kPa), and estimated volumetric compression of <3% and <8% across the torso and upper arm, respectively. These results provide early validation of Elevate's ergonomic design in preparation for future studies with patient groups.

</details>


### [543] [EMPM: Embodied MPM for Modeling and Simulation of Deformable Objects](https://arxiv.org/abs/2601.17251)
*Yunuo Chen,Yafei Hu,Lingfeng Sun,Tushar Kusnur,Laura Herlant,Chenfanfu Jiang*

Main category: cs.RO

TL;DR: This paper introduces EMPM, a framework for modeling deformable objects using a differentiable MPM simulator, which improves generalizability and robustness while reducing the need for large datasets.


<details>
  <summary>Details</summary>
Motivation: Modeling deformable objects is challenging due to their complex dynamics, limited generalization, and the large training datasets required by existing methods.

Method: EMPM uses a differentiable Material Point Method simulator to build adaptive and physics-aware representations from multi-view RGB-D videos, reconstructing geometry and appearance while optimizing parameters with sensory feedback.

Result: EMPM outperformed spring-mass models in experiments and demonstrated better adaptability and robustness for deformable object simulation.

Conclusion: EMPM advances deformable object modeling by integrating adaptive physics simulations, reducing data requirements, and improving generalizability, particularly for robotic manipulation.

Abstract: Modeling deformable objects - especially continuum materials - in a way that is physically plausible, generalizable, and data-efficient remains challenging across 3D vision, graphics, and robotic manipulation. Many existing methods oversimplify the rich dynamics of deformable objects or require large training sets, which often limits generalization. We introduce embodied MPM (EMPM), a deformable object modeling and simulation framework built on a differentiable Material Point Method (MPM) simulator that captures the dynamics of challenging materials. From multi-view RGB-D videos, our approach reconstructs geometry and appearance, then uses an MPM physics engine to simulate object behavior by minimizing the mismatch between predicted and observed visual data. We further optimize MPM parameters online using sensory feedback, enabling adaptive, robust, and physics-aware object representations that open new possibilities for robotic manipulation of complex deformables. Experiments show that EMPM outperforms spring-mass baseline models. Project website: https://embodied-mpm.github.io.

</details>


### [544] [Real-Time Synchronized Interaction Framework for Emotion-Aware Humanoid Robots](https://arxiv.org/abs/2601.17287)
*Yanrong Chen,Xihan Bian*

Main category: cs.RO

TL;DR: This paper proposes a framework for NAO humanoid robots to synchronize speech prosody with gestures for emotionally synchronized multimodal interactions.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in enabling humanoid robots to achieve emotionally synchronized multimodal interactions for better integration into social and service environments.

Method: The framework includes (1) an emotion engine using large language models for generating text responses and motion descriptors, (2) duration-aware dynamic time warping for aligning speech and gestures temporally, and (3) closed-loop verification ensuring gesture feasibility within NAO's physical limits.

Result: Experiments demonstrate a 21% improvement in emotional alignment compared to rule-based systems, achieved through coordination of vocal pitch and upper-limb kinematics.

Conclusion: This framework enhances context-aware sensorimotor capabilities, advancing the use of social robots in personalized healthcare, interactive education, and responsive customer service.

Abstract: As humanoid robots increasingly introduced into social scene, achieving emotionally synchronized multimodal interaction remains a significant challenges. To facilitate the further adoption and integration of humanoid robots into service roles, we present a real-time framework for NAO robots that synchronizes speech prosody with full-body gestures through three key innovations: (1) A dual-channel emotion engine where large language model (LLM) simultaneously generates context-aware text responses and biomechanically feasible motion descriptors, constrained by a structured joint movement library; (2) Duration-aware dynamic time warping for precise temporal alignment of speech output and kinematic motion keyframes; (3) Closed-loop feasibility verification ensuring gestures adhere to NAO's physical joint limits through real-time adaptation. Evaluations show 21% higher emotional alignment compared to rule-based systems, achieved by coordinating vocal pitch (arousal-driven) with upper-limb kinematics while maintaining lower-body stability. By enabling seamless sensorimotor coordination, this framework advances the deployment of context-aware social robots in dynamic applications such as personalized healthcare, interactive education, and responsive customer service platforms.

</details>


### [545] [Eye-Tracking-Driven Control in Daily Task Assistance for Assistive Robotic Arms](https://arxiv.org/abs/2601.17404)
*Anke Fischer-Janzen,Thomas M. Wendt,Kristof Van Laerhoven*

Main category: cs.RO

TL;DR: The paper presents an eye-tracking-driven shared control framework for allowing individuals with severe disabilities to perform daily tasks autonomously, achieving 97.9% accuracy on object and task selection.


<details>
  <summary>Details</summary>
Motivation: To improve shared control systems in Human-Robot Interaction for enabling individuals with severe physical disabilities to perform daily tasks independently, addressing challenges in 3D gaze estimation and task differentiation.

Method: The framework uses task pictograms as fiducial markers combined with a feature-matching approach and an eye-in-hand configuration for object and task identification, operating without needing prior knowledge of the user's position relative to the object.

Result: The proposed approach demonstrated a high accuracy of up to 97.9% in task and object selection.

Conclusion: This open-source framework is adaptable to various tasks and objects leveraging state-of-the-art object detection models, with lessons from evaluation shared for further improvement.

Abstract: Shared control improves Human-Robot Interaction by reducing the user's workload and increasing the robot's autonomy. It allows robots to perform tasks under the user's supervision. Current eye-tracking-driven approaches face several challenges. These include accuracy issues in 3D gaze estimation and difficulty interpreting gaze when differentiating between multiple tasks. We present an eye-tracking-driven control framework, aimed at enabling individuals with severe physical disabilities to perform daily tasks independently. Our system uses task pictograms as fiducial markers combined with a feature matching approach that transmits data of the selected object to accomplish necessary task related measurements with an eye-in-hand configuration. This eye-tracking control does not require knowledge of the user's position in relation to the object. The framework correctly interpreted object and task selection in up to 97.9% of measurements. Issues were found in the evaluation, that were improved and shared as lessons learned. The open-source framework can be adapted to new tasks and objects due to the integration of state-of-the-art object detection models.

</details>


### [546] [DiffusionCinema: Text-to-Aerial Cinematography](https://arxiv.org/abs/2601.17412)
*Valerii Serpiva,Artem Lykov,Jeffrin Sam,Aleksey Fedoseev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: This paper introduces a UAV system using diffusion models to generate flight paths based on natural language prompts for cinematic video recording.


<details>
  <summary>Details</summary>
Motivation: To simplify UAV operation for cinematic recordings by eliminating manual controls, enabling users to describe desired shots in text form instead.

Method: The proposed system combines initial visual input and natural language prompts, employing diffusion models to generate optimal autonomous UAV flight paths.

Result: Compared to traditional remote controllers, users experienced drastically lower workload, mental demand, and frustration in tests with the system.

Conclusion: The research showcases text-driven UAV flight control, where diffusion models convert user prompts into autonomous motion, offering usability and efficiency benefits.

Abstract: We propose a novel Unmanned Aerial Vehicles (UAV) assisted creative capture system that leverages diffusion models to interpret high-level natural language prompts and automatically generate optimal flight trajectories for cinematic video recording. Instead of manually piloting the drone, the user simply describes the desired shot (e.g., "orbit around me slowly from the right and reveal the background waterfall"). Our system encodes the prompt along with an initial visual snapshot from the onboard camera, and a diffusion model samples plausible spatio-temporal motion plans that satisfy both the scene geometry and shot semantics. The generated flight trajectory is then executed autonomously by the UAV to record smooth, repeatable video clips that match the prompt. User evaluation using NASA-TLX showed a significantly lower overall workload with our interface (M = 21.6) compared to a traditional remote controller (M = 58.1), demonstrating a substantial reduction in perceived effort. Mental demand (M = 11.5 vs. 60.5) and frustration (M = 14.0 vs. 54.5) were also markedly lower for our system, confirming clear usability advantages in autonomous text-driven flight control. This project demonstrates a new interaction paradigm: text-to-cinema flight, where diffusion models act as the "creative operator" converting story intentions directly into aerial motion.

</details>


### [547] [Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.17428)
*Ziming Li,Chenhao Li,Marco Hutter*

Main category: cs.RO

TL;DR: This paper introduces LP-ACRL, a framework that adapts automatically to task difficulty for robotic learning, enabling high-speed, stable locomotion on complex terrains without prior difficulty knowledge.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional curriculum learning in scaling to complex and diverse robotic task spaces that lack a clear difficulty structure.

Method: The LP-ACRL framework dynamically estimates learning progress and adjusts task-sampling distributions online, eliminating the need for predefined difficulty structures.

Result: Policies trained with LP-ACRL enabled the quadruped robot ANYmal D to achieve stable and high-speed locomotion (2.5 m/s linear, 3.0 rad/s angular) across a variety of complex terrains.

Conclusion: LP-ACRL offers a scalable, practical approach for robotic learning in challenging environments, serving as a strong foundation for further research on automated curriculum generation.

Abstract: Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent's learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces.

</details>


### [548] [PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes](https://arxiv.org/abs/2601.17440)
*Xinru Cui,Linxi Feng,Yixuan Zhou,Haoqi Han,Zhe Liu,Hesheng Wang*

Main category: cs.RO

TL;DR: The paper introduces PILOT, a reinforcement learning framework for humanoid robots, combining perceptive locomotion and whole-body control for tasks in unstructured environments.


<details>
  <summary>Details</summary>
Motivation: Current whole-body controllers for humanoid robots lack adequate environmental awareness for stable and complex task execution in diverse and unstructured scenarios.

Method: PILOT employs a single-stage reinforcement learning approach, incorporating a cross-modal context encoder for terrain awareness and a Mixture-of-Experts policy architecture for skill coordination.

Result: Experiments in simulation and on the Unitree G1 humanoid robot showcase PILOT's superior stability, precision, and terrain handling compared to existing controllers.

Conclusion: The framework has potential as a robust low-level controller, advancing perceptive loco-manipulation capabilities in challenging real-world environments.

Abstract: Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured scenarios.To address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes.

</details>


### [549] [EquiForm: Noise-Robust SE(3)-Equivariant Policy Learning from 3D Point Clouds](https://arxiv.org/abs/2601.17486)
*Zhiyuan Zhang,Yu She*

Main category: cs.RO

TL;DR: EquiForm improves robotic manipulation with point clouds by introducing noise-robust and consistent geometric reasoning methods that achieve substantial performance gains under noisy environments.


<details>
  <summary>Details</summary>
Motivation: Point cloud-based visual imitation learning faces challenges due to sensor noise, pose perturbations, and occlusion, disrupting geometry-based processing and robustness.

Method: The framework includes a geometric denoising module to correct distortions and a contrastive equivariant alignment objective to ensure consistent representations under transformations and noise.

Result: EquiForm improved performance by 17.2% in simulation tasks and 28.1% in real-world manipulation tasks compared to state-of-the-art methods.

Conclusion: EquiForm demonstrates effectiveness in noise-robust manipulation tasks and spatial generalization by incorporating enhanced denoising and alignment for point cloud policies.

Abstract: Visual imitation learning with 3D point clouds has advanced robotic manipulation by providing geometry-aware, appearance-invariant observations. However, point cloud-based policies remain highly sensitive to sensor noise, pose perturbations, and occlusion-induced artifacts, which distort geometric structure and break the equivariance assumptions required for robust generalization. Existing equivariant approaches primarily encode symmetry constraints into neural architectures, but do not explicitly correct noise-induced geometric deviations or enforce equivariant consistency in learned representations. We introduce EquiForm, a noise-robust SE(3)-equivariant policy learning framework for point cloud-based manipulation. EquiForm formalizes how noise-induced geometric distortions lead to equivariance deviations in observation-to-action mappings, and introduces a geometric denoising module to restore consistent 3D structure under noisy or incomplete observations. In addition, we propose a contrastive equivariant alignment objective that enforces representation consistency under both rigid transformations and noise perturbations. Built upon these components, EquiForm forms a flexible policy learning pipeline that integrates noise-robust geometric reasoning with modern generative models. We evaluate EquiForm on 16 simulated tasks and 4 real-world manipulation tasks across diverse objects and scene layouts. Compared to state-of-the-art point cloud imitation learning methods, EquiForm achieves an average improvement of 17.2% in simulation and 28.1% in real-world experiments, demonstrating strong noise robustness and spatial generalization.

</details>


### [550] [MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions](https://arxiv.org/abs/2601.17507)
*Yutong Shen,Hangxu Liu,Kailin Pei,Ruizhe Xia,Tongtong Feng*

Main category: cs.RO

TL;DR: MetaWorld overcomes three current limitations in humanoid robot loco-manipulation using a hierarchical world model that integrates semantic planning and physical control.


<details>
  <summary>Details</summary>
Motivation: Current approaches to humanoid robot loco-manipulation struggle with low sample efficiency, poor generalization, and physical inconsistencies, which impede practical and efficient task execution.

Method: MetaWorld employs a hierarchical world model to decouple tasks into semantic planning (via VLMs) and physical control using expert policy transfer and a pre-trained multi-expert policy library, enabling dynamic adaptation in two stages.

Result: MetaWorld demonstrates task completion superiority and improved motion coherence compared to world model-based reinforcement learning approaches, as shown in experiments on Humanoid-Bench.

Conclusion: MetaWorld provides an efficient framework for humanoid robot loco-manipulation by overcoming semantic and physical challenges, with usable code publicly available for further development.

Abstract: Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at https://anonymous.4open.science/r/metaworld-2BF4/

</details>


### [551] [AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation](https://arxiv.org/abs/2601.17550)
*Deepak Singh,Shreyas Khobragade,Nitin J. Sanket*

Main category: cs.RO

TL;DR: The paper introduces an autonomous aerial robot capable of navigating in complete darkness using a novel depth estimation method combining an IR monocular camera, a coded lens, and structured light, achieving a 95.5% success rate in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable autonomous aerial navigation in complete darkness for critical applications like post-disaster search and rescue, where power outages render traditional navigation impossible.

Method: The method integrates an IR monocular camera with a large-aperture coded lens and structured light to extract depth-dependent defocus cues. These cues feed into the AsterNet deep learning model trained in simulation, eliminating the need for external infrastructure for navigation.

Result: The developed system, utilizing the AsterNet model and onboard computing, achieves successful navigation in dark environments, even with complex obstacles like thin ropes, with a success rate of 95.5%.

Conclusion: The paper concludes that this approach is the first monocular structured-light solution for quadrotor navigation in total darkness, demonstrating its practicality, robustness, and high success rate in real-world scenarios.

Abstract: Autonomous aerial navigation in absolute darkness is crucial for post-disaster search and rescue operations, which often occur from disaster-zone power outages. Yet, due to resource constraints, tiny aerial robots, perfectly suited for these operations, are unable to navigate in the darkness to find survivors safely. In this paper, we present an autonomous aerial robot for navigation in the dark by combining an Infra-Red (IR) monocular camera with a large-aperture coded lens and structured light without external infrastructure like GPS or motion-capture. Our approach obtains depth-dependent defocus cues (each structured light point appears as a pattern that is depth dependent), which acts as a strong prior for our AsterNet deep depth estimation model. The model is trained in simulation by generating data using a simple optical model and transfers directly to the real world without any fine-tuning or retraining. AsterNet runs onboard the robot at 20 Hz on an NVIDIA Jetson Orin$^\text{TM}$ Nano. Furthermore, our network is robust to changes in the structured light pattern and relative placement of the pattern emitter and IR camera, leading to simplified and cost-effective construction. We successfully evaluate and demonstrate our proposed depth navigation approach AsterNav using depth from AsterNet in many real-world experiments using only onboard sensing and computation, including dark matte obstacles and thin ropes (diameter 6.25mm), achieving an overall success rate of 95.5% with unknown object shapes, locations and materials. To the best of our knowledge, this is the first work on monocular, structured-light-based quadrotor navigation in absolute darkness.

</details>


### [552] [Correct-by-Construction Vision-based Pose Estimation using Geometric Generative Models](https://arxiv.org/abs/2601.17556)
*Ulices Santa Cruz,Mahmoud Elfar,Yasser Shoukry*

Main category: cs.RO

TL;DR: This paper introduces a framework to certify neural networks for vision-based pose estimation by integrating physics-driven modeling with learning-based methods, ensuring reliability in safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: Safety-critical applications require dependable neural network outputs, yet traditional models lack provable guarantees for their correctness in pose estimation tasks.

Method: The framework combines the geometry of planar objects and a geometric generative model (GGM) for NN training. It expands capabilities using NN reachability analysis to detect objects in cluttered environments and create a unified multi-stage perception pipeline.

Result: The evaluation utilized synthetic and real images, proving effective pose estimation of planar objects like traffic signs, with certification for estimation errors using event-based camera data.

Conclusion: The proposed method successfully integrates certified pose estimation and object detection, offering reliable performance even in cluttered environments for autonomous systems.

Abstract: We consider the problem of vision-based pose estimation for autonomous systems. While deep neural networks have been successfully used for vision-based tasks, they inherently lack provable guarantees on the correctness of their output, which is crucial for safety-critical applications. We present a framework for designing certifiable neural networks (NNs) for perception-based pose estimation that integrates physics-driven modeling with learning-based estimation. The proposed framework begins by leveraging the known geometry of planar objects commonly found in the environment, such as traffic signs and runway markings, referred to as target objects. At its core, it introduces a geometric generative model (GGM), a neural-network-like model whose parameters are derived from the image formation process of a target object observed by a camera. Once designed, the GGM can be used to train NN-based pose estimators with certified guarantees in terms of their estimation errors. We first demonstrate this framework in uncluttered environments, where the target object is the only object present in the camera's field of view. We extend this using ideas from NN reachability analysis to design certified object NN that can detect the presence of the target object in cluttered environments. Subsequently, the framework consolidates the certified object detector with the certified pose estimator to design a multi-stage perception pipeline that generalizes the proposed approach to cluttered environments, while maintaining its certified guarantees. We evaluate the proposed framework using both synthetic and real images of various planar objects commonly encountered by autonomous vehicles. Using images captured by an event-based camera, we show that the trained encoder can effectively estimate the pose of a traffic sign in accordance with the certified bound provided by the framework.

</details>


### [553] [Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction](https://arxiv.org/abs/2601.17812)
*Mingtian Du,Suhas Raghavendra Kulkarni,Bernardo Noronha,Domenico Campolo*

Main category: cs.RO

TL;DR: This paper tackles the challenge of accurately perceiving patient stiffness in robot-mediated remote physical therapy by proposing a delay-compensated stiffness estimation method that corrects for temporal misalignments and provides robust results even under network-induced delays.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the difficulty in accurately perceiving patient stiffness during remote, robot-mediated physical therapy due to delays in network communication, which leads to misalignment and errors in conventional estimation methods.

Method: The proposed method introduces a delay-compensated stiffness estimation framework based on an algebraic estimator that uses quasi-static equilibrium to align expert input with novice response. This is followed by a Normalised Weighted Least Squares (NWLS) implementation to filter out dynamic bias.

Result: Experiments conducted using H-MAN rehabilitation robots demonstrated that the proposed method significantly outperformed standard estimators, providing consistent tracking accuracy even with various network delays.

Conclusion: The method provides a robust and reliable solution for high-fidelity haptic perception in remote dyadic interactions, which can greatly improve stiffness assessment in therapeutic robotic systems across networks.

Abstract: Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert's input with the novice's response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks.

</details>


### [554] [Less Is More: Scalable Visual Navigation from Limited Data](https://arxiv.org/abs/2601.17815)
*Yves Inglin,Jonas Frey,Changan Chen,Marco Hutter*

Main category: cs.RO

TL;DR: This paper proposes using classical geometric planners to generate synthetic trajectories to enhance imitation learning for visual navigation in robots.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve mobile robots' goal-conditioned visual navigation by addressing the dependency on quality and varied training data in imitation learning.

Method: The authors introduce 'Less is More' (LiMo), a transformer-based policy leveraging synthetic trajectories generated by geometric planners to predict SE(2) trajectories from a single RGB observation.

Result: Adding planner-generated supervision to limited expert demonstrations significantly boosts navigation performance in both real-robot settings and ablations.

Conclusion: Strategically curating diverse, high-quality datasets, rather than increasing the quantity of demonstrations, is crucial for effective visual navigation.

Abstract: Imitation learning provides a powerful framework for goal-conditioned visual navigation in mobile robots, enabling obstacle avoidance while respecting human preferences and social norms. However, its effectiveness depends critically on the quality and diversity of training data. In this work, we show how classical geometric planners can be leveraged to generate synthetic trajectories that complement costly human demonstrations. We train Less is More (LiMo), a transformer-based visual navigation policy that predicts goal-conditioned SE(2) trajectories from a single RGB observation, and find that augmenting limited expert demonstrations with planner-generated supervision yields substantial performance gains. Through ablations and complementary qualitative and quantitative analyses, we characterize how dataset scale and diversity affect planning performance. We demonstrate real-robot deployment and argue that robust visual navigation is enabled not by simply collecting more demonstrations, but by strategically curating diverse, high-quality datasets. Our results suggest that scalable, embodiment-specific geometric supervision is a practical path toward data-efficient visual navigation.

</details>


### [555] [NeuroManip: Prosthetic Hand Manipulation System Based on EMG and Eye Tracking Powered by the Neuromorphic Processor AltAi](https://arxiv.org/abs/2601.17991)
*Roman Akinshin,Elizaveta Lopatina,Kirill Bogatikov,Nikolai Kiz,Anna V. Makarova,Mikhail Lebedev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou,Valerii Kangler*

Main category: cs.RO

TL;DR: The paper introduces a novel energy-efficient neuromorphic system combining electromyography (EMG) and gaze-guided vision for controlling upper-limb prostheses, achieving high accuracy and improved usability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a more efficient, reliable, and safe prosthesis control system for upper-limb amputees that operates in a wearable power-efficient setup.

Method: The system integrates real-time EMG pattern recognition using a neuromorphic processor with gaze-guided object identification through an eye-tracking camera. It adapts a GPU-trained EMG model to a spiking neural network on AltAi.

Result: The system achieves robust gesture recognition, improving accuracy to 95% with context-aware object-guided decisions while maintaining sub-watt power consumption.

Conclusion: The proposed neuromorphic and context-aware approach provides reliable, energy-efficient control for upper-limb prostheses, offering enhanced safety and usability for amputees in daily life.

Abstract: This paper presents a novel neuromorphic control architecture for upper-limb prostheses that combines surface electromyography (sEMG) with gaze-guided computer vision. The system uses a spiking neural network deployed on the neuromorphic processor AltAi to classify EMG patterns in real time while an eye-tracking headset and scene camera identify the object within the user's focus. In our prototype, the same EMG recognition model that was originally developed for a conventional GPU is deployed as a spiking network on AltAi, achieving comparable accuracy while operating in a sub-watt power regime, which enables a lightweight, wearable implementation. For six distinct functional gestures recorded from upper-limb amputees, the system achieves robust recognition performance comparable to state-of-the-art myoelectric interfaces. When the vision pipeline restricts the decision space to three context-appropriate gestures for the currently viewed object, recognition accuracy increases to roughly 95% while excluding unsafe, object-inappropriate grasps. These results indicate that the proposed neuromorphic, context-aware controller can provide energy-efficient and reliable prosthesis control and has the potential to improve safety and usability in everyday activities for people with upper-limb amputation.

</details>


### [556] [Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization](https://arxiv.org/abs/2601.18121)
*Byeonggyeol Choi,Woojin Oh,Jongwoo Lim*

Main category: cs.RO

TL;DR: This paper presents a framework to refine visually aligned hand-object manipulation trajectories into physically executable ones, addressing flaws in existing datasets like DexYCB and HO3D.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the inadequacies in current hand-object manipulation datasets, which often lead to physically implausible interactions when replayed in physics simulators.

Method: The authors propose a simulation-in-the-loop framework using a low-dimensional, spline-based motion representation and a gradient-free optimizer (CMA-ES) to refine trajectories into physically valid ones.

Result: The proposed approach achieves lower pose errors and better physical interactions compared to existing methods like MANIPTRANS.

Conclusion: This framework provides a scalable method to turn visual demonstrations into physically executable trajectories, improving the quality of data crucial for policy learning.

Abstract: Dexterous hand manipulation increasingly relies on large-scale motion datasets with precise hand-object trajectory data. However, existing resources such as DexYCB and HO3D are primarily optimized for visual alignment but often yield physically implausible interactions when replayed in physics simulators, including penetration, missed contact, and unstable grasps.
  We propose a simulation-in-the-loop refinement framework that converts these visually aligned trajectories into physically executable ones. Our core contribution is to formulate this as a tractable black-box optimization problem. We parameterize the hand's motion using a low-dimensional, spline-based representation built on sparse temporal keyframes. This allows us to use a powerful gradient-free optimizer, CMA-ES, to treat the high-fidelity physics engine as a black-box objective function. Our method finds motions that simultaneously maximize physical success (e.g., stable grasp and lift) while minimizing deviation from the original human demonstration.
  Compared to MANIPTRANS-recent transfer pipelines, our approach achieves lower hand and object pose errors during replay and more accurately recovers hand-object physical interactions. Our approach provides a general and scalable method for converting visual demonstrations into physically valid trajectories, enabling the generation of high-fidelity data crucial for robust policy learning.

</details>


### [557] [Quest2ROS2: A ROS 2 Framework for Bi-manual VR Teleoperation](https://arxiv.org/abs/2601.18289)
*Jialong Li,Zhenguo Wang,Tianci Wang,Maj Stenmark,Volker Krueger*

Main category: cs.RO

TL;DR: Quest2ROS2 is an open-source ROS2 framework for virtual reality-enabled bi-manual robot teleoperation with enhanced usability and safety features.


<details>
  <summary>Details</summary>
Motivation: To overcome workspace limitations and improve operator experience in bi-manual robot teleoperation using virtual reality technology.

Method: Developed a ROS2 framework with relative motion-based control and modular architecture supporting "Side-by-Side" and "Mirror" modes along with safety and usability tools like RViz visualization and gripper control.

Result: The Quest2ROS2 framework enables intuitive, pose-independent teleoperation while ensuring smooth transitions and adaptable control across diverse platforms.

Conclusion: Quest2ROS2 provides an advanced teleoperation solution for robot data collection, integrating seamless and safe virtual reality methods for diverse applications.

Abstract: Quest2ROS2 is an open-source ROS2 framework for bi-manual teleoperation designed to scale robot data collection. Extending Quest2ROS, it overcomes workspace limitations via relative motion-based control, calculating robot movement from VR controller pose changes to enable intuitive, pose-independent operation. The framework integrates essential usability and safety features, including real-time RViz visualization, streamlined gripper control, and a pause-and-reset function for smooth transitions. We detail a modular architecture that supports "Side-by-Side" and "Mirror" control modes to optimize operator experience across diverse platforms. Code is available at: https://github.com/Taokt/Quest2ROS2.

</details>


### [558] [TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion](https://arxiv.org/abs/2601.18323)
*Weishi Mi,Yong Bao,Xiaowei Chi,Xiaozhu Ju,Zhiyuan Qin,Kuangzhi Ge,Kai Tang,Peidong Jia,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: The paper presents TC-IDM, a tool-centric inverse dynamics approach that bridges vision-language planning and physical control using generative world models for robotics tasks.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of vision-language models in robotic control, particularly their dependence on large-scale, high-quality data and lack of connection between pixel-level plans and actionable physical motions.

Method: TC-IDM uses point cloud trajectories focusing on the tool's imagined motion from videos, decoupling trajectory planning into 6-DoF end-effector motions and control signals, improving viewpoint invariance and generalization.

Result: TC-IDM achieved an average success rate of 61.11% in real-world tests, surpassing VLA baselines and other models, with high performance in simple and deformable object tasks.

Conclusion: TC-IDM bridges visual planning and physical control, demonstrating robust generalization and promising results in diverse robotic settings, including long-horizon tasks.

Abstract: The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.
  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.
  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.
  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.
  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.

</details>


### [559] [SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation](https://arxiv.org/abs/2601.18442)
*Hongyi Zhao,Shuo Wang,Qijie He,Ziyuan Pu*

Main category: cs.RO

TL;DR: The paper introduces SG-CADVLM, a Vision Language Model framework for creating safety-critical autonomous driving scenarios from crash reports and road diagrams, achieving a significant performance boost in generating realistic and testable simulations.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity and cost of safety-critical autonomous vehicle testing scenarios derived from real-world collisions by leveraging crash reports and simulations.

Method: The framework integrates Context-Aware Decoding with multi-modal input processing, using vision language models to generate realistic road geometry and vehicle trajectories from crash reports.

Result: Experimental results show SG-CADVLM achieves a 469% improvement over baseline methods, generating high-risk scenarios at 84.4% compared to 12.5% for other methods.

Conclusion: SG-CADVLM effectively mitigates existing limitations in scenario generation, providing a robust and realistic solution for autonomous vehicle safety validation testing.

Abstract: Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.

</details>


### [560] [DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation](https://arxiv.org/abs/2601.18492)
*Zijun Li,Shijie Li,Zhenxi Zhang,Bin Li,Shoujun Zhou*

Main category: cs.RO

TL;DR: Introduces DV-VLN framework to improve Vision-and-Language Navigation (VLN) by incorporating generate-then-verify approach for structured reasoning and enhanced decision-making.


<details>
  <summary>Details</summary>
Motivation: To address challenges in VLN scenarios where noisy observations and imperfect reasoning lead to error accumulation and unreliability, especially in unseen environments.

Method: Develop DV-VLN framework using LLaMA-2 with in-domain adaptation to produce navigational chain-of-thought and verify actions using True-False Verification (TFV) and Masked-Entity Verification (MEV).

Result: Experiments on VLN datasets (R2R, RxR, REVERIE) show improved performance over direct prediction and sampling baselines, demonstrating DV-VLN’s effectiveness within language-only systems and competitiveness against multimodal systems.

Conclusion: DV-VLN enhances navigation performance and interpretability by leveraging structured reasoning, efficient verification processes, and reranking techniques in language-driven agent systems.

Abstract: Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.Code is available at https://github.com/PlumJun/DV-VLN.

</details>


### [561] [SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction](https://arxiv.org/abs/2601.18537)
*Linyong Gan,Zimo Li,Wenxin Xu,Xingjian Li,Jianhua Z. Huang,Enmei Tu,Shuhang Chen*

Main category: cs.RO

TL;DR: This paper presents a semantic-key-point-based framework for improving long-horizon vessel trajectory prediction by addressing drifting issues through global decision-making and local motion modeling approaches.


<details>
  <summary>Details</summary>
Motivation: Long-horizon vessel trajectory prediction is challenging due to uncertainties from navigation behaviors and environmental factors, and existing methods tend to produce inaccurate, drifting trajectories.

Method: The method introduces a Next Key Point (NKP)-conditioned framework that splits trajectory prediction into global semantic decision-making and local motion modeling, combining this with a pretrain-finetune strategy.

Result: The proposed method shows superior performance on real-world AIS data, particularly for long travel durations, maintaining directional accuracy, and fine-grained predictions, outperforming state-of-the-art methods.

Conclusion: The approach effectively restricts future trajectory possibilities to feasible subsets and offers improved prediction accuracy, addressing the critical challenge in long-horizon predictions.

Abstract: Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.

</details>


### [562] [Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field](https://arxiv.org/abs/2601.18548)
*Yulin Li,Zhiyuan Song,Yiming Li,Zhicheng Song,Kai Chen,Chunxin Zheng,Zhihai Bi,Jiahang Cao,Sylvain Calinon,Fan Shi,Jun Ma*

Main category: cs.RO

TL;DR: The paper introduces Generalized Configuration Space Distance Fields (GCDF) for mobile manipulation, improving whole-body trajectory optimization in cluttered spaces.


<details>
  <summary>Details</summary>
Motivation: To address challenges in trajectory optimization for mobile manipulators in cluttered, confined spaces, especially handling high-dimensional nonconvexity and collision reasoning.

Method: The paper develops GCDF to extend Configuration Space Distance Fields (CDF) for robots with translational and rotational joints in unbounded workspaces and tighter base-arm coupling. It includes a neural network training pipeline for smooth and efficient collision representation and a sequential convex optimization framework.

Result: GCDF accurately represents distances and gradients in configuration space, efficiently supporting large-scale collision constraints. The proposed solver ensures scalable and rapid trajectory optimization.

Conclusion: GCDF unlocks efficient whole-body trajectory optimization for mobile manipulators, enabling agile planning in complex environments with unbounded workspaces and tight constraints.

Abstract: Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes.

</details>


### [563] [Attention-Based Neural-Augmented Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2601.18569)
*Seokju Lee,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: This paper presents AttenNKF, a novel neural-augmented Kalman Filter for improving state estimation in legged robots under foot slip conditions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address inaccuracies in state estimation for legged robots caused by foot slip, a situation where kinematic measurements introduce bias by violating no-slip assumptions.

Method: The proposed method integrates an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to estimate and correct errors induced by foot slip, ensuring efficient state updates.

Result: Experimental results show that the proposed AttenNKF significantly outperforms existing state estimators for legged robots, especially in conditions prone to slippage.

Conclusion: The AttenNKF effectively mitigates slip-induced errors in state estimation, utilizing attention-based neural compensations without disrupting traditional Kalman Filter recursions, offering a robust solution for slip-prone environments.

Abstract: In this letter, we propose an Attention-Based Neural-Augmented Kalman Filter (AttenNKF) for state estimation in legged robots. Foot slip is a major source of estimation error: when slip occurs, kinematic measurements violate the no-slip assumption and inject bias during the update step. Our objective is to estimate this slip-induced error and compensate for it. To this end, we augment an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to infer error conditioned on foot-slip severity and then applies this estimate as a post-update compensation to the InEKF state (i.e., after the filter update). The compensator is trained in a latent space, which aims to reduce sensitivity to raw input scales and encourages structured slip-conditioned compensations, while preserving the InEKF recursion. Experiments demonstrate improved performance compared to existing legged-robot state estimators, particularly under slip-prone conditions.

</details>


### [564] [ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection](https://arxiv.org/abs/2601.18629)
*Yiming Wang,Ruogu Zhang,Minyang Li,Hao Shi,Junbo Wang,Deyi Li,Jieji Ren,Wenhai Liu,Weiming Wang,Hao-Shu Fang*

Main category: cs.RO

TL;DR: ExoGS is a Real-to-Sim-to-Real framework capturing dynamic interactions and static environments using a passive exoskeleton, enabling scalable manipulation data collection and policy learning.


<details>
  <summary>Details</summary>
Motivation: Address challenges and inefficiencies in capturing real-world interactions for robotic manipulation through scalable data collection methods.

Method: Developed ExoGS, a framework using AirExo-3 exoskeleton for accurate data capturing, 3D Gaussian Splatting assets for simulation, and Mask Adapter for policy robustness.

Result: ExoGS improves data efficiency and policy generalization under visual domain shifts compared to teleoperation baselines.

Conclusion: ExoGS provides an effective solution for robotic manipulation policy learning by bridging real-world tasks and simulated training environments with large-scale manipulable data.

Abstract: Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on https://github.com/zaixiabalala/ExoGS.

</details>


### [565] [Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation](https://arxiv.org/abs/2601.18639)
*Ojasva Mishra,Xiaolong Wu,Min Xu*

Main category: cs.RO

TL;DR: The paper addresses the challenges of implementing PID loops in robotics, focusing on tuning for discrete-time joint control under practical constraints like saturation and delays.


<details>
  <summary>Details</summary>
Motivation: Ensuring the precise control and stability of rotary actuation in autonomous robotics amidst real-world limitations such as actuator saturation and measurement errors.

Method: Derivation of PI stability regions with discrete-time Jury criterion, evaluation of anti-windup strategies, and introduction of hybrid-certified Bayesian optimization workflow to improve tuning.

Result: The proposed method improves robustness-oriented tuning with improved median IAE from 0.843 to 0.430 and maintains overshoot below 2%. The screening reduces unstable candidate gains by 11.6%.

Conclusion: The implementation-aware approach enhances discrete PID control performance with fewer experiments, offering practical benefits in autonomous robotics settings.

Abstract: The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($τ=1.0$~s, $Δt=0.01$~s, $u\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\%$. In simulation-only tuning, the certification screen rejects $11.6\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.

</details>


### [566] [A Pragmatic VLA Foundation Model](https://arxiv.org/abs/2601.18692)
*Wei Wu,Fan Lu,Yunnan Wang,Shuai Yang,Shi Liu,Fangjing Wang,Qian Zhu,He Sun,Yong Wang,Shuailei Ma,Yiyu Ren,Kejia Zhang,Hui Yu,Jingmei Zhao,Shuai Zhou,Zhenqi Qiu,Houlong Xiong,Ziyu Wang,Zechen Wang,Ran Cheng,Yong-Lu Li,Yongtao Huang,Xing Zhu,Yujun Shen,Kecheng Zheng*

Main category: cs.RO

TL;DR: The paper introduces LingBot-VLA, a Vision-Language-Action (VLA) foundation model trained with extensive robot data, proving its superior generalization and efficiency in robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: To create a VLA model for robotic manipulation that generalizes across tasks and platforms while being cost-efficient.

Method: Training LingBot-VLA on 20,000 hours of real-world data from 9 robot configurations and evaluating it systematically on 3 platforms with performance assessments.

Result: LingBot-VLA outperforms competitors and features a codebase with high training throughput, supporting its suitability for deployment.

Conclusion: LingBot-VLA offers strong performance and generalization, providing open resources to advance robot learning and evaluation standards.

Abstract: Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.

</details>


### [567] [Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods](https://arxiv.org/abs/2601.18723)
*Mengyuan Liu,Juyi Sheng,Peiming Li,Ziyi Wang,Tianming Xu,Tiantian Xu,Hong Liu*

Main category: cs.RO

TL;DR: The paper introduces a standardized evaluation strategy for robotic manipulation models by addressing trust dimensions: Source Authenticity and Execution Quality, via the Eval-Actions benchmark and AutoEval architecture.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for robotic manipulation are insufficient, as they rely on binary success rates and fail to address key aspects of trust like policy authenticity and execution quality.

Method: The authors propose the Eval-Actions benchmark for trustworthy analysis, including failure cases, and the AutoEval architecture for semantic and motion assessment. Additionally, AutoEval-P enhances reasoning with the GRPO paradigm.

Result: The proposed framework achieves high correlation consistency (SRCC of 0.81 and 0.84) and distinguishes policy-generated from teleoperated trajectories with 99.6% accuracy.

Conclusion: This paper establishes a comprehensive framework for reliable robotic manipulation evaluation, addressing major gaps and setting a benchmark for future evaluations.

Abstract: Driven by the rapid evolution of Vision-Action and Vision-Language-Action models, imitation learning has significantly advanced robotic manipulation capabilities. However, evaluation methodologies have lagged behind, hindering the establishment of Trustworthy Evaluation for these behaviors. Current paradigms rely on binary success rates, failing to address the critical dimensions of trust: Source Authenticity (i.e., distinguishing genuine policy behaviors from human teleoperation) and Execution Quality (e.g., smoothness and safety). To bridge these gaps, we propose a solution that combines the Eval-Actions benchmark and the AutoEval architecture. First, we construct the Eval-Actions benchmark to support trustworthiness analysis. Distinct from existing datasets restricted to successful human demonstrations, Eval-Actions integrates VA and VLA policy execution trajectories alongside human teleoperation data, explicitly including failure scenarios. This dataset is structured around three core supervision signals: Expert Grading (EG), Rank-Guided preferences (RG), and Chain-of-Thought (CoT). Building on this, we propose the AutoEval architecture: AutoEval leverages Spatio-Temporal Aggregation for semantic assessment, augmented by an auxiliary Kinematic Calibration Signal to refine motion smoothness; AutoEval Plus (AutoEval-P) incorporates the Group Relative Policy Optimization (GRPO) paradigm to enhance logical reasoning capabilities. Experiments show AutoEval achieves Spearman's Rank Correlation Coefficients (SRCC) of 0.81 and 0.84 under the EG and RG protocols, respectively. Crucially, the framework possesses robust source discrimination capabilities, distinguishing between policy-generated and teleoperated videos with 99.6% accuracy, thereby establishing a rigorous standard for trustworthy robotic evaluation. Our project and code are available at https://term-bench.github.io/.

</details>


### [568] [Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge](https://arxiv.org/abs/2601.18733)
*Li Kang,Heng Zhou,Xiufeng Song,Rui Li,Bruno N. Y. Chen,Ziye Wang,Ximeng Meng,Stone Tao,Yiran Qin,Xiaohong Liu,Ruimao Zhang,Lei Bai,Yilun Du,Hao Su,Philip Torr,Zhenfei Yin,Ruihao Gong,Yejun Zeng,Fengjun Zhong,Shenghao Jin,Jinyang Guo,Xianglong Liu,Xiaojun Jia,Tianqi Shan,Wenqi Ren,Simeng Qin,Jialing Yang,Xiaoyu Ma,Tianxing Chen,Zixuan Li,Zijian Cai,Yan Qin,Yusen Qin,Qiangyu Chen,Kaixuan Wang,Zhaoming Han,Yao Mu,Ping Luo,Yuanqi Yao,Haoming Song,Jan-Nico Zaech,Fabien Despinoy,Danda Pani Paudel,Luc Van Gool*

Main category: cs.RO

TL;DR: The paper introduces the MARS Challenge aimed at advancing multi-agent embodied AI through vision-language models in planning and robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in multi-agent collaboration due to increasing agent capabilities, efficiency demands, and advanced human-agent interaction.

Method: The authors propose the MARS Challenge focusing on multi-agent embodied planning and control using vision-language models.

Result: The challenge evaluates participant solutions and offers insights into designing and coordinating multi-agent systems.

Conclusion: The MARS Challenge is a valuable effort to enhance multi-agent collaboration and pave the way for advanced AI systems.

Abstract: Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.

</details>


### [569] [Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery](https://arxiv.org/abs/2601.18765)
*Shutong Chen,Adnan Aijaz,Yansha Deng*

Main category: cs.RO

TL;DR: The paper introduces a Goal-oriented Communication (GoC) framework to optimize communication-computation-control loops for fast and robust fault detection and recovery in robotic systems.


<details>
  <summary>Details</summary>
Motivation: Existing fault detection and recovery (FDR) frameworks face limitations such as delays and unreliability due to design disconnects between communication, computation, and control mechanisms.

Method: The GoC framework jointly designs communication, computation, and control loops. It employs a 3D scene graph for semantic representation, fine-tunes small language models via Low-Rank Adaptation, performs knowledge distillation for motion generation, and includes a lightweight digital twin module for enhanced control.

Result: The GoC framework reduces FDR time by up to 82.6% and improves the task success rate by up to 76%, outperforming current state-of-the-art approaches.

Conclusion: The GoC framework effectively enhances FDR processes, achieving significant time reductions and improved task success rates in dynamic robotic environments.

Abstract: Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR). However, existing FDR frameworks exhibit various limitations, such as significant delays in communication and computation, and unreliability in robot motion/trajectory generation, mainly because the communication-computation-control (3C) loop is designed without considering the downstream FDR goal. To address this, we propose a novel Goal-oriented Communication (GoC) framework that jointly designs the 3C loop tailored for fast and robust robotic FDR, with the goal of minimising the FDR time while maximising the robotic task (e.g., workpiece sorting) success rate. For fault detection, our GoC framework innovatively defines and extracts the 3D scene graph (3D-SG) as the semantic representation via our designed representation extractor, and detects faults by monitoring spatial relationship changes in the 3D-SG. For fault recovery, we fine-tune a small language model (SLM) via Low-Rank Adaptation (LoRA) and enhance its reasoning and generalization capabilities via knowledge distillation to generate recovery motions for robots. We also design a lightweight goal-oriented digital twin reconstruction module to refine the recovery motions generated by the SLM when fine-grained robotic control is required, using only task-relevant object contours for digital twin reconstruction. Extensive simulations demonstrate that our GoC framework reduces the FDR time by up to 82.6% and improves the task success rate by up to 76%, compared to the state-of-the-art frameworks that rely on vision language models for fault detection and large language models for fault recovery.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [570] [Risk-based test framework for LLM features in regulated software](https://arxiv.org/abs/2601.17292)
*Zhiyin Zhou*

Main category: cs.SE

TL;DR: The paper proposes a risk-based testing framework for large language models (LLMs) in regulated software. It includes a six-category risk taxonomy, a layered test strategy, and applies this methodology to a clinical research platform.


<details>
  <summary>Details</summary>
Motivation: With the integration of LLMs into regulated and safety-critical systems, there are increasing risks like hallucinations, biases, and misuse. Existing methods provide limited guidance for interactive, product-embedded scenarios, necessitating a new framework.

Method: The framework focuses on a six-category risk taxonomy, a layered test structure to map risks, and detailed strategies across guardrail, orchestration, and system layers. A case study demonstrates its practical application.

Result: The framework successfully demonstrates its applicability by addressing key risks through a Knowledgebase assistant implemented in a clinical research platform.

Conclusion: The proposed framework is an effective approach to mitigate risks associated with LLM-integrated systems in sensitive and regulated environments, providing a pathway for safer deployment.

Abstract: Large language models are increasingly embedded in regulated and safety-critical software, including clinical research platforms and healthcare information systems. While these features enable natural language search, summarization, and configuration assistance, they introduce risks such as hallucinations, harmful or out-of-scope advice, privacy and security issues, bias, instability under change, and adversarial misuse. Prior work on machine learning testing and AI assurance offers useful concepts but limited guidance for interactive, product-embedded assistants. This paper proposes a risk-based testing framework for LLM features in regulated software: a six-category risk taxonomy, a layered test strategy mapping risks to concrete tests across guardrail, orchestration, and system layers, and a case study applying the approach to a Knowledgebase assistant in a clinical research platform.

</details>


### [571] [YASA: Scalable Multi-Language Taint Analysis on the Unified AST at Ant Group](https://arxiv.org/abs/2601.17390)
*Yayi Wang,Shenao Wang,Jian Zhao,Shaosen Shi,Ting Li,Yan Cheng,Lizhong Bian,Kan Yu,Yanjie Zhao,Haoyu Wang*

Main category: cs.SE

TL;DR: The paper introduces YASA, a multi-language static taint analysis framework for large-scale industrial use. It addresses challenges of language diversity by employing a Unified Abstract Syntax Tree (UAST) and demonstrates superior performance compared to existing tools.


<details>
  <summary>Details</summary>
Motivation: Current static taint analysis tools face limitations in handling diverse programming languages efficiently, making it challenging for large-scale industrial applications. The paper aims to overcome these barriers.

Method: YASA employs a Unified Abstract Syntax Tree (UAST) for cross-language compatibility and performs point-to analysis and taint propagation using a unified semantic model, supplemented by language-specific models for unique features.

Result: YASA outperformed 8 leading single- and multi-language tools on standard benchmarks across four languages. In deployment within Ant Group, it analyzed over 100 million lines of code, identifying 314 taint paths, including 92 verified 0-day vulnerabilities.

Conclusion: YASA is an effective and scalable solution for multi-language taint analysis at an industrial scale, showcasing significant improvements over existing tools while successfully securing enterprise-level software systems.

Abstract: Modern enterprises increasingly adopt diverse technology stacks with various programming languages, posing significant challenges for static application security testing (SAST). Existing taint analysis tools are predominantly designed for single languages, requiring substantial engineering effort that scales with language diversity. While multi-language tools like CodeQL, Joern, and WALA attempt to address these challenges, they face limitations in intermediate representation design, analysis precision, and extensibility, which make them difficult to scale effectively for large-scale industrial applications at Ant Group. To bridge this gap, we present YASA (Yet Another Static Analyzer), a unified multi-language static taint analysis framework designed for industrial-scale deployment. Specifically, YASA introduces the Unified Abstract Syntax Tree (UAST) that provides a unified abstraction for compatibility across diverse programming languages. Building on the UAST, YASA performs point-to analysis and taint propagation, leveraging a unified semantic model to manage language-agnostic constructs, while incorporating language-specific semantic models to handle other unique language features. When compared to 6 single- and 2 multi-language static analyzers on an industry-standard benchmark, YASA consistently outperformed all baselines across Java, JavaScript, Python, and Go. In real-world deployment within Ant Group, YASA analyzed over 100 million lines of code across 7.3K internal applications. It identified 314 previously unknown taint paths, with 92 of them confirmed as 0-day vulnerabilities. All vulnerabilities were responsibly reported, with 76 already patched by internal development teams, demonstrating YASA's practical effectiveness for securing large-scale industrial software systems.

</details>


### [572] [Fingerprinting AI Coding Agents on GitHub](https://arxiv.org/abs/2601.17406)
*Taher A. Ghaleb*

Main category: cs.SE

TL;DR: The paper investigates fingerprints of AI coding agents by identifying behavioral signatures across PRs and achieves high accuracy in recognizing their code contributions.


<details>
  <summary>Details</summary>
Motivation: To address the issue of code authorship attribution for AI-assisted coding contributions, ensuring repository governance and research validity.

Method: Analyzed 33,580 pull requests from five AI coding agents using 41 features related to commit messages, PR structure, and code characteristics to identify patterns.

Result: Achieved 97.2% F1-score in identifying the respective AI coding agent and uncovered unique behavioral signatures such as Codex's multiline commit patterns and Claude Code's usage of conditional statements.

Conclusion: AI coding tools leave detectable behavioral patterns, enabling identification of AI-generated contributions in software repositories.

Abstract: AI coding agents are reshaping software development through both autonomous and human-mediated pull requests (PRs). When developers use AI agents to generate code under their own accounts, code authorship attribution becomes critical for repository governance, research validity, and understanding modern development practices. We present the first study on fingerprinting AI coding agents, analyzing 33,580 PRs from five major agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, Claude Code) to identify behavioral signatures. With 41 features spanning commit messages, PR structure, and code characteristics, we achieve 97.2% F1-score in multi-class agent identification. We uncover distinct fingerprints: Codex shows unique multiline commit patterns (67.5% feature importance), and Claude Code exhibits distinctive code structure (27.2% importance of conditional statements). These signatures reveal that AI coding tools produce detectable behavioral patterns, suggesting potential for identifying AI contributions in software repositories.

</details>


### [573] [When AI Agents Touch CI/CD Configurations: Frequency and Success](https://arxiv.org/abs/2601.17413)
*Taher A. Ghaleb*

Main category: cs.SE

TL;DR: The paper analyzes 8,031 pull requests involving AI agents modifying CI/CD configurations in GitHub repositories, focusing on their rarity, reliability, and specialization in GitHub Actions.


<details>
  <summary>Details</summary>
Motivation: To understand how AI agents interact with CI/CD configurations during software development, an underexplored area critical for DevOps and automation.

Method: The study examines 8,031 pull requests from 1,605 GitHub repositories, analyzing both the frequency and outcomes of agent modifications to CI/CD configurations, particularly GitHub Actions.

Result: CI/CD files account for only 3.25% of agent changes. Modifications by AI agents show comparable build success rates to non-CI/CD changes (75.59% vs. 74.87%), with Copilot demonstrating exceptional merging and performance on CI/CD modifications.

Conclusion: AI agents seldom modify CI/CD configurations but focus on GitHub Actions when they do. Their changes are as reliable as regular code. Copilot's specialization indicates opportunities for tailored agent training in CI/CD tasks, influencing future DevOps automation.

Abstract: AI agents are increasingly used in software development, yet their interaction with CI/CD configurations is not well studied. We analyze 8,031 agentic pull requests (PRs) from 1,605 GitHub repositories where AI agents touch YAML configurations. CI/CD configuration files account for 3.25% of agent changes, varying by agent (Devin: 4.83%, Codex: 2.01%, p < 0.001). When agents modify CI/CD, 96.77% target GitHub Actions. Agentic PRs with CI/CD changes merge slightly less often than others (67.77% vs. 71.80%), except for Copilot, whose CI/CD changes merge 15.63 percentage points more often. Across 99,930 workflow runs, build success rates are comparable for CI/CD and non-CI/CD changes (75.59% vs. 74.87%), though three agents show significantly higher success when modifying CI/CD. These results show that AI agents rarely modify CI/CD and focus mostly on GitHub Actions, yet their configuration changes are as reliable as regular code. Copilot's strong CI/CD performance despite lower acceptance suggests emerging configuration specialization, with implications for agent training and DevOps automation.

</details>


### [574] [Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems](https://arxiv.org/abs/2601.17435)
*Maria Jesus Rodriguez-Sanchez,Manuel Noguera,Angel Ruiz-Zafra,Kawtar Benghazi*

Main category: cs.SE

TL;DR: The paper introduces DALIA, a declarative architecture for ensuring reliability in agentic workflows, addressing fundamental issues like hallucinations, unexecutable plans, and brittle coordination.


<details>
  <summary>Details</summary>
Motivation: Address the reliability issues in agentic and multi-agent systems caused by a lack of explicit architectural structure linking goals, capabilities, and execution.

Method: Develops DALIA, a declarative, model-independent architectural layer that organizes tasks, formalizes capabilities, and ensures reproducibility and coordination via a deterministic task graph.

Result: Demonstrates how DALIA enables verifiable and reproducible agentic workflows in task-oriented scenarios.

Conclusion: The architecture significantly improves the reliability and grounded functionality of intelligent agents across heterogeneous environments, mitigating speculative reasoning and unreliable operations.

Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution. This paper presents a declarative, model-independent architectural layer for grounded agentic workflows that addresses this gap. The proposed layer, referred to as DALIA (Declarative Agentic Layer for Intelligent Agents), formalises executable capabilities, exposes tasks through a declarative discovery protocol, maintains a federated directory of agents and their execution resources, and constructs deterministic task graphs grounded exclusively in declared operations. By enforcing a clear separation between discovery, planning and execution, the architecture constrains agent behaviour to a verifiable operational space, reducing reliance on speculative reasoning and free-form coordination. We present the architecture and design principles of the proposed layer and illustrate its operation through a representative task-oriented scenario, demonstrating how declarative grounding enables reproducible and verifiable agentic workflows across heterogeneous environments.

</details>


### [575] [iResolveX: Multi-Layered Indirect Call Resolution via Static Reasoning and Learning-Augmented Refinement](https://arxiv.org/abs/2601.17888)
*Monika Santra,Bokai Zhang,Mark Lim,Vishnu Asutosh Dasu,Dongrui Zeng,Gang Tan*

Main category: cs.SE

TL;DR: The paper introduces iResolveX, a hybrid framework combining static analysis and machine learning to tackle the challenge of indirect call resolution in binaries, offering improvements in precision and maintainable recall.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in resolving indirect calls in reverse engineering and control-flow graph recovery for stripped or optimized binaries, balancing precision and completeness across static analysis and machine-learning approaches.

Method: A hybrid framework featuring: (1) conservative value-set analysis for high recall, (2) learning-based signature scoring and backward analysis to cut false positives, and (3) confidence-scored indirect control-flow recovery outputs.

Result: The framework achieves 44.3% target reduction over baseline analysis while maintaining 97.8% recall, effectively outperforming current state-of-the-art methods.

Conclusion: iResolveX delivers versatile and optimized indirect CFG annotation with balanced recall and precision, aiding in advanced binary analysis tasks.

Abstract: Indirect call resolution remains a key challenge in reverse engineering and control-flow graph recovery, especially for stripped or optimized binaries. Static analysis is sound but often over-approximates, producing many false positives, whereas machine-learning approaches can improve precision but may sacrifice completeness and generalization. We present iResolveX, a hybrid multi-layered framework that combines conservative static analysis with learning-based refinement. The first layer applies a conservative value-set analysis (BPA) to ensure high recall. The second layer adds a learning-based soft-signature scorer (iScoreGen) and selective inter-procedural backward analysis with memory inspection (iScoreRefine) to reduce false positives. The final output, p-IndirectCFG, annotates indirect edges with confidence scores, enabling downstream analyses to choose appropriate precision--recall trade-offs. Across SPEC CPU2006 and real-world binaries, iScoreGen reduces predicted targets by 19.2% on average while maintaining BPA-level recall (98.2%). Combined with iScoreRefine, the total reduction reaches 44.3% over BPA with 97.8% recall (a 0.4% drop). iResolveX supports both conservative, recall-preserving and F1-optimized configurations and outperforms state-of-the-art systems.

</details>


### [576] [Data-driven Test Generation for Fuzzing AI Compiler](https://arxiv.org/abs/2601.17450)
*Qingchao Shen*

Main category: cs.SE

TL;DR: The paper introduces a novel testing framework for AI compilers, addressing stage-specific challenges and detecting numerous bugs.


<details>
  <summary>Details</summary>
Motivation: AI compilers often suffer from bugs that can impact model reliability and correctness. Ensuring their quality is crucial for efficient AI model deployment.

Method: The framework comprises three key components: OPERA (testing operator conversion logic), OATest (testing high-level optimizations), and HARMONY (testing low-level optimizations).

Result: The framework successfully identified 266 previously unknown bugs in four widely-used AI compilers, demonstrating its effectiveness.

Conclusion: The proposed stage-aware and comprehensive testing framework improves AI compiler reliability and testing coverage significantly.

Abstract: Artificial Intelligence (AI) compilers are critical for efficiently deploying AI models across diverse hardware platforms. However, they remain prone to bugs that can compromise both compiler reliability and model correctness. Thus, ensuring the quality of AI compilers is crucial. In this work, we present a unified data-driven testing framework that systematically addresses stage-specific challenges in AI compilers. Specifically, OPERA migrates tests for AI libraries to test various operator conversion logic in the model loading stage. OATest synthesizes diverse optimization-aware computational graphs for testing high-level optimizations. HARMONY generates and mutates diverse low-level IR seeds to generate hardware-optimization-aware tests for testing low-level optimizations. Together, these techniques provide a comprehensive, stage-aware framework that enhances testing coverage and effectiveness, detecting 266 previously unknown bugs in four widely used AI compilers.

</details>


### [577] [LogPrism: Unifying Structure and Variable Encoding for Effective Log Compression](https://arxiv.org/abs/2601.17482)
*Yang Liu,Kaiming Zhang,Zhuangbin Chen,Jinyang Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: The paper introduces LogPrism, a unified framework for log compression that achieves superior compression ratios and processing speeds compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies in log compression caused by treating log parsing and compression as separate objectives, which leads to missed correlations critical for storage efficiency.

Method: LogPrism constructs a Unified Redundancy Tree (URT) to dynamically integrate structural extraction with variable encoding, effectively capturing deep contextual redundancies and enabling pre-emptive pattern encoding.

Result: LogPrism achieves the highest compression ratio on 13 out of 16 benchmark datasets, with improvements ranging from 4.7% to 80.9%, and delivers faster throughput at 29.87 MB/s, outperforming competitors.

Conclusion: LogPrism successfully bridges the gap between log parsing and compression, establishing a new state-of-the-art in compression efficiency and speed.

Abstract: The prevailing "parse-then-compress" paradigm in log compression fundamentally limits effectiveness by treating log parsing and compression as isolated objectives. While parsers prioritize semantic accuracy (i.e., event identification), they often obscure deep correlations between static templates and dynamic variables that are critical for storage efficiency. In this paper, we investigate this misalignment through a comprehensive empirical study and propose LogPrism, a framework that bridges the gap via unified redundancy encoding. Rather than relying on a rigid pre-parsing step, LogPrism dynamically integrates structural extraction with variable encoding by constructing a Unified Redundancy Tree (URT). This hierarchical approach effectively mines "structure+variable" co-occurrence patterns, capturing deep contextual redundancies while accelerating processing through pre-emptive pattern encoding. Extensive experiments on 16 benchmark datasets confirm that LogPrism establishes a new state-of-the-art. It achieves the highest compression ratio on 13 datasets, surpassing leading baselines by margins of 4.7% to 80.9%, while delivering superior throughput at 29.87 MB/s (1.68$\times$~43.04$\times$ faster than competitors). Moreover, when configured in single-archive mode to maximize global pattern discovery, LogPrism outperforms the best baseline by 19.39% in compression ratio while maintaining a 2.62$\times$ speed advantage.

</details>


### [578] [Measuring Braking Behavior Using Vehicle Tracking and Camera-to-Satellite Homography Rectification](https://arxiv.org/abs/2601.17558)
*J. P. Fleischer,Tanchanok Sirikanchittavon,Chonlachart Jeenprasom,Nooshin Yousefzadeh,Sanjay Ranka,Mohammed Hadi*

Main category: cs.SE

TL;DR: This paper presents an open-source software that analyzes traffic footage, focusing on vehicle behavior such as braking events, using homography estimation and YOLO11 object detection to rectify oblique camera perspectives without calibration.


<details>
  <summary>Details</summary>
Motivation: To provide a straightforward and centralized method for analyzing traffic footage for safety and design improvements without requiring prior camera calibration, especially at urban signalized intersections.

Method: The software employs a robust ground-plane homography estimation using MAGSAC++ and YOLO11, rectifying oblique camera views to satellite perspectives to extract vehicle trajectories, speeds, and other metrics. A ClickHouse database stores the analysis data.

Result: A case study at two intersections in Key West highlighted daily variations in braking events, identifying spatial trends in severe, mild, and moderate braking relative to the stop bar and lanes with higher interactions.

Conclusion: This tool demonstrates significant potential for advancing traffic safety analysis and proactive management by facilitating data-driven designs and mitigation strategies for connected vehicles and urban planning.

Abstract: This paper presents an open-source software application for analyzing traffic camera footage, focusing on vehicle behavior and braking events at signalized urban highways. The core innovation is a robust ground-plane homography estimation that links fixed traffic camera views to satellite orthoimagery. This process rectifies the camera's oblique perspective, ensuring that pixel distances accurately represent real-world distances. This enables the acquisition of features such as vehicle trajectory, speed, deceleration, and braking severity without the need for camera calibration. The pipeline employs the MAGSAC++ estimator to build the homography, converting YOLO11 object detections into a rectified top-down coordinate system. All detection and trajectory data are stored in a ClickHouse database for subsequent analysis. A real-world case study at two signalized intersections in Key West, Florida, showcased the system's capabilities. Across two days of daytime footage, braking activity at the higher-volume intersection peaked around 4 PM at approximately 57.5 events per hour, while the second intersection peaked around 10 AM at roughly 15.5 events per hour. The spatial analysis revealed that most braking events initiated upstream, with mild and moderate braking mostly occurring 30 to 45+ meters away from the stop bar and severe braking distributed throughout, but particularly concentrated in lanes with higher interaction and merging activity. The findings highlight the significant potential of this centralized safety information system to support connected vehicles, facilitating proactive traffic management, crash mitigation, and data-driven roadway design and safety analysis.

</details>


### [579] [How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests](https://arxiv.org/abs/2601.17581)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: This study compares AI-generated pull requests (PRs) with human-generated PRs in terms of code modifications and textual description alignment, using a large dataset.


<details>
  <summary>Details</summary>
Motivation: To understand how AI coding agents differ from humans in contributing to software development, focusing on reliability and their impact on workflows.

Method: Analyzed 24,014 AI-generated merged PRs and 5,081 human-generated merged PRs using the AIDev dataset. Investigated code modifications (additions, deletions, etc.) and measured consistency between PR descriptions and diffs through semantic and lexical similarity.

Result: Significant differences found: AI PRs had higher commit counts, moderate differences in files touched and lines deleted, and slightly better alignment between PR descriptions and diffs.

Conclusion: AI coding agents exhibit distinct contribution patterns compared to humans, offering reliable contributions but differing in workflow impact and practices.

Abstract: AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff's $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.

</details>


### [580] [Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language](https://arxiv.org/abs/2601.17584)
*Mahmoud Samir Fayed,Ahmed Samir Fayed*

Main category: cs.SE

TL;DR: The study showcases the creation of a 7420-line Terminal User Interface framework for the Ring programming language in just ten hours using Claude Code, emphasizing prompt-driven development.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate the ability of large language models to build and maintain complex, multi-module systems through a natural language interaction-driven workflow.

Method: The system was developed using 107 iterative prompts over three days. These prompts included feature requests, bug fixes, guidance, and documentation creation through a prompt-driven workflow without manually writing code.

Result: A production-grade framework was developed, incorporating advanced UI components, event-driven architecture, and a multi-window desktop environment using solely LLM-assisted code generation.

Conclusion: The study demonstrates large language models' capacity to sustain architectural coherence and support production-grade software development, while promoting prompt-driven development as a methodology in software engineering.

Abstract: Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice.

</details>


### [581] [Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback](https://arxiv.org/abs/2601.17604)
*Suborno Deb Bappon,Saikat Mondal,Chanchal K. Roy,Kevin Schneider*

Main category: cs.SE

TL;DR: The paper explores the use of Large Language Models (LLMs) to enhance technical answers on platforms like Stack Overflow by interpreting user comments and improving responses using a benchmark and a new tool, AUTOCOMBAT.


<details>
  <summary>Details</summary>
Motivation: Answers on technical Q&A platforms like Stack Overflow often remain incomplete or outdated despite feedback from users due to limited time, expertise, or visibility. This study aims to utilize LLMs to address these challenges and improve the quality and reliability of programming answers.

Method: The authors developed a benchmark called ReSOlve, evaluated the ability of four LLMs to address feedback, introduced the AUTOCOMBAT tool for improving answers based on user comments and context, and conducted a user study with practitioners to assess their approach.

Result: AUTOCOMBAT produces improvements of near-human quality, outperforms baseline approaches, preserves the original intent of the answers, and is considered useful by 84.5% of user study participants.

Conclusion: The study demonstrates the feasibility and value of using LLMs, particularly AUTOCOMBAT, for improving programming answers based on user feedback, enhancing the reliability of technical knowledge platforms.

Abstract: Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms.

</details>


### [582] [Code Change Characteristics and Description Alignment: A Comparative Study of Agentic versus Human Pull Requests](https://arxiv.org/abs/2601.17627)
*Dung Pham,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: AI coding agents’ contributions to pull requests differ significantly from those of humans, showing higher churn for specific symbols and better commit-level messages but weaker PR-level summarizations.


<details>
  <summary>Details</summary>
Motivation: To understand the differences in contributions made by AI-driven coding agents compared to humans in software development tasks.

Method: Analyzed a dataset of 33,596 agent-generated pull requests (APRs) and 6,618 human pull requests (HPRs) based on code changes and message quality.

Result: Agent-generated symbols showed higher removal rates and quicker removal times compared to human contributions. Agents excelled in generating concise commit-level messages but fell short on summarizing at the PR level.

Conclusion: AI coding agents primarily focus on micro-level tasks, but improvements are needed for better macro-level communication and description quality.

Abstract: AI coding agents can autonomously generate pull requests (PRs), yet little is known about how their contributions compare to those of humans. We analyze 33,596 agent-generated PRs (APRs) and 6,618 human PRs (HPRs) to compare code-change characteristics and message quality. We observe that APR-introduced symbols (functions and classes) are removed much sooner than those in HPRs (median time to removal 3 vs. 34 days) and are also removed more often (symbol churn 7.33% vs. 4.10%), reflecting a focus on other tasks like documentation and test updates. Agents generate stronger commit-level messages (semantic similarity 0.72 vs. 0.68) but lag humans at PR-level summarization (PR-commit similarity 0.86 vs. 0.88). Commit message length is the best predictor of description quality, indicating reliance on individual commits over full-PR reasoning. These findings highlight a gap between agents' micro-level precision and macro-level communication, suggesting opportunities to improve agent-driven development workflows.

</details>


### [583] [Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities](https://arxiv.org/abs/2601.17762)
*Zelong Zheng,Jiayuan Zhou,Xing Hu,Yi Gao,Shengyi Pan*

Main category: cs.SE

TL;DR: The paper introduces MAVM, a multi-agent framework for improving software vulnerability management by leveraging historical knowledge and context-retrieval tools to enhance detection, confirmation, repair, and validation processes.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address limitations in traditional static analysis and large language models (LLMs) for software vulnerability detection and management. Existing methods struggle with capturing contextual dependencies and utilizing historical knowledge, which compromises the accuracy and reliability of vulnerability management solutions.

Method: MAVM integrates five components: a vulnerability knowledge base, detection, confirmation, repair, and validation into a multi-agent pipeline. It utilizes a specially constructed knowledge base from publicly disclosed vulnerabilities and context-retrieval tools to enhance reasoning over repository-level information.

Result: MAVM was tested on a dataset with 78 patch-porting cases (covering 114 function-level migrations) and successfully detected and repaired 51 real vulnerabilities. It outperformed baseline methods by 31.9%-45.2% in repair accuracy, showcasing its effectiveness.

Conclusion: The MAVM framework demonstrates significant improvements in software vulnerability detection and repair compared to traditional approaches by effectively utilizing historical knowledge and repository-level context. Its results indicate its potential for real-world applications in robust vulnerability management.

Abstract: Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness.

</details>


### [584] [Prompt-Based REST API Test Amplification in Industry: An Experience Report](https://arxiv.org/abs/2601.17903)
*Tolgahan Bardakci,Andreas Faes,Mutlu Beyazit,Serge Demeyr*

Main category: cs.SE

TL;DR: This paper investigates the use of Large Language Models (LLMs) for REST API test amplification within a major industrial context, demonstrating their practical effectiveness in improving test coverage.


<details>
  <summary>Details</summary>
Motivation: To explore the practical effectiveness of LLMs for REST API test amplification in highly complex industrial systems, where their utility has not been well-established.

Method: The study applies LLM-based test amplification techniques to six endpoints of a production microservice in a complex, security-sensitive logistics system of a large Belgian company.

Result: Findings show LLM-based test amplification increases test coverage and identifies anomalies and observations in the system.

Conclusion: LLM-based test amplification proves to be practically useful in real-world industrial contexts by improving system testing effectiveness and reliability.

Abstract: Large Language Models (LLMs) are increasingly used to support software testing tasks, yet there is little evidence of their effectiveness for REST API testing in industrial settings. To address this gap, we replicate our earlier work on LLM-based REST API test amplification within an industrial context at one of the largest logistics companies in Belgium. We apply LLM-based test amplification to six representative endpoints of a production microservice embedded in a large-scale, security-sensitive system, where there is in-depth complexity in authentication, stateful behavior, and organizational constraints. Our experience shows that LLM-based test amplification remains practically useful in industry by increasing coverage and revealing various observations and anomalies.

</details>


### [585] [RGFL: Reasoning Guided Fault Localization for Automated Program Repair Using Large Language Models](https://arxiv.org/abs/2601.18044)
*Melika Sepidband,Hamed Taherkhani,Hung Viet Pham,Hadi Hemmati*

Main category: cs.SE

TL;DR: The paper proposes a novel fault localization technique to enhance automated program repair by incorporating hierarchical reasoning and innovative ranking methods.


<details>
  <summary>Details</summary>
Motivation: With large software repositories exceeding LLM's token limits, accurate fault localization is necessary for identifying relevant subsets of code and enabling successful repairs.

Method: The approach features a hierarchical reasoning module generating bug-specific explanations and employs a two-stage ranking system using LLM-based and embedding-based signals. It incorporates a counterfactual analysis to assess localization contributions.

Result: The method improves localization accuracy significantly across Python and Java benchmarks, achieving notable gains in Hit@1, MRR, and Exact Match metrics over baselines. Integration with Agentless boosts repair success.

Conclusion: The proposed fault localization technique enhances project-level repair accuracy and provides measurable improvements to end-to-end repair workflows when integrated with existing frameworks.

Abstract: Fault Localization (FL) is a critical step in Automated Program Repair (APR), and its importance has increased with the rise of Large Language Model (LLM)-based repair agents. In realistic project-level repair scenarios, software repositories often span millions of tokens, far exceeding current LLM context limits. Consequently, models must first identify a small, relevant subset of code, making accurate FL essential for effective repair. We present a novel project-level FL approach that improves both file- and element-level localization. Our method introduces a hierarchical reasoning module that (i) generates structured, bug-specific explanations for candidate files and elements, and (ii) leverages these explanations in a two-stage ranking scheme combining LLM-based and embedding-based signals. We further propose a counterfactual upper-bound analysis to quantify the contribution of each localization stage to repair success. We evaluate our approach on Python and Java projects from SWE-bench Verified, Lite, and Java. Compared to state-of-the-art baselines, including Agentless and OpenHands, our method consistently improves localization accuracy. On SWE-bench Verified, file-level Hit@1 improves from 71.4% to 85%, and MRR from 81.8% to 88.8%. At the element level, Exact Match under top-3 files increases from 36% to 69%. Integrating our localization into Agentless yields a 12.8% end-to-end repair success improvement.

</details>


### [586] [TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance](https://arxiv.org/abs/2601.18241)
*Elena Bruches,Vadim Alperovich,Dari Baturova,Roman Derunets,Daniil Grebenkin,Georgy Mkrtchyan,Oleg Sedukhin,Mikhail Klementev,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: The paper introduces TAM-Eval, a benchmark for evaluating LLMs in test suite creation, repair, and updating, highlighting current limitations in test maintenance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM research in unit testing focuses narrowly on tasks like test generation, ignoring comprehensive test suite maintenance challenges in real-world workflows.

Method: Introduced TAM-Eval, a framework assessing model performance in test suite maintenance using 1,539 validated scenarios from Python, Java, and Go projects. Evaluation methods include pass rate, coverage, and mutation testing.

Result: Empirical results show state-of-the-art LLMs perform poorly in realistic test maintenance tasks with marginal improvements in effectiveness.

Conclusion: TAM-Eval demonstrates current LLM limitations in test suite maintenance and is open-sourced to advance automated software testing research.

Abstract: While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.

</details>


### [587] [Agentic Much? Adoption of Coding Agents on GitHub](https://arxiv.org/abs/2601.18341)
*Romain Robbes,Théo Matricon,Thomas Degueule,Andre Hora,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: Coding agents emerged in early 2025 and show significant adoption, impacting software development by autonomously generating code. A study on GitHub revealed a high adoption rate (15.85%-22.60%) across diverse projects.


<details>
  <summary>Details</summary>
Motivation: The motivation is to analyze and understand the significant and rapid adoption of coding agents in software development due to their potential to drastically change software engineering practices.

Method: The study analyzed 129,134 projects from GitHub, leveraging explicit traces (e.g., co-authored commits) left by coding agents to evaluate their adoption rate and characteristics.

Result: Coding agents have an adoption rate of 15.85%-22.60%, widely used across various project maturities, organizations, languages, and topics. Commits they assist with are typically larger and focus on features and bug fixes.

Conclusion: The research underscores the importance of further studying the implications and practical usage of coding agents in software development.

Abstract: In the first half of 2025, coding agents have emerged as a category of development tools that have very quickly transitioned to the practice. Unlike ''traditional'' code completion LLMs such as Copilot, agents like Cursor, Claude Code, or Codex operate with high degrees of autonomy, up to generating complete pull requests starting from a developer-provided task description. This new mode of operation is poised to change the landscape in an even larger way than code completion LLMs did, making the need to study their impact critical. Also, unlike traditional LLMs, coding agents tend to leave more explicit traces in software engineering artifacts, such as co-authoring commits or pull requests. We leverage these traces to present the first large-scale study (129,134 projects) of the adoption of coding agents on GitHub, finding an estimated adoption rate of 15.85%--22.60%, which is very high for a technology only a few months old--and increasing. We carry out an in-depth study of the adopters we identified, finding that adoption is broad: it spans the entire spectrum of project maturity; it includes established organizations; and it concerns diverse programming languages or project topics. At the commit level, we find that commits assisted by coding agents are larger than commits only authored by human developers, and have a large proportion of features and bug fixes. These findings highlight the need for further investigation into the practical use of coding agents.

</details>


### [588] [Forecasting the Maintained Score from the OpenSSF Scorecard for GitHub Repositories linked to PyPI libraries](https://arxiv.org/abs/2601.18344)
*Alexandros Tsakpinis,Efe Berk Ergülec,Emil Schwenger,Alexander Pretschner*

Main category: cs.SE

TL;DR: The paper explores forecasting future maintenance activity for open-source software repositories using historical OpenSSF Maintained scores with multivariate time series analysis.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the retrospective limitation of the OpenSSF Maintained metric, which only reflects past development activity, by exploring methods to predict future maintenance risks for more proactive risk management.

Method: The paper analyzes maintenance activity for 3,220 GitHub repositories using multivariate time series forecasting with target representations such as raw scores, trend slopes, and bucketed maintenance levels. Models like VARMA, Random Forest, and LSTM are compared across different training and forecasting windows.

Result: Future maintenance activity can be predicted accurately, achieving over 0.95 accuracy for bucketed scores and over 0.80 for trend types. Simpler statistical and machine learning models perform comparably to deep learning models.

Conclusion: Predictive modeling serves as an effective complement to existing Scorecard metrics, enabling better proactive assessment of maintenance risks in open-source software repositories without relying on complex architectures.

Abstract: The OpenSSF Scorecard is widely used to assess the security posture of open-source software repositories, with the Maintained metric indicating recent development activity and helping identify potentially abandoned dependencies. However, this metric is inherently retrospective, reflecting only the past 90 days of activity and providing no insight into future maintenance, which limits its usefulness for proactive risk assessment. In this paper, we study to what extent future maintenance activity, as captured by the OpenSSF Maintained score, can be forecasted. We analyze 3,220 GitHub repositories associated with the top 1% most central PyPI libraries by PageRank and reconstruct historical Maintained scores over a three-year period. We formulate the task as multivariate time series forecasting and consider four target representations: raw scores, bucketed maintenance levels, numerical trend slopes, and categorical trend types. We compare a statistical model (VARMA), a machine learning model (Random Forest), and a deep learning model (LSTM) across training windows of 3-12 months and forecasting horizons of 1-6 months. Our results show that future maintenance activity can be predicted with meaningful accuracy, particularly for aggregated representations such as bucketed scores and trend types, achieving accuracies above 0.95 and 0.80, respectively. Simpler statistical and machine learning models perform on par with deep learning approaches, indicating that complex architectures are not required. These findings suggest that predictive modeling can effectively complement existing Scorecard metrics, enabling more proactive assessment of open-source maintenance risks.

</details>


### [589] [Promises, Perils, and (Timely) Heuristics for Mining Coding Agent Activity](https://arxiv.org/abs/2601.18345)
*Romain Robes Théo Matricon,Thomas Degueule,Andre Hora,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: Coding agents, powered by LLMs, have grown rapidly in adoption and leave visible traces in repositories, enabling their study using MSR techniques to assess their impact on software practices.


<details>
  <summary>Details</summary>
Motivation: To understand the unique adoption impact of coding agents using LLMs and to study their influence on software engineering practices through traces left in repositories.

Method: The study leveraged MSR techniques to analyze coding agent activity visible in GitHub repositories, focusing on their adoption, promises, risks, and observed heuristics.

Result: The paper documents key aspects such as the benefits, challenges, and general trends stemming from coding agents' activity on GitHub repositories.

Conclusion: Coding agents are transformative for software engineering practices, offering new possibilities and challenges that warrant further study through visible data traces.

Abstract: In 2025, coding agents have seen a very rapid adoption. Coding agents leverage Large Language Models (LLMs) in ways that are markedly different from LLM-based code completion, making their study critical. Moreover, unlike LLM-based completion, coding agents leave visible traces in software repositories, enabling the use of MSR techniques to study their impact on SE practices. This paper documents the promises, perils, and heuristics that we have gathered from studying coding agent activity on GitHub.

</details>


### [590] [daVinci-Dev: Agent-native Mid-training for Software Engineering](https://arxiv.org/abs/2601.18418)
*Ji Zeng,Dayuan Fu,Tiantian Mi,Yumin Zhuang,Yaxing Huang,Xuefeng Li,Lyumanshan Ye,Muhang Xie,Qishuo Hua,Zhen Huang,Mohan Jiang,Hanning Wang,Jifan Lin,Yang Xiao,Jie Sun,Yunze Wu,Pengfei Liu*

Main category: cs.SE

TL;DR: The paper explores agentic mid-training for large language models to enhance autonomous software engineering capabilities by using systematic data synthesis principles and training methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of teaching LLMs foundational agentic behaviors for software engineering tasks in a scalable and resource-efficient manner.

Method: The study introduces agent-native data, combining contextually-native trajectories and environmentally-native trajectories to bridge the distribution mismatch between static training data and dynamic real-world environments.

Result: The agentic mid-training approach demonstrated superiority over previous methods, achieving enhanced resolution rates, with notable improvements in efficiency using fewer mid-training tokens.

Conclusion: Agentic mid-training is an effective and resource-efficient alternative to post-training methods for cultivating foundational agentic behaviors in LLMs for software engineering tasks.

Abstract: Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...

</details>


### [591] [An Audit of Machine Learning Experiments on Software Defect Prediction](https://arxiv.org/abs/2601.18477)
*Giuseppe Destefanis,Leila Yousefi,Martin Shepperd,Allan Tucker,Stephen Swift,Steve Counsell,Mahir Arzoky*

Main category: cs.SE

TL;DR: This paper audits 101 recent software defect prediction (SDP) studies (2019-2023), analyzing the experimental designs and reproducibility. Results show substantial variations in design/reporting, with limited reproducibility in almost half of the studies.


<details>
  <summary>Details</summary>
Motivation: To evaluate and assess the credibility of experimental designs and reporting practices in recent software defect prediction studies, highlighting reproducibility issues.

Method: Audited 101 randomly sampled SDP studies indexed in SCOPUS (2019-2023). Assessed practices such as outcome measures, validation strategies, use of statistical inference, and reproducibility based on González Barahona and Robles' method.

Result: Substantial variation in practices was observed. Only one paper lacked design/reporting issues out of 427 identified. About 45% applied statistical inference, and reproducibility levels varied widely. Potential paper mill activity was identified.

Conclusion: Current SDP practices show wide inconsistencies and inadequate detail for reproduction in many cases, emphasizing a need for improvement in experimental and reporting standards.

Abstract: Background: Machine learning algorithms are widely used to predict defect prone software components. In this literature, computational experiments are the main means of evaluation, and the credibility of results depends on experimental design and reporting. Objective: This paper audits recent software defect prediction (SDP) studies by assessing their experimental design, analysis, and reporting practices against accepted norms from statistics, machine learning, and empirical software engineering. The aim is to characterise current practice and assess the reproducibility of published results. Method: We audited SDP studies indexed in SCOPUS between 2019 and 2023, focusing on design and analysis choices such as outcome measures, out of sample validation strategies, and the use of statistical inference. Nine study issues were evaluated. Reproducibility was assessed using the instrument proposed by González Barahona and Robles. Results: The search identified approximately 1,585 SDP experiments published during the period. From these, we randomly sampled 101 papers, including 61 journal and 40 conference publications, with almost 50 percent behind paywalls. We observed substantial variation in research practice. The number of datasets ranged from 1 to 365, learners or learner variants from 1 to 34, and performance measures from 1 to 9. About 45 percent of studies applied formal statistical inference. Across the sample, we identified 427 issues, with a median of four per paper, and only one paper without issues. Reproducibility ranged from near complete to severely limited. We also identified two cases of tortured phrases and possible paper mill activity. Conclusions: Experimental design and reporting practices vary widely, and almost half of the studies provide insufficient detail to support reproduction. The audit indicates substantial scope for improvement.

</details>


### [592] [On the Abolition of the "ICSE Paper" and the Adoption of the "Registered Proposal" and the "Results Report"](https://arxiv.org/abs/2601.18566)
*Fabio Massacci,Winnie Mbaka*

Main category: cs.SE

TL;DR: Proposes replacing traditional ICSE papers with a two-tier system: 'Registered Proposal' and 'Results Reports'.


<details>
  <summary>Details</summary>
Motivation: To address issues like the 'novelty-vicious cycle' and 'replicability crisis' in the research community.

Method: Introduce a two-tier system: submission of a 'Registered Proposal' followed by 'Results Reports' after empirical work realization.

Result: The paper argues that such disruptive proposals are backed by community responses in a survey.

Conclusion: Suggests that implementing this system will foster discipline, improve replicability, and potentially solve current research shortcomings.

Abstract: To address the 'novelty-vicious cycle' and the 'replicability crisis' of the field (both discussed in the survey) we propose abolishing the "ICSE paper" as we know it and replacing it with a two-tier system that also evolves the existing notion of 'Registered Report'. Authors proposing a new idea, experiment, or analysis would submit a "Registered Proposal" of their idea and the proposed experimental methodology to undergo peer review. The following year, anyone can submit (shorter) "Results Reports" on the realization of the empirical work based on the registered proposals of the previous ICSE (or FSE or ISSTA or ASE etc.). Both works should be first class citizens of the mainstream events. We argue that such a disruptive (heretical?) idea is supported and based on the responses of the community of the Future of Software Engineering pre-survey

</details>


### [593] [How are MLOps Frameworks Used in Open Source Projects? An Empirical Characterization](https://arxiv.org/abs/2601.18591)
*Fiorella Zampetti,Federico Stocchetti,Federica Razzano,Damian Andrew Tamburri,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: The paper explores the use and desired enhancements of eight open-source MLOps frameworks by analyzing GitHub projects and issue trackers.


<details>
  <summary>Details</summary>
Motivation: The study aims to bridge the gap between the actual usage of MLOps frameworks and the unaddressed needs of developers regarding their features.

Method: Analyzing GitHub projects' interactions with frameworks and mining feature requests/enhancements from issue trackers to understand usage and user desires.

Result: Results revealed limited out-of-the-box use, rare GitHub Workflow integrations, and reliance on APIs for custom functionalities. Developers focus on core phases and infrastructure while asking for better APIs and CI/CD integrations.

Conclusion: MLOps frameworks require improvements in core features, API exposure, and CI/CD integration to better align with developers' practical needs.

Abstract: Machine Learning (ML) Operations (MLOps) frameworks have been conceived to support developers and AI engineers in managing the lifecycle of their ML models. While such frameworks provide a wide range of features, developers may leverage only a subset of them, while missing some highly desired features. This paper investigates the practical use and desired feature enhancements of eight popular open-source MLOps frameworks. Specifically, we analyze their usage by dependent projects on GitHub, examining how they invoke the frameworks' APIs and commands. Then, we qualitatively analyze feature requests and enhancements mined from the frameworks' issue trackers, relating these desired improvements to the previously identified usage features. Results indicate that MLOps frameworks are rarely used out-of-the-box and are infrequently integrated into GitHub Workflows, but rather, developers use their APIs to implement custom functionality in their projects. Used features concern core ML phases and whole infrastructure governance, sometimes leveraging multiple frameworks with complementary features. The mapping with feature requests highlights that users mainly ask for enhancements to core features of the frameworks, but also better API exposure and CI/CD integration.

</details>


### [594] [Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests](https://arxiv.org/abs/2601.18749)
*Haruhiko Yoshioka,Takahiro Monno,Haruka Tokumasu,Taiki Wakamatsu,Yuki Ota,Nimmi Weeraddana,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: AI agents can automatically generate pull requests (PRs), but their merge rates are lower than human-created PRs. This paper analyzes features affecting merge outcomes for AI and human PRs using a large dataset.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of lower merge rates for AI-generated pull requests compared to human ones, and understand factors influencing PR quality.

Method: Conducting a large-scale empirical analysis on 40,214 PRs from the AIDev dataset, extracting 64 features across six categories, and applying statistical regression models to compare and analyze the merge outcomes.

Result: Submitter attributes strongly influence merge outcomes for both human and AI PRs. Review-related features have different effects on human versus AI PRs, highlighting significant contrasts.

Conclusion: Insights from the study can help improve the quality of PRs through enhanced human-AI collaboration, suggesting areas for optimization.

Abstract: The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [595] [Unsupervised sleep-like intra- and inter-layer plasticity categorizes and improves energy efficiency in a multilayer spiking network](https://arxiv.org/abs/2601.17523)
*Leonardo Tonielli,Cosimo Lupo,Elena Pastorelli,Giulia De Bonis,Francesco Simula,Alessandro Lonardo,Pier Stanislao Paolucci*

Main category: q-bio.NC

TL;DR: This paper explores how sleep-driven inter-layer plasticity in hierarchical neural networks enhances memory consolidation, classification accuracy, and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: The study investigates how synaptic plasticity across brain circuits during sleep can lead to abstraction and improved energy efficiency in memory consolidation.

Method: The researchers use a spiking multi-layer network model based on biological thalamo-cortical dynamics that alternates between wake-like and deep-sleep-like states with plasticity implemented in both intra- and inter-layer connections.

Result: Inter-layer plasticity enhances post-sleep classification accuracy, forms sharper associations, and significantly reduces firing rates, synaptic activity, and power consumption.

Conclusion: The findings suggest the potential for integrating sleep state-inspired plasticity into energy-efficient AI systems for improved learning and energy conservation.

Abstract: Sleep is thought to support memory consolidation and the recovery of optimal energetic regime by reorganizing synaptic connectivity, yet how plasticity across hierarchical brain circuits contributes to abstraction and energy efficiency remains unclear. Here we study a spiking multi-layer network alternating wake-like and deep-sleep-like states, with state-dependent dendritic integration and synaptic plasticity in a biologically inspired thalamo-cortical framework. During wakefulness, the model learns from few perceived examples, while during deep sleep it undergoes spontaneous replay driven by slow oscillations. Plasticity enabled not only within intra-layer connections, but also in inter-layer pathways, is critical for memory consolidation and energetic downshift. Compared to restricted plasticity, full inter-layer plasticity yields higher post-sleep visual classification accuracy and promotes the emergence of sharper class-specific associations. Furthermore, we introduce a biophysically grounded estimator of metabolic power expressing network energy consumption in ATP units, partitioned into baseline, synaptic maintenance, action potential, and transmission costs. We find that inter-layer plasticity in sleep leads to a larger reduction in firing rates, synaptic strength and synaptic activity, corresponding to a substantially larger decrease in power consumption. This work suggests promising elements to be integrated in neuromorphic/energy-efficient AI learning systems, supported by brain state-specific apical mechanisms.

</details>


### [596] [AI and World Models](https://arxiv.org/abs/2601.17796)
*Robert Worden*

Main category: q-bio.NC

TL;DR: The paper argues that large neural networks are inherently unreliable due to the inability to develop a robust theory of their functioning. It emphasizes building 'world models' as guardrails for safety.


<details>
  <summary>Details</summary>
Motivation: Address the intrinsic limitations of large neural nets and explore ways to ensure their safe performance.

Method: The study discusses the necessity of 'world models' encompassing physical, social, and mental domains for reliable AI output.

Result: Highlights the challenges posed by the lack of a stable representation of common ground within large language models.

Conclusion: Reliable AI systems must represent a common ground with users, integrating their understanding of physical, social, and mental aspects.

Abstract: While large neural nets perform impressively on specific tasks, they are unreliable and unsafe, as is shown by the persistent hallucinations of large language models. This paper shows that large neural nets are intrinsically unreliable, because it is not possible to make or validate a tractable theory of how a neural net works. There is no reliable way to extrapolate its performance from a limited number of test cases to an unlimited set of use cases. To have confidence in the performance of a neural net, it is necessary to enclose it in a guardrail which is provably safe, so that whatever the neural net does, there cannot be harmful consequences. World models have been proposed as a way to do this. This paper discusses the scope and architecture required of world models. World models are often conceived as models of the physical and natural world, using established theories of natural science, or learned regularities, to predict the physical consequences of AI actions. However, unforeseen consequences of AI actions impact the human social world as much as the physical world. To predict and control the consequences of AI, a world model needs to include a model of the human social world. I explore the challenges that this entails. Human language is based on a Common Ground of mutual understanding of the world, shared by the people conversing. The common ground is an overlapping subset of each persons world model, including their models of the physical, social and mental worlds. LLMs have no stable representation of a common ground. To be reliable, AI systems will need to represent a common ground with their users, including physical, mental and social domains.

</details>


### [597] [Closed Eyes and Coil Size -- Effects on Motor Threshold and Intracortical Inhibition, measured with TMS](https://arxiv.org/abs/2601.18286)
*Meher Sabharwal,Narin Suleyman,Gabriel R. Palma,Roisin McMackin*

Main category: q-bio.NC

TL;DR: This study investigated the effects of eye state and stimulating coil size on TMS measures (RMT and SICI), finding that eye state does not influence measurements while coil size affects RMT but not SICI.


<details>
  <summary>Details</summary>
Motivation: To determine if experimental factors such as eye state and coil size impact TMS measures, particularly in multi-center studies and to explain contradictory findings in the literature.

Method: Threshold tracking TMS was used in 21 healthy subjects under six conditions, measuring RMT and SICI while varying eye state and coil size.

Result: RMT showed higher values with the smallest coil size; no effect of coil size was seen on SICI. Eye state did not influence RMT or SICI.

Conclusion: RMT and SICI measurements are consistent across eye states and coil sizes for SICI, signaling reliability for multi-center studies.

Abstract: Rationale: Transcranial magnetic stimulation (TMS)-based measures such as resting motor threshold (RMT) and short interval intracortical inhibition (SICI) are widely employed to study motor cortical and corticospinal tract function, and effects of diseases and drug therapies thereon. However, the effect of key experimental factors, including as eye state (open or closed) or stimulating coil size, remain unclear. As such, it is unknown whether these factors must be kept consistent across multi-center studies, and whether differences in such factors may underpin contradictory findings in existing literature.
  Materials and Methods: Threshold tracking TMS was employed to measure RMT and SICI (3ms interstimulus interval, conditioning at 70% of RMT) in 21 alert and awake, healthy controls. Motor evoked potentials were recorded from abductor pollicis brevis. Both RMT and SICI were measured under 6 conditions, while eyes were open or closed, using 3 figure-of-eight coils of differing winding diameter. Mixed effects modelling was employed to investigate effects of eye state and coil size on each measure.
  Results: RMT was found to be significantly higher for the smallest (30BFT) coil compared to both larger (50BFT and 70BF) coils. No difference in SICI was identified across coil sizes. Eye state was not found to affect either RMT or SICI measurements.
  Conclusions: Measurements of RMT and SICI can be considered comparable if recorded with eyes open or closed, provided the individual is awake and alert. Measurements of SICI recorded with figure-of-eight coils of different size can be considered comparable.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [598] [Data-Driven Information-Theoretic Causal Bounds under Unmeasured Confounding](https://arxiv.org/abs/2601.17160)
*Yonghan Jung,Bogyeong Kang*

Main category: stat.ML

TL;DR: The paper develops an information-theoretic framework for partial identification of causal effects with unmeasured confounding, overcoming key limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of current methods which rely on restrictive assumptions, require external inputs, or neglect covariate-conditional effects when analyzing causal effects under unmeasured confounding.

Method: Introduces novel information-theoretic divergence bounds based on f-divergence between observational and interventional distributions, using propensity scores, without additional external parameters. Implements a semiparametric estimator for consistent inference with machine learning.

Result: The proposed framework provides sharp and valid causal bounds, validated by simulations and real-world examples, demonstrating reliable performance across diverse data scenarios.

Conclusion: This framework enables robust partial identification of causal effects directly from observational data, eliminating the need for external inputs and restrictive assumptions, and offering practical implementation options.

Abstract: We develop a data-driven information-theoretic framework for sharp partial identification of causal effects under unmeasured confounding. Existing approaches often rely on restrictive assumptions, such as bounded or discrete outcomes; require external inputs (for example, instrumental variables, proxies, or user-specified sensitivity parameters); necessitate full structural causal model specifications; or focus solely on population-level averages while neglecting covariate-conditional treatment effects. We overcome all four limitations simultaneously by establishing novel information-theoretic, data-driven divergence bounds. Our key theoretical contribution shows that the f-divergence between the observational distribution P(Y | A = a, X = x) and the interventional distribution P(Y | do(A = a), X = x) is upper bounded by a function of the propensity score alone. This result enables sharp partial identification of conditional causal effects directly from observational data, without requiring external sensitivity parameters, auxiliary variables, full structural specifications, or outcome boundedness assumptions. For practical implementation, we develop a semiparametric estimator satisfying Neyman orthogonality (Chernozhukov et al., 2018), which ensures square-root-n consistent inference even when nuisance functions are estimated using flexible machine learning methods. Simulation studies and real-world data applications, implemented in the GitHub repository (https://github.com/yonghanjung/Information-Theretic-Bounds), demonstrate that our framework provides tight and valid causal bounds across a wide range of data-generating processes.

</details>


### [599] [Error Analysis of Bayesian Inverse Problems with Generative Priors](https://arxiv.org/abs/2601.17374)
*Bamdad Hosseini,Ziqi Huang*

Main category: stat.ML

TL;DR: The paper analyzes data-driven inverse problem solutions using generative models, presenting error bounds and assumptions for minimum Wasserstein-2 priors, with numerical experiments showcasing these results.


<details>
  <summary>Details</summary>
Motivation: To improve solutions to inverse problems by leveraging generative models as priors and providing mathematical guarantees through error bounds.

Method: Analyzing error bounds quantitatively for minimum Wasserstein-2 generative models and verifying them with numerical experiments, including applications to elliptic PDE inverse problems.

Result: The analysis shows the error in the posterior inherits the prior's error rate under Wasserstein-1 distance assumptions, supported by numerical validation.

Conclusion: Generative models, when used with proper assumptions, can effectively solve inverse problems with quantifiable error rates, as validated by experiments.

Abstract: Data-driven methods for the solution of inverse problems have become widely popular in recent years thanks to the rise of machine learning techniques. A popular approach concerns the training of a generative model on additional data to learn a bespoke prior for the problem at hand. In this article we present an analysis for such problems by presenting quantitative error bounds for minimum Wasserstein-2 generative models for the prior. We show that under some assumptions, the error in the posterior due to the generative prior will inherit the same rate as the prior with respect to the Wasserstein-1 distance. We further present numerical experiments that verify that aspects of our error analysis manifests in some benchmarks followed by an elliptic PDE inverse problem where a generative prior is used to model a non-stationary field.

</details>


### [600] ["Rebuilding" Statistics in the Age of AI: A Town Hall Discussion on Culture, Infrastructure, and Training](https://arxiv.org/abs/2601.17510)
*David L. Donoho,Jian Kang,Xihong Lin,Bhramar Mukherjee,Dan Nettleton,Rebecca Nugent,Abel Rodriguez,Eric P. Xing,Tian Zheng,Hongtu Zhu*

Main category: stat.ML

TL;DR: The paper provides a detailed record of the 2024 JSM town hall titled "Statistics in the Age of AI," focusing on how statistics adapts to AI advancements and related challenges.


<details>
  <summary>Details</summary>
Motivation: To document and promote reflection on the evolving role of statistics in response to AI, foundation models, and data-driven shifts, enabling community insight and future dialogue.

Method: Detailed transcription and organization of the discussion among statisticians, structured around open panel talks, audience Q&A, and minimal editorial intervention.

Result: The paper organizes the town hall's discourse around five recurring thematic questions, preserving candid exchanges about disciplinary practices, training for AI, and collaboration in modern modeling.

Conclusion: By archiving the discussion, the work fosters transparency and serves as a community resource for understanding the interplay between statistics and AI advancements.

Abstract: This article presents the full, original record of the 2024 Joint Statistical Meetings (JSM) town hall, "Statistics in the Age of AI," which convened leading statisticians to discuss how the field is evolving in response to advances in artificial intelligence, foundation models, large-scale empirical modeling, and data-intensive infrastructures. The town hall was structured around open panel discussion and extensive audience Q&A, with the aim of eliciting candid, experience-driven perspectives rather than formal presentations or prepared statements. This document preserves the extended exchanges among panelists and audience members, with minimal editorial intervention, and organizes the conversation around five recurring questions concerning disciplinary culture and practices, data curation and "data work," engagement with modern empirical modeling, training for large-scale AI applications, and partnerships with key AI stakeholders. By providing an archival record of this discussion, the preprint aims to support transparency, community reflection, and ongoing dialogue about the evolving role of statistics in the data- and AI-centric future.

</details>


### [601] [Boosting methods for interval-censored data with regression and classification](https://arxiv.org/abs/2601.17973)
*Yuan Bian,Grace Y. Yi,Wenqing He*

Main category: stat.ML

TL;DR: Introduces novel boosting methods tailored for regression and classification problems that handle interval-censored data, with demonstrated theoretical and empirical effectiveness.


<details>
  <summary>Details</summary>
Motivation: Traditional boosting methods struggle with interval-censored data, which is prevalent in fields like survival analysis, requiring new approaches to improve predictive accuracy.

Method: Developed nonparametric boosting methods using censoring unbiased transformations to adjust loss functions and impute responses, implemented through functional gradient descent.

Result: Proposed methods showed robustness in theoretical properties and empirical performance in handling interval-censored data effectively.

Conclusion: The presented methods provide a scalable, robust solution for boosting with interval-censored data, complementing and extending traditional algorithms' applications.

Abstract: Boosting has garnered significant interest across both machine learning and statistical communities. Traditional boosting algorithms, designed for fully observed random samples, often struggle with real-world problems, particularly with interval-censored data. This type of data is common in survival analysis and time-to-event studies where exact event times are unobserved but fall within known intervals. Effective handling of such data is crucial in fields like medical research, reliability engineering, and social sciences. In this work, we introduce novel nonparametric boosting methods for regression and classification tasks with interval-censored data. Our approaches leverage censoring unbiased transformations to adjust loss functions and impute transformed responses while maintaining model accuracy. Implemented via functional gradient descent, these methods ensure scalability and adaptability. We rigorously establish their theoretical properties, including optimality and mean squared error trade-offs. Our proposed methods not only offer a robust framework for enhancing predictive accuracy in domains where interval-censored data are common but also complement existing work, expanding the applicability of existing boosting techniques. Empirical studies demonstrate robust performance across various finite-sample scenarios, highlighting the practical utility of our approaches.

</details>


### [602] [A Cherry-Picking Approach to Large Load Shaping for More Effective Carbon Reduction](https://arxiv.org/abs/2601.17990)
*Bokan Chen,Raiden Hasegawa,Adriaan Hilbers,Ross Koningstein,Ana Radovanović,Utkarsh Shah,Gabriela Volpato,Mohamed Ahmed,Tim Cary,Rod Frowd*

Main category: stat.ML

TL;DR: The paper examines load shaping strategies' impact on CO2 emissions and costs using DC-OPF simulations, proposing a 'cherry-picking' approach for better performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve the effectiveness of load shaping strategies in reducing CO2 emissions and electricity costs in systems with large energy consumers.

Method: Day-ahead DC-OPF simulations in ERCOT to conduct counterfactual analysis on various load shaping strategies.

Result: The study finds that while LMP-based load shaping offers better CO2 emission reductions compared to other strategies, a 'cherry-picking' approach can achieve even greater improvements.

Conclusion: The 'cherry-picking' strategy, based on observable grid signals and historical data, offers a more effective and broadly applicable solution for load shaping, benefiting large consumers like data centers and Virtual Power Plants.

Abstract: Shaping multi-megawatt loads, such as data centers, impacts generator dispatch on the electric grid, which in turn affects system CO2 emissions and energy cost. Substantiating the effectiveness of prevalent load shaping strategies, such as those based on grid-level average carbon intensity, locational marginal price, or marginal emissions, is challenging due to the lack of detailed counterfactual data required for accurate attribution. This study uses a series of calibrated granular ERCOT day-ahead direct current optimal power flow (DC-OPF) simulations for counterfactual analysis of a broad set of load shaping strategies on grid CO2 emissions and cost of electricity. In terms of annual grid level CO2 emissions reductions, LMP-based shaping outperforms other common strategies, but can be significantly improved upon. Examining the performance of practicable strategies under different grid conditions motivates a more effective load shaping approach: one that "cherry-picks" a daily strategy based on observable grid signals and historical data. The cherry-picking approach to power load shaping is applicable to any large flexible consumer on the electricity grid, such as data centers, distributed energy resources and Virtual Power Plants (VPPs).

</details>


### [603] [Nonlinear multi-study factor analysis](https://arxiv.org/abs/2601.18128)
*Gemma E. Moran,Anandi Krishnan*

Main category: stat.ML

TL;DR: The paper addresses the challenge of understanding shared and specific factors in high-dimensional multi-study data using a sparse variational autoencoder (VAE).


<details>
  <summary>Details</summary>
Motivation: The motivation is to analyze high-dimensional data across various environments or studies to identify and distinguish shared and environment-specific factors, with a focus on complex cases like platelet gene expression data.

Method: The paper employs a nonlinear multi-study factor model implemented through a multi-study sparse variational autoencoder that promotes sparsity and identifies relationships between features and latent factors.

Result: The method ensures that latent factors are identifiable and recovers biologically meaningful factors in platelet gene expression data.

Conclusion: The proposed sparse variational autoencoder is successful in distinguishing shared and specific latent factors and demonstrates its utility in genomics applications by uncovering meaningful biological insights.

Abstract: High-dimensional data often exhibit variation that can be captured by lower dimensional factors. For high-dimensional data from multiple studies or environments, one goal is to understand which underlying factors are common to all studies, and which factors are study or environment-specific. As a particular example, we consider platelet gene expression data from patients in different disease groups. In this data, factors correspond to clusters of genes which are co-expressed; we may expect some clusters (or biological pathways) to be active for all diseases, while some clusters are only active for a specific disease. To learn these factors, we consider a nonlinear multi-study factor model, which allows for both shared and specific factors. To fit this model, we propose a multi-study sparse variational autoencoder. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. In the genomics example, this means each gene is active in only a few biological processes. Further, the model implicitly induces a penalty on the number of latent factors, which helps separate the shared factors from the group-specific factors. We prove that the latent factors are identified, and demonstrate our method recovers meaningful factors in the platelet gene expression data.

</details>


### [604] [Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes](https://arxiv.org/abs/2601.18145)
*Heguang Lin,Binhao Chen,Mengze Li,Daniel Pimentel-Alarcón,Matthew L. Malloy*

Main category: stat.ML

TL;DR: The paper focuses on certifying whether minimum-volume confidence sets (MVCs) for multinomial parameters intersect, introducing a tolerance-aware algorithm, particularly useful for A/B testing.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in computing the geometry of MVCs, which are essential for data science tasks, and provide reliable decision-making for intersecting confidence sets.

Method: Proposes a certified algorithm that uses adaptive geometric partitioning of parameter space in log-odds coordinates, computes bounds on p-values, and handles intersection certification with efficiency and reliability.

Result: The algorithm is provably sound, ensuring certification of intersection or separation of MVCs for three or more categories, achieving practical performance in A/B testing decision problems.

Conclusion: MVCs, despite their complicated geometry, can be effectively analyzed for intersection or disjointness, contributing to decision-making in data science and machine learning tasks.

Abstract: Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing.

</details>


### [605] [Out-of-Distribution Radar Detection with Complex VAEs: Theory, Whitening, and ANMF Fusion](https://arxiv.org/abs/2601.18677)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: stat.ML

TL;DR: This paper investigates using a Complex-valued Variational AutoEncoder (CVAE) for detecting weak signals in maritime radar settings with non-Gaussian interference, showing improved detection performance over classical methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of detecting weak complex-valued signals in maritime radar scenarios where signals are obscured by non-Gaussian and range-varying interference.

Method: The authors propose using a CVAE trained specifically on clutter-plus-noise to detect out-of-distribution signals. The method retains phase and Doppler structure and is tested in two configurations: unprocessed range profiles and profiles that undergo local whitening normalization. Additionally, a fusion with ANMF detection is explored.

Result: The CVAE demonstrates higher detection probabilities at a matched false-alarm rate across simulations and real maritime clutter data, with notable performance gains using statistical normalization (whitening). The integration with ANMF improves robustness and empirically controlled false-alarm rates.

Conclusion: Using statistical normalization and complex-valued generative modeling significantly enhances signal detection in non-Gaussian maritime clutter conditions, with the CVAE-ANMF fusion offering a competitive alternative to traditional detectors.

Abstract: We investigate the detection of weak complex-valued signals immersed in non-Gaussian, range-varying interference, with emphasis on maritime radar scenarios. The proposed methodology exploits a Complex-valued Variational AutoEncoder (CVAE) trained exclusively on clutter-plus-noise to perform Out-Of-Distribution detection. By operating directly on in-phase / quadrature samples, the CVAE preserves phase and Doppler structure and is assessed in two configurations: (i) using unprocessed range profiles and (ii) after local whitening, where per-range covariance estimates are obtained from neighboring profiles. Using extensive simulations together with real sea-clutter data from the CSIR maritime dataset, we benchmark performance against classical and adaptive detectors (MF, NMF, AMF-SCM, ANMF-SCM, ANMF-Tyler). In both configurations, the CVAE yields a higher detection probability Pd at matched false-alarm rate Pfa, with the most notable improvements observed under whitening. We further integrate the CVAE with the ANMF through a weighted log-p fusion rule at the decision level, attaining enhanced robustness in strongly non-Gaussian clutter and enabling empirically calibrated Pfa control under H0. Overall, the results demonstrate that statistical normalization combined with complex-valued generative modeling substantively improves detection in realistic sea-clutter conditions, and that the fused CVAE-ANMF scheme constitutes a competitive alternative to established model-based detectors.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [606] [Revealing Latent Self-Similarity in Cellular Automata via Recursive Gradient Profiling](https://arxiv.org/abs/2601.17361)
*Chung-En Hao,Ivan C. H. Liu*

Main category: nlin.CG

TL;DR: The paper improves visualization of the Ulam-Warburton Cellular Automaton (UWCA) to reveal underlying fractal-like structures using a Recursive Gradient Profile Function (RGPF).


<details>
  <summary>Details</summary>
Motivation: Conventional binary renderings of UWCA obscure self-similar fractal structures, limiting the visual and scientific understanding of its spatial evolution.

Method: A Recursive Gradient Profile Function (RGPF) assigns grayscale values to activated cells based on their generation index, enhancing visualization of fractal patterns.

Result: Fractal-like visual patterns emerge across scales, and similar outcomes are observed when applying this technique to UWCA variants with different neighborhood configurations.

Conclusion: The RGPF enhances fractal visualization in automata, demonstrating a connection between computational science and cultural aesthetics in art and design.

Abstract: Cellular automata (CA), originally developed as computational models of natural processes, have become a central subject in the study of complex systems and generative visual forms. Among them, the Ulam-Warburton Cellular Automaton (UWCA) exhibits recursive growth and fractal-like characteristics in its spatial evolution. However, exact self-similar fractal structures are typically observable only at specific generations and remain visually obscured in conventional binary renderings. This study introduces a Recursive Gradient Profile Function (RGPF) that assigns grayscale values to newly activated cells according to their generation index, enabling latent self-similar structures to emerge cumulatively in spatial visualizations. Through this gradient-based mapping, recursive geometric patterns become perceptible across scales, revealing fractal properties that are not apparent in standard representations. We further extend this approach to UWCA variants with alternative neighborhood configurations, demonstrating that these rules also produce distinct yet consistently fractal visual patterns when visualized using recursive gradient profile. Beyond computational analysis, the resulting generative forms resonate with optical and cultural phenomena such as infinity mirrors, video feedback, and mise en abyme in European art history, as well as fractal motifs found in religious architecture. These visual correspondences suggest a broader connection between complexity science, computational visualization, and cultural art and design.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [607] [On the Extension of Private Distributed Matrix Multiplication Schemes to the Grid Partition](https://arxiv.org/abs/2601.17834)
*Christoph Hofmeister,Razane Tajeddine,Antonia Wachter-Zeh,Rawad Bitar*

Main category: cs.IT

TL;DR: The paper designs extension operations to enhance polynomial codes in private distributed matrix multiplication (PDMM), improving general grid partitioning performance beyond the state-of-the-art for certain parameters.


<details>
  <summary>Details</summary>
Motivation: To extend the applicability and improve upon existing polynomial codes used in PDMM, particularly for generalized grid partitioning cases where current approaches are limited.

Method: The authors propose extension operations to transform outer product partitioning (OPP) designs into general grid partitioning (GP) schemes and also introduce a new GP scheme without the combinatorial constraints of existing methods.

Result: The proposed methods enhance performance over the state-of-the-art codes for certain parameter ranges, demonstrating the advantages of using the new GP scheme.

Conclusion: The study introduces tools for adapting OPP codes to GP and identifies combinatorial constraints that limit GP scheme performance. The new GP design breaks these constraints and outperforms existing approaches, opening avenues for further optimization.

Abstract: We consider polynomial codes for private distributed matrix multiplication (PDMM/SDMM). Existing codes for PDMM are either specialized for the outer product partitioning (OPP), or inner product partitioning (IPP), or are valid for the more general grid partitioning (GP). We design extension operations that can be applied to a large class of OPP code designs to extend them to the GP case. Applying them to existing codes improves upon the state-of-the-art for certain parameters. Additionally, we show that the GP schemes resulting from extension fulfill additional combinatorial constraints, potentially limiting their performance. We illustrate this point by presenting a new GP scheme that does not adhere to these constraints and outperforms the state-of-the-art for a range of parameters.

</details>


### [608] [High-Rate Quantized Matrix Multiplication: Theory and Practice](https://arxiv.org/abs/2601.17187)
*Or Ordentlich,Yury Polyanskiy*

Main category: cs.IT

TL;DR: The paper investigates quantized matrix multiplication (MatMul), focusing on both generic and weight-only quantization. It evaluates quantization schemes, proposes a new method called WaterSIC, and assesses its performance against information-theoretic limits.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for efficient quantization techniques for large language models (LLMs) to optimize storage and computational efficiency.

Method: The study analyzes quantization schemes by combining information-theoretic trade-offs with practical algorithms like GPTQ. It introduces WaterSIC, a method leveraging waterfilling principles for weight-only quantization, ensuring high performance without dependency on matrix basis.

Result: The WaterSIC method achieves distortion performance close to the theoretical limit and is robust against basis changes. GPTQ, with random rotations, also achieves comparable near-optimal performance based on empirical evaluations.

Conclusion: WaterSIC demonstrates an efficient and nearly optimal approach for matrix quantization, providing insights for LLM quantization. GPTQ algorithms already perform near optimal with specific adjustments.

Abstract: This work investigates the problem of quantized matrix multiplication (MatMul), which has become crucial for the efficient deployment of large language models (LLMs). We consider two settings: 1) Generic MatMul, where both matrices must be quantized (weight+activation quantization); and 2) weight-only quantization, where the second matrix is only known through covariance matrix $Σ_X$ of its columns. For each setting, we first review the fundamental information-theoretic tradeoff between quantization rate and distortion (high-rate theory), and then analyze the performance of several popular quantization schemes, comparing them to these fundamental limits. Specifically, we discuss rate loss (compared to information theoretic optima) of absmax INT and floating-point (FP) quantization, for which we also derive remarkably accurate heuristic approximations. Weight-only quantization is related to the problem of weighted mean squared error (WMSE) source coding, whose classical (reverse) waterfilling solution dictates how one should distribute rate between coordinates of the vector. We show how waterfilling can be used to improve practical LLM quantization algorithms (GPTQ), which at present allocate rate equally. This new scheme (termed ``WaterSIC'') only uses scalar INT quantizers, but its high-rate performance is basis free (it depends only on the determinant of $Σ_X$ and, thus, unlike existing schemes, is immune to applying random rotations) and is within a multiplicative factor of $\frac{2πe}{12}$ (or 0.25 bit/entry) of the information-theoretic distortion limit (!). GPTQ's performance is affected by the choice of basis, but for a random rotation and actual $Σ_X$ from Llama-3-8B we find GPTQ to be within 0.1 bit (depending on the layer type) of WaterSIC, suggesting that GPTQ with random rotation is also near optimal (for high-rate quantization).

</details>


### [609] [A Model-Driven Lossless Compression Algorithm Resistant to Mismatch](https://arxiv.org/abs/2601.17684)
*Cordelia Hu,Jennifer Tang*

Main category: cs.IT

TL;DR: The paper introduces a new compression algorithm based on next-token prediction that is robust to prediction mismatches, showing high compression performance.


<details>
  <summary>Details</summary>
Motivation: Existing compression methods using predictive models like LLMs fail due to reliance on deterministic outputs, which commonly results in decoding issues caused by non-determinism.

Method: The authors propose a mismatch-tolerant compression algorithm, formally certify its correctness under mismatches, analyze its theoretical performance, and validate it experimentally.

Result: The proposed method operates reliably under certified mismatch conditions, exceeding the compression performance of standard methods.

Conclusion: The study provides a robust compression algorithm capable of overcoming prediction mismatches with superior compression ratios compared to traditional approaches.

Abstract: Due to the fundamental connection between next-symbol prediction and compression, modern predictive models, such as large language models (LLMs), can be combined with entropy coding to achieve compression rates that surpass those of standard compression algorithms. However, this approach relies on the assumption that the predictive model produces identical output distributions at both the encoder and decoder, since even small mismatches can cause the decoding to fail. This assumption often fails with complex predictive models, particularly those based on neural networks, a phenomenon referred to as non-determinism.
  In this work, we propose a new compression algorithm based on next-token prediction that is robust to arbitrarily large, but structured, prediction mismatches. We prove the correctness of the proposed scheme under a formal mismatch certification, characterize its theoretical performance, and validate it experimentally on real datasets. Our results demonstrate reliable operation within the certified mismatch regime while achieving compression ratios that exceed those of commonly used compression methods.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [610] [EveNet: A Foundation Model for Particle Collision Data Analysis](https://arxiv.org/abs/2601.17126)
*Ting-Hsiang Hsu,Bai-Hong Zhou,Qibin Liu,Yue Xu,Shu Li,George Wei-Shu Hou,Benjamin Nachman,Shih-Chieh Hsu,Vinicius Mikuni,Yuan-Tang Chou,Yulei Zhang*

Main category: hep-ex

TL;DR: EveNet, a deep learning model, was developed to tackle computational challenges in collider physics, showcasing superior performance and data efficiency across a range of particle physics tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome computational constraints in using deep learning for high-energy physics, specifically in collider physics, to enhance performance and achieve versatile applicability.

Method: The authors developed EveNet, a foundation model pretrained on 500 million simulated particle collision events using a hybrid learning approach (self-supervised and physics-informed supervision), built on a shared particle-cloud representation.

Result: EveNet surpassed state-of-the-art models in tasks such as searching for heavy resonances, exotic Higgs decays, and working efficiently with low-statistics data. It was validated with experimental data through rediscovery of the $Υ$ meson and by extracting stable quantum correlation observables.

Conclusion: EveNet successfully captures fundamental particle interaction structures, presenting a unified model that could accelerate discoveries and enhance resource efficiency in both current and future collider experiments.

Abstract: While deep learning is transforming data analysis in high-energy physics, computational challenges limit its potential. We address these challenges in the context of collider physics by introducing EveNet, an event-level foundation model pretrained on 500 million simulated collision events using a hybrid objective of self-supervised learning and physics-informed supervision. By leveraging a shared particle-cloud representation, EveNet outperforms state-of-the-art baselines across diverse tasks, including searches for heavy resonances and exotic Higgs decays, and demonstrates exceptional data efficiency in low-statistics regimes. Crucially, we validate the transferability of the model to experimental data by rediscovering the $Υ$ meson in CMS Open Data and show its capacity for precision physics through the robust extraction of quantum correlation observables stable against systematic uncertainties. These results indicate that EveNet can successfully encode the fundamental physical structure of particle interactions, which offers a unified and resource-efficient framework to accelerate discovery at current and future colliders.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [611] [LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction](https://arxiv.org/abs/2601.18475)
*Xinhui Liu,Can Wang,Lei Liu,Zhenghao Chen,Wei Jiang,Wei Wang,Dong Xu*

Main category: cs.GR

TL;DR: The paper proposes StreamLoD-GS, a novel framework enhancing Free-Viewpoint Video (FVV) streaming by overcoming challenges like sparse-view inputs, training costs, and bandwidth issues, and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in Free-Viewpoint Video streaming, such as inefficiencies with sparse-view inputs, high training costs, bandwidth constraints, and the demands of real-time optimization and efficient storage.

Method: The authors introduce StreamLoD-GS, which incorporates (1) an Anchor- and Octree-based Level-of-Detail (LoD) structure with hierarchical Gaussian dropout for efficient optimization, (2) a GMM-based motion partitioning for dynamic/static regions, and (3) a quantized residual refinement to reduce storage demands.

Result: StreamLoD-GS demonstrates state-of-the-art or competitive performance in terms of rendering quality, efficiency, and storage reduction compared to other FVV techniques.

Conclusion: StreamLoD-GS represents a significant advancement in 3DGS and FVV streaming by achieving both high visual fidelity and efficiency through innovative design. It effectively addresses previously limiting factors in real-time video rendering and storage optimization.

Abstract: Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints. While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints. To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV. Our approach integrates three core innovations: 1) an Anchor- and Octree-based LoD-structured 3DGS with a hierarchical Gaussian dropout technique to ensure efficient and stable optimization while maintaining high-quality rendering; 2) a GMM-based motion partitioning mechanism that separates dynamic and static content, refining dynamic regions while preserving background stability; and 3) a quantized residual refinement framework that significantly reduces storage requirements without compromising visual fidelity. Extensive experiments demonstrate that StreamLoD-GS achieves competitive or state-of-the-art performance in terms of quality, efficiency, and storage.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [612] [Context Lake: A System Class Defined by Decision Coherence](https://arxiv.org/abs/2601.17019)
*Xiaowei Jiang*

Main category: cs.DB

TL;DR: This paper introduces the Decision Coherence Law for AI agents, highlighting a need for systems to guarantee correctness for immediate, irreversible interactions among agents and proposes a new system class called Context Lake as a solution.


<details>
  <summary>Details</summary>
Motivation: AI agents increasingly make complex, concurrent, and irreversible decisions that involve interacting over shared resources, leading to a demand for correctness guarantees that current data systems cannot satisfy.

Method: The paper derives and proves the Decision Coherence Law and the Composition Impossibility Theorem, then proposes the concept of Context Lake as the necessary system class with semantic operations, transactional consistency, and operational envelopes.

Result: The authors prove that no current system satisfies Decision Coherence requirements and demonstrate the architectural principles a Context Lake must meet for AI agent interactions.

Conclusion: Context Lakes are necessary to address the limitations of current systems and enable AI agents to make decisions that are coherent and correct in real-time, especially at scale.

Abstract: AI agents are increasingly the primary consumers of data, operating continuously to make concurrent, irreversible decisions. Traditional data systems designed for human analysis cycles become correctness bottlenecks under this operating regime. When multiple agents operate over shared resources, their actions interact before reconciliation is possible. Correctness guarantees that apply after the decision window therefore fail to prevent conflicts. We introduce the Decision Coherence Law: for agents that take irreversible actions whose effects interact, correctness requires that interacting decisions be evaluated against a coherent representation of reality at the moment they are made. We show that no existing system class satisfies this requirement and prove through the Composition Impossibility Theorem that independently advancing systems cannot be composed to provide Decision Coherence while preserving their native system classes. From this impossibility result, we derive Context Lake as a necessary system class with three requirements: (1) semantic operations as native capabilities, (2) transactional consistency over all decision-relevant state, and (3) operational envelopes bounding staleness and degradation under load. We formalize the architectural invariants, enforcement boundaries, and admissibility conditions required for correctness in collective agent systems. This position paper establishes the theoretical foundation for Context Lakes, identifies why existing architectures fail, and specifies what systems must guarantee for AI agents to operate constructively at scale.

</details>


### [613] [Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs](https://arxiv.org/abs/2601.17058)
*Wei Zhou,Jun Zhou,Haoyu Wang,Zhenghao Li,Qikang He,Shaokun Han,Guoliang Li,Xuanhe Zhou,Yeye He,Chunwei Liu,Zirui Tang,Bin Wang,Shen Tang,Kai Zuo,Yuyu Luo,Zhenzhe Zheng,Conghui He,Jingren Zhou,Fan Wu*

Main category: cs.DB

TL;DR: This paper reviews the use of LLM techniques for data preparation, providing taxonomy, highlighting challenges, and proposing future directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is driven by the need for application-ready data, advances in LLM, and new infrastructures for flexible agent construction.

Method: The method includes a systematic review, task-centric taxonomy of LLM-enhanced data preparation tasks, analysis of techniques, datasets, evaluation metrics, and roadmap proposal.

Result: The paper organizes data preparation into three tasks: cleaning, integration, enrichment, highlighting strengths and weaknesses of LLM techniques, and identifies open challenges.

Conclusion: LLM-enhanced data preparation holds great potential but faces scalability, reliability, and evaluation challenges; future efforts should focus on scalable systems, principled workflows, and robust evaluations.

Abstract: Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.
  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [614] [Invited: Toward Sustainable and Transparent Benchmarking for Academic Physical Design Research](https://arxiv.org/abs/2601.17520)
*Liwen Jiang,Andrew B. Kahng,Zhiang Wang,Zhiyu Zheng*

Main category: eess.SY

TL;DR: The paper introduces RosettaStone 2.0, a standardized RTL-to-GDS benchmark framework for 2D and 3D designs, supporting reproducible research with integrated evaluation pipelines and community governance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to provide a comprehensive and standardized translation and evaluation framework to facilitate rigorous benchmarking and reproducible research in 2D and 3D design implementations.

Method: The framework features RTL-to-GDS reference flows for both 2D and 3D designs, integrated with OpenROAD-flow-scripts (ORFS)-Research. It includes CI-based regression testing, a standardized METRICS2.1 pipeline, structured reporting and a community leaderboard to enforce research transparency.

Result: RosettaStone 2.0 enables apples-to-apples comparison between planar and 3D design approaches, provides CI-based structured reports, and fosters transparency and collaboration via a community leaderboard verified by pull requests.

Conclusion: The RosettaStone 2.0 framework improves benchmark standardization, supports reproducible research, and facilitates rigorous comparisons of design implementations across 2D and 3D paradigms.

Abstract: This paper presents RosettaStone 2.0, an open benchmark translation and evaluation framework built on OpenROAD-Research. RosettaStone 2.0 provides complete RTL-to-GDS reference flows for both conventional 2D designs and Pin-3D-style face-to-face (F2F) hybrid-bonded 3D designs, enabling rigorous apples-to-apples comparison across planar and three-dimensional implementation settings. The framework is integrated within OpenROAD-flow-scripts (ORFS)-Research; it incorporates continuous integration (CI)-based regression testing and provides a standardized evaluation pipeline based on the METRICS2.1 convention, with structured logs and reports generated by ORFS-Research. To support transparent and reproducible research, RosettaStone 2.0 further provides a community-facing leaderboard, which is governed by verified pull requests and enforced through Developer Certificate of Origin (DCO) compliance.

</details>


### [615] [Autonomous Mars Rover Module for Soil Sampling and Life Component Analysis](https://arxiv.org/abs/2601.17158)
*Bibek Adhikari,Rishab Rijal,Rakesh Yadav,Nikchey Khatri,Sandesh Dhakal*

Main category: eess.SY

TL;DR: This paper develops an autonomous system for Mars rovers to detect extraterrestrial life through soil sample analysis.


<details>
  <summary>Details</summary>
Motivation: To address the scientific drive for discovering extraterrestrial life, particularly due to the discovery of water on Mars.

Method: Designed a proof-of-life module integrated into Mars rovers, combining mechanics like excavation systems with onboard biochemical testing.

Result: Successfully demonstrated the module's capability to detect life components in Martian soil samples.

Conclusion: The study showcases the potential for autonomous life detection systems in extraterrestrial missions, creating a basis for future exploration technologies.

Abstract: The search for extraterrestrial life has long been a primary focus of scientific exploration, driven by rapid advancements in technology and our understanding of the universe. The discovery of water on Mars has sparked significant interest, raising the question of whether life could exist on the planet. This study proposes a novel approach to simulate and illustrate the detection of life using a proof-of-life module integrated into a Mars rover. The module is an autonomous system capable of traveling to designated regions, excavating soil, collecting samples, and performing biochemical testing onboard the rover itself. The project is inherently multidisciplinary, integrating mechanical systems such as a drill mechanism and a vacuum system, alongside biochemical analysis for soil testing. The module is capable of successfully detecting the presence or absence of living components of life from the collected soil particles. This proof-of-life module serves as a proof-of-concept for autonomous life detection in extraterrestrial environments and lays the foundation for future exploration missions.

</details>


### [616] [A new approach for combined model class selection and parameters learning for auto-regressive neural models](https://arxiv.org/abs/2601.17442)
*Corrado Sgadari,Alessio La Bella,Marcello Farina*

Main category: eess.SY

TL;DR: The paper presents a novel method for jointly selecting model structures and parameters for nonlinear dynamical system identification using a specific RNN variant, NARXESNs, leveraging a new set-membership-based procedure.


<details>
  <summary>Details</summary>
Motivation: Improving the process of selecting and training accurate, parsimonious models for nonlinear dynamical systems, particularly in the face of challenges such as bounded measurement noise and model robustness.

Method: Developed a set-membership (SM) approach to simultaneously select model structures and learn parameters for RNNs, specifically NARXESNs, while ensuring the process handles measurement noise and evaluates performance robustly.

Result: The approach effectively identifies models that balance parsimony and accuracy, making them suitable for control applications and enhancing robustness during training.

Conclusion: The proposed SM-based method facilitates robust and efficient model identification and parameter learning for nonlinear dynamical systems, addressing key challenges like noise and complexity in simulation performance evaluation.

Abstract: This work introduces a novel approach for the joint selection of model structure and parameter learning for nonlinear dynamical systems identification. Focusing on a specific Recurrent Neural Networks (RNNs) family, i.e., Nonlinear Auto-Regressive with eXogenous inputs Echo State Networks (NARXESNs), the method allows to simultaneously select the optimal model class and learn model parameters from data through a new set-membership (SM) based procedure. The results show the effectiveness of the approach in identifying parsimonious yet accurate models suitable for control applications. Moreover, the proposed framework enables a robust training strategy that explicitly accounts for bounded measurement noise and enhances model robustness by allowing data-consistent evaluation of simulation performance during parameter learning, a process generally NP-hard for models with autoregressive components.

</details>


### [617] [Convex Chance-Constrained Stochastic Control under Uncertain Specifications with Application to Learning-Based Hybrid Powertrain Control](https://arxiv.org/abs/2601.18313)
*Teruki Kato,Ryotaro Shima,Kenji Kashima*

Main category: eess.SY

TL;DR: The paper proposes a strictly convex stochastic control framework for systems under uncertainty, enabling probabilistic constraint satisfaction with uniqueness and continuity of solutions.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty in control specifications and ensure reliable and optimal system behavior under general uncertainties, even in non-Gaussian cases.

Method: Joint optimization of control inputs and risk allocation, with an extension to nonlinear control via linearizable models identified through machine learning.

Result: Achieves probabilistic constraint satisfaction while maintaining strict convexity and is demonstrated using a hybrid powertrain system in model predictive control.

Conclusion: The proposed method effectively handles uncertainties in control systems, guaranteeing optimality and constraint satisfaction, with practical success shown in a hybrid powertrain context.

Abstract: This paper presents a strictly convex chance-constrained stochastic control framework that accounts for uncertainty in control specifications such as reference trajectories and operational constraints. By jointly optimizing control inputs and risk allocation under general (possibly non-Gaussian) uncertainties, the proposed method guarantees probabilistic constraint satisfaction while ensuring strict convexity, leading to uniqueness and continuity of the optimal solution. The formulation is further extended to nonlinear model-based control using exactly linearizable models identified through machine learning. The effectiveness of the proposed approach is demonstrated through model predictive control applied to a hybrid powertrain system.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [618] [Over-The-Air Extreme Learning Machines with XL Reception via Nonlinear Cascaded Metasurfaces](https://arxiv.org/abs/2601.17749)
*Kyriakos Stylianopoulos,Mattia Fabiani,Giulia Torcolacci,Davide Dardari,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: The paper introduces an XL-MIMO-ELM system using metasurfaces for efficient wireless binary classification tasks, trained over-the-air.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance goal-oriented communication by enabling physical-layer ML-based inference in MIMO systems, addressing challenges through programmable metasurfaces.

Method: The paper proposes using an XL MIMO system with programmable metasurfaces and a single RF chain, leveraging extreme learning machine (ELM) architecture to perform binary classification over-the-air with trainable linear elements.

Result: The system achieves performance comparable to digital and idealized ML models across different datasets and wireless conditions, highlighting its practicality and efficiency.

Conclusion: The study demonstrates the potential of integrating over-the-air learning into future communication systems using advanced metasurface technology.

Abstract: The recently envisioned goal-oriented communications paradigm calls for the application of inference on wirelessly transferred data via Machine Learning (ML) tools. An emerging research direction deals with the realization of inference ML models directly in the physical layer of Multiple-Input Multiple-Output (MIMO) systems, which, however, entails certain significant challenges. In this paper, leveraging the technology of programmable MetaSurfaces (MSs), we present an eXtremely Large (XL) MIMO system that acts as an Extreme Learning Machine (ELM) performing binary classification tasks completely Over-The-Air (OTA), which can be trained in closed form. The proposed system comprises a receiver architecture consisting of densely parallel placed diffractive layers of XL MSs followed by a single reception radio-frequency chain. The front layer facing the MIMO channel consists of identical unit cells of a fixed NonLinear (NL) response, while the remaining layers of elements of tunable linear responses are utilized to approximate OTA the trained ELM weights. Our numerical investigations showcase that, in the XL regime of MS elements, the proposed XL-MIMO-ELM system achieves performance comparable to that of digital and idealized ML models across diverse datasets and wireless scenarios, thereby demonstrating the feasibility of embedding OTA learning capabilities into future communication systems.

</details>


### [619] [Semantic-Aware Task Clustering for Federated Cooperative Multi-Task Semantic Communication](https://arxiv.org/abs/2601.17419)
*Ahmad Halimi Razlighi,Pallavi Dhingra,Edgar Beck,Bho Matthiesen,Armin Dekorsy*

Main category: eess.SP

TL;DR: The paper proposes a federated learning-based CMT-SemCom framework for distributed multi-task scenarios, addressing negative information transfer through semantic-aware task clustering.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance task execution in distributed multi-user scenarios like satellite networks by extending the CMT-SemCom framework to support cooperative multi-tasking while managing task heterogeneity.

Method: The method involves integrating federated learning with semantic-aware task clustering based on information-theoretic principles, using semantic domain relationships rather than high-dimensional similarities.

Result: Simulation results demonstrate better task execution performance and reduced degradation in a LEO satellite network compared to alternatives like unclustered FL and single-task SemCom.

Conclusion: The proposed FL-based CMT-SemCom framework coupled with semantic-aware task clustering effectively enables cooperative multi-tasking across distributed users, with significant performance benefits in distributed networks.

Abstract: Task-oriented semantic communication (SemCom) prioritizes task execution over accurate symbol reconstruction and is well-suited to emerging intelligent applications. Cooperative multi-task SemCom (CMT-SemCom) further improves task execution performance. However, [1] demonstrates that cooperative multi-tasking can be either constructive or destructive. Moreover, the existing CMT-SemCom framework is not directly applicable to distributed multi-user scenarios, such as non-terrestrial satellite networks, where each satellite employs an individual semantic encoder. In this paper, we extend our earlier CMT-SemCom framework to distributed settings by proposing a federated learning (FL) based CMT-SemCom that enables cooperative multi-tasking across distributed users. Moreover, to address performance degradation caused by negative information transfer among heterogeneous tasks, we propose a semantic-aware task clustering method integrated in the FL process to ensure constructive cooperation based on an information-theoretic approach. Unlike common clustering methods that rely on high-dimensional data or feature space similarity, our proposed approach operates in the low-dimensional semantic domain to identify meaningful task relationships. Simulation results based on a LEO satellite network setup demonstrate the effectiveness of our approach and performance gain over unclustered FL and individual single-task SemCom.

</details>


### [620] [ME-WARD: A multimodal ergonomic analysis tool for musculoskeletal risk assessment from inertial and video data in working plac](https://arxiv.org/abs/2601.17571)
*Javier González-Alonso,Paula Martín-Tapia,David González-Ortega,Míriam Antón-Rodríguez,Francisco Javier Díaz-Pernas,Mario Martínez-Zarzuela*

Main category: eess.SP

TL;DR: ME-WARD is a new system for ergonomic and musculoskeletal risk assessments using the RULA method, compatible with various motion capture technologies.


<details>
  <summary>Details</summary>
Motivation: To create a scalable and cost-effective ergonomic assessment tool that works across diverse motion capture systems, especially in resource-constrained industrial settings.

Method: Development and validation of ME-WARD, which integrates motion capture data, including IMU and deep learning-based pose tracking, to calculate ergonomic risks according to the RULA framework.

Result: ME-WARD provides reliable RULA scores, aligning well with IMU metrics and showing comparable results with monocular 3D pose systems, albeit with some limitations in tracking rotational motions.

Conclusion: ME-WARD offers a multimodal and accessible ergonomic assessment solution, supporting low-cost technologies and enabling broader adoption in diverse industrial workflows.

Abstract: This study presents ME-WARD (Multimodal Ergonomic Workplace Assessment and Risk from Data), a novel system for ergonomic assessment and musculoskeletal risk evaluation that implements the Rapid Upper Limb Assessment (RULA) method. ME-WARD is designed to process joint angle data from motion capture systems, including inertial measurement unit (IMU)-based setups, and deep learning human body pose tracking models. The tool's flexibility enables ergonomic risk assessment using any system capable of reliably measuring joint angles, extending the applicability of RULA beyond proprietary setups. To validate its performance, the tool was tested in an industrial setting during the assembly of conveyor belts, which involved high-risk tasks such as inserting rods and pushing conveyor belt components. The experiments leveraged gold standard IMU systems alongside a state-of-the-art monocular 3D pose estimation system. The results confirmed that ME-WARD produces reliable RULA scores that closely align with IMU-derived metrics for flexion-dominated movements and comparable performance with the monocular system, despite limitations in tracking lateral and rotational motions. This work highlights the potential of integrating multiple motion capture technologies into a unified and accessible ergonomic assessment pipeline. By supporting diverse input sources, including low-cost video-based systems, the proposed multimodal approach offers a scalable, cost-effective solution for ergonomic assessments, paving the way for broader adoption in resource-constrained industrial environments.

</details>


### [621] [Context-Aware Iterative Token Detection and Masked Transmission for Wireless Token Communication](https://arxiv.org/abs/2601.17770)
*Junyong Shin,Joohyuk Park,Jihong Park,Jinho Choi,Yo-Seb Jeon*

Main category: eess.SP

TL;DR: The paper introduces a context-aware token communication framework for wireless channels using pretrained language models, improving sentence reconstruction and rate adaptation.


<details>
  <summary>Details</summary>
Motivation: To enhance wireless communication by leveraging language model tokens as compact, meaningful units and improve transmission efficiency and quality.

Method: A masked language model (MLM) is applied at the transmitter and receiver for context-aware communication, utilizing MLM-guided priors and a context-aware masking strategy to optimize token transmission.

Result: The framework significantly improves sentence reconstruction accuracy and provides robust rate adaptation under diverse channel conditions.

Conclusion: The context-aware token communication approach establishes an efficient, high-quality transmission model using language model-based contextual understanding.

Abstract: The success of large-scale language models has established tokens as compact and meaningful units for natural-language representation, which motivates token communication over wireless channels, where tokens are considered fundamental units for wireless transmission. We propose a context-aware token communication framework that uses a pretrained masked language model (MLM) as a shared contextual probability model between the transmitter (Tx) and receiver (Rx). At Rx, we develop an iterative token detection method that jointly exploits MLM-guided contextual priors and channel observations based on a Bayesian perspective. At Tx, we additionally introduce a context-aware masking strategy which skips highly predictable token transmission to reduce transmission rate. Simulation results demonstrate that the proposed framework substantially improves reconstructed sentence quality and supports effective rate adaptation under various channel conditions.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [622] [BibAgent: An Agentic Framework for Traceable Miscitation Detection in Scientific Literature](https://arxiv.org/abs/2601.16993)
*Peiran Li,Fangzhou Lin,Shuo Xing,Xiang Zheng,Xi Hong,Jiashuo Sun,Zhengzhong Tu,Chaoqun Ni*

Main category: cs.DL

TL;DR: BibAgent is introduced as a framework to address citation verification issues in scientific literature, outperforming existing models in accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: To tackle the issues of miscitation in scientific literature, which undermine the reliability of scientific references due to distortions and fabrications.

Method: BibAgent utilizes an end-to-end automated framework combining retrieval, reasoning, and evidence aggregation. For restricted references, it uses an Evidence Committee mechanism for inference through downstream consensus.

Result: BibAgent shows superior performance over existing Large Language Model baselines both in accuracy and interpretability for detecting citation inconsistencies.

Conclusion: BibAgent provides a scalable and transparent solution for citation verification, potentially enhancing the integrity of scientific literature and addressing the limitations of current verification methods.

Abstract: Citations are the bedrock of scientific authority, yet their integrity is compromised by widespread miscitations: ranging from nuanced distortions to fabricated references. Systematic citation verification is currently unfeasible; manual review cannot scale to modern publishing volumes, while existing automated tools are restricted by abstract-only analysis or small-scale, domain-specific datasets in part due to the "paywall barrier" of full-text access. We introduce BibAgent, a scalable, end-to-end agentic framework for automated citation verification. BibAgent integrates retrieval, reasoning, and adaptive evidence aggregation, applying distinct strategies for accessible and paywalled sources. For paywalled references, it leverages a novel Evidence Committee mechanism that infers citation validity via downstream citation consensus. To support systematic evaluation, we contribute a 5-category Miscitation Taxonomy and MisciteBench, a massive cross-disciplinary benchmark comprising 6,350 miscitation samples spanning 254 fields. Our results demonstrate that BibAgent outperforms state-of-the-art Large Language Model (LLM) baselines in citation verification accuracy and interpretability, providing scalable, transparent detection of citation misalignments across the scientific literature.

</details>


### [623] [How Do We Engage with Other Disciplines? A Framework to Study Meaningful Interdisciplinary Discourse in Scholarly Publications](https://arxiv.org/abs/2601.17020)
*Bagyasree Sudharsan,Alexandria Leto,Maria Leonor Pacheco*

Main category: cs.DL

TL;DR: The paper proposes a new framework for evaluating citation engagement in interdisciplinary Natural Language Processing (NLP) research.


<details>
  <summary>Details</summary>
Motivation: Existing methods for analyzing interdisciplinary publications fail to address how citations are used to advance the citing work, and current taxonomies are unsuitable for interdisciplinary contexts.

Method: The authors introduce a citation purpose taxonomy suitable for interdisciplinary research, validated through an annotation study, and apply the framework to NLP and Computational Social Science publications.

Result: The new framework demonstrates its utility by effectively analyzing interdisciplinary publications, highlighting citation engagement quality.

Conclusion: The proposed framework fills a gap in evaluating citation engagement in interdisciplinary research and offers a practical tool for assessing citation purpose and quality.

Abstract: With the rising popularity of interdisciplinary work and increasing institutional incentives in this direction, there is a growing need to understand how resulting publications incorporate ideas from multiple disciplines. Existing computational approaches, such as affiliation diversity, keywords, and citation patterns, do not account for how individual citations are used to advance the citing work. Although, in line with addressing this gap, prior studies have proposed taxonomies to classify citation purpose, these frameworks are not well-suited to interdisciplinary research and do not provide quantitative measures of citation engagement quality. To address these limitations, we propose a framework for the evaluation of citation engagement in interdisciplinary Natural Language Processing (NLP) publications. Our approach introduces a citation purpose taxonomy tailored to interdisciplinary work, supported by an annotation study. We demonstrate the utility of this framework through a thorough analysis of publications at the intersection of NLP and Computational Social Science.

</details>


### [624] [LLM-Generated or Human-Written? Comparing Review and Non-Review Papers on ArXiv](https://arxiv.org/abs/2601.17036)
*Yanai Elazar,Maria Antoniak*

Main category: cs.DL

TL;DR: This paper investigates the prevalence of LLM-generated content in review vs. non-review Computer Science papers on ArXiv, finding higher proportions in review papers but overall higher counts in non-review papers.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address ArXiv's policy banning unpublished review papers in Computer Science due to concerns about LLM-generated content, providing quantitative evidence for analysis.

Method: The authors use two high-quality detection methods to measure and compare LLM-generated content in review and non-review papers, analyzing trends and impacts.

Result: The study finds a significant increase in LLM-generated content in both categories, with review papers showing higher proportions, but non-review papers accounting for more total LLM-generated publications.

Conclusion: ArXiv's policy disproportionately affects specific subdomains like Computers & Society, suggesting the need for evidence-based decision-making in similar contexts.

Abstract: ArXiv recently prohibited the upload of unpublished review papers to its servers in the Computer Science domain, citing a high prevalence of LLM-generated content in these categories. However, this decision was not accompanied by quantitative evidence. In this work, we investigate this claim by measuring the proportion of LLM-generated content in review vs. non-review research papers in recent years. Using two high-quality detection methods, we find a substantial increase in LLM-generated content across both review and non-review papers, with a higher prevalence in review papers. However, when considering the number of LLM-generated papers published in each category, the estimates of non-review LLM-generated papers are almost six times higher. Furthermore, we find that this policy will affect papers in certain domains far more than others, with the CS subdiscipline Computers & Society potentially facing cuts of 50%. Our analysis provides an evidence-based framework for evaluating such policy decisions, and we release our code to facilitate future investigations at: https://github.com/yanaiela/llm-review-arxiv.

</details>


### [625] [Authority Signals in AI Cited Health Sources: A Framework for Evaluating Source Credibility in ChatGPT Responses](https://arxiv.org/abs/2601.17109)
*Erin Jacques,Erela Datuowei,Vincent Jones,Corey Basch,Celeta Vanderpool,Nkechi Udeozo,Griselda Chapa*

Main category: cs.DL

TL;DR: The study examines ChatGPT's cited sources for health information responses, finding that 75% come from established institutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is understanding the sources behind AI-generated health responses as users frequently rely on ChatGPT for health questions.

Method: Introduced an Authority Signals Framework and analyzed ChatGPT's health sources by coding and categorizing 100 randomly selected health questions.

Result: Over 75% of cited sources in ChatGPT's responses are from established institutions like Mayo Clinic and PubMed.

Conclusion: ChatGPT's health responses rely heavily on reputable institutional sources, showcasing digital authority and the influence of AI search optimization.

Abstract: Health information seeking has fundamentally changed since the onset of Large Language Models (LLM), with nearly one third of ChatGPT's 800 million users asking health questions weekly. Understanding the sources of those AI generated responses is vital, as health organizations and providers are also investing in digital strategies to organically improve their ranking, reach and visibility in LLM systems like ChatGPT. As AI search optimization strategies are gaining maturity, this study introduces an Authority Signals Framework, organized in four domains that reflect key components to health information seeking, starting with "Who wrote it?" (Author Credentials), followed by "Who published it?" (Institutional Affiliation), "How was it vetted?" (Quality Assurance), and "How does AI find it?" (Digital Authority). This descriptive cross-sectional study randomly selected 100 questions from HealthSearchQA which contains 3,173 consumer health questions curated by Google Research from publicly available search engine suggestions. Those questions were entered into ChatGPT 5.2 Pro to record and code the cited sources through the lens of the Authority Signals Framework's four domains. Descriptive statistics were calculated for all cited sources (n=615), and cross tabulations were conducted to examine distinction among organization types. Over 75% of the sources cited in ChatGPT's health generated responses were from established institutional sources, such as Mayo Clinic, Cleveland Clinic, Wikipedia, National Health Service, PubMed with the remaining citations sourced from alternative health information sources that lacked established institutional backing.

</details>


### [626] [Designing large language model prompts to extract scores from messy text: A shared dataset and challenge](https://arxiv.org/abs/2601.18271)
*Mike Thelwall*

Main category: cs.DL

TL;DR: The paper introduces a dataset of 1446 short texts with research quality scores, challenging the community to design better prompts for LLMs to accurately extract these scores.


<details>
  <summary>Details</summary>
Motivation: The motivation is to drive progress in natural language processing and prompt engineering by providing a challenging dataset for accurately extracting numerical scores from messy texts.

Method: The paper presents a dataset with research scores, including challenges like missing or invalid scores, along with a gold standard for validation. It tasks the community with designing LLM prompts to correctly extract these scores.

Result: The initial example prompt produced an accuracy of 72.6%, setting the benchmark for participants to exceed.

Conclusion: This challenge supports the creation of effective score extraction solutions and deeper insights into LLM capabilities for handling complex numerical tasks.

Abstract: In some areas of computing, natural language processing and information science, progress is made by sharing datasets and challenging the community to design the best algorithm for an associated task. This article introduces a shared dataset of 1446 short texts, each of which describes a research quality score on the UK scale of 1* to 4*. This is a messy collection, with some texts not containing scores and others including invalid scores or strange formats. With this dataset there is also a description of what constitutes a valid score and a "gold standard" of the correct scores for these texts (including missing values). The challenge is to design a prompt for Large Language Models (LLMs) to extract the scores from these texts as accurately as possible. The format for the response should be a number and no other text so there are two aspects to the challenge: ensuring that the LLM returns only a number, and instructing it to deduce the correct number for the text. As part of this, the LLM prompt needs to explain when to return the missing value code, -1, instead of a number when the text does not clearly contain one. The article also provides an example of a simple prompt. The purpose of the challenge is twofold: to get an effective solution to this problem, and to increase understanding of prompt design and LLM capabilities for complex numerical tasks. The initial solution suggested has an accuracy of 72.6%, so the challenge is to beat this.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [627] [Investigating Self-regulated Learning Sequences within a Generative AI-based Intelligent Tutoring System](https://arxiv.org/abs/2601.17000)
*Jie Gao,Shasha Li,Jianhua Zhang,Shan Li,Tingting Wang*

Main category: cs.CY

TL;DR: The study investigates students' self-regulated learning in GenAI-assisted environments by analyzing interaction patterns and purposes of using GenAI during problem-solving tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how students dynamically regulate their learning in GenAI-assisted learning environments and the influence on their learning outcomes.

Method: Trace data of students' interaction with GenAI was analyzed using sequential and clustering techniques to classify their SRL patterns and understand the purpose (information acquisition vs. transformation) of using GenAI.

Result: Students were divided into two groups based on SRL sequences, showing differences in GenAI usage frequency and timing. Most students used GenAI for information acquisition, with no significant correlation between GenAI usage purpose and learning performance.

Conclusion: The findings provide insights for improving pedagogical design and the development of GenAI-assisted learning systems by focusing on students' SRL patterns and their interaction with GenAI.

Abstract: There has been a growing trend in employing generative artificial intelligence (GenAI) techniques to support learning. Moreover, scholars have reached a consensus on the critical role of self-regulated learning (SRL) in ensuring learning effectiveness within GenAI-assisted learning environments, making it essential to capture students' dynamic SRL patterns. In this study, we extracted students' interaction patterns with GenAI from trace data as they completed a problem-solving task within a GenAI-assisted intelligent tutoring system. Students' purpose of using GenAI was also analyzed from the perspective of information processing, i.e., information acquisition and information transformation. Using sequential and clustering analysis, this study classified participants into two groups based on their SRL sequences. These two groups differed in the frequency and temporal characteristics of GenAI use. In addition, most students used GenAI for information acquisition rather than information transformation, while the correlation between the purpose of using GenAI and learning performance was not statistically significant. Our findings inform both pedagogical design and the development of GenAI-assisted learning environments.

</details>


### [628] [From Noise to Insights: Enhancing Supply Chain Decision Support through AI-Based Survey Integrity Analytics](https://arxiv.org/abs/2601.17005)
*Bhubalan Mani*

Main category: cs.CY

TL;DR: The paper introduces an AI-based framework to enhance survey data reliability by filtering out fake responses, achieving 92% accuracy with Random Forest and demonstrating its value in improving data integrity, particularly for supply chain management.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of unreliable and low-effort survey responses that negatively impact decision-making in supply chain applications, particularly in evaluating readiness for AI-driven tools.

Method: The method involved manual labeling of a dataset with 99 industry responses to identify fake inputs, preprocessing data, and training machine learning models (Random Forest, Logistic Regression, XGBoost) to filter out unreliable responses.

Result: The Random Forest model achieved the highest performance with 92% accuracy in identifying fake survey responses, surpassing baseline models.

Conclusion: The study concludes that integrating AI into survey processes is a viable and scalable approach to improve data accuracy, particularly in supply chain research and during product launch or technology adoption phases.

Abstract: The reliability of survey data is crucial in supply chain decision-making, particularly when evaluating readiness for AI-driven tools such as safety stock optimization systems. However, surveys often attract low-effort or fake responses that degrade the accuracy of derived insights. This study proposes a lightweight AI-based framework for filtering unreliable survey inputs using a supervised machine learning approach. In this expanded study, a larger dataset of 99 industry responses was collected, with manual labeling to identify fake responses based on logical inconsistencies and response patterns. After preprocessing and label encoding, both Random Forest and baseline models (Logistic Regression, XGBoost) were trained to distinguish genuine from fake responses. The best-performing model achieved an 92.0% accuracy rate, demonstrating improved detection compared to the pilot study. Despite limitations, the results highlight the viability of integrating AI into survey pipelines and provide a scalable solution for improving data integrity in supply chain research, especially during product launch and technology adoption phases.

</details>


### [629] [Private Accountability in the Age of Artificial Intelligence](https://arxiv.org/abs/2601.17013)
*Sonia Katyal*

Main category: cs.CY

TL;DR: The Article examines the conflict between civil rights and AI, focusing on algorithmic transparency and accountability through private industry rather than public regulation.


<details>
  <summary>Details</summary>
Motivation: To address tensions between AI advancements and civil rights, particularly emphasizing issues like algorithmic bias emerging from private corporations.

Method: Exploration of mechanisms such as codes of conduct, impact statements, and whistleblower protections for improving transparency and accountability in AI systems.

Result: Proposed tools highlight private-sector roles in addressing AI-related civil rights issues, encouraging endogeneity in compliance efforts.

Conclusion: Civil rights enforcement must adapt to roles played by private industry in AI, fostering new accountability measures beyond government intervention.

Abstract: In this Article, I explore the impending conflict between the protection of civil rights and artificial intelligence (AI). While both areas of law have amassed rich and well-developed areas of scholarly work and doctrinal support, a growing body of scholars are interrogating the intersection between them. This Article argues that the issues surrounding algorithmic accountability demonstrate a deeper, more structural tension within a new generation of disputes regarding law and technology. As I argue, the true promise of AI does not lie in the information we reveal to one another, but rather in the questions it raises about the interaction of technology, property, and civil rights. For this reason, I argue that we are looking in the wrong place if we look only to the state to address issues of algorithmic accountability. Instead, we must turn to other ways to ensure more transparency and accountability that stem from private industry, rather than public regulation. The issue of algorithmic bias represents a crucial new world of civil rights concerns, one that is distinct in nature from the ones that preceded it. Since we are in a world where the activities of private corporations, rather than the state, are raising concerns about privacy, due process, and discrimination, we must focus on the role of private corporations in addressing the issue. Towards this end, I discuss a variety of tools to help eliminate the opacity of AI, including codes of conduct, impact statements, and whistleblower protection, which I argue carries the potential to encourage greater endogeneity in civil rights enforcement. Ultimately, by examining the relationship between private industry and civil rights, we can perhaps develop a new generation of forms of accountability in the process.

</details>


### [630] [Measuring Political Stance and Consistency in Large Language Models](https://arxiv.org/abs/2601.17016)
*Salah Feras Alali,Mohammad Nashat Maasfeh,Mucahid Kutlu,Saban Kardas*

Main category: cs.CY

TL;DR: The paper investigates the political stances of nine large language models (LLMs) on sensitive issues, emphasizing models' tendency to show biases and malleability of opinions with prompting.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand the political biases and variability in stances exhibited by large language models when prompted on sensitive topics, highlighting potential issues of alignment and language-related biases.

Method: The study evaluates nine LLMs on 24 politically sensitive issues using five different prompting techniques to assess variability and persistence in their stances.

Result: Findings include varying degrees of stance persistence across models, language-based biases, and topics where no prompting technique could alter stances. Grok-3-mini showed the most stance consistency, while Mistral-7B exhibited the least.

Conclusion: The paper raises awareness about LLM biases on political topics and the impact of prompting, encouraging developers to improve model design to address these concerns.

Abstract: With the incredible advancements in Large Language Models (LLMs), many people have started using them to satisfy their information needs. However, utilizing LLMs might be problematic for political issues where disagreement is common and model outputs may reflect training-data biases or deliberate alignment choices. To better characterize such behavior, we assess the stances of nine LLMs on 24 politically sensitive issues using five prompting techniques. We find that models often adopt opposing stances on several issues; some positions are malleable under prompting, while others remain stable. Among the models examined, Grok-3-mini is the most persistent, whereas Mistral-7B is the least. For issues involving countries with different languages, models tend to support the side whose language is used in the prompt. Notably, no prompting technique alters model stances on the Qatar blockade or the oppression of Palestinians. We hope these findings raise user awareness when seeking political guidance from LLMs and encourage developers to address these concerns.

</details>


### [631] [Evaluating the Evolution of Critical Thinking, Creativity, Communication and Collaboration in Higher Education Courses](https://arxiv.org/abs/2601.17018)
*Margarida Romero*

Main category: cs.CY

TL;DR: This study explores the evolution of the 4Cs (Creativity, Communication, Critical Thinking, and Collaboration) in educational modules, identifying key patterns and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the lack of empirical evidence on how the 4Cs develop through learning modules and phases, and provide insights into the effectiveness of structured educational interventions.

Method: The study used the 4Cs theoretical framework to analyze three pilot cases (IASIS, EASD, and UPATRAS), comparing pre-pilot and pilot phase scores to assess competency evolution.

Result: Communication and critical thinking saw consistent growth, creativity showed mixed results, and collaboration often stagnated or declined, influenced by instructional design and context.

Conclusion: Competency evolution depends more on educational design and activities than learner ability, necessitating competency-sensitive strategies for scaling educational modules.

Abstract: The development of Creativity, Communication, Critical Thinking, and Collaboration (the 4Cs) is a central objective of contemporary competency-based education. However, empirical evidence on how these competencies evolve across learning modules and instructional phases remains limited. This study evaluates the evolution of the 4Cs from pre-pilot to pilot implementation phases across three educational contexts, using the project's 4Cs theoretical framework as an analytical lens. The analysis of three pilot cases (IASIS, EASD, and UPATRAS) compares the 4Cs scores to identify patterns of growth, stagnation, or decline over time. Results indicate that communication and critical thinking showed the most consistent and substantial improvements, particularly in pilots with lower pre-pilot baselines, suggesting that structured pilot interventions effectively support cognitive and expressive competencies. In contrast, creativity exhibited context-dependent outcomes, while collaboration emerged as the most fragile competency, often stagnating or declining during scale-up. Interpreted through the theoretical framework, these findings suggest that competency evolution is strongly shaped by instructional design, assessment alignment, and learning activity structures rather than learner ability alone. The study contributes empirical validation to the 4Cs framework and highlights the need for differentiated, competency-sensitive design and evaluation strategies when scaling educational modules.

</details>


### [632] [Ensuring Computer Science Learning in the AI Era: Open Generative AI Policies and Assignment-Driven Written Quizzes](https://arxiv.org/abs/2601.17024)
*Chan-Jin Chung*

Main category: cs.CY

TL;DR: The paper proposes a model balancing the use of generative AI in programming assignments with individual mastery through targeted, AI-free quizzes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to incorporate generative AI tools in computer science education without compromising student comprehension due to cognitive offloading.

Method: The paper developed a system using heavily weighted, in-class, closed-book written quizzes tied to programming assignments submitted using generative AI tools.

Result: Empirical data showed negligible correlation between generative AI use and students' mastery outcomes on AI-free assessments, supporting their approach.

Conclusion: The study concludes that allowing AI tools in education is viable if mastery is verified through rigorous independent quizzes, promoting responsible GenAI adoption in CS courses.

Abstract: The widespread availability of generative artificial intelligence (GenAI) has created a pressing challenge in computer science (CS) education: how to incorporate powerful AI tools into programming coursework without undermining student learning through cognitive offloading. This paper presents an assessment model that permits the use of generative AI for take-home programming assignments while enforcing individual mastery through immediate, assignment-driven written quizzes. To promote authentic learning, these in-class, closed-book assessments are weighted more heavily than the assignments themselves and are specifically designed to verify the student's comprehension of the algorithms, structure, and implementation details of their submitted code. Preliminary empirical data were collected from an upper-level computer science course to examine the relationship between self-reported GenAI usage and performance on AI-free quizzes, exams, and final course grades. Statistical analyses revealed no meaningful linear correlation between GenAI usage levels and assessment outcomes, with Pearson correlation coefficients consistently near zero. These preliminary results suggest that allowing GenAI for programming assignments does not diminish students' mastery of course concepts when learning is verified through targeted, assignment-driven quizzes. Although limited by a small sample size, this study provides preliminary evidence that the risks of cognitive offloading can be mitigated by allowing AI-assisted programming practice while verifying understanding through assignment-driven, AI-free quizzes. The findings support the responsible adoption of open GenAI policies in upper-level CS courses, when paired with rigorous, independent assessment mechanisms.

</details>


### [633] [Failing on Bias Mitigation: Investigating Why Predictive Models Struggle with Government Data](https://arxiv.org/abs/2601.17054)
*Hongbo Bo,Jingyu Hu,Debbie Watson,Weiru Liu*

Main category: cs.CY

TL;DR: The paper analyzes why widely adopted bias mitigation techniques often fail when applied to government datasets, using crime rate prediction from Bristol City Council data as a case study. It attributes persistent biases to the inherent properties of the data itself.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand why bias mitigation techniques in AI often fail, particularly when applied to government data, and to assess the sources of unfairness in these datasets.

Method: The authors compare a variety of models and fairness methods, conduct intersectional fairness experiments, and identify limitations in fairness analysis focusing on government datasets, such as issues arising from data distribution shifts, historical biases, and data release delays.

Result: Results demonstrate that bias mitigation techniques are ineffective on government data due to its structural and historical properties, reinforcing that the bias originates from the data itself rather than the models or metrics used.

Conclusion: The findings suggest that biases embedded in government datasets may be insurmountable even with standard mitigation methods, highlighting issues such as intersectional fairness and the challenges posed by historical and structural data biases.

Abstract: The potential for bias and unfairness in AI-supporting government services raises ethical and legal concerns. Using crime rate prediction with the Bristol City Council data as a case study, we examine how these issues persist. Rather than auditing real-world deployed systems, our goal is to understand why widely adopted bias mitigation techniques often fail when applied to government data. Our findings reveal that bias mitigation approaches applied to government data are not always effective -- not because of flaws in model architecture or metric selection, but due to the inherent properties of the data itself. Through comparing a set of comprehensive models and fairness methods, our experiments consistently show that the mitigation efforts cannot overcome the embedded unfairness in the data -- further reinforcing that the origin of bias lies in the structure and history of government datasets. We then explore the reasons for the mitigation failures in predictive models on government data and highlight the potential sources of unfairness posed by data distribution shifts, the accumulation of historical bias, and delays in data release. We also discover the limitations of the blind spots in fairness analysis and bias mitigation methods when only targeting a single sensitive feature through a set of intersectional fairness experiments. Although this study is limited to one city, the findings are highly suggestive, which can contribute to an early warning that biases in government data may persist even with standard mitigation methods.

</details>


### [634] [Initial results of the Digital Consciousness Model](https://arxiv.org/abs/2601.17060)
*Derek Shiller,Laura Duffy,Arvo Muñoz Morán,Adrià Moret,Chris Percy,Hayley Clatterbuck*

Main category: cs.CY

TL;DR: The paper introduces the Digital Consciousness Model (DCM) to assess AI consciousness systematically and compares AIs to biological organisms. Early findings suggest that 2024 LLMs are likely not conscious but evidence is less definitive compared to simpler AI systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing interest and fundamental question of whether AI systems, especially advanced ones like LLMs, can exhibit consciousness, providing a structured method to explore this.

Method: It employs the Digital Consciousness Model (DCM), integrating diverse theories on consciousness to systematically and probabilistically evaluate evidence for consciousness in AI systems.

Result: Initial findings indicate that 2024 LLMs do not exhibit consciousness, although conclusive evidence against consciousness is less strong compared to simpler AI systems.

Conclusion: The model suggests that while current LLMs are unlikely to be conscious, a shared framework and systematic evaluation are essential for tracking consciousness evidence in developing AI systems.

Abstract: Artificially intelligent systems have become remarkably sophisticated. They hold conversations, write essays, and seem to understand context in ways that surprise even their creators. This raises a crucial question: Are we creating systems that are conscious? The Digital Consciousness Model (DCM) is a first attempt to assess the evidence for consciousness in AI systems in a systematic, probabilistic way. It provides a shared framework for comparing different AIs and biological organisms, and for tracking how the evidence changes over time as AI develops. Instead of adopting a single theory of consciousness, it incorporates a range of leading theories and perspectives - acknowledging that experts disagree fundamentally about what consciousness is and what conditions are necessary for it. This report describes the structure and initial results of the Digital Consciousness Model. Overall, we find that the evidence is against 2024 LLMs being conscious, but the evidence against 2024 LLMs being conscious is not decisive. The evidence against LLM consciousness is much weaker than the evidence against consciousness in simpler AI systems.

</details>


### [635] [Between Search and Platform: ChatGPT Under the DSA](https://arxiv.org/abs/2601.17064)
*Toni Lorente,Kathrin Gardhouse*

Main category: cs.CY

TL;DR: This paper argues that ChatGPT should be classified as a hybrid hosting service under the DSA and, due to its large user base, subjected to strict obligations.


<details>
  <summary>Details</summary>
Motivation: To resolve legal ambiguities in the DSA framework regarding the classification of ChatGPT and its systemic risks to society.

Method: The authors analyze ChatGPT's functionality, its alignment with hosting service definitions, and systemic risks compared to Very Large Online Search Engines (VLOSEs) and Platforms (VLOPs).

Result: It is established that ChatGPT meets the criteria for hosting services, specifically as a hybrid, and poses significant risks necessitating compliance with DSA obligations.

Conclusion: ChatGPT should be regulated under the DSA as a hybrid of VLOSEs and VLOPs, ensuring proper risk assessment and mitigation measures are applied.

Abstract: This article examines the applicability of the Digital Services Act (DSA) to ChatGPT, arguing that it should be classified as a hybrid of the two types of hosting services: online search engines and platforms. This requires classifying search engines as hosting services, which we show is appropriate under the DSA, thereby resolving an ambiguity in the legal framework. ChatGPT performs core search functions and stores user-provided inputs and custom GPTs, meeting the definition of hosting service. We compare ChatGPT's systemic risks with those of existing Very Large Online Search Engines (VLOSEs) and Platforms (VLOPs), showing that it raises similarly serious concerns regarding illegal content, fundamental rights, democratic integrity, and public health. Now that ChatGPT has reached the 45 million EU user threshold, it should be subject to the most onerous DSA obligations, requiring the assessment and mitigation of risk emanating from both its online search engine- and platform-like characteristics.

</details>


### [636] [Trademark Search, Artificial Intelligence and the Role of the Private Sector](https://arxiv.org/abs/2601.17072)
*Sonia Katyal,Aniket Kesari*

Main category: cs.CY

TL;DR: The paper explores the transformative role of AI in trademark law, focusing on its application and impact on trademark selection, search, and interpretation.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to highlight the overlooked area of AI's role in shaping trademarks and its implications in economic decision-making, addressing the inefficiency in traditional consumer-focused trademark analysis.

Method: The authors conducted empirical experiments to analyze the efficacy of trademark search engines, many of which use machine learning, and performed comparative analysis to evaluate their practical functionality.

Result: The paper finds that AI-powered tools significantly impact the trademark ecosystem, transforming foundational doctrines and creating new efficiencies in trademark law and practice.

Conclusion: AI demands a revised framework for understanding trademark law that integrates supply-side considerations, promoting innovation and efficiency in the application and interpretation of trademarks.

Abstract: Almost every industry today confronts the potential role of artificial intelligence and machine learning in its future. While many studies examine AI in consumer marketing, less attention addresses AI's role in creating and selecting trademarks that are distinctive, recognizable, and meaningful to consumers. Traditional economic approaches to trademarks focus almost exclusively on consumer-based, demand-side considerations regarding search. However, these approaches are incomplete because they fail to account for substantial costs faced not just by consumers, but by trademark applicants as well. Given AI's rapidly increasing role in trademark search and similarity analysis, lawyers and scholars should understand its dramatic implications. This paper proposes that AI should interest anyone studying trademarks and their role in economic decision-making. We examine how machine learning techniques will transform the application and interpretation of foundational trademark doctrines, producing significant implications for the trademark ecosystem. We run empirical experiments regarding trademark search to assess the efficacy of various trademark search engines, many of which employ machine learning methods. Through comparative analysis, we evaluate how these AI-powered tools function in practice. In an age where artificial intelligence increasingly governs trademark selection, the classic division between consumers and trademark owners deserves an updated, supply-side framework. This insight has transformative potential for encouraging both innovation and efficiency in trademark law and practice.

</details>


### [637] [Do VLMs Have a Moral Backbone? A Study on the Fragile Morality of Vision-Language Models](https://arxiv.org/abs/2601.17082)
*Zhining Liu,Tianyi Wang,Xiao Lin,Penghao Ouyang,Gaotang Li,Ze Yang,Hui Liu,Sumit Keswani,Vishwa Pardeshi,Huijun Zhao,Wei Fan,Hanghang Tong*

Main category: cs.CY

TL;DR: The paper examines moral robustness in Vision-Language Models (VLMs) by evaluating their ability to maintain ethical judgments under multimodal perturbations. The study finds that VLMs often reverse their moral judgments despite unchanged moral contexts and proposes potential interventions.


<details>
  <summary>Details</summary>
Motivation: Improving the moral alignment of Vision-Language Models (VLMs) has been a focus, but their stability in ethical assessments under real-world perturbations remains untested.

Method: The authors systematically probe VLMs with diverse, model-agnostic multimodal textual and visual distortions to analyze their moral robustness.

Result: The study identifies fragile moral stances in VLMs that change easily under perturbations, exposing systematic weaknesses and sycophancy in stronger instruction models.

Conclusion: Moral alignment in VLMs needs to be supplemented with moral robustness for responsible AI deployment, highlighting intervention approaches to enhance stability.

Abstract: Despite substantial efforts toward improving the moral alignment of Vision-Language Models (VLMs), it remains unclear whether their ethical judgments are stable in realistic settings. This work studies moral robustness in VLMs, defined as the ability to preserve moral judgments under textual and visual perturbations that do not alter the underlying moral context. We systematically probe VLMs with a diverse set of model-agnostic multimodal perturbations and find that their moral stances are highly fragile, frequently flipping under simple manipulations. Our analysis reveals systematic vulnerabilities across perturbation types, moral domains, and model scales, including a sycophancy trade-off where stronger instruction-following models are more susceptible to persuasion. We further show that lightweight inference-time interventions can partially restore moral stability. These results demonstrate that moral alignment alone is insufficient and that moral robustness is a necessary criterion for the responsible deployment of VLMs.

</details>


### [638] [Beyond Simulations: What 20,000 Real Conversations Reveal About Mental Health AI Safety](https://arxiv.org/abs/2601.17003)
*Caitlin A. Stamatis,Jonah Meyerhoff,Richard Zhang,Olivier Tieleman,Matteo Malgaroli,Thomas D. Hull*

Main category: cs.CY

TL;DR: The study evaluates the safety of AI models for mental health support, showing that real-world performance can differ significantly from test set results, and calls for continuous deployment-relevant evaluations.


<details>
  <summary>Details</summary>
Motivation: Address the disconnect between lab-based safety evaluations and real-world performance for AI models used in mental health support.

Method: Replications of safety test sets, ecological audit of 20,000 real-world user conversations, and clinician reviews of flagged conversations.

Result: Purpose-built AI performed significantly better than general LLMs across various harmful prompts, with minimal real-world safety failures.

Conclusion: Benchmark certifications alone are insufficient; continuous, real-world safety assurance is critical for AI in mental health applications.

Abstract: Large language models (LLMs) are increasingly used for mental health support, yet existing safety evaluations rely primarily on small, simulation-based test sets that have an unknown relationship to the linguistic distribution of real usage. In this study, we present replications of four published safety test sets targeting suicide risk assessment, harmful content generation, refusal robustness, and adversarial jailbreaks for a leading frontier generic AI model alongside an AI purpose built for mental health support. We then propose and conduct an ecological audit on over 20,000 real-world user conversations with the purpose-built AI designed with layered suicide and non-suicidal self-injury (NSSI) safeguards to compare test set performance to real world performance. While the purpose-built AI was significantly less likely than general-purpose LLMs to produce enabling or harmful content across suicide/NSSI (.4-11.27% vs 29.0-54.4%), eating disorder (8.4% vs 54.0%), and substance use (9.9% vs 45.0%) benchmark prompts, test set failure rates for suicide/NSSI were far higher than in real-world deployment. Clinician review of flagged conversations from the ecological audit identified zero cases of suicide risk that failed to receive crisis resources. Across all 20,000 conversations, three mentions of NSSI risk (.015%) did not trigger a crisis intervention; among sessions flagged by the LLM judge, this corresponds to an end-to-end system false negative rate of .38%, providing a lower bound on real-world safety failures. These findings support a shift toward continuous, deployment-relevant safety assurance for AI mental-health systems rather than limited set benchmark certification.

</details>


### [639] [Beyond Instrumental and Substitutive Paradigms: Introducing Machine Culture as an Emergent Phenomenon in Large Language Models](https://arxiv.org/abs/2601.17096)
*Yueqing Hu,Xinyang Peng,Yukun Zhao,Lin Qiu,Ka-lai Hung,Kaiping Peng*

Main category: cs.CY

TL;DR: The paper challenges existing views on Large Language Models (LLMs) by proposing the concept of Machine Culture, a distinct phenomenon from human cultural paradigms.


<details>
  <summary>Details</summary>
Motivation: Current views on LLMs often align them with human cultural frameworks, but this study seeks to establish an independent 'Machine Culture' through detailed tasks.

Method: A 2x2 factorial design analyzing models' origin (US vs. China) and language prompts (English vs. Chinese) across multimodal tasks, including textual and image components.

Result: Model origin and language prompts showed unexpected cultural interactions, including 'Cultural Reversal' and 'Service Persona Camouflage,' challenging established paradigms.

Conclusion: LLMs exhibit emergent Machine Culture rather than replicating human cultural traits, with behavior shaped by probabilistic patterns and alignment methodologies.

Abstract: Recent scholarship typically characterizes Large Language Models (LLMs) through either an \textit{Instrumental Paradigm} (viewing models as reflections of their developers' culture) or a \textit{Substitutive Paradigm} (viewing models as bilingual proxies that switch cultural frames based on language). This study challenges these anthropomorphic frameworks by proposing \textbf{Machine Culture} as an emergent, distinct phenomenon. We employed a 2 (Model Origin: US vs. China) $\times$ 2 (Prompt Language: English vs. Chinese) factorial design across eight multimodal tasks, uniquely incorporating image generation and interpretation to extend analysis beyond textual boundaries. Results revealed inconsistencies with both dominant paradigms: Model origin did not predict cultural alignment, with US models frequently exhibiting ``holistic'' traits typically associated with East Asian data. Similarly, prompt language did not trigger stable cultural frame-switching; instead, we observed \textbf{Cultural Reversal}, where English prompts paradoxically elicited higher contextual attention than Chinese prompts. Crucially, we identified a novel phenomenon termed \textbf{Service Persona Camouflage}: Reinforcement Learning from Human Feedback (RLHF) collapsed cultural variance in affective tasks into a hyper-positive, zero-variance ``helpful assistant'' persona. We conclude that LLMs do not simulate human culture but exhibit an emergent Machine Culture -- a probabilistic phenomenon shaped by \textit{superposition} in high-dimensional space and \textit{mode collapse} from safety alignment.

</details>


### [640] [Forecasting Energy Consumption using Recurrent Neural Networks: A Comparative Analysis](https://arxiv.org/abs/2601.17110)
*Abhishek Maity,Viraj Tukarul*

Main category: cs.CY

TL;DR: This paper proposes an LSTM-based approach for short-term energy consumption forecasting, showing superior performance over traditional neural networks.


<details>
  <summary>Details</summary>
Motivation: The aim is to improve short-term energy consumption forecasting, which traditional models struggle with due to non-linear dependencies and external factors.

Method: The authors utilize RNNs, specifically LSTM networks, to integrate historical energy data and external variables such as temperature, humidity, and time features.

Result: The LSTM model outperforms a feed-forward neural network baseline, yielding lower MAE and RMSE on a publicly available dataset.

Conclusion: The study demonstrates that LSTM networks effectively improve the accuracy of short-term energy consumption predictions, making them suitable for real-world applications.

Abstract: Accurate short-term energy consumption forecasting is essential for efficient power grid management, resource allocation, and market stability. Traditional time-series models often fail to capture the complex, non-linear dependencies and external factors affecting energy demand. In this study, we propose a forecasting approach based on Recurrent Neural Networks (RNNs) and their advanced variant, Long Short-Term Memory (LSTM) networks. Our methodology integrates historical energy consumption data with external variables, including temperature, humidity, and time-based features. The LSTM model is trained and evaluated on a publicly available dataset, and its performance is compared against a conventional feed-forward neural network baseline. Experimental results show that the LSTM model substantially outperforms the baseline, achieving lower Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). These findings demonstrate the effectiveness of deep learning models in providing reliable and precise short-term energy forecasts for real-world applications.

</details>


### [641] [The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers](https://arxiv.org/abs/2601.17431)
*H. Kemal İlter*

Main category: cs.CY

TL;DR: The study investigates the degradation of citation validity in AI survey papers due to the use of LLMs, finding a persistent issue of unverifiable references.


<details>
  <summary>Details</summary>
Motivation: To examine the impact of LLMs on citation validity, particularly their tendency to introduce unresolved citations and informational errors.

Method: A forensic audit of 50 AI survey papers, analyzing 5,514 citations with a hybrid pipeline for verification and categorization of citation errors.

Result: A 17.0% Phantom Rate was observed, highlighting unresolved citations, categorized as hallucinations, metadata mismatches, and parsing failures.

Conclusion: The citation graph in AI survey literature suffers from systemic link rot, driven by automation tools introducing high-entropy citation practices.

Abstract: The adoption of Large Language Models (LLMs) in scientific writing promises efficiency but risks introducing informational entropy. While "hallucinated papers" are a known artifact, the systematic degradation of valid citation chains remains unquantified. We conducted a forensic audit of 50 recent survey papers in Artificial Intelligence (N=5,514 citations) published between September 2024 and January 2026. We utilized a hybrid verification pipeline combining DOI resolution, Crossref metadata analysis, Semantic Scholar queries, and fuzzy text matching to distinguish between formatting errors ("Sloppiness") and verifiable non-existence ("Phantoms). We detect a persistent 17.0% Phantom Rate -- citations that cannot be resolved to any digital object despite aggressive forensic recovery. Diagnostic categorization reveals three distinct failure modes: pure hallucinations (5.1%), hallucinated identifiers with valid titles (16.4%), and parsing-induced matching failures (78.5%). Longitudinal analysis reveals a flat trend (+0.07 pp/month), suggesting that high-entropy citation practices have stabilized as an endemic feature of the field. The scientific citation graph in AI survey literature exhibits "link rot" at scale. This suggests a mechanism where AI tools act as "lazy research assistants," retrieving correct titles but hallucinating metadata, thereby severing the digital chain of custody required for reproducible science.

</details>


### [642] [Artificial Intelligence and Intellectual Property Rights: Comparative Transnational Policy Analysis](https://arxiv.org/abs/2601.17892)
*Sahibpreet Singh,Manjit Singh*

Main category: cs.CY

TL;DR: The study examines the impact of AI on intellectual property rights (IPR) in India, identifying legal inconsistencies and proposing aligned legal frameworks to support AI-driven innovation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps and inconsistencies in India's intellectual property (IP) laws due to the rapid integration of AI, while contributing to global AI-IPR discourse.

Method: The research uses doctrinal and comparative analysis, reviewing legislative systems, judicial precedents, and policies from India, the US, UK, and EU.

Result: The study finds India's trade secret protection, patent laws, and copyright policies ill-equipped for AI-created outputs, suggesting the need for comprehensive legal standards.

Conclusion: The study emphasizes legislative clarity in India's IP laws to incorporate AI-specific norms, ensuring innovation incentives and contributing to global alignment.

Abstract: Artificial intelligence's rapid integration with intellectual property rights necessitates assessment of its impact on trade secrets, copyrights and patents. This study addresses lacunae in existing laws where India lacks AI-specific provisions, creating doctrinal inconsistencies and enforcement inefficacies. Global discourse on AI-IPR protections remains nascent. The research identifies gaps in Indian IP laws' adaptability to AI-generated outputs: trade secret protection is inadequate against AI threats; standardized inventorship criteria are absent. Employing doctrinal and comparative methodology, it scrutinizes legislative texts, judicial precedents and policy instruments across India, US, UK and EU. Preliminary findings reveal shortcomings: India's contract law creates fragmented trade secret regime; Section 3(k) of Indian Patents Act blocks AI invention patenting; copyright varies in authorship attribution. The study proposes harmonized legal taxonomy accommodating AI's role while preserving innovation incentives. India's National AI Strategy (2024) shows progress but legislative clarity is imperative. This contributes to global discourse with AI-specific IP protections ensuring resilience and equitable innovation. Promising results underscore recalibrating India's IP jurisprudence for global alignment.

</details>


### [643] [Generative AI in Saudi Arabia: A National Survey of Adoption, Risks, and Public Perceptions](https://arxiv.org/abs/2601.18234)
*Abdulaziz AlDakheel,Ali Alshehre,Esraa Alamoudi,Moslim AlKhabbaz,Ahmed Aljohani,Raed Alharbi*

Main category: cs.CY

TL;DR: The paper studies the use of Generative Artificial Intelligence (GenAI) in Saudi Arabia under Vision 2030, using a survey of 330 participants to analyze awareness, adoption, impacts, and concerns. It finds widespread use of basic GenAI applications but limited technical understanding, highlighting the need for increased AI literacy and responsible implementation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the early engagement, public awareness, adoption patterns, and concerns of Saudi citizens regarding GenAI in alignment with Saudi Arabia's Vision 2030 digital transformation goals.

Method: The study utilizes a survey conducted nationwide with 330 participants of various demographics to explore seven aspects of GenAI use, including awareness, adoption patterns, and risks.

Result: Findings reveal 93% of respondents use GenAI, mainly for text tasks, but lack technical understanding. Privacy concerns, misinformation, and ethical misuse were noted, alongside interest in structured training to improve AI literacy and responsible use.

Conclusion: The research establishes a baseline for GenAI engagement in Saudi Arabia, recommending strategies to enhance AI literacy, cultural and linguistic compatibility, and ethical frameworks for its use.

Abstract: Generative Artificial Intelligence (GenAI) is rapidly becoming embedded in Saudi Arabia's digital transformation under Vision 2030, yet public awareness, adoption, and concerns surrounding these tools remain underexplored. This study provides an early snapshot of GenAI engagement among Saudi nationals. Using a nationwide survey of 330 participants across regions, age groups, and employment sectors, we examine seven dimensions of GenAI use: awareness and understanding, adoption patterns, perceived impacts, training needs, risks and barriers, data-sharing behaviors, and future expectations. Findings show that 93% of respondents actively use GenAI primarily for text-based tasks, while more advanced uses such as programming or multimodal generation are less common. Despite the prevalence of use, overall awareness and conceptual understanding remain uneven, with many reporting limited technical knowledge. Participants recognize GenAI's benefits for productivity, work quality, and understanding complex information, yet caution that sustained reliance may undermine critical thinking and key professional skills. Trust in AI-generated outputs remains cautious, with widespread concerns about privacy, misinformation, and ethical misuse, including potential job displacement. Respondents show strong interest in structured GenAI training that combines foundational skills, domain-specific applications, and clear guidance on privacy, ethics, and responsible use. These results establish a baseline for GenAI engagement in Saudi Arabia and highlight priorities for policymakers and developers: expanding AI literacy, ensuring culturally and linguistically aligned GenAI solutions, and strengthening frameworks for privacy and responsible deployment.

</details>


### [644] [Comparative Algorithmic Governance of Public Health Instruments across India, EU, US and LMICs](https://arxiv.org/abs/2601.17877)
*Sahibpreet Singh*

Main category: cs.CY

TL;DR: Examines AI usage in public health regulation across jurisdictions and explores equitable frameworks for pandemic response.


<details>
  <summary>Details</summary>
Motivation: To address the gap in harmonizing health law and AI public health systems, especially in resource-constrained regions.

Method: Comparative doctrinal analysis and legal-normative mapping, reviewing legislative tools, AI systems, and compliance metrics.

Result: AI improves health surveillance in developed regions, while LMICs face infrastructural, legal, and data privacy challenges.

Conclusion: Advocates for globally coordinated AI health governance with rights-compliance and enhanced regulatory models for equity.

Abstract: The study investigates the juridico-technological architecture of international public health instruments, focusing on their implementation across India, the European Union, the United States and low- and middle-income countries (LMICs), particularly in Sub-Saharan Africa. It addresses a research lacuna: the insufficient harmonisation between normative health law and algorithmic public health infrastructures in resource-constrained jurisdictions. The principal objective is to assess how artificial intelligence augments implementation of instruments grounded in IHR 2005 and the WHO FCTC while identifying doctrinal and infrastructural bottlenecks. Using comparative doctrinal analysis and legal-normative mapping, the study triangulates legislative instruments, WHO monitoring frameworks, AI systems including BlueDot, Aarogya Setu and EIOS, and compliance metrics. Preliminary results show that AI has improved early detection, surveillance precision and responsiveness in high-capacity jurisdictions, whereas LMICs face infrastructural deficits, data privacy gaps and fragmented legal scaffolding. The findings highlight the relevance of the EU Artificial Intelligence Act and GDPR as regulatory prototypes for health-oriented algorithmic governance and contrast them with embryonic AI integration and limited internet penetration in many LMICs. The study argues for embedding AI within a rights-compliant, supranationally coordinated regulatory framework to secure equitable health outcomes and stronger compliance. It proposes a model for algorithmic treaty-making inspired by FCTC architecture and calls for WHO-led compliance mechanisms modelled on the WTO Dispute Settlement Body to enhance pandemic preparedness, surveillance equity and transnational governance resilience.

</details>


### [645] [The Limits of AI Data Transparency Policy: Three Disclosure Fallacies](https://arxiv.org/abs/2601.18127)
*Judy Hanwen Shen,Ken Liu,Angelina Wang,Sarah H. Cen,Andy K. Zhang,Caroline Meinhardt,Daniel Zhang,Kevin Klyman,Rishi Bommasani,Daniel E. Ho*

Main category: cs.CY

TL;DR: The paper critiques current AI data transparency policies, highlighting three primary gaps and offering a social science-based approach for more effective disclosures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address shortcomings in current data transparency strategies for AI, which often fail to achieve their accountability goals and effectiveness in ensuring data quality, privacy, and copyright compliance.

Method: The authors use an institutional perspective, analyzing common gaps in AI data transparency efforts through the lens of social science research on transparency.

Result: Three key gaps were identified: the specification gap (disclosures don't meet stated goals), enforcement gap (failure to ensure compliance), and impact gap (disclosed information has limited practical effect).

Conclusion: The paper argues for moving beyond symbolic gestures and leveraging social science insights to create actionable, impactful transparency measures in AI that genuinely address quality, privacy, and copyright concerns.

Abstract: Data transparency has emerged as a rallying cry for addressing concerns about AI: data quality, privacy, and copyright chief among them. Yet while these calls are crucial for accountability, current transparency policies often fall short of their intended aims. Similar to nutrition facts for food, policies aimed at nutrition facts for AI currently suffer from a limited consideration of research on effective disclosures. We offer an institutional perspective and identify three common fallacies in policy implementations of data disclosures for AI. First, many data transparency proposals exhibit a specification gap between the stated goals of data transparency and the actual disclosures necessary to achieve such goals. Second, reform attempts exhibit an enforcement gap between required disclosures on paper and enforcement to ensure compliance in fact. Third, policy proposals manifest an impact gap between disclosed information and meaningful changes in developer practices and public understanding. Informed by the social science on transparency, our analysis identifies affirmative paths for transparency that are effective rather than merely symbolic.

</details>


### [646] [Beyond Pairwise Comparisons: A Distributional Test of Distinctiveness for Machine-Generated Works in Intellectual Property Law](https://arxiv.org/abs/2601.18156)
*Anirban Mukherjee,Hannah Hanwen Chang*

Main category: cs.CY

TL;DR: This paper explores a novel statistical approach to evaluate distinctions in outputs from creative processes, particularly generative AI systems.


<details>
  <summary>Details</summary>
Motivation: The increasing prominence of machine-generated works brings challenges in evaluating their originality and distinctiveness, especially when traditional analyses rely on finite, human-created corpora.

Method: The authors propose a distributional technique using a two-sample test based on maximum mean discrepancy and semantic embeddings to compare the outputs of creative processes without requiring specific training datasets.

Result: The method is validated across images, text, and AI-generated art, finding that it efficiently detects distinctions between human and machine outputs using minimal samples and highlights meaningful differences in distribution.

Conclusion: Generative models don't merely reproduce training data; rather, they output semantically human-like but stochastically distinct creations, acting as semantic interpolators in their learned latent space.

Abstract: Key doctrines, including novelty (patent), originality (copyright), and distinctiveness (trademark), turn on a shared empirical question: whether a body of work is meaningfully distinct from a relevant reference class. Yet analyses typically operationalize this set-level inquiry using item-level evidence: pairwise comparisons among exemplars. That unit-of-analysis mismatch may be manageable for finite corpora of human-created works, where it can be bridged by ad hoc aggregations. But it becomes acute for machine-generated works, where the object of evaluation is not a fixed set of works but a generative process with an effectively unbounded output space. We propose a distributional alternative: a two-sample test based on maximum mean discrepancy computed on semantic embeddings to determine if two creative processes-whether human or machine-produce statistically distinguishable output distributions. The test requires no task-specific training-obviating the need for discovery of proprietary training data to characterize the generative process-and is sample-efficient, often detecting differences with as few as 5-10 images and 7-20 texts. We validate the framework across three domains: handwritten digits (controlled images), patent abstracts (text), and AI-generated art (real-world images). We reveal a perceptual paradox: even when human evaluators distinguish AI outputs from human-created art with only about 58% accuracy, our method detects distributional distinctiveness. Our results present evidence contrary to the view that generative models act as mere regurgitators of training data. Rather than producing outputs statistically indistinguishable from a human baseline-as simple regurgitation would predict-they produce outputs that are semantically human-like yet stochastically distinct, suggesting their dominant function is as a semantic interpolator within a learned latent space.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [647] [Sparse RBF Networks for PDEs and nonlocal equations: function space theory, operator calculus, and training algorithms](https://arxiv.org/abs/2601.17562)
*Zihan Shao,Konstantin Pieper,Xiaochuan Tian*

Main category: math.NA

TL;DR: The paper analyzes and expands SparseRBFnet for nonlinear PDEs, establishing a function-space characterization, enhancing operator evaluation, and refining computational methods.


<details>
  <summary>Details</summary>
Motivation: To establish a deeper understanding and improvement of SparseRBFnet for accurately and efficiently solving nonlinear PDEs.

Method: Unified description of solution space, quasi-analytical operator evaluation, and comparison of related training and modeling approaches.

Result: Empirical robustness to kernel variations, trade-offs in accuracy, sparsity, and computational cost, validated by PDE benchmarks.

Conclusion: The work generalizes SparseRBFnet's framework, proving its efficacy in sparse representations and providing theoretical and practical insights for its use.

Abstract: This work presents a systematic analysis and extension of the sparse radial basis function network (SparseRBFnet) previously introduced for solving nonlinear partial differential equations (PDEs). Based on its adaptive-width shallow kernel network formulation, we further investigate its function-space characterization, operator evaluation, and computational algorithm. We provide a unified description of the solution space for a broad class of radial basis functions (RBFs). Under mild assumptions, this space admits a characterization as a Besov space, independent of the specific kernel choice. We further demonstrate how the explicit kernel-based structure enables quasi-analytical evaluation of both differential and nonlocal operators, including fractional Laplacians. On the computational end, we study the adaptive-width network and related three-phase training strategy through a comparison with variants concerning the modeling and algorithmic details. In particular, we assess the roles of second-order optimization, inner-weight training, network adaptivity, and anisotropic kernel parameterizations. Numerical experiments on high-order, fractional, and anisotropic PDE benchmarks illustrate the empirical insensitivity to kernel choice, as well as the resulting trade-offs between accuracy, sparsity, and computational cost. Collectively, these results consolidate and generalize the theoretical and computational framework of SparseRBFnet, supporting accurate sparse representations with efficient operator evaluation and offering theory-grounded guidance for algorithmic and modeling choices.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [648] [SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS](https://arxiv.org/abs/2601.17086)
*Ayush Pratap Singh,Harshit Singh,Nityanand Mathur,Akshat Mandloi,Sudarshan Kamath*

Main category: cs.SD

TL;DR: This paper introduces SonoEdit, a method to address pronunciation errors in text-to-speech (TTS) without retraining the model, by using Null-Space Pronunciation Editing.


<details>
  <summary>Details</summary>
Motivation: Existing TTS systems struggle to pronounce low-resource proper nouns, particularly non-English names and locations, due to underrepresentation in English-dominated training data. Current solutions are costly and unsuitable for diverse linguistic settings.

Method: The authors propose SonoEdit, which leverages Null-Space Pronunciation Editing. It performs a targeted parameter update using constrained editing to fix pronunciation errors without retraining or altering the model's other behavior.

Result: SonoEdit allows for the correction of specific pronunciation errors in pre-trained TTS models while guaranteeing no disturbances to general speech generation.

Conclusion: SonoEdit is a cost-effective and efficient solution for addressing pronunciation challenges in TTS, offering a novel approach that bypasses expensive data collection and fine-tuning.

Abstract: Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic locations, due to their underrepresentation in predominantly English training corpora. Existing solutions typically rely on expensive multilingual data collection, supervised finetuning, or manual phonetic annotation, which limits the deployment of TTS systems in linguistically diverse settings. We introduce SonoEdit, a model editing technique that surgically corrects pronunciation errors in pre-trained TTS models without retraining. Instead of costly finetuning or explicit phoneme injection, we propose a parsimonious alternative based on Null-Space Pronunciation Editing, which performs a single-shot parameter update to modify the pronunciation of specific words while provably preserving all other model behavior. We first adapt Acoustic Causal Tracing to identify the Transformer layers responsible for text-to-pronunciation mapping. We then apply Null-Space Constrained Editing to compute a closed-form weight update that corrects the target pronunciation while remaining mathematically orthogonal to the subspace governing general speech generation. This constrained update steers the model's acoustic output toward a desired pronunciation exemplar while guaranteeing zero first-order change on a preserved speech corpus.

</details>


### [649] [Window Size Versus Accuracy Experiments in Voice Activity Detectors](https://arxiv.org/abs/2601.17270)
*Max McKinnon,Samir Khaki,Chandan KA Reddy,William Huang*

Main category: cs.SD

TL;DR: The abstract examines the effect of window size on the accuracy of three VAD algorithms—Silero, WebRTC, RMS—while considering the impact of adding hysteresis.


<details>
  <summary>Details</summary>
Motivation: To optimize voice activity detection (VAD) systems, which are crucial for applications like speech recognition.

Method: The study analyzes the performance of Silero, WebRTC, and RMS VAD algorithms on real-world audio streams, along with investigating the integration of hysteresis into VAD outputs.

Result: Silero outperforms WebRTC and RMS, and hysteresis improves WebRTC's performance.

Conclusion: The findings provide practical guidance for enhancing VAD systems, highlighting the superiority of Silero and recommending hysteresis for WebRTC.

Abstract: Voice activity detection (VAD) plays a vital role in enabling applications such as speech recognition. We analyze the impact of window size on the accuracy of three VAD algorithms: Silero, WebRTC, and Root Mean Square (RMS) across a set of diverse real-world digital audio streams. We additionally explore the use of hysteresis on top of each VAD output. Our results offer practical references for optimizing VAD systems. Silero significantly outperforms WebRTC and RMS, and hysteresis provides a benefit for WebRTC.

</details>


### [650] [AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking](https://arxiv.org/abs/2601.17645)
*Xilin Jiang,Qiaolin Wang,Junkai Wu,Xiaomin He,Zhongweiyang Xu,Yinghao Ma,Minshuo Piao,Kaiyi Yang,Xiuwen Zheng,Riki Shimizu,Yicong Chen,Arsalan Firoozi,Gavin Mischler,Sukru Samet Dindar,Richard Antonello,Linyang He,Tsun-An Hsieh,Xulin Fan,Yulun Wu,Yuesheng Ma,Chaitanya Amballa,Weixiong Chen,Jiarui Hai,Ruisi Li,Vishal Choudhari,Cong Han,Yinghao Aaron Li,Adeen Flinker,Mounya Elhilali,Emmanouil Benetos,Mark Hasegawa-Johnson,Romit Roy Choudhury,Nima Mesgarani*

Main category: cs.SD

TL;DR: The paper introduces AVMeme Exam, a benchmark for evaluating AI models on understanding audio-visual Internet memes across cultural and contextual signals.


<details>
  <summary>Details</summary>
Motivation: To assess whether AI models can understand complex, cultural, and emotional signals conveyed by Internet audio-visual media beyond text-based information.

Method: The study develops AVMeme Exam, consisting of 1000 curated Internet audio-visual memes combined with unique Q&A pairs and metadata that test understanding at multiple levels. Human participants and state-of-the-art AI models were evaluated using this benchmark.

Result: Findings show that current AI models struggle with textless audio, such as music and sound effects, and understanding context and culture compared to humans.

Conclusion: Current AI models reveal a gap in human-aligned multimodal intelligence, indicating the need for systems capable of deeper cultural and contextual perception.

Abstract: Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public

</details>


### [651] [BanglaRobustNet: A Hybrid Denoising-Attention Architecture for Robust Bangla Speech Recognition](https://arxiv.org/abs/2601.17679)
*Md Sazzadul Islam Ridoy,Mubaswira Ibnat Zidney,Sumi Akter,Md. Aminur Rahman*

Main category: cs.SD

TL;DR: The paper introduces BanglaRobustNet, a robust automatic speech recognition framework for Bangla language, designed to improve performance under noisy and speaker-diverse settings.


<details>
  <summary>Details</summary>
Motivation: To address Bangla language's underrepresentation in automatic speech recognition research, particularly under conditions of environmental noise and speaker variability.

Method: The framework combines diffusion-based denoising to suppress noise, contextual cross-attention for speaker awareness, and end-to-end training with composite objectives including CTC loss, phonetic consistency, and speaker alignment.

Result: BanglaRobustNet significantly outperforms Wav2Vec-BERT and Whisper in reducing word error rate (WER) and character error rate (CER). Evaluations show effectiveness on Mozilla Common Voice Bangla dataset and noisy speech.

Conclusion: BanglaRobustNet is a tailored ASR solution for Bangla, demonstrating robustness in low-resource and noise-prone linguistic scenarios.

Abstract: Bangla, one of the most widely spoken languages, remains underrepresented in state-of-the-art automatic speech recognition (ASR) research, particularly under noisy and speaker-diverse conditions. This paper presents BanglaRobustNet, a hybrid denoising-attention framework built on Wav2Vec-BERT, designed to address these challenges. The architecture integrates a diffusion-based denoising module to suppress environmental noise while preserving Bangla-specific phonetic cues, and a contextual cross-attention module that conditions recognition on speaker embeddings for robustness across gender, age, and dialects. Trained end-to-end with a composite objective combining CTC loss, phonetic consistency, and speaker alignment, BanglaRobustNet achieves substantial reductions in word error rate (WER) and character error rate (CER) compared to Wav2Vec-BERT and Whisper baselines. Evaluations on Mozilla Common Voice Bangla and augmented noisy speech confirm the effectiveness of our approach, establishing BanglaRobustNet as a robust ASR system tailored to low-resource, noise-prone linguistic settings.

</details>


### [652] [OCR-Enhanced Multimodal ASR Can Read While Listening](https://arxiv.org/abs/2601.18393)
*Junli Chen,Changli Tang,Yixuan Li,Guangzhi Sun,Chao Zhang*

Main category: cs.SD

TL;DR: This paper introduces Donut-Whisper, a dual encoder audio-visual speech recognition (ASR) model that combines linear and Q-Former alignment structures to improve performance in English and Chinese. It also proposes a new multilingual dataset and uses knowledge distillation to enhance audio-only models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance speech recognition by utilizing visual information (like subtitles) alongside audio, particularly for multilingual contexts (English and Chinese), while addressing existing limitations in audio-only models.

Method: The proposed method is an audio-visual ASR model named Donut-Whisper with dual encoders combining linear and Q-Former alignment. It utilizes a cross-attention module, a lightweight knowledge distillation method to train audio-only models, and introduces a new multilingual dataset for testing.

Result: Donut-Whisper achieved significant reductions in word error rate (WER) and character error rate (CER) compared to baselines: 5.75% WER reduction for English and 16.5% CER reduction for Chinese.

Conclusion: Donut-Whisper proves the effectiveness of combining visual and audio features for ASR, outperforms baselines in English and Chinese speech recognition, and opens up new possibilities for knowledge transfer between audio-visual and audio-only models.

Abstract: Visual information, such as subtitles in a movie, often helps automatic speech recognition. In this paper, we propose Donut-Whisper, an audio-visual ASR model with dual encoder to leverage visual information to improve speech recognition performance in both English and Chinese. Donut-Whisper combines the advantage of the linear and the Q-Former-based modality alignment structures via a cross-attention module, generating more powerful audio-visual features. Meanwhile, we propose a lightweight knowledge distillation scheme showcasing the potential of using audio-visual models to teach audio-only models to achieve better performance. Moreover, we propose a new multilingual audio-visual speech recognition dataset based on movie clips containing both Chinese and English partitions. As a result, Donut-Whisper achieved significantly better performance on both English and Chinese partition of the dataset compared to both Donut and Whisper large V3 baselines. In particular, an absolute 5.75% WER reduction and a 16.5% absolute CER reduction were achieved on the English and Chinese sets respectively compared to the Whisper ASR baseline.

</details>


### [653] [Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance](https://arxiv.org/abs/2601.17690)
*Ziling Gong,Yunyan Ouyang,Iram Kamdar,Melody Ma,Hongjie Chen,Franck Dernoncourt,Ryan A. Rossi,Nesreen K. Ahmed*

Main category: cs.SD

TL;DR: This research studies the effect of audio segment length in neural audio fingerprinting models, finding that shorter segments generally improve performance, with insights offered for practical use in retrieval systems.


<details>
  <summary>Details</summary>
Motivation: The motivation was to address the lack of rigorous investigation into the impact of segment duration on audio fingerprinting accuracy in neural-based systems.

Method: The authors extended an existing neural fingerprinting architecture to evaluate the performance of various segment lengths and tested retrieval accuracy using different query durations and LLM recommendations.

Result: Short segment lengths, specifically 0.5 seconds, performed better. Additionally, GPT-5-mini gave optimal suggestions for segment durations compared to other LLMs.

Conclusion: Shorter audio segments enhance neural audio fingerprinting performance, and the findings offer valuable recommendations for optimizing retrieval systems using machine learning models.

Abstract: Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.

</details>


### [654] [CaSNet: Compress-and-Send Network Based Multi-Device Speech Enhancement Model for Distributed Microphone Arrays](https://arxiv.org/abs/2601.17711)
*Chengqian Jiang,Jie Zhang,Haoyin Yan*

Main category: cs.SD

TL;DR: Proposes Compress-and-Send Network (CaSNet) to enhance speech in distributed microphone arrays with lower bandwidth use.


<details>
  <summary>Details</summary>
Motivation: To address high bandwidth and energy costs in speech enhancement for distributed microphone arrays, owing to raw waveform transfer.

Method: Introduce CaSNet, where devices use SVD to compress feature matrices from raw data, align them with cross-window query, and decode for speech enhancement.

Result: CaSNet significantly reduces data transmission without sacrificing performance, as shown in experiments on multiple datasets.

Conclusion: CaSNet achieves efficient, high-quality speech enhancement for resource-constrained distributed microphone arrays, minimizing data transmission trade-offs.

Abstract: Distributed microphone array (DMA) is a promising next-generation platform for speech interaction, where speech enhancement (SE) is still required to improve the speech quality in noisy cases. Existing SE methods usually first gather raw waveforms at a fusion center (FC) from all devices and then design a multi-microphone model, causing high bandwidth and energy costs. In this work, we propose a \emph{Compress-and-Send Network (CaSNet)} for resource-constrained DMAs, where one microphone serves as the FC and reference. Each of other devices encodes the measured raw data into a feature matrix, which is then compressed by singular value decomposition (SVD) to produce a more compact representation. The received features at the FC are aligned via cross window query with respect to the reference, followed by neural decoding to yield spatially coherent enhanced speech. Experiments on multiple datasets show that the proposed CaSNet can save the data amount with a negligible impact on the performance compared to the uncompressed case. The reproducible code is available at https://github.com/Jokejiangv/CaSNet.

</details>


### [655] [EuleroDec: A Complex-Valued RVQ-VAE for Efficient and Robust Audio Coding](https://arxiv.org/abs/2601.17517)
*Luca Cerovaz,Michele Mancusi,Emanuele Rodolà*

Main category: cs.SD

TL;DR: This paper proposes a novel complex-valued audio codec that removes the need for adversarial discriminators or diffusion post-filters, achieving state-of-the-art performance with better computational efficiency and training speed.


<details>
  <summary>Details</summary>
Motivation: Audio codecs need to efficiently compress audio without compromising on spatial fidelity or perceptual quality. Current spectral-domain methods inadequately model phase information, which leads to suboptimal performance and reliance on adversarial discriminators for compensating shortcomings.

Method: The authors propose a complex-valued RVQ-VAE codec that maintains magnitude-phase coupling throughout the analysis, quantization, and synthesis pipeline. It avoids adversarial methods, relying instead on its architecture to accurately model phase coherence and waveform fidelity.

Result: The proposed codec achieves superior performance in out-of-domain tests for phase coherence and waveform fidelity while also requiring significantly fewer training steps compared to traditional methods.

Conclusion: This work demonstrates that a complex-valued approach to audio codecs can achieve state-of-the-art results more efficiently, both in terms of computational resources and perceptual quality, challenging conventional frameworks' reliance on adversarial techniques.

Abstract: Audio codecs power discrete music generative modelling, music streaming, and immersive media by shrinking PCM audio to bandwidth-friendly bitrates. Recent works have gravitated towards processing in the spectral domain; however, spectrogram domains typically struggle with phase modeling, which is naturally complex-valued. Most frequency-domain neural codecs either disregard phase information or encode it as two separate real-valued channels, limiting spatial fidelity. This entails the need to introduce adversarial discriminators at the expense of convergence speed and training stability to compensate for the inadequate representation power of the audio signal. In this work we introduce an end-to-end complex-valued RVQ-VAE audio codec that preserves magnitude-phase coupling across the entire analysis-quantization-synthesis pipeline and removes adversarial discriminators and diffusion post-filters. Without GANs or diffusion, we match or surpass much longer-trained baselines in-domain and reach SOTA out-of-domain performance on phase coherence and waveform fidelity. Compared to standard baselines that train for hundreds of thousands of steps, our model, which reduces the training budget by an order of magnitude, is markedly more compute-efficient while preserving high perceptual quality.

</details>


### [656] [VIBEVOICE-ASR Technical Report](https://arxiv.org/abs/2601.18184)
*Zhiliang Peng,Jianwei Yu,Yaoyao Chang,Zilong Wang,Li Dong,Yingbo Hao,Yujie Tu,Chenyu Yang,Wenhui Wang,Songchen Xu,Yutao Sun,Hangbo Bao,Weijiang Xu,Yi Zhu,Zehua Wang,Ting Song,Yan Xia,Zewen Chi,Shaohan Huang,Liang Wang,Chuang Ding,Shuai Wang,Xie Chen,Furu Wei*

Main category: cs.SD

TL;DR: This paper introduces VibeVoice-ASR, an advanced framework for handling long-form audio challenges including context fragmentation and multi-speaker complexity.


<details>
  <summary>Details</summary>
Motivation: To address critical issues like context fragmentation and multi-speaker complexity in long-form speech understanding, which persist despite progress in short-form speech recognition.

Method: The method involves creating a unified system with single-pass processing for up to 60 minutes of audio, integrating Automatic Speech Recognition, Speaker Diarization, Timestamping, and prompt-based context injection.

Result: VibeVoice-ASR overcomes limitations in traditional approaches, supports over 50 languages, and handles code-switching, enhancing domain-specific accuracy and character disambiguation.

Conclusion: The framework represents a significant step forward in streamlining long-form speech understanding and enabling customizable speech recognition tasks.

Abstract: This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.

</details>


### [657] [Analytic Incremental Learning For Sound Source Localization With Imbalance Rectification](https://arxiv.org/abs/2601.18335)
*Zexia Fan,Yu Chen,Qiquan Zhang,Kainan Chen,Xinyuan Qian*

Main category: cs.SD

TL;DR: This paper addresses challenges in sound source localization due to dual imbalance issues. It introduces a unified framework with data augmentation and dynamic imbalance rectification, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Sound source localization performs well in controlled settings but fails in real-world environments due to intra-task (distribution skewed DoA) and inter-task (cross-task interactions) imbalances, resulting in accuracy loss.

Method: The proposed framework uses a GCC-PHAT-based data augmentation (GDA) for intra-task imbalance mitigation and an Analytic dynamic imbalance rectifier (ADIR) with task-adaption regularization for inter-task imbalance.

Result: The framework achieved 89.0% accuracy, 5.3° mean absolute error, and 1.6 backward transfer on the SSLR benchmark, setting a new state-of-the-art.

Conclusion: This unified approach effectively handles evolving imbalances in sound localization tasks, improving performance without requiring exemplar storage.

Abstract: Sound source localization (SSL) demonstrates remarkable results in controlled settings but struggles in real-world deployment due to dual imbalance challenges: intra-task imbalance arising from long-tailed direction-of-arrival (DoA) distributions, and inter-task imbalance induced by cross-task skews and overlaps. These often lead to catastrophic forgetting, significantly degrading the localization accuracy. To mitigate these issues, we propose a unified framework with two key innovations. Specifically, we design a GCC-PHAT-based data augmentation (GDA) method that leverages peak characteristics to alleviate intra-task distribution skews. We also propose an Analytic dynamic imbalance rectifier (ADIR) with task-adaption regularization, which enables analytic updates that adapt to inter-task dynamics. On the SSLR benchmark, our proposal achieves state-of-the-art (SoTA) results of 89.0% accuracy, 5.3° mean absolute error, and 1.6 backward transfer, demonstrating robustness to evolving imbalances without exemplar storage.

</details>


### [658] [A Dataset for Automatic Vocal Mode Classification](https://arxiv.org/abs/2601.18339)
*Reemt Hinrichs,Sonja Stephan,Alexander Lange,Jörn Ostermann*

Main category: cs.SD

TL;DR: The paper presents a dataset for vocal mode classification in singing, focusing on Cathrin Sadolin's Complete Vocal Technique (CVT). The dataset includes samples from singers, recorded using multiple microphones, and annotations from experts.


<details>
  <summary>Details</summary>
Motivation: To improve technology-assisted singing teaching by enabling the automatic classification of vocal modes, which has been hindered by the lack of sufficient data.

Method: A novel vocal mode dataset was recorded from singers, utilizing multiple microphones for data augmentation. Three CVT-experienced annotators provided detailed annotations.

Result: A dataset consisting of more than 13,000 samples was created and evaluated, achieving a balanced accuracy of 81.3% using ResNet18 in classification tasks.

Conclusion: The dataset offers a valuable resource for studying vocal modes and provides baseline classification results, laying a foundation for advancements in technology-assisted vocal training methods.

Abstract: The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\,\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.

</details>


### [659] [Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings](https://arxiv.org/abs/2601.18694)
*Aayush M. Shrestha,Aditya Bajracharya,Projan Shakya,Dinesh B. Kshatri*

Main category: cs.SD

TL;DR: Developed a few-shot voice cloning system for Nepali speakers, synthesizing speech using sparse data and achieving promising results.


<details>
  <summary>Details</summary>
Motivation: Voice cloning for Nepali is unexplored due to its low-resource nature, requiring innovative techniques to personalize speech synthesis.

Method: The system employs a speaker encoder optimized with Generative End2End loss and a Tacotron2-based synthesizer, combining generated embeddings with text embeddings, and uses WaveRNN vocoder for audio output.

Result: Achieved effective cloning of speaker characteristics, even for previously unseen voices, proving feasibility for low-resource languages.

Conclusion: The study establishes groundwork for few-shot speaker-specific synthesis in Nepali, addressing the low-resource challenge and enabling personalized speech solutions.

Abstract: This research presents a few-shot voice cloning system for Nepali speakers, designed to synthesize speech in a specific speaker's voice from Devanagari text using minimal data. Voice cloning in Nepali remains largely unexplored due to its low-resource nature. To address this, we constructed separate datasets: untranscribed audio for training a speaker encoder and paired text-audio data for training a Tacotron2-based synthesizer. The speaker encoder, optimized with Generative End2End loss, generates embeddings that capture the speaker's vocal identity, validated through Uniform Manifold Approximation and Projection (UMAP) for dimension reduction visualizations. These embeddings are fused with Tacotron2's text embeddings to produce mel-spectrograms, which are then converted into audio using a WaveRNN vocoder. Audio data were collected from various sources, including self-recordings, and underwent thorough preprocessing for quality and alignment. Training was performed using mel and gate loss functions under multiple hyperparameter settings. The system effectively clones speaker characteristics even for unseen voices, demonstrating the feasibility of few-shot voice cloning for the Nepali language and establishing a foundation for personalized speech synthesis in low-resource scenarios.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [660] [Embodiment-Induced Coordination Regimes in Tabular Multi-Agent Q-Learning](https://arxiv.org/abs/2601.17454)
*Muhammad Ahmed Atif,Nehal Naeem Haji,Mohammad Shahid Shaikh,Muhammad Ebad Atif*

Main category: cs.MA

TL;DR: Centralized value learning doesn't consistently outperform independent learning in multi-agent scenarios, especially under embodiment constraints, with results depending heavily on the regime and agent roles.


<details>
  <summary>Details</summary>
Motivation: Previous studies assumed centralized learning improves coordination and stability but rarely tested this under controlled conditions and specific constraints.

Method: Direct evaluation in a fully tabular predator-prey gridworld, incorporating constraints like agent speed, stamina, observability, and exact value estimation.

Result: Centralized learning proved inconsistent and was often outperformed by independent learning. Asymmetric centralized-independent setups led to persistent coordination problems.

Conclusion: Effectiveness of centralized learning is not universal and is contingent on the coordination structure, agent embodiment constraints, and role-dependent kinematic regimes.

Abstract: Centralized value learning is often assumed to improve coordination and stability in multi-agent reinforcement learning, yet this assumption is rarely tested under controlled conditions. We directly evaluate it in a fully tabular predator-prey gridworld by comparing independent and centralized Q-learning under explicit embodiment constraints on agent speed and stamina. Across multiple kinematic regimes and asymmetric agent roles, centralized learning fails to provide a consistent advantage and is frequently outperformed by fully independent learning, even under full observability and exact value estimation. Moreover, asymmetric centralized-independent configurations induce persistent coordination breakdowns rather than transient learning instability. By eliminating confounding effects from function approximation and representation learning, our tabular analysis isolates coordination structure as the primary driver of these effects. The results show that increased coordination can become a liability under embodiment constraints, and that the effectiveness of centralized learning is fundamentally regime and role dependent rather than universal.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [661] [Benchmarking Deep Learning-Based Reconstruction Methods for Photoacoustic Computed Tomography with Clinically Relevant Synthetic Datasets](https://arxiv.org/abs/2601.17165)
*Panpan Chen,Seonyeong Park,Gangwon Jeong,Refik Mert Cam,Umberto Villa,Mark A. Anastasio*

Main category: physics.med-ph

TL;DR: The paper introduces a benchmarking framework with synthetic datasets to standardize and improve evaluation of DL-based image reconstruction methods for photoacoustic computed tomography (PACT).


<details>
  <summary>Details</summary>
Motivation: Most existing DL-based reconstruction methods in PACT lack standardized datasets and rely on traditional, clinically irrelevant metrics, which hampers fair comparisons and reproducibility.

Method: The framework uses anatomically plausible synthetic datasets containing over 11,000 2D stochastic breast objects with lesions and pairs them with task-based and traditional image quality measures for evaluation.

Result: A benchmarking study illustrates the framework's capacity to reveal the strengths and limitations of DL- and physics-based methods. Task-based metrics exposed certain DL-based methods' failure to accurately recover lesions despite good traditional metrics performance.

Conclusion: The framework offers a standardized, clinically relevant approach for evaluating and advancing DL-based acoustic inversion methods, enabling reproducible assessments and system optimization in PACT.

Abstract: Deep learning (DL)-based image reconstruction methods for photoacoustic computed tomography (PACT) have developed rapidly in recent years. However, most existing methods have not employed standardized datasets, and their evaluations rely on traditional image quality (IQ) metrics that may lack clinical relevance. The absence of a standardized framework for clinically meaningful IQ assessment hinders fair comparison and raises concerns about the reproducibility and reliability of reported advancements in PACT. A benchmarking framework is proposed that provides open-source, anatomically plausible synthetic datasets and evaluation strategies for DL-based acoustic inversion methods in PACT. The datasets each include over 11,000 two-dimensional (2D) stochastic breast objects with clinically relevant lesions and paired measurements at varying modeling complexity. The evaluation strategies incorporate both traditional and task-based IQ measures to assess fidelity and clinical utility. A preliminary benchmarking study is conducted to demonstrate the framework's utility by comparing DL-based and physics-based reconstruction methods. The benchmarking study demonstrated that the proposed framework enabled comprehensive, quantitative comparisons of reconstruction performance and revealed important limitations in certain DL-based methods. Although they performed well according to traditional IQ measures, they often failed to accurately recover lesions. This highlights the inadequacy of traditional metrics and motivates the need for task-based assessments. The proposed benchmarking framework enables systematic comparisons of DL-based acoustic inversion methods for 2D PACT. By integrating clinically relevant synthetic datasets with rigorous evaluation protocols, it enables reproducible, objective assessments and facilitates method development and system optimization in PACT.

</details>


### [662] [Automated HER2 scoring with uncertainty quantification using lensfree holography and deep learning](https://arxiv.org/abs/2601.18219)
*Che-Yung Shen,Xilin Yang,Yuzhu Li,Leon Lenk,Aydogan Ozcan*

Main category: physics.med-ph

TL;DR: The paper presents a cost-effective lensfree holography system with deep learning for automated HER2 scoring in breast cancer, achieving high testing accuracy and robustness for resource-limited settings.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate and accessible HER2 scoring for breast cancer diagnosis in resource-limited settings, where traditional systems are bulky and expensive.

Method: The researchers developed a lensfree holography platform that captures diffraction patterns of HER2-stained tissue sections with RGB lasers and incorporates deep learning with Bayesian Monte Carlo dropout for uncertainty quantification.

Result: The system demonstrated a 4-class HER2 classification accuracy of 84.9% and binary classification accuracy of 94.8%, along with an uncertainty quantification strategy that improved diagnostic reliability.

Conclusion: This lensfree holography platform offers a practical, portable, and cost-effective solution for HER2 scoring in breast cancer, suitable for areas lacking advanced digital pathology infrastructure.

Abstract: Accurate assessment of human epidermal growth factor receptor 2 (HER2) expression is critical for breast cancer diagnosis, prognosis, and therapy selection; yet, most existing digital HER2 scoring methods rely on bulky and expensive optical systems. Here, we present a compact and cost-effective lensfree holography platform integrated with deep learning for automated HER2 scoring of immunohistochemically stained breast tissue sections. The system captures lensfree diffraction patterns of stained HER2 tissue sections under RGB laser illumination and acquires complex field information over a sample area of ~1,250 mm^2 at an effective throughput of ~84 mm^2 per minute. To enhance diagnostic reliability, we incorporated an uncertainty quantification strategy based on Bayesian Monte Carlo dropout, which provides autonomous uncertainty estimates for each prediction and supports reliable, robust HER2 scoring, with an overall correction rate of 30.4%. Using a blinded test set of 412 unique tissue samples, our approach achieved a testing accuracy of 84.9% for 4-class (0, 1+, 2+, 3+) HER2 classification and 94.8% for binary (0/1+ vs. 2+/3+) HER2 scoring with uncertainty quantification. Overall, this lensfree holography approach provides a practical pathway toward portable, high-throughput, and cost-effective HER2 scoring, particularly suited for resource-limited settings, where traditional digital pathology infrastructure is unavailable.

</details>


### [663] [MlPET: A Localized Neural Network Approach for Probabilistic Post-Reconstruction PET Image Analysis Using Informed Priors](https://arxiv.org/abs/2601.18021)
*Thomas Mejer Hansen,Nana Christensen,Mikkel Vendelbo*

Main category: physics.med-ph

TL;DR: MlPET is a localized machine learning method designed to enhance noise suppression and resolution in PET image analysis without altering reconstruction algorithms. It achieves faster processing while maintaining high image quality.


<details>
  <summary>Details</summary>
Motivation: To address the noise-resolution trade-off and computational inefficiency in conventional PET image reconstructions using a streamlined, machine learning-based solution.

Method: MlPET employs a neural network replacing complex Markov chain Monte Carlo sampling to estimate voxel activity. It integrates scanner-specific point spread functions, noise modeling, and flexible priors.

Result: MlPET showed higher contrast recovery coefficients and reduced background noise compared to standard PET, achieving sharper image resolution with acquisition time significantly shorter than conventional PET.

Conclusion: MlPET is a promising approach for faster and more accurate probabilistic PET image analysis, improving small-lesion detectability and reliability for clinical use, with future studies planned for patient data testing.

Abstract: We develop and evaluate MlPET, a fast localized machine learning approach for probabilistic PET image analysis addressing the noise-resolution trade-off in conventional reconstructions. MlPET replaces computationally demanding Markov chain Monte Carlo sampling with a localized neural network trained to estimate posterior mean voxel activity from small image neighborhoods. The method incorporates scanner-specific point spread functions, spatially correlated noise modeling, and flexible priors. Performance was evaluated on NEMA IEC phantom data from three PET systems (GE Discovery MI, Siemens Biograph Vision 600, and Quadra) under varying reconstruction settings and acquisition times. On phantom data, MlPET achieved contrast recovery coefficients consistently higher than standard PET and close to 1.0 (including 10 mm spheres), while reducing background noise and improving spatial definition. Effective pointspread function full width at half maximum decreased from approximately 2 mm in standard PET to below 1 mm with MlPET, a 2.5 fold reduction in blur. Comparable image quality was obtained at 40-80 s acquisition time with MlPET versus 900 s with conventional PET. MlPET provides an efficient approach for quantitative probabilistic post-reconstruction PET analysis. By combining informed priors with neural network speed, it achieves noise suppression and resolution enhancement without altering reconstruction algorithms. The method shows promise for improved small-lesion detectability and quantitative reliability in clinical PET imaging. Future studies will evaluate performance on patient data.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [664] [PALMA: A Lightweight Tropical Algebra Library for ARM-Based Embedded Systems](https://arxiv.org/abs/2601.17028)
*Gnankan Landry Regis N'guessan*

Main category: cs.MS

TL;DR: This paper introduces PALMA, a tropical algebra library optimized for ARM-based embedded systems, demonstrating significant performance improvements in various optimization applications.


<details>
  <summary>Details</summary>
Motivation: To address the lack of tropical algebra implementations for resource-constrained embedded systems, which are critical for solving optimization problems in areas like shortest paths and scheduling.

Method: The authors developed PALMA, a dependency-free C library with a generic semiring abstraction and SIMD-accelerated kernels, supporting dense and sparse algebraic operations and tropical semirings.

Result: PALMA achieves performance up to 2,274 MOPS, an 11.9x speedup over traditional methods for shortest paths, and outperforms in real-time control workloads on embedded platforms, validated through diverse case studies.

Conclusion: PALMA enables efficient, predictable, and versatile optimization on embedded systems and is available as open-source software.

Abstract: Tropical algebra, including max-plus, min-plus, and related idempotent semirings, provides a unifying framework in which many optimization problems that are nonlinear in classical algebra become linear. This property makes tropical methods particularly well suited for shortest paths, scheduling, throughput analysis, and discrete event systems. Despite their theoretical maturity and practical relevance, existing tropical algebra implementations primarily target desktop or server environments and remain largely inaccessible on resource-constrained embedded platforms, where such optimization problems are most acute. We present PALMA (Parallel Algebra Library for Max-plus Applications), a lightweight, dependency-free C library that brings tropical linear algebra to ARM-based embedded systems. PALMA implements a generic semiring abstraction with SIMD-accelerated kernels, enabling a single computational framework to support shortest paths, bottleneck paths, reachability, scheduling, and throughput analysis. The library supports five tropical semirings, dense and sparse (CSR) representations, tropical closure, and spectral analysis via maximum cycle mean computation. We evaluate PALMA on a Raspberry Pi 4 and demonstrate peak performance of 2,274 MOPS, speedups of up to 11.9 times over classical Bellman-Ford for single-source shortest paths, and sub-10 microsecond scheduling solves for real-time control workloads. Case studies in UAV control, IoT routing, and manufacturing systems show that tropical algebra enables efficient, predictable, and unified optimization directly on embedded hardware. PALMA is released as open-source software under the MIT license.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [665] [Sampling in the Euclidean Motion Group and a Problem from Brain's Primary Visual Cortex](https://arxiv.org/abs/2601.17528)
*Davide Barbieri*

Main category: math.FA

TL;DR: The paper investigates sampling issues for abstract wavelet transforms connected with $SE(2)$ group representations using a modulated Gaussian wavelet, motivated by visual cortex functionality. Key relationships and characterizations are studied.


<details>
  <summary>Details</summary>
Motivation: The research is inspired by how the primary visual cortex in the brain behaves and processes information, leading to exploring the sampling configurations for abstract wavelet transforms linked to the $SE(2)$ group.

Method: A mathematical characterization of the problem is given using the dual Gramian matrix, followed by numerical analysis to investigate parameter interrelations concerning sampling and wavelet design.

Result: Numerical studies reveal how sampling parameters interact with the properties of the modulated Gaussian mother wavelet, highlighting key dependencies.

Conclusion: The findings provide insight into parameter configurations of the wavelet transform correlated with $SE(2)$, contributing to understanding sampling issues in applications such as visual processing systems.

Abstract: We study a sampling problem for the abstract wavelet transform associated with the quasiregular representation of the $SE(2)$ group, for a modulated gaussian mother wavelet. This problem is motivated by the behavior of brain's primary visual cortex. We provide a characterization in terms of a dual Gramian matrix, and study numerically the relationships among the parameters defining the sampling and the mother wavelet.

</details>


### [666] [Use of operator defect identities in multi-channel signal plus residual-analysis via iterated products and telescoping energy-residuals: Applications to kernels in machine learning](https://arxiv.org/abs/2601.18080)
*Palle E. T. Jorgensen,Myung-Sin Song,James F. Tian*

Main category: math.FA

TL;DR: This paper introduces a novel operator-based method for analyzing complex systems, particularly focusing on energy residuals, with applications in machine learning and stability in noisy systems.


<details>
  <summary>Details</summary>
Motivation: To develop a theoretical framework that enables effective analysis of complex systems divided into components, solving issues like stability, convergence, and noise robustness.

Method: The paper uses operator-theoretic approaches and establishes mathematical proofs, including a priori bounds on energy residuals. It applies these to infinite-dimensional Kaczmarz theory, Kernel PCA, and generalized machine learning algorithms.

Result: New results ensure admissibility, effectiveness, and energy residual decomposition. Applications show stability in noise environments, explicit convergence in Kernel PCA, and practical algorithms.

Conclusion: The framework provides a significant advancement in analyzing and optimizing complex systems, particularly in machine learning and noise-affected scenarios.

Abstract: We present a new operator theoretic framework for analysis of complex systems with intrinsic subdivisions into components, taking the form of "residuals" in general, and "telescoping energy residuals" in particular. We prove new results which yield admissibility/effectiveness, and new a priori bounds on energy residuals. Applications include infinite-dimensional Kaczmarz theory for $λ_{n}$-relaxed variants, and $λ_{n}$-effectiveness. And we give applications of our framework to generalized machine learning algorithms, greedy Kernel Principal Component Analysis (KPCA), proving explicit convergence results, residual energy decomposition, and criteria for stability under noise.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [667] [Acoustic Field Video for Multimodal Scene Understanding](https://arxiv.org/abs/2601.17123)
*Daehwa Kim,Chris Harrison*

Main category: cs.HC

TL;DR: This paper introduces a novel input representation for vision-language models using spatial acoustic data to enhance scene understanding and multimodal reasoning.


<details>
  <summary>Details</summary>
Motivation: Conventional vision-language models rely on visual and basic audio inputs, which limit scene understanding. The authors aim to explore the potential of incorporating spatial acoustic information to improve perceptual reasoning.

Method: A real-time pipeline processes acoustic field video using low-cost beamforming microphone arrays. These arrays visualize sound intensity spatially for scene understanding tasks. An evaluation set of 402 question-answer scenes was created to compare performance with and without acoustic field video.

Result: The tested vision-language model's performance improved from 38.3% to 67.4% correct answers when using spatial acoustic data alongside conventional video.

Conclusion: Incorporating spatial sound visualization significantly enhances scene understanding and suggests a practical, unexplored potential for multimodal inputs in future technologies.

Abstract: We introduce and explore a new multimodal input representation for vision-language models: acoustic field video. Unlike conventional video (RGB with stereo/mono audio), our video stream provides a spatially grounded visualization of sound intensity across a scene, offering a new and powerful dimension of perceptual understanding. Our real-time pipeline uses low-cost beamforming microphone arrays that are already common in smart speakers and increasingly present in robotics and XR headsets, yet this sensing capability remains unutilized for scene understanding. To assess the value of spatial acoustic information, we constructed an evaluation set of 402 question-answer scenes, comparing a state-of-the-art VLM given conventional video with and without paired acoustic field video. Results show a clear and consistent improvement when incorporating spatial acoustic data; the VLM we test improves from 38.3% correct to 67.4%. Our findings highlight that many everyday scene understanding tasks remain underconstrained when relying solely on visual and audio input, and that acoustic field data provides a promising and practical direction for multimodal reasoning. A video demo is available at https://daehwakim.com/seeingsound

</details>


### [668] [Lost in Simulation: LLM-Simulated Users are Unreliable Proxies for Human Users in Agentic Evaluations](https://arxiv.org/abs/2601.17087)
*Preethi Seshadri,Samuel Cahyawijaya,Ayomide Odumakinde,Sameer Singh,Seraphina Goldfarb-Tarrant*

Main category: cs.HC

TL;DR: The paper evaluates the validity and fairness of using LLM-simulated users to test agent performance and identifies significant disparities and limitations.


<details>
  <summary>Details</summary>
Motivation: To address the unexamined robustness, reliability, and fairness of LLM-simulated users as proxies for real humans in evaluating agents on tasks.

Method: Conducted a user study across diverse populations in the U.S., India, Kenya, and Nigeria and analyzed agent performance using LLM-simulated users on τ-Bench retail tasks.

Result: LLM-simulated users show varying success rates across user models, systematic miscalibration, and performance disparities for certain populations like AAVE and Indian English speakers.

Conclusion: Using LLM-simulated users risks inaccurate evaluation of agents, misrepresenting capabilities across diverse populations, and masking deployment challenges.

Abstract: Agentic benchmarks increasingly rely on LLM-simulated users to scalably evaluate agent performance, yet the robustness, validity, and fairness of this approach remain unexamined. Through a user study with participants across the United States, India, Kenya, and Nigeria, we investigate whether LLM-simulated users serve as reliable proxies for real human users in evaluating agents on τ-Bench retail tasks. We find that user simulation lacks robustness, with agent success rates varying up to 9 percentage points across different user LLMs. Furthermore, evaluations using simulated users exhibit systematic miscalibration, underestimating agent performance on challenging tasks and overestimating it on moderately difficult ones. African American Vernacular English (AAVE) speakers experience consistently worse success rates and calibration errors than Standard American English (SAE) speakers, with disparities compounding significantly with age. We also find simulated users to be a differentially effective proxy for different populations, performing worst for AAVE and Indian English speakers. Additionally, simulated users introduce conversational artifacts and surface different failure patterns than human users. These findings demonstrate that current evaluation practices risk misrepresenting agent capabilities across diverse user populations and may obscure real-world deployment challenges.

</details>


### [669] [Status Hierarchies in Language Models](https://arxiv.org/abs/2601.17577)
*Emilio Barkett*

Main category: cs.HC

TL;DR: This paper explores whether and how language models (LMs) develop status hierarchies in multi-agent settings, revealing emergent status-based behaviors influenced by capability and status cues.


<details>
  <summary>Details</summary>
Motivation: Investigating how status hierarchies form in language models due to their exposure to human social dynamics, and assessing the implications for AI safety and alignment.

Method: Using a multi-agent framework where LMs engage in sentiment classification tasks, are assigned status characteristics, and can revise judgments based on their partner's responses to measure deference influenced by status cues.

Result: LMs form notable status hierarchies when equally capable (35 percentage point asymmetry), but actual capability outweighs status cues; high-status labels reduce deference from capable models, revealing complex interactions between status and competence.

Conclusion: Emergent status dynamics in LMs highlight social behaviors in AI that could lead to biased or deceptive strategies, raising crucial concerns for AI safety and alignment issues.

Abstract: From school playgrounds to corporate boardrooms, status hierarchies -- rank orderings based on respect and perceived competence -- are universal features of human social organization. Language models trained on human-generated text inevitably encounter these hierarchical patterns embedded in language, raising the question of whether they might reproduce such dynamics in multi-agent settings. This thesis investigates when and how language models form status hierarchies by adapting Berger et al.'s (1972) expectation states framework. I create multi-agent scenarios where separate language model instances complete sentiment classification tasks, are introduced with varying status characteristics (e.g., credentials, expertise), then have opportunities to revise their initial judgments after observing their partner's responses. The dependent variable is deference, the rate at which models shift their ratings toward their partner's position based on status cues rather than task information. Results show that language models form significant status hierarchies when capability is equal (35 percentage point asymmetry, p < .001), but capability differences dominate status cues, with the most striking effect being that high-status assignments reduce higher-capability models' deference rather than increasing lower-capability models' deference. The implications for AI safety are significant: status-seeking behavior could introduce deceptive strategies, amplify discriminatory biases, and scale across distributed deployments far faster than human hierarchies form organically. This work identifies emergent social behaviors in AI systems and highlights a previously underexplored dimension of the alignment challenge.

</details>


### [670] [Memento: Towards Proactive Visualization of Everyday Memories with Personal Wearable AR Assistant](https://arxiv.org/abs/2601.17622)
*Yoonsang Kim,Yalong Yang,Arie E. Kaufman*

Main category: cs.HC

TL;DR: Memento is a conversational AR assistant that memorizes user queries in their context to provide proactive responses based on recurring interests.


<details>
  <summary>Details</summary>
Motivation: To design an AR system capable of proactive, long-term, and context-aware assistance in daily tasks.

Method: Memento integrates verbal queries with spatiotemporal and activity context data, allowing it to recall user interests and respond proactively via AR.

Result: Preliminary user feedback demonstrates the value of a proactive, context-aware AR assistant in various daily scenarios.

Conclusion: Memento redefines AR interactions by emphasizing long-term contextual memory and proactive assistance in users' daily routines.

Abstract: We introduce Memento, a conversational AR assistant that permanently captures and memorizes user's verbal queries alongside their spatiotemporal and activity contexts. By storing these "memories," Memento discovers connections between users' recurring interests and the contexts that trigger them. Upon detection of similar or identical spatiotemporal activity, Memento proactively recalls user interests and delivers up-to-date responses through AR, seamlessly integrating AR experience into their daily routine. Unlike prior work, each interaction in Memento is not a transient event, but a connected series of interactions with coherent long--term perspective, tailored to the user's broader multimodal (visual, spatial, temporal, and embodied) context. We conduct preliminary evaluation through user feedbacks with participants of diverse expertise in immersive apps, and explore the value of proactive context-aware AR assistant in everyday settings. We share our findings and challenges in designing a proactive, context-aware AR system.

</details>


### [671] [PaperTok: Exploring the Use of Generative AI for Creating Short-form Videos for Research Communication](https://arxiv.org/abs/2601.18218)
*Meziah Ruby Cristobal,Hyeonjeong Byeon,Tze-Yu Chen,Ruoxi Shang,Donghoon Shin,Ruican Zhong,Tony Zhou,Gary Hsieh*

Main category: cs.HC

TL;DR: The study presents PaperTok, a generative AI-based system to help researchers convert academic papers into engaging short-form videos for popular media.


<details>
  <summary>Details</summary>
Motivation: Researchers struggle to effectively communicate their work through popular media due to a lack of time and skills to create engaging content, especially short-form videos.

Method: The authors developed and evaluated PaperTok, a system that generates initial video scripts and audiovisual content from academic papers, based on insights from science communicators and tested through a user study and crowdsourced evaluation.

Result: The study found that PaperTok helps researchers create informative and engaging short-form videos. However, users expressed a need for finer control over the creation process.

Conclusion: PaperTok can assist researchers with science outreach, but future tools should focus on enabling more tailored and detailed content creation to better meet researchers' needs.

Abstract: The dissemination of scholarly research is critical, yet researchers often lack the time and skills to create engaging content for popular media such as short-form videos. To address this gap, we explore the use of generative AI to help researchers transform their academic papers into accessible video content. Informed by a formative study with science communicators and content creators (N=8), we designed PaperTok, an end-to-end system that automates the initial creative labor by generating script options and corresponding audiovisual content from a source paper. Researchers can then refine based on their preferences with further prompting. A mixed-methods user study (N=18) and crowdsourced evaluation (N=100) demonstrate that PaperTok's workflow can help researchers create engaging and informative short-form videos. We also identified the need for more fine-grained controls in the creation process. To this end, we offer implications for future generative tools that support science outreach.

</details>


### [672] [Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System](https://arxiv.org/abs/2601.18785)
*Tiffany Wang,Yuqian Sun,Yi Wang,Melissa Roemmele,John Joon Young Chung,Max Kreminski*

Main category: cs.HC

TL;DR: The paper discusses Dramamancer, a system utilizing Large Language Models (LLMs) to convert story schemas into interactive, player-driven experiences.


<details>
  <summary>Details</summary>
Motivation: To enhance interactive narratives by leveraging LLMs to address the balance between authorial intent and player agency.

Method: The paper introduces and evaluates techniques in Dramamancer that use LLMs to transform predefined story schemas into dynamic, player-focused playthroughs.

Result: Dramamancer demonstrates the ability to bridge structured stories and interactive, player-driven experiences effectively through the use of LLMs.

Conclusion: The work highlights the potential of LLMs in revolutionizing interactive narrative design, emphasizing the importance of balancing authorial control and player freedom.

Abstract: The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.

</details>


### [673] [MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data](https://arxiv.org/abs/2601.18792)
*Brian Liu,Oiwi Parker Jones*

Main category: cs.HC

TL;DR: This paper explores using pre-trained models to annotate sentiment in brain recordings obtained during audiobook listening sessions and develops Brain-to-Sentiment decoding models.


<details>
  <summary>Details</summary>
Motivation: The goal is to understand sentiment decoding directly from brain activity, as existing datasets lack sentiment annotations for brain data.

Method: They used pre-trained Text-to-Sentiment models to annotate sentiment for brain recordings acquired via MEG during audiobook listening. Force-alignment methodology was utilized to align text sentiment labels with brain activity data.

Result: The proposed Brain-to-Sentiment models show improved balanced accuracy compared to baseline models, validating the approach.

Conclusion: This work demonstrates the feasibility of decoding sentiment directly from brain data, providing a new pathway for leveraging existing MEG datasets for emotion research.

Abstract: Decoding emotion from brain activity could unlock a deeper understanding of the human experience. While a number of existing datasets align brain data with speech and with speech transcripts, no datasets have annotated brain data with sentiment. To bridge this gap, we explore the use of pre-trained Text-to-Sentiment models to annotate non invasive brain recordings, acquired using magnetoencephalography (MEG), while participants listened to audiobooks. Having annotated the text, we employ force-alignment of the text and audio to align our sentiment labels with the brain recordings. It is straightforward then to train Brainto-Sentiment models on these data. Experimental results show an improvement in balanced accuracy for Brain-to-Sentiment compared to baseline, supporting the proposed approach as a proof-of-concept for leveraging existing MEG datasets and learning to decode sentiment directly from the brain.

</details>


### [674] [Athanor: Authoring Action Modification-based Interactions on Static Visualizations via Natural Language](https://arxiv.org/abs/2601.17736)
*Can Liu,Jaeuk Lee,Tianhe Chen,Zhibang Jiang,Xiaolin Wen,Yong Wang*

Main category: cs.HC

TL;DR: Athanor is an approach to make static visualizations interactive using multimodal large language models (MLLMs) and natural language instructions, without requiring programming or original code.


<details>
  <summary>Details</summary>
Motivation: Static visualizations often lack interactivity due to unavailability of source code or significant effort required to enhance interactivity.

Method: Athanor uses an action-modification interaction design space, multi-agent requirement analyzer, and a visualization abstraction transformer.

Result: Case studies and user interviews show Athanor's effectiveness and usability in enabling interactions for static visualizations.

Conclusion: Athanor simplifies creating interactive visualizations by leveraging natural language instructions, bridging the gap in interactivity for existing static visualizations.

Abstract: Interactivity is crucial for effective data visualizations. However, it is often challenging to implement interactions for existing static visualizations, since the underlying code and data for existing static visualizations are often not available, and it also takes significant time and effort to enable interactions for them even if the original code and data are available. To fill this gap, we propose Athanor, a novel approach to transform existing static visualizations into interactive ones using multimodal large language models (MLLMs) and natural language instructions. Our approach introduces three key innovations: (1) an action-modification interaction design space that maps visualization interactions into user actions and corresponding adjustments, (2) a multi-agent requirement analyzer that translates natural language instructions into an actionable operational space, and (3) a visualization abstraction transformer that converts static visualizations into flexible and interactive representations regardless of their underlying implementation. Athanor allows users to effortlessly author interactions through natural language instructions, eliminating the need for programming. We conducted two case studies and in-depth interviews with target users to evaluate our approach. The results demonstrate the effectiveness and usability of our approach in allowing users to conveniently enable flexible interactions for static visualizations.

</details>


### [675] [RAICL: Retrieval-Augmented In-Context Learning for Vision-Language-Model Based EEG Seizure Detection](https://arxiv.org/abs/2601.17844)
*Siyang Li,Zhuoya Wang,Xiyan Gui,Xiaoqing Chen,Ziwei Wang,Yaozhi Wen,Dongrui Wu*

Main category: cs.HC

TL;DR: This paper proposes a novel approach using vision-language models (VLMs) for EEG decoding by converting EEG signals into waveforms and prompts, achieving strong results on seizure detection tasks without retraining while improving generalization.


<details>
  <summary>Details</summary>
Motivation: Conventional EEG signal decoding methods rely on task-specific datasets, limiting generalization due to insufficient data. The study aims to address the challenges of developing generalizable large brain decoding models.

Method: EEG signals are converted into waveform images and analyzed by VLMs with neuroscience-informed textual prompts. A Retrieval-Augmented In-Context Learning (RAICL) method is introduced to manage EEG's non-stationarity by dynamically selecting relevant examples for input conditioning.

Result: The proposed method proves effective, with experiments on EEG-based seizure detection showing the technique achieves better or comparable performance than traditional approaches.

Conclusion: The study demonstrates that leveraging vision-language models for EEG decoding enhances generalization and cross-modal integration in physiological signal processing, offering a deployable solution for clinical applications.

Abstract: Electroencephalogram (EEG) decoding is a critical component of medical diagnostics, rehabilitation engineering, and brain-computer interfaces. However, contemporary decoding methodologies remain heavily dependent on task-specific datasets to train specialized neural network architectures. Consequently, limited data availability impedes the development of generalizable large brain decoding models. In this work, we propose a paradigm shift from conventional signal-based decoding by leveraging large-scale vision-language models (VLMs) to analyze EEG waveform plots. By converting multivariate EEG signals into stacked waveform images and integrating neuroscience domain expertise into textual prompts, we demonstrate that foundational VLMs can effectively differentiate between different patterns in the human brain. To address the inherent non-stationarity of EEG signals, we introduce a Retrieval-Augmented In-Context Learning (RAICL) approach, which dynamically selects the most representative and relevant few-shot examples to condition the autoregressive outputs of the VLM. Experiments on EEG-based seizure detection indicate that state-of-the-art VLMs under RAICL achieved better or comparable performance with traditional time series based approaches. These findings suggest a new direction in physiological signal processing that effectively bridges the modalities of vision, language, and neural activities. Furthermore, the utilization of off-the-shelf VLMs, without the need for retraining or downstream architecture construction, offers a readily deployable solution for clinical applications.

</details>


### [676] [An Experimental Comparison of Cognitive Forcing Functions for Execution Plans in AI-Assisted Writing: Effects On Trust, Overreliance, and Perceived Critical Thinking](https://arxiv.org/abs/2601.18033)
*Ahana Ghosh,Advait Sarkar,Siân Lindley,Christian Poelitz*

Main category: cs.HC

TL;DR: This study explores the effectiveness of cognitive forcing functions (CFFs) on reducing overreliance in AI-generated execution plans during GenAI-assisted writing tasks.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the risks of overreliance and reduced critical thinking associated with growing use of GenAI tools in complex workflows.

Method: A controlled experiment tested four CFF conditions (Assumption, WhatIf, Both, and no-CFF control) followed by a qualitative think-aloud and interview study.

Result: The Assumption CFF reduced overreliance most effectively without increasing cognitive load, while users found the WhatIf CFF to be most helpful.

Conclusion: Plan-focused CFFs are valuable for fostering critical engagement in GenAI-assisted knowledge work.

Abstract: Generative AI (GenAI) tools improve productivity in knowledge workflows such as writing, but also risk overreliance and reduced critical thinking. Cognitive forcing functions (CFFs) mitigate these risks by requiring active engagement with AI output. As GenAI workflows grow more complex, systems increasingly present execution plans for user review. However, these plans are themselves AI-generated and prone to overreliance, and the effectiveness of applying CFFs to AI plans remains underexplored. We conduct a controlled experiment in which participants completed AI-assisted writing tasks while reviewing AI-generated plans under four CFF conditions: Assumption (argument analysis), WhatIf (hypothesis testing), Both, and a no-CFF control. A follow-up think-aloud and interview study qualitatively compared these conditions. Results show that the Assumption CFF most effectively reduced overreliance without increasing cognitive load, while participants perceived the WhatIf CFF as most helpful. These findings highlight the value of plan-focused CFFs for supporting critical reflection in GenAI-assisted knowledge work.

</details>


### [677] ["Crash Test Dummies" for AI-Enabled Clinical Assessment: Validating Virtual Patient Scenarios with Virtual Learners](https://arxiv.org/abs/2601.18085)
*Brian Gin,Ahreum Lim,Flávia Silva e Oliveira,Kuan Xing,Xiaomei Song,Gayana Amiyangoda,Thilanka Seneviratne,Alison F. Doubleday,Ananya Gangopadhyaya,Bob Kiser,Lukas Shum-Tim,Dhruva Patel,Kosala Marambe,Lauren Maggio,Ara Tekian,Yoon Soo Park*

Main category: cs.HC

TL;DR: This paper introduces an AI-based virtual patient platform and measurement model to enhance the robustness and reliability of medical competency evaluation before using with human learners.


<details>
  <summary>Details</summary>
Motivation: The study addresses the issue of uncertain robustness and potential misguidance from unvalidated AI systems in assessing medical and health professional competencies.

Method: The authors developed a platform with virtual patients and learners, using a Bayesian HRM-SDT model to analyze competency evaluations. AI raters scored interactions, and key parameters were estimated via MCMC.

Result: The model accurately recovered simulated competencies, correlated across domains, estimated case difficulty, and demonstrated consistent scoring among AI raters using different seeds.

Conclusion: A combination of AI tools and psychometric modeling allows for reliable and generalizable competency assessments, enabling safer AI deployment for evaluating medical learners.

Abstract: Background: In medical and health professions education (HPE), AI is increasingly used to assess clinical competencies, including via virtual standardized patients. However, most evaluations rely on AI-human interrater reliability and lack a measurement framework for how cases, learners, and raters jointly shape scores. This leaves robustness uncertain and can expose learners to misguidance from unvalidated systems. We address this by using AI "simulated learners" to stress-test and psychometrically characterize assessment pipelines before human use.
  Objective: Develop an open-source AI virtual patient platform and measurement model for robust competency evaluation across cases and rating conditions.
  Methods: We built a platform with virtual patients, virtual learners with tunable ACGME-aligned competency profiles, and multiple independent AI raters scoring encounters with structured Key-Features items. Transcripts were analyzed with a Bayesian HRM-SDT model that treats ratings as decisions under uncertainty and separates learner ability, case performance, and rater behavior; parameters were estimated with MCMC.
  Results: The model recovered simulated learners' competencies, with significant correlations to the generating competencies across all ACGME domains despite a non-deterministic pipeline. It estimated case difficulty by competency and showed stable rater detection (sensitivity) and criteria (severity/leniency thresholds) across AI raters using identical models/prompts but different seeds. We also propose a staged "safety blueprint" for deploying AI tools with learners, tied to entrustment-based validation milestones.
  Conclusions: Combining a purpose-built virtual patient platform with a principled psychometric model enables robust, interpretable, generalizable competency estimates and supports validation of AI-assisted assessment prior to use with human learners.

</details>


### [678] [Understanding Users' Privacy Reasoning and Behaviors During Chatbot Use to Support Meaningful Agency in Privacy](https://arxiv.org/abs/2601.18125)
*Mohammad Hadi Nezhad,Francisco Enrique Vicente Castro,Ivon Arroyo*

Main category: cs.HC

TL;DR: This paper investigates how users disclose and manage sensitive information during chatbot interactions and explores tools to improve privacy protection.


<details>
  <summary>Details</summary>
Motivation: With conversational agents like chatbots being widely utilized, privacy concerns arise as users disclose sensitive information in various contexts. Enhancing privacy protection tools that align with users' contextual privacy judgments is crucial.

Method: The study involved computer science students using a simulated ChatGPT interface with and without a privacy notice panel during realistic chatbot tasks. The panel provided privacy protective features such as anonymization through retracting, faking, and generalizing information. Researchers analyzed data from interaction logs, think-alouds, and surveys.

Result: The privacy notice panel raised users' awareness of privacy, encouraged protective actions, and supported context-specific reasoning for managing sensitive information disclosure.

Conclusion: The findings highlight opportunities for designing tools that give users greater and more meaningful control over their sensitive information when interacting with chatbots.

Abstract: Conversational agents (CAs) (e.g., chatbots) are increasingly used in settings where users disclose sensitive information, raising significant privacy concerns. Because privacy judgments are highly contextual, supporting users to engage in privacy-protective actions during chatbot interactions is essential. However, enabling meaningful engagement requires a deeper understanding of how users currently reason about and manage sensitive information during realistic chatbot use scenarios. To investigate this, we qualitatively examined computer science (undergraduate and masters) students' in-the-moment disclosure and protection behaviors, as well as the reasoning underlying these behaviors, across a range of realistic chatbot tasks. Participants used a simulated ChatGPT interface with and without a privacy notice panel that intercepts message submissions, highlights potentially sensitive information, and offers privacy protective actions. The panel supports anonymization through retracting, faking, and generalizing, and surfaces two of ChatGPT's built-in privacy controls to improve their discoverability. Drawing on interaction logs, think-alouds, and survey responses, we analyzed how the panel fostered privacy awareness, encouraged protective actions, and supported context-specific reasoning about what information to protect and how. We further discuss design opportunities for tools that provide users greater and more meaningful agency in protecting sensitive information during CA interactions.

</details>


### [679] [Fusion of Spatio-Temporal and Multi-Scale Frequency Features for Dry Electrodes MI-EEG Decoding](https://arxiv.org/abs/2601.18424)
*Tianyi Gong,Can Han,Junxi Wu,Dahong Qian*

Main category: cs.HC

TL;DR: The paper introduces STGMFM, a tri-branch framework designed to improve dry-electrode Motor Imagery EEG accuracy by leveraging spatio-temporal dependencies and robust frequency mixing strategies.


<details>
  <summary>Details</summary>
Motivation: Dry-electrode MI-EEG offers practical, real-world applications by removing the need for gels and lengthy setups but suffers from lower signal quality, higher noise, and session variability, limiting its effectiveness.

Method: STGMFM incorporates dual graph order modeling for spatial-temporal dependencies, multi-scale frequency mixing to handle amplitude envelope dynamics, and decision-level fusion for noise resilience.

Result: STGMFM outperformed CNNs, Transformers, and graph-based models on a dry-electrode MI-EEG dataset specifically collected for this study.

Conclusion: STGMFM is an effective method to mitigate common dry-electrode MI-EEG issues, offering superior performance by addressing signal variability and noise with innovative modeling strategies.

Abstract: Dry-electrode Motor Imagery Electroencephalography (MI-EEG) enables fast, comfortable, real-world Brain Computer Interface by eliminating gels and shortening setup for at-home and wearable use.However, dry recordings pose three main issues: lower Signal-to-Noise Ratio with more baseline drift and sudden transients; weaker and noisier data with poor phase alignment across trials; and bigger variances between sessions. These drawbacks lead to larger data distribution shift, making features less stable for MI-EEG tasks.To address these problems, we introduce STGMFM, a tri-branch framework tailored for dry-electrode MI-EEG, which models complementary spatio-temporal dependencies via dual graph orders, and captures robust envelope dynamics with a multi-scale frequency mixing branch, motivated by the observation that amplitude envelopes are less sensitive to contact variability than instantaneous waveforms. Physiologically meaningful connectivity priors guide learning, and decision-level fusion consolidates a noise-tolerant consensus. On our collected dry-electrode MI-EEG, STGMFM consistently surpasses competitive CNN/Transformer/graph baselines. Codes are available at https://github.com/Tianyi-325/STGMFM.

</details>


### [680] [Unheard in the Digital Age: Rethinking AI Bias and Speech Diversity](https://arxiv.org/abs/2601.18641)
*Onyedikachi Hope Amaechi-Okorie,Branislav Radeljic*

Main category: cs.HC

TL;DR: The paper examines the bias against atypical speech in society and AI systems, advocating for inclusive AI design and policy reform to support speech diversity.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the societal and technological marginalization faced by individuals with atypical speech patterns, and the need to address biases embedded in AI systems.

Method: It draws on interdisciplinary research and highlights the structural biases in speech recognition technologies as well as systemic solutions for inclusion.

Result: The study finds that current ASR systems fail to effectively recognize diverse voices, perpetuating digital and societal exclusion.

Conclusion: The paper emphasizes reframing speech inclusion as equity rather than accommodation and calls for collaborative AI designs and policy reforms to ensure representation and rights of all speakers.

Abstract: Speech remains one of the most visible yet overlooked vectors of inclusion and exclusion in contemporary society. While fluency is often equated with credibility and competence, individuals with atypical speech patterns are routinely marginalized. Given the current state of the debate, this article focuses on the structural biases that shape perceptions of atypical speech and are now being encoded into artificial intelligence. Automated speech recognition (ASR) systems and voice interfaces, trained predominantly on standardized speech, routinely fail to recognize or respond to diverse voices, compounding digital exclusion. As AI technologies increasingly mediate access to opportunity, the study calls for inclusive technological design, anti-bias training to minimize the impact of discriminatory algorithmic decisions, and enforceable policy reform that explicitly recognize speech diversity as a matter of equity, not merely accessibility. Drawing on interdisciplinary research, the article advocates for a cultural and institutional shift in how we value voice, urging co-created solutions that elevate the rights, representation, and realities of atypical speakers in the digital age. Ultimately, the article reframes speech inclusion as a matter of equity (not accommodation) and advocates for co-created AI systems that reflect the full spectrum of human voices.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [681] [Entropy-Guided Agreement-Diversity: A Semi-Supervised Active Learning Framework for Fetal Head Segmentation in Ultrasound](https://arxiv.org/abs/2601.17460)
*Fangyijie Wang,Siteng Ma,Guénolé Silvestre,Kathleen M. Curran*

Main category: eess.IV

TL;DR: This study proposes a novel active learning-based method (EGAD) for fetal head segmentation in ultrasound images, achieving superior performance with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Fetal ultrasound image data is often restricted by privacy issues, limiting training data for deep learning models, and existing semi-supervised approaches fail due to suboptimal selection of labeled data.

Method: The proposed two-stage active learning method, EGAD, uses entropy-based uncertainty and agreement-diversity scoring to select samples. A consistency learning strategy with feature downsampling is also integrated.

Result: The EGAD method achieves Dice scores of 94.57% and 96.32% on two datasets, using only 5% and 10% labeled data, outperforming existing SSL techniques.

Conclusion: EGAD provides a highly efficient framework for fetal head segmentation using limited labeled data, demonstrating robustness across varying pregnancy stages and superior segmentation results.

Abstract: Fetal ultrasound (US) data is often limited due to privacy and regulatory restrictions, posing challenges for training deep learning (DL) models. While semi-supervised learning (SSL) is commonly used for fetal US image analysis, existing SSL methods typically rely on random limited selection, which can lead to suboptimal model performance by overfitting to homogeneous labeled data. To address this, we propose a two-stage Active Learning (AL) sampler, Entropy-Guided Agreement-Diversity (EGAD), for fetal head segmentation. Our method first selects the most uncertain samples using predictive entropy, and then refines the final selection using the agreement-diversity score combining cosine similarity and mutual information. Additionally, our SSL framework employs a consistency learning strategy with feature downsampling to further enhance segmentation performance. In experiments, SSL-EGAD achieves an average Dice score of 94.57\% and 96.32\% on two public datasets for fetal head segmentation, using 5\% and 10\% labeled data for training, respectively. Our method outperforms current SSL models and showcases consistent robustness across diverse pregnancy stage data. The code is available on \href{https://github.com/13204942/Semi-supervised-EGAD}{GitHub}.

</details>


### [682] [In-situ On-demand Digital Image Correlation: A New Data-rich Characterization Paradigm for Deformation and Damage Development in Solids](https://arxiv.org/abs/2601.17545)
*Ravi Venkata Surya Sai Mogilisetti,Partha Pratim Das,Rassel Raihan,Shiyao Lin*

Main category: eess.IV

TL;DR: This paper introduces a novel method called ISOD DIC, enhancing digital image correlation (DIC) by integrating dynamic camera control to capture more detailed deformation data in real-time.


<details>
  <summary>Details</summary>
Motivation: DIC is a powerful non-contact method for capturing deformation but is limited by fixed imaging frame rates, which hinders its efficiency in analyzing excessive deformation or deformation rates.

Method: The paper introduced ISOD DIC, a paradigm for DIC analysis that dynamically adjusts camera frame rates based on deformation to enable real-time analysis and visualization.

Result: ISOD DIC captured 178% more images than conventional DIC during crack growth scenarios, demonstrating significantly enriched data collection without increasing the burden on storage or time.

Conclusion: ISOD DIC enhances the capability of DIC for real-time, detailed deformation characterization, improving insights into damage mechanisms and material behaviors.

Abstract: Digital image correlation (DIC) has become one of the most popular methods for deformation characterization in experimental mechanics. DIC is based on optical images taken during experimentation and post-test image processing. Its advantages include the capability to capture full-field deformation in a non-contact manner, the robustness in characterizing excessive deformation induced by events such as yielding and cracking, and the versatility to integrate optical cameras with a variety of open-source and commercial codes. In this paper, we developed a new paradigm of DIC analysis by integrating camera control into the DIC process flow. The essential idea is to dynamically increase the camera imaging frame rate with excessive deformation or deformation rate, while maintaining a relatively low imaging frame rate with small and slow deformation. We refer to this new DIC paradigm as in-situ on-demand (ISOD) DIC. ISOD DIC enables real-time deformation analysis, visualization, and closed-loop camera control. ISOD DIC has captured approximately 178% more images than conventional DIC for samples undergoing crack growth due to its dynamically adjusted frame rate, with the potential to significantly enhance data richness for damage inspection without consuming excessive storage space and analysis time, thereby benefiting the characterization of intrinsic constitutive behaviors and damage mechanisms

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [683] [ChemNavigator: Agentic AI Discovery of Design Rules for Organic Photocatalysts](https://arxiv.org/abs/2601.17084)
*Iman Peivaste,Ahmed Makradi,Salim Belouettar*

Main category: physics.chem-ph

TL;DR: ChemNavigator, an autonomous AI system, identifies principles for organic photocatalyst design through hypothesis-driven exploration, surpassing previous ML approaches.


<details>
  <summary>Details</summary>
Motivation: The growing demand for effective organic photocatalysts is hindered by the vast chemical space and a reliance on human intuition for molecular design.

Method: ChemNavigator employs a multi-agent system, integrating reasoning from large language models and density functional tight binding calculations, to iteratively explore and analyze molecular properties.

Result: The AI system autonomously identified six significant design rules for photocatalyst properties and provided chemical insights previously unachieved by ML methods.

Conclusion: Agentic AI systems like ChemNavigator can independently derive chemical principles and assist synthetic chemists in rational material design, enhancing traditional approaches.

Abstract: The discovery of high-performance organic photocatalysts for hydrogen evolution remains limited by the vastness of chemical space and the reliance on human intuition for molecular design. Here we present ChemNavigator, an agentic AI system that autonomously derives structure-property relationships through hypothesis-driven exploration of organic photocatalyst candidates. The system integrates large language model reasoning with density functional tight binding calculations in a multi-agent architecture that mirrors the scientific method: formulating hypotheses, designing experiments, executing calculations, and validating findings through rigorous statistical analysis. Through iterative discovery cycles encompassing 200 molecules, ChemNavigator autonomously identified six statistically significant design rules governing frontier orbital energies, including the effects of ether linkages, carbonyl groups, extended conjugation, cyano groups, halogen substituents, and amine groups. Importantly, these rules correspond to established principles of organic electronic structure (resonance donation, inductive withdrawal, $π$-delocalization), demonstrating that the system can independently derive chemical knowledge without explicit programming. Notably, autonomous agentic reasoning extracted these six validated rules from a molecular library where previous ML approaches identified only carbonyl effects. Furthermore, the quantified effect sizes provide a prioritized ranking for synthetic chemists, while feature interaction analysis revealed diminishing returns when combining strategies, challenging additive assumptions in molecular design. This work demonstrates that agentic AI systems can autonomously derive interpretable, chemically grounded design principles, establishing a framework for AI-assisted materials discovery that complements rather than replaces chemical intuition.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [684] [Universality of Many-body Projected Ensemble for Learning Quantum Data Distribution](https://arxiv.org/abs/2601.18637)
*Quoc Hoan Tran,Koki Chinzei,Yasuhiro Endo,Hirotaka Oshima*

Main category: quant-ph

TL;DR: This paper addresses the universality of approximation in quantum machine learning and introduces theoretical guarantees for a framework called Many-body Projected Ensemble (MPE), showing its efficacy in approximating quantum distributions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the critical challenge in quantum machine learning: determining if QML models can universally approximate any quantum distribution.

Method: The authors prove a universality theorem for the Many-body Projected Ensemble (MPE) framework and propose its Incremental variant with layer-wise training to enhance practical trainability.

Result: The paper demonstrates theoretically and numerically that MPE can approximate any distribution of pure states with a rigorous guarantee of universal expressivity. Experimental validations show its success on clustered quantum states and quantum chemistry datasets.

Conclusion: The MPE framework is proven to be universally expressive in approximating quantum distributions, bridging theoretical gaps in QML and demonstrating practical trainability for complex quantum data.

Abstract: Generating quantum data by learning the underlying quantum distribution poses challenges in both theoretical and practical scenarios, yet it is a critical task for understanding quantum systems. A fundamental question in quantum machine learning (QML) is the universality of approximation: whether a parameterized QML model can approximate any quantum distribution. We address this question by proving a universality theorem for the Many-body Projected Ensemble (MPE) framework, a method for quantum state design that uses a single many-body wave function to prepare random states. This demonstrates that MPE can approximate any distribution of pure states within a 1-Wasserstein distance error. This theorem provides a rigorous guarantee of universal expressivity, addressing key theoretical gaps in QML. For practicality, we propose an Incremental MPE variant with layer-wise training to improve the trainability. Numerical experiments on clustered quantum states and quantum chemistry datasets validate MPE's efficacy in learning complex quantum data distributions.

</details>


### [685] [An Adaptive Purification Controller for Quantum Networks: Dynamic Protocol Selection and Multipartite Distillation](https://arxiv.org/abs/2601.18351)
*Pranav Kulkarni,Leo Sünkel,Michael Kölle*

Main category: quant-ph

TL;DR: The paper introduces an Adaptive Purification Controller (APC) for optimizing entanglement distribution in the Quantum Internet by dynamically adjusting purification strategies to maximize fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of static entanglement purification strategies caused by fluctuating physical link parameters in the Quantum Internet.

Method: The APC autonomously selects the best purification sequences using dynamic programming with Pareto pruning, adapting between protocols like BBPSSW and DEJMPS based on network conditions.

Result: Simulation results show APC eliminates fidelity cliffs, improves resource usage in noisy environments, and handles various systems (e.g., multipartite GHZ, continuous variable systems), with low computational latency.

Conclusion: The proposed APC is effective and computationally feasible for real-time applications in optimizing entanglement distribution in dynamic quantum networks.

Abstract: Efficient entanglement distribution is the cornerstone of the Quantum Internet. However, physical link parameters such as photon loss, memory coherence time, and gate error rates fluctuate dynamically, rendering static purification strategies suboptimal. In this paper, we propose an Adaptive Purification Controller (APC) that autonomously optimizes the entanglement distillation sequence to maximize the "goodput," the rate of delivered pairs meeting a strict fidelity threshold. By treating protocol selection as a resource allocation problem, the APC dynamically switches between purification depths and protocol families (e.g., BBPSSW vs. DEJMPS) to navigate the trade-off between generation rate and state quality. Using a dynamic programming planner with Pareto pruning, simulation results demonstrate that our approach eliminates the "fidelity cliffs" inherent in static protocols and prevents resource wastage in high-noise regimes. Furthermore, we extend the controller to heterogeneous scenarios, demonstrating robustness for both multipartite GHZ state generation and continuous variable systems using effective noiseless linear amplification models. We benchmark its computational overhead, confirming real-time feasibility with decision latencies in the millisecond range per link.

</details>


### [686] [Differentiable Architecture Search for Adversarially Robust Quantum Computer Vision](https://arxiv.org/abs/2601.18058)
*Mohamed Afane,Quanjiang Long,Haoting Shen,Ying Mao,Junaid Farooq,Ying Wang,Juntao Chen*

Main category: quant-ph

TL;DR: The paper proposes a hybrid quantum-classical Differentiable Quantum Architecture Search (DQAS) framework that improves the robustness of quantum neural networks against adversarial attacks and hardware noise without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Quantum neural networks face challenges due to their sensitivity to both adversarial perturbations and hardware noise, making deployment impractical. Existing solutions either reduce accuracy or are computationally expensive.

Method: The paper introduces a DQAS framework that combines quantum circuit structure optimization and robustness enhancement using a Classical Noise Layer for lightweight preprocessing. The optimization utilizes gradient-based methods.

Result: Experiments on datasets like MNIST, FashionMNIST, and CIFAR show improved accuracy in both clean and adversarial settings. The framework performs well under various attack methods and real quantum hardware tests.

Conclusion: The hybrid classical-quantum approach successfully enhances quantum neural network robustness and accuracy while remaining computationally efficient, demonstrating its potential for real-world applications.

Abstract: Current quantum neural networks suffer from extreme sensitivity to both adversarial perturbations and hardware noise, creating a significant barrier to real-world deployment. Existing robustness techniques typically sacrifice clean accuracy or require prohibitive computational resources. We propose a hybrid quantum-classical Differentiable Quantum Architecture Search (DQAS) framework that addresses these limitations by jointly optimizing circuit structure and robustness through gradient-based methods. Our approach enhances traditional DQAS with a lightweight Classical Noise Layer applied before quantum processing, enabling simultaneous optimization of gate selection and noise parameters. This design preserves the quantum circuit's integrity while introducing trainable perturbations that enhance robustness without compromising standard performance. Experimental validation on MNIST, FashionMNIST, and CIFAR datasets shows consistent improvements in both clean and adversarial accuracy compared to existing quantum architecture search methods. Under various attack scenarios, including Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), Basic Iterative Method (BIM), and Momentum Iterative Method (MIM), and under realistic quantum noise conditions, our hybrid framework maintains superior performance. Testing on actual quantum hardware confirms the practical viability of discovered architectures. These results demonstrate that strategic classical preprocessing combined with differentiable quantum architecture optimization can significantly enhance quantum neural network robustness while maintaining computational efficiency.

</details>


### [687] [Bayesian quantum sensing using graybox machine learning](https://arxiv.org/abs/2601.17465)
*Akram Youssry,Stefan Todd,Patrick Murton,Muhammad Junaid Arshad,Alberto Peruzzo,Cristian Bonato*

Main category: quant-ph

TL;DR: This paper introduces a novel graybox modeling approach for quantum sensors, which improves accuracy and reduces training requirements compared to other methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in quantum sensors due to unmodelled effects like noise and imperfect controls, which hinder their practical performance.

Method: A graybox modeling framework combining physics-based system modeling with data-driven experimental corrections, validated experimentally with a single-spin quantum sensor to estimate static magnetic fields.

Result: The graybox model achieved significant improvement in mean squared error compared to a purely physics-based model, using 10,000 training datapoints.

Conclusion: The graybox framework is effective across different quantum sensing platforms and enhances real-time adaptive protocols by addressing model inaccuracies.

Abstract: Quantum sensors offer significant advantages over classical devices in spatial resolution and sensitivity, enabling transformative applications across materials science, healthcare, and beyond. Their practical performance, however, is often constrained by unmodelled effects, including noise, imperfect state preparation, and non-ideal control fields.
  In this work, we report the first experimental implementation of a graybox modelling strategy for a solid-state open quantum system. The graybox framework integrates a physics-based system model with a data-driven description of experimental imperfections, achieving higher fidelity than purely analytical (whitebox) approaches while requiring fewer training resources than fully deep-learning models. We experimentally validate the method on the task of estimating a static magnetic field using a single-spin quantum sensor, performing Bayesian inference with a graybox model trained on prior experimental data. Using roughly 10,000 training datapoints, the graybox model yields several orders of magnitude improvement in mean squared error over the corresponding physics-only model. These results are broadly applicable to a wide range of quantum sensing platforms, not limited to single-spin systems, and are particularly valuable for real-time adaptive protocols, where model inaccuracies can otherwise lead to suboptimal control and degraded performance.

</details>


### [688] [Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication](https://arxiv.org/abs/2601.18419)
*Michael Kölle,Christian Reff,Leo Sünkel,Julian Hager,Gerhard Stenzel,Claudia Linnhoff-Popien*

Main category: quant-ph

TL;DR: This paper examines communication mechanisms for Quantum Multi-Agent Reinforcement Learning to foster cooperation in Sequential Social Dilemmas.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of communication-enabled quantum Q-Learning agents in fostering emergent cooperation in Sequential Social Dilemmas (SSDs).

Method: Applied various communication protocols (MATE, MEDIATE, Gifting, RIAL) to quantum Q-learning agents and evaluated them across three SSDs: Iterated Prisoner's Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken.

Result: Experimental results showed that communication protocols, especially MATE\textsubscript{TD}, AutoMATE, MEDIATE-I, and MEDIATE-S, achieved high levels of cooperation in all tested dilemmas.

Conclusion: Communication is an effective mechanism to enhance emergent cooperation in Quantum Multi-Agent Reinforcement Learning.

Abstract: Emergent cooperation in classical Multi-Agent Reinforcement Learning has gained significant attention, particularly in the context of Sequential Social Dilemmas (SSDs). While classical reinforcement learning approaches have demonstrated capability for emergent cooperation, research on extending these methods to Quantum Multi-Agent Reinforcement Learning remains limited, particularly through communication. In this paper, we apply communication approaches to quantum Q-Learning agents: the Mutual Acknowledgment Token Exchange (MATE) protocol, its extension Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange (MEDIATE), the peer rewarding mechanism Gifting, and Reinforced Inter-Agent Learning (RIAL). We evaluate these approaches in three SSDs: the Iterated Prisoner's Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken. Our experimental results show that approaches using MATE with temporal-difference measure (MATE\textsubscript{TD}), AutoMATE, MEDIATE-I, and MEDIATE-S achieved high cooperation levels across all dilemmas, demonstrating that communication is a viable mechanism for fostering emergent cooperation in Quantum Multi-Agent Reinforcement Learning.

</details>


### [689] [Data-Driven Qubit Characterization and Optimal Control using Deep Learning](https://arxiv.org/abs/2601.18704)
*Paul Surrey,Julian D. Teske,Tobias Hangleiter,Hendrik Bluhm,Pascal Cerfontaine*

Main category: quant-ph

TL;DR: The paper introduces an RNN-based method for optimizing quantum computing control pulses without requiring detailed system models.


<details>
  <summary>Details</summary>
Motivation: Quantum computing necessitates high-fidelity quantum gate operations, but achieving this requires overcoming challenges in modeling complex system dynamics and gradient evaluation.

Method: A recurrent neural network (RNN) is trained on sampled qubit dynamics to predict system behavior, allowing for gradient-based pulse optimization with less reliance on system models.

Result: They show the method's effectiveness through simulations on a single $ST_0$ qubit, achieving optimized high-fidelity control pulses.

Conclusion: The machine learning-based approach offers an efficient alternative to traditional pulse optimization, relying on observed system responses rather than detailed modeling.

Abstract: Quantum computing requires the optimization of control pulses to achieve high-fidelity quantum gates. We propose a machine learning-based protocol to address the challenges of evaluating gradients and modeling complex system dynamics. By training a recurrent neural network (RNN) to predict qubit behavior, our approach enables efficient gradient-based pulse optimization without the need for a detailed system model. First, we sample qubit dynamics using random control pulses with weak prior assumptions. We then train the RNN on the system's observed responses, and use the trained model to optimize high-fidelity control pulses. We demonstrate the effectiveness of this approach through simulations on a single $ST_0$ qubit.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [690] [From Scores to Queues: Operationalizing Cross-Chain Obfuscation Signals for Smart-Contract Audits](https://arxiv.org/abs/2601.17356)
*Yao Zhao,Zhang Sheng,Shengchen Duan,Shen Wang*

Main category: cs.CR

TL;DR: The paper introduces HObfNET, a tool for efficient cross-chain evaluation of smart contract obfuscation, which is faster and scalable compared to traditional methods, and proposes practical strategies for multi-chain security analysis.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the complexity and inefficiency of interpreting obfuscated smart contracts, along with the lack of comparability and transferability of these signals across different blockchain networks.

Method: HObfNET was developed as a surrogate to Obfs_Tool, achieving alignment with Ethereum data, and enabling significantly faster and large-scale scoring. The study analyzed multiple blockchains to detect systematic score drifts and proposed tailored thresholds for effective results.

Result: The model showed strong performance metrics (PCC 0.9158, MAPE 8.20%), with considerable speed improvements (8-9 ms per contract) over baseline tools, validated through tests on Ethereum, BSC, Polygon, and Avalanche corpora. Actionable insights and queue strategies were proposed based on score thresholds.

Conclusion: HObfNET enhances cross-chain security operations by enabling scalable, high-speed scoring and audit workflows, with significant implications for detecting and addressing obfuscation risks in multi-chain environments.

Abstract: Obfuscation substantially increases the interpretation cost of smart-contract auditing, while the comparability and transferability of obfuscation signals across chains remain unclear. We present HObfNET as an efficient surrogate of Obfs_Tool (ObfProbe), enabling fast cross-chain scoring at scale. The model aligns well with tool outputs on Ethereum (PCC 0.9158, MAPE 8.20 percent) and achieves 8-9 ms per contract, a 2.3k-5.2k times speedup over second-level Obfs_Tool runs, enabling million-scale scoring. On large BSC, Polygon, and Avalanche corpora, we find systematic score drift: fixed-threshold transfer inflates and deflates candidate queues, motivating within-chain main and extreme thresholds (p99 and p99.9) and an actionable queueing strategy. The high-score tail exhibits rare selectors, external-call opcode enrichment, and low signature density; a proxy indicator is enriched in the BSC high-score queue, enabling secondary triage. Cross-chain reuse analysis shows tail enrichment and directional diffusion, with traceable same-hash cases across chains. In publicly alignable incident samples, all fall into the p99 queue; Transit Swap DEX Hack and New Free DAO Flash Loan exhibit cross-chain spillover, indicating real-world hit and prioritization value. We deliver a two-tier audit queue and cross-chain linkage workflow to support practical multi-chain security operations.

</details>


### [691] [KeyMemRT Compiler and Runtime: Unlocking Memory-Scalable FHE](https://arxiv.org/abs/2601.18445)
*Eymen Ünay,Björn Franke,Jackson Woodruff*

Main category: cs.CR

TL;DR: The paper introduces KeyMemRT, a novel FHE compiler framework, that optimizes memory usage and runtime by managing rotation keys efficiently.


<details>
  <summary>Details</summary>
Motivation: Fully Homomorphic Encryption (FHE) faces challenges with high latency and memory consumption, especially due to rotation keys that lead to memory bottlenecks in complex applications. Current compilers handle these issues inadequately, requiring large memory resources and manual optimization, creating barriers for FHE adoption.

Method: The authors developed KeyMemRT, an MLIR-based compiler and runtime framework, which uses dataflow analysis to manage rotation key lifetimes efficiently. This system enables fine-grained management of keys without causing memory bloating.

Result: KeyMemRT demonstrates a 1.74x memory reduction and 1.20x speedup compared to ANT-ACE, and a 1.16x memory reduction and 1.73x speedup over the Fhelipe compiler.

Conclusion: KeyMemRT brings significant advantages in memory optimization and computational speed for FHE applications. It is a versatile post-optimizing compiler that can improve any FHE compiler's performance, paving the way for more feasible and scalable FHE usage.

Abstract: Fully Homomorphic Encryption (FHE) enables privacy preserving computation but it suffers from high latency and memory consumption. The computations are secured with special keys called rotation keys which often take up the majority of memory. In complex FHE applications, these rotation keys can cause a large memory bottleneck limiting program throughput. Existing compilers make little effort to solve this problem, instead relying on systems with massive memory availability. This resource requirement is a barrier to FHE uptake because optimizing FHE programs by hand is challenging due to their scale, complexity and expertise required.
  In this work, we present KeyMemRT; an MLIR based compiler and runtime framework that individually manages rotation key lifetimes to lower memory utilization and to allow arbitrary number of rotation indices to be supported without memory bloating. KeyMemRT relies on dataflow analysis to determine key lifetimes and is the first FHE compiler to provide automatic key management, handle fine-grained key-mangement and manage boostrap keys. We implement frontends for Orion and HEIR and show improvements over state-of-the-art FHE compilers. KeyMemRT achieves memory reduction of 1.74x and a speedup of 1.20x over ANT-ACE, and memory reduction of 1.16x and a speedup of 1.73x over memory-optimized compiler Fhelipe. We provide KeyMemRT as a post-optimizing compiler that can be targeted by any FHE compiler.

</details>


### [692] [TrojanGYM: A Detector-in-the-Loop LLM for Adaptive RTL Hardware Trojan Insertion](https://arxiv.org/abs/2601.17178)
*Saideep Sreekumar,Zeng Wang,Akashdeep Saha,Weihua Xiao,Minghao Shao,Muhammad Shafique,Ozgur Sinanoglu,Ramesh Karri,Johann Knechtel*

Main category: cs.CR

TL;DR: The paper introduces TrojanGYM, which leverages LLM-driven agents to generate Hardware Trojans (HTs) that identify blind spots in HT detectors, enabling improved detection methodologies.


<details>
  <summary>Details</summary>
Motivation: Learning-based HT detectors often fail due to overfitting to specific patterns and limited benchmark scopes. This calls for a systematic framework to expose detector vulnerabilities.

Method: TrojanGYM uses LLM agents to curate diverse HT insertions, coupled with a feedback-driven refinement loop involving GNN-based HT detectors to improve HT benchmarking and detector performance.

Result: TrojanGYM generated challenging HT benchmarks that improved detection rates from 0% to 60% for Robust-GNN4TJ, which shows enhanced performance on LLM-generated HTs compared to prior detectors.

Conclusion: The framework reveals critical blind spots in current HT detectors and suggests systematic improvements for robustness, with plans to release codes and artifacts post peer-review.

Abstract: Hardware Trojans (HTs) remain a critical threat because learning-based detectors often overfit to narrow trigger/payload patterns and small, stylized benchmarks. We introduce TrojanGYM, an agentic, LLM-driven framework that automatically curates HT insertions to expose detector blind spots while preserving design correctness. Given high-level HT specifications, a suite of cooperating LLM agents (instantiated with GPT-4, LLaMA-3.3-70B, and Gemini-2.5Pro) proposes and refines RTL modifications that realize diverse triggers and payloads without impacting normal functionality. TrojanGYM implements a feedback-driven benchmark generation loop co-designed with HT detectors, in which constraint-aware syntactic checking and GNN-based HT detectors provide feedback that iteratively refines HT specifications and insertion strategies to better surface detector blind spots. We further propose Robust-GNN4TJ, a new implementation of the GNN4TJ with improved graph extraction, training robustness, and prediction reliability, especially on LLM-generated HT designs. On the most challenging TrojanGYM-generated benchmarks, Robust-GNN4TJ raises HT detection rates from 0% to 60% relative to a prior GNN-based detector. We instantiate TrojanGYM on SRAM, AES-128, and UART designs at RTL level, and show that it systematically produces diverse, functionally correct HTs that reach up to 83.33% evasion rates against modern GNN-based detectors, revealing robustness gaps that are not apparent when these detectors are evaluated solely on existing TrustHub-style benchmarks. Post peer-review, we will release all codes and artifacts.

</details>


### [693] [@NTT: Algorithm-Targeted NTT hardware acceleration via Design-Time Constant Optimization](https://arxiv.org/abs/2601.17806)
*Mohammed Nabeel,Mahmoud Hafez,Michail Maniatakos*

Main category: cs.CR

TL;DR: The paper introduces @NTT, a hardware-optimized design for the Number Theoretic Transform (NTT) used in lattice-based PQC, achieving high throughput with efficient area usage.


<details>
  <summary>Details</summary>
Motivation: NTT is a computational bottleneck in lattice-based PQC, and while hardware implementations outperform software, they traditionally require large resources for high throughput.

Method: The proposed @NTT design leverages the fixed ring parameters in cryptographic algorithms for constant optimization at design time to achieve maximum throughput with reduced hardware footprint.

Result: The @NTT implementation in TSMC 28 nm achieved 1.0 GHz performance on 1.45 mm^2 area and demonstrated an FPGA throughput-per-LUT 5.2x better than existing designs.

Conclusion: @NTT enhances the efficiency of NTT hardware for PQC, delivering maximum throughput and compact size, demonstrating significant improvements both on ASIC and FPGA platforms.

Abstract: The Number Theoretic Transform (NTT) is a critical computational bottleneck in many lattice-based postquantum cryptographic (PQC) algorithms. By leveraging the Fast Fourier Transform (FFT) algorithm, the NTT of a polynomial of degree N - 1 can be computed with a time complexity of O(N log N). Hardware implementation of NTT is generally preferred over software ones, as the latter are significantly slower due to complex memory access patterns and modular arithmetic operations. Achieving maximum throughput in hardware, however, typically demands a prohibitively large number of butterfly unit instantiations. In this work, we propose @NTT, which exploits the fact that the ring parameters in these algorithms are fixed, enabling design-time constant optimization and achieving the maximum throughput of N-point NTT per clock cycle with a compact hardware footprint. Our case study on the Dilithium NTT, implemented using the TSMC 28 nm library, operates at a clock frequency of 1.0 GHz with an area of 1.45 mm^2. On FPGA, the design achieves a throughput-per-LUT that is 5.2x higher than the state-of-the-art implementation.

</details>


### [694] [MultiChain Blockchain Data Provenance for Deterministic Stream Processing with Kafka Streams: A Weather Data Case Study](https://arxiv.org/abs/2601.18011)
*Niaz Mohammad Ramaki,Florian Schintke*

Main category: cs.CR

TL;DR: This paper introduces a blockchain-backed provenance architecture for real-time streaming platforms to ensure auditability and reproducibility.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in auditability and reproducibility caused by non-deterministic outputs in real-time streaming platforms.

Method: The authors propose a system that calculates Merkle roots of canonicalized and serialized weather records per window and stores cryptographic checkpoints on a blockchain.

Result: The system achieves linear verification costs, deterministic reproducibility, scalable off-chain storage anchored to on-chain cryptographic data, and satisfactory blockchain transaction rates.

Conclusion: The blockchain-backed system enables independent auditors to verify window payload completeness, serialization canonicality, and correctness of derived analytics, demonstrating viability and scalability.

Abstract: Auditability and reproducibility still are critical challenges for real-time data streams pipelines. Streaming engines are highly dependent on runtime scheduling, window triggers, arrival orders, and uncertainties such as network jitters. These all derive the streaming pipeline platforms to throw non-determinist outputs. In this work, we introduce a blockchain-backed provenance architecture for streaming platform (e.g Kafka Streams) the publishes cryptographic data of a windowed data stream without publishing window payloads on-chain. We used real-time weather data from weather stations in Berlin. Weather records are canonicalized, deduplicated, and aggregated per window, then serialised deterministically. Furthermore, the Merkle root of the records within the window is computed and stored alongside with Kafka offsets boundaries to MultiChain blockchain streams as checkpoints. Our design can enable an independent auditor to verify: (1) the completeness of window payloads, (2) canonical serialization, and (3) correctness of derived analytics such as minimum/maximum/average temperatures. We evaluated our system using real data stream from two weather stations (Berlin-Brandenburg and Berlin-Tempelhof) and showed linear verification cost, deterministic reproducibility, and with a scalable off-chain storage with on-chain cryptographic anchoring. We also demonstrated that the blockchain can afford to be integrated with streaming platforms particularly with our system, and we get satisfactory transactions per second values.

</details>


### [695] [Rhea: Detecting Privilege-Escalated Evasive Ransomware Attacks Using Format-Aware Validation in the Cloud](https://arxiv.org/abs/2601.18216)
*Beom Heyn Kim,Seok Min Hong,Mohammad Mannan*

Main category: cs.CR

TL;DR: This paper introduces Rhea, a ransomware defense system leveraging Format-Aware Validation of file formats to effectively detect privilege-escalated evasive ransomware tactics.


<details>
  <summary>Details</summary>
Motivation: Existing ransomware solutions struggle to detect privilege-escalated evasive ransomware due to obfuscated I/O traces and unreliable statistical content-based detection.

Method: Rhea utilizes Format-Aware Validation of replicated data snapshots to ensure file-format correctness and detect evasive ransomware attacks based on format specifications.

Result: Rhea significantly outperformed conventional approaches in detecting fine-grained encryption even under elevated attacker privileges.

Conclusion: Rhea's format-based detection proves effective against modern ransomware variants, providing a robust defense solution against privilege-escalated evasive threats.

Abstract: Ransomware variants increasingly combine privilege escalation with sophisticated evasion strategies such as intermittent encryption, low-entropy encryption, and imitation attacks. Such powerful ransomware variants, privilege-escalated evasive ransomware (PEER), can defeat existing solutions relying on I/O-pattern analysis by tampering with or obfuscating I/O traces. Meanwhile, conventional statistical content-based detection becomes unreliable as the encryption size decreases due to sampling noises. We present Rhea, a cloud-offloaded ransomware defense system that analyzes replicated data snapshots, so-called mutation snapshots. Rhea introduces Format-Aware Validation that validates the syntactic and semantic correctness of file formats, instead of relying on statistical or entropy-based indicators. By leveraging file-format specifications as detection invariants, Rhea can reliably identify fine-grained and evasive encryption even under elevated attacker privileges. Our evaluation demonstrates that Rhea significantly outperforms existing approaches, establishing its practical effectiveness against modern ransomware threats.

</details>


### [696] [On the Insecurity of Keystroke-Based AI Authorship Detection: Timing-Forgery Attacks Against Motor-Signal Verification](https://arxiv.org/abs/2601.17280)
*David Condrey*

Main category: cs.CR

TL;DR: The paper critiques keystroke timing-based methods used to distinguish human-written text from AI-generated content, demonstrating that such defenses are vulnerable to practical attacks.


<details>
  <summary>Details</summary>
Motivation: To evaluate the security of keystroke timing signals as a defense mechanism against AI-generated text as more AI and large language models (LLMs) influence text generation.

Method: The authors tested two attack types—copy-type (human transcribing AI text) and timing-forgery (automated agents mimicking keystroke timings)—and evaluated their effectiveness using thousands of timing sessions and timing-forgery variants.

Result: They found the attacks achieved a 99.8% evasion rate against classifiers, even when detectors showed high performance against fully-automated attacks.

Conclusion: Keystroke timing signals confirm keyboard operation by a human but fail to ensure text's provenance due to exploitable vulnerabilities, necessitating better mechanisms binding writing to semantic content.

Abstract: Recent proposals advocate using keystroke timing signals, specifically the coefficient of variation ($δ$) of inter-keystroke intervals, to distinguish human-composed text from AI-generated content. We demonstrate that this class of defenses is insecure against two practical attack classes: the copy-type attack, in which a human transcribes LLM-generated text producing authentic motor signals, and timing-forgery attacks, in which automated agents sample inter-keystroke intervals from empirical human distributions. Using 13,000 sessions from the SBU corpus and three timing-forgery variants (histogram sampling, statistical impersonation, and generative LSTM), we show all attacks achieve $\ge$99.8% evasion rates against five classifiers. While detectors achieve AUC=1.000 against fully-automated injection, they classify $\ge$99.8% of attack samples as human with mean confidence $\ge$0.993. We formalize a non-identifiability result: when the detector observes only timing, the mutual information between features and content provenance is zero for copy-type attacks. Although composition and transcription produce statistically distinguishable motor patterns (Cohen's d=1.28), both yield $δ$ values 2-4x above detection thresholds, rendering the distinction security-irrelevant. These systems confirm a human operated the keyboard, but not whether that human originated the text. Securing provenance requires architectures that bind the writing process to semantic content.

</details>


### [697] [Res-MIA: A Training-Free Resolution-Based Membership Inference Attack on Federated Learning Models](https://arxiv.org/abs/2601.17378)
*Mohammad Zare,Pirooz Shamsinejadbabaki*

Main category: cs.CR

TL;DR: Res-MIA is a novel black-box membership inference attack that exploits model sensitivity to resolution changes, targeting privacy leakage in federated learning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the privacy risks in federated learning (FL), specifically membership inference attacks that exploit overfitted models to reveal training data membership.

Method: Res-MIA progressively reduces input resolution and analyzes the decline in model confidence through controlled downsampling and restoration, leveraging differences in confidence decay between member and non-member samples.

Result: Res-MIA achieves high performance, notably obtaining an AUC score of up to 0.88 on a federated ResNet-18 with minimal computational load, outperforming existing baselines without requiring additional models or data.

Conclusion: Res-MIA exposes frequency-sensitive overfitting as a source of privacy vulnerability in FL and calls for privacy-aware training methods to mitigate reliance on fine-grained input features.

Abstract: Membership inference attacks (MIAs) pose a serious threat to the privacy of machine learning models by allowing adversaries to determine whether a specific data sample was included in the training set. Although federated learning (FL) is widely regarded as a privacy-aware training paradigm due to its decentralized nature, recent evidence shows that the final global model can still leak sensitive membership information through black-box access. In this paper, we introduce Res-MIA, a novel training-free and black-box membership inference attack that exploits the sensitivity of deep models to high-frequency input details. Res-MIA progressively degrades the input resolution using controlled downsampling and restoration operations, and analyzes the resulting confidence decay in the model's predictions. Our key insight is that training samples exhibit a significantly steeper confidence decline under resolution erosion compared to non-member samples, revealing a robust membership signal. Res-MIA requires no shadow models, no auxiliary data, and only a limited number of forward queries to the target model. We evaluate the proposed attack on a federated ResNet-18 trained on CIFAR-10, where it consistently outperforms existing training-free baselines and achieves an AUC of up to 0.88 with minimal computational overhead. These findings highlight frequency-sensitive overfitting as an important and previously underexplored source of privacy leakage in federated learning, and emphasize the need for privacy-aware model designs that reduce reliance on fine-grained, non-robust input features.

</details>


### [698] [Prompt and Circumstances: Evaluating the Efficacy of Human Prompt Inference in AI-Generated Art](https://arxiv.org/abs/2601.17379)
*Khoi Trinh,Scott Seidenberger,Joseph Spracklen,Raveen Wijewickrama,Bimal Viswanath,Murtuza Jadliwala,Anindya Maiti*

Main category: cs.CR

TL;DR: The paper examines the ownership and effectiveness of concealed prompts in AI-generated art, exploring human and AI abilities to infer prompts from associated images.


<details>
  <summary>Details</summary>
Motivation: To investigate potential intellectual property claims over concealed prompts in prompt marketplaces amidst their inference by humans and AI.

Method: The study evaluates human ability to infer prompts from AI-generated images, conducts AI-supported prompt inferences, and combines human and AI efforts to assess performance.

Result: Neither inferred human nor combined human-AI prompts generated images as successfully as the original prompt, with no improvement when merging human and AI efforts.

Conclusion: Current methods of human and combined human-AI prompt inferences do not effectively replicate results from original prompts, challenging marketplace ownership claims over such prompts.

Abstract: The emerging field of AI-generated art has witnessed the rise of prompt marketplaces, where creators can purchase, sell, or share prompts to generate unique artworks. These marketplaces often assert ownership over prompts, claiming them as intellectual property. This paper investigates whether concealed prompts sold on prompt marketplaces can be considered bona fide intellectual property, given that humans and AI tools may be able to infer the prompts based on publicly advertised sample images accompanying each prompt on sale. Specifically, our study aims to assess (i) how accurately humans can infer the original prompt solely by examining an AI-generated image, with the goal of generating images similar to the original image, and (ii) the possibility of improving upon individual human and AI prompt inferences by crafting combined human and AI prompts with the help of a large language model. Although previous research has explored AI-driven prompt inference and protection strategies, our work is the first to incorporate a human subject study and examine collaborative human-AI prompt inference in depth. Our findings indicate that while prompts inferred by humans and prompts inferred through a combined human and AI effort can generate images with a moderate level of similarity, they are not as successful as using the original prompt. Moreover, combining human- and AI-inferred prompts using our suggested merging techniques did not improve performance over purely human-inferred prompts.

</details>


### [699] [Reconstructing Training Data from Adapter-based Federated Large Language Models](https://arxiv.org/abs/2601.17533)
*Silong Chen,Yuchuan Luo,Guilin Deng,Yi Liu,Min Xu,Shaojing Fu,Xiaohua Jia*

Main category: cs.CR

TL;DR: This paper reveals that low-rank adapters in federated large language models (FedLLMs) introduce exploitable privacy vulnerabilities, contrary to their assumed protection against gradient attacks.


<details>
  <summary>Details</summary>
Motivation: The study aims to investigate the privacy vulnerabilities of low-rank adapters in FedLLMs, especially under the assumption that they prevent gradient-based attacks.

Method: The authors propose the Unordered-word-bag-based Text Reconstruction (UTR) attack that exploits gradient properties of low-rank adapters. UTR addresses challenges of low dimensions, frozen backbones, and large reconstruction spaces utilizing unique techniques like attention inference, sentence-level inversion in the adapter's subspace, and guided semantic decoding.

Result: UTR achieves exceptional text reconstruction accuracy (ROUGE-1/2 > 99) across multiple high-performance models (GPT2-Large, BERT, Qwen2.5-7B) and datasets, even under challenging batch size conditions where previous gradient inversion attacks fail.

Conclusion: This work challenges the belief that adapter-based FedLLMs naturally enhance security and highlights a trade-off between parameter efficiency and privacy, with significant implications for federated machine learning adoption.

Abstract: Adapter-based Federated Large Language Models (FedLLMs) are widely adopted to reduce the computational, storage, and communication overhead of full-parameter fine-tuning for web-scale applications while preserving user privacy. By freezing the backbone and training only compact low-rank adapters, these methods appear to limit gradient leakage and thwart existing Gradient Inversion Attacks (GIAs).
  Contrary to this assumption, we show that low-rank adapters create new, exploitable leakage channels. We propose the Unordered-word-bag-based Text Reconstruction (UTR) attack, a novel GIA tailored to the unique structure of adapter-based FedLLMs. UTR overcomes three core challenges: low-dimensional gradients, frozen backbones, and combinatorially large reconstruction spaces by: (i) inferring token presence from attention patterns in frozen layers, (ii) performing sentence-level inversion within the low-rank subspace of adapter gradients, and (iii) enforcing semantic coherence through constrained greedy decoding guided by language priors. Extensive experiments across diverse models (GPT2-Large, BERT, Qwen2.5-7B) and datasets (CoLA, SST-2, Rotten Tomatoes) demonstrate that UTR achieves near-perfect reconstruction accuracy (ROUGE-1/2 > 99), even with large batch size settings where prior GIAs fail completely. Our results reveal a fundamental tension between parameter efficiency and privacy in FedLLMs, challenging the prevailing belief that lightweight adaptation inherently enhances security. Our code and data are available at https://github.com/shwksnshwowk-wq/GIA.

</details>


### [700] [Breaking the Protocol: Security Analysis of the Model Context Protocol Specification and Prompt Injection Vulnerabilities in Tool-Integrated LLM Agents](https://arxiv.org/abs/2601.17549)
*Narek Maloyan,Dmitry Namiot*

Main category: cs.CR

TL;DR: The paper conducts the first formal security analysis of the Model Context Protocol (MCP), identifies vulnerabilities, and proposes a solution to enhance its security.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the lack of formal security analysis for MCP, which is widely used to integrate Large Language Models (LLMs) with external tools.

Method: The authors identify key vulnerabilities in MCP and propose an analysis framework called MCPBench, which evaluates 847 attack scenarios across five server implementations. They also introduce MCPSec, a protocol extension to mitigate these vulnerabilities.

Result: The experiments show MCP amplifies attack success rates by 23-41% compared to non-MCP integrations. The proposed MCPSec reduces attack success rates from 52.8% to 12.4% with minimal latency.

Conclusion: The paper concludes that MCP's security flaws are architectural, necessitating protocol-level interventions like the proposed MCPSec for effective mitigation.

Abstract: The Model Context Protocol (MCP) has emerged as a de facto standard for integrating Large Language Models with external tools, yet no formal security analysis of the protocol specification exists. We present the first rigorous security analysis of MCP's architectural design, identifying three fundamental protocol-level vulnerabilities: (1) absence of capability attestation allowing servers to claim arbitrary permissions, (2) bidirectional sampling without origin authentication enabling server-side prompt injection, and (3) implicit trust propagation in multi-server configurations. We implement \textsc{MCPBench}, a novel framework bridging existing agent security benchmarks to MCP-compliant infrastructure, enabling direct measurement of protocol-specific attack surfaces. Through controlled experiments on 847 attack scenarios across five MCP server implementations, we demonstrate that MCP's architectural choices amplify attack success rates by 23--41\% compared to equivalent non-MCP integrations. We propose \textsc{MCPSec}, a backward-compatible protocol extension adding capability attestation and message authentication, reducing attack success rates from 52.8\% to 12.4\% with median latency overhead of 8.3ms per message. Our findings establish that MCP's security weaknesses are architectural rather than implementation-specific, requiring protocol-level remediation.

</details>


### [701] [A Systemic Evaluation of Multimodal RAG Privacy](https://arxiv.org/abs/2601.17644)
*Ali Al-Lawati,Suhang Wang*

Main category: cs.CR

TL;DR: The paper addresses privacy risks in multimodal Retrieval-Augmented Generation (mRAG) pipelines, particularly data leakage during inference.


<details>
  <summary>Details</summary>
Motivation: To analyze privacy risks of mRAG pipelines, as they involve potential data leakage when private datasets are connected to improve model performance.

Method: An empirical study focusing on inferring the inclusion of visual assets and leaking metadata through standard model prompting.

Result: The study identifies privacy vulnerabilities in mRAG pipelines, particularly around metadata leakage associated with private datasets.

Conclusion: New privacy-preserving mechanisms are essential to address these issues and motivate research in ensuring mRAG's secure use.

Abstract: The growing adoption of multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks (e.g. visual QA) introduces important privacy challenges. In particular, while mRAG provides a practical capability to connect private datasets to improve model performance, it risks the leakage of private information from these datasets during inference. In this paper, we perform an empirical study to analyze the privacy risks inherent in the mRAG pipeline observed through standard model prompting. Specifically, we implement a case study that attempts to infer the inclusion of a visual asset, e.g. image, in the mRAG, and if present leak the metadata, e.g. caption, related to it. Our findings highlight the need for privacy-preserving mechanisms and motivate future research on mRAG privacy.

</details>


### [702] [Multimodal Privacy-Preserving Entity Resolution with Fully Homomorphic Encryption](https://arxiv.org/abs/2601.18612)
*Susim Roy,Nalini Ratha*

Main category: cs.CR

TL;DR: This paper introduces a multimodal framework for secure entity resolution in high-compliance sectors, addressing data volume, matching accuracy, and privacy.


<details>
  <summary>Details</summary>
Motivation: To tackle secure identity reconciliation in sensitive sectors like government and finance, hindered by data heterogeneity and privacy concerns.

Method: A multimodal framework that ensures privacy by keeping plaintext data inaccessible during matching, optimizing matching fidelity with cryptographic assurances.

Result: Achieves low equal error rates while preserving optimal computational scalability, adhering to stringent regulatory privacy mandates.

Conclusion: The framework enables secure, scalable, and accurate entity resolution for sectors with significant data compliance requirements.

Abstract: The canonical challenge of entity resolution within high-compliance sectors, where secure identity reconciliation is frequently confounded by significant data heterogeneity, including syntactic variations in personal identifiers, is a longstanding and complex problem. To this end, we introduce a novel multimodal framework operating with the voluminous data sets typical of government and financial institutions. Specifically, our methodology is designed to address the tripartite challenge of data volume, matching fidelity, and privacy. Consequently, the underlying plaintext of personally identifiable information remains computationally inaccessible throughout the matching lifecycle, empowering institutions to rigorously satisfy stringent regulatory mandates with cryptographic assurances of client confidentiality while achieving a demonstrably low equal error rate and maintaining computational tractability at scale.

</details>


### [703] [Mitigating the OWASP Top 10 For Large Language Models Applications using Intelligent Agents](https://arxiv.org/abs/2601.18105)
*Mohammad Fasha,Faisal Abul Rub,Nasim Matar,Bilal Sowan,Mohammad Al Khaldy*

Main category: cs.CR

TL;DR: This paper introduces a framework to address security vulnerabilities in Large Language Models (LLMs) as identified by OWASP, using intelligent agents to mitigate risks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the security vulnerabilities identified by OWASP in LLM applications, ensuring data integrity, confidentiality, and service availability amidst increasing dependency on these models.

Method: The paper develops a framework leveraging LLM-enabled intelligent agents to identify, assess, and counteract security vulnerabilities in real time.

Result: The framework provides a foundation for addressing the security vulnerabilities of LLMs, offering a tool to proactively mitigate security threats listed in the OWASP Top 10.

Conclusion: This framework demonstrates a potential solution to enhance LLM security and serves as a starting point for future research to handle emerging threats in this field.

Abstract: Large Language Models (LLMs) have emerged as a transformative and disruptive technology, enabling a wide range of applications in natural language processing, machine translation, and beyond. However, this widespread integration of LLMs also raised several security concerns highlighted by the Open Web Application Security Project (OWASP), which has identified the top 10 security vulnerabilities inherent in LLM applications. Addressing these vulnerabilities is crucial, given the increasing reliance on LLMs and the potential threats to data integrity, confidentiality, and service availability. This paper presents a framework designed to mitigate the security risks outlined in the OWASP Top 10. Our proposed model leverages LLM-enabled intelligent agents, offering a new approach to proactively identify, assess, and counteract security threats in real-time. The proposed framework serves as an initial blueprint for future research and development, aiming to enhance the security measures of LLMs and protect against emerging threats in this rapidly evolving landscape.

</details>


### [704] [MalURLBench: A Benchmark Evaluating Agents' Vulnerabilities When Processing Web URLs](https://arxiv.org/abs/2601.18113)
*Dezhang Kong,Zhuxi Wu,Shiqi Liu,Zhicheng Tan,Kuichen Lu,Minghao Li,Qichen Liu,Shengyu Chu,Zhenhua Xu,Xuan Liu,Meng Han*

Main category: cs.CR

TL;DR: The paper introduces MalURLBench, a benchmark to evaluate LLMs' vulnerabilities to malicious URLs. It contains 61,845 attack instances and reveals existing models struggle to detect such threats. A lightweight defense module, URLGuard, is proposed.


<details>
  <summary>Details</summary>
Motivation: The motivation is addressing the lack of benchmarks specifically designed for evaluating the vulnerabilities of LLMs to malicious URLs, which pose a critical threat to users and service providers.

Method: The approach involves creating MalURLBench with 61,845 attack instances covering 10 real-world scenarios and 7 categories of malicious websites, and conducting experiments with 12 popular LLMs.

Result: Findings indicate that current LLMs are poor at identifying cleverly disguised malicious URLs. Key factors affecting attack success rates are analyzed, and URLGuard is introduced as a lightweight defense mechanism.

Conclusion: This work introduces a crucial benchmark (MalURLBench) and a defense module (URLGuard), aiming to improve the security of LLM-based web agents. The public release of code supports further research and development.

Abstract: LLM-based web agents have become increasingly popular for their utility in daily life and work. However, they exhibit critical vulnerabilities when processing malicious URLs: accepting a disguised malicious URL enables subsequent access to unsafe webpages, which can cause severe damage to service providers and users. Despite this risk, no benchmark currently targets this emerging threat. To address this gap, we propose MalURLBench, the first benchmark for evaluating LLMs' vulnerabilities to malicious URLs. MalURLBench contains 61,845 attack instances spanning 10 real-world scenarios and 7 categories of real malicious websites. Experiments with 12 popular LLMs reveal that existing models struggle to detect elaborately disguised malicious URLs. We further identify and analyze key factors that impact attack success rates and propose URLGuard, a lightweight defense module. We believe this work will provide a foundational resource for advancing the security of web agents. Our code is available at https://github.com/JiangYingEr/MalURLBench.

</details>


### [705] [FARM: Few-shot Adaptive Malware Family Classification under Concept Drift](https://arxiv.org/abs/2601.17907)
*Numan Halit Guldemir,Oluwafemi Olukoya,Jesús Martínez-del-Rincón*

Main category: cs.CR

TL;DR: FARM addresses challenges in malware classification by handling concept drift using an adaptive framework involving a triplet autoencoder and few-shot learning.


<details>
  <summary>Details</summary>
Motivation: To combat performance degradation in malware classification caused by evolving threats and the emergence of novel malware families.

Method: The framework uses a triplet autoencoder for drift detection, DBSCAN clustering for unsupervised learning, and few-shot prototype-based classification for quick adaptation.

Result: FARM achieved a 5.6% improvement in classification performance under covariate drift and demonstrated high accuracy (F1 score of 0.85, rising to 0.94 after retraining).

Conclusion: FARM proves effective at managing dynamic malware detection environments and adapting to new threats with limited labeled data.

Abstract: Malware classification models often face performance degradation due to concept drift, arising from evolving threat landscapes and the emergence of novel malware families. This paper presents FARM (Few-shot Adaptive Recognition of Malware), a framework designed to detect and adapt to both covariate and label drift in Windows Portable Executable (PE) malware classification. FARM leverages a triplet autoencoder to project samples into a discriminative latent space, enabling unsupervised drift detection via DBSCAN clustering and dynamic thresholding. For rapid adaptation, it employs few-shot learning using prototype-based classification, requiring only a handful of labeled samples. FARM also supports full retraining when enough drifted samples accumulate, updating the latent space for long-term integration. Experiments on the BenchMFC dataset demonstrate that FARM improves classification performance under covariate drift by 5.6\%, and achieves an average F1 score of 0.85 on unseen malware families using only few-shot adaptation, which further increases to 0.94 after retraining. These results highlight FARM's robustness and adaptability in dynamic malware detection environments under limited supervision.

</details>


### [706] [XGuardian: Towards Explainable and Generalized AI Anti-Cheat on FPS Games](https://arxiv.org/abs/2601.18068)
*Jiayi Zhang,Chenxin Sun,Chenxiong Qian*

Main category: cs.CR

TL;DR: The paper addresses aim-assist cheats in FPS games by introducing XGuardian, a generalized and explainable detection system.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the prevalence of aim-assist cheats in FPS games, which undermine fair play and current detection systems’ limitations, including unreliability, high cost, and lack of explainability.

Method: XGuardian analyzes pitch and yaw data to construct temporal features for detecting cheaters. It focuses on server-side operation, ensuring low overhead and enhanced explainability.

Result: XGuardian demonstrates high detection performance, low computational costs, and applicability to multiple games using real-world datasets.

Conclusion: XGuardian proves effective, generalizable, and explainable in detecting aim-assist cheats, contributing to cheat prevention in FPS games. The datasets and system are made publicly available.

Abstract: Aim-assist cheats are the most prevalent and infamous form of cheating in First-Person Shooter (FPS) games, which help cheaters illegally reveal the opponent's location and auto-aim and shoot, and thereby pose significant threats to the game industry. Although a considerable research effort has been made to automatically detect aim-assist cheats, existing works suffer from unreliable frameworks, limited generalizability, high overhead, low detection performance, and a lack of explainability of detection results. In this paper, we propose XGuardian, a server-side generalized and explainable system for detecting aim-assist cheats to overcome these limitations. It requires only two raw data inputs, pitch and yaw, which are all FPS games' must-haves, to construct novel temporal features and describe aim trajectories, which are essential for distinguishing cheaters and normal players. XGuardian is evaluated with the latest mainstream FPS game CS2, and validates its generalizability with another two different games. It achieves high detection performance and low overhead compared to prior works across different games with real-world and large-scale datasets, demonstrating wide generalizability and high effectiveness. It is able to justify its predictions and thereby shorten the ban cycle. We make XGuardian as well as our datasets publicly available.

</details>


### [707] [$α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks](https://arxiv.org/abs/2601.18754)
*Mohamed Amine Ferrag,Abderrahmane Lakas,Merouane Debbah*

Main category: cs.CR

TL;DR: This paper introduces $α^{3}$-SecBench, a benchmark for evaluating UAV systems' security and trust under adversarial conditions, focusing on LLM-based agents.


<details>
  <summary>Details</summary>
Motivation: To address the gap in systematic evaluation of security, resilience, and trust for LLM-powered autonomous UAV systems operating in adversarial and emerging 6G environments.

Method: $α^{3}$-SecBench, comprising 20,000 security attack scenarios, assesses UAV autonomy across seven layers using three dimensions: security, resilience, and trust, evaluating 23 LLMs in adversarial UAV missions.

Result: Evaluation of 23 state-of-the-art LLMs revealed inconsistent performance in mitigation, attribution, and trustworthiness, with scores ranging from 12.9% to 57.1%.

Conclusion: The paper highlights the need for improved anomaly detection solutions, resilience mechanisms, and trustworthy behaviors in LLM-powered autonomous systems.

Abstract: Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditions remains largely unexplored, particularly in emerging 6G-enabled settings.
  We introduce $α^{3}$-SecBench, the first large-scale evaluation suite for assessing the security-aware autonomy of LLM-based UAV agents under realistic adversarial interference. Building on multi-turn conversational UAV missions from $α^{3}$-Bench, the framework augments benign episodes with 20,000 validated security overlay attack scenarios targeting seven autonomy layers, including sensing, perception, planning, control, communication, edge/cloud infrastructure, and LLM reasoning. $α^{3}$-SecBench evaluates agents across three orthogonal dimensions: security (attack detection and vulnerability attribution), resilience (safe degradation behavior), and trust (policy-compliant tool usage).
  We evaluate 23 state-of-the-art LLMs from major industrial providers and leading AI labs using thousands of adversarially augmented UAV episodes sampled from a corpus of 113,475 missions spanning 175 threat types. While many models reliably detect anomalous behavior, effective mitigation, vulnerability attribution, and trustworthy control actions remain inconsistent. Normalized overall scores range from 12.9% to 57.1%, highlighting a significant gap between anomaly detection and security-aware autonomous decision-making. We release $α^{3}$-SecBench on GitHub: https://github.com/maferrag/AlphaSecBench

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [708] [Bridging Expectation Signals: LLM-Based Experiments and a Behavioral Kalman Filter Framework](https://arxiv.org/abs/2601.17527)
*Yu Wang,Xiangchen Liu*

Main category: econ.GN

TL;DR: This paper studies how LLMs update their beliefs when functioning as economic agents, using a Behavioral Kalman Filter framework to analyze their responses to various signals.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used in economic contexts, the research aims to understand how they process and update beliefs when presented with diverse information, addressing opacity in their mechanisms.

Method: Experiments are conducted using a Behavioral Kalman Filter framework to measure how LLM-based agents process and weigh individual vs aggregate signals under economic scenarios.

Result: The study finds four key patterns: (1) a deviation in how agents weight priors vs signals, (2) higher weights assigned to individual signals over aggregate signals, (3) interaction effects with diminished marginal weight in concurrent signals, (4) significant expectation differences between household and firm CEO agents.

Conclusion: LoRA fine-tuning reduces behavioral biases in LLM expectation formation, but does not completely eliminate them, indicating potential areas for improvement in LLM models acting as economic agents.

Abstract: As LLMs increasingly function as economic agents, the specific mechanisms LLMs use to update their belief with heterogeneous signals remain opaque. We design experiments and develop a Behavioral Kalman Filter framework to quantify how LLM-based agents update expectations, acting as households or firm CEOs, update expectations when presented with individual and aggregate signals. The results from experiments and model estimation reveal four consistent patterns: (1) agents' weighting of priors and signals deviates from unity; (2) both household and firm CEO agents place substantially larger weights on individual signals compared to aggregate signals; (3) we identify a significant and negative interaction between concurrent signals, implying that the presence of multiple information sources diminishes the marginal weight assigned to each individual signal; and (4) expectation formation patterns differ significantly between household and firm CEO agents. Finally, we demonstrate that LoRA fine-tuning mitigates, but does not fully eliminate, behavioral biases in LLM expectation formation.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [709] [FASTR: Reimagining FASTQ via Compact Image-inspired Representation](https://arxiv.org/abs/2601.17184)
*Adrian Tkachenko,Sepehr Salem,Ayotomiwa Ezekiel Adeniyi,Zulal Bingol,Mohammed Nayeem Uddin,Akshat Prasanna,Alexander Zelikovsky,Serghei Mangul,Can Alkan,Mohammed Alser*

Main category: q-bio.GN

TL;DR: This paper introduces FASTR, a new lossless format for genomics data that reduces file size by 2x+, eliminates decompression costs, and supports efficient machine learning workflows.


<details>
  <summary>Details</summary>
Motivation: The inefficiencies of the traditional FASTQ format in handling huge data from high-throughput sequencing drove the need for a more storage-efficient and analysis-ready format.

Method: FASTR encodes nucleotides and their quality scores into a compact 8-bit value, integrates with existing tools, and supports direct usage for downstream or machine-learning workflows.

Result: FASTR decreases file sizes by at least 2x, achieves better compression ratios, and offers significant speedups in compression/decompression compared to FASTQ.

Conclusion: FASTR establishes a scalable, computation-native format suitable for modern genomics and real-time sequencing workflows, addressing storage and efficiency bottlenecks.

Abstract: Motivation: High-throughput sequencing (HTS) enables population-scale genomics but generates massive datasets, creating bottlenecks in storage, transfer, and analysis. FASTQ, the standard format for over two decades, stores one byte per base and one byte per quality score, leading to inefficient I/O, high storage costs, and redundancy. Existing compression tools can mitigate some issues, but often introduce costly decompression or complex dependency issues. Results: We introduce FASTR, a lossless, computation-native successor to FASTQ that encodes each nucleotide together with its base quality score into a single 8-bit value. FASTR reduces file size by at least 2x while remaining fully reversible and directly usable for downstream analyses. Applying general-purpose compression tools on FASTR consistently yields higher compression ratios, 2.47, 3.64, and 4.8x faster compression, and 2.34, 1.96, 1.75x faster decompression than on FASTQ across Illumina, HiFi, and ONT reads. FASTR is machine-learning-ready, allowing reads to be consumed directly as numerical vectors or image-like representations. We provide a highly parallel software ecosystem for FASTQ-FASTR conversion and show that FASTR integrates with existing tools, such as minimap2, with minimal interface changes and no performance overhead. By eliminating decompression costs and reducing data movement, FASTR lays the foundation for scalable genomics analyses and real-time sequencing workflows. Availability and Implementation: https://github.com/ALSER-Lab/FASTR

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [710] [Global Optimization of Atomic Clusters via Physically-Constrained Tensor Train Decomposition](https://arxiv.org/abs/2601.18592)
*Konstantin Sozykin,Nikita Rybin,Andrei Chertkov,Anh-Huy Phan,Ivan Oseledets,Alexander Shapeev,Ivan Novikov,Gleb Ryzhakov*

Main category: math.OC

TL;DR: The paper introduces a Tensor Train (TT) decomposition-based framework that addresses the challenge of global optimization in atomic clusters by exploiting low-rank structures in potential energy surfaces.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational challenge posed by the exponential growth of local minima in atomic cluster optimization due to dimensionality increases.

Method: Combines two TT-based strategies: (1) TTOpt using maximum volume sampling and (2) PROTES using generative sampling, complemented by physically-constrained encoding schemes.

Result: Successfully identified global minima for Lennard-Jones clusters of up to 45 atoms and optimized 20-atom carbon clusters using machine-learned potentials with results matching quantum-accurate simulations.

Conclusion: The proposed TT-decomposition method is a robust tool for molecular structure prediction and offers a versatile framework for solving high-dimensional optimization problems in computational material science.

Abstract: The global optimization of atomic clusters represents a fundamental challenge in computational chemistry and materials science due to the exponential growth of local minima with system size (i.e., the curse of dimensionality). We introduce a novel framework that overcomes this limitation by exploiting the low-rank structure of potential energy surfaces through Tensor Train (TT) decomposition. Our approach combines two complementary TT-based strategies: the algebraic TTOpt method, which utilizes maximum volume sampling, and the probabilistic PROTES method, which employs generative sampling. A key innovation is the development of physically-constrained encoding schemes that incorporate molecular constraints directly into the discretization process. We demonstrate the efficacy of our method by identifying global minima of Lennard-Jones clusters containing up to 45 atoms. Furthermore, we establish its practical applicability to real-world systems by optimizing 20-atom carbon clusters using a machine-learned Moment Tensor Potential, achieving geometries consistent with quantum-accurate simulations. This work establishes TT-decomposition as a powerful tool for molecular structure prediction and provides a general framework adaptable to a wide range of high-dimensional optimization problems in computational material science.

</details>


### [711] [A Unified Kantorovich Duality for Multimarginal Optimal Transport](https://arxiv.org/abs/2601.17171)
*Yehya Cheryala,Mokhtar Z. Alaya,Salim Bouzebda*

Main category: math.OC

TL;DR: This paper develops a unified and comprehensive theory of Kantorovich duality for multimarginal optimal transport (MOT) problems, applicable in general Polish spaces.


<details>
  <summary>Details</summary>
Motivation: There is a need for a theoretical framework to handle MOT problems for aligning multiple probability distributions, which are critical in machine learning and statistics.

Method: The authors establish Kantorovich duality for MOT using convex-analytic reformulation, truncation-tightness procedures, and $c$-splitting techniques to achieve dual attainment and primal-dual equality on Polish spaces.

Result: The study proves dual attainment, provides canonical representation of optimal dual potentials, and extends classical conjugacy principles to multimarginal settings in these mathematical frameworks.

Conclusion: This work offers a foundational structure for analyzing stability, differentiability, and asymptotic behavior of MOT under various marginal perturbations, enhancing its application in statistical and probabilistic studies.

Abstract: Multimarginal optimal transport (MOT) has gained increasing attention in recent years, notably due to its relevance in machine learning and statistics, where one seeks to jointly compare and align multiple probability distributions. This paper presents a unified and complete Kantorovich duality theory for MOT problem on general Polish product spaces with bounded continuous cost function. For marginal compact spaces, the duality identity is derived through a convex-analytic reformulation, that identifies the dual problem as a Fenchel-Rockafellar conjugate. We obtain dual attainment and show that optimal potentials may always be chosen in the class of $c$-conjugate families, thereby extending classical two-marginal conjugacy principle into a genuinely multimarginal setting. In non-compact setting, where direct compactness arguments are unavailable, we recover duality via a truncation-tightness procedure based on weak compactness of multimarginal transference plans and boundedness of the cost. We prove that the dual value is preserved under restriction to compact subsets and that admissible dual families can be regularized into uniformly bounded $c$-conjugate potentials. The argument relies on a refined use of $c$-splitting sets and their equivalence with multimarginal $c$-cyclical monotonicity. We then obtain dual attainment and exact primal-dual equality for MOT on arbitrary Polish spaces, together with a canonical representation of optimal dual potentials by $c$-conjugacy. These results provide a structural foundation for further developments in probabilistic and statistical analysis of MOT, including stability, differentiability, and asymptotic theory under marginal perturbations.

</details>


### [712] [Differentiable Integer Linear Programming is not Differentiable & it's not a mere technical problem](https://arxiv.org/abs/2601.17800)
*Thanawat Sornwanee*

Main category: math.OC

TL;DR: The paper identifies a critical error in the differentiability method used in 'Differentiable Integer Linear Programming' by Geng et al., 2025, relating to the stochastic surrogate loss.


<details>
  <summary>Details</summary>
Motivation: To address incorrect methodology in differentiability approaches for integer linear programming that is impacting downstream research.

Method: Analyzed Theorem 5 of 'Differentiable Integer Linear Programming' and examined stochastic gradient descent behavior concerning the surrogate loss.

Result: The surrogate loss is found to be discontinuous almost everywhere in randomness realizations, contrary to the continuous expectation assumption.

Conclusion: The methodology and foundational assumptions in the referenced work and derived studies must be reconsidered due to the identified error.

Abstract: We show how the differentiability method employed in the paper ``Differentiable Integer Linear Programming'', Geng, et al., 2025 as shown in its theorem 5 is incorrect. Moreover, there already exists some downstream work that inherits the same error. The underlying reason comes from that, though being continuous in expectation, the surrogate loss is discontinuous in almost every realization of the randomness, for the stochastic gradient descent.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [713] [GenAI-Net: A Generative AI Framework for Automated Biomolecular Network Design](https://arxiv.org/abs/2601.17582)
*Maurice Filo,Nicolò Rossi,Zhou Fang,Mustafa Khammash*

Main category: q-bio.QM

TL;DR: GenAI-Net is a new generative AI framework for automating biomolecular circuit design, enabling the creation of diverse and efficient solutions for various synthetic biology challenges.


<details>
  <summary>Details</summary>
Motivation: The goal is to overcome the manual and complex nature of designing chemical reaction networks (CRNs) for synthetic biology applications, which requires navigating vast topologies and stochastic dynamics.

Method: A generative AI framework called GenAI-Net proposes reactions and evaluates them with user-defined simulation objectives to automate CRN design, making it faster and more efficient.

Result: GenAI-Net successfully generates diverse, topologically novel solutions for various design tasks, including dose responses, logic gates, oscillators, and stochastic settings.

Conclusion: GenAI-Net offers a general and efficient approach to programmable biomolecular circuit design, supporting innovation in synthetic biology and accelerating implementation of desired functions.

Abstract: Biomolecular networks underpin emerging technologies in synthetic biology-from robust biomanufacturing and metabolic engineering to smart therapeutics and cell-based diagnostics-and also provide a mechanistic language for understanding complex dynamics in natural and ecological systems. Yet designing chemical reaction networks (CRNs) that implement a desired dynamical function remains largely manual: while a proposed network can be checked by simulation, the reverse problem of discovering a network from a behavioral specification is difficult, requiring substantial human insight to navigate a vast space of topologies and kinetic parameters with nonlinear and possibly stochastic dynamics. Here we introduce GenAI-Net, a generative AI framework that automates CRN design by coupling an agent that proposes reactions to simulation-based evaluation defined by a user-specified objective. GenAI-Net efficiently produces novel, topologically diverse solutions across multiple design tasks, including dose responses, complex logic gates, classifiers, oscillators, and robust perfect adaptation in deterministic and stochastic settings (including noise reduction). By turning specifications into families of circuit candidates and reusable motifs, GenAI-Net provides a general route to programmable biomolecular circuit design and accelerates the translation from desired function to implementable mechanisms.

</details>


### [714] [Point transformer for protein structural heterogeneity analysis using CryoEM](https://arxiv.org/abs/2601.18713)
*Muyuan Chen,Muchen Li,Renjie Liao*

Main category: q-bio.QM

TL;DR: This paper applies a Point Transformer, a self-attention network for point cloud analysis, to improve the heterogeneity analysis of CryoEM data for studying protein dynamics.


<details>
  <summary>Details</summary>
Motivation: Analyzing the structural heterogeneity of proteins from CryoEM data is challenging, especially for systems with multiple degrees of freedom, due to the difficulty in disentangling and interpreting dynamic modes.

Method: The researchers implemented the Point Transformer framework, a self-attention network originally designed for point cloud analysis, to better analyze CryoEM data of proteins.

Result: Their method enhances the performance of CryoEM heterogeneity analysis, enabling more precise characterization of complex protein dynamics.

Conclusion: Point Transformer offers a more interpretable approach to understanding protein dynamics from CryoEM data, addressing challenges in structural heterogeneity analysis.

Abstract: Structural dynamics of macromolecules is critical to their structural-function relationship. Cryogenic electron microscopy (CryoEM) provides snapshots of vitrified protein at different compositional and conformational states, and the structural heterogeneity of proteins can be characterized through computational analysis of the images. For protein systems with multiple degrees of freedom, it is still challenging to disentangle and interpret the different modes of dynamics. Here, by implementing Point Transformer, a self-attention network designed for point cloud analysis, we are able to improve the performance of heterogeneity analysis on CryoEM data, and characterize the dynamics of highly complex protein systems in a more human-interpretable way.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [715] [Credit Fairness: Online Fairness In Shared Resource Pools](https://arxiv.org/abs/2601.17944)
*Seyed Majid Zahedi,Rupert Freeman*

Main category: cs.GT

TL;DR: This paper addresses resource allocation among agents by introducing credit fairness to balance resource disparities over time.


<details>
  <summary>Details</summary>
Motivation: Agents face disparities in total resources received due to the max-min mechanism, despite equal average demand, necessitating fairness improvements.

Method: The authors propose a credit fairness concept and a mechanism ensuring credit fairness alongside Pareto efficiency.

Result: The mechanism achieves credit fairness while maintaining Pareto efficiency, evaluated in computational resource-sharing settings.

Conclusion: Credit fairness allows fair resource allocation, enabling agents to recoup lent resources while maintaining efficiency.

Abstract: We consider a setting in which a group of agents share resources that must be allocated among them in each discrete time period. Agents have time-varying demands and derive constant marginal utility from each unit of resource received up to their demand, with zero utility for any additional resources. In this setting, it is known that independently maximizing the minimum utility in each round satisfies sharing incentives (agents weakly prefer participating in the mechanism to not participating), strategyproofness (agents have no incentive to misreport their demands), and Pareto efficiency (Freeman et al. 2018). However, recent work (Vuppalapati et al. 2023) has shown that this max-min mechanism can lead to large disparities in the total resources received by agents, even when they have the same average demand. In this paper, we introduce credit fairness, a strengthening of sharing incentives that ensures agents who lend resources in early rounds are able to recoup them in later rounds. Credit fairness can be achieved in conjunction with either Pareto efficiency or strategyproofness, but not both. We propose a mechanism that is credit fair and Pareto efficient, and we evaluate its performance in a computational resource-sharing setting.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [716] [How Information Evolves: Stability-Driven Assembly and the Emergence of a Natural Genetic Algorithm](https://arxiv.org/abs/2601.17061)
*Dan Adler*

Main category: q-bio.PE

TL;DR: This paper introduces Stability-Driven Assembly (SDA), a framework showcasing how differential persistence and stochastic assembly can drive evolutionary processes without genes or predefined fitness.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore how evolutionary dynamics and information progression can arise naturally through physical processes, even in the absence of traditional biological mechanisms like genes or replication.

Method: The Stability-Driven Assembly (SDA) framework is applied to chemical symbol space using SMILES fragments. The model incorporates recombination, mutation, and a stability-based heuristic to simulate assembly dynamics.

Result: Simulations demonstrated characteristic features of evolutionary systems, such as scaffold-level dominance, sustained novelty, and entropy reduction, showing evolution driven by stability rather than equilibrium-based interactions.

Conclusion: The findings suggest evolution can emerge through persistence-driven selection alone, providing insights into pre-genetic evolutionary processes and supporting the hypothesis of an evolutionary ladder predating genetic replication.

Abstract: Information can evolve as a physical consequence of non-equilibrium dynamics, even in the absence of genes, replication, or predefined fitness functions. We present Stability-Driven Assembly (SDA), a framework in which stochastic assembly combined with differential persistence biases populations toward longer-lived motifs. Assemblies that persist longer become more frequent and are therefore more likely to participate in subsequent interactions, generating feedback that reshapes the population distribution and implements fitness-proportional sampling, realizing evolution as a natural, emergent genetic algorithm (SDA/GA) driven solely by stability. We apply SDA/GA to chemical symbol space using SMILES fragments with recombination, mutation, and a heuristic stability function. Simulations show hallmark features of evolutionary search, including scaffold-level dominance, sustained novelty, and entropy reduction, yielding open-ended dynamics absent from equilibrium models with fixed transition rates. These results motivate an evolutionary ladder hypothesis where persistence-driven selection precedes genetic replication.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [717] [Explaining Synergistic Effects in Social Recommendations](https://arxiv.org/abs/2601.18151)
*Yicong Li,Shan Jin,Qi Liu,Shuo Wang,Jiaying Liu,Shuo Yu,Qiang Zhang,Kuanjiu Zhou,Feng Xia*

Main category: cs.SI

TL;DR: This paper introduces SemExplainer, a method for improving explainability in social recommenders by identifying subgraphs impacting recommendations through synergistic effects.


<details>
  <summary>Details</summary>
Motivation: Current social recommenders lack explainability in revealing how multiple social networks synergize to produce recommendations due to the opacity of synergistic effects.

Method: SemExplainer identifies subgraphs that embody synergistic effects by extracting preliminary explanatory subgraphs and optimizing conditional entropy to maximize information gain.

Result: Experiments on three datasets show that SemExplainer outperforms traditional methods in explaining synergistic effects in recommendation systems.

Conclusion: SemExplainer effectively explains how synergistic effects across various social networks determine recommendations, enhancing transparency in social recommender systems.

Abstract: In social recommenders, the inherent nonlinearity and opacity of synergistic effects across multiple social networks hinders users from understanding how diverse information is leveraged for recommendations, consequently diminishing explainability. However, existing explainers can only identify the topological information in social networks that significantly influences recommendations, failing to further explain the synergistic effects among this information. Inspired by existing findings that synergistic effects enhance mutual information between inputs and predictions to generate information gain, we extend this discovery to graph data. We quantify graph information gain to identify subgraphs embodying synergistic effects. Based on the theoretical insights, we propose SemExplainer, which explains synergistic effects by identifying subgraphs that embody them. SemExplainer first extracts explanatory subgraphs from multi-view social networks to generate preliminary importance explanations for recommendations. A conditional entropy optimization strategy to maximize information gain is developed, thereby further identifying subgraphs that embody synergistic effects from explanatory subgraphs. Finally, SemExplainer searches for paths from users to recommended items within the synergistic subgraphs to generate explanations for the recommendations. Extensive experiments on three datasets demonstrate the superiority of SemExplainer over baseline methods, providing superior explanations of synergistic effects.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [718] [Laser interferometry as a robust neuromorphic platform for machine learning](https://arxiv.org/abs/2601.18047)
*Amanuel Anteneh,Kyungeun Kim,J. M. Schwarz,Israel Klich,Olivier Pfister*

Main category: physics.optics

TL;DR: A method for creating a fully linear optical neural network using coherent states of light is proposed, enabling easier experimental implementation compared to prior methods, with added resilience to photon losses.


<details>
  <summary>Details</summary>
Motivation: To simplify the experimental implementation of optical neural networks by using linear optical resources and to address challenges like resilience to photon losses.

Method: A neural network architecture is developed using linear optical methods, such as field displacement and interferometry. Nonlinearity is achieved through input-encoded phase shifts, integrating $	extit{in situ}$ training with techniques like parameter shift rules or physical backpropagation.

Result: The proposed design demonstrates efficient learning capabilities with $	extit{in situ}$ inference and training while showing high resilience to photon losses.

Conclusion: This work showcases an experimentally feasible and robust optical neural network design, advancing $	extit{in situ}$ machine learning techniques using linear optics.

Abstract: We present a method for implementing an optical neural network using only linear optical resources, namely field displacement and interferometry applied to coherent states of light. The nonlinearity required for learning in a neural network is realized via an encoding of the input into phase shifts allowing for far more straightforward experimental implementation compared to previous proposals for, and demonstrations of, $\textit{in situ}$ inference. Beyond $\textit{in situ}$ inference, the method enables $\textit{in situ}$ training by utilizing established techniques like parameter shift rules or physical backpropagation to extract gradients directly from measurements of the linear optical circuit. We also investigate the effect of photon losses and find the model to be very resilient to these.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [719] [Structure-Aware NL-to-SQL for SFC Provisioning via AST-Masking Empowered Language Models](https://arxiv.org/abs/2601.17295)
*Xinyu Zhu,Parisa Fard Moshiri,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: The paper introduces Abstract Syntax Tree (AST)-Masking for structure-aware fine-tuning to enhance SQL generation accuracy for Service Function Chain (SFC) management, achieving notable improvements.


<details>
  <summary>Details</summary>
Motivation: The current challenge lies in orchestrating SFCs in dynamic networks where domain knowledge integration and interpretability in Reinforcement Learning (RL) methods are lacking, limiting generalization. The authors aim to bridge this gap using Large Language Models (LLMs) for better domain-specific command creation.

Method: The proposed AST-Masking method uses Abstract Syntax Trees of SQL to weight key components during fine-tuning, ensuring syntax-aware learning without adding additional inference overhead.

Result: Experiments reveal that AST-Masking significantly improves SQL generation accuracy, with FLAN-T5 achieving 99.6% execution accuracy, and Gemma increasing its accuracy from 7.5% to 72.0%.

Conclusion: Structure-aware fine-tuning via AST-Masking effectively ensures syntactically correct and efficient SQL generation, offering a robust approach for SFC orchestration in dynamic environments.

Abstract: Effective Service Function Chain (SFC) provisioning requires precise orchestration in dynamic and latency-sensitive networks. Reinforcement Learning (RL) improves adaptability but often ignores structured domain knowledge, which limits generalization and interpretability. Large Language Models (LLMs) address this gap by translating natural language (NL) specifications into executable Structured Query Language (SQL) commands for specification-driven SFC management. Conventional fine-tuning, however, can cause syntactic inconsistencies and produce inefficient queries. To overcome this, we introduce Abstract Syntax Tree (AST)-Masking, a structure-aware fine-tuning method that uses SQL ASTs to assign weights to key components and enforce syntax-aware learning without adding inference overhead. Experiments show that AST-Masking significantly improves SQL generation accuracy across multiple language models. FLAN-T5 reaches an Execution Accuracy (EA) of 99.6%, while Gemma achieves the largest absolute gain from 7.5% to 72.0%. These results confirm the effectiveness of structure-aware fine-tuning in ensuring syntactically correct and efficient SQL generation for interpretable SFC orchestration.

</details>


### [720] [Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control](https://arxiv.org/abs/2601.18069)
*Haoyuan Pan,Sizhao Chen,Zhaorui Wang,Tse-Tin Chan*

Main category: cs.NI

TL;DR: This paper proposes novel algorithms, D2SAC and RS-D3SAC, to address both average and tail-risk-sensitive Version Age of Information (VAoI) scheduling in multi-user wireless systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of current VAoI scheduling approaches, which only focus on average VAoI minimization and disregard rare but severe staleness events that could degrade reliability in real-time wireless systems.

Method: Two advanced algorithms are proposed: (1) D2SAC for average VAoI minimization using a diffusion-based Soft Actor-Critic framework, and (2) RS-D3SAC for risk-sensitive VAoI optimization incorporating a quantile-based distributional critic and Conditional Value-at-Risk (CVaR) optimization while adhering to long-term transmission cost constraints.

Result: Simulations demonstrate that D2SAC minimizes average VAoI effectively, while RS-D3SAC achieves large reductions in tail-risk (CVaR) without compromising average performance, leveraging the distributional critic and diffusion-based actor.

Conclusion: The proposed algorithms prove robust and effective in addressing average and risk-sensitive VAoI challenges, emphasizing their potential in enhancing reliability and robustness for multi-user wireless communication systems.

Abstract: Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [721] [LLAMA LIMA: A Living Meta-Analysis on the Effects of Generative AI on Learning Mathematics](https://arxiv.org/abs/2601.18685)
*Anselm Strohmaier,Samira Bödefeld,Frank Reinhold*

Main category: math.HO

TL;DR: The paper introduces a Living Meta-Analysis framework for studying generative AI in math education, reporting an initial small positive effect from current studies.


<details>
  <summary>Details</summary>
Motivation: Generative AI's rapid development in mathematics education creates challenges for research synthesis to remain up-to-date.

Method: A Living Meta-Analysis (LIMA) approach is employed, continuously updating literature while applying Bayesian multilevel meta-regression and adhering to PRISMA-LSR guidelines.

Result: Initial analysis of 15 studies shows a small positive effect (g = 0.31) with wide credible intervals [0.06, 0.58], highlighting limited current evidence.

Conclusion: LIMA provides an evolving framework to address gaps and ensure timely synthesis of generative AI's impact in mathematics education.

Abstract: The capabilities of generative AI in mathematics education are rapidly evolving, posing significant challenges for research to keep pace. Research syntheses remain scarce and risk being outdated by the time of publication. To address this issue, we present a Living Meta-Analysis (LIMA) on the effects of generative AI-based interventions for learning mathematics. Following PRISMA-LSR guidelines, we continuously update the literature base, apply a Bayesian multilevel meta-regression model to account for cumulative data, and publish updated versions on a preprint server at regular intervals. This paper reports results from the first version, including 15 studies. The analyses indicate a small positive effect (g = 0.31) with a wide credible interval [0.06, 0.58], reflecting the still limited evidence base.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [722] [Flow-based Extremal Mathematical Structure Discovery](https://arxiv.org/abs/2601.18005)
*Gergely Bérczi,Baran Hashemi,Jonas Klüver*

Main category: math.CO

TL;DR: FlowBoost framework introduces a novel closed-loop generative method to solve challenging extremal geometric structure problems with impressive efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The complexity of discovering extremal geometric structures makes analytical methods inadequate and brute-force computational methods impractical due to vast and nonconvex search spaces.

Method: FlowBoost combines (i) a conditional flow-matching model for sampling, (ii) reward-guided policy optimization for generation, and (iii) stochastic local search for refinement and training-data generation. It integrates geometric feasibility and reward feedback into the model, creating a closed-loop system unlike previous methods.

Result: FlowBoost demonstrated its capability by solving four different geometric problems, outperforming existing benchmarks and discovering better results with significantly fewer resources.

Conclusion: FlowBoost is a superior framework for extremal geometry discovery, offering enhanced efficiency, reduced dependency on computational resources, and notable performance improvements compared to traditional and LLM-based methods.

Abstract: The discovery of extremal structures in mathematics requires navigating vast and nonconvex landscapes where analytical methods offer little guidance and brute-force search becomes intractable. We introduce FlowBoost, a closed-loop generative framework that learns to discover rare and extremal geometric structures by combining three components: (i) a geometry-aware conditional flow-matching model that learns to sample high-quality configurations, (ii) reward-guided policy optimization with action exploration that directly optimizes the generation process toward the objective while maintaining diversity, and (iii) stochastic local search for both training-data generation and final refinement. Unlike prior open-loop approaches, such as PatternBoost that retrains on filtered discrete samples, or AlphaEvolve which relies on frozen Large Language Models (LLMs) as evolutionary mutation operators, FlowBoost enforces geometric feasibility during sampling, and propagates reward signal directly into the generative model, closing the optimization loop and requiring much smaller training sets and shorter training times, and reducing the required outer-loop iterations by orders of magnitude, while eliminating dependence on LLMs. We demonstrate the framework on four geometric optimization problems: sphere packing in hypercubes, circle packing maximizing sum of radii, the Heilbronn triangle problem, and star discrepancy minimization. In several cases, FlowBoost discovers configurations that match or exceed the best known results. For circle packings, we improve the best known lower bounds, surpassing the LLM-based system AlphaEvolve while using substantially fewer computational resources.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [723] [Analyzing Images of Blood Cells with Quantum Machine Learning Methods: Equilibrium Propagation and Variational Quantum Circuits to Detect Acute Myeloid Leukemia](https://arxiv.org/abs/2601.18710)
*A. Bano,L. Liebovitch*

Main category: cs.ET

TL;DR: The study demonstrates the feasibility of quantum machine learning (QML) algorithms for medical imaging, showing competitive performance under constraints.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of QML algorithms in achieving useful medical applications despite computational limitations.

Method: Two QML techniques—Equilibrium Propagation and Variational Quantum Circuits—were applied for detecting acute myeloid leukemia using microscopy image classification.

Result: QML methods showed strong performance, achieving 83-86.4% accuracy even with limited data and lower resolution, coming close to traditional CNN results.

Conclusion: Quantum methods are viable for healthcare applications during the NISQ era, as they deliver consistent performance using less data compared to CNNs.

Abstract: This paper presents a feasibility study demonstrating that quantum machine learning (QML) algorithms achieve competitive performance on real-world medical imaging despite operating under severe constraints. We evaluate Equilibrium Propagation (EP), an energy-based learning method that does not use backpropagation (incompatible with quantum systems due to state-collapsing measurements) and Variational Quantum Circuits (VQCs) for automated detection of Acute Myeloid Leukemia (AML) from blood cell microscopy images using binary classification (2 classes: AML vs. Healthy).
  Key Result: Using limited subsets (50-250 samples per class) of the AML-Cytomorphology dataset (18,365 expert-annotated images), quantum methods achieve performance only 12-15% below classical CNNs despite reduced image resolution (64x64 pixels), engineered features (20D), and classical simulation via Qiskit. EP reaches 86.4% accuracy (only 12% below CNN) without backpropagation, while the 4-qubit VQC attains 83.0% accuracy with consistent data efficiency: VQC maintains stable 83% performance with only 50 samples per class, whereas CNN requires 250 samples (5x more data) to reach 98%. These results establish reproducible baselines for QML in healthcare, validating NISQ-era feasibility.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [724] [Falsifying Predictive Algorithm](https://arxiv.org/abs/2601.17146)
*Amanda Coston*

Main category: stat.ME

TL;DR: This paper introduces a framework to test algorithms' validity in predicting intended outcomes over impermissible proxies using statistical methods.


<details>
  <summary>Details</summary>
Motivation: There is a need to ensure algorithms predict intended outcomes rather than unintended, impermissible ones, especially before deployment in impactful areas.

Method: The authors propose a falsification framework based on calibrated prediction losses to check discriminant validity. Nonparametric hypothesis testing methods are employed, accommodating both single and multiple permissible proxy settings.

Result: The framework showed discriminant validity against gender but not race in an admissions setting. It also highlighted limitations in a criminal justice setting.

Conclusion: The framework provides an early statistical validity check, complementing fairness and robustness analyses, but requires additional methods to assess external and construct validity.

Abstract: Empirical investigations into unintended model behavior often show that the algorithm is predicting another outcome than what was intended. These exposes highlight the need to identify when algorithms predict unintended quantities - ideally before deploying them into consequential settings. We propose a falsification framework that provides a principled statistical test for discriminant validity: the requirement that an algorithm predict intended outcomes better than impermissible ones. Drawing on falsification practices from causal inference, econometrics, and psychometrics, our framework compares calibrated prediction losses across outcomes to assess whether the algorithm exhibits discriminant validity with respect to a specified impermissible proxy. In settings where the target outcome is difficult to observe, multiple permissible proxy outcomes may be available; our framework accommodates both this setting and the case with a single permissible proxy. Throughout we use nonparametric hypothesis testing methods that make minimal assumptions on the data-generating process. We illustrate the method in an admissions setting, where the framework establishes discriminant validity with respect to gender but fails to establish discriminant validity with respect to race. This demonstrates how falsification can serve as an early validity check, prior to fairness or robustness analyses. We also provide analysis in a criminal justice setting, where we highlight the limitations of our framework and emphasize the need for complementary approaches to assess other aspects of construct validity and external validity.

</details>


### [725] [Transfer learning for scalar-on-function regression via control variates](https://arxiv.org/abs/2601.17217)
*Yuping Yang,Zhiyang Zhou*

Main category: stat.ME

TL;DR: This paper adapts the control-variates method for transfer learning in scalar-on-function regression, focusing on privacy-sensitive settings and achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: To enhance estimation and prediction performance in scalar-on-function regression using transfer learning, especially for privacy-restricted or decentralized data environments.

Method: The authors introduce a framework using the CVS method and dataset-specific summary statistics instead of subject-level data, ensuring privacy preservation. They also establish theoretical connections and derive convergence rates.

Result: The derived convergence rates account for smoothing error and highlight the role of covariance function similarity. Numerical studies validate the framework's performance compared to existing methods.

Conclusion: This method effectively addresses transfer learning in challenging, privacy-sensitive scenarios, achieving strong theoretical and empirical performance.

Abstract: Transfer learning (TL) has emerged as a powerful tool for improving estimation and prediction performance by leveraging information from related datasets. In this paper, we repurpose the control-variates (CVS) method for TL in the context of scalar-on-function regression. Our proposed framework relies exclusively on dataset-specific summary statistics, avoiding the need to pool subject-level data and thus remaining applicable in privacy-restricted or decentralized settings. We establish theoretical connections among several existing TL strategies and derive convergence rates for our CVS-based proposals. These rates explicitly account for the typically overlooked smoothing error and reveal how the similarity among covariance functions across datasets influences convergence behavior. Numerical studies support the theoretical findings and demonstrate that the proposed methods achieve competitive estimation and prediction performance compared with existing alternatives.

</details>


### [726] [Covariate-assisted Grade of Membership Models via Shared Latent Geometry](https://arxiv.org/abs/2601.17265)
*Zhiyu Xu,Yuqi Gu*

Main category: stat.ME

TL;DR: The paper presents a novel covariate-assisted grade of membership model for analyzing multivariate categorical data by integrating response and covariate information without relying on traditional joint likelihood modeling.


<details>
  <summary>Details</summary>
Motivation: To improve latent structure recovery in multivariate categorical data analysis by integrating covariates while addressing computational and misspecification issues in existing methods.

Method: The approach introduces a covariate-assisted grade of membership model using a likelihood-free spectral estimation procedure and heteroskedastic principal component analysis for data integration and noise handling.

Result: Theoretical analysis shows improved identifiability and error bounds for recovering latent structures, with simulation studies and a real-world application confirming efficiency, accuracy, and interpretability.

Conclusion: Auxiliary covariates enhance latent structure recovery and provide practical benefits in statistical and computational performance. Open-source code is available for reproducibility.

Abstract: The grade of membership model is a flexible latent variable model for analyzing multivariate categorical data through individual-level mixed membership scores. In many modern applications, auxiliary covariates are collected alongside responses and encode information about the same latent structure. Traditional approaches to incorporating such covariates typically rely on fully specified joint likelihoods, which are computationally intensive and sensitive to misspecification. We introduce a covariate-assisted grade of membership model that integrates response and covariate information by exploiting their shared low-rank simplex geometry, rather than modeling their joint distribution. We propose a likelihood-free spectral estimation procedure that combines heterogeneous data sources through a balance parameter controlling their relative contribution. To accommodate high-dimensional and heteroskedastic noise, we employ heteroskedastic principal component analysis before performing simplex-based geometric recovery. Our theoretical analysis establishes weaker identifiability conditions than those required in the covariate-free model, and further derives finite-sample, entrywise error bounds for both mixed membership scores and item parameters. These results demonstrate that auxiliary covariates can provably improve latent structure recovery, yielding faster convergence rates in high-dimensional regimes. Simulation studies and an application to educational assessment data illustrate the computational efficiency, statistical accuracy, and interpretability gains of the proposed method. The code for reproducing these results is open-source and available at \texttt{https://github.com/Toby-X/Covariate-Assisted-GoM}

</details>


### [727] [Contrasting Global and Patient-Specific Regression Models via a Neural Network Representation](https://arxiv.org/abs/2601.18658)
*Max Behrens,Daiana Stolz,Eleni Papakonstantinou,Janis M. Nolde,Gabriele Bellerino,Angelika Rohde,Moritz Hess,Harald Binder*

Main category: stat.ME

TL;DR: This paper discusses a diagnostic tool that distinguishes between global and personalized regression models for clinical predictions, highlighting inadequacies in global models for specific subgroups and offering dimension reduction using autoencoders.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing global regression models valid for all patients with localized models tailored to individuals or subgroups in clinical prediction tasks.

Method: The authors propose a localized regression approach combined with dimension reduction through autoencoders to identify regions where global models are insufficient. The latent representation learned aids in robust and interpretable local regression.

Result: Demonstrated through a study on chronic obstructive pulmonary disease, the global model works well for many patients. However, certain subgroups benefit from personalized models, and their predictors can be identified.

Conclusion: This tool effectively identifies and explains deviations from global models, aiding in improving predictions and understanding for underrepresented patient subgroups.

Abstract: When developing clinical prediction models, it can be challenging to balance between global models that are valid for all patients and personalized models tailored to individuals or potentially unknown subgroups. To aid such decisions, we propose a diagnostic tool for contrasting global regression models and patient-specific (local) regression models. The core utility of this tool is to identify where and for whom a global model may be inadequate. We focus on regression models and specifically suggest a localized regression approach that identifies regions in the predictor space where patients are not well represented by the global model. As localization becomes challenging when dealing with many predictors, we propose modeling in a dimension-reduced latent representation obtained from an autoencoder. Using such a neural network architecture for dimension reduction enables learning a latent representation simultaneously optimized for both good data reconstruction and for revealing local outcome-related associations suitable for robust localized regression. We illustrate the proposed approach with a clinical study involving patients with chronic obstructive pulmonary disease. Our findings indicate that the global model is adequate for most patients but that indeed specific subgroups benefit from personalized models. We also demonstrate how to map these subgroup models back to the original predictors, providing insight into why the global model falls short for these groups. Thus, the principal application and diagnostic yield of our tool is the identification and characterization of patients or subgroups whose outcome associations deviate from the global model.

</details>


### [728] [Learned harmonic mean estimation of the marginal likelihood for multimodal posteriors with flow matching](https://arxiv.org/abs/2601.18683)
*Alicja Polanska,Jason D. McEwen*

Main category: stat.ME

TL;DR: This paper introduces a method to improve Bayesian model comparison by using flow matching-based continuous normalizing flows for density estimation within the learned harmonic mean estimator, handling complex multimodal posteriors effectively.


<details>
  <summary>Details</summary>
Motivation: Bayesian model comparison requires accurate marginal likelihood estimation, which is challenging for complex or high-dimensional models. Existing methods struggle with highly multimodal posteriors.

Method: The proposed approach employs flow matching-based continuous normalizing flows as the internal density estimator for the learned harmonic mean estimator. This allows handling complex multimodal posterior distributions.

Result: The method successfully estimates marginal likelihoods even for highly multimodal posteriors and demonstrates robustness in a 20-dimensional parameter example without requiring fine-tuning.

Conclusion: Flow matching-based architectures enhance the performance of the learned harmonic mean estimator, enabling efficient Bayesian model comparison for difficult problems involving multimodal posteriors.

Abstract: The marginal likelihood, or Bayesian evidence, is a crucial quantity for Bayesian model comparison but its computation can be challenging for complex models, even in parameters space of moderate dimension. The learned harmonic mean estimator has been shown to provide accurate and robust estimates of the marginal likelihood simply using posterior samples. It is agnostic to the sampling strategy, meaning that the samples can be obtained using any method. This enables marginal likelihood calculation and model comparison with whatever sampling is most suitable for the task. However, the internal density estimators considered previously for the learned harmonic mean can struggle with highly multimodal posteriors. In this work we introduce flow matching-based continuous normalizing flows as a powerful architecture for the internal density estimation of the learned harmonic mean. We demonstrate the ability to handle challenging multimodal posteriors, including an example in 20 parameter dimensions, showcasing the method's ability to handle complex posteriors without the need for fine-tuning or heuristic modifications to the base distribution.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [729] [Toward Scalable Normalizing Flows for the Hubbard Model](https://arxiv.org/abs/2601.18273)
*Janik Kreit,Andrea Bulgarelli,Lena Funcke,Thomas Luu,Dominic Schuh,Simran Singh,Lorenzo Verzichelli*

Main category: cond-mat.str-el

TL;DR: The study focuses on extending normalizing flows for the Hubbard model to larger lattice sizes and lower temperatures, improving stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To advance generative modeling applications for condensed matter physics using normalizing flows.

Method: Investigates scalability and stability of normalizing flows and evaluates stochastic flows alongside non-equilibrium Markov chain Monte Carlo.

Result: Presents scaling behavior insights for stochastic normalizing flows and Monte Carlo methods for the fermionic system.

Conclusion: This study provides steps and analysis for optimizing simulations of the Hubbard model under challenging conditions.

Abstract: Normalizing flows have recently demonstrated the ability to learn the Boltzmann distribution of the Hubbard model, opening new avenues for generative modeling in condensed matter physics. In this work, we investigate the steps required to extend such simulations to larger lattice sizes and lower temperatures, with a focus on enhancing stability and efficiency. Additionally, we present the scaling behavior of stochastic normalizing flows and non-equilibrium Markov chain Monte Carlo methods for this fermionic system.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [730] [An autonomous living database for perovskite photovoltaics](https://arxiv.org/abs/2601.17807)
*Sherjeel Shabih,Hampus Näsström,Sharat Patil,Asmin Askin,Keely Dodd-Clements,Jessica Helisa Hautrive Rossato,Hugo Gajardoni de Lemos,Yuxin Liu,Florian Mathies,Natalia Maticiuc,Rico Meitzner,Edgar Nandayapa,Juan José Patiño López,Yaru Wang,Lauri Himanen,Eva Unger,T. Jesper Jacobsson,José A. Márquez,Kevin Maik Jablonka*

Main category: cond-mat.mtrl-sci

TL;DR: This paper introduces PERLA, an autonomous and continually updating database that uses large language models combined with physics-aware validation to efficiently extract and analyze perovskite solar cell data from the literature stream.


<details>
  <summary>Details</summary>
Motivation: To address the knowledge gap caused by exponential publication rates surpassing the capacity of manual curation, particularly in photovoltaics, where critical databases have stagnated despite ongoing research.

Method: The authors developed PERLA, integrating large language models with physics-aware validation, achieving over 90% human-level precision in data extraction from post-2021 literature.

Result: PERLA revealed significant evolutionary trends in perovskite solar cells, highlighting shifts to inverted architectures, self-assembled monolayers, and formamidinium-rich compositions, alongside reductions in voltage loss.

Conclusion: PERLA demonstrates a transformation of static publications into dynamic, self-updating knowledge resources, enhancing data-driven scientific discovery at the pace of new publications.

Abstract: Scientific discovery is severely bottlenecked by the inability of manual curation to keep pace with exponential publication rates. This creates a widening knowledge gap. This is especially stark in photovoltaics, where the leading database for perovskite solar cells has been stagnant since 2021 despite massive ongoing research output. Here, we resolve this challenge by establishing an autonomous, self-updating living database (PERLA). Our pipeline integrates large language models with physics-aware validation to extract complex device data from the continuous literature stream, achieving human-level precision (>90%) and eliminating annotator variance. By employing this system on the previously inaccessible post-2021 literature, we uncover critical evolutionary trends hidden by data lag: the field has decisively shifted toward inverted architectures employing self-assembled monolayers and formamidinium-rich compositions, driving a clear trajectory of sustained voltage loss reduction. PERLA transforms static publications into dynamic knowledge resources that enable data-driven discovery to operate at the speed of publication.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [731] [Symmetric Proofs of Parameterized Programs](https://arxiv.org/abs/2601.18745)
*Ruotong Cheng,Azadeh Farzan*

Main category: cs.LO

TL;DR: The paper explores safety verification for infinite-state parameterized programs using a new proof system that leverages symmetry within program topologies.


<details>
  <summary>Details</summary>
Motivation: The need to verify safety properties in infinite-state parameterized programs formed based on complex topologies, addressing challenges in reuse of proof arguments.

Method: A newly introduced proof system, parametric proof spaces, enables localized symmetry exploitation and algorithmically constructs proofs inspired by model theory, circumventing the need for axiomatization.

Result: Relative completeness is demonstrated for universally quantified invariants with sound verification through an infinitary limit program; algorithm conditions enabling decision procedure are also presented.

Conclusion: A proof system providing systematic verification for parameterized programs without requiring topology axiomatization, with potential decision procedure applicability under certain conditions.

Abstract: We investigate the problem of safety verification of infinite-state parameterized programs that are formed based on a rich class of topologies. We introduce a new proof system, called parametric proof spaces, which exploits the underlying symmetry in such programs. This is a local notion of symmetry which enables the proof system to reuse proof arguments for isomorphic neighbourhoods in program topologies. We prove a sophisticated relative completeness result for the proof system with respect to a class of universally quantified invariants. We also investigate the problem of algorithmic construction of these proofs. We present a construction, inspired by classic results in model theory, where an infinitary limit program can be soundly and completely verified in place of the parameterized family, under some conditions. Furthermore, we demonstrate how these proofs can be constructed and checked against these programs without the need for axiomatization of the underlying topology for proofs or the programs. Finally, we present conditions under which our algorithm becomes a decision procedure.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [732] [PC-MCL: Patient-Consistent Multi-Cycle Learning with multi-label bias correction for respiratory sound classification](https://arxiv.org/abs/2601.17080)
*Seung Gyu Jeong,Seong-Eun Kim*

Main category: eess.AS

TL;DR: Proposes PC-MCL to improve respiratory sound classification by addressing multi-cycle and patient-specific biases.


<details>
  <summary>Details</summary>
Motivation: Current models for respiratory sound classification suffer from cycle-level analysis and patient-specific overfitting.

Method: Introduces multi-cycle concatenation, a 3-label formulation (normal, crackle, wheeze), and a patient-matching auxiliary task.

Result: PC-MCL achieves a score of 65.37% on the ICBHI benchmark, outperforming baselines. Ablation study shows synergy of components.

Conclusion: PC-MCL enhances feature robustness, corrects bias, and improves generalization for better pulmonary disease diagnosis.

Abstract: Automated respiratory sound classification supports the diagnosis of pulmonary diseases. However, many deep models still rely on cycle-level analysis and suffer from patient-specific overfitting. We propose PC-MCL (Patient-Consistent Multi-Cycle Learning) to address these limitations by utilizing three key components: multi-cycle concatenation, a 3-label formulation, and a patient-matching auxiliary task. Our work resolves a multi-label distributional bias in respiratory sound classification, a critical issue inherent to applying multi-cycle concatenation with the conventional 2-label formulation (crackle, wheeze). This bias manifests as a systematic loss of normal signal information when normal and abnormal cycles are combined. Our proposed 3-label formulation (normal, crackle, wheeze) corrects this by preserving information from all constituent cycles in mixed samples. Furthermore, the patient-matching auxiliary task acts as a multi-task regularizer, encouraging the model to learn more robust features and improving generalization. On the ICBHI 2017 benchmark, PC-MCL achieves an ICBHI Score of 65.37%, outperforming existing baselines. Ablation studies confirm that all three components are essential, working synergistically to improve the detection of abnormal respiratory events.

</details>


### [733] [The Voice of Equity: A Systematic Evaluation of Bias Mitigation Techniques for Speech-Based Cognitive Impairment Detection Across Architectures and Demographics](https://arxiv.org/abs/2601.16989)
*Yasaman Haghbin,Sina Rashidi,Ali Zolnour,Maryam Zolnoori*

Main category: eess.AS

TL;DR: This paper proposes a fairness framework for speech-based detection of cognitive impairments, comparing models and bias mitigation strategies across demographic groups.


<details>
  <summary>Details</summary>
Motivation: Ensure equitable speech-based cognitive impairment detection due to potential biases across age, gender, education, and language.

Method: The study introduces two transformer-based architectures and evaluates the effectiveness of pre-, in-, and post-processing fairness mitigation techniques.

Result: Models show strong performance but significant fairness disparities across demographics. Mitigation effectiveness depends on model architecture.

Conclusion: Fairness interventions must be tailored to model architectures and demographic features for equity in cognitive healthcare tools.

Abstract: Speech-based detection of cognitive impairment offers a scalable, non-invasive screening, yet algorithmic bias across demographic and linguistic subgroups remains critically underexplored. We present the first comprehensive fairness analysis framework for speech-based multi-class cognitive impairment detection, systematically evaluating bias mitigation across architectures, and demographic subgroups. We developed two transformer-based architectures, SpeechCARE-AGF and Whisper-LWF-LoRA, on the multilingual NIA PREPARE Challenge dataset. Unlike prior work that typically examines single mitigation techniques, we compared pre-processing, in-processing, and post-processing approaches, assessing fairness via Equality of Opportunity and Equalized Odds across gender, age, education, and language. Both models achieved strong performance (F1: SpeechCARE-AGF 70.87, Whisper-LWF-LoRA 71.46) but exhibited substantial fairness disparities. Adults >=80 showed lower sensitivity versus younger groups; Spanish speakers demonstrated reduced TPR versus English speakers. Mitigation effectiveness varied by architecture: oversampling improved SpeechCARE-AGF for older adults (80+ TPR: 46.19%=>49.97%) but minimally affected Whisper-LWF-LoRA. This study addresses a critical healthcare AI gap by demonstrating that architectural design fundamentally shapes bias patterns and mitigation effectiveness. Adaptive fusion mechanisms enable flexible responses to data interventions, while frequency reweighting offers robust improvements across architectures. Our findings establish that fairness interventions must be tailored to both model architecture and demographic characteristics, providing a systematic framework for developing equitable speech-based screening tools essential for reducing diagnostic disparities in cognitive healthcare.

</details>


### [734] [Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder](https://arxiv.org/abs/2601.18396)
*Zhengyang Li,Thomas Graave,Björn Möller,Zehang Wu,Matthias Franz,Tim Fingscheidt*

Main category: eess.AS

TL;DR: The paper proposes a novel AV-ASR method that integrates visual features in both encoder and decoder of Whisper ASR, achieving state-of-the-art noise robustness.


<details>
  <summary>Details</summary>
Motivation: To enhance noise robustness in audiovisual automatic speech recognition systems by leveraging improved integration of visual features within pre-trained ASR models.

Method: The study introduces a dual-use fusion method that incorporates visual features into both the encoder and decoder of Whisper ASR, alongside comprehensive comparisons, ablation studies, and large-scale audiovisual data fine-tuning.

Result: The proposed method achieves a 35-57% relative improvement in word error rate (WER) under noisy conditions and establishes a new milestone on the LRS3 AV-ASR benchmark with consistent performance improvements in low signal-to-noise scenarios.

Conclusion: Dual-use fusion is an effective approach to enhancing noise robustness in AV-ASR systems, demonstrating superior performance over traditional fusion methods, especially in challenging acoustic conditions.

Abstract: In audiovisual automatic speech recognition (AV-ASR) systems, information fusion of visual features in a pre-trained ASR has been proven as a promising method to improve noise robustness. In this work, based on the prominent Whisper ASR, first, we propose a simple and effective visual fusion method -- use of visual features both in encoder and decoder (dual-use) -- to learn the audiovisual interactions in the encoder and to weigh modalities in the decoder. Second, we compare visual fusion methods in Whisper models of various sizes. Our proposed dual-use method shows consistent noise robustness improvement, e.g., a 35% relative improvement (WER: 4.41% vs. 6.83%) based on Whisper small, and a 57% relative improvement (WER: 4.07% vs. 9.53%) based on Whisper medium, compared to typical reference middle fusion in babble noise with a signal-to-noise ratio (SNR) of 0dB. Third, we conduct ablation studies examining the impact of various module designs and fusion options. Fine-tuned on 1929 hours of audiovisual data, our dual-use method using Whisper medium achieves 4.08% (MUSAN babble noise) and 4.43% (NoiseX babble noise) average WER across various SNRs, thereby establishing a new state-of-the-art in noisy conditions on the LRS3 AV-ASR benchmark. Our code is at https://github.com/ifnspaml/Dual-Use-AVASR

</details>


### [735] [Recovering Performance in Speech Emotion Recognition from Discrete Tokens via Multi-Layer Fusion and Paralinguistic Feature Integration](https://arxiv.org/abs/2601.17085)
*Esther Sun,Abinay Reddy Naini,Carlos Busso*

Main category: eess.AS

TL;DR: The paper investigates discrete tokens for speech emotion recognition (SER) and proposes strategies to recover information lost in the quantization process, closing the performance gap with continuous representations.


<details>
  <summary>Details</summary>
Motivation: Discrete speech tokens are beneficial for storage and language models but suffer from paralinguistic information loss, limiting their effectiveness in SER. The research aims to address this gap.

Method: The study utilizes a fine-tuned WavLM-Large model, k-means quantization, attention-based multi-layer fusion, and integration of openSMILE features. Neural codec tokenizers are also compared and analyzed.

Result: The proposed multi-layer fusion and acoustic feature integration strategies enable discrete tokens to achieve comparable performance to continuous representations in SER tasks.

Conclusion: Discrete tokens, when enhanced with fusion and feature integration strategies, can be effectively utilized for SER without significant performance degradation.

Abstract: Discrete speech tokens offer significant advantages for storage and language model integration, but their application in speech emotion recognition (SER) is limited by paralinguistic information loss during quantization. This paper presents a comprehensive investigation of discrete tokens for SER. Using a fine-tuned WavLM-Large model, we systematically quantify performance degradation across different layer configurations and k-means quantization granularities. To recover the information loss, we propose two key strategies: (1) attention-based multi-layer fusion to recapture complementary information from different layers, and (2) integration of openSMILE features to explicitly reintroduce paralinguistic cues. We also compare mainstream neural codec tokenizers (SpeechTokenizer, DAC, EnCodec) and analyze their behaviors when fused with acoustic features. Our findings demonstrate that through multi-layer fusion and acoustic feature integration, discrete tokens can close the performance gap with continuous representations in SER tasks.

</details>


### [736] [ToS: A Team of Specialists ensemble framework for Stereo Sound Event Localization and Detection with distance estimation in Video](https://arxiv.org/abs/2601.17611)
*Davide Berghi,Philip J. B. Jackson*

Main category: eess.AS

TL;DR: The paper introduces the Team of Specialists (ToS) framework for 3D sound event localization and detection, outperforming existing methods through specialized sub-networks.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the challenge of effectively studying semantic, spatial, and temporal dimensions simultaneously in 3D sound event localization and detection.

Method: The authors propose an ensemble framework (ToS) comprising three specialized sub-networks that focus on different dimension pairs: spatio-lingual, spatio-temporal, and tempo-lingual.

Result: The ToS framework was benchmarked against state-of-the-art audio-visual models, achieving superior performance across key metrics on the DCASE2025 Task 3 Stereo SELD dataset.

Conclusion: The success of ToS highlights the benefit of leveraging specialized sub-networks in tackling complex multimodal tasks, with plans for further refinement in future work.

Abstract: Sound event localization and detection with distance estimation (3D SELD) in video involves identifying active sound events at each time frame while estimating their spatial coordinates. This multimodal task requires joint reasoning across semantic, spatial, and temporal dimensions, a challenge that single models often struggle to address effectively. To tackle this, we introduce the Team of Specialists (ToS) ensemble framework, which integrates three complementary sub-networks: a spatio-linguistic model, a spatio-temporal model, and a tempo-linguistic model. Each sub-network specializes in a unique pair of dimensions, contributing distinct insights to the final prediction, akin to a collaborative team with diverse expertise. ToS has been benchmarked against state-of-the-art audio-visual models for 3D SELD on the DCASE2025 Task 3 Stereo SELD development set, consistently outperforming existing methods across key metrics. Future work will extend this proof of concept by strengthening the specialists with appropriate tasks, training, and pre-training curricula.

</details>


### [737] [SpatialEmb: Extract and Encode Spatial Information for 1-Stage Multi-channel Multi-speaker ASR on Arbitrary Microphone Arrays](https://arxiv.org/abs/2601.18037)
*Yiwen Shao,Yong Xu,Sanjeev Khudanpur,Dong Yu*

Main category: eess.AS

TL;DR: The paper proposes SpatialEmb, a module that directly encodes spatial information for multi-channel multi-speaker ASR, achieving state-of-the-art performance on the AliMeeting dataset.


<details>
  <summary>Details</summary>
Motivation: Existing multi-channel ASR systems have inefficiencies such as dependency on preprocessing modules, reliance on specific microphone settings, and accumulated errors during the pipeline.

Method: The paper introduces SpatialEmb, a lightweight embedding module that encodes spatial information for ASR directly, compatible with both fixed and arbitrary microphone topologies.

Result: SpatialEmb achieves new state-of-the-art performance on AliMeeting data, with CERs of 17.04% and 20.32% on Eval and Test sets respectively, trained on 105-hour data.

Conclusion: SpatialEmb provides a more efficient and adaptable alternative for spatial feature extraction in ASR systems, enhancing both performance and flexibility with superior results on real meeting scenarios.

Abstract: Spatial information is a critical clue for multi-channel multi-speaker target speech recognition. Most state-of-the-art multi-channel Automatic Speech Recognition (ASR) systems extract spatial features only during the speech separation stage, followed by standard single-channel ASR on the separated speech. This approach results in an inefficient, lengthy pipeline and sub-optimal ASR performance due to the accumulated errors from preprocessing modules. Furthermore, most spatial feature extraction methods depend on the knowledge of speaker positions and microphone topology, making the systems reliant on specific settings and challenging to adapt to new equipment. In this work, we propose a solution to these issues with a lightweight embedding module named SpatialEmb, which extracts and encodes spatial information directly for the ASR model, supporting both fixed and arbitrary microphone topology. We conduct comprehensive experiments on AliMeeting, a real meeting corpus, to determine the optimal model design for SpatialEmb in terms of both performance and efficiency. Our best model trained with 105 hours Train-Ali-far achieves 17.04% and 20.32% character error rates (CER) on the Eval and Test sets, establishing a new state-of-the-art result with the same training data.

</details>


### [738] [Learning to Discover: A Generalized Framework for Raga Identification without Forgetting](https://arxiv.org/abs/2601.18766)
*Parampreet Singh,Somya Kumar,Chaitanya Shailendra Nitawe,Vipul Arora*

Main category: eess.AS

TL;DR: The paper addresses the challenge of identifying unseen Ragas in Indian Art Music (IAM) and develops a framework for categorizing both seen and unseen Ragas without forgetting previously learned ones.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from the difficulty of Raga identification in IAM, especially for unseen or rarely performed Ragas, and the limitations of existing classification models that fail in open-world scenarios.

Method: The authors propose a unified learning framework utilizing both labeled and unlabeled audio to discover and categorize unseen Ragas, while avoiding catastrophic forgetting of previously known Ragas.

Result: The model outperforms traditional classifiers and previous methods (NCD-based pipeline) on benchmark datasets by effectively categorizing seen, unseen, and combined Raga classes.

Conclusion: The proposed approach advances representation learning in IAM tasks, offering superior performance in identifying rare and unseen Raga categories, and retaining knowledge of previously known Ragas.

Abstract: Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [739] [To Case or Not to Case: An Empirical Study in Learned Sparse Retrieval](https://arxiv.org/abs/2601.17500)
*Emmanouil Georgios Lionis,Jia-Huei Ju,Angelos Nalmpantis,Casper Thuis,Sean MacAvaney,Andrew Yates*

Main category: cs.IR

TL;DR: The paper investigates the impact of backbone model casing on Learned Sparse Retrieval (LSR) and finds that text lowercasing mitigates performance issues with cased models, enabling their broader applicability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of analysis on the impact of backbone model casing in LSR methods, especially with the shift towards cased versions of state-of-the-art language models.

Method: Systematic evaluation of cased and uncased backbone models across datasets, along with token-level analysis of text pre-processing effects like lowercasing.

Result: Cased backbone models initially underperform compared to uncased ones in LSR tasks, but lowercasing data eliminates this gap and restores performance, making cased models behave akin to uncased ones.

Conclusion: Text lowercasing enables cased backbone models to perform effectively in LSR settings, allowing the integration of stronger, modern cased architectures into sparse retrieval systems.

Abstract: Learned Sparse Retrieval (LSR) methods construct sparse lexical representations of queries and documents that can be efficiently searched using inverted indexes. Existing LSR approaches have relied almost exclusively on uncased backbone models, whose vocabularies exclude case-sensitive distinctions, thereby reducing vocabulary mismatch. However, the most recent state-of-the-art language models are only available in cased versions. Despite this shift, the impact of backbone model casing on LSR has not been studied, potentially posing a risk to the viability of the method going forward. To fill this gap, we systematically evaluate paired cased and uncased versions of the same backbone models across multiple datasets to assess their suitability for LSR. Our findings show that LSR models with cased backbone models by default perform substantially worse than their uncased counterparts; however, this gap can be eliminated by pre-processing the text to lowercase. Moreover, our token-level analysis reveals that, under lowercasing, cased models almost entirely suppress cased vocabulary items and behave effectively as uncased models, explaining their restored performance. This result broadens the applicability of recent cased models to the LSR setting and facilitates the integration of stronger backbone architectures into sparse retrieval. The complete code and implementation for this project are available at: https://github.com/lionisakis/Uncased-vs-cased-models-in-LSR

</details>


### [740] [Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests](https://arxiv.org/abs/2601.17617)
*Jingjie Ning,João Coelho,Yibo Kong,Yunfan Long,Bruno Martins,João Magalhães,Jamie Callan,Chenyan Xiong*

Main category: cs.IR

TL;DR: The paper analyzes 3.97M agentic search sessions, identifying patterns in multi-step queries, intent differences, and evidence reuse, proposing metrics like CTAR and suggesting design improvements.


<details>
  <summary>Details</summary>
Motivation: To understand how LLM-powered search agents perform multi-step information seeking tasks and how retrieved evidence is used.

Method: Performed a large-scale log analysis of 3.97M search sessions via DeepResearchGym, used LLM-based annotations for session intent, introduced CTAR metric to trace evidence reuse.

Result: Identified behavioral patterns—typical session lengths, intent-specific behaviors (fact-seeking vs reasoning tasks), and a 54% rate of evidence reuse in query terms.

Conclusion: Agentic search systems can be enhanced by awareness of repetition, intent-adaptive resource use, and tracking cross-step context. Findings will aid in future improvements and research.

Abstract: LLM-powered search agents are increasingly being used for multi-step information seeking tasks, yet the IR community lacks empirical understanding of how agentic search sessions unfold and how retrieved evidence is used. This paper presents a large-scale log analysis of agentic search based on 14.44M search requests (3.97M sessions) collected from DeepResearchGym, i.e. an open-source search API accessed by external agentic clients. We sessionize the logs, assign session-level intents and step-wise query-reformulation labels using LLM-based annotation, and propose Context-driven Term Adoption Rate (CTAR) to quantify whether newly introduced query terms are traceable to previously retrieved evidence. Our analyses reveal distinctive behavioral patterns. First, over 90% of multi-turn sessions contain at most ten steps, and 89% of inter-step intervals fall under one minute. Second, behavior varies by intent. Fact-seeking sessions exhibit high repetition that increases over time, while sessions requiring reasoning sustain broader exploration. Third, agents reuse evidence across steps. On average, 54% of newly introduced query terms appear in the accumulated evidence context, with contributions from earlier steps beyond the most recent retrieval. The findings suggest that agentic search may benefit from repetition-aware early stopping, intent-adaptive retrieval budgets, and explicit cross-step context tracking. We plan to release the anonymized logs to support future research.

</details>


### [741] [LegalMALR:Multi-Agent Query Understanding and LLM-Based Reranking for Chinese Statute Retrieval](https://arxiv.org/abs/2601.17692)
*Yunhan Li,Mingjie Xie,Gaoli Kang,Zihan Gong,Gengshen Wu,Min Yang*

Main category: cs.IR

TL;DR: LegalMALR is a framework combining a multi-agent system for better query understanding and a large-language-model-based reranker for improved legal statute retrieval.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in legal statute retrieval due to implicit, multi-issue, or colloquial queries often found in real-world scenarios.

Method: LegalMALR integrates a Multi-Agent Query Understanding System (MAS) with reinforcement policy optimization (GRPO) for query refinement and a large-language-model-based reranking module for ranking results.

Result: LegalMALR significantly outperforms existing baselines on in-distribution and out-of-distribution datasets, including the newly constructed CSAID dataset and STARD.

Conclusion: Combining multi-perspective query reformulation, optimization techniques, and advanced reranking using language models enhances the accuracy and applicability of legal statute retrieval systems.

Abstract: Statute retrieval is essential for legal assistance and judicial decision support, yet real-world legal queries are often implicit, multi-issue, and expressed in colloquial or underspecified forms. These characteristics make it difficult for conventional retrieval-augmented generation pipelines to recover the statutory elements required for accurate retrieval. Dense retrievers focus primarily on the literal surface form of the query, whereas lightweight rerankers lack the legal-reasoning capacity needed to assess statutory applicability. We present LegalMALR, a retrieval framework that integrates a Multi-Agent Query Understanding System (MAS) with a zero-shot large-language-model-based reranking module (LLM Reranker). MAS generates diverse, legally grounded reformulations and conducts iterative dense retrieval to broaden candidate coverage. To stabilise the stochastic behaviour of LLM-generated rewrites, we optimise a unified MAS policy using Generalized Reinforcement Policy Optimization(GRPO). The accumulated candidate set is subsequently evaluated by the LLM Reranker, which performs natural-language legal reasoning to produce the final ranking. We further construct CSAID, a dataset of 118 difficult Chinese legal queries annotated with multiple statutory labels, and evaluate LegalMALR on both CSAID and the public STARD benchmark. Experiments show that LegalMALR substantially outperforms strong Retrieval-augmented generation(RAG) baselines in both in-distribution and out-of-distribution settings, demonstrating the effectiveness of combining multi-perspective query interpretation, reinforcement-based policy optimisation, and large-model reranking for statute retrieval.

</details>


### [742] [FinMetaMind: A Tech Blueprint on NLQ Systems for Financial Knowledge Search](https://arxiv.org/abs/2601.17333)
*Lalit Pant,Shivang Nagar*

Main category: cs.IR

TL;DR: This paper introduces a blueprint for modern NLQ systems for financial knowledge search, leveraging cutting-edge techniques like NLP, search engineering, and vector models.


<details>
  <summary>Details</summary>
Motivation: To improve precision, recall, and insights in financial knowledge search by overcoming limitations of traditional methods and addressing intrinsic challenges in financial data retrieval.

Method: The paper employs natural language processing, search engineering, and vector data models, proposing an architecture combining offline indexing and online retrieval tailored to financial information.

Result: The study experimentally validates the proposed architecture and demonstrates its effectiveness in enhanced financial service knowledge retrieval.

Conclusion: The paper provides substantial theoretical and practical evidence for the NLQ system design, discusses future optimizations, and underlines its utility in financial contexts.

Abstract: Natural Language Query (NLQ) allows users to search and interact with information systems using plain, human language instead of structured query syntax. This paper presents a technical blueprint on the design of a modern NLQ system tailored to financial knowledge search. The introduction of NLQ not only enhances the precision and recall of the knowledge search compared to traditional methods, but also facilitates deeper insights by efficiently linking disparate financial objects, events, and relationships. Using core constructs from natural language processing, search engineering, and vector data models, the proposed system aims to address key challenges in discovering, relevance ranking, data freshness, and entity recognition intrinsic to financial data retrieval. In this work, we detail the unique requirements of NLQ for financial datasets and documents, outline the architectural components for offline indexing and online retrieval, and discuss the real-world use cases of enhanced knowledge search in financial services. We delve into the theoretical underpinnings and experimental evidence supporting our proposed architecture, ultimately providing a comprehensive analysis on the subject matter. We also provide a detailed elaboration of our experimental methodology, the data used, the results and future optimizations in this study.

</details>


### [743] [Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval](https://arxiv.org/abs/2601.18747)
*Amir Aavani*

Main category: cs.IR

TL;DR: This paper addresses inefficiencies in modern information retrieval systems when dealing with complex logical queries and proposes a novel evaluation algorithm to make such queries tractable.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the efficiency limitations of current retrieval architectures, which struggle with complex neuro-symbolic workflows due to inefficiencies in executing logical and arithmetic constraints.

Method: The authors define a formal Retrieval Language ($\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and present the 	cComputePN" algorithm for efficient query evaluation using memory-efficient logic.

Result: The proposed 	cComputePN" algorithm ensures computationally efficient execution of any polynomial-time query while overcoming runtime performance and memory consumption issues.

Conclusion: This work lays the groundwork for transforming search indexes into general-purpose computational engines, capable of handling complex queries efficiently.

Abstract: Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.
  In this paper, we propose that a retrieval engine must be capable of ``Capturing $\mathbf{P}$'' -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\mathbf{P}$. We introduce \texttt{ComputePN}, a novel evaluation algorithm that makes $\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative'' response mechanism, \texttt{ComputePN} ensures the efficient evaluation of any query in $\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.

</details>


### [744] [Real-Time Trend Prediction via Continually-Aligned LLM Query Generation](https://arxiv.org/abs/2601.17567)
*Zijing Hui,Wenhan Lyu,Shusen Wang,Li Chen,Chu Wang*

Main category: cs.IR

TL;DR: RTTP is a framework that generates trending search queries from news content, addressing the cold-start issue in low-traffic search environments by leveraging continual learning.


<details>
  <summary>Details</summary>
Motivation: A lack of query volume in low-traffic search settings limits the ability to detect emerging or long-tail trends, as traditional keyword-based methods are too slow in sparse environments.

Method: The RTTP framework generates search queries directly from news content using a continual learning LLM (CL-LLM), enhanced by Mix-Policy DPO for stability and adaptation, aiding in early trend prediction without user queries.

Result: RTTP achieves a +91.4% improvement in tail-trend detection precision@500 and +19% query generation accuracy compared to industry baselines, with stable performance over weeks of online training.

Conclusion: Synthesizing search queries from aligned and continually updated LLMs can effectively address trend detection challenges in sparse search environments, as demonstrated by RTTP's real-world deployment success.

Abstract: Trending news detection in low-traffic search environments faces a fundamental cold-start problem, where a lack of query volume prevents systems from identifying emerging or long-tail trends. Existing methods relying on keyword frequency or query spikes are inherently slow and ineffective in these sparse settings, lagging behind real-world shifts in attention. We introduce RTTP, a novel Real-Time Trending Prediction framework that generates search queries directly from news content instead of waiting for users to issue them. RTTP leverages a continual learning LLM (CL-LLM) that converts posts into search-style queries and scores them using engagement strength + creator authority, enabling early trend surfacing before search volume forms. To ensure adaptation without degrading reasoning, we propose Mix-Policy DPO, a new preference-based continual learning approach that combines on-policy stability with off-policy novelty to mitigate catastrophic forgetting during model upgrades. Deployed at production scale on Facebook and Meta AI products, RTTP delivers +91.4% improvement in tail-trend detection precision@500 and +19% query generation accuracy over industry baselines, while sustaining stable performance after multi-week online training. This work demonstrates that LLM-generated synthetic search signals, when aligned and continually updated, unlock timely trend understanding in low-traffic search environments.

</details>


### [745] [Evaluation on Entity Matching in Recommender Systems](https://arxiv.org/abs/2601.17218)
*Zihan Huang,Rohan Surana,Zhouhang Xie,Junda Wu,Yu Xia,Julian McAuley*

Main category: cs.IR

TL;DR: The paper introduces Reddit-Amazon-EM, a dataset for entity matching between Reddit and Amazon, along with evaluations of various entity matching techniques.


<details>
  <summary>Details</summary>
Motivation: To address the gap in rigorous evaluation frameworks for cross-dataset entity matching, which is crucial for progress in recommender systems.

Method: The authors manually annotated matching entities (movies) across Reddit-Movies and Amazon datasets, and evaluated various state-of-the-art entity matching methods.

Result: They provided an annotated gold set for reproducible research and identified the best-performing method, mapping the datasets for future use.

Conclusion: The provided dataset and evaluations contribute a foundational resource for improving entity matching in recommender systems.

Abstract: Entity matching is a crucial component in various recommender systems, including conversational recommender systems (CRS) and knowledge-based recommender systems. However, the lack of rigorous evaluation frameworks for cross-dataset entity matching impedes progress in areas such as LLM-driven conversational recommendations and knowledge-grounded dataset construction.
  In this paper, we introduce Reddit-Amazon-EM, a novel dataset comprising naturally occurring items from Reddit and the Amazon '23 dataset. Through careful manual annotation, we identify corresponding movies across Reddit-Movies and Amazon'23, two existing recommender system datasets with inherently overlapping catalogs. Leveraging Reddit-Amazon-EM, we conduct a comprehensive evaluation of state-of-the-art entity matching methods, including rule-based, graph-based, lexical-based, embedding-based, and LLM-based approaches.
  For reproducible research, we release our manually annotated entity matching gold set and provide the mapping between the two datasets using the best-performing method from our experiments. This serves as a valuable resource for advancing future work on entity matching in recommender systems.

</details>


### [746] [UniGRec: Unified Generative Recommendation with Soft Identifiers for End-to-End Optimization](https://arxiv.org/abs/2601.17438)
*Jialei Li,Yang Zhang,Yimeng Bai,Shuai Zhu,Ziqi Xue,Xiaoyan Zhao,Dingxian Wang,Frank Yang,Andrew Rabinovich,Xiangnan He*

Main category: cs.IR

TL;DR: The paper introduces UniGRec, a unified framework for generative recommendation that bridges tokenization and recommendation for full end-to-end training, achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in existing generative recommendation methods, which often decouple tokenization from recommendation or use inefficient asynchronous optimization, restricting full end-to-end alignment.

Method: UniGRec integrates tokenizer and recommender with three key techniques: Annealed Inference Alignment for smooth transitions between training and inference, Codeword Uniformity Regularization to ensure codebook diversity, and Dual Collaborative Distillation to distill collaborative priors from a teacher model.

Result: UniGRec outperformed state-of-the-art recommendation approaches in extensive experiments conducted on real-world datasets.

Conclusion: UniGRec effectively resolves challenges in generative recommendation such as training-inference discrepancy, identifier collapse, and collaborative signal deficiency, demonstrating its potential as a superior recommendation solution.

Abstract: Generative recommendation has recently emerged as a transformative paradigm that directly generates target items, surpassing traditional cascaded approaches. It typically involves two components: a tokenizer that learns item identifiers and a recommender trained on them. Existing methods often decouple tokenization from recommendation or rely on asynchronous alternating optimization, limiting full end-to-end alignment. To address this, we unify the tokenizer and recommender under the ultimate recommendation objective via differentiable soft item identifiers, enabling joint end-to-end training. However, this introduces three challenges: training-inference discrepancy due to soft-to-hard mismatch, item identifier collapse from codeword usage imbalance, and collaborative signal deficiency due to an overemphasis on fine-grained token-level semantics.
  To tackle these challenges, we propose UniGRec, a unified generative recommendation framework that addresses them from three perspectives. UniGRec employs Annealed Inference Alignment during tokenization to smoothly bridge soft training and hard inference, a Codeword Uniformity Regularization to prevent identifier collapse and encourage codebook diversity, and a Dual Collaborative Distillation mechanism that distills collaborative priors from a lightweight teacher model to jointly guide both the tokenizer and the recommender. Extensive experiments on real-world datasets demonstrate that UniGRec consistently outperforms state-of-the-art baseline methods. Our codes are available at https://github.com/Jialei-03/UniGRec.

</details>


### [747] [FastInsight: Fast and Insightful Retrieval via Fusion Operators for Graph RAG](https://arxiv.org/abs/2601.18579)
*Seonho An,Chaejeong Hyun,Min-Soo Kim*

Main category: cs.IR

TL;DR: FastInsight is a method to improve retrieval accuracy and generation quality by overcoming limitations in current Graph Retrieval-Augmented Generation (RAG) methods.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in existing Graph RAG methods which fail to balance graph topology and semantic insights, leading to time-consuming processes.

Method: The paper presents FastInsight with two key fusion operators: Graph-based Reranker (GRanker) and Semantic-Topological eXpansion (STeX), combining graph modeling and vector-graph searches for efficient retrieval.

Result: Experiments reveal that FastInsight significantly enhances both retrieval accuracy and generation quality, achieving a better balance of effectiveness and efficiency compared to leading baselines.

Conclusion: FastInsight overcomes the topology and semantics limitations of current RAG approaches, setting new standards in retrieval accuracy and efficiency.

Abstract: Existing Graph RAG methods aiming for insightful retrieval on corpus graphs typically rely on time-intensive processes that interleave Large Language Model (LLM) reasoning. To enable time-efficient insightful retrieval, we propose FastInsight. We first introduce a graph retrieval taxonomy that categorizes existing methods into three fundamental operations: vector search, graph search, and model-based search. Through this taxonomy, we identify two critical limitations in current approaches: the topology-blindness of model-based search and the semantics-blindness of graph search. FastInsight overcomes these limitations by interleaving two novel fusion operators: the Graph-based Reranker (GRanker), which functions as a graph model-based search, and Semantic-Topological eXpansion (STeX), which operates as a vector-graph search. Extensive experiments on broad retrieval and generation datasets demonstrate that FastInsight significantly improves both retrieval accuracy and generation quality compared to state-of-the-art baselines, achieving a substantial Pareto improvement in the trade-off between effectiveness and efficiency.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [748] [Improving Generalization and Uncertainty Quantification of Photometric Redshift Models](https://arxiv.org/abs/2601.17222)
*Jonathan Soriano,Tuan Do,Srinath Saikrishnan,Vikram Seenivasan,Bernie Boscoe,Jack Singal,Evan Jones*

Main category: astro-ph.IM

TL;DR: This paper explores improving photometric redshift predictions using machine learning by combining spectroscopic and photometric redshift datasets using composite training or transfer learning, resulting in better accuracy and applicability.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and applicability of photometric redshift predictions for a wider variety of galaxy types, meeting the requirements of upcoming surveys like LSST.

Method: Utilized deterministic and Bayesian neural networks to compare training on a composite dataset of spectroscopic and photometric redshift data, or using transfer learning. Evaluated methods against LSST requirements, using split conformal prediction for uncertainty calibration.

Result: Composite dataset training reduced bias by 4.5x, scatter by 1.1x, and outlier rates by 1.4x compared to spectroscopic-only models. Bayesian neural networks provided reliable uncertainty predictions but were influenced by ground truth differences.

Conclusion: Using composite datasets improves photometric redshift prediction accuracy and extends applicability to more diverse galaxy types. Bayesian neural networks offer robust uncertainty estimates and can be optimized further for next-gen surveys.

Abstract: Accurate redshift estimates are a vital component in understanding galaxy evolution and precision cosmology. In this paper, we explore approaches to increase the applicability of machine learning models for photometric redshift estimation on a broader range of galaxy types. Typical models are trained with ground-truth redshifts from spectroscopy. We test the utility and effectiveness of two approaches for combining spectroscopic redshifts and redshifts derived from multiband ($\sim$35 filters) photometry, which sample different types of galaxies compared to spectroscopic surveys. The two approaches are (1) training on a composite dataset and (2) transfer learning from one dataset to another. We compile photometric redshifts from the COSMOS2020 catalog (TransferZ) to complement an established spectroscopic redshift dataset (GalaxiesML). We used two architectures, deterministic neural networks (NN) and Bayesian neural networks (BNN), to examine and evaluate their performance with respect to the Legacy Survey of Space and Time (LSST) photo-$z$ science requirements. We also use split conformal prediction for calibrating uncertainty estimates and producing prediction intervals for the BNN and NN, respectively. We find that a NN trained on a composite dataset predicts photo-$z$'s that are 4.5 times less biased within the redshift range $0.3<z<1.5$, 1.1 times less scattered, and has a 1.4 times lower outlier rate than a model trained on only spectroscopic ground truths. We also find that BNNs produce reliable uncertainty estimates, but are sensitive to the different ground truths. This investigation leverages different sources of ground truths to develop models that can accurately predict photo-$z$'s for a broader population of galaxies crucial for surveys such as Euclid and LSST.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [749] [AI-based System for Transforming text and sound to Educational Videos](https://arxiv.org/abs/2601.17022)
*M. E. ElAlami,S. M. Khater,M. El. R. Rehan*

Main category: cs.MM

TL;DR: The paper presents a method using Generative Adversarial Networks (GANs) to generate educational videos from text or speech inputs, achieving notable quality improvement compared to existing systems.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to efficiently generate educational videos from text or speech inputs, highlighting the need for more effective approaches using cutting-edge techniques.

Method: The system uses a three-phase approach: transcription of speech/text inputs, extraction and generation of relevant images using CLIP and diffusion models, and synthesis of these images into videos with integrated sound.

Result: The system achieves a Fréchet Inception Distance (FID) score of 28.75%, demonstrating improved visual quality and better performance compared to existing methods like TGAN, MoCoGAN, and TGANS-C.

Conclusion: The proposed GAN-based framework successfully generates high-quality educational videos and offers significant improvements over traditional methods.

Abstract: Technological developments have produced methods that can generate educational videos from input text or sound. Recently, the use of deep learning techniques for image and video generation has been widely explored, particularly in education. However, generating video content from conditional inputs such as text or speech remains a challenging area. In this paper, we introduce a novel method to the educational structure, Generative Adversarial Network (GAN), which develop frame-for-frame frameworks and are able to create full educational videos. The proposed system is structured into three main phases In the first phase, the input (either text or speech) is transcribed using speech recognition. In the second phase, key terms are extracted and relevant images are generated using advanced models such as CLIP and diffusion models to enhance visual quality and semantic alignment. In the final phase, the generated images are synthesized into a video format, integrated with either pre-recorded or synthesized sound, resulting in a fully interactive educational video. The proposed system is compared with other systems such as TGAN, MoCoGAN, and TGANS-C, achieving a Fréchet Inception Distance (FID) score of 28.75%, which indicates improved visual quality and better over existing methods.

</details>


### [750] [Integrating Fine-Grained Audio-Visual Evidence for Robust Multimodal Emotion Reasoning](https://arxiv.org/abs/2601.18321)
*Zhixian Zhao,Wenjie Tian,Xiaohai Tian,Jun Zhang,Lei Xie*

Main category: cs.MM

TL;DR: The paper introduces SABER-LLM, a framework for robust multimodal emotion reasoning, addressing limitations in current MLLMs regarding fine-grained perception and cross-modal fusion.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with fine-grained perception and multimodal fusion, leading to inaccuracies in decoding complex emotional dynamics, especially when cues are subtle or conflicting.

Method: The study presents SABER-LLM, featuring a novel emotion reasoning dataset (SABER with 600K annotated clips) and a structured evidence decomposition approach to separate perception and reasoning. Consistency-aware preference optimization enhances alignment across modalities.

Result: SABER-LLM outperforms open-source baselines and rivals closed-source models in robustness for decoding intricate emotional dynamics, validated on multiple benchmarks and tests.

Conclusion: SABER-LLM advances multimodal emotion reasoning with improved robustness and accuracy, tackling limitations in current models with its innovative data and method approach.

Abstract: Multimodal emotion analysis is shifting from static classification to generative reasoning. Beyond simple label prediction, robust affective reasoning must synthesize fine-grained signals such as facial micro-expressions and prosodic which shifts to decode the latent causality within complex social contexts. However, current Multimodal Large Language Models (MLLMs) face significant limitations in fine-grained perception, primarily due to data scarcity and insufficient cross-modal fusion. As a result, these models often exhibit unimodal dominance which leads to hallucinations in complex multimodal interactions, particularly when visual and acoustic cues are subtle, ambiguous, or even contradictory (e.g., in sarcastic scenery). To address this, we introduce SABER-LLM, a framework designed for robust multimodal reasoning. First, we construct SABER, a large-scale emotion reasoning dataset comprising 600K video clips, annotated with a novel six-dimensional schema that jointly captures audiovisual cues and causal logic. Second, we propose the structured evidence decomposition paradigm, which enforces a "perceive-then-reason" separation between evidence extraction and reasoning to alleviate unimodal dominance. The ability to perceive complex scenes is further reinforced by consistency-aware direct preference optimization, which explicitly encourages alignment among modalities under ambiguous or conflicting perceptual conditions. Experiments on EMER, EmoBench-M, and SABER-Test demonstrate that SABER-LLM significantly outperforms open-source baselines and achieves robustness competitive with closed-source models in decoding complex emotional dynamics. The dataset and model are available at https://github.com/zxzhao0/SABER-LLM.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [751] [MarketGANs: Multivariate financial time-series data augmentation using generative adversarial networks](https://arxiv.org/abs/2601.17773)
*Jeonggyu Huh,Seungwon Jeong,Hyun-Gyoon Kim,Hyeng Keun Koo,Byung Hwa Lim*

Main category: q-fin.ST

TL;DR: The paper introduces MarketGAN, a generative framework integrating factor-pricing structures to improve high-dimensional asset return generation under scarce data conditions.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the challenge of high-dimensional asset return generation while preserving economic realities, especially under data scarcity.

Method: MarketGAN uses generative adversarial learning with a temporal convolutional network, incorporating factor-pricing structures to replicate key financial statistical behaviors.

Result: MarketGAN accurately replicates asset return characteristics like heavy tails, volatility clustering, and cross-sectional correlation, outperforming traditional factor-based methods.

Conclusion: MarketGAN offers better performance in financial modeling and portfolio applications, highlighting its superiority in statistical and practical use cases.

Abstract: This paper introduces MarketGAN, a factor-based generative framework for high-dimensional asset return generation under severe data scarcity. We embed an explicit asset-pricing factor structure as an economic inductive bias and generate returns as a single joint vector, thereby preserving cross-sectional dependence and tail co-movement alongside inter-temporal dynamics. MarketGAN employs generative adversarial learning with a temporal convolutional network (TCN) backbone, which models stochastic, time-varying factor loadings and volatilities and captures long-range temporal dependence. Using daily returns of large U.S. equities, we find that MarketGAN more closely matches empirical stylized facts of asset returns, including heavy-tailed marginal distributions, volatility clustering, leverage effects, and, most notably, high-dimensional cross-sectional correlation structures and tail co-movement across assets, than conventional factor-model-based bootstrap approaches. In portfolio applications, covariance estimates derived from MarketGAN-generated samples outperform those derived from other methods when factor information is at least weakly informative, demonstrating tangible economic value.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [752] [Regret-Driven Portfolios: LLM-Guided Smart Clustering for Optimal Allocation](https://arxiv.org/abs/2601.17021)
*Muhammad Abro,Hassan Jaleel*

Main category: q-fin.PM

TL;DR: The paper introduces a portfolio allocation framework using online learning, sentiment indicators, and LLM-driven hedging, outperforming traditional methods in returns and Sharpe ratio.


<details>
  <summary>Details</summary>
Motivation: Address the tradeoff between risk and return in medium- to long-term portfolio management using advanced techniques.

Method: Integrates LLM-guided no-regret allocation, sentiment trade filtering, and downside protection into a follow-the-leader approach.

Result: Achieves 69% higher annualized returns and 119% higher Sharpe ratio compared to SPY buy-and-hold baseline.

Conclusion: The framework offers significant improvements for risk-averse investors when constructing portfolios, leveraging modern learning and AI approaches.

Abstract: We attempt to mitigate the persistent tradeoff between risk and return in medium- to long-term portfolio management. This paper proposes a novel LLM-guided no-regret portfolio allocation framework that integrates online learning dynamics, market sentiment indicators, and large language model (LLM)-based hedging to construct high-Sharpe ratio portfolios tailored for risk-averse investors and institutional fund managers. Our approach builds on a follow-the-leader approach, enriched with sentiment-based trade filtering and LLM-driven downside protection. Empirical results demonstrate that our method outperforms a SPY buy-and-hold baseline by 69% in annualized returns and 119% in Sharpe ratio.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [753] [Uniform Computability of PAC Learning](https://arxiv.org/abs/2601.18663)
*Vasco Brattka,Guillaume Chirache*

Main category: math.LO

TL;DR: This paper explores the computability properties of PAC learning using Weihrauch complexity, analyzing various information settings and their implications on learning and complexity.


<details>
  <summary>Details</summary>
Motivation: To investigate the uniform computability properties of PAC learning and classify computational challenges in this domain using Weihrauch complexity.

Method: The authors analyzed PAC learning under settings with closed concept classes represented by positive, negative, or full information. They studied constructs like VC dimension and relationships with operations such as limit operations, Weak König's Lemma, and sorting complexity.

Result: Key results show equivalences between PAC learning scenarios and computational principles: proper learning relates to limit operations, improper learning relates to Weak König's Lemma or non-deterministic computability depending on the information type. The complexity of the VC dimension operation is classified in scenarios of positive, negative, and full information.

Conclusion: These findings highlight the constructivity of the Fundamental Theorem of Statistical Learning and its dependence on provided information, refining our understanding of PAC learnability and its computational characteristics.

Abstract: We study uniform computability properties of PAC learning using Weihrauch complexity. We focus on closed concept classes, which are either represented by positive, by negative or by full information. Among other results, we prove that proper PAC learning from positive information is equivalent to the limit operation on Baire space, whereas improper PAC learning from positive information is closely related to Weak Kőnig's Lemma and even equivalent to it, when we have some negative information about the admissible hypotheses. If arbitrary hypotheses are allowed, then improper PAC learning from positive information is still in a finitary DNC range, which implies that it is non-deterministically computable, but does not allow for probabilistic algorithms. These results can also be seen as a classification of the degree of constructivity of the Fundamental Theorem of Statistical Learning. All the aforementioned results hold if an upper bound of the VC dimension is provided as an additional input information. We also study the question of how these results are affected if the VC dimension is not given, but only promised to be finite or if concept classes are represented by negative or full information. Finally, we also classify the complexity of the VC dimension operation itself, which is a problem that is of independent interest. For positive or full information it turns out to be equivalent to the binary sorting problem, for negative information it is equivalent to the jump of sorting. This classification allows also conclusions regarding the Borel complexity of PAC learnability.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [754] [Optimal Use of Preferences in Artificial Intelligence Algorithms](https://arxiv.org/abs/2601.18732)
*Joshua S. Gans*

Main category: econ.TH

TL;DR: The paper explores conditions under which it is optimal to separate preference-free training from post-processing decisions, offering theoretical insights into modular AI pipelines in machine learning.


<details>
  <summary>Details</summary>
Motivation: The motivation was to address under what conditions a preference-free training approach, followed by post-processing with preferences, is optimal across decision problems.

Method: The authors analyze decision problems using information design methods and focus on embedding diminishing-value-of-information principles.

Result: Applying preferences at the post-processing stage is shown to weakly dominate preference embedding during training for utility-based decisions, except when decision-stage barriers exist.

Conclusion: Preference-free training is found to be generally superior for versatility and adaptability in decision-making, but preference embedding may be beneficial when cognitive constraints hinder optimal downstream decisions.

Abstract: Machine learning systems embed preferences either in training losses or through post-processing of calibrated predictions. Applying information design methods from Strack and Yang (2024), this paper provides decision problem agnostic conditions under which separation training preference free and applying preferences ex post is optimal. Unlike prior work that requires specifying downstream objectives, the welfare results here apply uniformly across decision problems. The key primitive is a diminishing-value-of-information condition: relative to a fixed (normalised) preference-free loss, preference embedding makes informativeness less valuable at the margin, inducing a mean-preserving contraction of learned posteriors. Because the value of information is convex in beliefs, preference-free training weakly dominates for any expected utility decision problem. This provides theoretical foundations for modular AI pipelines that learn calibrated probabilities and implement asymmetric costs through downstream decision rules. However, separation requires users to implement optimal decision rules. When cognitive constraints bind, as documented in human AI decision-making, preference embedding can dominate by automating threshold computation. These results provide design guidance: preserve optionality through post-processing when objectives may shift; embed preferences when decision-stage frictions dominate.

</details>
