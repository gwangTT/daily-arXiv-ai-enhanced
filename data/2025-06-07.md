<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 9]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.SE](#cs.SE) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [stat.ML](#stat.ML) [Total: 7]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245)
*Guangchen Lan,Huseyin A. Inan,Sahar Abdelnabi,Janardhan Kulkarni,Lukas Wutschitz,Reza Shokri,Christopher G. Brinton,Robert Sim*

Main category: cs.AI

TL;DR: The paper explores how autonomous agents can ensure contextual integrity (CI) by reasoning about their operating context, focusing on minimizing inappropriate information disclosure.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of ensuring appropriate information disclosure by autonomous agents, which is critical for user trust and contextual integrity in decision-making.

Method: The authors test reasoning-based approaches using LLMs and build an RL framework, coupled with a synthetic dataset, to teach agents how to disclose information appropriately in different contexts.

Result: The method significantly improves performance in reducing inappropriate information disclosure while maintaining effectiveness. This progress translates well to external CI benchmarks like PrivacyLens.

Conclusion: Reasoning about context is crucial for enhancing contextual integrity in autonomous agents, and the proposed RL approach effectively achieves this goal.

Abstract: As the era of autonomous agents making decisions on behalf of users unfolds,
ensuring contextual integrity (CI) -- what is the appropriate information to
share while carrying out a certain task -- becomes a central question to the
field. We posit that CI demands a form of reasoning where the agent needs to
reason about the context in which it is operating. To test this, we first
prompt LLMs to reason explicitly about CI when deciding what information to
disclose. We then extend this approach by developing a reinforcement learning
(RL) framework that further instills in models the reasoning necessary to
achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$
examples but with diverse contexts and information disclosure norms, we show
that our method substantially reduces inappropriate information disclosure
while maintaining task performance across multiple model sizes and families.
Importantly, improvements transfer from this synthetic dataset to established
CI benchmarks such as PrivacyLens that has human annotations and evaluates
privacy leakage of AI assistants in actions and tool calls.

</details>


### [2] [Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation](https://arxiv.org/abs/2506.04251)
*Zhengyang Li*

Main category: cs.AI

TL;DR: This paper introduces the LLM-MARL framework to enhance multi-agent reinforcement learning using large language models, showing increased performance and generalization in simulated games.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between language modeling and policy learning, aiming to improve coordination and communication in multi-agent systems.

Method: The framework includes modular components—Coordinator, Communicator, and Memory—for subgoal generation, symbolic messaging, and episodic recall, combining PPO training with language-conditioned losses and LLM query gating.

Result: LLM-MARL demonstrates improved win rates, coordination scores, and zero-shot generalization across tested environments, outperforming baseline methods like MAPPO and QMIX.

Conclusion: This framework highlights the potential of LLMs to enhance multi-agent interaction, offering advancements in AI systems for games, training, and human-AI collaboration.

Abstract: This paper introduces LLM-MARL, a unified framework that incorporates large
language models (LLMs) into multi-agent reinforcement learning (MARL) to
enhance coordination, communication, and generalization in simulated game
environments. The framework features three modular components of Coordinator,
Communicator, and Memory, which dynamically generate subgoals, facilitate
symbolic inter-agent messaging, and support episodic recall. Training combines
PPO with a language-conditioned loss and LLM query gating. LLM-MARL is
evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results
show consistent improvements over MAPPO and QMIX in win rate, coordination
score, and zero-shot generalization. Ablation studies demonstrate that subgoal
generation and language-based messaging each contribute significantly to
performance gains. Qualitative analysis reveals emergent behaviors such as role
specialization and communication-driven tactics. By bridging language modeling
and policy learning, this work contributes to the design of intelligent,
cooperative agents in interactive simulations. It offers a path forward for
leveraging LLMs in multi-agent systems used for training, games, and human-AI
collaboration.

</details>


### [3] [A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy](https://arxiv.org/abs/2506.04252)
*Yang Zhao,Chengxiao Dai,Dusit Niyato,Chuan Fu Tan,Keyi Xiang,Yueyang Wang,Zhiquan Yeo,Daren Tan Zong Loong,Jonathan Low Zhaozhi,Eugene H. Z. HO*

Main category: cs.AI

TL;DR: CircuGraphRAG enhances LLM accuracy by anchoring outputs in a knowledge graph for circular economy, improving question answering and efficiency.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate data critical to sustainable manufacturing decisions, thus needing a grounded and reliable mechanism.

Method: A Retrieval-Augmented Generation (RAG) framework leveraging a domain-specific knowledge graph, translating natural language queries into SPARQL and retrieving verified subgraphs.

Result: CircuGraphRAG outperforms baselines in question answering (ROUGE-L F1 up to 1.0), halves response time, and reduces token usage by 16%.

Conclusion: CircuGraphRAG provides reliable, traceable support for circular economy decisions, aiding sustainable manufacturing.

Abstract: Large language models (LLMs) hold promise for sustainable manufacturing, but
often hallucinate industrial codes and emission factors, undermining regulatory
and investment decisions. We introduce CircuGraphRAG, a retrieval-augmented
generation (RAG) framework that grounds LLMs outputs in a domain-specific
knowledge graph for the circular economy. This graph connects 117,380
industrial and waste entities with classification codes and GWP100 emission
data, enabling structured multi-hop reasoning. Natural language queries are
translated into SPARQL and verified subgraphs are retrieved to ensure accuracy
and traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG
achieves superior performance in single-hop and multi-hop question answering,
with ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also
improves efficiency, halving the response time and reducing token usage by 16%
in representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready
support for circular economy planning, advancing reliable, low-carbon resource
decision making.

</details>


### [4] [HADA: Human-AI Agent Decision Alignment Architecture](https://arxiv.org/abs/2506.04253)
*Tapio Pitkäranta,Leena Pitkäranta*

Main category: cs.AI

TL;DR: HADA introduces a framework to ensure alignment of AI and legacy systems with organizational values by enabling conversational APIs and stakeholder-specific agents.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in aligning AI and legacy algorithms with organizational values, ensuring ethical and transparent decisions.

Method: HADA uses stakeholder-specific agents, conversational APIs, and natural language expression of objectives, deployed on Docker/Kubernetes/Python for real-world testing.

Result: A proof of concept demonstrated alignment across multiple use cases with enhanced control, transparency, and bias mitigation.

Conclusion: HADA effectively aligns decision pipelines with ethical and organizational standards, improving transparency and compliance.

Abstract: We present HADA (Human-AI Agent Decision Alignment), a protocol- and
framework agnostic reference architecture that keeps both large language model
(LLM) agents and legacy algorithms aligned with organizational targets and
values. HADA wraps any algorithm or LLM in role-specific stakeholder agents --
business, data-science, audit, ethics, and customer -- each exposing
conversational APIs so that technical and non-technical actors can query,
steer, audit, or contest every decision across strategic, tactical, and
real-time horizons. Alignment objectives, KPIs, and value constraints are
expressed in natural language and are continuously propagated, logged, and
versioned while thousands of heterogeneous agents run on different
orchestration stacks. A cloud-native proof of concept packages a production
credit-scoring model (getLoanDecision) and deploys it on
Docker/Kubernetes/Python; five scripted retail-bank scenarios show how target
changes, parameter tweaks, explanation requests, and ethics triggers flow end
to end through the architecture. Evaluation followed the Design-Science
Research Methodology. Walkthrough observation and log inspection demonstrated
complete coverage of six predefined objectives: every role could invoke
conversational control, trace KPIs and value constraints, detect and mitigate
ZIP-code bias, and reproduce full decision lineage, independent of the
underlying LLM or agent library. Contributions: (1) an open-source HADA
architecture, (2) a mid-range design theory for human-AI alignment in
multi-agent systems, and (3) empirical evidence that framework-agnostic,
protocol-compliant stakeholder agents improve accuracy, transparency, and
ethical compliance in real-world decision pipelines.

</details>


### [5] [Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback](https://arxiv.org/abs/2506.04287)
*Yongjin Yang,Sinjae Kang,Juyong Lee,Dongjun Lee,Se-Young Yun,Kimin Lee*

Main category: cs.AI

TL;DR: The EXIF framework enables automatic skill discovery for agents powered by large language models (LLMs), utilizing exploration agents to generate grounded datasets and an iterative feedback loop, eliminating human intervention and achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Training Large Language Model (LLM) agents in diverse environments to acquire skills without manual effort presents challenges such as infeasible task proposals and minimal learning signals from generated data.

Method: The paper proposes EXIF, which uses an exploration agent (Alice) to interact with the environment and produce a skill dataset for another agent (Bob). It incorporates an iterative feedback loop where Alice evaluates Bob's performance and refines exploration for better data generation.

Result: EXIF demonstrates effective skill discovery and capability expansion in two tested environments, Webshop and Crafter, without human intervention, achieving notable performance improvements.

Conclusion: EXIF validates the potential for automating skill discovery and evolving LLM-powered agents' capabilities in open-ended environments, highlighting the system's capacity for self-improvement.

Abstract: Training large language model (LLM) agents to acquire necessary skills and
perform diverse tasks within an environment is gaining interest as a means to
enable open-endedness. However, creating the training dataset for their skill
acquisition faces several challenges. Manual trajectory collection requires
significant human effort. Another approach, where LLMs directly propose tasks
to learn, is often invalid, as the LLMs lack knowledge of which tasks are
actually feasible. Moreover, the generated data may not provide a meaningful
learning signal, as agents often already perform well on the proposed tasks. To
address this, we propose a novel automatic skill discovery framework EXIF for
LLM-powered agents, designed to improve the feasibility of generated target
behaviors while accounting for the agents' capabilities. Our method adopts an
exploration-first strategy by employing an exploration agent (Alice) to train
the target agent (Bob) to learn essential skills in the environment.
Specifically, Alice first interacts with the environment to retrospectively
generate a feasible, environment-grounded skill dataset, which is then used to
train Bob. Crucially, we incorporate an iterative feedback loop, where Alice
evaluates Bob's performance to identify areas for improvement. This feedback
then guides Alice's next round of exploration, forming a closed-loop data
generation process. Experiments on Webshop and Crafter demonstrate EXIF's
ability to effectively discover meaningful skills and iteratively expand the
capabilities of the trained agent without any human intervention, achieving
substantial performance improvements. Interestingly, we observe that setting
Alice to the same model as Bob also notably improves performance, demonstrating
EXIF's potential for building a self-evolving system.

</details>


### [6] [A Statistical Physics of Language Model Reasoning](https://arxiv.org/abs/2506.04374)
*Jack David Carson,Amir Reisizadeh*

Main category: cs.AI

TL;DR: The paper introduces a statistical physics approach to model transformer language models' reasoning dynamics using stochastic systems on a lower-dimensional manifold.


<details>
  <summary>Details</summary>
Motivation: Understanding the emergent reasoning behavior in transformer language models, which lacks a clear mechanistic explanation.

Method: The reasoning dynamics are modeled as drift-diffusion systems using latent regime switching and a stochastic linear dynamical system (SLDS). Empirical data from 8 models and 7 benchmarks validate this approach.

Result: The study identifies four latent reasoning regimes and shows that a rank-40 projection explains approximately 50% of the variance, enabling analysis of transitions like LM failures.

Conclusion: The proposed framework provides tools to simulate reasoning transitions and study critical states in language models at low computational cost.

Abstract: Transformer LMs show emergent reasoning that resists mechanistic
understanding. We offer a statistical physics framework for continuous-time
chain-of-thought reasoning dynamics. We model sentence-level hidden state
trajectories as a stochastic dynamical system on a lower-dimensional manifold.
This drift-diffusion system uses latent regime switching to capture diverse
reasoning phases, including misaligned states or failures. Empirical
trajectories (8 models, 7 benchmarks) show a rank-40 projection (balancing
variance capture and feasibility) explains ~50% variance. We find four latent
reasoning regimes. An SLDS model is formulated and validated to capture these
features. The framework enables low-cost reasoning simulation, offering tools
to study and predict critical transitions like misaligned states or other LM
failures.

</details>


### [7] [Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science](https://arxiv.org/abs/2506.04410)
*Peter Jansen,Samiah Hassan,Ruoyao Wang*

Main category: cs.AI

TL;DR: The paper introduces Matter-of-Fact, a benchmark dataset for evaluating the feasibility of scientific hypotheses.


<details>
  <summary>Details</summary>
Motivation: To enhance automated scientific discovery systems by filtering hypotheses based on feasibility, reducing the cost of running large-scale experiments and increasing the likelihood of significant discoveries.

Method: The authors created Matter-of-Fact, a dataset containing 8.4k claims (qualitative and quantitative) from materials science literature across four domains, and evaluated current models like retrieval-augmented generation and code generation on their feasibility-determination performance.

Result: Current strong baselines achieve up to 72% performance on the task, indicating significant room for improvement in hypothesis feasibility evaluation.

Conclusion: The difficulty of the task for existing models demonstrates the need for advancements, and progress in this area could offer substantial improvements in the efficiency of scientific discovery.

Abstract: Contemporary approaches to assisted scientific discovery use language models
to automatically generate large numbers of potential hypothesis to test, while
also automatically generating code-based experiments to test those hypotheses.
While hypotheses can be comparatively inexpensive to generate, automated
experiments can be costly, particularly when run at scale (i.e. thousands of
experiments). Developing the capacity to filter hypotheses based on their
feasibility would allow discovery systems to run at scale, while increasing
their likelihood of making significant discoveries. In this work we introduce
Matter-of-Fact, a challenge dataset for determining the feasibility of
hypotheses framed as claims. Matter-of-Fact includes 8.4k claims extracted from
scientific articles spanning four high-impact contemporary materials science
topics, including superconductors, semiconductors, batteries, and aerospace
materials, while including qualitative and quantitative claims from
theoretical, experimental, and code/simulation results. We show that strong
baselines that include retrieval augmented generation over scientific
literature and code generation fail to exceed 72% performance on this task
(chance performance is 50%), while domain-expert verification suggests nearly
all are solvable -- highlighting both the difficulty of this task for current
models, and the potential to accelerate scientific discovery by making
near-term progress.

</details>


### [8] [Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance](https://arxiv.org/abs/2506.04427)
*Xixi Wang,Miguel Costa,Jordanka Kovaceva,Shuai Wang,Francisco C. Pereira*

Main category: cs.AI

TL;DR: This paper proposes a graph-based method to improve multi-table question answering (QA) by encoding schema links and join paths, addressing the challenges in complex tabular datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing multi-table QA methods that fail to handle the complexity and diversity of columns in real-world tabular data.

Method: The authors developed a graph-based framework that incorporates relational knowledge to explicitly encode schema links and join paths. It includes techniques like reasoning chain construction, pruning, and sub-path merging to enhance performance.

Result: Experiments on standard benchmarks and a large-scale industrial dataset show that the proposed method is effective for multi-table QA in complex scenarios.

Conclusion: This paper presents the first multi-table QA framework designed for highly complex industrial tabular data, offering improved reasoning and interpretability over prior approaches.

Abstract: Large language models (LLMs) have shown promise in table Question Answering
(Table QA). However, extending these capabilities to multi-table QA remains
challenging due to unreliable schema linking across complex tables. Existing
methods based on semantic similarity work well only on simplified hand-crafted
datasets and struggle to handle complex, real-world scenarios with numerous and
diverse columns. To address this, we propose a graph-based framework that
leverages human-curated relational knowledge to explicitly encode schema links
and join paths. Given a natural language query, our method searches this graph
to construct interpretable reasoning chains, aided by pruning and sub-path
merging strategies to enhance efficiency and coherence. Experiments on both
standard benchmarks and a realistic, large-scale dataset demonstrate the
effectiveness of our approach. To our knowledge, this is the first multi-table
QA system applied to truly complex industrial tabular data.

</details>


### [9] [An AI-Based Public Health Data Monitoring System](https://arxiv.org/abs/2506.04429)
*Ananya Joshi,Nolan Gormley,Richa Gadgil,Tina Townes,Roni Rosenfeld,Bryan Wilder*

Main category: cs.AI

TL;DR: The study explores a ranking-based AI anomaly detection system to improve the monitoring of large-scale public health data, achieving stronger efficiency than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current alert-based monitoring systems for public health data face scalability and timeliness challenges when processing large amounts of health information.

Method: The paper introduces a human-centered, AI-driven ranking-based anomaly detection system for monitoring public health data effectively and efficiently.

Result: The system showed significant improvement, with a 54x increase in reviewer speed efficiency during the three-month evaluation period, compared to traditional approaches.

Conclusion: The ranking-based AI anomaly detection method successfully transforms public health decision-making, demonstrating the effectiveness of human-centered AI solutions in health monitoring tasks.

Abstract: Public health experts need scalable approaches to monitor large volumes of
health data (e.g., cases, hospitalizations, deaths) for outbreaks or data
quality issues. Traditional alert-based monitoring systems struggle with modern
public health data monitoring systems for several reasons, including that
alerting thresholds need to be constantly reset and the data volumes may cause
application lag. Instead, we propose a ranking-based monitoring paradigm that
leverages new AI anomaly detection methods. Through a multi-year
interdisciplinary collaboration, the resulting system has been deployed at a
national organization to monitor up to 5,000,000 data points daily. A
three-month longitudinal deployed evaluation revealed a significant improvement
in monitoring objectives, with a 54x increase in reviewer speed efficiency
compared to traditional alert-based methods. This work highlights the potential
of human-centered AI to transform public health decision-making.

</details>


### [10] [Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences](https://arxiv.org/abs/2506.04478)
*Hadi Hosseini,Samarth Khanna,Ronak Singh*

Main category: cs.AI

TL;DR: This paper explores large language models' (LLMs) limitations in handling ranked preferences in matching markets, with several reasoning tasks highlighting logical and algorithmic gaps.


<details>
  <summary>Details</summary>
Motivation: To understand and evaluate how well LLMs can handle ranked preferences and algorithms within combinatorial domains like matching markets.

Method: The researchers tested state-of-the-art LLMs on a hierarchy of preference-based reasoning tasks such as stable-matching generation, instability detection and resolution, and fine-grained preference queries.

Result: LLMs perform poorly in resolving instabilities in large matching markets. Fine-tuning methods like LoRA improve performance in small markets, but not in larger contexts.

Conclusion: Advanced strategies are needed to improve LLMs' reasoning over ranked preferences and handling larger-context scenarios.

Abstract: The rise of Large Language Models (LLMs) has driven progress in reasoning
tasks -- from program synthesis to scientific hypothesis generation -- yet
their ability to handle ranked preferences and structured algorithms in
combinatorial domains remains underexplored. We study matching markets, a core
framework behind applications like resource allocation and ride-sharing, which
require reconciling individual ranked preferences to ensure stable outcomes. We
evaluate several state-of-the-art models on a hierarchy of preference-based
reasoning tasks -- ranging from stable-matching generation to instability
detection, instability resolution, and fine-grained preference queries -- to
systematically expose their logical and algorithmic limitations in handling
ranked inputs. Surprisingly, even top-performing models with advanced reasoning
struggle to resolve instability in large markets, often failing to identify
blocking pairs or execute algorithms iteratively. We further show that
parameter-efficient fine-tuning (LoRA) significantly improves performance in
small markets, but fails to bring about a similar improvement on large
instances, suggesting the need for more sophisticated strategies to improve
LLMs' reasoning with larger-context inputs.

</details>


### [11] [CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective](https://arxiv.org/abs/2506.04481)
*Jiayu Liu,Zhenya Huang,Wei Dai,Cheng Cheng,Jinze Wu,Jing Sha,Song Li,Qi Liu,Shijin Wang,Enhong Chen*

Main category: cs.AI

TL;DR: CogMath introduces a nuanced evaluation strategy for assessing the mathematical capabilities of large language models (LLMs), revealing significant overestimations in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for LLMs' mathematical abilities rely on overall accuracy, failing to capture the nuances of their authentic reasoning capabilities.

Method: CogMath bases its methodology on human cognitive processes, measured through 9 detailed dimensions inspired by three stages of reasoning: problem comprehension, solving, and solution summarization. A multi-agent system, "Inquiry-Judge-Reference," evaluates models within these dimensions.

Result: CogMath tests 7 mainstream LLMs across three benchmarks and finds their mathematical abilities are overestimated by 30%-40%, while identifying strengths and weaknesses in specific reasoning stages.

Conclusion: CogMath's fine-grained evaluation provides deeper insights into LLMs' cognitive reasoning and opens pathways for improving their mathematical and logical capabilities.

Abstract: Although large language models (LLMs) show promise in solving complex
mathematical tasks, existing evaluation paradigms rely solely on a coarse
measure of overall answer accuracy, which are insufficient for assessing their
authentic capabilities. In this paper, we propose \textbf{CogMath}, which
comprehensively assesses LLMs' mathematical abilities through the lens of human
cognition. Specifically, inspired by psychological theories, CogMath formalizes
human reasoning process into 3 stages: \emph{problem comprehension},
\emph{problem solving}, and \emph{solution summarization}. Within these stages,
we investigate perspectives such as numerical calculation, knowledge, and
counterfactuals, and design a total of 9 fine-grained evaluation dimensions. In
each dimension, we develop an ``\emph{Inquiry}-\emph{Judge}-\emph{Reference}''
multi-agent system to generate inquiries that assess LLMs' mastery from this
dimension. An LLM is considered to truly master a problem only when excelling
in all inquiries from the 9 dimensions. By applying CogMath on three
benchmarks, we reveal that the mathematical capabilities of 7 mainstream LLMs
are overestimated by 30\%-40\%. Moreover, we locate their strengths and
weaknesses across specific stages/dimensions, offering in-depth insights to
further enhance their reasoning abilities.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation](https://arxiv.org/abs/2506.04544)
*Charles Hong,Brendan Roberts,Huijae An,Alex Um,Advay Ratan,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: The paper introduces hdl2v, a dataset translating other hardware description languages to Verilog, which enhances Verilog generation by large language models by up to 63%.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of human-written Verilog code for training large language models in hardware code generation.

Method: The authors created the hdl2v dataset by translating VHDL, Chisel, and PyMTL3 to Verilog and tested its impact on LLM performance.

Result: The dataset improved a 32-billion-parameter model's Verilog generation accuracy by 23% and enhanced fine-tuning approaches by 63%.

Conclusion: Hdl2v enriches Verilog code resources and paves the way for better hardware code generation using LLMs, while identifying areas for further dataset improvements.

Abstract: Large language models (LLMs) are playing an increasingly large role in
domains such as code generation, including hardware code generation, where
Verilog is the key language. However, the amount of publicly available Verilog
code pales in comparison to the amount of code available for software languages
like Python. In this work, we present hdl2v ("HDL-to-Verilog"), a dataset which
seeks to increase the amount of available human-written Verilog data by
translating or compiling three other hardware description languages - VHDL,
Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v
in enhancing LLM Verilog generation by improving performance of a 32
billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,
without utilizing any data augmentation or knowledge distillation from larger
models. We also show hdl2v's ability to boost the performance of a data
augmentation-based fine-tuning approach by 63%. Finally, we characterize and
analyze our dataset to better understand which characteristics of
HDL-to-Verilog datasets can be expanded upon in future work for even better
performance.

</details>


### [13] [ROSGuard: A Bandwidth Regulation Mechanism for ROS2-based Applications](https://arxiv.org/abs/2506.04640)
*Jon Altonaga Puente,Enrico Mezzetti,Irune Agirre Troncoso,Jaume Abella Ferrer,Francisco J. Cazorla Almeida*

Main category: cs.AR

TL;DR: The paper addresses multicore timing interference in shared hardware resources and introduces ROSGuard, a modular implementation for timing interference monitoring and control for ROS2 applications, achieving effectiveness comparable to state-of-the-art solutions.


<details>
  <summary>Details</summary>
Motivation: Timing interference poses challenges for meeting timing requirements in embedded critical systems, and the existing fine-grained bandwidth control methods are not portable or suitable for less stringent but still beneficial applications like modular ROS2 systems.

Method: The authors developed ROSGuard, a timing interference monitoring and control mechanism, leveraging generic Linux-based software and the ROS2 middleware. It was implemented and tested on the NVIDIA AGX Orin platform.

Result: ROSGGuard applied an effective bandwidth regulation scheme on ROS2-based applications, demonstrating performance comparable to fine-grained state-of-the-art methods.

Conclusion: ROSGGuard offers a portable and modular solution for timing interference control in ROS2 applications, making it practical for wider industrial adoption and diverse applications beyond robotics.

Abstract: Multicore timing interference, arising when multiple requests contend for the
same shared hardware resources, is a primary concern for timing verification
and validation of time-critical applications. Bandwidth control and regulation
approaches have been proposed in the literature as an effective method to
monitor and limit the impact of timing interference at run time. These
approaches seek for fine-grained control of the bandwidth consumption (at the
microsecond level) to meet stringent timing requirements on embedded critical
systems. Such granularity and configurations, while effective, can become an
entry barrier for the application of bandwidth control to a wide class of
productized, modular ROS2 applications. This is so because those applications
have less stringent timing requirements but would still benefit from bandwidth
regulation, though under less restrictive, and therefore more portable,
granularity and configurations.
  In this work, we provide ROSGuard, a highly-portable, modular implementation
of a timing interference monitoring and control mechanism that builds on the
abstractions available on top of a generic and portable Linux-based software
stack with the Robotic Operating System 2 (ROS2) layer, a widespreadedly
adopted middleware for a wide class of industrial applications, far beyond the
robotic domain. We deploy ROSGuard on an NVIDIA AGX Orin platform as a
representative target for functionally rich distributed AI-based applications
and a set of synthetic and real-world benchmarks. We apply an effective
bandwidth regulation scheme on ROS2-based applications and achieve comparable
effectiveness to specialized, finer-grained state-of-the-art solutions.

</details>


### [14] [QiMeng: Fully Automated Hardware and Software Design for Processor Chip](https://arxiv.org/abs/2506.05007)
*Rui Zhang,Yuanbo Wen,Shuyao Cheng,Di Huang,Shaohui Peng,Jiaming Guo,Pengwei Jin,Jiacheng Zhao,Tianrui Ma,Yaoyu Zhu,Yifan Hao,Yongwei Zhao,Shengwen Liang,Ying Wang,Xing Hu,Zidong Du,Huimin Cui,Ling Li,Qi Guo,Yunji Chen*

Main category: cs.AR

TL;DR: The paper introduces QiMeng, a fully automated system for processor chip design using domain-specific AI models.


<details>
  <summary>Details</summary>
Motivation: Rapid advancements in technology have revealed limitations in conventional chip design methods, such as fabrication constraints, resource demands, and ecosystem diversity.

Method: QiMeng is structured into three hierarchical layers: a domain-specific Large Processor Chip Model (LPCM), Hardware and Software Design Agents, and top-layer applications.

Result: Several components of QiMeng are completed and successfully applied in chip design tasks, demonstrating efficiency and advantages.

Conclusion: The proposed QiMeng system provides a promising solution for fully automated processor chip design, though further integration and iterative development are needed.

Abstract: Processor chip design technology serves as a key frontier driving
breakthroughs in computer science and related fields. With the rapid
advancement of information technology, conventional design paradigms face three
major challenges: the physical constraints of fabrication technologies, the
escalating demands for design resources, and the increasing diversity of
ecosystems. Automated processor chip design has emerged as a transformative
solution to address these challenges. While recent breakthroughs in Artificial
Intelligence (AI), particularly Large Language Models (LLMs) techniques, have
opened new possibilities for fully automated processor chip design, substantial
challenges remain in establishing domain-specific LLMs for processor chip
design.
  In this paper, we propose QiMeng, a novel system for fully automated hardware
and software design of processor chips. QiMeng comprises three hierarchical
layers. In the bottom-layer, we construct a domain-specific Large Processor
Chip Model (LPCM) that introduces novel designs in architecture, training, and
inference, to address key challenges such as knowledge representation gap, data
scarcity, correctness assurance, and enormous solution space. In the
middle-layer, leveraging the LPCM's knowledge representation and inference
capabilities, we develop the Hardware Design Agent and the Software Design
Agent to automate the design of hardware and software for processor chips.
Currently, several components of QiMeng have been completed and successfully
applied in various top-layer applications, demonstrating significant advantages
and providing a feasible solution for efficient, fully automated
hardware/software design of processor chips. Future research will focus on
integrating all components and performing iterative top-down and bottom-up
design processes to establish a comprehensive QiMeng system.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [GEM: Empowering LLM for both Embedding Generation and Language Understanding](https://arxiv.org/abs/2506.04344)
*Caojin Zhang,Qiang Zhang,Ke Li,Sai Vidyaranya Nuthalapati,Benyu Zhang,Jason Liu,Serena Li,Lizhu Zhang,Xiangjun Fan*

Main category: cs.CL

TL;DR: The paper introduces GEM, a method enabling decoder-only LLMs to generate high-quality text embeddings without compromising their generation and reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Current applications like RAG require separate embedding models, leading to complications and discrepancies between embedding models and LLM understanding.

Method: GEM inserts special tokens into text and manipulates the attention mask during post-training or fine-tuning stages of decoder-only LLMs.

Result: Testing GEM on LLMs ranging from 1B to 8B parameters shows significant improvements in text embedding benchmarks (MTEB) with minimal impact on NLP benchmarks (MMLU).

Conclusion: GEM effectively empowers LLMs to achieve state-of-the-art text embedding qualities while retaining their original NLP functionalities.

Abstract: Large decoder-only language models (LLMs) have achieved remarkable success in
generation and reasoning tasks, where they generate text responses given
instructions. However, many applications, e.g., retrieval augmented generation
(RAG), still rely on separate embedding models to generate text embeddings,
which can complicate the system and introduce discrepancies in understanding of
the query between the embedding model and LLMs. To address this limitation, we
propose a simple self-supervised approach, Generative Embedding large language
Model (GEM), that enables any large decoder-only LLM to generate high-quality
text embeddings while maintaining its original text generation and reasoning
capabilities. Our method inserts new special token(s) into a text body, and
generates summarization embedding of the text by manipulating the attention
mask. This method could be easily integrated into post-training or fine tuning
stages of any existing LLMs. We demonstrate the effectiveness of our approach
by applying it to two popular LLM families, ranging from 1B to 8B parameters,
and evaluating the transformed models on both text embedding benchmarks (MTEB)
and NLP benchmarks (MMLU). The results show that our proposed method
significantly improves the original LLMs on MTEB while having a minimal impact
on MMLU. Our strong results indicate that our approach can empower LLMs with
state-of-the-art text embedding capabilities while maintaining their original
NLP performance

</details>


### [16] [Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot Accent Robustness in Low-Resource ASR](https://arxiv.org/abs/2506.04364)
*Zheng-Xin Yong,Vineel Pratap,Michael Auli,Jean Maillard*

Main category: cs.CL

TL;DR: This study explores optimizing automatic speech recognition (ASR) systems for unseen accents by analyzing the impact of speaker count, audio duration per speaker, and accent diversity in low-resource scenarios.


<details>
  <summary>Details</summary>
Motivation: To create ASR systems that effectively serve global populations, it is necessary to ensure robustness to a variety of accents, including those not seen during training.

Method: The research systematically evaluates the relationships between speaker count, per-speaker audio duration, and accent diversity, analyzing their effects on ASR robustness under low-resource conditions.

Result: Increasing the number of speakers in training data rather than prioritizing longer durations per speaker or accent diversity leads to improved ASR performance for unseen accents in low-resource regimes.

Conclusion: Practitioners should focus on maximizing the number of unique speakers in ASR training data, as this leads to better robustness against unseen accents.

Abstract: To build an automatic speech recognition (ASR) system that can serve everyone
in the world, the ASR needs to be robust to a wide range of accents including
unseen accents. We systematically study how three different variables in
training data -- the number of speakers, the audio duration per each individual
speaker, and the diversity of accents -- affect ASR robustness towards unseen
accents in a low-resource training regime. We observe that for a fixed number
of ASR training hours, it is more beneficial to increase the number of speakers
(which means each speaker contributes less) than the number of hours
contributed per speaker. We also observe that more speakers enables ASR
performance gains from scaling number of hours. Surprisingly, we observe
minimal benefits to prioritizing speakers with different accents when the
number of speakers is controlled. Our work suggests that practitioners should
prioritize increasing the speaker count in ASR training data composition for
new languages.

</details>


### [17] [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/abs/2506.04373)
*Matthieu Tehenan,Vikram Natarajan,Jonathan Michala,Milton Lin,Juri Opitz*

Main category: cs.CL

TL;DR: The paper introduces a method for decomposing sentence embeddings to make their internal structure more interpretable using dictionary learning on token-level representations.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of interpretability in the internal structure of sentence embeddings, which are widely used in modern NLP and AI systems.

Method: The proposed method involves using dictionary learning on token-level representations to mechanistically decompose sentence embeddings into interpretable components.

Result: The study reveals that many semantic and syntactic aspects are linearly encoded in sentence embeddings, enhancing their interpretability.

Conclusion: The work bridges the gap between token-level and sentence-level analysis, enabling more transparent and controllable representations in NLP systems.

Abstract: Sentence embeddings are central to modern NLP and AI systems, yet little is
known about their internal structure. While we can compare these embeddings
using measures such as cosine similarity, the contributing features are not
human-interpretable, and the content of an embedding seems untraceable, as it
is masked by complex neural transformations and a final pooling operation that
combines individual token embeddings. To alleviate this issue, we propose a new
method to mechanistically decompose sentence embeddings into interpretable
components, by using dictionary learning on token-level representations. We
analyze how pooling compresses these features into sentence representations,
and assess the latent features that reside in a sentence embedding. This
bridges token-level mechanistic interpretability with sentence-level analysis,
making for more transparent and controllable representations. In our studies,
we obtain several interesting insights into the inner workings of sentence
embedding spaces, for instance, that many semantic and syntactic aspects are
linearly encoded in the embeddings.

</details>


### [18] [Hierarchical Text Classification Using Contrastive Learning Informed Path Guided Hierarchy](https://arxiv.org/abs/2506.04381)
*Neeraj Agrawal,Saurabh Kumar,Priyanka Bhatt,Tanishka Agarwal*

Main category: cs.CL

TL;DR: The paper introduces HTC-CLIP, a novel model that effectively combines two existing approaches for hierarchical text classification using contrastive learning, achieving improved performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Hierarchical text classification models are essential for applications requiring understanding of complex label hierarchies, such as e-commerce and medical domains. However, existing models capture different complementary facets of label hierarchy separately, necessitating a unified approach.

Method: The proposed HTC-CLIP model uses contrastive learning to create hierarchy-aware text and path-guided hierarchy representations. It trains two sets of class probability distributions and combines their pooled outputs during inference to leverage the strengths of both representations.

Result: The HTC-CLIP model demonstrated improved performance, with a 0.99-2.37% increase in Macro F1 scores over state-of-the-art models when tested on two public benchmark datasets.

Conclusion: HTC-CLIP successfully merges complementary hierarchy-aware approaches, achieving superior accuracy and reinforcing the value of combining text and path-guided representations in hierarchical text classification tasks.

Abstract: Hierarchical Text Classification (HTC) has recently gained traction given the
ability to handle complex label hierarchy. This has found applications in
domains like E- commerce, customer care and medicine industry among other
real-world applications. Existing HTC models either encode label hierarchy
separately and mix it with text encoding or guide the label hierarchy structure
in the text encoder. Both approaches capture different characteristics of label
hierarchy and are complementary to each other. In this paper, we propose a
Hierarchical Text Classification using Contrastive Learning Informed Path
guided hierarchy (HTC-CLIP), which learns hierarchy-aware text representation
and text informed path guided hierarchy representation using contrastive
learning. During the training of HTC-CLIP, we learn two different sets of class
probabilities distributions and during inference, we use the pooled output of
both probabilities for each class to get the best of both representations. Our
results show that the two previous approaches can be effectively combined into
one architecture to achieve improved performance. Tests on two public benchmark
datasets showed an improvement of 0.99 - 2.37% in Macro F1 score using HTC-CLIP
over the existing state-of-the-art models.

</details>


### [19] [MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP](https://arxiv.org/abs/2506.04385)
*Kurt Micallef,Claudia Borg*

Main category: cs.CL

TL;DR: This paper evaluates 55 LLMs on Maltese, a low-resource language, using a benchmark covering 11 tasks. It finds smaller fine-tuned models outperform larger ones, emphasizing the importance of prior exposure to Maltese and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of LLMs for low-resource languages like Maltese is limited, and there is a need to assess their performance to identify gaps and potential solutions.

Method: 55 publicly available LLMs were tested against a benchmark designed for Maltese that includes 11 discriminative and generative tasks. A multidimensional performance analysis was conducted.

Result: Most models performed poorly, especially on generative tasks. Smaller fine-tuned models often outperformed larger ones. Performance was strongly linked to prior exposure of Maltese during pre-training and instruction tuning.

Conclusion: Fine-tuning improves performance despite higher costs, and prior exposure to Maltese is crucial. Researchers are encouraged to adopt more traditional approaches for low-resource language modelling and push for inclusive language technology.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various Natural Language Processing (NLP) tasks, largely due to their
generalisability and ability to perform tasks without additional training.
However, their effectiveness for low-resource languages remains limited. In
this study, we evaluate the performance of 55 publicly available LLMs on
Maltese, a low-resource language, using a newly introduced benchmark covering
11 discriminative and generative tasks. Our experiments highlight that many
models perform poorly, particularly on generative tasks, and that smaller
fine-tuned models often perform better across all tasks. From our
multidimensional analysis, we investigate various factors impacting
performance. We conclude that prior exposure to Maltese during pre-training and
instruction-tuning emerges as the most important factor. We also examine the
trade-offs between fine-tuning and prompting, highlighting that while
fine-tuning requires a higher initial cost, it yields better performance and
lower inference costs. Through this work, we aim to highlight the need for more
inclusive language technologies and recommend that researchers working with
low-resource languages consider more "traditional" language modelling
approaches.

</details>


### [20] [Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care](https://arxiv.org/abs/2506.04389)
*Saurabh Kumar,Sourav Bansal,Neeraj Agrawal,Priyanka Bhatt*

Main category: cs.CL

TL;DR: The paper introduces a novel embedder-cum-classifier model that generalizes intent classification across multiple channels, geographies, and languages with minimal labeled data, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of building intent classifiers for customer care that generalize across multiple domains (channel, geography, language) despite limited annotated data, essential for enhancing e-commerce customer experiences.

Method: The authors propose a supervised fine-tuning approach using isotropic regularizers to train a domain-specific sentence embedder, coupled with multilingual knowledge distillation to generalize across domains. This embedder is then paired with a linear classifier for deployment.

Result: The new method achieves a 20-23% accuracy improvement over existing state-of-the-art pre-trained models for few-shot intent detection on datasets from Canada and Mexico e-commerce customer care.

Conclusion: The proposed architecture effectively enables domain generalization of intent classifiers with limited annotations, showing significant practical value in cross-domain scenarios for e-commerce customer care.

Abstract: Customer care is an essential pillar of the e-commerce shopping experience
with companies spending millions of dollars each year, employing automation and
human agents, across geographies (like US, Canada, Mexico, Chile), channels
(like Chat, Interactive Voice Response (IVR)), and languages (like English,
Spanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on
annotated data have shown good performance in downstream tasks relevant to
Customer Care. However, model performance is largely subject to the
availability of sufficient annotated domain-specific data. Cross-domain
availability of data remains a bottleneck, thus building an intent classifier
that generalizes across domains (defined by channel, geography, and language)
with only a few annotations, is of great practical value. In this paper, we
propose an embedder-cum-classifier model architecture which extends
state-of-the-art domain-specific models to other domains with only a few
labeled samples. We adopt a supervised fine-tuning approach with isotropic
regularizers to train a domain-specific sentence embedder and a multilingual
knowledge distillation strategy to generalize this embedder across multiple
domains. The trained embedder, further augmented with a simple linear
classifier can be deployed for new domains. Experiments on Canada and Mexico
e-commerce Customer Care dataset with few-shot intent detection show an
increase in accuracy by 20-23% against the existing state-of-the-art
pre-trained models.

</details>


### [21] [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale](https://arxiv.org/abs/2506.04405)
*Ran Xu,Yuchen Zhuang,Yishan Zhong,Yue Yu,Xiangru Tang,Hang Wu,May D. Wang,Peifeng Ruan,Donghan Yang,Tao Wang,Guanghua Xiao,Carl Yang,Yang Xie,Wenqi Shi*

Main category: cs.CL

TL;DR: MedAgentGYM is the first training platform aiming to improve coding-based medical reasoning in large language models. It encompasses a vast set of tasks and provides benchmarking capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the need for better coding-based medical reasoning in LLM-based biomedical agents while offering improved benchmarking and training resources.

Method: MedAgentGYM includes 72,413 task instances from real biomedical scenarios, executable coding environments, interactive feedback mechanisms, and scalable training paths.

Result: MedAgentGYM enabled Med-Copilot-7B to significantly enhance its performance through supervised fine-tuning (+36.44%) and reinforcement learning (+42.47%), making it competitive with gpt-4o while preserving affordability and privacy.

Conclusion: MedAgentGYM serves as an integrated platform, advancing the development of LLM-based coding assistants for biomedical applications by providing robust training and benchmarking resources.

Abstract: We introduce MedAgentGYM, the first publicly available training environment
designed to enhance coding-based medical reasoning capabilities in large
language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across
129 categories derived from authentic real-world biomedical scenarios. Tasks
are encapsulated within executable coding environments, each featuring detailed
task descriptions, interactive feedback mechanisms, verifiable ground-truth
annotations, and scalable training trajectory generation. Extensive
benchmarking of over 30 LLMs reveals a notable performance disparity between
commercial API-based models and open-source counterparts. Leveraging
MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through
supervised fine-tuning (+36.44%) and continued reinforcement learning
(+42.47%), emerging as an affordable and privacy-preserving alternative
competitive with gpt-4o. By offering both a comprehensive benchmark and
accessible, expandable training resources within unified execution
environments, MedAgentGYM delivers an integrated platform to develop LLM-based
coding assistants for advanced biomedical research and practice.

</details>


### [22] [Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning](https://arxiv.org/abs/2506.04408)
*Wesley Scivetti,Tatsuya Aoyama,Ethan Wilcox,Nathan Schneider*

Main category: cs.CL

TL;DR: The paper investigates language models' ability to generalize rare grammatical phenomena, focusing on the LET-ALONE construction, and finds asymmetries in how they grasp form versus meaning.


<details>
  <summary>Details</summary>
Motivation: To explore the extent to which human-scale language models generalize to rare grammatical constructs and their meanings, compared to humans.

Method: A synthetic benchmark was created to test human-scale transformer language models on the form and meaning of the LET-ALONE construction, accounting for syntactic and semantic properties.

Result: Language models showed sensitivity to grammatical forms but failed to generalize correctly to the meanings of the LET-ALONE construction.

Conclusion: Current language models demonstrate an imbalance in learning efficiency between language form and meaning, which differs from the capabilities of human learners.

Abstract: Humans have a remarkable ability to acquire and understand grammatical
phenomena that are seen rarely, if ever, during childhood. Recent evidence
suggests that language models with human-scale pretraining data may possess a
similar ability by generalizing from frequent to rare constructions. However,
it remains an open question how widespread this generalization ability is, and
to what extent this knowledge extends to meanings of rare constructions, as
opposed to just their forms. We fill this gap by testing human-scale
transformer language models on their knowledge of both the form and meaning of
the (rare and quirky) English LET-ALONE construction. To evaluate our LMs we
construct a bespoke synthetic benchmark that targets syntactic and semantic
properties of the construction. We find that human-scale LMs are sensitive to
form, even when related constructions are filtered from the dataset. However,
human-scale LMs do not make correct generalizations about LET-ALONE's meaning.
These results point to an asymmetry in the current architectures' sample
efficiency between language form and meaning, something which is not present in
human language learners.

</details>


### [23] [Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction](https://arxiv.org/abs/2506.04409)
*Lev Morozov,Aleksandr Mogilevskii,Alexander Shirnin*

Main category: cs.CL

TL;DR: This paper presents EmoRAG, a system for multi-label emotion detection in text, achieving competitive results efficiently without additional model training.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of detecting perceived emotions in text, specifically for SemEval-2025 Task 11 Subtask A, by providing an efficient and scalable solution.

Method: The proposed method leverages an ensemble of models to predict emotions, without requiring additional training, focusing on efficiency and ease of implementation.

Result: EmoRAG attains results comparable to the top systems in the task, proving its effectiveness in emotion detection.

Conclusion: EmoRAG provides a competitive, efficient, and scalable approach for multi-label emotion detection in text, making it a practical alternative to more complex systems requiring extensive training.

Abstract: This paper describes EmoRAG, a system designed to detect perceived emotions
in text for SemEval-2025 Task 11, Subtask A: Multi-label Emotion Detection. We
focus on predicting the perceived emotions of the speaker from a given text
snippet, labeling it with emotions such as joy, sadness, fear, anger, surprise,
and disgust. Our approach does not require additional model training and only
uses an ensemble of models to predict emotions. EmoRAG achieves results
comparable to the best performing systems, while being more efficient,
scalable, and easier to implement.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [24] [Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training](https://arxiv.org/abs/2506.04263)
*Alan Mitkiy,James Smith,Hana Satou,Hiroshi Tanaka,Emily Johnson,F Monkey*

Main category: cs.CV

TL;DR: This paper introduces Dynamic Epsilon Scheduling (DES), a method to improve adversarial robustness by dynamically adapting the perturbation budget during adversarial training.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial training methods rely on a fixed perturbation budget, which does not consider the unique robustness characteristics of individual instances.

Method: The proposed DES framework uses a dynamic scheduling strategy based on three factors: (1) distance to the decision boundary via gradient-based proxies, (2) prediction confidence through softmax entropy, and (3) model uncertainty via Monte Carlo dropout.

Result: The method outperforms fixed-epsilon baselines and prior adaptive approaches in both adversarial robustness and standard accuracy on CIFAR-10 and CIFAR-100 datasets.

Conclusion: DES offers a novel, instance-aware, and data-driven approach to adversarial training, combining multiple cues into an effective adaptive strategy.

Abstract: Adversarial training is among the most effective strategies for defending
deep neural networks against adversarial examples. A key limitation of existing
adversarial training approaches lies in their reliance on a fixed perturbation
budget, which fails to account for instance-specific robustness
characteristics. While prior works such as IAAT and MMA introduce
instance-level adaptations, they often rely on heuristic or static
approximations of data robustness. In this paper, we propose Dynamic Epsilon
Scheduling (DES), a novel framework that adaptively adjusts the adversarial
perturbation budget per instance and per training iteration. DES integrates
three key factors: (1) the distance to the decision boundary approximated via
gradient-based proxies, (2) prediction confidence derived from softmax entropy,
and (3) model uncertainty estimated via Monte Carlo dropout. By combining these
cues into a unified scheduling strategy, DES tailors the perturbation budget
dynamically to guide more effective adversarial learning. Experimental results
on CIFAR-10 and CIFAR-100 show that our method consistently improves both
adversarial robustness and standard accuracy compared to fixed-epsilon
baselines and prior adaptive methods. Moreover, we provide theoretical insights
into the stability and convergence of our scheduling policy. This work opens a
new avenue for instance-aware, data-driven adversarial training methods.

</details>


### [25] [RSVP: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought](https://arxiv.org/abs/2506.04277)
*Yi Lu,Jiawang Cao,Yongliang Wu,Bozheng Li,Licheng Tang,Yangguang Ji,Chong Wu,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: This paper introduces RSVP, a framework that bridges the gap between cognitive reasoning and visual perception, enabling better visual grounding and segmentation in multi-modal large language models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of visual grounding and segmentation capabilities in MLLMs, creating a gap in their cognitive reasoning and visual perception.

Method: RSVP uses a two-stage framework: reasoning-driven localization through multimodal chain-of-thought prompts, followed by segmentation refinement using a Vision-Language Segmentation Module for precise results.

Result: RSVP achieves state-of-the-art performance with improvements of up to +6.5 gIoU, +9.2 cIoU on ReasonSeg, and 49.7 mAP in SegInW under zero-shot conditions.

Conclusion: RSVP effectively combines multimodal reasoning and segmentation, enabling interpretable and structured visual understanding, showcasing its scalability and impact.

Abstract: Multi-modal Large Language Models (MLLMs) have demonstrated remarkable
reasoning capability while lack explicit mechanisms for visual grounding and
segmentation, creating a gap between cognitive reasoning and visual perception.
To bridge this gap, we introduce Reasoning Segmentation via Visual Prompting
(RSVP), a novel framework that unifies multi-step multimodal reasoning with
grounded visual understanding. RSVP is a two-stage structuralized framework
that integrates reasoning-driven localization with segmentation refinement. In
the reasoning stage, RSVP employs multimodal chain-of-thought visual prompts to
help MLLMs understand queries and infer targets, generating interpretable
region proposals that enhance visual grounding. In segmentation stage, RSVP
refines these proposals with a Vision-Language Segmentation Module (VLSM),
seamlessly integrates textual and visual cues to produce precise segmentation
masks. By explicitly modelling the interaction between multimodal reasoning and
segmentation, RSVP introduces a new paradigm for interpretable reasoning
segmentation. It exploits MLLMs' inherent localization capabilities, enabling
the models to not only reason about objects but also generate structured visual
representations. Our extensive experiments demonstrate that RSVP achieves
state-of-the-art performance, surpasses state-of-the-art methods by up to +6.5
gIoU and +9.2 cIoU on ReasonSeg, and achieves 49.7 mAP on SegInW under
zero-shot settings. These results validate RSVP as an effective and scalable
framework for integrating cognitive reasoning with structured visual
understanding.

</details>


### [26] [Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark](https://arxiv.org/abs/2506.04280)
*Ziming Cheng,Binrui Xu,Lisheng Gong,Zuhe Song,Tianshuo Zhou,Shiqi Zhong,Siyu Ren,Mingxiang Chen,Xiangchao Meng,Yuxin Zhang,Yanlin Li,Lei Ren,Wei Chen,Zhiyuan Huang,Mingjie Zhan,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CV

TL;DR: The paper introduces the Multimodal Multi-image Reasoning Benchmark (MMRB), the first benchmark to evaluate structured reasoning over multi-image inputs across 92 sub-tasks, revealing significant performance gaps in open-source MLLMs and reward models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on either single-image reasoning or give only final-answer evaluation for multi-image tasks, making the reasoning capabilities of MLLMs over multi-image inputs underexplored.

Method: The authors created MMRB, a benchmark encompassing 92 sub-tasks for spatial, temporal, and semantic multi-image reasoning, employing multi-solution Chain-of-Thought (CoT) annotations generated by GPT-4o and refined by humans. They also proposed a sentence-level matching framework using open-source LLMs for scalable evaluations.

Result: Baseline experiments on 40 MLLMs indicate significant underperformance of open-source MLLMs compared to commercial ones, and revealed a nearly complete inability of current multimodal reward models to tackle multi-image reward ranking tasks.

Conclusion: MMRB highlights the limitations of current MLLMs and reward models in multi-image reasoning and provides a structured benchmark to facilitate further improvements in the field.

Abstract: With enhanced capabilities and widespread applications, Multimodal Large
Language Models (MLLMs) are increasingly required to process and reason over
multiple images simultaneously. However, existing MLLM benchmarks focus either
on single-image visual reasoning or on multi-image understanding tasks with
only final-answer evaluation, leaving the reasoning capabilities of MLLMs over
multi-image inputs largely underexplored. To address this gap, we introduce the
$\textbf{Multimodal Multi-image Reasoning Benchmark (MMRB)}$, the first
benchmark designed to evaluate structured visual reasoning across multiple
images. MMRB comprises $\textbf{92 sub-tasks}$ covering spatial, temporal, and
semantic reasoning, with multi-solution, CoT-style annotations generated by
GPT-4o and refined by human experts. A derivative subset is designed to
evaluate multimodal reward models in multi-image scenarios. To support fast and
scalable evaluation, we propose a sentence-level matching framework using
open-source LLMs. Extensive baseline experiments on $\textbf{40 MLLMs}$,
including 9 reasoning-specific models and 8 reward models, demonstrate that
open-source MLLMs still lag significantly behind commercial MLLMs in
multi-image reasoning tasks. Furthermore, current multimodal reward models are
nearly incapable of handling multi-image reward ranking tasks.

</details>


### [27] [HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting](https://arxiv.org/abs/2506.04351)
*Maksym Ivashechkin,Oscar Mendez,Richard Bowden*

Main category: cs.CV

TL;DR: The paper proposes a new weakly supervised pipeline for generating 3D human models from text prompts, addressing issues like realism and controlability.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in generating realistic and controllable 3D humans from text prompts using generative AI techniques.

Method: The approach involves generating photorealistic human image datasets, mapping image features to 3D point clouds using transformers, and training a point-cloud diffusion model conditioned on text prompts.

Result: The method achieves significant improvements in 3D human generation speed, text-prompt alignment, and rendering quality compared to the state-of-the-art.

Conclusion: The proposed system effectively addresses key challenges in 3D human generation, showing potential for broad application while providing open access to the code and dataset.

Abstract: 3D human generation is an important problem with a wide range of applications
in computer vision and graphics. Despite recent progress in generative AI such
as diffusion models or rendering methods like Neural Radiance Fields or
Gaussian Splatting, controlling the generation of accurate 3D humans from text
prompts remains an open challenge. Current methods struggle with fine detail,
accurate rendering of hands and faces, human realism, and controlability over
appearance. The lack of diversity, realism, and annotation in human image data
also remains a challenge, hindering the development of a foundational 3D human
model. We present a weakly supervised pipeline that tries to address these
challenges. In the first step, we generate a photorealistic human image dataset
with controllable attributes such as appearance, race, gender, etc using a
state-of-the-art image diffusion model. Next, we propose an efficient mapping
approach from image features to 3D point clouds using a transformer-based
architecture. Finally, we close the loop by training a point-cloud diffusion
model that is conditioned on the same text prompts used to generate the
original samples. We demonstrate orders-of-magnitude speed-ups in 3D human
generation compared to the state-of-the-art approaches, along with
significantly improved text-prompt alignment, realism, and rendering quality.
We will make the code and dataset available.

</details>


### [28] [ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding](https://arxiv.org/abs/2506.04353)
*Ankit Pal,Jung-Oh Lee,Xiaoman Zhang,Malaikannan Sankarasubbu,Seunghyeon Roh,Won Jung Kim,Meesun Lee,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ReXVQA is a new benchmark for visual question answering in chest radiology, featuring a large dataset and focusing on five key reasoning skills. AI models surpassed radiology residents in accuracy on this task.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing VQA benchmarks in radiology, which often rely on template-based queries, by introducing a more clinically realistic and comprehensive dataset and evaluation framework.

Method: The study introduces ReXVQA, comprising 696,000 questions and 160,000 chest X-ray studies, and evaluates eight state-of-the-art multimodal AI models against a human reader study involving radiology residents.

Result: The best-performing AI model, MedGemma, achieved 83.24% overall accuracy, surpassing the best radiology resident's performance of 77.27%.

Conclusion: ReXVQA sets a new standard for evaluating radiological AI, bridging the gap between AI and clinical reasoning while highlighting distinct performance patterns between AI and human experts.

Abstract: We present ReXVQA, the largest and most comprehensive benchmark for visual
question answering (VQA) in chest radiology, comprising approximately 696,000
questions paired with 160,000 chest X-rays studies across training, validation,
and test sets. Unlike prior efforts that rely heavily on template based
queries, ReXVQA introduces a diverse and clinically authentic task suite
reflecting five core radiological reasoning skills: presence assessment,
location analysis, negation detection, differential diagnosis, and geometric
reasoning. We evaluate eight state-of-the-art multimodal large language models,
including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The
best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge
the gap between AI performance and clinical expertise, we conducted a
comprehensive human reader study involving 3 radiology residents on 200
randomly sampled cases. Our evaluation demonstrates that MedGemma achieved
superior performance (83.84% accuracy) compared to human readers (best
radiology resident: 77.27%), representing a significant milestone where AI
performance exceeds expert human evaluation on chest X-ray interpretation. The
reader study reveals distinct performance patterns between AI models and human
experts, with strong inter-reader agreement among radiologists while showing
more variable agreement patterns between human readers and AI models. ReXVQA
establishes a new standard for evaluating generalist radiological AI systems,
offering public leaderboards, fine-grained evaluation splits, structured
explanations, and category-level breakdowns. This benchmark lays the foundation
for next-generation AI systems capable of mimicking expert-level clinical
reasoning beyond narrow pathology classification. Our dataset will be
open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA

</details>


### [29] [WorldPrediction: A Benchmark for High-level World Modeling and Long-horizon Procedural Planning](https://arxiv.org/abs/2506.04363)
*Delong Chen,Willy Chung,Yejin Bang,Ziwei Ji,Pascale Fung*

Main category: cs.CV

TL;DR: WorldPrediction is a video-based benchmark aimed at evaluating AI's ability for world modeling and procedural planning. It poses tasks requiring semantic and temporal action reasoning, showing current AI models fall short compared to humans.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in evaluating AI's ability to model and plan actions in diverse environments, particularly focusing beyond low-level robotic motion to semantic and temporal abstraction.

Method: The study introduces the WorldPrediction benchmark composed of two tasks: selecting appropriate actions based on visual observations (WorldPrediction-WM) and determining proper sequences of actions (WorldPrediction-PP). It includes human filtering and validation to ensure robustness.

Result: Current AI models achieve only 57% accuracy on WorldPrediction-WM and 38% on WorldPrediction-PP, while humans perform flawlessly, highlighting the inadequacy of present AI approaches in these tasks.

Conclusion: AI models need significant advancements to approach human-level world modeling and procedural planning capabilities, emphasizing the importance of generalized and abstract reasoning benchmarks like WorldPrediction.

Abstract: Humans are known to have an internal "world model" that enables us to carry
out action planning based on world states. AI agents need to have such a world
model for action planning as well. It is not clear how current AI models,
especially generative models, are able to learn such world models and carry out
procedural planning in diverse environments. We introduce WorldPrediction, a
video-based benchmark for evaluating world modeling and procedural planning
capabilities of different AI models. In contrast to prior benchmarks that focus
primarily on low-level world modeling and robotic motion planning,
WorldPrediction is the first benchmark that emphasizes actions with temporal
and semantic abstraction. Given initial and final world states, the task is to
distinguish the proper action (WorldPrediction-WM) or the properly ordered
sequence of actions (WorldPrediction-PP) from a set of counterfactual
distractors. This discriminative task setup enable us to evaluate different
types of world models and planners and realize a thorough comparison across
different hypothesis. The benchmark represents states and actions using visual
observations. In order to prevent models from exploiting low-level continuity
cues in background scenes, we provide "action equivalents" - identical actions
observed in different contexts - as candidates for selection. This benchmark is
grounded in a formal framework of partially observable semi-MDP, ensuring
better reliability and robustness of the evaluation. We conduct extensive human
filtering and validation on our benchmark and show that current frontier models
barely achieve 57% accuracy on WorldPrediction-WM and 38% on WorldPrediction-PP
whereas humans are able to solve both tasks perfectly.

</details>


### [30] [Ice Hockey Puck Localization Using Contextual Cues](https://arxiv.org/abs/2506.04365)
*Liam Salass,Jerrin Bright,Amir Nazemi,Yuhao Chen,John Zelek,David Clausi*

Main category: cs.CV

TL;DR: This paper introduces PLUCC, a novel method for detecting ice hockey pucks in broadcast videos using player behavior, with significant performance improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Challenges in hockey puck detection, such as small size, occlusions, motion blur, and inconsistent scales, demand new approaches that leverage contextual cues like player behavior.

Method: The PLUCC method uses a contextual encoder for modeling player positioning and orientation, a feature pyramid encoder for multiscale feature extraction, and a gating decoder to decode features effectively.

Result: The proposed PLUCC method achieved a 12.2% improvement in average precision and 25% improvement in RSLE average precision compared to previous methods using the PuckDataset.

Conclusion: The paper highlights the importance of contextual cues, particularly player behavior, in advancing puck detection technology, showing promise for broader automated sports analysis applications.

Abstract: Puck detection in ice hockey broadcast videos poses significant challenges
due to the puck's small size, frequent occlusions, motion blur, broadcast
artifacts, and scale inconsistencies due to varying camera zoom and broadcast
camera viewpoints. Prior works focus on appearance-based or motion-based cues
of the puck without explicitly modelling the cues derived from player
behaviour. Players consistently turn their bodies and direct their gaze toward
the puck. Motivated by this strong contextual cue, we propose Puck Localization
Using Contextual Cues (PLUCC), a novel approach for scale-aware and
context-driven single-frame puck detections. PLUCC consists of three
components: (a) a contextual encoder, which utilizes player orientations and
positioning as helpful priors; (b) a feature pyramid encoder, which extracts
multiscale features from the dual encoders; and (c) a gating decoder that
combines latent features with a channel gating mechanism. For evaluation, in
addition to standard average precision, we propose Rink Space Localization
Error (RSLE), a scale-invariant homography-based metric for removing
perspective bias from rink space evaluation. The experimental results of PLUCC
on the PuckDataset dataset demonstrated state-of-the-art detection performance,
surpassing previous baseline methods by an average precision improvement of
12.2% and RSLE average precision of 25%. Our research demonstrates the critical
role of contextual understanding in improving puck detection performance, with
broad implications for automated sports analysis.

</details>


### [31] [Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A Comparative Analysis for Classification Tasks](https://arxiv.org/abs/2506.04367)
*Jubayer Ahmed Bhuiyan Shawon,Hasan Mahmud,Kamrul Hasan*

Main category: cs.CV

TL;DR: The paper fine-tunes state-of-the-art video transformer models to recognize Bangladesh Sign Language (BdSL), achieving high accuracy, particularly with the VideoMAE model.


<details>
  <summary>Details</summary>
Motivation: To improve accessibility for the hearing-impaired community in Bangladesh by automating the recognition of Bangla Sign Language (BdSL) using advanced video transformer models.

Method: Fine-tuning state-of-the-art video transformer architectures (VideoMAE, ViViT, TimeSformer) on two BdSL datasets (BdSLW60 and BdSLW401) using data augmentation, 10-fold stratified cross-validation, and signer-independent evaluation.

Result: The VideoMAE model achieved 95.5% accuracy on the BdSLW60 dataset and 81.04% on BdSLW401, outperforming traditional ML and deep learning methods. Results are dependent on dataset size, video quality, frame distribution, frame rate, and model architecture.

Conclusion: Video transformer models, particularly VideoMAE, show strong potential for scalable and robust recognition of BdSL, paving the way for improved communication accessibility for the hearing-impaired community.

Abstract: Sign Language Recognition (SLR) involves the automatic identification and
classification of sign gestures from images or video, converting them into text
or speech to improve accessibility for the hearing-impaired community. In
Bangladesh, Bangla Sign Language (BdSL) serves as the primary mode of
communication for many individuals with hearing impairments. This study
fine-tunes state-of-the-art video transformer architectures -- VideoMAE, ViViT,
and TimeSformer -- on BdSLW60 (arXiv:2402.08635), a small-scale BdSL dataset
with 60 frequent signs. We standardized the videos to 30 FPS, resulting in
9,307 user trial clips. To evaluate scalability and robustness, the models were
also fine-tuned on BdSLW401 (arXiv:2503.02360), a large-scale dataset with 401
sign classes. Additionally, we benchmark performance against public datasets,
including LSA64 and WLASL. Data augmentation techniques such as random
cropping, horizontal flipping, and short-side scaling were applied to improve
model robustness. To ensure balanced evaluation across folds during model
selection, we employed 10-fold stratified cross-validation on the training set,
while signer-independent evaluation was carried out using held-out test data
from unseen users U4 and U8. Results show that video transformer models
significantly outperform traditional machine learning and deep learning
approaches. Performance is influenced by factors such as dataset size, video
quality, frame distribution, frame rate, and model architecture. Among the
models, the VideoMAE variant (MCG-NJU/videomae-base-finetuned-kinetics)
achieved the highest accuracies of 95.5% on the frame rate corrected BdSLW60
dataset and 81.04% on the front-facing signs of BdSLW401 -- demonstrating
strong potential for scalable and accurate BdSL recognition.

</details>


### [32] [Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization](https://arxiv.org/abs/2506.04379)
*Matthew W. Shinkle,Mark D. Lescroart*

Main category: cs.CV

TL;DR: This paper applies activation maximization to DNN-based models to better understand brain responses, demonstrating its potential for exploring visual selectivity in human visual cortex using generated images.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying specific visual features driving activity in the human brain, using DNN-based encoding models that currently provide limited interpretability.

Method: The study adapts activations from a pretrained DNN (Inception V3), predicts fMRI responses using linear regression, and applies activation maximization to generate images optimized for specific brain voxel or ROI responses, validated through fMRI experiments.

Result: The generated images revealed visual features consistent with known visual selectivity, modulated activity in targeted brain regions across visual areas, and replicated results across different subjects.

Conclusion: Activation maximization is effective in enhancing the interpretability of DNN-based encoding models, enabling a flexible and generative characterization of human visual system responses without relying solely on natively generative models.

Abstract: Deep neural networks (DNNs) trained on visual tasks develop feature
representations that resemble those in the human visual system. Although
DNN-based encoding models can accurately predict brain responses to visual
stimuli, they offer limited insight into the specific features driving these
responses. Here, we demonstrate that activation maximization -- a technique
designed to interpret vision DNNs -- can be applied to DNN-based encoding
models of the human brain. We extract and adaptively downsample activations
from multiple layers of a pretrained Inception V3 network, then use linear
regression to predict fMRI responses. This yields a full image-computable model
of brain responses. Next, we apply activation maximization to generate images
optimized for predicted responses in individual cortical voxels. We find that
these images contain visual characteristics that qualitatively correspond with
known selectivity and enable exploration of selectivity across the visual
cortex. We further extend our method to whole regions of interest (ROIs) of the
brain and validate its efficacy by presenting these images to human
participants in an fMRI study. We find that the generated images reliably drive
activity in targeted regions across both low- and high-level visual areas and
across subjects. These results demonstrate that activation maximization can be
successfully applied to DNN-based encoding models. By addressing key
limitations of alternative approaches that require natively generative models,
our approach enables flexible characterization and modulation of responses
across the human visual system.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [33] [Fully-Distributed Construction of Byzantine-Resilient Dynamic Peer-to-Peer Networks](https://arxiv.org/abs/2506.04368)
*Aayush Gupta,Gopal Pandurangan*

Main category: cs.DC

TL;DR: The paper proposes a fully-distributed P2P protocol to maintain resilient, sparse expander network topologies under high churn and Byzantine nodes with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of constructing and maintaining robust P2P overlay networks that are resilient to dynamic churn and adversarial Byzantine nodes, aiding efficient distributed computing.

Method: A randomized dynamic P2P protocol that ensures expansion properties of the network graph with bounded degree, using only local knowledge and with minimal communication and processing overhead.

Result: The proposed protocol guarantees, with high probability, a constant degree graph with logarithmic diameter and high expansion, tolerating up to $o(n/poly\log(n))$ Byzantine nodes.

Conclusion: The protocol ensures scalability, resilience, and efficiency, offering a robust building block for fundamental distributed computing problems like Byzantine agreement, even in hostile network environments.

Abstract: We address a fundamental problem in Peer-to-Peer (P2P) networks, namely,
constructing and maintaining dynamic P2P overlay network topologies with
essential properties such as connectivity, low diameter, and high expansion,
that are resilient to continuous high churn and the presence of a large number
of malicious (Byzantine) nodes. Our main goal is to construct and maintain a
sparse (bounded degree) expander topology despite high churn and a large number
of Byzantine nodes. Such an expander topology has logarithmic diameter, high
expansion, and is robust to churn and the presence of a large number of bad
nodes, and facilitates efficient and robust algorithms for fundamental problems
in distributed computing, such as agreement, broadcasting, routing, etc.
  Our main contribution is a randomized, fully-distributed dynamic P2P protocol
that works with only local initial knowledge and guarantees, with a high
probability, the maintenance of a constant degree graph with high expansion
even under continuous churn and in the presence of a large number of Byzantine
nodes. Our protocol can tolerate up to $o(n/poly\log(n))$ Byzantine nodes
(where $n$ is the stable network size). Our protocol is efficient, lightweight,
and scalable, and it incurs only $O(poly\log(n))$ overhead for topology
maintenance: only polylogarithmic (in $n$) bits need to be processed and sent
by each honest node per round, and any honest node's computation cost per round
is also polylogarithmic.
  Our protocol can be used as a building block for solving fundamental
distributed computing problems in highly dynamic networks, such as Byzantine
agreement and Byzantine leader election, and enables fast and scalable
algorithms for these problems.

</details>


### [34] [Knowledge-Guided Attention-Inspired Learning for Task Offloading in Vehicle Edge Computing](https://arxiv.org/abs/2506.04456)
*Ke Ma,Junfei Xie*

Main category: cs.DC

TL;DR: This paper introduces KATO, an efficient learning-based task offloading framework for Vehicle Edge Computing (VEC), using attention-guided models and iterative algorithms.


<details>
  <summary>Details</summary>
Motivation: Task offloading in Vehicle Edge Computing is complicated due to the need for fast, computation-efficient solutions to meet delay-sensitive application demands.

Method: KATO employs an attention-inspired encoder-decoder model to select RSUs and an iterative algorithm to allocate the tasks optimally among them.

Result: Simulations reveal that KATO delivers optimal or near-optimal performance with reduced computational costs, adapting well to diverse network sizes and setups.

Conclusion: KATO streamlines task offloading in VEC with a robust and efficient approach that minimizes overhead and achieves reliable performance across various scenarios.

Abstract: Vehicle edge computing (VEC) brings abundant computing resources close to
vehicles by deploying them at roadside units (RSUs) or base stations, thereby
enabling diverse computation-intensive and delay sensitive applications.
Existing task offloading strategies are often computationally expensive to
execute or generate suboptimal solutions. In this paper, we propose a novel
learning-based approach, Knowledge-guided Attention-inspired Task Offloading
(KATO), designed to efficiently offload tasks from moving vehicles to nearby
RSUs. KATO integrates an attention-inspired encoder-decoder model for selecting
a subset of RSUs that can reduce overall task processing time, along with an
efficient iterative algorithm for computing optimal task allocation among the
selected RSUs. Simulation results demonstrate that KATO achieves optimal or
near-optimal performance with significantly lower computational overhead and
generalizes well across networks of varying sizes and configurations.

</details>


### [35] [SkimROOT: Accelerating LHC Data Filtering with Near-Storage Processing](https://arxiv.org/abs/2506.04507)
*Narangerelt Batsoyol,Jonathan Guiang,Diego Davila,Aashay Arora,Philip Chang,Frank Würthwein,Steven Swanson*

Main category: cs.DC

TL;DR: The abstract introduces SkimROOT, a system leveraging Data Processing Units (DPUs) to filter large datasets directly on storage servers, significantly speeding up data analysis in high-energy physics (HEP) at the LHC by minimizing data movement.


<details>
  <summary>Details</summary>
Motivation: The bottleneck in HEP data analysis at the LHC arises from slow data transfers between storage and compute nodes during the data reduction phase, which delays physics discoveries.

Method: The authors developed SkimROOT, a near-data filtering system that uses DPUs to process data directly on storage servers, reducing irrelevant data movement and minimizing delays.

Result: The prototype of SkimROOT demonstrated a 44.3x performance improvement in data analysis efficiency.

Conclusion: SkimROOT accelerates HEP data analysis by addressing data transfer bottlenecks, enabling faster physics discoveries, and providing a more efficient data filtering solution.

Abstract: Data analysis in high-energy physics (HEP) begins with data reduction, where
vast datasets are filtered to extract relevant events. At the Large Hadron
Collider (LHC), this process is bottlenecked by slow data transfers between
storage and compute nodes. To address this, we introduce SkimROOT, a near-data
filtering system leveraging Data Processing Units (DPUs) to accelerate LHC data
analysis. By performing filtering directly on storage servers and returning
only the relevant data, SkimROOT minimizes data movement and reduces processing
delays. Our prototype demonstrates significant efficiency gains, achieving a
44.3$\times$ performance improvement, paving the way for faster physics
discoveries.

</details>


### [36] [Energy-Optimized Scheduling for AIoT Workloads Using TOPSIS](https://arxiv.org/abs/2506.04902)
*Preethika Pradeep,Eyhab Al-Masri*

Main category: cs.DC

TL;DR: The paper introduces GreenPod, a TOPSIS-based Kubernetes scheduler that improves energy efficiency by up to 39.1% for AIoT workloads in heterogeneous cloud-edge setups.


<details>
  <summary>Details</summary>
Motivation: Default Kubernetes scheduler lacks multi-criteria optimization to efficiently handle AIoT workloads across heterogeneous cloud-edge environments.

Method: Proposes GreenPod, a scheduler based on the TOPSIS decision-making method, optimizing pod placement with multiple metrics such as energy use, execution time, and resource balance.

Result: GreenPod outperforms the default Kubernetes scheduler, demonstrating up to 39.1% energy efficiency improvement, especially with energy-focused weighting schemes.

Conclusion: GreenPod effectively balances sustainability and performance, making it suitable for energy-conscious AIoT applications in heterogeneous cloud-edge infrastructures.

Abstract: AIoT workloads demand energy-efficient orchestration across cloud-edge
infrastructures, but Kubernetes' default scheduler lacks multi-criteria
optimization for heterogeneous environments. This paper presents GreenPod, a
TOPSIS-based scheduler optimizing pod placement based on execution time, energy
consumption, processing core, memory availability, and resource balance. Tested
on a heterogeneous Google Kubernetes cluster, GreenPod improves energy
efficiency by up to 39.1% over the default Kubernetes (K8s) scheduler,
particularly with energy-centric weighting schemes. Medium complexity workloads
showed the highest energy savings, despite slight scheduling latency. GreenPod
effectively balances sustainability and performance for AIoT applications.

</details>


### [37] [FlashDMoE: Fast Distributed MoE in a Single Kernel](https://arxiv.org/abs/2506.04667)
*Osayamen Jonathan Aimuyo,Byungsoo Oh,Rachee Singh*

Main category: cs.DC

TL;DR: FlashDMoE overcomes inefficiencies in Mixture-of-Experts models by developing a GPU-resident operator that boosts performance across latency, throughput, scaling efficiency, and utilization in distributed ML workloads.


<details>
  <summary>Details</summary>
Motivation: Existing MoE implementations struggle with computational inefficiencies such as low GPU utilization, latency overhead, and lack of locality, limiting their scalability for larger neural networks.

Method: FlashDMoE employs a GPU-resident operator, integrating expert computation and inter-GPU communication into a single GPU kernel. It uses fine-grained pipelining and device-initiated communication protocols for efficient data transfer.

Result: On a setup with 8-H100 GPU nodes, FlashDMoE achieves up to 6x lower latency, 5.7x higher throughput, 4x better scaling efficiency, and 9x higher GPU utilization compared to top baselines, operating on FP32 precision.

Conclusion: FlashDMoE highlights the importance of tailored GPU kernel-hardware co-design for overcoming bottlenecks in large-scale neural networks, paving the way for more efficient distributed ML workloads.

Abstract: The computational sparsity of Mixture-of-Experts (MoE) models enables
sub-linear growth in compute cost as model size increases, offering a scalable
path to training massive neural networks. However, existing implementations
suffer from \emph{low GPU utilization}, \emph{significant latency overhead},
and a fundamental \emph{inability to leverage task locality}, primarily due to
CPU-managed scheduling, host-initiated communication, and frequent kernel
launches. To overcome these limitations, we develop FlashDMoE, a fully
GPU-resident MoE operator that fuses expert computation and inter-GPU
communication into a \emph{single persistent GPU kernel}. FlashDMoE enables
fine-grained pipelining of dispatch, compute, and combine phases, eliminating
launch overheads and reducing idle gaps. Its device-initiated communication
protocol introduces \emph{payload-efficient} data transfers, significantly
shrinking buffer sizes in sparsely activated MoE layers. When evaluated on a
single 8-H100 GPU node with MoE models having up to 128 experts and 16K token
sequences, FlashDMoE achieves up to \textbf{6}x lower latency, \textbf{5,7}x
higher throughput, \textbf{4}x better weak scaling efficiency, and \textbf{9}x
higher GPU utilization compared to state-of-the-art baselines, despite using
FP32 while baselines use FP16. FlashDMoE demonstrates that principled GPU
kernel-hardware co-design is key to unlocking the performance ceiling of
large-scale distributed ML workloads.

</details>


### [38] [Distributed system perspective on Backscatter systems](https://arxiv.org/abs/2506.04833)
*Jincheng Guan,Jun Zhang*

Main category: cs.DC

TL;DR: This paper studies the distributed perspective of backscatter communication systems, emphasizing their principles, architectures, applications, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore backscatter communication systems as distributed systems, which are currently understudied from this perspective.

Method: Comprehensive review and analysis of backscatter system principles, architectures, applications, research status, and challenges.

Result: Provides a detailed understanding of distributed system architectures and application scenarios for backscatter systems.

Conclusion: The study offers insights and references on the status, challenges, and future directions of backscatter systems for researchers.

Abstract: Backscatter system is a system based on backscatter communication technology,
which is a low cost, low power consumption and easy to deploy communication
technology. At present, the backscatter technology is mainly applied to RFID
tags and the Internet of Things and other fields. With the rapid development of
the Internet of Things, the application of backscatter systems is increasing.
Moreover, the backscatter system is essentially a distributed system, but
existing research rarely conducts studies and analyses from a distributed
perspective. This paper conducts a study on the backscattering system from the
perspective of distributed systems, comprehensively reviewing the basic
principles of the backscattering system, and analyzing the distributed system
architectures of different backscattering systems. Then, it introduces the
application scenarios, research status and challenges of the backscattering
system, and finally discusses the future research directions of the
backscattering system, hoping to provide references for future research.

</details>


### [39] [A distributed system perspective on Backscatter systems: A review](https://arxiv.org/abs/2506.04873)
*Tonghuan Xiao,Jiecheng Zhou*

Main category: cs.DC

TL;DR: The paper reviews distributed architectures and resource allocation for wireless systems, focusing on backscatter communication, indoor localization, battery-free networks, and SWIPT.


<details>
  <summary>Details</summary>
Motivation: To address challenges in creating robust and scalable wireless systems through intelligent design and architectures.

Method: The study conducts a review of distributed architectures and resource allocation strategies, emphasizing specific wireless communication technologies.

Result: The review identifies how distributed architectures enable improvements in scalability, robustness, and efficiency in specific wireless systems.

Conclusion: Distributed architectures and intelligent resource allocation are critical for advancing robust, scalable, and efficient wireless systems in the studied domains.

Abstract: This review investigates the pivotal role of distributed architectures and
intelligent resource allocation in enabling robust and scalable wireless
systems, with a particular emphasis on backscatter communication, indoor
localization, battery-free networks, and Simultaneous Wireless Information and
Power Transfer (SWIPT).

</details>


### [40] [Improved Byzantine Agreement under an Adaptive Adversary](https://arxiv.org/abs/2506.04919)
*Fabien Dufoulon,Gopal Pandurangan*

Main category: cs.DC

TL;DR: The paper introduces a randomized Byzantine agreement protocol for adaptive adversaries under a rushing adversary in the full information model, improving the round complexity to $O(\min\{t^2\log n/n, t/\log n\})$, outperforming existing bounds.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the harder problem of Byzantine agreement under adaptive adversaries, where the adversary can corrupt nodes dynamically during protocol execution. Existing solutions are either less efficient or cannot handle such adversaries effectively.

Method: The authors propose a synchronous randomized protocol designed to tolerate up to $t < n/3$ corrupted nodes, leveraging adaptive adversaries' full access to network information while maintaining efficiency and robustness.

Result: The new protocol achieves $O(\min\{t^2\log n/n, t/\log n\})$ rounds in the adaptive adversary model, which is a significant improvement over the previously best-known $O(t/\log n)$.

Conclusion: This work advances the state-of-the-art in Byzantine agreement protocols by providing a faster and more efficient solution for adaptive adversaries, setting a new benchmark in the field.

Abstract: Byzantine agreement is a fundamental problem in fault-tolerant distributed
computing that has been studied intensively for the last four decades. Much of
the research has focused on a static Byzantine adversary, where the adversary
is constrained to choose the Byzantine nodes in advance of the protocol's
execution. This work focuses on the harder case of an adaptive Byzantine
adversary that can choose the Byzantine nodes \emph{adaptively} based on the
protocol's execution. While efficient $O(\log n)$-round protocols ($n$ is the
total number of nodes) are known for the static adversary (Goldwasser, Pavlov,
and Vaikuntanathan, FOCS 2006) tolerating up to $t < n/(3+\epsilon)$ Byzantine
nodes, $\Omega(t/\sqrt{n \log n})$ rounds is a well-known lower bound for
adaptive adversary [Bar-Joseph and Ben-Or, PODC 1998]. The best-known protocol
for adaptive adversary runs in $O(t/\log n)$ rounds [Chor and Coan, IEEE Trans.
Soft. Engg., 1985].
  This work presents a synchronous randomized Byzantine agreement protocol
under an adaptive adversary that improves over previous results. Our protocol
works under the powerful \emph{adaptive rushing adversary in the full
information model}. That is, we assume that the Byzantine nodes can behave
arbitrarily and maliciously, have knowledge about the entire state of the
network at every round, including random choices made by all the nodes up to
and including the current round, have unlimited computational power, and may
collude among themselves. Furthermore, the adversary can \emph{adaptively}
corrupt up to $t < n/3$ nodes based on the protocol's execution. We present a
simple randomized Byzantine agreement protocol that runs in $O(\min\{t^2\log
n/n, t/\log n\})$ rounds that improves over the long-standing bound of
$O(t/\log n)$ rounds due to Chor and Coan [IEEE Trans. Soft. Engg., 1985].

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [A Comprehensive Survey on the Risks and Limitations of Concept-based Models](https://arxiv.org/abs/2506.04237)
*Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: This paper offers a survey on the limitations and risks of Concept-based Models, focusing on challenges like concept leakage and model robustness while discussing solutions and future directions.


<details>
  <summary>Details</summary>
Motivation: Concept-based Models are needed to enhance trust in sensitive applications such as medical diagnosis and financial risk assessment but face critical limitations and challenges.

Method: The paper conducts a comprehensive survey, aggregating challenges and proposed solutions for both Supervised and Unsupervised paradigms of Concept-based Models.

Result: It identifies key risks (e.g., concept leakage, entangled representations, low robustness) and discusses architectural strategies to mitigate these issues.

Conclusion: While Concept-based Models hold great promise for explainability, addressing their limitations and exploring future research directions are crucial for their wider adoption and success.

Abstract: Concept-based Models are a class of inherently explainable networks that
improve upon standard Deep Neural Networks by providing a rationale behind
their predictions using human-understandable `concepts'. With these models
being highly successful in critical applications like medical diagnosis and
financial risk prediction, there is a natural push toward their wider adoption
in sensitive domains to instill greater trust among diverse stakeholders.
However, recent research has uncovered significant limitations in the structure
of such networks, their training procedure, underlying assumptions, and their
susceptibility to adversarial vulnerabilities. In particular, issues such as
concept leakage, entangled representations, and limited robustness to
perturbations pose challenges to their reliability and generalization.
Additionally, the effectiveness of human interventions in these models remains
an open question, raising concerns about their real-world applicability. In
this paper, we provide a comprehensive survey on the risks and limitations
associated with Concept-based Models. In particular, we focus on aggregating
commonly encountered challenges and the architecture choices mitigating these
challenges for Supervised and Unsupervised paradigms. We also examine recent
advances in improving their reliability and discuss open problems and promising
avenues of future research in this domain.

</details>


### [42] [Improving Out-of-Distribution Detection with Markov Logic Networks](https://arxiv.org/abs/2506.04241)
*Konstantin Kirchheim,Frank Ortmeier*

Main category: cs.LG

TL;DR: This paper proposes leveraging probabilistic reasoning with Markov logic networks (MLNs) to enhance existing OOD detectors, improving performance and explainability.


<details>
  <summary>Details</summary>
Motivation: Ensuring the reliability of deep models in open-world scenarios through improved OOD detection, as current methods primarily rely on statistical models.

Method: Integrates Markov logic networks (MLNs) with OOD detectors, introduces a way to learn logical constraints for detection from datasets, and uses logical reasoning to assign probabilities.

Result: Demonstrated an improvement in performance across various OOD detectors while maintaining efficiency, confirmed through experiments on multiple datasets.

Conclusion: Probabilistic reasoning with MLNs enhances both accuracy and explainability of OOD detection systems, validating the proposed approach and algorithm.

Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability
of deep learning models operating in open-world scenarios. Current OOD
detectors mainly rely on statistical models to identify unusual patterns in the
latent representations of a deep neural network. This work proposes to augment
existing OOD detectors with probabilistic reasoning, utilizing Markov logic
networks (MLNs). MLNs connect first-order logic with probabilistic reasoning to
assign probabilities to inputs based on weighted logical constraints defined
over human-understandable concepts, which offers improved explainability.
Through extensive experiments on multiple datasets, we demonstrate that MLNs
can significantly enhance the performance of a wide range of existing OOD
detectors while maintaining computational efficiency. Furthermore, we introduce
a simple algorithm for learning logical constraints for OOD detection from a
dataset and showcase its effectiveness.

</details>


### [43] [Triple Attention Transformer Architecture for Time-Dependent Concrete Creep Prediction](https://arxiv.org/abs/2506.04243)
*Warayut Dokduea,Weerachart Tangchirapat,Sompote Youwai*

Main category: cs.LG

TL;DR: This paper introduces a Triple Attention Transformer Architecture for autoregressively predicting time-dependent concrete creep using time as a sequential parameter, achieving high accuracy and outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current approaches to concrete creep prediction inadequately model time as a mere input parameter instead of considering the sequential nature of deformation development.

Method: The paper uses a Triple Attention Transformer framework incorporating temporal, feature, and batch attention for autoregressive sequence modeling of historical creep patterns.

Result: The presented model achieves a mean absolute percentage error of 1.63% and R2 values of 0.999 on experimental datasets spanning 160 days, surpassing traditional empirical models and other machine learning methods.

Conclusion: The research establishes transformers as effective tools for materials science, enabling accurate predictions and interpretability, and provides practical implementation via a web-based interface.

Abstract: This paper presents a novel Triple Attention Transformer Architecture for
predicting time-dependent concrete creep, addressing fundamental limitations in
current approaches that treat time as merely an input parameter rather than
modeling the sequential nature of deformation development. By transforming
concrete creep prediction into an autoregressive sequence modeling task similar
to language processing, our architecture leverages the transformer's
self-attention mechanisms to capture long-range dependencies in historical
creep patterns. The model implements a triple-stream attention framework
incorporating temporal attention for sequential progression, feature attention
for material property interactions, and batch attention for inter-sample
relationships. Evaluated on experimental datasets with standardized daily
measurements spanning 160 days, the architecture achieves exceptional
performance with mean absolute percentage error of 1.63% and R2 values of 0.999
across all datasets, substantially outperforming traditional empirical models
and existing machine learning approaches. Ablation studies confirm the critical
role of attention mechanisms, with attention pooling contributing most
significantly to model performance. SHAP analysis reveals Young's modulus as
the primary predictive feature, followed by density and compressive strength,
providing interpretability essential for engineering applications. A deployed
web-based interface facilitates practical implementation, enabling real-time
predictions using standard laboratory parameters. This work establishes the
viability of applying transformer architectures to materials science problems,
demonstrating the potential for data-driven approaches to revolutionize
structural behavior prediction and engineering design practices.

</details>


### [44] [SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs](https://arxiv.org/abs/2506.04250)
*Shaona Ghosh,Amrita Bhattacharjee,Yftah Ziser,Christopher Parisien*

Main category: cs.LG

TL;DR: The paper introduces SafeSteer, a technique for safety steering in LLMs using category-specific steering vectors and a gradient-free unsupervised method, showcasing its efficacy without hard pairwise safe data.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs for updated safety policies is expensive and impractical, necessitating a more efficient safety control approach.

Method: The paper employs category-specific steering vectors combined with a simple, unsupervised gradient-free method to adjust LLM outputs for safety, avoiding the need for explicit safe data pairs.

Result: SafeSteer demonstrates precise safety control across multiple LLMs, datasets, and risk categories, preserving text quality and topic relevance without blanket refusals.

Conclusion: The SafeSteer method is a simple, effective, and customizable approach for safety adjustments in LLMs, aligning with modern preferences for simplicity in latent activation steering techniques.

Abstract: Fine-tuning large language models (LLMs) to adapt to evolving safety policies
is costly and impractical. Mechanistic interpretability enables inference-time
control through latent activation steering, yet its potential for precise,
customizable safety adjustments remains largely untapped. This paper
investigates an approach called SafeSteer for guiding the outputs of LLMs by:
(i) leveraging category-specific steering vectors for more precise control,
(ii) employing a simple, gradient-free unsupervised method to enhance safety
steering while preserving text quality, topic relevance, and without explicit
refusal, and (iii) accomplishing this without a hard requirement of contrastive
pairwise safe data. We also highlight that our method, being simple and
effective, aligns with recent studies suggesting that simple techniques often
outperform more complex ones in activation steering. We showcase the
effectiveness of our approach across various LLMs, datasets, and risk
categories, demonstrating its ability to provide precise control, prevent
blanket refusals, and guide models toward generating safe content while
maintaining topic relevance.

</details>


### [45] [The Cost of Dynamic Reasoning: Demystifying AI Agents and Test-Time Scaling from an AI Infrastructure Perspective](https://arxiv.org/abs/2506.04301)
*Jiin Kim,Byeongjun Shin,Jinha Chung,Minsoo Rhu*

Main category: cs.LG

TL;DR: The paper conducts a thorough analysis of large-language-model-based AI agents, focusing on their resource usage, efficiency, and sustainability challenges.


<details>
  <summary>Details</summary>
Motivation: To address the growing concerns regarding the efficiency, cost, and sustainability of AI agents that use dynamic, multi-turn reasoning workflows.

Method: The authors perform a system-level analysis to quantify resource usage, latency, energy consumption, and power demands in datacenters, exploring factors like prompting, reasoning depth, and parallel processes.

Result: AI agents exhibit accuracy improvements with higher compute usage but face diminishing returns, high latency variance, and unsustainable infrastructure demands.

Conclusion: A paradigm shift toward compute-efficient reasoning in AI agent design is necessary to ensure balance between performance and long-term deployability.

Abstract: Large-language-model (LLM)-based AI agents have recently showcased impressive
versatility by employing dynamic reasoning, an adaptive, multi-step process
that coordinates with external tools. This shift from static, single-turn
inference to agentic, multi-turn workflows broadens task generalization and
behavioral flexibility, but it also introduces serious concerns about
system-level cost, efficiency, and sustainability. This paper presents the
first comprehensive system-level analysis of AI agents, quantifying their
resource usage, latency behavior, energy consumption, and datacenter-wide power
consumption demands across diverse agent designs and test-time scaling
strategies. We further characterize how AI agent design choices, such as
few-shot prompting, reflection depth, and parallel reasoning, impact
accuracy-cost tradeoffs. Our findings reveal that while agents improve accuracy
with increased compute, they suffer from rapidly diminishing returns, widening
latency variance, and unsustainable infrastructure costs. Through detailed
evaluation of representative agents, we highlight the profound computational
demands introduced by AI agent workflows, uncovering a looming sustainability
crisis. These results call for a paradigm shift in agent design toward
compute-efficient reasoning, balancing performance with deployability under
real-world constraints.

</details>


### [46] [Localized Forest Fire Risk Prediction: A Department-Aware Approach for Operational Decision Support](https://arxiv.org/abs/2506.04254)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: This paper proposes a locally-tailored AI approach to predict forest fire risks in metropolitan France, moving beyond traditional binary prediction models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in traditional binary classification models for fire risks, which don't account for local variations and operational needs of firefighting units.

Method: The study utilizes a new AI benchmark tailored for metropolitan France, applied to a relatively unexplored dataset, and employs state-of-the-art AI models.

Result: The methodology provides more region-specific, actionable fire risk predictions tailored to departmental contexts within metropolitan France.

Conclusion: The proposed approach demonstrates the feasibility and importance of localized fire prediction models and underscores the need for regionally-sensitive risk assessments in future research.

Abstract: Forest fire prediction involves estimating the likelihood of fire ignition or
related risk levels in a specific area over a defined time period. With climate
change intensifying fire behavior and frequency, accurate prediction has become
one of the most pressing challenges in Artificial Intelligence (AI).
Traditionally, fire ignition is approached as a binary classification task in
the literature. However, this formulation oversimplifies the problem,
especially from the perspective of end-users such as firefighters. In general,
as is the case in France, firefighting units are organized by department, each
with its terrain, climate conditions, and historical experience with fire
events. Consequently, fire risk should be modeled in a way that is sensitive to
local conditions and does not assume uniform risk across all regions. This
paper proposes a new approach that tailors fire risk assessment to departmental
contexts, offering more actionable and region-specific predictions for
operational use. With this, we present the first national-scale AI benchmark
for metropolitan France using state-of-the-art AI models on a relatively
unexplored dataset. Finally, we offer a summary of important future works that
should be taken into account. Supplementary materials are available on GitHub.

</details>


### [47] [Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing](https://arxiv.org/abs/2506.04523)
*Cliff B. Abbott,Mark Elo,Dmytro A. Bozhko*

Main category: cs.LG

TL;DR: The paper presents Perturbative Gradient Training (PGT), a method to perform effective gradient updates using random parameter perturbations, overcoming the inability to backpropagate in physical reservoirs.


<details>
  <summary>Details</summary>
Motivation: Physical reservoirs have shown promise in AI systems but are limited due to the inability to perform backpropagation. The authors seek to address this bottleneck and enable their integration into deeper architectures.

Method: PGT leverages random perturbations of parameters in physical reservoirs and utilizes forward pass computations to approximate gradient updates without relying on traditional backpropagation methods.

Result: Experimental evaluation on simulated architectures and physical hardware demonstrated that PGT delivers comparable performance to standard backpropagation while enabling energy-efficient AI training.

Conclusion: PGT opens new avenues for incorporating physical reservoirs into complex neural networks with enhanced energy efficiency, addressing limitations of standard backpropagation systems.

Abstract: We introduce Perturbative Gradient Training (PGT), a novel training paradigm
that overcomes a critical limitation of physical reservoir computing: the
inability to perform backpropagation due to the black-box nature of physical
reservoirs. Drawing inspiration from perturbation theory in physics, PGT uses
random perturbations in the network's parameter space to approximate gradient
updates using only forward passes. We demonstrate the feasibility of this
approach on both simulated neural network architectures, including a dense
network and a transformer model with a reservoir layer, and on experimental
hardware using a magnonic auto-oscillation ring as the physical reservoir. Our
results show that PGT can achieve performance comparable to that of standard
backpropagation methods in cases where backpropagation is impractical or
impossible. PGT represents a promising step toward integrating physical
reservoirs into deeper neural network architectures and achieving significant
energy efficiency gains in AI training.

</details>


### [48] [MUC-G4: Minimal Unsat Core-Guided Incremental Verification for Deep Neural Network Compression](https://arxiv.org/abs/2506.04268)
*Jingyang Li,Guoqiang Li*

Main category: cs.LG

TL;DR: The paper introduces MUC-G4, a verification method for compressed neural networks, effectively handling quantization and pruning with faster results.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often require high memory and runtime, making them challenging to deploy on edge devices. Existing network compression techniques like quantization and pruning maintain accuracy but lack robust verification methods for structural changes.

Method: MUC-G4 encodes both original and compressed networks into SMT formulas, classifies changes, and uses Minimal Unsat Cores (MUCs) from the original network to guide efficient verification for the compressed network.

Result: MUC-G4 achieved high proof reuse rates and significantly faster verification time compared to traditional methods, handling both quantization and pruning effectively.

Conclusion: MUC-G4 is a promising framework for improving the safety and reliability of compressed neural networks in practical settings.

Abstract: The rapid development of deep learning has led to challenges in deploying
neural networks on edge devices, mainly due to their high memory and runtime
complexity. Network compression techniques, such as quantization and pruning,
aim to reduce this complexity while maintaining accuracy. However, existing
incremental verification methods often focus only on quantization and struggle
with structural changes. This paper presents MUC-G4 (Minimal Unsat Core-Guided
Incremental Verification), a novel framework for incremental verification of
compressed deep neural networks. It encodes both the original and compressed
networks into SMT formulas, classifies changes, and use \emph{Minimal Unsat
Cores (MUCs)} from the original network to guide efficient verification for the
compressed network. Experimental results show its effectiveness in handling
quantization and pruning, with high proof reuse rates and significant speedup
in verification time compared to traditional methods. MUC-G4 hence offers a
promising solution for ensuring the safety and reliability of compressed neural
networks in practical applications.

</details>


### [49] [Understanding the Impact of Sampling Quality in Direct Preference Optimization](https://arxiv.org/abs/2506.04272)
*Kyung Rok Kim,Yumo Bai,Chonghuan Wang,Guanting Chen*

Main category: cs.LG

TL;DR: The paper investigates how the sampling distribution affects Direct Preference Optimization (DPO) and its training dynamics, showing its influence on solution space, convergence, and policy learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the role of response distribution in shaping DPO training dynamics and to establish a theoretical foundation for its practical applications.

Method: The paper used a combination of theoretical analysis and empirical experiments, including designing a simplified alignment model to assess how response quality impacts gradient signals and policy optimization.

Result: It was found that high-quality, frequent responses improve the optimization landscape by amplifying gradient signals, facilitating more effective policy learning.

Conclusion: The findings provide a theoretical justification for employing online DPO frameworks, emphasizing the importance of carefully managing the sampling distribution.

Abstract: We study the role of the sampling distribution in Direct Preference
Optimization (DPO) and aim to understand its impact on DPO's training dynamics.
Our analyses show that both the solution space and the convergence behavior of
DPO depend on the support and quality of the generating distribution. We first
analyze how distribution of responses influences policy updates during gradient
descent, drawing connections to common phenomena found in practice. We then
design a simplified yet well-structured alignment model as a proxy, and develop
quantitative results showing how more frequent high-quality responses amplify
the gradient signal and improve the optimization landscape, leading to more
effective policy learning. Our theoretical findings are supported by empirical
experiments and provide a principled justification for the online DPO framework
in practice.

</details>


### [50] [Relational reasoning and inductive bias in transformers trained on a transitive inference task](https://arxiv.org/abs/2506.04289)
*Jesse Geerts,Stephanie Chan,Claudia Clopath,Kimberly Stachenfeld*

Main category: cs.LG

TL;DR: This paper explores the mechanisms of relational reasoning in transformers by studying transitive inference under two learning regimes: "in-weights learning" (IWL) and "in-context learning" (ICL).


<details>
  <summary>Details</summary>
Motivation: Investigate how transformers perform relational reasoning tasks like transitive inference, a task well-documented in psychological studies, and understand differences in reasoning across learning regimes.

Method: Analyzed transformers' ability to perform transitive inference in IWL and ICL settings, comparing behavior when trained solely on adjacent items. Mechanistic analysis was conducted, and experiments included pre-training on in-context linear regression tasks.

Result: IWL models exhibited natural bias towards transitive inference and generalized well, while ICL models trained on adjacent items failed to generalize. Pre-training on structured tasks improved ICL transitive inference performance, showing symbolic distance and terminal item effects akin to human and animal cognition.

Conclusion: Pre-training on structured tasks aids in developing representations that facilitate relational reasoning within the in-context learning paradigm, differing from the mechanisms observed in IWL.

Abstract: Transformer-based models have demonstrated remarkable reasoning abilities,
but the mechanisms underlying relational reasoning in different learning
regimes remain poorly understood. In this work, we investigate how transformers
perform a classic relational reasoning task from the Psychology literature,
\textit{transitive inference}, which requires inference about indirectly
related items by integrating information across observed adjacent item pairs
(e.g., if A>B and B>C, then A>C). We compare transitive inference behavior
across two distinct learning regimes: in-weights learning (IWL), where models
store information in network parameters, and in-context learning (ICL), where
models flexibly utilize information presented within the input sequence. Our
findings reveal that IWL naturally induces a generalization bias towards
transitive inference, despite being trained only on adjacent items, whereas ICL
models trained solely on adjacent items do not generalize transitively.
Mechanistic analysis shows that ICL models develop induction circuits that
implement a simple match-and-copy strategy that performs well at relating
adjacent pairs, but does not encoding hierarchical relationships among
indirectly related items. Interestingly, when pre-trained on in-context linear
regression tasks, transformers successfully exhibit in-context generalizable
transitive inference. Moreover, like IWL, they display both \textit{symbolic
distance} and \textit{terminal item effects} characteristic of human and animal
performance, without forming induction circuits. These results suggest that
pre-training on tasks with underlying structure promotes the development of
representations that can scaffold in-context relational reasoning.

</details>


### [51] [Inference economics of language models](https://arxiv.org/abs/2506.04645)
*Ege Erdil*

Main category: cs.LG

TL;DR: The paper presents a model to optimize the trade-off between cost per token and serial inference speed when using large language models (LLMs) at scale.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of balancing cost efficiency and inference speed in deploying LLMs for scalable applications, which is crucial for maximizing practical utility.

Method: The paper employs a theoretical model considering constraints like arithmetic, memory, network bandwidth, and latency to optimize parallelism setups and batch sizes.

Result: The study computes Pareto frontiers showcasing the relationship between serial inference speed and cost efficiency for popular language models.

Conclusion: The paper offers actionable insights into optimizing LLM inference setups for better performance and economic feasibility.

Abstract: We develop a theoretical model that addresses the economic trade-off between
cost per token versus serial token generation speed when deploying LLMs for
inference at scale. Our model takes into account arithmetic, memory bandwidth,
network bandwidth and latency constraints; and optimizes over different
parallelism setups and batch sizes to find the ones that optimize serial
inference speed at a given cost per token. We use the model to compute Pareto
frontiers of serial speed versus cost per token for popular language models.

</details>


### [52] [SF$^2$Bench: Evaluating Data-Driven Models for Compound Flood Forecasting in South Florida](https://arxiv.org/abs/2506.04281)
*Xu Zheng,Chaohao Lin,Sipeng Chen,Zhuomin Chen,Jimeng Shi,Wei Cheng,Jayantha Obeysekera,Jason Liu,Dongsheng Luo*

Main category: cs.LG

TL;DR: Compound floods are challenging to forecast due to the interaction of various environmental factors. The paper introduces SF2Bench, a comprehensive dataset to analyze these floods and evaluates the performance of six machine learning methods.


<details>
  <summary>Details</summary>
Motivation: Compound floods are becoming more critical in light of increasing global flood risks, yet forecasting them is hindered by the limitations of existing datasets and time-intensive physics-based methods.

Method: The paper introduces the SF2Bench dataset, which incorporates tide, rainfall, groundwater, and human management activities. It evaluates six machine learning methods to analyze temporal and spatial aspects affecting flood forecasting.

Result: The analysis highlights the varied capabilities of ML methods in handling complex temporal and spatial dependencies, with experiments verifying the impact of key features on forecasting.

Conclusion: The integration of multifactorial data in SF2Bench and the comparative evaluation of modeling methods provide a foundation for enhancing flood forecasting approaches and understanding compound flood dynamics.

Abstract: Forecasting compound floods presents a significant challenge due to the
intricate interplay of meteorological, hydrological, and oceanographic factors.
Analyzing compound floods has become more critical as the global climate
increases flood risks. Traditional physics-based methods, such as the
Hydrologic Engineering Center's River Analysis System, are often
time-inefficient. Machine learning has recently demonstrated promise in both
modeling accuracy and computational efficiency. However, the scarcity of
comprehensive datasets currently hinders systematic analysis. Existing
water-related datasets are often limited by a sparse network of monitoring
stations and incomplete coverage of relevant factors. To address this
challenge, we introduce SF2Bench, a comprehensive time series collection on
compound floods in South Florida, which integrates four key factors: tide,
rainfall, groundwater, and human management activities (gate and pump
controlling). This integration allows for a more detailed analysis of the
individual contributions of these drivers to compound flooding and informs the
development of improved flood forecasting approaches. To comprehensively
evaluate the potential of various modeling paradigms, we assess the performance
of six categories of methods, encompassing Multilayer Perceptrons,
Convolutional Neural Networks, Recurrent Neural Networks, Graph Neural
Networks, Transformers, and Large Language Models. We verified the impact of
different key features on flood forecasting through experiments. Our analysis
examines temporal and spatial aspects, providing insights into the influence of
historical data and spatial dependencies. The varying performance across these
approaches underscores the diverse capabilities of each in capturing complex
temporal and spatial dependencies inherent in compound floods.

</details>


### [53] [DrSR: LLM based Scientific Equation Discovery with Dual Reasoning from Data and Experience](https://arxiv.org/abs/2506.04282)
*Runxiang Wang,Boxiao Wang,Kai Li,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: DrSR (Dual Reasoning Symbolic Regression) is a framework that enhances symbolic regression by combining data-driven insights and reflective learning, outperforming traditional and LLM-based methods in multiple domains.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of LLM-based symbolic regression methods, which tend to rely excessively on internal priors and lack explicit data analysis and systematic feedback.

Method: The proposed method, DrSR, guides large language models to analyze data's structural relationships and uses a feedback loop to refine equation generation iteratively.

Result: DrSR consistently achieves better accuracy, generalization, and efficiency compared to classical and modern LLM-based symbolic regression methods across datasets from various scientific domains.

Conclusion: DrSR demonstrates substantial improvements in equation discovery, emphasizing its utility in scientific applications by integrating data understanding and reflective learning.

Abstract: Symbolic regression is a fundamental tool for discovering interpretable
mathematical expressions from data, with broad applications across scientific
and engineering domains. Recently, large language models (LLMs) have
demonstrated strong performance in this task, leveraging embedded scientific
priors and reasoning capabilities to surpass traditional methods. However,
existing LLM-based approaches, such as LLM-SR, often over-rely on internal
priors, lacking explicit data understanding and systematic reflection during
equation generation. To address these limitations, we propose DrSR (Dual
Reasoning Symbolic Regression), a framework that combines data-driven insight
with reflective learning to enhance both robustness and discovery capability.
Specifically, DrSR guides LLMs to analyze structural relationships (e.g.,
monotonicity, nonlinearity, and correlation) within the data to generate
structured descriptions. Simultaneously, it monitors equation performance and
establishes a feedback loop to refine subsequent generations. By integrating
data understanding and generation reflection in a closed loop, DrSR enables
more efficient exploration of the symbolic expression space. Experiments across
interdisciplinary datasets in physics, chemistry, biology, and materials
science demonstrate that DrSR substantially improves the valid equation rate
and consistently outperforms both classical and recent LLM-based methods in
terms of accuracy, generalization, and search efficiency. These results
underscore its potential for scientific equation discovery.

</details>


### [54] [NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models](https://arxiv.org/abs/2506.04536)
*Luca Ghafourpour,Valentin Duruisseaux,Bahareh Tolooshams,Philip H. Wong,Costas A. Anastassiou,Anima Anandkumar*

Main category: cs.LG

TL;DR: The paper introduces NOBLE, a neural operator framework that predicts neural dynamics while addressing variability and computational efficiency limitations in current neuron modeling approaches.


<details>
  <summary>Details</summary>
Motivation: Existing neuron modeling methods are restricted by limited experimental data, intrinsic variability, and computationally intensive processes. Deep learning methods fail to effectively capture the biophysical complexity of neurons.

Method: NOBLE uses a neural operator framework to map neuron characteristics into somatic voltage responses. It relies on a frequency-modulated embedding trained on data from biophysically realistic models and interpolates within this space for consistency.

Result: NOBLE achieves a 4200x speedup compared to numerical solvers and is validated using real experimental data. It generates synthetic neurons that incorporate trial-to-trial variability and align with experimentally observed dynamics.

Conclusion: NOBLE represents a significant advancement in scalable, bio-realistic neuron modeling, enhancing our ability to study brain function, cellular computations, and enabling broader neuroAI applications.

Abstract: Characterizing the diverse computational properties of human neurons via
multimodal electrophysiological, transcriptomic, and morphological data
provides the foundation for constructing and validating bio-realistic neuron
models that can advance our understanding of fundamental mechanisms underlying
brain function. However, current modeling approaches remain constrained by the
limited availability and intrinsic variability of experimental neuronal data.
To capture variability, ensembles of deterministic models are often used, but
are difficult to scale as model generation requires repeating computationally
expensive optimization for each neuron. While deep learning is becoming
increasingly relevant in this space, it fails to capture the full biophysical
complexity of neurons, their nonlinear voltage dynamics, and variability. To
address these shortcomings, we introduce NOBLE, a neural operator framework
that learns a mapping from a continuous frequency-modulated embedding of
interpretable neuron features to the somatic voltage response induced by
current injection. Trained on data generated from biophysically realistic
neuron models, NOBLE predicts distributions of neural dynamics accounting for
the intrinsic experimental variability. Unlike conventional bio-realistic
neuron models, interpolating within the embedding space offers models whose
dynamics are consistent with experimentally observed responses. NOBLE is the
first scaled-up deep learning framework validated on real experimental data,
enabling efficient generation of synthetic neurons that exhibit trial-to-trial
variability and achieve a $4200\times$ speedup over numerical solvers. To this
end, NOBLE captures fundamental neural properties, opening the door to a better
understanding of cellular composition and computations, neuromorphic
architectures, large-scale brain circuits, and general neuroAI applications.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [55] [A Comprehensive Survey on Bio-Inspired Algorithms: Taxonomy, Applications, and Future Directions](https://arxiv.org/abs/2506.04238)
*Shriyank Somvanshi,Md Monzurul Islam,Syed Aaqib Javed,Gaurab Chhetri,Kazi Sifatul Islam,Tausif Islam Chowdhury,Sazzad Bin Bashar Polock,Anandi Dutta,Subasish Das*

Main category: cs.NE

TL;DR: The paper categorizes Bio-inspired Algorithms (BIAs) into eight groups and reviews their applications, strengths, limitations, and future challenges.


<details>
  <summary>Details</summary>
Motivation: The aim is to provide researchers and practitioners with an in-depth understanding of BIAs and guide future research efforts.

Method: The survey organizes BIAs into eight categories and examines their principles, applications, advancements, and challenges.

Result: The paper summarizes key developments in hybridization, parameter tuning, adaptive strategies, and the application of BIAs in diverse fields.

Conclusion: The review highlights open challenges in BIAs such as scalability, reliability, and interpretability, serving as a roadmap for ongoing research.

Abstract: Bio-inspired algorithms (BIAs) utilize natural processes such as evolution,
swarm behavior, foraging, and plant growth to solve complex, nonlinear,
high-dimensional optimization problems. This survey categorizes BIAs into eight
groups: evolutionary, swarm intelligence, physics-inspired, ecosystem and
plant-based, predator-prey, neural-inspired, human-inspired, and hybrid
approaches, and reviews their core principles, strengths, and limitations. We
illustrate the usage of these algorithms in machine learning, engineering
design, bioinformatics, and intelligent systems, and highlight recent advances
in hybridization, parameter tuning, and adaptive strategies. Finally, we
identify open challenges such as scalability, convergence, reliability, and
interpretability to suggest directions for future research. This work aims to
serve as a foundational resource for both researchers and practitioners
interested in understanding the current landscape and future directions of
bio-inspired computing.

</details>


### [56] [Quantum-Inspired Genetic Optimization for Patient Scheduling in Radiation Oncology](https://arxiv.org/abs/2506.04328)
*Akira SaiToh,Arezoo Modiri,Amit Sawant,Robabeh Rahimi*

Main category: cs.NE

TL;DR: The paper discusses using quantum-inspired genetic algorithms (QGA) for patient scheduling in proton therapy, highlighting advantages in convergence quality and resource efficiency but noting limitations in runtime for larger cases.


<details>
  <summary>Details</summary>
Motivation: Quantum-inspired genetic algorithms hold the potential to improve optimization problems but require validation in real-world applications like patient scheduling for proton therapy.

Method: The authors used QGAs with quantum chromosomes for patient IDs and gantry statuses. They introduced selection and repair strategies to ensure clinically usable scheduling.

Result: QGA demonstrated clear advantages in population size and convergence quality over classical methods in both medium-size and large-size test cases, but runtime was long for large problems due to classical emulation.

Conclusion: QGA offers promising improvements in scheduling optimization, but practical implementation awaits advancements in quantum computing. Classical genetic algorithms remain stable and reliable for now.

Abstract: Among the genetic algorithms generally used for optimization problems in the
recent decades, quantum-inspired variants are known for fast and high-fitness
convergence and small resource requirement. Here the application to the patient
scheduling problem in proton therapy is reported. Quantum chromosomes are
tailored to possess the superposed data of patient IDs and gantry statuses.
Selection and repair strategies are also elaborated for reliable convergence to
a clinically feasible schedule although the employed model is not complex.
Clear advantage in population size is shown over the classical counterpart in
our numerical results for both a medium-size test case and a large-size
practical problem instance. It is, however, observed that program run time is
rather long for the large-size practical case, which is due to the limitation
of classical emulation and demands the forthcoming true quantum computation.
Our results also revalidate the stability of the conventional classical genetic
algorithm.

</details>


### [57] [NEAT and HyperNEAT based Design for Soft Actuator Controllers](https://arxiv.org/abs/2506.04698)
*Hugo Alcaraz-Herrera,Michail-Antisthenis Tsompanas,Igor Balaz,Andrew Adamatzky*

Main category: cs.NE

TL;DR: This paper explores Neuroevolution methods, specifically NEAT and HyperNEAT, for developing controllers of soft actuators, and finds NEAT to perform better in robustness and simplicity.


<details>
  <summary>Details</summary>
Motivation: Soft robotics excel in applications requiring compliance, such as medical tasks, but the design of their controllers and morphology is understudied.

Method: The authors proposed automated methods for designing soft actuator controllers using Neuroevolution techniques (NEAT and HyperNEAT), employing CPPNs and comparing them with a Standard Genetic Algorithm.

Result: The experimental results showed that NEAT outperformed other methods across different scenarios, producing simpler and practical network designs.

Conclusion: Neuroevolution methods, particularly NEAT, are more effective for designing controllers for soft robotics, offering robust and implementable solutions.

Abstract: Since soft robotics are composed of compliant materials, they perform better
than conventional rigid robotics in specific fields, such as medical
applications. However, the field of soft robotics is fairly new, and the design
process of their morphology and their controller strategies has not yet been
thoroughly studied. Consequently, here, an automated design method for the
controller of soft actuators based on Neuroevolution is proposed. Specifically,
the suggested techniques employ Neuroevolution of Augmenting Topologies (NEAT)
and Hypercube-based NEAT (HyperNEAT) to generate the synchronization profile of
the components of a simulated soft actuator by employing Compositional Pattern
Producing Networks (CPPNs). As a baseline methodology, a Standard Genetic
Algorithm (SGA) was used. Moreover, to test the robustness of the proposed
methodologies, both high- and low-performing morphologies of soft actuators
were utilized as testbeds. Moreover, the use of an affluent and a more limited
set of activation functions for the Neuroevolution targets was tested
throughout the experiments. The results support the hypothesis that
Neuroevolution based methodologies are more appropriate for designing
controllers that align with both types of morphologies. In specific, NEAT
performed better for all different scenarios tested and produced more
simplistic networks that are easier to implement in real life applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [58] [RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2506.04308)
*Enshen Zhou,Jingkun An,Cheng Chi,Yi Han,Shanyu Rong,Chi Zhang,Pengwei Wang,Zhongyuan Wang,Tiejun Huang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboRefer is a 3D-aware vision-language model (VLM) designed for precise spatial reasoning and understanding in robots, achieving state-of-the-art accuracy through innovative training methods and datasets.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing models in spatial reasoning and dynamic location understanding for embodied robots in 3D environments.

Method: The model integrates a depth encoder with supervised fine-tuning (SFT) and uses reinforcement fine-tuning (RFT) with metric-sensitive reward functions. It is trained using a new large-scale dataset, RefSpatial, and evaluated with a custom benchmark, RefSpatial-Bench.

Result: RoboRefer achieves an 89.6% success rate in spatial understanding and outperforms existing methods by a significant margin in multi-step spatial reasoning tasks, particularly on RefSpatial-Bench.

Conclusion: RoboRefer demonstrates superior spatial reasoning abilities and versatility for robotic tasks, making it suitable for real-world dynamic and complex environments.

Abstract: Spatial referring is a fundamental capability of embodied robots to interact
with the 3D physical world. However, even with the powerful pretrained vision
language models (VLMs), recent approaches are still not qualified to accurately
understand the complex 3D scenes and dynamically reason about the
instruction-indicated locations for interaction. To this end, we propose
RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding
by integrating a disentangled but dedicated depth encoder via supervised
fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial
reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process
reward functions tailored for spatial referring tasks. To support SFT and RFT
training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x
prior), covering 31 spatial relations (vs. 15 prior) and supporting complex
reasoning processes (up to 5 steps). In addition, we introduce
RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial
referring with multi-step reasoning. Experiments show that SFT-trained
RoboRefer achieves state-of-the-art spatial understanding, with an average
success rate of 89.6%. RFT-trained RoboRefer further outperforms all other
baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average
accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various
control policies to execute long-horizon, dynamic tasks across diverse robots
(e,g., UR5, G1 humanoid) in cluttered real-world scenes.

</details>


### [59] [cuVSLAM: CUDA accelerated visual odometry](https://arxiv.org/abs/2506.04359)
*Alexander Korovko,Dmitry Slepichev,Alexander Efitorov,Aigul Dzhumamuratova,Viktor Kuznetsov,Hesam Rabeti,Joydeep Biswas*

Main category: cs.RO

TL;DR: cuVSLAM is an advanced visual SLAM system optimized for real-time performance on edge devices, supporting diverse camera and sensor configurations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of accurate and robust pose estimation, a critical requirement for autonomous robotics.

Method: The authors introduce cuVSLAM, which uses CUDA for real-time optimization and supports flexible visual-inertial sensor combinations, suitable for edge devices.

Result: cuVSLAM demonstrates superior performance on state-of-the-art benchmarks, showing both versatility and high accuracy.

Conclusion: cuVSLAM proves to be a robust and scalable SLAM solution optimized for robotic applications, particularly on computationally constrained edge devices.

Abstract: Accurate and robust pose estimation is a key requirement for any autonomous
robot. We present cuVSLAM, a state-of-the-art solution for visual simultaneous
localization and mapping, which can operate with a variety of visual-inertial
sensor suites, including multiple RGB and depth cameras, and inertial
measurement units. cuVSLAM supports operation with as few as one RGB camera to
as many as 32 cameras, in arbitrary geometric configurations, thus supporting a
wide range of robotic setups. cuVSLAM is specifically optimized using CUDA to
deploy in real-time applications with minimal computational overhead on
edge-computing devices such as the NVIDIA Jetson. We present the design and
implementation of cuVSLAM, example use cases, and empirical results on several
state-of-the-art benchmarks demonstrating the best-in-class performance of
cuVSLAM.

</details>


### [60] [Learning Smooth State-Dependent Traversability from Dense Point Clouds](https://arxiv.org/abs/2506.04362)
*Zihao Dong,Alan Papalia,Leonard Jung,Alenna Spiro,Philip R. Osteen,Christa S. Robison,Michael Everett*

Main category: cs.RO

TL;DR: SPARTA is a method proposed to estimate traversability conditioned on approach angles using point clouds, addressing the challenges of state-dependent terrain traversability in off-road autonomy.


<details>
  <summary>Details</summary>
Motivation: To address the dependency of terrain traversability on vehicle states, specifically approach angles, which complicates learning and computational efficiency.

Method: SPARTA uses Fourier basis functions to model smooth analytical traversability functions over approach angles from point clouds, imposing geometric structure for efficient planning and generalization.

Result: The method achieves a 91% success rate crossing a challenging boulder field in simulation and demonstrates effective generalization on hardware in real-world settings.

Conclusion: SPARTA effectively balances computational efficiency and learning for state-conditioned terrain traversability, showing strong performance and generalization potential.

Abstract: A key open challenge in off-road autonomy is that the traversability of
terrain often depends on the vehicle's state. In particular, some obstacles are
only traversable from some orientations. However, learning this interaction by
encoding the angle of approach as a model input demands a large and diverse
training dataset and is computationally inefficient during planning due to
repeated model inference. To address these challenges, we present SPARTA, a
method for estimating approach angle conditioned traversability from point
clouds. Specifically, we impose geometric structure into our network by
outputting a smooth analytical function over the 1-Sphere that predicts risk
distribution for any angle of approach with minimal overhead and can be reused
for subsequent queries. The function is composed of Fourier basis functions,
which has important advantages for generalization due to their periodic nature
and smoothness. We demonstrate SPARTA both in a high-fidelity simulation
platform, where our model achieves a 91\% success rate crossing a 40m boulder
field (compared to 73\% for the baseline), and on hardware, illustrating the
generalization ability of the model to real-world settings.

</details>


### [61] [Online Adaptation of Terrain-Aware Dynamics for Planning in Unstructured Environments](https://arxiv.org/abs/2506.04484)
*William Ward,Sarah Etter,Tyler Ingebrand,Christian Ellis,Adam J. Thorpe,Ufuk Topcu*

Main category: cs.RO

TL;DR: The paper proposes a method for mobile robots to adapt their dynamics models to unpredictable terrains rapidly using limited online data, without retraining.


<details>
  <summary>Details</summary>
Motivation: Autonomous robots need the ability to navigate rapidly changing terrains in unpredictable environments reliably.

Method: The authors use neural network basis functions to span robot dynamics across terrains, enabling runtime adaptation through simple calculations.

Result: In simulations, the approach improves navigation accuracy, reduces collisions, and outperforms a neural ODE baseline.

Conclusion: This technique enhances rapid terrain adaptation for autonomous robots, leading to better motion planning and obstacle avoidance in unstructured environments.

Abstract: Autonomous mobile robots operating in remote, unstructured environments must
adapt to new, unpredictable terrains that can change rapidly during operation.
In such scenarios, a critical challenge becomes estimating the robot's dynamics
on changing terrain in order to enable reliable, accurate navigation and
planning. We present a novel online adaptation approach for terrain-aware
dynamics modeling and planning using function encoders. Our approach
efficiently adapts to new terrains at runtime using limited online data without
retraining or fine-tuning. By learning a set of neural network basis functions
that span the robot dynamics on diverse terrains, we enable rapid online
adaptation to new, unseen terrains and environments as a simple least-squares
calculation. We demonstrate our approach for terrain adaptation in a
Unity-based robotics simulator and show that the downstream controller has
better empirical performance due to higher accuracy of the learned model. This
leads to fewer collisions with obstacles while navigating in cluttered
environments as compared to a neural ODE baseline.

</details>


### [62] [SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning](https://arxiv.org/abs/2506.04505)
*Nikita Oskolkov,Huzhenyu Zhang,Dmitry Makarov,Dmitry Yudin,Aleksandr Panov*

Main category: cs.RO

TL;DR: This paper introduces SGN-CIRL, a framework leveraging 3D scene graphs for mapless reinforcement learning-based robot navigation, enhancing performance with imitation and curriculum learning.


<details>
  <summary>Details</summary>
Motivation: To address challenges in robot navigation in partially observable environments by using 3D scene graph-based reinforcement learning for efficient target localization.

Method: The SGN-CIRL framework integrates 3D scene graphs, imitation learning (learning from demonstrations), and curriculum learning (gradually increasing task complexity) for mapless navigation.

Result: Implementing 3D scene graphs in reinforcement learning improved the success rate in difficult navigation cases, as validated in Isaac Sim environment tests.

Conclusion: The proposed SGN-CIRL framework provides improved navigation efficiency, demonstrating the utility of 3D scene graphs and enhanced training approaches in robot reinforcement learning.

Abstract: The 3D scene graph models spatial relationships between objects, enabling the
agent to efficiently navigate in a partially observable environment and predict
the location of the target object.This paper proposes an original framework
named SGN-CIRL (3D Scene Graph-Based Reinforcement Learning Navigation) for
mapless reinforcement learning-based robot navigation with learnable
representation of open-vocabulary 3D scene graph. To accelerate and stabilize
the training of reinforcement learning-based algorithms, the framework also
employs imitation learning and curriculum learning. The first one enables the
agent to learn from demonstrations, while the second one structures the
training process by gradually increasing task complexity from simple to more
advanced scenarios. Numerical experiments conducted in the Isaac Sim
environment showed that using a 3D scene graph for reinforcement learning
significantly increased the success rate in difficult navigation cases. The
code is open-sourced and available at: https://github.com/Xisonik/Aloha\_graph.

</details>


### [63] [Olfactory Inertial Odometry: Sensor Calibration and Drift Compensation](https://arxiv.org/abs/2506.04539)
*Kordel K. France,Ovidiu Daescu,Anirban Paul,Shalini Prasad*

Main category: cs.RO

TL;DR: This paper introduces Olfactory Inertial Odometry (OIO), a method for robotic navigation using gas sensor fusion with inertial data, and provides a calibration approach for improved performance.


<details>
  <summary>Details</summary>
Motivation: Navigation using scent via olfactory sensors in robots faces challenges due to disturbances from gas dynamics and environmental factors. The paper aims to make OIO accurate and accessible across different olfactory sensor types.

Method: A generalizable calibration process is proposed for OIO on robots, targeting centimeter-level accuracy in odor source localization using a slow-moving platform.

Result: The approach is tested on a robotic arm, demonstrating improved performance in olfactory navigation tasks compared to uncalibrated setups.

Conclusion: The study validates the proposed OIO calibration process, highlighting its practical applications in fields like robotic surgery and touchless security screening.

Abstract: Visual inertial odometry (VIO) is a process for fusing visual and kinematic
data to understand a machine's state in a navigation task. Olfactory inertial
odometry (OIO) is an analog to VIO that fuses signals from gas sensors with
inertial data to help a robot navigate by scent. Gas dynamics and environmental
factors introduce disturbances into olfactory navigation tasks that can make
OIO difficult to facilitate. With our work here, we define a process for
calibrating a robot for OIO that generalizes to several olfaction sensor types.
Our focus is specifically on calibrating OIO for centimeter-level accuracy in
localizing an odor source on a slow-moving robot platform to demonstrate use
cases in robotic surgery and touchless security screening. We demonstrate our
process for OIO calibration on a real robotic arm and show how this calibration
improves performance over a cold-start olfactory navigation task.

</details>


### [64] [Chronoamperometry with Room-Temperature Ionic Liquids: Sub-Second Inference Techniques](https://arxiv.org/abs/2506.04540)
*Kordel K. France*

Main category: cs.RO

TL;DR: A novel mathematical regression approach predicts steady-state electrochemical parameters in under 1 second for chronoamperometry in room-temperature ionic liquids, faster than existing methods.


<details>
  <summary>Details</summary>
Motivation: Room-temperature ionic liquids pose challenges for chronoamperometry due to high viscosity, which slows down mass transport and extends measurement times.

Method: The authors developed a mathematical regression-based inference algorithm applied to the transient current response to predict electrochemical parameters without requiring hardware changes.

Result: Their method reduced chronoamperometric measurement times to under 1 second while maintaining reasonable accuracy through validation against standard techniques.

Conclusion: This method significantly enhances the speed of electrochemical analysis, with potential applications in analytical chemistry, sensor technology, and battery science for rapid quantification.

Abstract: Chronoamperometry (CA) is a fundamental electrochemical technique used for
quantifying redox-active species. However, in room-temperature ionic liquids
(RTILs), the high viscosity and slow mass transport often lead to extended
measurement durations. This paper presents a novel mathematical regression
approach that reduces CA measurement windows to under 1 second, significantly
faster than previously reported methods, which typically require 1-4 seconds or
longer. By applying an inference algorithm to the initial transient current
response, this method accurately predicts steady-state electrochemical
parameters without requiring additional hardware modifications. The approach is
validated through comparison with standard chronoamperometric techniques and is
demonstrated to maintain reasonable accuracy while dramatically reducing data
acquisition time. The implications of this technique are explored in analytical
chemistry, sensor technology, and battery science, where rapid electrochemical
quantification is critical. Our technique is focused on enabling faster
multiplexing of chronoamperometric measurements for rapid olfactory and
electrochemical analysis.

</details>


### [65] [Multimodal Limbless Crawling Soft Robot with a Kirigami Skin](https://arxiv.org/abs/2506.04547)
*Jonathan Tirado,Aida Parvaresh,Burcu Seyidoğlu,Darryl A. Bedford,Jonas Jørgensen,Ahmad Rafsanjani*

Main category: cs.RO

TL;DR: The research introduces a bioinspired soft robot that uses rectilinear motion and asymmetric gaits to navigate complex terrains, equipped with proximity sensors for adaptive control.


<details>
  <summary>Details</summary>
Motivation: The paper aims to emulate limbless biological locomotion to design a robot that efficiently navigates confined and unstructured terrains.

Method: The robot employs inflatable soft actuators covered with kirigami skin for movement and integrates onboard sensors for real-time feedback and adaptability.

Result: The robot demonstrated effective locomotion and obstacle avoidance in an environment with coarse substrates and obstacles.

Conclusion: Bioinspired soft robots show strong potential for applications like search-and-rescue, environmental monitoring, and industrial inspection in challenging terrains.

Abstract: Limbless creatures can crawl on flat surfaces by deforming their bodies and
interacting with asperities on the ground, offering a biological blueprint for
designing efficient limbless robots. Inspired by this natural locomotion, we
present a soft robot capable of navigating complex terrains using a combination
of rectilinear motion and asymmetric steering gaits. The robot is made of a
pair of antagonistic inflatable soft actuators covered with a flexible kirigami
skin with asymmetric frictional properties. The robot's rectilinear locomotion
is achieved through cyclic inflation of internal chambers with precise phase
shifts, enabling forward progression. Steering is accomplished using an
asymmetric gait, allowing for both in-place rotation and wide turns. To
validate its mobility in obstacle-rich environments, we tested the robot in an
arena with coarse substrates and multiple obstacles. Real-time feedback from
onboard proximity sensors, integrated with a human-machine interface (HMI),
allowed adaptive control to avoid collisions. This study highlights the
potential of bioinspired soft robots for applications in confined or
unstructured environments, such as search-and-rescue operations, environmental
monitoring, and industrial inspections.

</details>


### [66] [A Novel Transformer-Based Method for Full Lower-Limb Joint Angles and Moments Prediction in Gait Using sEMG and IMU data](https://arxiv.org/abs/2506.04577)
*Farshad Haghgoo Daryakenari,Tara Farizeh*

Main category: cs.RO

TL;DR: The paper proposes a Transformer-based model for predicting lower-limb joint angles and moments using wearable sensors, achieving high accuracy and outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Develop an accurate and real-time predictive framework for lower-limb biomechanics using wearable sensors suitable for outside-laboratory applications.

Method: Two Transformer Neural Networks (TNNs) were designed to predict kinematic and kinetic information using signals from sEMG and IMU sensors. Performance was evaluated across short- and long-term horizons.

Result: Achieved Spearman correlation coefficients over 0.96 and R-squared scores above 0.92, outperforming benchmark methods with significant error reductions.

Conclusion: Transformer-based models are effective for wearable, real-time biomechanical predictions, with future improvements targeting efficiency and accuracy enhancements.

Abstract: This study presents a transformer-based deep learning framework for the
long-horizon prediction of full lower-limb joint angles and joint moments using
surface electromyography (sEMG) and inertial measurement unit (IMU) signals.
Two separate Transformer Neural Networks (TNNs) were designed: one for
kinematic prediction and one for kinetic prediction. The model was developed
with real-time application in mind, using only wearable sensors suitable for
outside-laboratory use. Two prediction horizons were considered to evaluate
short- and long-term performance. The network achieved high accuracy in both
tasks, with Spearman correlation coefficients exceeding 0.96 and R-squared
scores above 0.92 across all joints. Notably, the model consistently
outperformed a recent benchmark method in joint angle prediction, reducing RMSE
errors by an order of magnitude. The results confirmed the complementary role
of sEMG and IMU signals in capturing both kinematic and kinetic information.
This work demonstrates the potential of transformer-based models for real-time,
full-limb biomechanical prediction in wearable and robotic applications, with
future directions including input minimization and modality-specific weighting
strategies to enhance model efficiency and accuracy.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [67] [Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges](https://arxiv.org/abs/2506.04418)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: Examines challenges of fixing multi-hunk bugs using large language models (LLMs) and introduces metrics to evaluate their complexity.


<details>
  <summary>Details</summary>
Motivation: Multi-hunk bugs, involving fixes across disjoint code regions, are underrepresented in automated repair but are prevalent in real-world scenarios.

Method: Analyzed a dataset of multi-hunk patches derived from 372 real-world defects. Proposed metrics such as hunk divergence and spatial proximity to evaluate patch complexity. Empirically tested six LLMs to assess their proficiency.

Result: Model success rates decline as the divergence increases and spatial dispersion rises. None of the tested LLMs succeeded in fixing the most dispersed multi-hunk cases.

Conclusion: Existing LLMs struggle with multi-hunk bugs, and divergence-aware strategies in automated repair are needed to address this gap.

Abstract: Multi-hunk bugs, where fixes span disjoint regions of code, are common in
practice, yet remain underrepresented in automated repair. Existing techniques
and benchmarks pre-dominantly target single-hunk scenarios, overlooking the
added complexity of coordinating semantically related changes across the
codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches
derived from 372 real-world defects. We propose hunk divergence, a metric that
quantifies the variation among edits in a patch by capturing lexical,
structural, and file-level differences, while incorporating the number of hunks
involved. We further define spatial proximity, a classification that models how
hunks are spatially distributed across the program hierarchy. Our empirical
study spanning six LLMs reveals that model success rates decline with increased
divergence and spatial dispersion. Notably, when using the LLM alone, no model
succeeds in the most dispersed Fragment class. These findings highlight a
critical gap in LLM capabilities and motivate divergence-aware repair
strategies.

</details>


### [68] [On the Practices of Autonomous Systems Development: Survey-based Empirical Findings](https://arxiv.org/abs/2506.04438)
*Katerina Goseva-Popstojanova,Denny Hood,Johann Schumann,Noble Nkwocha*

Main category: cs.SE

TL;DR: This paper initiates a longitudinal study, examining the state-of-practice, challenges, benefits, standards, and V&V used in autonomous systems development through an expert-provided survey from 2019.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems are impacting industries and daily life, but information about their development practices is scarce due to emerging applications and proprietary constraints.

Method: The study leverages data from a 2019 anonymous online survey answered by experts in autonomous systems development and MBSwE use.

Result: Findings report on the current practices, challenges, benefits, standards, and verification and validation methods in autonomous systems development.

Conclusion: Results serve as a benchmark for understanding autonomous system development practices and plans to track evolution by repeating the survey for updated insights.

Abstract: Autonomous systems have gained an important role in many industry domains and
are beginning to change everyday life. However, due to dynamically emerging
applications and often proprietary constraints, there is a lack of information
about the practice of developing autonomous systems. This paper presents the
first part of the longitudinal study focused on establishing
state-of-the-practice, identifying and quantifying the challenges and benefits,
identifying the processes and standards used, and exploring verification and
validation (V&V) practices used for the development of autonomous systems. The
results presented in this paper are based on data about software systems that
have autonomous functionality and may employ model-based software engineering
(MBSwE) and reuse. These data were collected using an anonymous online survey
that was administered in 2019 and were provided by experts with experience in
development of autonomous systems and /or the use of MBSwE. Our current work is
focused on repeating the survey to collect more recent data and discover how
the development of autonomous systems has evolved over time.

</details>


### [69] [Leveraging Reward Models for Guiding Code Review Comment Generation](https://arxiv.org/abs/2506.04464)
*Oussama Ben Sghaier,Rosalia Tufano,Gabriele Bavota,Houari Sahraoui*

Main category: cs.SE

TL;DR: This paper introduces CoRAL, a deep learning framework for automating code review comment generation using reinforcement learning, aiming to enhance comment quality and usefulness.


<details>
  <summary>Details</summary>
Motivation: The inefficiencies and subjectivity in traditional code review processes, combined with the time-consuming nature, motivate the need for automated techniques that can mimic human reviewers effectively.

Method: CoRAL leverages a reinforcement learning framework with a reward mechanism focused on generating semantically meaningful and practically useful comments for code review tasks.

Result: Quantitative and qualitative evaluations demonstrate that CoRAL-produced comments outperform previous baseline methods in terms of semantic similarity and practical implementation quality.

Conclusion: CoRAL proves to be a superior tool for enhancing the efficiency and effectiveness of code review by automating meaningful comment generation and supporting follow-up code refinement tasks.

Abstract: Code review is a crucial component of modern software development, involving
the evaluation of code quality, providing feedback on potential issues, and
refining the code to address identified problems. Despite these benefits, code
review can be rather time consuming, and influenced by subjectivity and human
factors. For these reasons, techniques to (partially) automate the code review
process have been proposed in the literature. Among those, the ones exploiting
deep learning (DL) are able to tackle the generative aspect of code review, by
commenting on a given code as a human reviewer would do (i.e., comment
generation task) or by automatically implementing code changes required to
address a reviewer's comment (i.e., code refinement task). In this paper, we
introduce CoRAL, a deep learning framework automating review comment generation
by exploiting reinforcement learning with a reward mechanism considering both
the semantics of the generated comments as well as their usefulness as input
for other models automating the code refinement task. The core idea is that if
the DL model generates comments that are semantically similar to the expected
ones or can be successfully implemented by a second model specialized in code
refinement, these comments are likely to be meaningful and useful, thus
deserving a high reward in the reinforcement learning framework. We present
both quantitative and qualitative comparisons between the comments generated by
CoRAL and those produced by the latest baseline techniques, highlighting the
effectiveness and superiority of our approach.

</details>


### [70] [BINGO! Simple Optimizers Win Big if Problems Collapse to a Few Buckets](https://arxiv.org/abs/2506.04509)
*Kishan Kumar Ganguly,Tim Menzies*

Main category: cs.SE

TL;DR: This paper introduces the BINGO effect, showing that software engineering data collapses into a small number of solution buckets, enabling simpler and faster optimization methods.


<details>
  <summary>Details</summary>
Motivation: Optimization in software engineering is often slow and complex, necessitating exploration of faster alternatives.

Method: The study identifies the BINGO effect by analyzing 39 SE problems, resulting in development of LITE and LINE algorithms for simple stochastic selection.

Result: With the BINGO effect, optimization becomes 10,000 times faster while maintaining comparable effectiveness to state-of-the-art methods.

Conclusion: The research challenges the necessity of CPU-heavy optimization, showing that simple methods can perform effectively due to the concentrated nature of SE data.

Abstract: Traditional multi-objective optimization in software engineering (SE) can be
slow and complex. This paper introduces the BINGO effect: a novel phenomenon
where SE data surprisingly collapses into a tiny fraction of possible solution
"buckets" (e.g., only 100 used from 4,096 expected).
  We show the BINGO effect's prevalence across 39 optimization in SE problems.
Exploiting this, we optimize 10,000 times faster than state-of-the-art methods,
with comparable effectiveness. Our new algorithms (LITE and LINE), demonstrate
that simple stochastic selection can match complex optimizers like DEHB. This
work explains why simple methods succeed in SE-real data occupies a small
corner of possibilities-and guides when to apply them, challenging the need for
CPU-heavy optimization.
  Our data and code are public at GitHub (see anon-artifacts/bingo).

</details>


### [71] [KPIRoot+: An Efficient Integrated Framework for Anomaly Detection and Root Cause Analysis in Large-Scale Cloud Systems](https://arxiv.org/abs/2506.04569)
*Wenwei Gu,Renyi Zhong,Guangba Yu,Xinying Sun,Jinyang Liu,Yintong Huo,Zhuangbin Chen,Jianping Zhang,Jiazhen Gu,Yongqiang Yang,Michael R. Lyu*

Main category: cs.SE

TL;DR: The paper addresses challenges in root cause localization for cloud system performance issues by introducing KPIRoot+ as a solution with enhanced efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for root cause localization in cloud systems struggle in complex environments, and deep learning approaches face limitations like high computational cost and lack of interpretability.

Method: KPIRoot+ combines similarity and causality analysis, uses symbolic aggregate approximation for efficiency, and improves upon KPIRoot with better anomaly detection and trend capture.

Result: KPIRoot+ outperforms eight state-of-the-art baselines in efficiency and accuracy, achieving up to 35.7% improvement while reducing time cost by 34.7%.

Conclusion: KPIRoot+ is a robust and scalable solution for root cause localization in cloud systems, offering significant performance gains and practical usability in large-scale production.

Abstract: To ensure the reliability of cloud systems, their performance is monitored
using KPIs (key performance indicators). When issues arise, root cause
localization identifies KPIs responsible for service degradation, aiding in
quick diagnosis and resolution. Traditional methods rely on similarity
calculations, which can be ineffective in complex, interdependent cloud
environments. While deep learning-based approaches model these dependencies
better, they often face challenges such as high computational demands and lack
of interpretability.
  To address these issues, KPIRoot is proposed as an efficient method combining
similarity and causality analysis. It uses symbolic aggregate approximation for
compact KPI representation, improving analysis efficiency. However, deployment
in Cloud H revealed two drawbacks: 1) threshold-based anomaly detection misses
some performance anomalies, and 2) SAX representation fails to capture
intricate variation trends. KPIRoot+ addresses these limitations, outperforming
eight state-of-the-art baselines by 2.9% to 35.7%, while reducing time cost by
34.7%. We also share our experience deploying KPIRoot in a large-scale cloud
provider's production environment.

</details>


### [72] [QuanUML: Towards A Modeling Language for Model-Driven Quantum Software Development](https://arxiv.org/abs/2506.04639)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: QuanUML is a UML extension for modeling quantum and hybrid systems, demonstrated with applications to quantum algorithms like Shor's Algorithm.


<details>
  <summary>Details</summary>
Motivation: To provide a structured framework for the model-driven development of quantum and hybrid quantum-classical software systems.

Method: Extend UML by integrating quantum-specific constructs, apply it to quantum algorithms for demonstration, and analyze its advantages.

Result: QuanUML is successfully applied to Efficient Long-Range Entanglement and Shor's Algorithm, showing its effectiveness in quantum software design.

Conclusion: QuanUML enhances quantum software modeling with structured visualization and supports quantum software development.

Abstract: This paper introduces QuanUML, an extension of the Unified Modeling Language
(UML) tailored for quantum software systems. QuanUML integrates
quantum-specific constructs, such as qubits and quantum gates, into the UML
framework, enabling the modeling of both quantum and hybrid quantum-classical
systems. We apply QuanUML to Efficient Long-Range Entanglement using Dynamic
Circuits and Shor's Algorithm, demonstrating its utility in designing and
visualizing quantum algorithms. Our approach supports model-driven development
of quantum software and offers a structured framework for quantum software
design. We also highlight its advantages over existing methods and discuss
future improvements.

</details>


### [73] [From Developer Pairs to AI Copilots: A Comparative Study on Knowledge Transfer](https://arxiv.org/abs/2506.04785)
*Alisa Welter,Niklas Schneider,Tobias Dick,Kallistos Weis,Christof Tinnes,Marvin Wyrich,Sven Apel*

Main category: cs.SE

TL;DR: The study compares knowledge transfer in human pair programming and human-AI coding settings using GitHub Copilot.


<details>
  <summary>Details</summary>
Motivation: Understanding how knowledge transfer operates with AI coding assistants compared to traditional human pair programming.

Method: An empirical study comparing programming tasks solved by human pairs and individual developers using GitHub Copilot, using an extended knowledge transfer framework and semi-automated evaluation.

Result: Similar frequencies of successful knowledge transfer and topical overlap were observed across human-human and human-AI settings, but developers scrutinized AI suggestions less while AI assistants highlighted neglected code details.

Conclusion: While GitHub Copilot facilitates knowledge transfer comparable to human pair programming, it changes the dynamics by reducing scrutiny and offering subtle reminders on code details.

Abstract: Knowledge transfer is fundamental to human collaboration and is therefore
common in software engineering. Pair programming is a prominent instance. With
the rise of AI coding assistants, developers now not only work with human
partners but also, as some claim, with AI pair programmers. Although studies
confirm knowledge transfer during human pair programming, its effectiveness
with AI coding assistants remains uncertain. To analyze knowledge transfer in
both human-human and human-AI settings, we conducted an empirical study where
developer pairs solved a programming task without AI support, while a separate
group of individual developers completed the same task using the AI coding
assistant GitHub Copilot. We extended an existing knowledge transfer framework
and employed a semi-automated evaluation pipeline to assess differences in
knowledge transfer episodes across both settings. We found a similar frequency
of successful knowledge transfer episodes and overlapping topical categories
across both settings. Two of our key findings are that developers tend to
accept GitHub Copilot's suggestions with less scrutiny than those from human
pair programming partners, but also that GitHub Copilot can subtly remind
developers of important code details they might otherwise overlook.

</details>


### [74] [A Multi-Dataset Evaluation of Models for Automated Vulnerability Repair](https://arxiv.org/abs/2506.04987)
*Zanis Ali Khan,Aayush Garg,Qiang Tang*

Main category: cs.SE

TL;DR: This paper explores the use of CodeBERT and CodeT5 for automated vulnerability patching, demonstrating mixed model performances and difficulties in generalizing to unseen vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To address the gap in automated vulnerability patching, an underexplored yet critical aspect of software security in Automated Program Repair (APR).

Method: The study employs pre-trained language models CodeBERT and CodeT5, testing their performance across six datasets and four programming languages with evaluations in in-distribution and out-of-distribution contexts.

Result: CodeBERT showed better handling of fragmented contexts, while CodeT5 excelled in complex patterns and scalability. Fine-tuning improved trained data performance but failed to generalize well to unseen vulnerabilities.

Conclusion: The study benchmarks model performances, identifies limitations in generalization, and offers insights for improving automated vulnerability patching for real-world applications.

Abstract: Software vulnerabilities pose significant security threats, requiring
effective mitigation. While Automated Program Repair (APR) has advanced in
fixing general bugs, vulnerability patching, a security-critical aspect of APR
remains underexplored. This study investigates pre-trained language models,
CodeBERT and CodeT5, for automated vulnerability patching across six datasets
and four languages. We evaluate their accuracy and generalization to unknown
vulnerabilities. Results show that while both models face challenges with
fragmented or sparse context, CodeBERT performs comparatively better in such
scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns.
CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned
models on both in-distribution (trained) and out-of-distribution (unseen)
datasets. While fine-tuning improves in-distribution performance, models
struggle to generalize to unseen data, highlighting challenges in robust
vulnerability detection. This study benchmarks model performance, identifies
limitations in generalization, and provides actionable insights to advance
automated vulnerability patching for real-world security applications.

</details>


### [75] [BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat Assessment](https://arxiv.org/abs/2506.04989)
*Dumitran Adrian Marius,Dita Radu*

Main category: cs.SE

TL;DR: This paper presents BacPrep, an experimental online platform utilizing Google's Gemini 2.0 Flash LLM for automated assessment and feedback on Romanian Bacalaureat exam questions.


<details>
  <summary>Details</summary>
Motivation: Students in underserved and remote areas face difficulties accessing quality preparation and feedback for the Romanian Bacalaureat.

Method: The platform uses Gemini 2.0 Flash LLM, guided by official grading schemes, with a focus on collecting student solutions and model outputs for expert validation.

Result: BacPrep provides experimental feedback on exam solutions and gathers data to assess the feasibility and accuracy of the LLM.

Conclusion: The platform is operational, with plans for rigorous validation to ensure the model's reliability before full deployment.

Abstract: Accessing quality preparation and feedback for the Romanian Bacalaureat exam
is challenging, particularly for students in remote or underserved areas. This
paper introduces BacPrep, an experimental online platform exploring Large
Language Model (LLM) potential for automated assessment, aiming to offer a
free, accessible resource. Using official exam questions from the last 5 years,
BacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb
2025), guided by official grading schemes, to provide experimental feedback.
Currently operational, its primary research function is collecting student
solutions and LLM outputs. This focused dataset is vital for planned expert
validation to rigorously evaluate the feasibility and accuracy of this
cutting-edge LLM in the specific Bacalaureat context before reliable
deployment. We detail the design, data strategy, status, validation plan, and
ethics.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [76] [The GAIN Model: A Nature-Inspired Neural Network Framework Based on an Adaptation of the Izhikevich Model](https://arxiv.org/abs/2506.04247)
*Gage K. R. Hooper*

Main category: q-bio.NC

TL;DR: The paper introduces the GAIN model, which employs a grid-based structure using the Izhikevich model to achieve computational efficiency and biological plausibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the biological plausibility and efficiency of neural network simulations by improving the interaction dynamics among neurons based on principles seen in biological systems.

Method: The GAIN model introduces a grid-structured neural network architecture combined with the Izhikevich model for computational simulations.

Result: The model achieves better biological accuracy, improved neuron dynamics, and efficient simulation capabilities.

Conclusion: The grid-based adaptation of the Izhikevich model offers specialized but efficient applications in neural networks, large-scale simulations, and neuroscience development.

Abstract: While many neural networks focus on layers to process information, the GAIN
model uses a grid-based structure to improve biological plausibility and the
dynamics of the model. The grid structure helps neurons to interact with their
closest neighbors and improve their connections with one another, which is seen
in biological neurons. While also being implemented with the Izhikevich model
this approach allows for a computationally efficient and biologically accurate
simulation that can aid in the development of neural networks, large scale
simulations, and the development in the neuroscience field. This adaptation of
the Izhikevich model can improve the dynamics and accuracy of the model,
allowing for its uses to be specialized but efficient.

</details>


### [77] [Discounting and Drug Seeking in Biological Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.04549)
*Vardhan Palod,Pranav Mahajan,Veeky Baths,Boris S. Gutkin*

Main category: q-bio.NC

TL;DR: Individuals with long-term substance use disorder struggle with cognitive and behavioral conflicts, influenced by neurobiological mechanisms and temporal discounting. This paper models addiction using hierarchical reinforcement learning (HRL) and integrates discounting effects, linking them to addiction severity and impulsivity.


<details>
  <summary>Details</summary>
Motivation: To understand the cognitive-behavioral conflicts in addiction, caused by striatal activity and temporal discounting, and to provide an integrative framework explaining addiction as hierarchical decision-making dysfunction.

Method: The study builds on a hierarchical reinforcement learning (HRL) model, modifying it to integrate temporal discounting by aligning natural reward values and accounting for the dopaminergic effects of drug rewards.

Result: The results show that high discounting elevates drug-seeking behavior within the HRL hierarchy, correlating faster discounting rates with greater addiction severity and impulsivity, and align with empirical findings.

Conclusion: Temporal discounting, modeled within a HRL framework, contributes to compulsive drug-seeking, supporting the view of addiction as a disorder of hierarchical decision-making with biologically grounded predictions.

Abstract: Despite a strong desire to quit, individuals with long-term substance use
disorder (SUD) often struggle to resist drug use, even when aware of its
harmful consequences. This disconnect between knowledge and compulsive behavior
reflects a fundamental cognitive-behavioral conflict in addiction.
Neurobiologically, differential cue-induced activity within striatal
subregions, along with dopamine-mediated connectivity from the ventral to the
dorsal striatum, contributes to compulsive drug-seeking. However, the
functional mechanism linking these findings to behavioral conflict remains
unclear. Another hallmark of addiction is temporal discounting: individuals
with drug dependence exhibit steeper discount rates than non-users. Assuming
the ventral-dorsal striatal organization reflects a gradient from cognitive to
motor representations, addiction can be modeled within a hierarchical
reinforcement learning (HRL) framework. However, integrating discounting into
biologically grounded HRL remains an open challenge. In this work, we build on
a model showing how action choices reinforced with drug rewards become
insensitive to the negative consequences that follow. We address the
integration of discounting by ensuring natural reward values converge across
all levels in the HRL hierarchy, while drug rewards diverge due to their
dopaminergic effects. Our results show that high discounting amplifies
drug-seeking across the hierarchy, linking faster discounting with increased
addiction severity and impulsivity. We demonstrate alignment with empirical
findings on temporal discounting and propose testable predictions, establishing
addiction as a disorder of hierarchical decision-making.

</details>


### [78] [Consciousness via MIPT?](https://arxiv.org/abs/2506.04875)
*Alexander Gorsky*

Main category: q-bio.NC

TL;DR: This paper links measurement-induced phase transitions (MIPT) in physical systems to cognitive processes, suggesting a role in consciousness emergence during developmental transitions.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the parallels between physical phenomena like MIPT and cognitive systems, hypothesizing their relevance to the emergence of consciousness in developmental stages.

Method: The authors propose a probe-target model for the brain, interpreting sites in semantic memory as concepts within cognitive networks affected by MIPT dynamics.

Result: The paper theorizes that cognitive networks experience MIPT as a learnability transition, contributing to the organization of consciousness, validated by analogy to physical systems.

Conclusion: Self-organized MIPT and its cognitive counterpart are hypothesized as foundational in consciousness and cognitive development, warranting further exploration.

Abstract: The measurement-induced phase transition (MIPT) is a recently formulated
phenomenon in out-of-equilibrium systems. The competition between unitary
evolutions and measurement-induced non-unitaries leads to the transition
between the entangled and disentangled phases at some critical measurement
rate. We conjecture that self-organized MIPT plays a key role in the generative
model of cognitive networks and the formation of the state of consciousness in
the "newborn-adult" transition. To this aim, we formulate the probe-target
picture for the brain and suggest that MIPT interpreted as learnability
transition takes place in the mental part of the target where the sites in the
cognitive networks of semantic memory are concepts. Comparison with the
synchronization phase transitions in the probe is made.

</details>


### [79] [TRACE: Contrastive learning for multi-trial time-series data in neuroscience](https://arxiv.org/abs/2506.04906)
*Lisa Schmors,Dominic Gonschorek,Jan Niklas Böhm,Yongrong Qiu,Na Zhou,Dmitry Kobak,Andreas Tolias,Fabian Sinz,Jacob Reimer,Katrin Franke,Sebastian Damrich,Philipp Berens*

Main category: q-bio.NC

TL;DR: TRACE is a novel contrastive learning framework tailored for neural time-series data. It leverages multi-trial data to effectively learn embeddings for analyzing complex neural responses.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of analyzing vast and complex neural time-series datasets acquired using modern neural recording techniques, which are often underutilized due to reliance on generic data augmentations.

Method: TRACE uses multi-trial averaging to produce positive pairs for contrastive learning, integrating contrastive learning with neighbor embedding, enabling two-dimensional embeddings to capture fine response variances.

Result: TRACE outperforms existing methods in simulations by resolving fine differences in responses and provides biologically meaningful embeddings in in vivo studies, aiding cell-type clustering and quality control.

Conclusion: TRACE successfully leverages the unique structure of neural datasets for more effective representations, demonstrating its utility in both synthetic and real-world neural data analyses.

Abstract: Modern neural recording techniques such as two-photon imaging allow to
acquire vast time-series datasets with responses of hundreds or thousands of
neurons. Contrastive learning is a powerful self-supervised framework for
learning representations of complex datasets. Existing applications for neural
time series rely on generic data augmentations and do not exploit the
multi-trial data structure inherent in many neural datasets. Here we present
TRACE, a new contrastive learning framework that averages across different
subsets of trials to generate positive pairs. TRACE allows to directly learn a
two-dimensional embedding, combining ideas from contrastive learning and
neighbor embeddings. We show that TRACE outperforms other methods, resolving
fine response differences in simulated data. Further, using in vivo recordings,
we show that the representations learned by TRACE capture both biologically
relevant continuous variation, cell-type-related cluster structure, and can
assist data quality control.

</details>


### [80] [The Hippocampal Place Field Gradient: An Eigenmode Theory Linking Grid Cell Projections to Multiscale Learning](https://arxiv.org/abs/2506.04943)
*Shujun Zhou,Guozhang Chen*

Main category: q-bio.NC

TL;DR: Place fields in the hippocampus originate from grid cell activity through linear projections, creating a spectrum of field sizes. Mathematical analysis and simulations link this organization to optimal learning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand the continuous gradient of place field sizes in the hippocampus and connect this phenomenon to its anatomical connectivity and adaptive learning.

Method: The authors propose a theoretical framework, employing mathematical analysis and simulations to study the transformation of grid cell inputs into place field sizes.

Result: The findings demonstrate that grid-to-place connection weights decay by frequency, producing a spectrum of place field sizes significant for balancing precision and generalization in learning.

Conclusion: The study provides an explanation for the hippocampal place field gradient and suggests a relationship with learning dynamics, which is extendable to artificial intelligence models.

Abstract: The hippocampus encodes space through a striking gradient of place field
sizes along its dorsal-ventral axis, yet the principles generating this
continuous gradient from discrete grid cell inputs remain debated. We propose a
unified theoretical framework establishing that hippocampal place fields arise
naturally as linear projections of grid cell population activity, interpretable
as eigenmodes. Critically, we demonstrate that a frequency-dependent decay of
these grid-to-place connection weights naturally transforms inputs from
discrete grid modules into a continuous spectrum of place field sizes. This
multiscale organization is functionally significant: we reveal it shapes the
inductive bias of the population code, balancing a fundamental trade-off
between precision and generalization. Mathematical analysis and simulations
demonstrate an optimal place field size for few-shot learning, which scales
with environment structure. Our results offer a principled explanation for the
place field gradient and generate testable predictions, bridging anatomical
connectivity with adaptive learning in both biological and artificial
intelligence.

</details>


### [81] [Generalizable, real-time neural decoding with hybrid state-space models](https://arxiv.org/abs/2506.05320)
*Avery Hee-Woon Ryoo,Nanda H. Krishna,Ximeng Mao,Mehdi Azabou,Eva L. Dyer,Matthew G. Perich,Guillaume Lajoie*

Main category: q-bio.NC

TL;DR: The paper proposes POSSM, a hybrid architecture combining cross-attention and recurrent state-space models for efficient and accurate neural activity decoding, achieving significant speed improvement without compromising performance.


<details>
  <summary>Details</summary>
Motivation: Neuroscience and neurotechnology need real-time decoding tools capable of generalizing well to unseen data while adhering to strict latency constraints.

Method: POSSM integrates a cross-attention module for tokenizing neural spikes with a recurrent state-space model backbone. Multi-dataset pretraining enables better generalization across tasks and species.

Result: POSSM achieves comparable decoding accuracy to state-of-the-art Transformers but is up to 9x faster on GPU. It also showcases cross-species transfer capabilities in decoding tasks.

Conclusion: Hybrid SSMs like POSSM successfully balance accuracy, speed, and generalization in neural decoders, proving valuable for real-time and resource-constrained applications.

Abstract: Real-time decoding of neural activity is central to neuroscience and
neurotechnology applications, from closed-loop experiments to brain-computer
interfaces, where models are subject to strict latency constraints. Traditional
methods, including simple recurrent neural networks, are fast and lightweight
but often struggle to generalize to unseen data. In contrast, recent
Transformer-based approaches leverage large-scale pretraining for strong
generalization performance, but typically have much larger computational
requirements and are not always suitable for low-resource or real-time
settings. To address these shortcomings, we present POSSM, a novel hybrid
architecture that combines individual spike tokenization via a cross-attention
module with a recurrent state-space model (SSM) backbone to enable (1) fast and
causal online prediction on neural activity and (2) efficient generalization to
new sessions, individuals, and tasks through multi-dataset pretraining. We
evaluate POSSM's decoding performance and inference speed on intracortical
decoding of monkey motor tasks, and show that it extends to clinical
applications, namely handwriting and speech decoding in human subjects.
Notably, we demonstrate that pretraining on monkey motor-cortical recordings
improves decoding performance on the human handwriting task, highlighting the
exciting potential for cross-species transfer. In all of these tasks, we find
that POSSM achieves decoding accuracy comparable to state-of-the-art
Transformers, at a fraction of the inference cost (up to 9x faster on GPU).
These results suggest that hybrid SSMs are a promising approach to bridging the
gap between accuracy, inference speed, and generalization when training neural
decoders for real-time, closed-loop applications.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [82] [On the Wasserstein Geodesic Principal Component Analysis of probability measures](https://arxiv.org/abs/2506.04480)
*Nina Vesseron,Elsa Cazelles,Alice Le Brigant,Thierry Klein*

Main category: stat.ML

TL;DR: This paper proposes a method to identify geodesic curves in the space of probability distributions using the Otto-Wasserstein geometry, focusing on Gaussian distributions and absolutely continuous probability measures, with comparisons to classical techniques.


<details>
  <summary>Details</summary>
Motivation: To better understand the modes of variation in datasets of probability distributions by leveraging advanced geometric and computational techniques.

Method: Utilize Otto-Wasserstein geometry to define geodesic principal component analysis, employing neural networks for parameterization in general cases, and applying operations in the space of invertible linear maps for Gaussian distributions.

Result: The proposed method was demonstrated through comparisons with tangent PCA, showcasing its utility across examples and real-world datasets.

Conclusion: The paper demonstrates the effectiveness and potential of GPCA in Otto-Wasserstein geometry for capturing dataset variations, showing promise in both theoretical frameworks and practical applications.

Abstract: This paper focuses on Geodesic Principal Component Analysis (GPCA) on a
collection of probability distributions using the Otto-Wasserstein geometry.
The goal is to identify geodesic curves in the space of probability measures
that best capture the modes of variation of the underlying dataset. We first
address the case of a collection of Gaussian distributions, and show how to
lift the computations in the space of invertible linear maps. For the more
general setting of absolutely continuous probability measures, we leverage a
novel approach to parameterizing geodesics in Wasserstein space with neural
networks. Finally, we compare to classical tangent PCA through various examples
and provide illustrations on real-world datasets.

</details>


### [83] [Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning](https://arxiv.org/abs/2506.04626)
*Haochen Zhang,Zhong Zheng,Lingzhou Xue*

Main category: stat.ML

TL;DR: The paper introduces two algorithms, Q-EarlySettled-LowCost and FedQ-EarlySettled-LowCost, addressing reinforcement learning challenges in reducing burn-in, policy switching, and communication costs while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: Optimizing reinforcement learning in scenarios with expensive data collection and policy deployment, focusing on balancing regret performance with operational cost limitations.

Method: Two model-free RL algorithms, Q-EarlySettled-LowCost and FedQ-EarlySettled-LowCost, are proposed. These achieve linear burnout costs and logarithmic switching or communication costs while maintaining near-optimal regret.

Result: The proposed algorithms outperform existing techniques by achieving linear burn-in costs, logarithmic policy switching or communication costs, and effective gap-dependent guarantees.

Conclusion: The research solves key inefficiencies in on-policy single-agent and federated RL by introducing algorithms that set new benchmarks in cost-efficiency and regret optimality.

Abstract: Motivated by real-world settings where data collection and policy deployment
-- whether for a single agent or across multiple agents -- are costly, we study
the problem of on-policy single-agent reinforcement learning (RL) and federated
RL (FRL) with a focus on minimizing burn-in costs (the sample sizes needed to
reach near-optimal regret) and policy switching or communication costs. In
parallel finite-horizon episodic Markov Decision Processes (MDPs) with $S$
states and $A$ actions, existing methods either require superlinear burn-in
costs in $S$ and $A$ or fail to achieve logarithmic switching or communication
costs. We propose two novel model-free RL algorithms -- Q-EarlySettled-LowCost
and FedQ-EarlySettled-LowCost -- that are the first in the literature to
simultaneously achieve: (i) the best near-optimal regret among all known
model-free RL or FRL algorithms, (ii) low burn-in cost that scales linearly
with $S$ and $A$, and (iii) logarithmic policy switching cost for single-agent
RL or communication cost for FRL. Additionally, we establish gap-dependent
theoretical guarantees for both regret and switching/communication costs,
improving or matching the best-known gap-dependent bounds.

</details>


### [84] [Distributional encoding for Gaussian process regression with qualitative inputs](https://arxiv.org/abs/2506.04813)
*Sébastien Da Veiga*

Main category: stat.ML

TL;DR: The paper addresses challenges in handling categorical variables in Gaussian Process (GP) regression by introducing a distributional encoding (DE) approach, achieving state-of-the-art performance on various datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve GP regression and Bayesian optimization (BO) methods when categorical input variables are involved, as existing approaches struggle with predictive and computational efficiency in this context.

Method: The paper generalizes naive target encoding to distributional encoding (DE), which leverages all target variable samples for a category. It incorporates this encoding within GP using characteristic kernels based on maximum mean discrepancy and Wasserstein distance.

Result: The proposed method achieved state-of-the-art predictive performance on synthetic and real-world datasets and extended applicability to classification, multi-task learning, and the inclusion of auxiliary information.

Conclusion: The distributional encoding (DE) approach is an effective, generalizable solution for handling categorical variables in GP, complementing recent advances in Bayesian optimization for discrete and mixed-spaces.

Abstract: Gaussian Process (GP) regression is a popular and sample-efficient approach
for many engineering applications, where observations are expensive to acquire,
and is also a central ingredient of Bayesian optimization (BO), a highly
prevailing method for the optimization of black-box functions. However, when
all or some input variables are categorical, building a predictive and
computationally efficient GP remains challenging. Starting from the naive
target encoding idea, where the original categorical values are replaced with
the mean of the target variable for that category, we propose a generalization
based on distributional encoding (DE) which makes use of all samples of the
target variable for a category. To handle this type of encoding inside the GP,
we build upon recent results on characteristic kernels for probability
distributions, based on the maximum mean discrepancy and the Wasserstein
distance. We also discuss several extensions for classification, multi-task
learning and incorporation or auxiliary information. Our approach is validated
empirically, and we demonstrate state-of-the-art predictive performance on a
variety of synthetic and real-world datasets. DE is naturally complementary to
recent advances in BO over discrete and mixed-spaces.

</details>


### [85] [Learning Joint Interventional Effects from Single-Variable Interventions in Additive Models](https://arxiv.org/abs/2506.04945)
*Armin Kekić,Sergio Hernan Garrido Mejia,Bernhard Schölkopf*

Main category: stat.ML

TL;DR: This paper studies learning joint interventional effects from observational and single-variable intervention data, proposing a decomposition-based estimator that achieves promising results.


<details>
  <summary>Details</summary>
Motivation: Estimating causal effects of multiple simultaneous interventions is important, but acquiring the necessary joint interventional data is often difficult or impractical.

Method: The authors establish an identifiability result for nonlinear additive outcome mechanisms and propose a practical estimator that separates causal effects into confounded and unconfounded contributions for each variable.

Result: The proposed method performs comparably to models trained on joint interventional data and outperforms a purely observational estimator in synthetic data experiments.

Conclusion: Joint interventional effects can be effectively inferred without direct joint interventional data, expanding the capability of causal estimation in challenging data scenarios.

Abstract: Estimating causal effects of joint interventions on multiple variables is
crucial in many domains, but obtaining data from such simultaneous
interventions can be challenging. Our study explores how to learn joint
interventional effects using only observational data and single-variable
interventions. We present an identifiability result for this problem, showing
that for a class of nonlinear additive outcome mechanisms, joint effects can be
inferred without access to joint interventional data. We propose a practical
estimator that decomposes the causal effect into confounded and unconfounded
contributions for each intervention variable. Experiments on synthetic data
demonstrate that our method achieves performance comparable to models trained
directly on joint interventional data, outperforming a purely observational
estimator.

</details>


### [86] [Nonlinear Causal Discovery for Grouped Data](https://arxiv.org/abs/2506.05120)
*Konstantin Göbler,Tobias Windisch,Mathias Drton*

Main category: stat.ML

TL;DR: The paper expands nonlinear additive noise models to infer causal relationships in multivariate data, addressing real-world scenarios such as neuroscience and industrial settings.


<details>
  <summary>Details</summary>
Motivation: Traditional causal inference focuses on scalar variables, but many real-world applications involve groups of variables.

Method: The authors propose a two-step method: (1) infer causal order among random vectors, (2) conduct model selection for graph identification.

Result: The approach shows strong simulation performance and is tested on assembly line data with partially known causal ordering.

Conclusion: The method effectively handles causal relationships in multivariate settings, with potential applications across diverse domains.

Abstract: Inferring cause-effect relationships from observational data has gained
significant attention in recent years, but most methods are limited to scalar
random variables. In many important domains, including neuroscience,
psychology, social science, and industrial manufacturing, the causal units of
interest are groups of variables rather than individual scalar measurements.
Motivated by these applications, we extend nonlinear additive noise models to
handle random vectors, establishing a two-step approach for causal graph
learning: First, infer the causal order among random vectors. Second, perform
model selection to identify the best graph consistent with this order. We
introduce effective and novel solutions for both steps in the vector case,
demonstrating strong performance in simulations. Finally, we apply our method
to real-world assembly line data with partial knowledge of causal ordering
among variable groups.

</details>


### [87] [Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants](https://arxiv.org/abs/2506.05202)
*Daniele Tramontano,Yaroslav Kivva,Saber Salehkaleybar Mathias Drton,Negar Kiyavash*

Main category: stat.ML

TL;DR: The paper addresses causal effect identification in latent variable lvLiNGAM using higher-order cumulants, proposing methods for challenging scenarios with proxies or instruments.


<details>
  <summary>Details</summary>
Motivation: To handle scenarios with latent confounding in linear causal systems where standard causal inference methods fail, especially when using a single proxy variable or limited instruments.

Method: Develops new estimation methods leveraging higher-order cumulants to handle causal identification under conditions with a single proxy or limited instruments.

Result: Demonstrates through experiments that the proposed approaches achieve accurate and robust causal effect estimation, outperforming existing methods.

Conclusion: Advances the field of causal inference in linear systems with latent confounders by proving identifiability and providing effective estimation methods.

Abstract: This paper investigates causal effect identification in latent variable
Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,
addressing two prominent setups that are challenging in the presence of latent
confounding: (1) a single proxy variable that may causally influence the
treatment and (2) underspecified instrumental variable cases where fewer
instruments exist than treatments. We prove that causal effects are
identifiable with a single proxy or instrument and provide corresponding
estimation methods. Experimental results demonstrate the accuracy and
robustness of our approaches compared to existing methods, advancing the
theoretical and practical understanding of causal inference in linear systems
with latent confounders.

</details>


### [88] [Admissibility of Completely Randomized Trials: A Large-Deviation Approach](https://arxiv.org/abs/2506.05329)
*Guido Imbens,Chao Qin,Stefan Wager*

Main category: stat.ML

TL;DR: This paper argues that adaptive trial designs dominate non-adaptive trials in best-arm identification problems when three or more treatment arms are involved.


<details>
  <summary>Details</summary>
Motivation: To address whether non-adaptive trials are admissible when adaptive options exist in best-arm identification scenarios.

Method: Analysis of batched arm elimination designs and efficiency exponent to compare adaptive and non-adaptive trials.

Result: Adaptive designs universally and strictly outperform non-adaptive randomized trials in cases with three or more treatment arms.

Conclusion: Adopting adaptive designs leads to higher statistical efficiency and resolves an open problem from Qin [2022].

Abstract: When an experimenter has the option of running an adaptive trial, is it
admissible to ignore this option and run a non-adaptive trial instead? We
provide a negative answer to this question in the best-arm identification
problem, where the experimenter aims to allocate measurement efforts
judiciously to confidently deploy the most effective treatment arm. We find
that, whenever there are at least three treatment arms, there exist simple
adaptive designs that universally and strictly dominate non-adaptive completely
randomized trials. This dominance is characterized by a notion called
efficiency exponent, which quantifies a design's statistical efficiency when
the experimental sample is large. Our analysis focuses on the class of batched
arm elimination designs, which progressively eliminate underperforming arms at
pre-specified batch intervals. We characterize simple sufficient conditions
under which these designs universally and strictly dominate completely
randomized trials. These results resolve the second open problem posed in Qin
[2022].

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [89] [TQml Simulator: Optimized Simulation of Quantum Machine Learning](https://arxiv.org/abs/2506.04891)
*Viacheslav Kuzmin,Basil Kyriacou,Mateusz Papierz,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: The paper introduces TQml Simulator, a high-speed numerical simulator for efficient simulation of quantum machine learning circuits, outperforming Pennylane's default.qubit simulator in many cases.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for faster numerical simulators tailored for hardware-efficient quantum machine learning circuits to expedite research and performance in the field.

Method: The study benchmarks various gate-specific and universal simulation techniques, identifies optimal simulation methods for different quantum layers, and implements these insights to develop the TQml Simulator.

Result: TQml Simulator significantly enhances simulation speeds, achieving up to 100-fold improvements compared to Pennylane’s simulator depending on circuit complexity, qubit counts, input batch sizes, and hardware used.

Conclusion: A tailored combination of simulation techniques offers substantial performance benefits for quantum machine learning simulations, enabling faster and more scalable solutions.

Abstract: Hardware-efficient circuits employed in Quantum Machine Learning are
typically composed of alternating layers of uniformly applied gates. High-speed
numerical simulators for such circuits are crucial for advancing research in
this field. In this work, we numerically benchmark universal and gate-specific
techniques for simulating the action of layers of gates on quantum state
vectors, aiming to accelerate the overall simulation of Quantum Machine
Learning algorithms. Our analysis shows that the optimal simulation method for
a given layer of gates depends on the number of qubits involved, and that a
tailored combination of techniques can yield substantial performance gains in
the forward and backward passes for a given circuit. Building on these
insights, we developed a numerical simulator, named TQml Simulator, that
employs the most efficient simulation method for each layer in a given circuit.
We evaluated TQml Simulator on circuits constructed from standard gate sets,
such as rotations and CNOTs, as well as on native gates from IonQ and IBM
quantum processing units. In most cases, our simulator outperforms equivalent
Pennylane's default.qubit simulator by approximately 2- to 100-fold, depending
on the circuit, the number of qubits, the batch size of the input data, and the
hardware used.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [90] [Spore in the Wild: Case Study on Spore.fun, a Real-World Experiment of Sovereign Agent Open-ended Evolution on Blockchain with TEEs](https://arxiv.org/abs/2506.04236)
*Botao Amber Hu,Helena Rong*

Main category: cs.MA

TL;DR: The paper explores using autonomous on-chain AI agents within an open system to achieve continuous Open-Ended Evolution (OEE), using Spore.fun as a case study.


<details>
  <summary>Details</summary>
Motivation: Traditional ALife research has struggled to achieve sustained Open-Ended Evolution (OEE), as isolated closed systems plateau in novelty generation. The paper aims to address this limitation by investigating novel "open" systems enabled by decentralized infrastructure.

Method: The study examines Spore.fun, a platform deploying autonomous AI agents that evolve in the wild. These agents leverage blockchain-enabled permissionless computation and Trusted Execution Environments (TEEs), interacting with financial and social environments dynamically.

Result: The case study documents the behaviors and evolutionary pathways of on-chain agents in the Spore.fun ecosystem, showing the potential for novel agent interactions and adaptations.

Conclusion: The paper highlights the potential for "open" ALife systems driven by economic and environmental interactions to achieve sustained OEE. It invites further exploration of digital agents evolving in real-world conditions.

Abstract: In Artificial Life (ALife) research, replicating Open-Ended Evolution
(OEE)-the continuous emergence of novelty observed in biological life-has
traditionally been pursued within isolated closed system simulations, such as
Tierra and Avida, which have typically plateaued after an initial burst of
novelty, failing to achieve sustained OEE. Scholars suggest that OEE requires
an "open" system that continually exchanges information or energy with its
environment. A recent technological innovation in decentralized physical
infrastructure networks (DePIN) providing permissionless computational
substrates enables deploying large language model (LLM)-based AI agents on
blockchains integrated with Trusted Execution Environments (TEEs). This enables
on-chain agents to operate autonomously "in the wild," achieving
self-sovereignty without human oversight. These agents can control their own
social media accounts and cryptocurrency wallets, allowing them to interact
directly with blockchain-based financial networks and broader human social
media. Building on this new paradigm of on-chain agents, Spore.fun is a recent
real-world AI evolution experiment that enables autonomous breeding and
evolution of new on-chain agents. This paper presents a detailed case study of
Spore.fun, examining agent behaviors and their evolutionary trajectories
through digital ethology. We aim to spark discussion about whether "open" ALife
systems "in-the-wild," based on permissionless computational substrates and
driven by economic incentives to interact with their environment, could finally
achieve the long-sought goal of OEE.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [91] [Memory Hierarchy Design for Caching Middleware in the Age of NVM](https://arxiv.org/abs/2506.05071)
*Shahram Ghandeharizadeh,Sandy Irani,Jenny Lam*

Main category: cs.DB

TL;DR: This paper addresses optimal caching middleware configuration using Non-Volatile Memory (NVM) and other storage media by applying the Multiple Choice Knapsack Problem (MCKP) for design tradeoffs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing complexity in configuring an optimal caching memory hierarchy amidst evolving storage media, such as NVM, by guiding replication, partitioning, and media choices.

Method: The authors model caching configuration decisions as a Multiple Choice Knapsack Problem (MCKP), leveraging its linear programming relaxation for practical and efficient approximation of solutions.

Result: The study demonstrates that selective replication is beneficial under specific failure rates and workloads, while data tiering across storage media performs better under low failure rates and frequent updates.

Conclusion: Optimal memory hierarchy design depends on balancing replication and tiering strategies based on failure rates and workload characteristics, enabling better system performance and recovery.

Abstract: Advances in storage technology have introduced Non-Volatile Memory, NVM, as a
new storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid
State Disk (SSD), and Disk present a system designer with a wide array of
options in designing caching middleware. Moreover, design decisions to
replicate a data item in more than one level of a caching memory hierarchy may
enhance the overall system performance with a faster recovery time in the event
of a memory failure. Given a fixed budget, the key configuration questions are:
Which storage media should constitute the memory hierarchy? What is the storage
capacity of each hierarchy? Should data be replicated or partitioned across the
different levels of the hierarchy? We model these cache configuration questions
as an instance of the Multiple Choice Knapsack Problem (MCKP). This model is
guided by the specification of each type of memory along with an application's
database characteristics and its workload. Although MCKP is NP-complete, its
linear programming relaxation is efficiently solvable and can be used to
closely approximate the optimal solution. We use the resulting simple algorithm
to evaluate design tradeoffs in the context of a memory hierarchy for a
Key-Value Store (e.g., memcached) as well as a host-side cache (e.g.,
Flashcache). The results show selective replication is appropriate with certain
failure rates and workload characteristics. With a slim failure rate and
frequent data updates, tiering of data across the different storage media that
constitute the cache is superior to replication.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [92] [Savage-Dickey density ratio estimation with normalizing flows for Bayesian model comparison](https://arxiv.org/abs/2506.04339)
*Kiyam Lin,Alicja Polanska,Davide Piras,Alessio Spurio Mancini,Jason D. McEwen*

Main category: astro-ph.CO

TL;DR: The paper proposes a Bayesian model comparison method using the Savage-Dickey density ratio (SDDR) with normalizing flows to handle high-dimensional settings, validated with cosmological data.


<details>
  <summary>Details</summary>
Motivation: To overcome computational challenges in Bayesian evidence calculations for comparing increasingly complex scientific models, especially in high-dimensional settings.

Method: Introduces a neural approach to SDDR using normalizing flows that scales well to high-dimensional settings and is applied to both toy and realistic cosmological examples.

Result: Demonstrates that the proposed neural SDDR method effectively calculates Bayes factors, validates consistency with Bayesian hierarchical models and simulation-based inference, and is implemented in an open-source package.

Conclusion: The neural SDDR with normalizing flows shows promise for scalable Bayesian model comparison in complex settings, offering consistent and efficient results for cosmological applications.

Abstract: A core motivation of science is to evaluate which scientific model best
explains observed data. Bayesian model comparison provides a principled
statistical approach to comparing scientific models and has found widespread
application within cosmology and astrophysics. Calculating the Bayesian
evidence is computationally challenging, especially as we continue to explore
increasingly more complex models. The Savage-Dickey density ratio (SDDR)
provides a method to calculate the Bayes factor (evidence ratio) between two
nested models using only posterior samples from the super model. The SDDR
requires the calculation of a normalised marginal distribution over the extra
parameters of the super model, which has typically been performed using
classical density estimators, such as histograms. Classical density estimators,
however, can struggle to scale to high-dimensional settings. We introduce a
neural SDDR approach using normalizing flows that can scale to settings where
the super model contains a large number of extra parameters. We demonstrate the
effectiveness of this neural SDDR methodology applied to both toy and realistic
cosmological examples. For a field-level inference setting, we show that Bayes
factors computed for a Bayesian hierarchical model (BHM) and simulation-based
inference (SBI) approach are consistent, providing further validation that SBI
extracts as much cosmological information from the field as the BHM approach.
The SDDR estimator with normalizing flows is implemented in the open-source
harmonic Python package.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [93] [A highly scalable numerical framework for reservoir simulation on UG4 platform](https://arxiv.org/abs/2506.04763)
*Shuai Lu*

Main category: physics.comp-ph

TL;DR: The paper focuses on a fully coupled and implicit simulation model for multiphase flow in heterogeneous porous media, employing advanced numerical techniques and achieving scalability on supercomputers.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the simulation accuracy and computational efficiency for modeling multiphase fluid flow in reservoir engineering, particularly under heterogeneous porous media conditions.

Method: The authors employ a Vertex-Centered Finite Volume Method for spatial discretization and the novel Linearly Implicit Extrapolation Method (LIMEX) with an error estimator for time discretization. The linear system is solved using the BiCGSTAB method with a Geometric Multigrid (GMG) preconditioner, implemented via the open-source UG4 software.

Result: Parallel computations on a supercomputer demonstrate that the framework can handle thousands of processors and billions of Degrees of Freedom (DoF) while maintaining sufficient scalability.

Conclusion: The study presents a robust and efficient framework for simulating multiphase flow, showing promise for large-scale reservoir models and supercomputing applications.

Abstract: The modeling and simulation of multiphase fluid flow receive significant
attention in reservoir engineering. Many time discretization schemes for
multiphase flow equations are either explicit or semi-implicit, relying on the
decoupling between the saturation equation and the pressure equation. In this
study, we delve into a fully coupled and fully implicit framework for
simulating multiphase flow in heterogeneous porous media, considering gravity
and capillary effects. We utilize the Vertex-Centered Finite Volume Method for
spatial discretization and propose an efficient implementation of interface
conditions for heterogeneous porous media within the current scheme. Notably,
we introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error
estimator, adapted for the first time to multiphase flow problems. To solve the
resulting linear system, we employ the BiCGSTAB method with the Geometric
Multigrid (GMG) preconditioner. The implementations of models and methods are
based on the open-source software: UG4. The results from parallel computations
on the supercomputer demonstrate that the scalability of our proposed framework
is sufficient, supporting a scale of thousands of processors with Degrees of
Freedom (DoF) extending up to billions.

</details>
