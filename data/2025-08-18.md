<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 94]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 30]
- [cs.SE](#cs.SE) [Total: 10]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [eess.IV](#eess.IV) [Total: 9]
- [math.NA](#math.NA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 10]
- [cs.DB](#cs.DB) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.ME](#stat.ME) [Total: 4]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: The paper addresses reasoning in ASPIC+ with first-order rules by proposing an intelligent grounding procedure using Datalog, optimizing for scalability and correctness.


<details>
  <summary>Details</summary>
Motivation: ASPIC+ examples commonly use first-order rules, but existing approaches only support propositional rules, requiring intelligent procedures for scalable grounding.

Method: The authors use Datalog translation to manage ASPIC+ grounding, combined with rule simplifications tailored to the formalism to prevent redundant groundings.

Result: The authors demonstrate the scalability of their proposed grounding approach through empirical evaluation of their prototype.

Conclusion: The proposed grounding procedure ensures correctness and scalability for reasoning with first-order instances in ASPIC+.

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [2] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: This paper introduces a novel framework for multi-agent algorithmic recourse, focusing on interactions between multiple recourse seekers and providers. It optimizes social welfare while considering real-world constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in existing algorithmic recourse research, which predominantly focuses on single-individual and single-model scenarios, whereas real-world systems involve multiple interacting agents competing for limited resources.

Method: The paper models the problem as a capacitated weighted bipartite matching problem and proposes a three-layer optimization framework: (1) basic capacitated matching, (2) capacity redistribution to minimize welfare gaps, and (3) cost-aware optimization balancing welfare with adjustment costs.

Result: The framework successfully achieves near-optimal social welfare with minimal modifications to system settings, validated on both synthetic and real-world datasets.

Conclusion: The study extends the domain of algorithmic recourse from individual-centric to system-level considerations, offering a scalable solution for enhanced social welfare while retaining individual actionability.

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [3] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: The paper proposes a framework integrating data-driven inverse optimization (L2O) and PPO-based planning to automate proton PBS treatment planning for H&N cancers, significantly improving efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: The goal is to reduce the human effort and time required for proton PBS treatment planning for H&N cancers, which traditionally involves experience-demanding parameter adjustments and theory-driven, time-intensive inverse optimization processes.

Method: A data-driven inverse optimization (L2O) method predicts update steps based on task-specific data. A PPO-based framework acts as a virtual planner adjusting objective parameters using a policy network, with the L2O optimizer computing machine-deliverable MU values.

Result: The proposed L2O-based inverse optimizer improved effectiveness by 22.97% and efficiency by 36.41% compared to traditional methods. Plans were generated in an average time of 2.55 hours, with better or comparable results to human-generated plans in terms of target coverage and OAR sparing.

Conclusion: The framework effectively automates proton PBS treatment planning, achieving clinically acceptable results faster and with less reliance on human expertise, showcasing a promising approach for scalable medical planning systems.

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [4] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Blümel,Anna Rapberger*

Main category: cs.AI

TL;DR: This paper investigates strong and weak admissibility for general assumption-based argumentation (ABA) and introduces related semantics using abstract bipolar set-based frameworks.


<details>
  <summary>Details</summary>
Motivation: Admissibility in ABA has traditionally focused on standard notions. Exploring alternatives like strong and weak admissibility can offer insights into modularity and semantic behaviors.

Method: The authors utilize abstract bipolar set-based frameworks (BSAFs) for modeling, extending weak admissibility into non-flat ABA, and introducing strong admissibility with corresponding properties.

Result: The paper finds that modularity properties hold under classical, strong, and weak admissibility. It also identifies shared shortcomings across admissible semantics and suggests resolutions.

Conclusion: Expanding admissibility notions in non-flat ABA adds theoretical depth and practical extensions, overcoming certain limitations at the semantic level.

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [5] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: The paper introduces a new dataset for evaluating Large Reasoning Models' (LRMs) ability to proactively handle incomplete mathematical problems, revealing limitations like lack of proactivity, overthinking, and hallucination.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks mainly evaluate LRMs on well-defined problems, overlooking their ability to handle incomplete information and request missing data proactively.

Method: The researchers created a dataset with two types of incomplete math problems and conducted systematic evaluations on LRMs to assess their proactive reasoning capabilities.

Result: Evaluations reveal that LRMs struggle with asking for additional information proactively, and exhibit problematic behavior like overthinking and hallucination.

Conclusion: The findings emphasize the need for developing LRMs with genuine intelligence that can effectively handle incomplete problems, rather than merely solving them.

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [6] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: This paper introduces SAGE, a dynamic framework for continual KG embeddings that adapts to update scales and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Continual knowledge graph embedding methods often fail to address varying update scales and lack thorough evaluations during graph evolution.

Method: SAGE determines embedding dimensions based on update scales and employs a Dynamic Distillation mechanism to balance knowledge preservation and integration.

Result: SAGE achieves consistent improvements in metrics like MRR (1.38%), H@1 (1.25%), and H@10 (1.6%) across seven benchmarks, outperforming competitors.

Conclusion: Adaptive embedding dimensions are crucial, and SAGE leads the field in continual KG embedding performance by bridging scalability and accuracy gaps.

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [7] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: CRAFT-GUI is a curriculum learning framework leveraging Reinforcement Learning for GUI tasks, overcoming limitations in training uniformity and reward granularity. It achieves superior performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing RL approaches for GUI tasks, mainly uniform training data treatment and coarse reward signals.

Method: Introduced CRAFT-GUI framework employing Group Relative Policy Optimization (GRPO) to account for task difficulty variations and a nuanced reward function combining rule-based signals and model evaluations.

Result: Outperformed prior state-of-the-art benchmarks, achieving 5.6% improvement on Android Control and 10.3% improvement on internal benchmarks.

Conclusion: Integrating curriculum learning with reinforcement learning significantly enhances task execution capabilities in GUI environments.

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [8] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: This paper introduces AIM-Bench, a benchmark to evaluate large language model (LLM) agents in inventory decision-making under uncertainty, revealing biases similar to human decision-making and exploring mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored capabilities and decision-making biases of large language models in inventory optimization under uncertain contexts.

Method: The authors developed AIM-Bench to assess LLMs via diverse inventory replenishment experiments, and analyzed decision biases and mitigation strategies like cognitive reflection and information sharing.

Result: Findings indicate LLMs exhibit human-like decision biases, and strategies to reduce specific biases, like pull-to-center and bullwhip effects, were explored.

Conclusion: The study emphasizes the need to consider biases in LLM deployment for inventory decisions and suggests insights for human-centered decision support system development.

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [9] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: The paper introduces Inclusion Arena, a live leaderboard for ranking LLMs and MLLMs based on human feedback during real-world applications, addressing limitations of static datasets.


<details>
  <summary>Details</summary>
Motivation: Benchmarks and leaderboards for LLMs and MLLMs largely depend on static datasets or generalized prompts, which don't effectively reflect real-world performance.

Method: Inclusion Arena collects human feedback through natural user interactions and uses the Bradley-Terry model with innovative mechanisms: Placement Matches for initial ratings and Proximity Sampling for effective model comparisons.

Result: The platform provides reliable and stable rankings with higher data transitivity than general crowdsourced datasets, while reducing risks of malicious manipulation.

Conclusion: Inclusion Arena enables the development of practically optimized LLMs and MLLMs by integrating real-world user feedback into an open, accessible platform.

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [10] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: This paper extends the concept of landmarks to stochastic domains, utilizing them in probabilistic planning to improve the UCT algorithm's performance while solving MDPs.


<details>
  <summary>Details</summary>
Motivation: The authors aim to leverage the benefits of landmarks, well-established in classical planning, to improve planning in stochastic domains.

Method: The paper formalizes probabilistic landmarks and integrates them into the UCT algorithm to decompose MDPs. The approach balances between greedy landmark achievement and final goal attainment.

Result: Improved performance of the UCT algorithm in benchmark domains using well-chosen landmarks, though the balance between short-term and long-term goals is problem-specific.

Conclusion: Landmarks can act as effective guidance tools, especially for anytime algorithms in solving MDPs.

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [11] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: The paper introduces a novel LLM-assisted planner to decompose and solve large-scale planning problems using two paradigms: LLM4Inspire for heuristic guidance and LLM4Predict for integrating domain-specific knowledge.


<details>
  <summary>Details</summary>
Motivation: The study addresses the state-space explosion issue in large-scale planning problems and the limitation of prior works which fail to integrate LLMs with domain-specific knowledge for valid plans.

Method: The proposed method decomposes large problems into simpler sub-tasks and utilizes two paradigms of LLMs: LLM4Inspire for heuristic guidance and LLM4Predict for leveraging domain-specific knowledge.

Result: Experiments confirm that the planner effectively partitions the search space and finds feasible solutions, with LLM4Predict outperforming LLM4Inspire due to its use of domain-specific knowledge.

Conclusion: Integrating domain-specific knowledge into LLMs significantly improves the efficiency of solving large-scale planning problems, demonstrating the promise of problem decomposition and LLM-assisted approaches.

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [OpenCXD: An Open Real-Device-Guided Hybrid Evaluation Framework for CXL-SSDs](https://arxiv.org/abs/2508.11477)
*Hyunsun Chung,Junhyeok Park,Taewan Noh,Seonghoon Ahn,Kihwan Kim,Ming Zhao,Youngjae Kim*

Main category: cs.AR

TL;DR: The paper introduces OpenCXD, a framework combining simulations and real hardware for evaluating performance and firmware-level interactions of Compute Express Link-enabled SSDs (CXL-SSDs).


<details>
  <summary>Details</summary>
Motivation: Evaluating the performance of CXL-SSDs is challenging due to the absence of hardware that supports native CXL.mem protocols. Existing methods fail to capture the intricate firmware-level and storage dynamics.

Method: The paper proposes OpenCXD, a hybrid evaluation framework, integrating a cycle-accurate CXL.mem simulator with a physical OpenSSD platform, allowing simulated memory requests to trigger real firmware execution.

Result: OpenCXD successfully bridges the gap between simulation and hardware, capturing device-level phenomena and providing deeper insights into CXL-SSD dynamics.

Conclusion: OpenCXD can serve as a critical tool for optimizing firmware design and advancing the development of CXL-SSDs.

Abstract: The advent of Compute Express Link (CXL) enables SSDs to participate in the
memory hierarchy as large-capacity, byte-addressable memory devices. These
CXL-enabled SSDs (CXL-SSDs) offer a promising new tier between DRAM and
traditional storage, combining NAND flash density with memory-like access
semantics. However, evaluating the performance of CXL-SSDs remains difficult
due to the lack of hardware that natively supports the CXL.mem protocol on
SSDs. As a result, most prior work relies on hybrid simulators combining CPU
models augmented with CXL.mem semantics and SSD simulators that approximate
internal flash behaviors. While effective for early-stage exploration, this
approach cannot faithfully model firmware-level interactions and low-level
storage dynamics critical to CXL-SSD performance. In this paper, we present
OpenCXD, a real-device-guided hybrid evaluation framework that bridges the gap
between simulation and hardware. OpenCXD integrates a cycle-accurate CXL.mem
simulator on the host side with a physical OpenSSD platform running real
firmware. This enables in-situ firmware execution triggered by simulated memory
requests. Through these contributions, OpenCXD reflects device-level phenomena
unobservable in simulation-only setups, providing critical insights for future
firmware design tailored to CXL-SSDs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: A2HCoder is a machine learning agent designed to simplify and enhance algorithm-to-hardware translation for wireless communication systems, using a hierarchical and fine-grained approach to improve reliability and efficiency.


<details>
  <summary>Details</summary>
Motivation: There is a substantial gap between algorithm design and hardware implementation in wireless communication systems due to mismatches in programming languages, requiring time and expertise to bridge.

Method: A2HCoder uses a hierarchical framework powered by large language models to decompose algorithms into modular blocks and translate them step-by-step. It leverages debugging and synthesis tools like MATLAB and Vitis HLS.

Result: A2HCoder demonstrated reliability and deployment efficiency in a real-world 5G wireless communication scenario, validating its capabilities.

Conclusion: The proposed framework efficiently bridges the gap between algorithm design and hardware implementation, ensuring both robustness and practicality.

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [14] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: The paper introduces PersonaTwin, a framework using large language models (LLMs) to create adaptive digital twins through demographic, behavioral, and psychometric data, delivering realistic and unbiased user simulations for better personalization.


<details>
  <summary>Details</summary>
Motivation: The need to enhance LLMs in capturing complex, multidimensional user nuances for better digital simulations and behavior analysis.

Method: PersonaTwin employs a multi-tier prompt conditioning framework using demographic, behavioral, and psychometric data. It benchmarks performance with healthcare data, ensuring accuracy through text similarity metrics and demographic parity assessments.

Result: PersonaTwin provides simulation accuracy comparable to oracle settings, achieving prediction and fairness metrics equivalent to models trained directly on individual data across GPT-4o and Llama-based models.

Conclusion: LLM-based digital twins are feasible for realistic and emotionally nuanced user modeling, aiding personalized behavior analysis with unbiased outputs.

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [15] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: The paper introduces open-weight reasoning models, gpt-oss-120b and gpt-oss-20b, optimized for accuracy and inference efficiency, and released under Apache 2.0 license.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning accuracy, reduce inference costs, and facilitate broader use and research in AI by providing open-weight models.

Method: The authors utilized a mixture-of-expert transformer architecture combined with large-scale distillation and reinforcement learning. The models were optimized for agentic capabilities such as research browsing, tool utilization, and instruction following in a chat format.

Result: The models demonstrated strong performance on benchmarks including mathematics, coding, and safety, proving effective for diverse tasks.

Conclusion: The paper contributes open-weight AI models that balance reasoning efficacy, efficiency, and usability, supporting advancements in AI research and applications.

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [16] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: This paper develops a framework to extract company risk factors from news articles, benchmarks various machine learning models, and demonstrates its application on Bloomberg articles to provide insights into company operations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to help investors and the financial market by identifying company risk factors in news articles, which is crucial for decision-making and market stability.

Method: The paper proposes a schema with seven company risk aspects, annotates 744 sampled news articles, tests machine learning models including LLMs, and fine-tunes pre-trained language models to improve performance.

Result: Zero-shot and few-shot prompting with state-of-the-art LLMs yields moderate to low results, while fine-tuned language models perform better. The framework successfully analyzes 277K Bloomberg news articles for risk insights.

Conclusion: The study concludes that pre-trained, fine-tuned models outperform LLMs in extracting company risk factors from news, making this approach valuable for assessing corporate and industry-level operations.

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [17] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: Rule2Text uses large language models to translate complex logical rules from knowledge graphs into clear natural language explanations for improved accessibility.


<details>
  <summary>Details</summary>
Motivation: The difficulty of interpreting complex, mined logical rules in knowledge graphs due to their labeling conventions and inherent complexity.

Method: The authors evaluate multiple large language models with diverse prompting strategies, employ human and LLM-based evaluation frameworks for explanation quality, and fine-tune an open-source model using human-in-the-loop feedback.

Result: The fine-tuning of the Zephyr model achieves significant improvements in explanation quality, especially for domain-specific datasets. An LLM judge framework aligns well with human evaluation results.

Conclusion: The framework enhances the interpretability of mined logical rules, making knowledge graphs more accessible and usable for humans.

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [18] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: The paper introduces a verifier-based inference-time scaling for masked diffusion language models (MDMs), improving generation quality for text-style transfer tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the generation quality of masked diffusion language models and establish them as a better alternative to autoregressive models for natural language generation.

Method: The authors propose an inference-time scaling method using a simple soft-value-based verifier with pre-trained embedding models, integrated into MDMs' denoising process.

Result: The proposed verifier improves the generation quality of MDMs and achieves better performance on text-style transfer tasks, surpassing traditional autoregressive language models.

Conclusion: Verifier-based inference scaling strengthens MDMs' position as a scalable and effective generative model for text tasks, offering significant quality improvements over existing methods.

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [19] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: This paper introduces SproutBench, a benchmarking tool to evaluate safety risks of large language models (LLMs) for children and adolescents.


<details>
  <summary>Details</summary>
Motivation: To address the lack of AI safety benchmarks specifically designed for children and adolescents, considering their developmental vulnerabilities.

Method: The paper develops and employs SproutBench, a collection of 1,283 prompts that test age-specific risks in areas like emotional, privacy, and behavioral safety, applied to 47 LLMs.

Result: The study identifies significant safety vulnerabilities among the evaluated LLMs, revealing correlations between Safety and Risk Prevention as well as conflicts between Interactivity and Age Appropriateness.

Conclusion: Insights from the paper inform practical recommendations for creating safer, child-compatible AI systems.

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [20] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: This study investigates why large language models (LLMs) struggle with cross-lingual knowledge transfer, introducing controlled experiments with synthetic multilingual data. It identifies key factors affecting knowledge unification across languages and suggests methods to enhance LLM cross-lingual performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of LLMs hallucinating when transferring knowledge between languages, a significant limitation in ensuring accurate cross-lingual performance.

Method: Small Transformer models are trained on synthetic multilingual datasets in controlled experiments to study the dynamics of representation unification across languages.

Result: The research identifies a key phase when models unify or separate representations across languages. Further, it highlights that unification depends on mutual information between training data languages and the facts, alongside tokenization strategies.

Conclusion: Insights from controlled settings reveal how pre-training dynamics affect cross-lingual knowledge transfer, introducing methods and metrics to improve it and suggesting new directions for enhancing LLMs.

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [21] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: The paper introduces a benchmark to evaluate language model agents' ability to adapt their plans amidst external failures and environmental feedback.


<details>
  <summary>Details</summary>
Motivation: Language model agents struggle to adapt to failures and feedback in complex real-world tasks, highlighting the need for systematic evaluation.

Method: The authors create a benchmark involving planning tasks solved via function calls, introducing external failures while ensuring solvability.

Result: Models often identify correct functions but fail to adapt to feedback or pursue alternative plans effectively.

Conclusion: Existing models face challenges in handling failures and adapting plans, prompting opportunities for future improvement.

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [22] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: This paper introduces a framework to evaluate sentiment and polarization-related biases in large language models using synthetic datasets, and applies it to analyze LLM biases on the Russia-Ukraine war.


<details>
  <summary>Details</summary>
Motivation: Biases in LLMs are prevalent in sensitive topics like political discourse and identity issues, yet current bias detection methods leave certain challenges unaddressed.

Method: The authors propose a framework combining sentiment metrics and systematic synthetic datasets categorized semantically for granular analysis, demonstrated using case studies with LLMs like GPT-4, Llama-3, etc.

Result: Across tested models, a bias leaning towards Ukraine was identified, with variations in semantic categories and adaptations to prompt and language modifications further influencing bias expression.

Conclusion: The framework is robust, reusable, works across topics and scenarios, and supplements other bias evaluation strategies by automating dataset generation alongside fine-grained analysis.

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [23] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: The study embeds digital dictionaries into AMR graphs using advanced language models, reduces these graphs while preserving their structure, and examines their properties concerning the symbol grounding problem.


<details>
  <summary>Details</summary>
Motivation: To explore how digital dictionaries can be embedded into Abstract Meaning Representation (AMR) graphs and investigate their relationship with the symbol grounding problem.

Method: Using pre-trained large language models, digital dictionaries are embedded into AMR digraphs. These graphs are then reduced using confluent transformations that retain circuit space.

Result: The reduced AMR digraphs maintain structural integrity and provide insights into their properties related to symbol grounding.

Conclusion: Embedding digital dictionaries into semantic graphs and analyzing their properties shows promise for addressing semantic representation challenges, notably the symbol grounding problem.

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [24] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: This paper presents RAMP, a multi-agent framework for audience curation using large language models (LLMs) that iterates planning, tool usage, verification, and improvement, achieving notable accuracy improvements and better user satisfaction.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limited literature on the reliability of large language models (LLMs) in real-world applications by focusing on a marketing task, audience curation.

Method: The RAMP framework iteratively performs planning, tool calling, output verification, and refinement to improve task quality. It also integrates a long-term memory store that retains client-specific knowledge and past interactions.

Result: By using LLM planning and long-term memory, the framework improves accuracy by 28 percentage points on 88 evaluation queries. Iterative verification significantly enhances recall (+20 percentage points) and user satisfaction, especially for ambiguous queries.

Conclusion: RAMP demonstrates that iterative verification, reflection, and memory integration can enhance the reliability and usability of LLM-based systems for dynamic industry contexts.

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [25] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: This paper introduces MoNaCo, a benchmark of 1,315 complex, natural questions requiring multiple steps to solve, designed to test the capabilities of LLMs on challenging queries.


<details>
  <summary>Details</summary>
Motivation: Existing LLM benchmarks fail to feature natural questions that are both information-seeking and time-consuming.

Method: The authors developed a decomposed annotation pipeline to scale the creation and manual answering of complex questions, forming the MoNaCo dataset.

Result: Frontier LLMs evaluated on MoNaCo achieve a maximum of 61.2% F1, displaying limitations in recall and hallucinations.

Conclusion: MoNaCo highlights the need for reasoning models capable of handling real-world complex queries, serving as a resource to track progress in LLM capabilities.

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [26] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: MobQA is a benchmark dataset to test the ability of large language models (LLMs) in understanding human mobility data through natural language question answering, revealing their limitations in semantic reasoning.


<details>
  <summary>Details</summary>
Motivation: To assess how well LLMs can semantically understand and interpret human mobility data, as current models excel in pattern prediction but lack interpretive context capabilities.

Method: MobQA contains 5,800 high-quality question-answer pairs covering factual retrieval, multiple-choice reasoning, and free-form explanations to evaluate spatial, temporal, and semantic reasoning within GPS trajectory data.

Result: The evaluation found LLMs excel in factual retrieval but struggle with semantic reasoning and explanation questions, with performance declining for longer trajectory data.

Conclusion: LLMs show progress in processing factual mobility data but face significant challenges in semantic mobility understanding, highlighting limitations in achieving deeper interpretive reasoning.

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [27] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: This study introduces the first benchmark dataset for identifying offensive language in code-mixed Tulu content from YouTube comments, evaluates deep learning models, and highlights challenges with transformer architectures in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Tulu lacks computational resources for offensive language identification despite its increasing digital footprint, necessitating foundational NLP exploration.

Method: The research collects and annotates 3,845 code-mixed Tulu YouTube comments into four categories, evaluates various deep learning architectures (e.g., GRU, BiGRU, LSTM, CNN) and transformer models (mBERT, XLM-RoBERTa).

Result: BiGRU with self-attention achieves the best performance (82% accuracy, 0.81 macro F1-score), while transformer models underperform, possibly due to pretraining limitations in low-resource, code-mixed scenarios.

Conclusion: This dataset and evaluation establish a stepping stone for offensive language identification in Tulu and similar languages, paving the way for further NLP advancements in low-resource linguistic settings.

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [28] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: This paper introduces a method for generating personalized distractors in multiple-choice questions (MCQs) by inferring each student's unique misconceptions from their past answers.


<details>
  <summary>Details</summary>
Motivation: Current distractor generation methods fail to capture individual students' diverse reasoning errors, limiting the effectiveness of diagnostic assessments.

Method: A two-stage, training-free framework uses Monte Carlo Tree Search (MCTS) to recover reasoning trajectories and simulate personalized distractors based on individual misconceptions.

Result: The proposed method outperforms existing approaches in generating plausible and tailored distractors for 140 students, while also generalizing effectively to group-level scenarios.

Conclusion: The framework offers a robust and adaptable solution for diagnostic educational assessments by tailoring distractors to students’ specific reasoning errors.

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [29] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: The paper presents a Parasitic Dual-Scale Approach to improve inference efficiency in multilingual speech-to-text translation while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing inference efficiency and performance in multilingual speech translation models, particularly for local deployments.

Method: The authors propose a Parasitic Dual-Scale Approach that incorporates enhanced speculative sampling, model compression, and knowledge distillation into the Whisper Medium model.

Result: The proposed methods enable a 40% speedup with no BLEU degradation and, when combined with distillation, achieve a 2.6x speedup over the original Whisper Medium model while enhancing performance.

Conclusion: The study demonstrates the feasibility of improving both efficiency and performance in multilingual speech models, achieving SOTA results across six languages.

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [30] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: E-CaTCH is a scalable framework optimized for the detection of multimodal misinformation on social media by leveraging event-level clustering, multi-modal alignment, and trend-aware temporal modeling.


<details>
  <summary>Details</summary>
Motivation: Multimodal misinformation detection on social media is challenging due to inconsistencies across text and visuals, temporal dynamics, and class imbalance. Current methods lack the ability to capture interconnected event structures.

Method: E-CaTCH clusters posts into pseudo-events based on textual and temporal proximity, processes textual and visual features using pre-trained BERT and ResNet encoders, refines them with self and cross-modal attentions, and models temporal evolution using a specialized LSTM with trend-aware signals.

Result: Experiments on datasets such as Fakeddit, IND, and COVID-19 MISINFOGRAPH show E-CaTCH consistently outperforms current state-of-the-art models in misinformation detection.

Conclusion: E-CaTCH proves to be robust, interpretable, generalizable, and effective for diverse misinformation detection tasks, bridging gaps in event-level modeling and cross-modal integration.

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [31] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: The paper introduces HGRAG, a novel hypergraph-based approach that enhances multi-hop question answering (MHQA) by effectively integrating structural and semantic information, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: There is a need to improve retrieval-augmented generation (RAG) approaches in MHQA tasks by addressing limitations in semantic and structural integration.

Method: HGRAG constructs entity hypergraphs with entities as nodes and passages as hyperedges, integrates semantic and structural information via hypergraph diffusion, and employs retrieval enhancement for refining context for answer generation.

Result: HGRAG achieved better QA performance and a 6× speedup in retrieval efficiency compared to state-of-the-art methods in experimental evaluations.

Conclusion: HGRAG demonstrates the potential of using hypergraphs to achieve cross-granularity connectivity between semantics and structures, effectively advancing MHQA tasks.

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [32] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: Large language models (LLMs) struggle with linguistics puzzles from low-resource languages but show improvement with morphological pre-processing.


<details>
  <summary>Details</summary>
Motivation: To evaluate and identify weaknesses in LLMs' linguistic reasoning capabilities, especially regarding low-resource languages.

Method: Analyzed LLM performance on 629 problems across 41 low-resource languages using linguistically informed features; explored pre-processing techniques like splitting words into morphemes.

Result: LLMs perform poorly with morphological complexity but better on English-like features; word splitting into morphemes improved puzzle solvability.

Conclusion: LLMs require more language-specific tokenization strategies to enhance reasoning in low-resource linguistic contexts.

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [33] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: A novel framework, LETToT, is proposed to evaluate large language models (LLMs) in tourism without relying on annotated benchmarks by leveraging expert-derived reasoning structures.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLMs in domain-specific contexts like tourism is difficult due to high costs of annotated datasets and issues like hallucinations.

Method: The study introduces LETToT, which uses expert feedback and hierarchical reasoning structures (Tree-of-Thoughts) to iteratively refine and evaluate LLMs' quality in tourism.

Result: Using LETToT, the optimized reasoning structures improved performance by 4.99-14.15% compared to baselines. It was shown that explicit reasoning benefits smaller models in accuracy and conciseness, while larger models scale well in specialized domains.

Conclusion: LETToT provides a scalable, label-free framework for evaluating LLMs in specific domains like tourism, offering a viable alternative to traditional benchmarks.

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [34] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: This paper introduces TOXIFRENCH, a dataset for French toxicity detection, and proposes a novel fine-tuning strategy that leads to state-of-the-art performance, outperforming larger language models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of development in French toxicity detection due to insufficient culturally relevant datasets.

Method: The researchers created the TOXIFRENCH dataset using a semi-automated annotation pipeline and applied a Chain-of-Thought fine-tuning strategy with dynamic weighted loss.

Result: The fine-tuned 4B model achieved state-of-the-art performance, surpassing larger models like GPT-4 and Gemini-2.5 in French toxicity detection.

Conclusion: This work demonstrates the stronger generalization and robustness of smaller language models and shows potential for extending the methodology to multilingual and safety-critical tasks.

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [35] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: The paper evaluates the emotional and sentiment responses of eight LLMs to questions about depression, anxiety, and stress across different user profiles, revealing significant differences in emotional patterns depending on the model and mental health condition.


<details>
  <summary>Details</summary>
Motivation: To explore how various LLMs handle mental health-related queries and understand their emotional and sentiment responses, which are critical for user experience and outcomes in mental health applications.

Method: Eight LLMs responded to 20 mental health-related questions across six user profiles. Their 2,880 responses were analyzed for sentiment and emotion using state-of-the-art tools, examining both model-specific and condition-specific emotional variations.

Result: Emotions like optimism, fear, and sadness were prevalent across responses, with specific LLMs displaying distinct emotional profiles (e.g., Mixtral showed high negative emotions, Llama was more optimistic). Anxiety prompts elicited high fear, depression evoked sadness, and stress produced optimism.

Conclusion: LLMs exhibit unique emotional signatures that significantly affect user interactions. Selecting the right model is crucial for mental health-related applications, while demographic framing of queries has minimal influence on emotional tone.

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [36] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: This paper addresses the problem of LLMs refusing benign prompts due to safety mechanisms. The authors propose SafeConstellations, a trajectory-shifting approach, reducing such over-refusals by up to 73%.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the issue of over-refusal in LLMs, where models reject harmless prompts due to superficial resemblance to harmful content, thereby lowering utility in practical applications.

Method: They perform a mechanistic analysis of task-specific behavior in embedding space and introduce SafeConstellations, an inference-time method to guide task trajectories toward non-refusal pathways selectively.

Result: SafeConstellations reduces over-refusal rates by up to 73% while preserving the model's general utility.

Conclusion: The proposed method provides a principled and effective approach to mitigate over-refusal behaviors without significantly compromising the utility of LLMs.

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [37] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: This paper presents SGSimEval, a benchmark designed to improve evaluation methods for automatic survey generation systems by using human preference metrics and multifaceted metrics combining LLM-based scoring and quantitative assessments.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for automatic survey generation are limited by biases, lack of human preference integration, and over-reliance on LLMs for judging.

Method: The authors introduce SGSimEval, which evaluates survey generation systems by assessing outline, content, and references while combining LLM-based scoring, quantitative metrics, and human preference metrics.

Result: Experiments showed ASG systems perform well in outline creation, but need improvement in generating survey content and references. SGSimEval showed strong alignment with human evaluations.

Conclusion: SGSimEval offers a robust benchmarking framework for evaluating automatic survey generation systems, addressing limitations of previous methods and enhancing evaluation consistency and reliability.

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [38] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: The paper evaluates the effectiveness of 4-bit quantization techniques, GSQ and GPTQ, on multiple NLP tasks across three different language models, analyzing accuracy and efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: To enhance the accessibility of large language models (LLMs) by reducing memory usage and computational costs without sacrificing performance.

Method: Applied 4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer Quantization (GPTQ) techniques to three models (LLaMA 1B, Qwen 0.5B, PHI 1.5B) and benchmarked their performance on diverse NLP tasks, including MS MARCO, BoolQ, and GSM8K datasets.

Result: The study highlights trade-offs between compression techniques and task performance, measuring accuracy, latency, and throughput to provide actionable insights.

Conclusion: Low-bit quantization is a potential pathway for real-world deployment of highly efficient LLMs, with benchmarks for GSQ and GPTQ techniques serving as a reference for further research.

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [39] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: The paper introduces SpecDetect, leveraging spectral energy analysis in the frequency domain to detect text generated by large language models more efficiently and accurately through signal processing.


<details>
  <summary>Details</summary>
Motivation: The widespread use of large language models (LLMs) necessitates robust detection methods to distinguish between human and machine-generated text accurately and efficiently.

Method: The authors propose reframing text detection as a signal processing problem using spectral analysis techniques such as Discrete Fourier Transform (DFT) and Short-Time Fourier Transform (STFT) to analyze token-level log-probabilities.

Result: Human-written texts consistently exhibit higher spectral energy compared to LLM-generated texts due to more pronounced amplitude fluctuations, enabling SpecDetect and its enhanced version SpecDetect++ to outperform existing detectors while being faster.

Conclusion: Signal processing techniques, particularly spectral energy analysis, provide a powerful, efficient, and interpretable method for distinguishing LLM-generated text, opening new avenues for detection methodologies.

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [40] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: This paper investigates extracting feedback indicators from student submissions in a language learning course using the Llama 3.1 language model, finding high alignment with human ratings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance student learning and optimize teacher efforts by automating the generation of targeted, timely feedback based on extracted indicators from student submissions.

Method: The study explores the initial phase of employing the Llama 3.1 large language model to extract relevant feedback indicators and compares these indicators with human ratings across diverse feedback criteria.

Result: The findings reveal statistically significant strong correlations between indicators generated by the LLM and human ratings, including for unexpected indicator-criteria combinations.

Conclusion: The paper concludes that using LLMs like Llama 3.1 to extract indicators is a promising approach for generating explainable and transparent formative feedback in educational settings.

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [41] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: The paper evaluates 5 methods to enhance prompt robustness in Large Language Models across multiple tasks, focusing on sensitivity to subtle formatting changes.


<details>
  <summary>Details</summary>
Motivation: Improving robustness of LLMs to handle non-semantic variations in prompts effectively for reliable real-world applications.

Method: Systematic evaluation of 5 robustness techniques across 8 models on 52 tasks using Natural Instructions dataset, including analysis on GPT-4.1 and DeepSeek V3.

Result: Insights into the effectiveness of robustness strategies across models, identifying strengths and limitations under varied distribution shifts.

Conclusion: Provides actionable strategies for enhancing LLM stability and reliability, enabling informed decision-making for practitioners.

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [42] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: The paper presents a novel approach for integrating reasoning and retrieval-augmented generation (RAG) into a lightweight, resource-efficient language model for domain-specific, secure environments.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address increasing demand for privacy-preserving and performant RAG systems that can operate under resource constraints without relying on large-scale models or external APIs.

Method: The proposed system incorporates a dense retriever with fine-tuned Qwen2.5-Instruct models using synthetic query generation and reasoning traces derived from advanced models, applied to a curated corpus like NHS condition pages. It also examines document compression, synthetic data design, and reasoning-aware fine-tuning.

Result: The approach delivers significant improvements in answer accuracy and consistency, with performance approaching frontier-level models, and it is feasible for secure, local deployment.

Conclusion: The system demonstrates the viability of building lean, domain-specific RAG systems with strong performance, sharing implementation details and code for broader applicability and reproducibility.

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [43] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: The paper proposes a method to generate explanations for neural network predictions by masking non-indicative input parts, using gradient-based optimization and regularization.


<details>
  <summary>Details</summary>
Motivation: To address the growing need for interpretability of neural-network models in NLP and computer vision, which often operate as black-boxes.

Method: Using gradient-based optimization and a novel regularization scheme that ensures sufficiency, comprehensiveness, and compactness of explanations.

Result: Demonstrated high-quality extractive explanations for both textual and image classifications using the proposed method.

Conclusion: Bridges model interpretability and rationale extraction, showing that effective explanation extraction can be achieved without specialized models, and principles for NLP are applicable across multiple input types.

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [44] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: The paper proposes a stable, simplified, and state-of-the-art approach for training rationalized transformer classifiers, focusing on human-aligned token scoring.


<details>
  <summary>Details</summary>
Motivation: Existing methods for rationalized transformer classifiers are prone to training instabilities and inefficiencies, requiring distinct models for different tasks.

Method: The authors develop an end-to-end differentiable training paradigm where a single model serves the roles of rationale selector, classifier, and complement classifier, extended for better human-aligned rationale generation.

Result: The new approach achieves substantial improvement in alignment with human annotations and overcomes training instabilities found in previous methods.

Conclusion: The paper presents a more efficient, stable, and human-aligned method for training rationalized classifiers by unifying multiple tasks into a single model.

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [45] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: This paper examines whether the value system of large language models (LLMs) can be modified by fine-tuning them on answers to value survey questions, demonstrating success in both survey responses and downstream behavioral changes.


<details>
  <summary>Details</summary>
Motivation: To explore a lightweight method to steer large language models (LLMs) towards desired human value systems, as existing approaches require extensive training data.

Method: The researchers fine-tuned open-source LLMs on responses to value survey questions. They then evaluated the impact of this fine-tuning using both in-domain held-out survey questions and out-of-domain benchmarks, including a dataset of contextualized moral judgments based on Reddit posts and text-based adventure games.

Result: The proposed fine-tuning approach successfully altered the models' baseline value responses in surveys and induced noticeable shifts in behavior in both in-domain and out-of-domain evaluation tasks.

Conclusion: Fine-tuning LLMs on value survey responses is an effective and simple method for aligning their behavior with specific human values, impacting both survey answers and broader task-based decision-making settings.

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [46] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: The paper introduces HumorPlanSearch, a modular pipeline that generates context-aware humor using various strategies like templates, knowledge graphs, and iterative revisions, evaluated using a novel Humor Generation Score (HGS).


<details>
  <summary>Details</summary>
Motivation: Humor generation with Large Language Models often produces generic or context-insensitive jokes, failing to align with listeners' cultural and situational needs.

Method: The HumorPlanSearch pipeline incorporates Plan-Search for tailored humor strategies, Humor Chain-of-Thought templates, a Knowledge Graph for historical strategies, novelty filters, and a revision loop, evaluated via the Humor Generation Score.

Result: HumorPlanSearch demonstrated a 15.4% improvement in mean HGS over the baseline when tested on nine topics with feedback from human judges.

Conclusion: By emphasizing cultural and contextual understanding at every stage, HumorPlanSearch significantly enhances AI-driven humor quality and contextual appropriateness.

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [47] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: The paper investigates how large language models (LLMs) misclassify anti-sexist speech as harmful, especially during politically charged events featuring female MPs in the UK.


<details>
  <summary>Details</summary>
Motivation: To understand the challenges LLMs face in accurately distinguishing anti-sexist speech from sexism, which is vital for fostering democratic debate online.

Method: Analyzed the performance of five LLMs using political tweets from high-salience events involving female Members of Parliament in the UK in 2022.

Result: Models often misclassified anti-sexist speech as harmful, particularly during events where styles of harm and resistance converged.

Conclusion: Moderation systems need nuanced, context-aware designs, incorporating human review and counter-speech data for safeguarding resistance speech effectively.

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [48] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: The paper introduces CoDiEmb, a method to simultaneously optimize text embeddings for Information Retrieval and Semantic Textual Similarity, addressing trade-offs between these tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome negative transfer issues in creating unified text embeddings that work well for the divergent tasks of Information Retrieval and Semantic Textual Similarity.

Method: The CoDiEmb framework employs task-specialized objectives, a delta-guided model fusion strategy, and a single-stage, efficient training pipeline.

Result: Experiments on 15 benchmarks show CoDiEmb mitigates trade-offs between tasks and enhances embedding space properties.

Conclusion: The CoDiEmb framework successfully reconciles divergent task requirements, offering practical and measurable improvements in text embedding performance.

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [49] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: This paper investigates whether how supplementary information in structured JSON prompts affects sentiment analysis performance with a lightweight 3B parameter language model.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of typical sentiment classification models that focus solely on textual reviews, incorporating additional information informed by marketing theories such as prospect theory.

Method: The authors tested natural language versus structured JSON-formatted prompts with a 3B parameter language model across Yelp review categories, measuring Macro-F1, RMSE, and contextual reasoning.

Result: Using structured JSON prompts with supplementary information improved Macro-F1 by 1.6% and 4%, reduced RMSE by 16% and 9.1% across Restaurant and Nightlife review datasets.

Conclusion: Structured prompting, particularly with JSON, significantly enhances sentiment analysis performance without needing fine-tuning, providing a viable solution for resource-limited environments.

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [50] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: This paper explores whether large language models (LLMs) exhibit speciesist biases and assesses their ethical tendencies towards non-human animals across three paradigms: benchmarks, psychological measures, and text-generation tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to investigate ethical biases in LLMs, particularly focusing on speciesist discrimination and how they morally evaluate non-human animals.

Method: The analysis involves three approaches: SpeciesismBench for evaluating recognition and moral evaluation of speciesist statements; psychological measures comparing LLM responses to human behaviors; and text-generation tasks assessing LLM rationalizations or resistance to speciesist views.

Result: LLMs detect speciesist statements but generally don't condemn them, showing mixed results in ethical decisions about humans and animals. They often align with entrenched cultural norms, rationalizing harm toward farmed animals.

Conclusion: LLMs reflect a blend of progressive and traditional human ethics but tend to perpetuate speciesist biases influenced by societal norms. The authors emphasize incorporating non-human moral considerations into AI fairness frameworks to mitigate speciesism.

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>


### [51] [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
*Maria Ryskina,Greta Tuckute,Alexander Fung,Ashley Malkin,Evelina Fedorenko*

Main category: cs.CL

TL;DR: The paper studies the relationship between language models (LMs) and brain processes, finding evidence that LMs may internally represent conceptual meaning across different modalities.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of separating language representations from conceptual meaning representations in both cognitive sciences and language models.

Method: The authors use two neural metrics: brain activation during sentence processing and a measure of cross-modal meaning consistency, analyzed using fMRI data from prior studies.

Result: Language models, whether language-only or language-vision, align better with brain areas showing high meaning consistency, even in regions less sensitive to linguistic processing.

Conclusion: Language models may capture cross-modal conceptual meaning, bridging insights between neuroscience and artificial intelligence.

Abstract: Cognitive science and neuroscience have long faced the challenge of
disentangling representations of language from representations of conceptual
meaning. As the same problem arises in today's language models (LMs), we
investigate the relationship between LM--brain alignment and two neural
metrics: (1) the level of brain activation during processing of sentences,
targeting linguistic processing, and (2) a novel measure of meaning consistency
across input modalities, which quantifies how consistently a brain region
responds to the same concept across paradigms (sentence, word cloud, image)
using an fMRI dataset (Pereira et al., 2018). Our experiments show that both
language-only and language-vision models predict the signal better in more
meaning-consistent areas of the brain, even when these areas are not strongly
sensitive to language processing, suggesting that LMs might internally
represent cross-modal conceptual meaning.

</details>


### [52] [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
*Jinpeng Hu,Ao Wang,Qianqian Xie,Hui Ma,Zhuo Li,Dan Guo*

Main category: cs.CL

TL;DR: The paper introduces an AI-driven multi-agent framework for mental health assessment simulating doctor-patient dialogues, improving upon static text-based methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional clinician-based and static text analysis methods in mental health assessments.

Method: The framework involves multiple specialized agents handling questioning, evaluation, scoring, and updating. It features adaptive questioning and tree-structured memory for dynamic interaction.

Result: The proposed method achieves superior performance on the DAIC-WOZ dataset compared to existing methods.

Conclusion: The approach enhances automated mental health evaluation through adaptive questioning and dynamic memory frameworks, effectively simulating clinical dialogues.

Abstract: Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

</details>


### [53] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: A new framework, DR. SAF, improves large language models' efficiency and accuracy in reasoning tasks by dynamically adjusting their reasoning depth, achieving substantial gains in token efficiency and reduced training time.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address inefficiencies in reasoning tasks performed by large language models, particularly focusing on reducing redundancy and delays caused by long Chain-of-Thought approaches.

Method: The paper introduces the Dynamic Reasoning-Boundary Self-Awareness Framework (DR. SAF) composed of three components: Boundary Self-Awareness Alignment, Adaptive Reward Management, and Boundary Preservation Mechanism, enabling dynamic adjustment of reasoning depth.

Result: Experimental results show DR. SAF reduced response tokens by 49.27% with minimal accuracy loss, achieved a 6.59x gain in token efficiency, and reduced training time by 5x. During extreme training, it surpassed traditional models with over 16% accuracy improvement.

Conclusion: DR. SAF enhances reasoning efficiency without sacrificing performance, making it suitable for resource-constrained environments and improving accuracy in comparison to traditional instruction-based models.

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>


### [54] [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
*Greta Tuckute,Klemen Kotar,Evelina Fedorenko,Daniel L. K. Yamins*

Main category: cs.CL

TL;DR: The paper introduces AuriStream, a two-stage biologically inspired model mimicking human auditory processing for speech representation, excelling in diverse speech tasks and enabling audio predictions.


<details>
  <summary>Details</summary>
Motivation: To develop a speech representation model inspired by human auditory processes that efficiently performs various speech tasks.

Method: AuriStream employs a two-stage process: first, converting raw audio into time-frequency representation and discrete cochlear tokens; second, using an autoregressive sequence model on these tokens.

Result: AuriStream achieves strong performance in speech tasks, learns meaningful speech representations, and can generate audio continuations for better prediction insights.

Conclusion: AuriStream advances speech representation learning with a human-like efficient framework, excelling in tasks and enhancing understanding of its prediction processes.

Abstract: We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

</details>


### [55] [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
*Rob Reijtenbach,Suzan Verberne,Gijs Wijnholds*

Main category: cs.CL

TL;DR: The paper introduces and evaluates a synthetic dataset, based on SNLI and generated using Stable Diffusion, for training visual entailment models, showing only minor performance drops compared to real-world data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing small and sparse datasets for visual entailment by creating a scalable synthetic alternative, leveraging generative image models to reduce manual labor.

Method: The researchers generated a synthetic dataset by replacing SNLI textual premises with images created using Stable Diffusion. They evaluated the dataset both intrinsically and extrinsically, training visual entailment classifiers with CLIP feature vectors.

Result: When compared to models trained on real data, synthetic data produced a slight drop in F-scores: from 0.703 to 0.686 on SNLI-VE and from 0.400 to 0.384 on SICK-VTE.

Conclusion: The study concludes that synthetic data can serve as a viable solution in data-sparse environments, efficiently supporting the training of visual entailment models with minimal performance degradation.

Abstract: In this paper we present and validate a new synthetic dataset for training
visual entailment models. Existing datasets for visual entailment are small and
sparse compared to datasets for textual entailment. Manually creating datasets
is labor-intensive. We base our synthetic dataset on the SNLI dataset for
textual entailment. We take the premise text from SNLI as input prompts in a
generative image model, Stable Diffusion, creating an image to replace each
textual premise. We evaluate our dataset both intrinsically and extrinsically.
For extrinsic evaluation, we evaluate the validity of the generated images by
using them as training data for a visual entailment classifier based on CLIP
feature vectors. We find that synthetic training data only leads to a slight
drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when
trained on real data. We also compare the quality of our generated training
data to original training data on another dataset: SICK-VTE. Again, there is
only a slight drop in F-score: from 0.400 to 0.384. These results indicate that
in settings with data sparsity, synthetic data can be a promising solution for
training visual entailment models.

</details>


### [56] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: TinyTim is a fine-tuned large language model based on the text of James Joyce's 'Finnegans Wake', showing unique generative traits.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of fine-tuning language models on specific, unconventional texts to enhance creativity and problem-solving.

Method: Fine-tuned a large language model on the text of 'Finnegans Wake' and quantitatively evaluated its generative performance compared to baseline models.

Result: TinyTim V1 exhibited high lexical diversity and low semantic coherence, resulting in a statistically distinct generative profile.

Conclusion: Fine-tuning on niche texts can produce models that act as divergent knowledge sources, contributing to creative and problem-solving processes.

Abstract: This work introduces TinyTim, a family of large language models fine-tuned on
James Joyce's `Finnegans Wake'. Through quantitative evaluation against
baseline models, we demonstrate that TinyTim V1 produces a statistically
distinct generative profile characterized by high lexical diversity and low
semantic coherence. These findings are interpreted through theories of
creativity and complex problem-solving, arguing that such specialized models
can function as divergent knowledge sources within more extensive creative
architectures, powering automated discovery mechanisms in diverse settings.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder](https://arxiv.org/abs/2508.10918)
*Samantha Aziz,Oleg Komogortsev*

Main category: cs.CV

TL;DR: The paper introduces a privacy-enhancing method for gaze signals using a latent-noise autoencoder, reducing re-identification risks while preserving data utility.


<details>
  <summary>Details</summary>
Motivation: Protecting user privacy in gaze-based systems by minimizing the risk of re-identification across play sessions without compromising usability for other tasks.

Method: Using a latent-noise autoencoder designed to obfuscate biometric identifiability while preserving physiologically realistic gaze patterns.

Result: The approach significantly reduces the ability to re-identify users while only minimally affecting the utility of gaze data for prediction tasks.

Conclusion: The proposed mechanism offers an effective balance between privacy and utility, enhancing privacy in gaze-based systems and ensuring usability for downstream analysis.

Abstract: We present a privacy-enhancing mechanism for gaze signals using a
latent-noise autoencoder that prevents users from being re-identified across
play sessions without their consent, while retaining the usability of the data
for benign tasks. We evaluate privacy-utility trade-offs across biometric
identification and gaze prediction tasks, showing that our approach
significantly reduces biometric identifiability with minimal utility
degradation. Unlike prior methods in this direction, our framework retains
physiologically plausible gaze patterns suitable for downstream use, which
produces favorable privacy-utility trade-off. This work advances privacy in
gaze-based systems by providing a usable and effective mechanism for protecting
sensitive gaze data.

</details>


### [58] [A Survey on Video Temporal Grounding with Multimodal Large Language Model](https://arxiv.org/abs/2508.10922)
*Jianlong Wu,Wei Liu,Ye Liu,Meng Liu,Liqiang Nie,Zhouchen Lin,Chang Wen Chen*

Main category: cs.CV

TL;DR: The paper offers a survey focusing on video temporal grounding (VTG) techniques driven by multimodal large language models (MLLMs), exploring their functional architecture, training paradigms, and video processing methodologies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to fill the gap in systematic reviews specific to VTG methods that utilize MLLMs, which surpass traditional approaches in performance and generalization across diverse settings.

Method: A three-dimensional taxonomy is applied for analysis: functional roles of MLLMs, training strategies for temporal reasoning, and video feature processing techniques, along with evaluation benchmarks and empirical findings.

Result: The survey systematically organizes current research, highlighting advancements and limitations, while pointing out areas for improvement and future explorations.

Conclusion: The authors conclude by identifying limitations in VTG-MLLM approaches and suggest research directions to advance the field further.

Abstract: The recent advancement in video temporal grounding (VTG) has significantly
enhanced fine-grained video understanding, primarily driven by multimodal large
language models (MLLMs). With superior multimodal comprehension and reasoning
abilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing
traditional fine-tuned methods. They not only achieve competitive performance
but also excel in generalization across zero-shot, multi-task, and multi-domain
settings. Despite extensive surveys on general video-language understanding,
comprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill
this gap, this survey systematically examines current research on VTG-MLLMs
through a three-dimensional taxonomy: 1) the functional roles of MLLMs,
highlighting their architectural significance; 2) training paradigms, analyzing
strategies for temporal reasoning and task adaptation; and 3) video feature
processing techniques, which determine spatiotemporal representation
effectiveness. We further discuss benchmark datasets, evaluation protocols, and
summarize empirical findings. Finally, we identify existing limitations and
propose promising research directions. For additional resources and details,
readers are encouraged to visit our repository at
https://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.

</details>


### [59] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \underline{V}alue \underline{S}ign \underline{F}lip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: The paper introduces Value Sign Flip (VSF), a method to improve negative prompt guidance in image generation models by dynamically altering attention values, enhancing performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to enhance negative prompt guidance in few-step diffusion and flow-matching image generation models, addressing limitations of existing methods such as classifier-free guidance and NASA.

Method: The VSF method flips the sign of attention values from negative prompts to suppress undesired content dynamically. It integrates with models using MMDiT-style architectures and cross-attention techniques.

Result: VSF demonstrates superior performance in datasets with complex prompt pairs, excelling in static and video generation tasks. It improves negative prompt adherence while maintaining competitive image quality.

Conclusion: VSF offers an efficient and effective solution for negative prompt guidance, outperforming previous methods in adherence and maintaining quality in few-step models. The method requires minimal computational resources.

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for
incorporating negative prompt guidance in few-step diffusion and flow-matching
image generation models. Unlike existing approaches such as classifier-free
guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by
flipping the sign of attention values from negative prompts. Our method
requires only small computational overhead and integrates effectively with
MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as
cross-attention-based models like Wan. We validate VSF on challenging datasets
with complex prompt pairs and demonstrate superior performance in both static
image and video generation tasks. Experimental results show that VSF
significantly improves negative prompt adherence compared to prior methods in
few-step models, and even CFG in non-few-step models, while maintaining
competitive image quality. Code and ComfyUI node are available in
https://github.com/weathon/VSF/tree/main.

</details>


### [60] [Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications](https://arxiv.org/abs/2508.10933)
*Yoli Shavit,Yosi Keller*

Main category: cs.CV

TL;DR: The paper improves camera localization accuracy in retail using Camera Pose Auto-Encoders (PAEs) refined by Relative Pose Regression (RPR), reducing data needs.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and accurate camera localization in retail environments, improving customer experiences, inventory management, and autonomous functions.

Method: The authors extend PAEs for Relative Pose Regression (RPR) and introduce a re-localization approach that refines Absolute Pose Regression (APR) predictions using these PAEs without extra storage.

Result: The proposed method enhances APR localization accuracy, performs well on indoor benchmarks, and achieves competitive results using only 30% of the training data.

Conclusion: The method shows that APR can be improved with PAE-based RPR refinements, offering an accurate, efficient solution for data-scarce retail applications.

Abstract: Accurate camera localization is crucial for modern retail environments,
enabling enhanced customer experiences, streamlined inventory management, and
autonomous operations. While Absolute Pose Regression (APR) from a single image
offers a promising solution, approaches that incorporate visual and spatial
scene priors tend to achieve higher accuracy. Camera Pose Auto-Encoders (PAEs)
have recently been introduced to embed such priors into APR. In this work, we
extend PAEs to the task of Relative Pose Regression (RPR) and propose a novel
re-localization scheme that refines APR predictions using PAE-based RPR,
without requiring additional storage of images or pose data. We first introduce
PAE-based RPR and establish its effectiveness by comparing it with image-based
RPR models of equivalent architectures. We then demonstrate that our refinement
strategy, driven by a PAE-based RPR, enhances APR localization accuracy on
indoor benchmarks. Notably, our method is shown to achieve competitive
performance even when trained with only 30% of the data, substantially reducing
the data collection burden for retail deployment. Our code and pre-trained
models are available at: https://github.com/yolish/camera-pose-auto-encoders

</details>


### [61] [ViPE: Video Pose Engine for 3D Geometric Perception](https://arxiv.org/abs/2508.10934)
*Jiahui Huang,Qunjie Zhou,Hesam Rabeti,Aleksandr Korovko,Huan Ling,Xuanchi Ren,Tianchang Shen,Jun Gao,Dmitry Slepichev,Chen-Hsuan Lin,Jiawei Ren,Kevin Xie,Joydeep Biswas,Laura Leal-Taixe,Sanja Fidler*

Main category: cs.CV

TL;DR: The paper introduces ViPE, a video processing engine for efficient 3D perception, capable of handling varied scenarios and camera models. It generates robust camera intrinsics, motion data, and depth maps, and outperforms baselines significantly.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of acquiring consistent and precise 3D annotations from in-the-wild videos, which is crucial for spatial AI systems.

Method: Development of ViPE, a processing engine that estimates camera parameters and depth maps, benchmarked on datasets, and annotates a large collection of videos with precise 3D information.

Result: ViPE surpasses existing baseline methods by 18%/50% on TUM/KITTI and achieves 3-5FPS on a single GPU. Annotated dataset comprises 96M frames, including real-world and AI-generated videos.

Conclusion: ViPE improves 3D perception tasks, bridges existing data gaps, and provides open-source tools and datasets to support spatial AI research and development.

Abstract: Accurate 3D geometric perception is an important prerequisite for a wide
range of spatial AI systems. While state-of-the-art methods depend on
large-scale training data, acquiring consistent and precise 3D annotations from
in-the-wild videos remains a key challenge. In this work, we introduce ViPE, a
handy and versatile video processing engine designed to bridge this gap. ViPE
efficiently estimates camera intrinsics, camera motion, and dense, near-metric
depth maps from unconstrained raw videos. It is robust to diverse scenarios,
including dynamic selfie videos, cinematic shots, or dashcams, and supports
various camera models such as pinhole, wide-angle, and 360{\deg} panoramas. We
have benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing
uncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and
runs at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to
annotate a large-scale collection of videos. This collection includes around
100K real-world internet videos, 1M high-quality AI-generated videos, and 2K
panoramic videos, totaling approximately 96M frames -- all annotated with
accurate camera poses and dense depth maps. We open-source ViPE and the
annotated dataset with the hope of accelerating the development of spatial AI
systems.

</details>


### [62] [HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model](https://arxiv.org/abs/2508.10935)
*Qi Liu,Yabei Li,Hongsong Wang,Lei He*

Main category: cs.CV

TL;DR: The paper introduces HQ-OV3D, a framework to improve pseudo-label quality for open-vocabulary 3D detection by focusing on geometric precision.


<details>
  <summary>Details</summary>
Motivation: Traditional closed-set 3D detection frameworks are inadequate for open-world scenarios, and existing methods fail to ensure geometric precision in pseudo-label generation.

Method: The authors propose the HQ-OV3D framework, featuring an Intra-Modality Cross-Validated (IMCV) Proposal Generator for initial 3D proposals and an Annotated-Class Assisted (ACA) Denoiser to refine them using geometric priors.

Result: The proposed method improves the mean Average Precision (mAP) on novel classes by 7.37%, surpassing existing approaches.

Conclusion: HQ-OV3D enhances pseudo-label quality and serves as both an effective detector for open-vocabulary 3D detection and a high-quality pseudo-label generator for related pipelines.

Abstract: Traditional closed-set 3D detection frameworks fail to meet the demands of
open-world applications like autonomous driving. Existing open-vocabulary 3D
detection methods typically adopt a two-stage pipeline consisting of
pseudo-label generation followed by semantic alignment. While vision-language
models (VLMs) recently have dramatically improved the semantic accuracy of
pseudo-labels, their geometric quality, particularly bounding box precision,
remains commonly neglected.To address this issue, we propose a High Box Quality
Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and
refine high-quality pseudo-labels for open-vocabulary classes. The framework
comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal
Generator that utilizes cross-modality geometric consistency to generate
high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)
Denoiser that progressively refines 3D proposals by leveraging geometric priors
from annotated categories through a DDIM-based denoising mechanism.Compared to
the state-of-the-art method, training with pseudo-labels generated by our
approach achieves a 7.37% improvement in mAP on novel classes, demonstrating
the superior quality of the pseudo-labels produced by our framework. HQ-OV3D
can serve not only as a strong standalone open-vocabulary 3D detector but also
as a plug-in high-quality pseudo-label generator for existing open-vocabulary
detection or annotation pipelines.

</details>


### [63] [Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction](https://arxiv.org/abs/2508.10936)
*Cheng Chen,Hao Huang,Saurabh Bagchi*

Main category: cs.CV

TL;DR: The paper introduces a new method for collaborative 3D semantic occupancy prediction using sparse Gaussian splatting, enabling better performance with reduced communication overhead.


<details>
  <summary>Details</summary>
Motivation: Address challenges in collaborative perception by overcoming occlusion, extending sensing range, and reducing communication costs associated with dense data sharing.

Method: Utilized sparse 3D semantic Gaussian splatting to share and fuse intermediate primitives, enabling cross-agent fusion, joint geometry-semantic encoding, and object-centric messaging.

Result: The proposed method improves mIoU by +8.42 and +3.28 points compared to single-agent and baseline collaborative methods, respectively. It also enhances IoU by +5.11 and +22.41 points with reduced communication volume.

Conclusion: The approach proves superior in collaborative perception, delivering enhanced accuracy and efficiency even under restricted communication conditions, enabling practical applications.

Abstract: Collaborative perception enables connected vehicles to share information,
overcoming occlusions and extending the limited sensing range inherent in
single-agent (non-collaborative) systems. Existing vision-only methods for 3D
semantic occupancy prediction commonly rely on dense 3D voxels, which incur
high communication costs, or 2D planar features, which require accurate depth
estimation or additional supervision, limiting their applicability to
collaborative scenarios. To address these challenges, we propose the first
approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D
semantic occupancy prediction. By sharing and fusing intermediate Gaussian
primitives, our method provides three benefits: a neighborhood-based
cross-agent fusion that removes duplicates and suppresses noisy or inconsistent
Gaussians; a joint encoding of geometry and semantics in each primitive, which
reduces reliance on depth supervision and allows simple rigid alignment; and
sparse, object-centric messages that preserve structural information while
reducing communication volume. Extensive experiments demonstrate that our
approach outperforms single-agent perception and baseline collaborative methods
by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,
respectively. When further reducing the number of transmitted Gaussians, our
method still achieves a +1.9 improvement in mIoU, using only 34.6%
communication volume, highlighting robust performance under limited
communication budgets.

</details>


### [64] [Personalized Face Super-Resolution with Identity Decoupling and Fitting](https://arxiv.org/abs/2508.10937)
*Jiarui Yang,Hang Guo,Wen Huang,Tao Dai,Shutao Xia*

Main category: cs.CV

TL;DR: This paper introduces IDFSR, a face super-resolution method that retains identity consistency and reduces hallucination effects under severe image degradation conditions such as scales greater than 8x.


<details>
  <summary>Details</summary>
Motivation: Existing face super-resolution methods struggle to maintain ID consistency in scenarios with extreme degradation, where crucial facial details and ID information are lost, leading to unrealistic reconstructed faces.

Method: The proposed IDFSR method includes masking unreliable ID cues in low-resolution images, warping a reference image for style guidance, and integrating ID embeddings for fine-grained ID modeling. A diffusion-based model is pre-trained to separate style and ID information, followed by lightweight fine-tuning with a small set of target ID images.

Result: IDFSR significantly enhances ID consistency and perceptual quality, outperforming existing methods, especially under extreme degradation scenarios.

Conclusion: The IDFSR approach effectively addresses ID loss and hallucinatory outputs in degraded inputs, providing realistic and ID-consistent face reconstructions.

Abstract: In recent years, face super-resolution (FSR) methods have achieved remarkable
progress, generally maintaining high image fidelity and identity (ID)
consistency under standard settings. However, in extreme degradation scenarios
(e.g., scale $> 8\times$), critical attributes and ID information are often
severely lost in the input image, making it difficult for conventional models
to reconstruct realistic and ID-consistent faces. Existing methods tend to
generate hallucinated faces under such conditions, producing restored images
lacking authentic ID constraints. To address this challenge, we propose a novel
FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID
restoration under large scaling factors while mitigating hallucination effects.
Our approach involves three key designs: 1) \textbf{Masking} the facial region
in the low-resolution (LR) image to eliminate unreliable ID cues; 2)
\textbf{Warping} a reference image to align with the LR input, providing style
guidance; 3) Leveraging \textbf{ID embeddings} extracted from ground truth (GT)
images for fine-grained ID modeling and personalized adaptation. We first
pretrain a diffusion-based model to explicitly decouple style and ID by forcing
it to reconstruct masked LR face regions using both style and identity
embeddings. Subsequently, we freeze most network parameters and perform
lightweight fine-tuning of the ID embedding using a small set of target ID
images. This embedding encodes fine-grained facial attributes and precise ID
information, significantly improving both ID consistency and perceptual
quality. Extensive quantitative evaluations and visual comparisons demonstrate
that the proposed IDFSR substantially outperforms existing approaches under
extreme degradation, particularly achieving superior performance on ID
consistency.

</details>


### [65] [Deep Learning for Automated Identification of Vietnamese Timber Species: A Tool for Ecological Monitoring and Conservation](https://arxiv.org/abs/2508.10938)
*Tianyu Song,Van-Doan Duong,Thi-Phuong Le,Ton Viet Ta*

Main category: cs.CV

TL;DR: This study examines deep learning for automating the classification of ten wood species in Vietnam, achieving high accuracy (99.29%) with ShuffleNetV2.


<details>
  <summary>Details</summary>
Motivation: The paper aims to simplify and automate the labor-intensive process of wood species identification, which is crucial for ecological monitoring and conservation.

Method: The authors used a custom image dataset of wood samples and evaluated five deep learning models (e.g., ResNet50, ShuffleNetV2) to classify species.

Result: ShuffleNetV2 emerged as the best model, achieving an average classification accuracy of 99.29% and F1-score of 99.35% over 20 trials.

Conclusion: The study demonstrates the feasibility of lightweight deep learning models for accurate, scalable, and real-time wood species identification in resource-limited settings.

Abstract: Accurate identification of wood species plays a critical role in ecological
monitoring, biodiversity conservation, and sustainable forest management.
Traditional classification approaches relying on macroscopic and microscopic
inspection are labor-intensive and require expert knowledge. In this study, we
explore the application of deep learning to automate the classification of ten
wood species commonly found in Vietnam. A custom image dataset was constructed
from field-collected wood samples, and five state-of-the-art convolutional
neural network architectures--ResNet50, EfficientNet, MobileViT, MobileNetV3,
and ShuffleNetV2--were evaluated. Among these, ShuffleNetV2 achieved the best
balance between classification performance and computational efficiency, with
an average accuracy of 99.29\% and F1-score of 99.35\% over 20 independent
runs. These results demonstrate the potential of lightweight deep learning
models for real-time, high-accuracy species identification in
resource-constrained environments. Our work contributes to the growing field of
ecological informatics by providing scalable, image-based solutions for
automated wood classification and forest biodiversity assessment.

</details>


### [66] [NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification](https://arxiv.org/abs/2508.10940)
*Nirmal Gaud,Krishna Kumar Jha,Jhimli Adhikari,Adhini Nasarin P S,Joydeep Das,Samarth S Deshpande,Nitasha Barara,Vaduguru Venkata Ramya,Santu Saha,Mehmet Tarik Baran,Sarangi Venkateshwarlu,Anusha M D,Surej Mouli,Preeti Katiyar,Vipin Kumar Chaudhary*

Main category: cs.CV

TL;DR: The paper introduces NIRMAL Pooling, a new pooling layer for CNNs combining adaptive max pooling and non-linear activation to improve image classification performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the feature expressiveness and robustness of CNNs through an improved pooling method for better performance on image recognition tasks.

Method: NIRMAL Pooling adapts pooling parameters dynamically and applies a ReLU activation function post-pooling. It is evaluated against Max Pooling on datasets like MNIST Digits, MNIST Fashion, and CIFAR-10.

Result: NIRMAL Pooling achieves higher accuracy compared to Max Pooling on all tested datasets, with notable improvements on CIFAR-10.

Conclusion: The proposed pooling layer improves CNN performance, making it a flexible and reliable alternative to traditional pooling methods for image recognition tasks.

Abstract: This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional
Neural Networks (CNNs) that integrates adaptive max pooling with non-linear
activation function for image classification tasks. The acronym NIRMAL stands
for Non-linear Activation, Intermediate Aggregation, Reduction, Maximum,
Adaptive, and Localized. By dynamically adjusting pooling parameters based on
desired output dimensions and applying a Rectified Linear Unit (ReLU)
activation post-pooling, NIRMAL Pooling improves robustness and feature
expressiveness. We evaluated its performance against standard Max Pooling on
three benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL
Pooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on
MNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on
CIFAR-10, demonstrating consistent improvements, particularly on complex
datasets. This work highlights the potential of NIRMAL Pooling to enhance CNN
performance in diverse image recognition tasks, offering a flexible and
reliable alternative to traditional pooling methods.

</details>


### [67] [Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram](https://arxiv.org/abs/2508.10942)
*Liming Xu,Dave Towey,Andrew P. French,Steve Benford*

Main category: cs.CV

TL;DR: The paper addresses the detection of Artcodes, which are decorative markers camouflaged in freeform appearances, using a novel feature descriptor and evaluates its effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: To create systems that seamlessly integrate Artcodes, decorative markers blending into the physical world, with virtual elements for new interaction opportunities and applications.

Method: Developed a feature descriptor called the "shape of orientation histogram" to represent topological structures. Conducted experiments using collected datasets to test an Artcode detection system built on it.

Result: The proposed feature vector effectively represents topological structures and enables reliable detection of Artcode proposals, with experimental results validating its feasibility.

Conclusion: The research demonstrates the potential for detecting topological objects via feature-based systems, paving the way for innovative uses and applications in environments with mixed virtual and physical elements.

Abstract: The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it
is expected that our everyday environment may soon be decorating with objects
connecting with virtual elements. Alerting to the presence of these objects is
therefore the first step for motivating follow-up further inspection and
triggering digital material attached to the objects. This work studies a
special kind of these objects -- Artcodes -- a human-meaningful and
machine-readable decorative markers that camouflage themselves with freeform
appearance by encoding information into their topology. We formulate this
problem of recongising the presence of Artcodes as Artcode proposal detection,
a distinct computer vision task that classifies topologically similar but
geometrically and semantically different objects as a same class. To deal with
this problem, we propose a new feature descriptor, called the shape of
orientation histogram, to describe the generic topological structure of an
Artcode. We collect datasets and conduct comprehensive experiments to evaluate
the performance of the Artcode detection proposer built upon this new feature
vector. Our experimental results show the feasibility of the proposed feature
vector for representing topological structures and the effectiveness of the
system for detecting Artcode proposals. Although this work is an initial
attempt to develop a feature-based system for detecting topological objects
like Artcodes, it would open up new interaction opportunities and spark
potential applications of topological object detection.

</details>


### [68] [Analysis of the Compaction Behavior of Textile Reinforcements in Low-Resolution In-Situ CT Scans via Machine-Learning and Descriptor-Based Methods](https://arxiv.org/abs/2508.10943)
*Christian Düreth,Jan Condé-Wolter,Marek Danczak,Karsten Tittmann,Jörn Jaschinski,Andreas Hornig,Maik Gude*

Main category: cs.CV

TL;DR: This paper introduces a framework utilizing low-resolution CT and 3D-UNet for quantifying nesting behavior in textile reinforcements, achieving high segmentation accuracy and enabling structural insights.


<details>
  <summary>Details</summary>
Motivation: To understand how nesting in textile reinforcements impacts mechanical properties like stiffness and damage tolerance, enabling improved predictive modeling of composites.

Method: The study uses in-situ compaction experiments, low-resolution CT scanning, and a tailored 3D-UNet for segmentation of textile phases, followed by spatial analysis using the two-point correlation function.

Result: The 3D-UNet achieved a minimum mean IoU of 0.822 and an F1 score of 0.902, and the spatial structure analysis showed strong agreement with traditional micrograph validation.

Conclusion: The method offers a reliable approach for analyzing geometrical features of textile composites from CT data, facilitating advanced structural modeling and analysis.

Abstract: A detailed understanding of material structure across multiple scales is
essential for predictive modeling of textile-reinforced composites. Nesting --
characterized by the interlocking of adjacent fabric layers through local
interpenetration and misalignment of yarns -- plays a critical role in defining
mechanical properties such as stiffness, permeability, and damage tolerance.
This study presents a framework to quantify nesting behavior in dry textile
reinforcements under compaction using low-resolution computed tomography (CT).
In-situ compaction experiments were conducted on various stacking
configurations, with CT scans acquired at 20.22 $\mu$m per voxel resolution. A
tailored 3D{-}UNet enabled semantic segmentation of matrix, weft, and fill
phases across compaction stages corresponding to fiber volume contents of
50--60 %. The model achieved a minimum mean Intersection-over-Union of 0.822
and an $F1$ score of 0.902. Spatial structure was subsequently analyzed using
the two-point correlation function $S_2$, allowing for probabilistic extraction
of average layer thickness and nesting degree. The results show strong
agreement with micrograph-based validation. This methodology provides a robust
approach for extracting key geometrical features from industrially relevant CT
data and establishes a foundation for reverse modeling and descriptor-based
structural analysis of composite preforms.

</details>


### [69] [iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities](https://arxiv.org/abs/2508.10945)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoad is an automated system for detecting and mapping potholes, using dashcam footage, YOLO model, and GPS tagging for accurate geolocation and data visualization on OpenStreetMap.


<details>
  <summary>Details</summary>
Motivation: To address the road safety and maintenance challenges posed by potholes, particularly in India where under-maintained roads are prevalent.

Method: The system uses curated dashcam footage to fine-tune a YOLO model for real-time pothole detection, a custom OCR module for timestamp extraction, and GPS integration for geotagging.

Result: The system demonstrates improved detection accuracy under challenging conditions and produces government-compatible metadata for use in road maintenance planning.

Conclusion: iWatchRoad offers a scalable, cost-effective tool for urban and rural road management, aiding in automated road assessment and maintenance.

Abstract: Potholes on the roads are a serious hazard and maintenance burden. This poses
a significant threat to road safety and vehicle longevity, especially on the
diverse and under-maintained roads of India. In this paper, we present a
complete end-to-end system called iWatchRoad for automated pothole detection,
Global Positioning System (GPS) tagging, and real time mapping using
OpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000
frames captured across various road types, lighting conditions, and weather
scenarios unique to Indian environments, leveraging dashcam footage. This
dataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to
perform real time pothole detection, while a custom Optical Character
Recognition (OCR) module was employed to extract timestamps directly from video
frames. The timestamps are synchronized with GPS logs to geotag each detected
potholes accurately. The processed data includes the potholes' details and
frames as metadata is stored in a database and visualized via a user friendly
web interface using OSM. iWatchRoad not only improves detection accuracy under
challenging conditions but also provides government compatible outputs for road
assessment and maintenance planning through the metadata visible on the
website. Our solution is cost effective, hardware efficient, and scalable,
offering a practical tool for urban and rural road management in developing
regions, making the system automated. iWatchRoad is available at
https://smlab.niser.ac.in/project/iwatchroad

</details>


### [70] [IPG: Incremental Patch Generation for Generalized Adversarial Patch Training](https://arxiv.org/abs/2508.10946)
*Wonho Lee,Hyunsik Na,Jisu Lee,Daeseon Choi*

Main category: cs.CV

TL;DR: The paper introduces Incremental Patch Generation (IPG), a method for efficiently creating adversarial patches, which attack specific areas in images and disrupt AI models, showing improved efficiency while maintaining attack performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by adversarial patches in computer vision tasks like object detection, which compromise AI model functionality by targeting specific image regions.

Method: Proposes Incremental Patch Generation (IPG), a technique to generate adversarial patches 11.1 times more efficiently than existing methods, validated through experiments and feature visualization studies.

Result: IPG generates highly generalized adversarial patches, demonstrating improved efficiency, comparable attack performance, and broader coverage of model vulnerabilities. It also enhances adversarial training and provides robust datasets for defense strategies.

Conclusion: IPG holds significant promise for improving defense in AI models against adversarial patches, with potential applications in critical fields like autonomous vehicles, security systems, and medical imaging.

Abstract: The advent of adversarial patches poses a significant challenge to the
robustness of AI models, particularly in the domain of computer vision tasks
such as object detection. In contradistinction to traditional adversarial
examples, these patches target specific regions of an image, resulting in the
malfunction of AI models. This paper proposes Incremental Patch Generation
(IPG), a method that generates adversarial patches up to 11.1 times more
efficiently than existing approaches while maintaining comparable attack
performance. The efficacy of IPG is demonstrated by experiments and ablation
studies including YOLO's feature distribution visualization and adversarial
training results, which show that it produces well-generalized patches that
effectively cover a broader range of model vulnerabilities. Furthermore,
IPG-generated datasets can serve as a robust knowledge foundation for
constructing a robust model, enabling structured representation, advanced
reasoning, and proactive defenses in AI security ecosystems. The findings of
this study suggest that IPG has considerable potential for future utilization
not only in adversarial patch defense but also in real-world applications such
as autonomous vehicles, security systems, and medical imaging, where AI models
must remain resilient to adversarial attacks in dynamic and high-stakes
environments.

</details>


### [71] [MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text](https://arxiv.org/abs/2508.10947)
*Ronghao Xu,Zhen Huang,Yangbo Wei,Xiaoqian Zhou,Zikang Xu,Ting Liu,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: MedAtlas is a benchmarking framework aimed at enhancing medical AI by evaluating models on realistic, multi-modal medical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing medical AI benchmarks fail to represent the complex, interactive, and multi-modal aspects of clinical practices.

Method: The framework includes multi-turn dialogues, multi-task integration, multi-modal medical image reasoning, and expert-annotated gold standards, focusing on tasks like open- and closed-ended multi-turn QA, and comprehensive diagnoses.

Result: Benchmarking with current models revealed significant deficiencies in handling multi-stage clinical reasoning.

Conclusion: MedAtlas offers a robust platform to guide advancements in reliable and adaptable medical AI systems.

Abstract: Artificial intelligence has demonstrated significant potential in clinical
decision-making; however, developing models capable of adapting to diverse
real-world scenarios and performing complex diagnostic reasoning remains a
major challenge. Existing medical multi-modal benchmarks are typically limited
to single-image, single-turn tasks, lacking multi-modal medical image
integration and failing to capture the longitudinal and multi-modal interactive
nature inherent to clinical practice. To address this gap, we introduce
MedAtlas, a novel benchmark framework designed to evaluate large language
models on realistic medical reasoning tasks. MedAtlas is characterized by four
key features: multi-turn dialogue, multi-modal medical image interaction,
multi-task integration, and high clinical fidelity. It supports four core
tasks: open-ended multi-turn question answering, closed-ended multi-turn
question answering, multi-image joint reasoning, and comprehensive disease
diagnosis. Each case is derived from real diagnostic workflows and incorporates
temporal interactions between textual medical histories and multiple imaging
modalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to
perform deep integrative reasoning across images and clinical texts. MedAtlas
provides expert-annotated gold standards for all tasks. Furthermore, we propose
two novel evaluation metrics: Round Chain Accuracy and Error Propagation
Resistance. Benchmark results with existing multi-modal models reveal
substantial performance gaps in multi-stage clinical reasoning. MedAtlas
establishes a challenging evaluation platform to advance the development of
robust and trustworthy medical AI.

</details>


### [72] [From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement](https://arxiv.org/abs/2508.10950)
*Xinyi Wang,Michael Barnett,Frederique Boonstra,Yael Barnett,Mariano Cabezas,Arkiev D'Souza,Matthew C. Kiernan,Kain Kyle,Meng Law,Lynette Masters,Zihao Tang,Stephen Tisch,Sicong Tu,Anneke Van Der Walt,Dongang Wang,Fernando Calamante,Weidong Cai,Chenyu Wang*

Main category: cs.CV

TL;DR: This paper introduces FastFOD-Net, a deep learning framework for enhancing diffusion MRI fiber orientation distribution (FODs), showcasing its effectiveness on both healthy subjects and patients with six neurological disorders.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of generating accurate FODs from widely used clinical MRI protocols with single-shell, low-angular-resolution acquisitions, particularly for clinical applications beyond healthy subjects.

Method: The authors developed and validated FastFOD-Net, an optimized deep learning framework that enhances FODs efficiently, providing an accelerated ($60\times$ faster) end-to-end process suitable for clinical settings.

Result: FastFOD-Net outperforms its predecessor in speed and performance, achieving robust FOD enhancement across healthy and diseased populations with comprehensive clinical evaluation.

Conclusion: The study highlights FastFOD-Net's potential for widespread clinical adoption, boosting its utility in neuroscience research, disease differentiation, connectome analysis, and enhancing the reliability of clinical diffusion MRI data.

Abstract: Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling
technique that represents complex white matter fiber configurations, and a key
step for subsequent brain tractography and connectome analysis. Its reliability
and accuracy, however, heavily rely on the quality of the MRI acquisition and
the subsequent estimation of the FODs at each voxel. Generating reliable FODs
from widely available clinical protocols with single-shell and
low-angular-resolution acquisitions remains challenging but could potentially
be addressed with recent advances in deep learning-based enhancement
techniques. Despite advancements, existing methods have predominantly been
assessed on healthy subjects, which have proved to be a major hurdle for their
clinical adoption. In this work, we validate a newly optimized enhancement
framework, FastFOD-Net, across healthy controls and six neurological disorders.
This accelerated end-to-end deep learning framework enhancing FODs with
superior performance and delivering training/inference efficiency for clinical
use ($60\times$ faster comparing to its predecessor). With the most
comprehensive clinical evaluation to date, our work demonstrates the potential
of FastFOD-Net in accelerating clinical neuroscience research, empowering
diffusion MRI analysis for disease differentiation, improving interpretability
in connectome applications, and reducing measurement errors to lower sample
size requirements. Critically, this work will facilitate the more widespread
adoption of, and build clinical trust in, deep learning based methods for
diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of
real-world, clinical diffusion MRI data, comparable to that achievable with
high-quality research acquisitions.

</details>


### [73] [Empowering Multimodal LLMs with External Tools: A Comprehensive Survey](https://arxiv.org/abs/2508.10955)
*Wenbin An,Jiahao Nie,Yaqiang Wu,Feng Tian,Shijian Lu,Qinghua Zheng*

Main category: cs.CV

TL;DR: Survey explores enhancing Multimodal Large Language Models (MLLMs) like GPT-4V with external tools to improve data quality, task performance, and evaluation, addressing current limitations.


<details>
  <summary>Details</summary>
Motivation: Limited multimodal data quality and inadequate performance on complex tasks hinder the reliability of MLLMs.

Method: Presents a structured survey examining the use of external tools (APIs, expert systems, knowledge bases) for data acquisition, task enhancement, evaluation, and future directions.

Result: Identifies ways external tools improve MLLMs in areas like data quality, domain-specific tasks, and evaluation accuracy.

Conclusion: External tools hold significant potential to overcome current limitations of MLLMs and guide their capabilities toward further advancement and applications.

Abstract: By integrating the perception capabilities of multimodal encoders with the
generative power of Large Language Models (LLMs), Multimodal Large Language
Models (MLLMs), exemplified by GPT-4V, have achieved great success in various
multimodal tasks, pointing toward a promising pathway to artificial general
intelligence. Despite this progress, the limited quality of multimodal data,
poor performance on many complex downstream tasks, and inadequate evaluation
protocols continue to hinder the reliability and broader applicability of MLLMs
across diverse domains. Inspired by the human ability to leverage external
tools for enhanced reasoning and problem-solving, augmenting MLLMs with
external tools (e.g., APIs, expert models, and knowledge bases) offers a
promising strategy to overcome these challenges. In this paper, we present a
comprehensive survey on leveraging external tools to enhance MLLM performance.
Our discussion is structured along four key dimensions about external tools:
(1) how they can facilitate the acquisition and annotation of high-quality
multimodal data; (2) how they can assist in improving MLLM performance on
challenging downstream tasks; (3) how they enable comprehensive and accurate
evaluation of MLLMs; (4) the current limitations and future directions of
tool-augmented MLLMs. Through this survey, we aim to underscore the
transformative potential of external tools in advancing MLLM capabilities,
offering a forward-looking perspective on their development and applications.
The project page of this paper is publicly available
athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.

</details>


### [74] [ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks](https://arxiv.org/abs/2508.10956)
*Abhishek Kolari,Mohammadhossein Khojasteh,Yifan Jiang,Floris den Hengst,Filip Ilievski*

Main category: cs.CV

TL;DR: The paper introduces ORBIT, a systematic VQA benchmark assessing vision-language models' reasoning on object properties. Despite advancements, these models significantly underperform compared to humans.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether vision-language models can abstract and reason over object properties, addressing limitations in current VQA benchmarks that blend perception and reasoning.

Method: The authors created a systematic evaluation framework involving 360 images across three types, three reasoning levels, and four object property dimensions, resulting in ORBIT VQA benchmark with 1,080 questions.

Result: Experiments with 12 state-of-the-art VLMs showed severe limitations in reasoning abilities, particularly with realistic images, counterfactual reasoning, and higher counts, achieving only 40% accuracy at best.

Conclusion: Vision-language models struggle with object property reasoning tasks, highlighting the need for improved benchmarking, generalized annotation guidelines, and better reasoning approaches in future research.

Abstract: While vision-language models (VLMs) have made remarkable progress on many
popular visual question answering (VQA) benchmarks, it remains unclear whether
they abstract and reason over depicted objects. Inspired by human object
categorisation, object property reasoning involves identifying and recognising
low-level details and higher-level abstractions. While current VQA benchmarks
consider a limited set of object property attributes like size, they typically
blend perception and reasoning, and lack representativeness in terms of
reasoning and image categories. To this end, we introduce a systematic
evaluation framework with images of three representative types, three reasoning
levels of increasing complexity, and four object property dimensions driven by
prior work on commonsense reasoning. We develop a procedure to instantiate this
benchmark into ORBIT, a multi-level reasoning VQA benchmark for object
properties comprising 360 images paired with a total of 1,080 count-based
questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings
reveal significant limitations compared to humans, with the best-performing
model only reaching 40\% accuracy. VLMs struggle particularly with realistic
(photographic) images, counterfactual reasoning about physical and functional
properties, and higher counts. ORBIT points to the need to develop methods for
scalable benchmarking, generalize annotation guidelines, and explore additional
reasoning VLMs. We make the ORBIT benchmark and the experimental code available
to support such endeavors.

</details>


### [75] [CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving](https://arxiv.org/abs/2508.10962)
*Jiarong Li,Imad Ali Shah,Diarmaid Geever,Fiachra Collins,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: The paper proposes a hyperspectral imaging-based method to improve the detection of vulnerable road users in advanced driving systems by addressing metamerism, a common visual ambiguity in RGB imagery.


<details>
  <summary>Details</summary>
Motivation: The research aims to tackle the safety challenge posed by metamerism, where distinct materials appear visually identical in RGB imagery, hampering the accurate identification of vulnerable road users in automotive systems.

Method: A hyperspectral imaging (HSI) approach is proposed, integrating joint mutual information, correlation analysis, and an image quality metric to select the most informative spectral bands, focusing on the Near-Infrared region.

Result: The selected spectral bands (497 nm, 607 nm, 895 nm) greatly enhance object separability and reduce metameric confusion, achieving significant performance improvements in similarity and perception metrics over RGB imagery.

Conclusion: HSI-based spectral optimization significantly enhances vulnerable road users' distinguishability, paving the way for safer advanced driver assistance and autonomous driving systems.

Abstract: Protecting Vulnerable Road Users (VRU) is a critical safety challenge for
automotive perception systems, particularly under visual ambiguity caused by
metamerism, a phenomenon where distinct materials appear similar in RGB
imagery. This work investigates hyperspectral imaging (HSI) to overcome this
limitation by capturing unique material signatures beyond the visible spectrum,
especially in the Near-Infrared (NIR). To manage the inherent
high-dimensionality of HSI data, we propose a band selection strategy that
integrates information theory techniques (joint mutual information
maximization, correlation analysis) with a novel application of an image
quality metric (contrast signal-to-noise ratio) to identify the most spectrally
informative bands. Using the Hyperspectral City V2 (H-City) dataset, we
identify three informative bands (497 nm, 607 nm, and 895 nm, $\pm$27 nm) and
reconstruct pseudo-color images for comparison with co-registered RGB.
Quantitative results demonstrate increased dissimilarity and perceptual
separability of VRU from the background. The selected HSI bands yield
improvements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity
(Euclidean, SAM, $T^2$) and perception (CIE $\Delta E$) metrics, consistently
outperforming RGB and confirming a marked reduction in metameric confusion. By
providing a spectrally optimized input, our method enhances VRU separability,
establishing a robust foundation for downstream perception tasks in Advanced
Driver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately
contributing to improved road safety.

</details>


### [76] [EVCtrl: Efficient Control Adapter for Visual Generation](https://arxiv.org/abs/2508.10963)
*Zixiang Yang,Yue Ma,Yinhan Zhang,Shanhui Mo,Dongrui Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: This paper introduces EVCtrl, a lightweight adapter for efficient and controllable image and video generation, improving computational efficiency without retraining.


<details>
  <summary>Details</summary>
Motivation: There is a need for efficient and controllable visual content generation, as existing methods like ControlNet suffer from high latency and redundant computations, especially for videos.

Method: The authors propose EVCtrl, utilizing a spatio-temporal dual caching strategy. Spatially, the network is divided into global and local zones with a locality-aware cache for focusing on relevant regions. Temporally, unnecessary denoising steps are skipped.

Result: EVCtrl achieves up to 2.16× speedups on CogVideo-Controlnet and 2.05× speedups on Wan2.1-Controlnet with minimal degradation in generation quality.

Conclusion: EVCtrl significantly reduces computational overhead in image and video generation while maintaining quality, making it a practical, retraining-free enhancement.

Abstract: Visual generation includes both image and video generation, training
probabilistic models to create coherent, diverse, and semantically faithful
content from scratch. While early research focused on unconditional sampling,
practitioners now demand controllable generation that allows precise
specification of layout, pose, motion, or style. While ControlNet grants
precise spatial-temporal control, its auxiliary branch markedly increases
latency and introduces redundant computation in both uncontrolled regions and
denoising steps, especially for video. To address this problem, we introduce
EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead
without retraining the model. Specifically, we propose a spatio-temporal dual
caching strategy for sparse control information. For spatial redundancy, we
first profile how each layer of DiT-ControlNet responds to fine-grained
control, then partition the network into global and local functional zones. A
locality-aware cache focuses computation on the local zones that truly need the
control signal, skipping the bulk of redundant computation in global regions.
For temporal redundancy, we selectively omit unnecessary denoising steps to
improve efficiency. Extensive experiments on CogVideo-Controlnet,
Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image
and video control generation without the need for training. For example, it
achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and
Wan2.1-Controlnet, respectively, with almost no degradation in generation
quality.Codes are available in the supplementary materials.

</details>


### [77] [Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision](https://arxiv.org/abs/2508.10972)
*Rosiana Natalie,Wenqian Xu,Ruei-Che Chang,Rada Mihalcea,Anhong Guo*

Main category: cs.CV

TL;DR: The paper investigates Vision Language Models (VLMs) in simulating the vision perception of low vision individuals, finding that combining vision information and examples improves agreement significantly, though additional examples add little benefit.


<details>
  <summary>Details</summary>
Motivation: To explore whether Vision Language Models can effectively simulate the vision perception of low vision individuals, a previously under-researched area in accessibility.

Method: A benchmark dataset was created with data from 40 low vision participants to construct prompts for VLMs. Simulated agents based on participant information and response styles were tested for agreement with the original participant data.

Result: Agreement between VLM-generated and participants' responses was generally low unless both detailed vision information and example responses were combined, achieving a higher agreement of 0.70.

Conclusion: VLMs can simulate low vision perception more accurately when provided both vision information and example responses, but the benefits plateau with the addition of further examples.

Abstract: Advances in vision language models (VLMs) have enabled the simulation of
general human behavior through their reasoning and problem solving
capabilities. However, prior research has not investigated such simulation
capabilities in the accessibility domain. In this paper, we evaluate the extent
to which VLMs can simulate the vision perception of low vision individuals when
interpreting images. We first compile a benchmark dataset through a survey
study with 40 low vision participants, collecting their brief and detailed
vision information and both open-ended and multiple-choice image perception and
recognition responses to up to 25 images. Using these responses, we construct
prompts for VLMs (GPT-4o) to create simulated agents of each participant,
varying the included information on vision information and example image
responses. We evaluate the agreement between VLM-generated responses and
participants' original answers. Our results indicate that VLMs tend to infer
beyond the specified vision ability when given minimal prompts, resulting in
low agreement (0.59). The agreement between the agent' and participants'
responses remains low when only either the vision information (0.59) or example
image responses (0.59) are provided, whereas a combination of both
significantly increase the agreement (0.70, p < 0.0001). Notably, a single
example combining both open-ended and multiple-choice responses, offers
significant performance improvements over either alone (p < 0.0001), while
additional examples provided minimal benefits (p > 0.05).

</details>


### [78] [Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?](https://arxiv.org/abs/2508.11011)
*Xuezheng Chen,Zhengbo Zou*

Main category: cs.CV

TL;DR: The paper introduces "ConstructionSite 10k," a dataset of 10,000 annotated construction site images aimed at improving construction safety inspections via Vision Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: To address the lack of open datasets for evaluating and fine-tuning Vision Language Models (VLMs) in construction safety inspection, thereby advancing their practical applicability.

Method: The researchers created a dataset containing annotations for image captioning, safety rule violation VQA, and construction element visual grounding. They also evaluated state-of-the-art VLMs in zero-shot and few-shot settings.

Result: State-of-the-art VLMs showed generalization abilities under zero-shot and few-shot conditions, but required additional training for practical use in construction safety inspections.

Conclusion: "ConstructionSite 10k" serves as a benchmark dataset that enables researchers to train and assess VLMs, encouraging advancements in construction safety technologies.

Abstract: Construction safety inspections typically involve a human inspector
identifying safety concerns on-site. With the rise of powerful Vision Language
Models (VLMs), researchers are exploring their use for tasks such as detecting
safety rule violations from on-site images. However, there is a lack of open
datasets to comprehensively evaluate and further fine-tune VLMs in construction
safety inspection. Current applications of VLMs use small, supervised datasets,
limiting their applicability in tasks they are not directly trained for. In
this paper, we propose the ConstructionSite 10k, featuring 10,000 construction
site images with annotations for three inter-connected tasks, including image
captioning, safety rule violation visual question answering (VQA), and
construction element visual grounding. Our subsequent evaluation of current
state-of-the-art large pre-trained VLMs shows notable generalization abilities
in zero-shot and few-shot settings, while additional training is needed to make
them applicable to actual construction sites. This dataset allows researchers
to train and evaluate their own VLMs with new architectures and techniques,
providing a valuable benchmark for construction safety inspection.

</details>


### [79] [Can Multi-modal (reasoning) LLMs detect document manipulation?](https://arxiv.org/abs/2508.11021)
*Zisheng Liang,Kidus Zewde,Rudra Pratap Singh,Disha Patil,Zexi Chen,Jiayu Xue,Yao Yao,Yifei Chen,Qinzhe Liu,Simiao Ren*

Main category: cs.CV

TL;DR: This study benchmarks state-of-the-art multi-modal large language models (LLMs) for detecting document fraud, finding that zero-shot generalization often outperforms conventional methods but requires task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to counter the growing threat of document fraud, a critical issue for industries dependent on secure and verifiable transactional documentation.

Method: The study evaluates multiple multi-modal LLMs, comparing their performance against each other and prior methods, using a standard dataset to test their fraud detection capabilities under various conditions.

Result: Top-performing LLMs demonstrate superior zero-shot fraud detection capabilities, outperforming traditional approaches in out-of-distribution tests, while certain vision-based LLMs show inconsistent performance.

Conclusion: Multi-modal LLMs are promising tools for enhancing fraud detection systems, but future research should emphasize task-specific fine-tuning and interpretability to overcome limitations in model size and reasoning capabilities.

Abstract: Document fraud poses a significant threat to industries reliant on secure and
verifiable documentation, necessitating robust detection mechanisms. This study
investigates the efficacy of state-of-the-art multi-modal large language models
(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,
Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and
3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against
each other and prior work on document fraud detection techniques using a
standard dataset with real transactional documents. Through prompt optimization
and detailed analysis of the models' reasoning processes, we evaluate their
ability to identify subtle indicators of fraud, such as tampered text,
misaligned formatting, and inconsistent transactional sums. Our results reveal
that top-performing multi-modal LLMs demonstrate superior zero-shot
generalization, outperforming conventional methods on out-of-distribution
datasets, while several vision LLMs exhibit inconsistent or subpar performance.
Notably, model size and advanced reasoning capabilities show limited
correlation with detection accuracy, suggesting task-specific fine-tuning is
critical. This study underscores the potential of multi-modal LLMs in enhancing
document fraud detection systems and provides a foundation for future research
into interpretable and scalable fraud mitigation strategies.

</details>


### [80] [MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation](https://arxiv.org/abs/2508.11032)
*Yanwu Yang,Guinan Su,Jiesi Hu,Francesco Sammarco,Jonas Geiping,Thomas Wolfers*

Main category: cs.CV

TL;DR: MedSAMix is a method that merges generalist and specialist models for medical image segmentation, improving task-specific accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Address limitations in medical segmentation models like MedSAM, including heterogeneity, limited data, and distributional shifts, by combining strengths of generalist and specialist models.

Method: Propose MedSAMix, a training-free model merging approach using zero-order optimization to discover optimal layer-wise merging solutions. Two optimization regimes for domain-specificity and generalizability are included.

Result: MedSAMix improves performance in medical image segmentation by 6.67% on specialized tasks and 4.37% on multi-task evaluations.

Conclusion: MedSAMix effectively enhances medical segmentation models by mitigating bias and improving accuracy and generalization through innovative model merging methods.

Abstract: Universal medical image segmentation models have emerged as a promising
paradigm due to their strong generalizability across diverse tasks, showing
great potential for a wide range of clinical applications. This potential has
been partly driven by the success of general-purpose vision models such as the
Segment Anything Model (SAM), which has inspired the development of various
fine-tuned variants for medical segmentation tasks. However, fine-tuned
variants like MedSAM are trained on comparatively limited medical imaging data
that often suffers from heterogeneity, scarce annotations, and distributional
shifts. These challenges limit their ability to generalize across a wide range
of medical segmentation tasks. In this regard, we propose MedSAMix, a
training-free model merging method that integrates the strengths of both
generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical
image segmentation. In contrast to traditional model merging approaches that
rely on manual configuration and often result in suboptimal outcomes, we
propose a zero-order optimization method to automatically discover optimal
layer-wise merging solutions. Furthermore, for clinical applications, we
develop two regimes to meet the demand of domain-specificity and
generalizability in different scenarios by single-task optimization and
multi-objective optimization respectively. Extensive evaluations on 25 medical
segmentation tasks demonstrate that MedSAMix effectively mitigates model bias
and consistently improves performance in both domain-specific accuracy and
generalization, achieving improvements of 6.67% on specialized tasks and 4.37%
on multi-task evaluations.

</details>


### [81] [Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset](https://arxiv.org/abs/2508.11058)
*Wentao Mo,Qingchao Chen,Yuxin Peng,Siyuan Huang,Yang Liu*

Main category: cs.CV

TL;DR: This paper identifies limitations in existing 3D vision-language datasets and introduces MV-ScanQA and TripAlign to address these issues. It also proposes LEGO as a baseline method, achieving state-of-the-art performance in 3D scene understanding.


<details>
  <summary>Details</summary>
Motivation: Existing 3D VL datasets lack the ability to require multi-view reasoning and often miss rich contextual alignments between multiple objects, limiting the development of robust multi-view 3D scene understanding models.

Method: The authors propose MV-ScanQA, a dataset that emphasizes multi-view reasoning, and TripAlign, a pre-training corpus that aligns 2D views, 3D objects, and text. They introduce LEGO, a baseline method that transfers knowledge from pre-trained 2D-large vision-language models (LVLMs) to the 3D domain using TripAlign.

Result: LEGO, pre-trained on TripAlign, achieves state-of-the-art results on the newly proposed MV-ScanQA dataset as well as existing benchmarks for 3D dense captioning and question answering.

Conclusion: The paper demonstrates that MV-ScanQA, TripAlign, and the LEGO baseline collectively address critical challenges in 3D VL learning, paving the way for advanced multi-view compositional reasoning capabilities in 3D scene understanding tasks.

Abstract: The advancement of 3D vision-language (3D VL) learning is hindered by several
limitations in existing 3D VL datasets: they rarely necessitate reasoning
beyond a close range of objects in single viewpoint, and annotations often link
instructions to single objects, missing richer contextual alignments between
multiple objects. This significantly curtails the development of models capable
of deep, multi-view 3D scene understanding over distant objects. To address
these challenges, we introduce MV-ScanQA, a novel 3D question answering dataset
where 68% of questions explicitly require integrating information from multiple
views (compared to less than 7% in existing datasets), thereby rigorously
testing multi-view compositional reasoning. To facilitate the training of
models for such demanding scenarios, we present TripAlign dataset, a
large-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D
view, set of 3D objects, text> triplets that explicitly aligns groups of
contextually related objects with text, providing richer, view-grounded
multi-object multimodal alignment signals than previous single-object
annotations. We further develop LEGO, a baseline method for the multi-view
reasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D
LVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign
achieves state-of-the-art performance not only on the proposed MV-ScanQA, but
also on existing benchmarks for 3D dense captioning and question answering.
Datasets and code are available at
https://matthewdm0816.github.io/tripalign-mvscanqa.

</details>


### [82] [Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts](https://arxiv.org/abs/2508.11063)
*Lucas W. Remedios,Chloe Choe,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: The study examines abdominal markers of type 2 diabetes by linking body composition measurements from 3D clinical imaging with machine learning tools.


<details>
  <summary>Details</summary>
Motivation: Type 2 diabetes affects people across various BMI levels, and detailed analysis of abdominal structures may provide insights into risk and protection factors.

Method: Abdominal CT scans were analyzed using segmentation and machine learning (random forests and SHAP analysis) to identify diabetes-related patterns on a large cohort divided by BMI groups.

Result: The machine learning model achieved AUCs of 0.72-0.74, identifying shared predictors such as fatty skeletal muscle, visceral fat, and fat-laden pancreas across BMI groups.

Conclusion: Abdominal markers of type 2 diabetes appear consistent across weight classes, underscoring the potential for universal biomarkers in assessing diabetes risk.

Abstract: Purpose: Although elevated BMI is a well-known risk factor for type 2
diabetes, the disease's presence in some lean adults and absence in others with
obesity suggests that detailed body composition may uncover abdominal
phenotypes of type 2 diabetes. With AI, we can now extract detailed
measurements of size, shape, and fat content from abdominal structures in 3D
clinical imaging at scale. This creates an opportunity to empirically define
body composition signatures linked to type 2 diabetes risk and protection using
large-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal
patterns from clinical CT, we applied our design four times: once on the full
cohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese
(n = 620) subgroups separately. Briefly, our experimental design transforms
abdominal scans into collections of explainable measurements through
segmentation, classifies type 2 diabetes through a cross-validated random
forest, measures how features contribute to model-estimated risk or protection
through SHAP analysis, groups scans by shared model decision patterns
(clustering from SHAP) and links back to anatomical differences
(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.
There were shared type 2 diabetes signatures in each group; fatty skeletal
muscle, older age, greater visceral and subcutaneous fat, and a smaller or
fat-laden pancreas. Univariate logistic regression confirmed the direction of
14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:
Our findings suggest that abdominal drivers of type 2 diabetes may be
consistent across weight classes.

</details>


### [83] [HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing](https://arxiv.org/abs/2508.11106)
*Xinjie Gao,Bi'an Du,Wei Hu*

Main category: cs.CV

TL;DR: The paper introduces HierOctFusion, a part-aware multi-scale 3D octree diffusion model that improves hierarchical feature interaction, achieving enhanced shape quality and generation efficiency.


<details>
  <summary>Details</summary>
Motivation: Current 3D generation methods often neglect semantic part hierarchies and suffer from computational inefficiency due to holistic high-resolution modeling.

Method: HierOctFusion uses a part-aware multi-scale octree diffusion approach with cross-attention conditioning to propagate semantic features effectively. A 3D dataset with part annotations is also constructed.

Result: HierOctFusion demonstrates superior shape quality and efficiency compared to existing 3D generation models.

Conclusion: The proposed model addresses structural complexity using hierarchical and part-based generation, showcasing improved results in 3D content creation.

Abstract: 3D content generation remains a fundamental yet challenging task due to the
inherent structural complexity of 3D data. While recent octree-based diffusion
models offer a promising balance between efficiency and quality through
hierarchical generation, they often overlook two key insights: 1) existing
methods typically model 3D objects as holistic entities, ignoring their
semantic part hierarchies and limiting generalization; and 2) holistic
high-resolution modeling is computationally expensive, whereas real-world
objects are inherently sparse and hierarchical, making them well-suited for
layered generation. Motivated by these observations, we propose HierOctFusion,
a part-aware multi-scale octree diffusion model that enhances hierarchical
feature interaction for generating fine-grained and sparse object structures.
Furthermore, we introduce a cross-attention conditioning mechanism that injects
part-level information into the generation process, enabling semantic features
to propagate effectively across hierarchical levels from parts to the whole.
Additionally, we construct a 3D dataset with part category annotations using a
pre-trained segmentation model to facilitate training and evaluation.
Experiments demonstrate that HierOctFusion achieves superior shape quality and
efficiency compared to prior methods.

</details>


### [84] [UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring](https://arxiv.org/abs/2508.11115)
*Haotang Li,Zhenyu Qi,Sen He,Kebin Peng,Sheng Tan,Yili Ren,Tomas Cerny,Jiyue Zhao,Zi Wang*

Main category: cs.CV

TL;DR: This paper introduces UWB-PostureGuard, a privacy-preserving system using ultra-wideband sensing to monitor sitting postures with high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Improper sitting posture during prolonged computer use is a growing public health issue, and existing solutions like camera-based systems or wearable sensors face challenges such as invasiveness and privacy concerns.

Method: The study utilizes commercial ultra-wideband (UWB) devices to analyze sitting posture through feature engineering and introduces PoseGBDT to address temporal dependencies in posture data.

Result: Through evaluation on 10 participants and 19 unique postures, the system demonstrated 99.11% accuracy while being unaffected by environment variables like clothing thickness, extra devices, or furniture settings.

Conclusion: UWB-PostureGuard offers a low-cost, scalable, and privacy-focused solution for continuous posture monitoring, promising advancements in proactive ergonomic health management and quality of life.

Abstract: Improper sitting posture during prolonged computer use has become a
significant public health concern. Traditional posture monitoring solutions
face substantial barriers, including privacy concerns with camera-based systems
and user discomfort with wearable sensors. This paper presents
UWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that
advances mobile technologies for preventive health management through
continuous, contactless monitoring of ergonomic sitting posture. Our system
leverages commercial UWB devices, utilizing comprehensive feature engineering
to extract multiple ergonomic sitting posture features. We develop PoseGBDT to
effectively capture temporal dependencies in posture patterns, addressing
limitations of traditional frame-wise classification approaches. Extensive
real-world evaluation across 10 participants and 19 distinct postures
demonstrates exceptional performance, achieving 99.11% accuracy while
maintaining robustness against environmental variables such as clothing
thickness, additional devices, and furniture configurations. Our system
provides a scalable, privacy-preserving mobile health solution on existing
platforms for proactive ergonomic management, improving quality of life at low
costs.

</details>


### [85] [Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation](https://arxiv.org/abs/2508.11134)
*Bing Liu,Le Wang,Hao Liu,Mingming Liu*

Main category: cs.CV

TL;DR: The paper proposes a new method, the Residual-Based Efficient Bidirectional Diffusion Model (RBDM), to handle both haze removal and haze generation efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing deep dehazing methods lack the ability to bidirectionally transition between hazy and haze-free images.

Method: The RBDM utilizes dual Markov chains for smooth transitions and introduces noise prediction through perturbation of images, focusing on image patches instead of entire images to reduce computational cost.

Result: RBDM enables bidirectional transitions between haze-free and hazy images within just 15 steps and outperforms or matches state-of-the-art methods on both synthetic and real-world datasets.

Conclusion: The proposed model is efficient, computationally cost-effective, and demonstrates strong performance, achieving flexible transitions between hazy and haze-free images.

Abstract: Current deep dehazing methods only focus on removing haze from hazy images,
lacking the capability to translate between hazy and haze-free images. To
address this issue, we propose a residual-based efficient bidirectional
diffusion model (RBDM) that can model the conditional distributions for both
dehazing and haze generation. Firstly, we devise dual Markov chains that can
effectively shift the residuals and facilitate bidirectional smooth transitions
between them. Secondly, the RBDM perturbs the hazy and haze-free images at
individual timesteps and predicts the noise in the perturbed data to
simultaneously learn the conditional distributions. Finally, to enhance
performance on relatively small datasets and reduce computational costs, our
method introduces a unified score function learned on image patches instead of
entire images. Our RBDM successfully implements size-agnostic bidirectional
transitions between haze-free and hazy images with only 15 sampling steps.
Extensive experiments demonstrate that the proposed method achieves superior or
at least comparable performance to state-of-the-art methods on both synthetic
and real-world datasets.

</details>


### [86] [A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations](https://arxiv.org/abs/2508.11141)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.CV

TL;DR: This paper proposes MICC, a cross-modal rumor detection method leveraging contrastive learning to analyze text and multi-scale image correlations.


<details>
  <summary>Details</summary>
Motivation: Existing rumor detection methods frequently overlook the deep interconnections between text and images, leading to incomplete analysis and potential misinformation.

Method: The authors designed SCLIP encoder for unified embeddings, used cross-modal alignment with mutual information optimization and created a scale-aware fusion network for enhanced integration of text and image features.

Result: MICC demonstrated improved rumor detection performance on two high-quality real-world datasets, outperforming state-of-the-art methods.

Conclusion: The proposed MICC algorithm effectively integrates visual and textual clues, offering a robust solution for cross-modal rumor detection in practical applications.

Abstract: Existing rumor detection methods often neglect the content within images as
well as the inherent relationships between contexts and images across different
visual scales, thereby resulting in the loss of critical information pertinent
to rumor identification. To address these issues, this paper presents a novel
cross-modal rumor detection scheme based on contrastive learning, namely the
Multi-scale Image and Context Correlation exploration algorithm (MICC).
Specifically, we design an SCLIP encoder to generate unified semantic
embeddings for text and multi-scale image patches through contrastive
pretraining, enabling their relevance to be measured via dot-product
similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is
introduced to identify image regions most relevant to the textual semantics,
guided by mutual information maximization and the information bottleneck
principle, through a Top-K selection strategy based on a cross-modal relevance
matrix constructed between the text and multi-scale image patches. Moreover, a
scale-aware fusion network is designed to integrate the highly correlated
multi-scale image features with global text features by assigning adaptive
weights to image regions based on their semantic importance and cross-modal
relevance. The proposed methodology has been extensively evaluated on two
real-world datasets. The experimental results demonstrate that it achieves a
substantial performance improvement over existing state-of-the-art approaches
in rumor detection, highlighting its effectiveness and potential for practical
applications.

</details>


### [87] [LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction](https://arxiv.org/abs/2508.11153)
*Maoquan Zhang,Bisser Raytchev,Xiujuan Sun*

Main category: cs.CV

TL;DR: LEARN is a novel diffusion framework for generating educational illustrations that focus on cognitive coherence, leveraging narrative layouts and semantic alignment.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in creating educational content that aligns pedagogically with Bloom's taxonomy and Cognitive Load Theory, reducing fragmented attention and extraneous cognitive load.

Method: LEARN employs layout-conditioned generation, contrastive visual-semantic training, and prompt modulation to produce illustrations aligned with STEM education.

Result: The framework generates coherent visual sequences that support conceptual reasoning, spatial organization, and narrative-driven learning, along with potential integration into multimodal and adaptive educational systems.

Conclusion: LEARN presents a unique generative AI approach for education, combining layout storytelling, semantic learning, and cognitive scaffolding. Its code and dataset will be made available for further research and deployment.

Abstract: LEARN is a layout-aware diffusion framework designed to generate
pedagogically aligned illustrations for STEM education. It leverages a curated
BookCover dataset that provides narrative layouts and structured visual cues,
enabling the model to depict abstract and sequential scientific concepts with
strong semantic alignment. Through layout-conditioned generation, contrastive
visual-semantic training, and prompt modulation, LEARN produces coherent visual
sequences that support mid-to-high-level reasoning in line with Bloom's
taxonomy while reducing extraneous cognitive load as emphasized by Cognitive
Load Theory. By fostering spatially organized and story-driven narratives, the
framework counters fragmented attention often induced by short-form media and
promotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates
potential for integration with multimodal systems and curriculum-linked
knowledge graphs to create adaptive, exploratory educational content. As the
first generative approach to unify layout-based storytelling, semantic
structure learning, and cognitive scaffolding, LEARN represents a novel
direction for generative AI in education. The code and dataset will be released
to facilitate future research and practical deployment.

</details>


### [88] [Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models](https://arxiv.org/abs/2508.11165)
*Bing Liu,Le Wang,Mingming Liu,Hao Liu,Rui Yao,Yong Zhou,Peng Liu,Tongqiang Xia*

Main category: cs.CV

TL;DR: This paper presents a semi-supervised image dehazing method called EM-B3DM using Expectation-Maximization and Brownian Bridge Diffusion Models to improve haze removal efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing dehazing methods struggle with real-world thick haze scenes due to limited paired data and weak priors.

Method: The method uses a two-stage learning process: first, the EM algorithm decouples data distribution for modeling with a Brownian Bridge diffusion model; second, unpaired data enhances the model. Additionally, a detail-enhancing RDC block is introduced.

Result: Experiments show that EM-B3DM performs better or comparably to state-of-the-art methods on various datasets.

Conclusion: The proposed approach efficiently handles image dehazing, addressing the challenges of paired data scarcity while improving performance.

Abstract: Existing dehazing methods deal with real-world haze images with difficulty,
especially scenes with thick haze. One of the main reasons is the lack of
real-world paired data and robust priors. To avoid the costly collection of
paired hazy and clear images, we propose an efficient semi-supervised image
dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge
Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first
stage, we employ the EM algorithm to decouple the joint distribution of paired
hazy and clear images into two conditional distributions, which are then
modeled using a unified Brownian Bridge diffusion model to directly capture the
structural and content-related correlations between hazy and clear images. In
the second stage, we leverage the pre-trained model and large-scale unpaired
hazy and clear images to further improve the performance of image dehazing.
Additionally, we introduce a detail-enhanced Residual Difference Convolution
block (RDC) to capture gradient-level information, significantly enhancing the
model's representation capability. Extensive experiments demonstrate that our
EM-B3DM achieves superior or at least comparable performance to
state-of-the-art methods on both synthetic and real-world datasets.

</details>


### [89] [VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images](https://arxiv.org/abs/2508.11167)
*Jianhong Han,Yupei Wang,Liang Chen*

Main category: cs.CV

TL;DR: The paper addresses the challenge of source-free object detection (SFOD) for remote sensing images by introducing VG-DETR, a semi-supervised approach leveraging Vision Foundation Models to mitigate noisy pseudo-labels and enhance feature extraction.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of unsupervised domain adaptation methods in remote-sensing scenarios where source domain data is unavailable due to privacy and transmission constraints, especially in handling remote sensing imagery with dense objects and complex backgrounds.

Method: The authors propose VG-DETR, a Vision foundation-Guided Detection Transformer, employing a Vision Foundation Model in a semi-supervised framework. It includes a VFM-guided pseudo-label mining strategy to improve pseudo-label quality and a dual-level VFM-guided alignment method that aligns features at the instance and image levels using contrastive learning and similarity matching.

Result: VG-DETR demonstrates superior detection performance in source-free remote sensing tasks, overcoming the training challenges caused by noisy pseudo-labels and domain gaps.

Conclusion: The proposed VG-DETR method effectively leverages Vision Foundation Models to address challenges in source-free object detection, enhancing both label quality and feature robustness, making it a significant step forward for practical applications in remote sensing.

Abstract: Unsupervised domain adaptation methods have been widely explored to bridge
domain gaps. However, in real-world remote-sensing scenarios, privacy and
transmission constraints often preclude access to source domain data, which
limits their practical applicability. Recently, Source-Free Object Detection
(SFOD) has emerged as a promising alternative, aiming at cross-domain
adaptation without relying on source data, primarily through a self-training
paradigm. Despite its potential, SFOD frequently suffers from training collapse
caused by noisy pseudo-labels, especially in remote sensing imagery with dense
objects and complex backgrounds. Considering that limited target domain
annotations are often feasible in practice, we propose a Vision
foundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised
framework for SFOD in remote sensing images. VG-DETR integrates a Vision
Foundation Model (VFM) into the training pipeline in a "free lunch" manner,
leveraging a small amount of labeled target data to mitigate pseudo-label noise
while improving the detector's feature-extraction capability. Specifically, we
introduce a VFM-guided pseudo-label mining strategy that leverages the VFM's
semantic priors to further assess the reliability of the generated
pseudo-labels. By recovering potentially correct predictions from
low-confidence outputs, our strategy improves pseudo-label quality and
quantity. In addition, a dual-level VFM-guided alignment method is proposed,
which aligns detector features with VFM embeddings at both the instance and
image levels. Through contrastive learning among fine-grained prototypes and
similarity matching between feature maps, this dual-level alignment further
enhances the robustness of feature representations against domain gaps.
Extensive experiments demonstrate that VG-DETR achieves superior performance in
source-free remote sensing detection tasks.

</details>


### [90] [Better Supervised Fine-tuning for VQA: Integer-Only Loss](https://arxiv.org/abs/2508.11170)
*Baihong Qian,Haotian Fan,Wenjie Liao,Yunqiu Wang,Tao Li,Junhui Cui*

Main category: cs.CV

TL;DR: The paper introduces IOVQA, a fine-tuning approach for vision language models (VLMs) to perform better in video quality assessment (VQA) by focusing on integer-only labels and targeted loss calculation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance the accuracy and focus of vision language models in evaluating video quality, which current methods struggle with due to imprecise results and inefficient loss computation.

Method: The proposed IOVQA approach uses integer-only numerical labels (range: 10-50) for stability, converts decimal labels to integers, and applies a target-mask strategy that unmask critical digits during loss computation.

Result: After fine-tuning the Qwen2.5-VL model with IOVQA, the method demonstrated significant improvement in accuracy and consistency, achieving 3rd position in the VQualA 2025 GenAI-Bench competition.

Conclusion: IOVQA effectively improves VLM performance in quantitative evaluation by focusing on integer-only labels and targeted learning strategies, offering an innovative solution for enhancing VQA tasks.

Abstract: With the rapid advancement of vision language models(VLM), their ability to
assess visual content based on specific criteria and dimensions has become
increasingly critical for applications such as video-theme consistency
assessment and visual quality scoring. However, existing methods often suffer
from imprecise results and inefficient loss calculation, which limit the focus
of the model on key evaluation indicators. To address this, we propose
IOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to
enhance their performance in video quality assessment tasks. The key innovation
of IOVQA lies in its label construction and its targeted loss calculation
mechanism. Specifically, during dataset curation, we constrain the model's
output to integers within the range of [10,50], ensuring numerical stability,
and convert decimal Overall_MOS to integer before using them as labels. We also
introduce a target-mask strategy: when computing the loss, only the first
two-digit-integer of the label is unmasked, forcing the model to learn the
critical components of the numerical evaluation. After fine-tuning the
Qwen2.5-VL model using the constructed dataset, experimental results
demonstrate that the proposed method significantly improves the model's
accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025
GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work
highlights the effectiveness of merely leaving integer labels during
fine-tuning, providing an effective idea for optimizing VLMs in quantitative
evaluation scenarios.

</details>


### [91] [Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery](https://arxiv.org/abs/2508.11173)
*Ruobing Jiang,Yang Liu,Haobing Liu,Yanwei Yu,Chunyang Wang*

Main category: cs.CV

TL;DR: This paper addresses the challenges of Continuous Category Discovery (CCD), proposing the IDOD framework for novel class discovery and classification with reduced error accumulation and memory usage.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenges of discovering novel classes in continuously arriving data without predetermined categories or labels, while addressing catastrophic forgetting.

Method: The proposed method, IDOD, includes modules for independent enrichment of diversity via contrastive loss, joint discovery of novelty to avoid error accumulation, and continuous increment by orthogonality for efficient classification and memory management.

Result: Experimental results demonstrate that the proposed IDOD method achieves improved performance over state-of-the-art CCD methods on fine-grained datasets.

Conclusion: IDOD successfully mitigates the limitations of existing CCD methods, offering effective novel class discovery, better classification results, reduced error accumulation, and lower memory overhead.

Abstract: Continuous category discovery (CCD) aims to automatically discover novel
categories in continuously arriving unlabeled data. This is a challenging
problem considering that there is no number of categories and labels in the
newly arrived data, while also needing to mitigate catastrophic forgetting.
Most CCD methods cannot handle the contradiction between novel class discovery
and classification well. They are also prone to accumulate errors in the
process of gradually discovering novel classes. Moreover, most of them use
knowledge distillation and data replay to prevent forgetting, occupying more
storage space. To address these limitations, we propose Independence-based
Diversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes
independent enrichment of diversity module, joint discovery of novelty module,
and continuous increment by orthogonality module. In independent enrichment,
the backbone is trained separately using contrastive loss to avoid it focusing
only on features for classification. Joint discovery transforms multi-stage
novel class discovery into single-stage, reducing error accumulation impact.
Continuous increment by orthogonality module generates mutually orthogonal
prototypes for classification and prevents forgetting with lower space overhead
via representative representation replay. Experimental results show that on
challenging fine-grained datasets, our method outperforms the state-of-the-art
methods.

</details>


### [92] [Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning](https://arxiv.org/abs/2508.11176)
*Yumiao Zhao,Bo Jiang,Yuhe Ding,Xiao Wang,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: LatHAdapter proposes a novel hyperbolic learning technique for fine-tuning Vision-Language Models (VLMs) on few-shot classification tasks by leveraging latent semantic hierarchies.


<details>
  <summary>Details</summary>
Motivation: Conventional adapter-based fine-tuning struggles with modeling one-to-many associations and generalizing to unknown categories in visual-language tasks.

Method: LatHAdapter introduces learnable attribute prompts, projects data into hyperbolic space, and applies hierarchical regularization to capture semantic hierarchies.

Result: Experiments across four few-shot classification tasks demonstrate LatHAdapter's superior performance over existing fine-tuning methods, especially in adapting known and unknown classes.

Conclusion: The method effectively improves the alignment between visual and textual data, providing better adaptation and generalization in few-shot learning scenarios.

Abstract: Adapter-based approaches have garnered attention for fine-tuning pre-trained
Vision-Language Models (VLMs) on few-shot classification tasks. These methods
strive to develop a lightweight module that better aligns visual and (category)
textual representations, thereby enhancing performance on downstream few-shot
learning tasks. However, existing adapters generally learn/align (category)
textual-visual modalities via explicit spatial proximity in the underlying
embedding space, which i) fails to capture the inherent one-to-many
associations between categories and image samples and ii) struggles to
establish accurate associations between the unknown categories and images. To
address these issues, inspired by recent works on hyperbolic learning, we
develop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs
on downstream few-shot classification tasks. The core of LatHAdapter is to
exploit the latent semantic hierarchy of downstream training data and employ it
to provide richer, fine-grained guidance for the adapter learning process.
Specifically, LatHAdapter first introduces some learnable `attribute' prompts
as the bridge to align categories and images. Then, it projects the categories,
attribute prompts, and images within each batch in a hyperbolic space, and
employs hierarchical regularization to learn the latent semantic hierarchy of
them, thereby fully modeling the inherent one-to-many associations among
categories, learnable attributes, and image samples. Extensive experiments on
four challenging few-shot tasks show that the proposed LatHAdapter consistently
outperforms many other fine-tuning approaches, particularly in adapting known
classes and generalizing to unknown classes.

</details>


### [93] [Versatile Video Tokenization with Generative 2D Gaussian Splatting](https://arxiv.org/abs/2508.11183)
*Zhenghao Chen,Zicong Chen,Lei Liu,Yiming Wu,Dong Xu*

Main category: cs.CV

TL;DR: This paper introduces the Gaussian Video Transformer (GVT), a generative video tokenizer that creates adaptive 2D Gaussian representations to efficiently encode spatial and temporal video content, allowing better reconstruction, action recognition, and compression.


<details>
  <summary>Details</summary>
Motivation: Existing video tokenization methods use fixed-grid, patch-wise tokens, which struggle with spatial over-encoding and redundancy in static and dynamic temporal content.

Method: The GVT employs a generative 2D Gaussian Splatting strategy, combining spatio-temporal Gaussian embedding for spatial adaptability and Gaussian set partitioning for temporal versatility.

Result: Experimental evaluations using UCF101, Kinetics, and DAVIS datasets show that GVT achieves state-of-the-art video reconstruction, surpasses MAGVIT-v2 in action recognition, and provides competitive compression results.

Conclusion: GVT demonstrates enhanced spatial and temporal adaptability and generalization in video tokenization tasks, offering superior performance across multiple benchmarks.

Abstract: Video tokenization procedure is critical for a wide range of video processing
tasks. Most existing approaches directly transform video into fixed-grid and
patch-wise tokens, which exhibit limited versatility. Spatially, uniformly
allocating a fixed number of tokens often leads to over-encoding in
low-information regions. Temporally, reducing redundancy remains challenging
without explicitly distinguishing between static and dynamic content. In this
work, we propose the Gaussian Video Transformer (GVT), a versatile video
tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We
first extract latent rigid features from a video clip and represent them with a
set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian
Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D
Gaussians not only enhance spatial adaptability by assigning higher (resp.,
lower) rendering weights to regions with higher (resp., lower) information
content during rasterization, but also improve generalization by avoiding
per-video optimization.To enhance the temporal versatility, we introduce a
Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into
static and dynamic sets, which explicitly model static content shared across
different time-steps and dynamic content specific to each time-step, enabling a
compact representation.We primarily evaluate GVT on the video reconstruction,
while also assessing its performance on action recognition and compression
using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments
demonstrate that GVT achieves a state-of-the-art video reconstruction quality,
outperforms the baseline MAGVIT-v2 in action recognition, and delivers
comparable compression performance.

</details>


### [94] [CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector](https://arxiv.org/abs/2508.11185)
*Abhinav Kumar,Yuliang Guo,Zhihao Zhang,Xinyu Huang,Liu Ren,Xiaoming Liu*

Main category: cs.CV

TL;DR: The study addresses challenges in monocular 3D object detection caused by variations in camera height. It introduces CHARM3R, which enhances generalization to unseen camera heights.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the issue of poor performance of monocular 3D detectors when faced with variations in camera height, a problem not extensively studied.

Method: The researchers carried out an analysis using the CARLA dataset with varying camera heights to identify depth estimation as a critical factor, and then proposed CHARM3R, which averages different depth estimates to enhance robustness.

Result: CHARM3R improves generalization to unseen camera heights by over 45% and achieves state-of-the-art performance on the CARLA dataset.

Conclusion: The findings emphasize the importance of depth estimation in monocular 3D detection under varied camera heights, and CHARM3R proves effective in mitigating related challenges.

Abstract: Monocular 3D object detectors, while effective on data from one ego camera
height, struggle with unseen or out-of-distribution camera heights. Existing
methods often rely on Plucker embeddings, image transformations or data
augmentation. This paper takes a step towards this understudied problem by
first investigating the impact of camera height variations on state-of-the-art
(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset
with multiple camera heights, we observe that depth estimation is a primary
factor influencing performance under height variations. We mathematically prove
and also empirically observe consistent negative and positive trends in mean
depth error of regressed and ground-based depth models, respectively, under
camera height changes. To mitigate this, we propose Camera Height Robust
Monocular 3D Detector (CHARM3R), which averages both depth estimates within the
model. CHARM3R improves generalization to unseen camera heights by more than
$45\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at
https://github.com/abhi1kumar/CHARM3R

</details>


### [95] [Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark](https://arxiv.org/abs/2508.11192)
*Lavisha Aggarwal,Vikas Bahirwani,Lin Li,Andrea Colaco*

Main category: cs.CV

TL;DR: The paper introduces a method to turn instructional videos into task-guidance dialogues using AI, creating a dataset (HowToDIV) for research on procedural-task dialogues.


<details>
  <summary>Details</summary>
Motivation: To address the lack of video-based dialogue datasets for real-world task guidance and assist in improving AI agents' abilities to aid in multi-step tasks.

Method: A fully automatic approach, leveraging large language models, transforms instructional videos into conversations between an expert and a novice, synchronized with task steps and video clips.

Result: Created HowToDIV dataset with 507 dialogues, 6636 Q&A pairs, and 24 hours of video for diverse tasks, setting benchmarks for AI research.

Conclusion: The approach is cost-efficient, scalable, and establishes groundwork for improving AI-driven procedural task dialogues.

Abstract: Many everyday tasks ranging from fixing appliances, cooking recipes to car
maintenance require expert knowledge, especially when tasks are complex and
multi-step. Despite growing interest in AI agents, there is a scarcity of
dialogue-video datasets grounded for real world task assistance. In this paper,
we propose a simple yet effective approach that transforms single-person
instructional videos into task-guidance two-person dialogues, aligned with fine
grained steps and video-clips. Our fully automatic approach, powered by large
language models, offers an efficient alternative to the substantial cost and
effort required for human-assisted data collection. Using this technique, we
build HowToDIV, a large-scale dataset containing 507 conversations, 6636
question-answer pairs and 24 hours of videoclips across diverse tasks in
cooking, mechanics, and planting. Each session includes multi-turn conversation
where an expert teaches a novice user how to perform a task step by step, while
observing user's surrounding through a camera and microphone equipped wearable
device. We establish the baseline benchmark performance on HowToDIV dataset
through Gemma-3 model for future research on this new task of dialogues for
procedural-task assistance.

</details>


### [96] [UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning](https://arxiv.org/abs/2508.11196)
*Jiajin Guan,Haibo Mei,Bonan Zhang,Dan Liu,Yuanshuang Fu,Yue Zhang*

Main category: cs.CV

TL;DR: The paper introduces UAV-VL-R1, a lightweight vision-language model tailored for reasoning tasks using UAV aerial imagery.


<details>
  <summary>Details</summary>
Motivation: General-purpose vision-language models fail to perform well in UAV-based aerial imagery due to challenges like high resolution, complex spatial semantics, and real-time constraints.

Method: UAV-VL-R1 uses supervised fine-tuning and multi-stage reinforcement learning, leveraging the GRPO algorithm to ensure structured reasoning through rule-guided rewards and intra-group policy alignment.

Result: The model achieves a 48.17% higher zero-shot accuracy than a baseline, surpassing even a much larger variant in task performance, and operates efficiently on resource-constrained platforms.

Conclusion: UAV-VL-R1 effectively addresses performance and resource challenges in aerial visual reasoning, demonstrating improved accuracy, logical flexibility, and real-time deployment capability.

Abstract: Recent advances in vision-language models (VLMs) have demonstrated strong
generalization in natural image tasks. However, their performance often
degrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features
high resolution, complex spatial semantics, and strict real-time constraints.
These challenges limit the applicability of general-purpose VLMs to structured
aerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a
lightweight VLM explicitly designed for aerial visual reasoning. It is trained
using a hybrid method that combines supervised fine-tuning (SFT) and
multi-stage reinforcement learning (RL). We leverage the group relative policy
optimization (GRPO) algorithm to promote structured and interpretable reasoning
through rule-guided rewards and intra-group policy alignment. To support model
training and evaluation, we introduce a high-resolution visual question
answering dataset named HRVQA-VL, which consists of 50,019 annotated samples
covering eight UAV-relevant reasoning tasks, including object counting,
transportation recognition, and spatial scene inference. Experimental results
show that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the
Qwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which
is 36x larger, on multiple tasks. Ablation studies reveal that while SFT
improves semantic alignment, it may reduce reasoning diversity in mathematical
tasks. GRPO-based RL compensates for this limitation by enhancing logical
flexibility and the robustness of inference. Additionally, UAV-VL-R1 requires
only 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with
INT8, supporting real-time deployment on resource-constrained UAV platforms.

</details>


### [97] [A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network](https://arxiv.org/abs/2508.11212)
*Zhangjian Ji,Wenjin Zhang,Shaotong Qiao,Kai Feng,Yuhua Qian*

Main category: cs.CV

TL;DR: The paper introduces a two-stage knowledge distillation framework to transfer pose knowledge from a teacher model to a lightweight student model for human pose estimation. It improves robustness and accuracy while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop a human pose estimation model that is accurate, robust, and lightweight by leveraging knowledge distillation to transfer knowledge from a complex teacher model to a simpler student model.

Method: The authors propose a coarse-to-fine two-stage knowledge distillation framework. In the first stage, a human joints structure loss is used to transfer structural information on human joints. In the second stage, an Image-Guided Progressive Graph Convolutional Network (IGP-GCN) refines the initial pose, supervised progressively by the teacher model's outputs.

Result: Experiments using the COCO keypoint and CrowdPose datasets demonstrate that the proposed method outperforms numerous state-of-the-art methods, showing particularly strong results on the complex CrowdPose dataset.

Conclusion: The proposed two-stage framework successfully balances lightweight computational demand with high accuracy and robustness in human pose estimation, making it especially effective in challenging scenarios.

Abstract: Human pose estimation has been widely applied in the human-centric
understanding and generation, but most existing state-of-the-art human pose
estimation methods require heavy computational resources for accurate
predictions. In order to obtain an accurate, robust yet lightweight human pose
estimator, one feasible way is to transfer pose knowledge from a powerful
teacher model to a less-parameterized student model by knowledge distillation.
However, the traditional knowledge distillation framework does not fully
explore the contextual information among human joints. Thus, in this paper, we
propose a novel coarse-to-fine two-stage knowledge distillation framework for
human pose estimation. In the first-stage distillation, we introduce the human
joints structure loss to mine the structural information among human joints so
as to transfer high-level semantic knowledge from the teacher model to the
student model. In the second-stage distillation, we utilize an Image-Guided
Progressive Graph Convolutional Network (IGP-GCN) to refine the initial human
pose obtained from the first-stage distillation and supervise the training of
the IGP-GCN in the progressive way by the final output pose of teacher model.
The extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose
datasets, show that our proposed method performs favorably against lots of the
existing state-of-the-art human pose estimation methods, especially for the
more complex CrowdPose dataset, the performance improvement of our model is
more significant.

</details>


### [98] [A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving](https://arxiv.org/abs/2508.11218)
*Jialin Li,Shuqi Wu,Ning Wang*

Main category: cs.CV

TL;DR: This paper introduces a lightweight framework called Uncertainty Modal Modeling (UMM) to improve pedestrian re-identification performance under uncertain or missing input modalities, while ensuring computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in pedestrian re-identification systems for autonomous driving, where uncertain or missing input modalities hinder conventional methods.

Method: The method involves a framework with multimodal token mapping, synthetic modality augmentation, and cross-modal cue interaction. It also utilizes CLIP's vision-language alignment for efficient multimodal fusion.

Result: The proposed UMM framework exhibits robustness, generalization, and computational efficiency under uncertain input conditions across experimental setups.

Conclusion: UMM offers a scalable, robust, and efficient solution for pedestrian re-identification in resource-constrained environments, critical for autonomous driving applications.

Abstract: Re-Identification (ReID) is a critical technology in intelligent perception
systems, especially within autonomous driving, where onboard cameras must
identify pedestrians across views and time in real-time to support safe
navigation and trajectory prediction. However, the presence of uncertain or
missing input modalities--such as RGB, infrared, sketches, or textual
descriptions--poses significant challenges to conventional ReID approaches.
While large-scale pre-trained models offer strong multimodal semantic modeling
capabilities, their computational overhead limits practical deployment in
resource-constrained environments. To address these challenges, we propose a
lightweight Uncertainty Modal Modeling (UMM) framework, which integrates a
multimodal token mapper, synthetic modality augmentation strategy, and
cross-modal cue interactive learner. Together, these components enable unified
feature representation, mitigate the impact of missing modalities, and extract
complementary information across different data types. Additionally, UMM
leverages CLIP's vision-language alignment ability to fuse multimodal inputs
efficiently without extensive finetuning. Experimental results demonstrate that
UMM achieves strong robustness, generalization, and computational efficiency
under uncertain modality conditions, offering a scalable and practical solution
for pedestrian re-identification in autonomous driving scenarios.

</details>


### [99] [FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation](https://arxiv.org/abs/2508.11255)
*MengChao Wang,Qiang Wang,Fan Jiang,Mu Xu*

Main category: cs.CV

TL;DR: This paper tackles the challenges in audio-driven portrait animation by introducing Talking-Critic, a multimodal reward model, and TLPO, a preference optimization framework, achieving better alignment with human preferences in video generation.


<details>
  <summary>Details</summary>
Motivation: Existing audio-driven portrait animation methods struggle to balance multiple dimensions such as motion naturalness, lip-sync accuracy, and visual quality due to conflicting optimization objectives and limited datasets.

Method: The authors propose Talking-Critic, a reward model for evaluating videos on multiple dimensions, and TLPO, a framework that leverages diffusion-based models and expert modules to improve alignment with human preferences.

Result: Talking-Critic demonstrated superior alignment with human preference ratings, and TLPO showed significant improvement in lip-sync accuracy, motion naturalness, and visual quality compared to baseline models.

Conclusion: The combination of Talking-Critic and TLPO effectively addresses the multidimensional challenges in portrait animation, resulting in better qualitative and quantitative performance.

Abstract: Recent advances in audio-driven portrait animation have demonstrated
impressive capabilities. However, existing methods struggle to align with
fine-grained human preferences across multiple dimensions, such as motion
naturalness, lip-sync accuracy, and visual quality. This is due to the
difficulty of optimizing among competing preference objectives, which often
conflict with one another, and the scarcity of large-scale, high-quality
datasets with multidimensional preference annotations. To address these, we
first introduce Talking-Critic, a multimodal reward model that learns
human-aligned reward functions to quantify how well generated videos satisfy
multidimensional expectations. Leveraging this model, we curate Talking-NSQ, a
large-scale multidimensional human preference dataset containing 410K
preference pairs. Finally, we propose Timestep-Layer adaptive multi-expert
Preference Optimization (TLPO), a novel framework for aligning diffusion-based
portrait animation models with fine-grained, multidimensional preferences. TLPO
decouples preferences into specialized expert modules, which are then fused
across timesteps and network layers, enabling comprehensive, fine-grained
enhancement across all dimensions without mutual interference. Experiments
demonstrate that Talking-Critic significantly outperforms existing methods in
aligning with human preference ratings. Meanwhile, TLPO achieves substantial
improvements over baseline models in lip-sync accuracy, motion naturalness, and
visual quality, exhibiting superior performance in both qualitative and
quantitative evaluations. Ours project page:
https://fantasy-amap.github.io/fantasy-talking2/

</details>


### [100] [Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception](https://arxiv.org/abs/2508.11256)
*Junjie Wang,Keyu Chen,Yulin Li,Bin Chen,Hengshuang Zhao,Xiaojuan Qi,Zhuotao Tian*

Main category: cs.CV

TL;DR: The paper introduces DeCLIP, a framework that improves upon CLIP for open-vocabulary dense visual perception tasks by enhancing local feature representation.


<details>
  <summary>Details</summary>
Motivation: Current dense visual perception approaches struggle due to reliance on predefined categories and limitations in CLIP's local feature representation.

Method: DeCLIP enhances CLIP by decoupling its self-attention module to extract 'content' and 'context' features, employing semantic and object cues for spatial consistency and local discriminability.

Result: Experiments showed DeCLIP surpasses state-of-the-art performances in 2D detection, segmentation, 3D instance segmentation, video analysis, and 6D object pose estimation.

Conclusion: DeCLIP successfully addresses CLIP's limitations, establishing a robust framework for open-vocabulary dense perception tasks.

Abstract: Dense visual perception tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense perception often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. \revise{The context features are enhanced by jointly distilling
semantic correlations from Vision Foundation Models (VFMs) and object integrity
cues from diffusion models, thereby enhancing spatial consistency. In parallel,
the content features are aligned with image crop representations and
constrained by region correlations from VFMs to improve local discriminability.
Extensive experiments demonstrate that DeCLIP establishes a solid foundation
for open-vocabulary dense perception, consistently achieving state-of-the-art
performance across a broad spectrum of tasks, including 2D detection and
segmentation, 3D instance segmentation, video instance segmentation, and 6D
object pose estimation.} Code is available at
https://github.com/xiaomoguhz/DeCLIP

</details>


### [101] [Vision-Language Models display a strong gender bias](https://arxiv.org/abs/2508.11262)
*Aiswarya Konavoor,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CV

TL;DR: This paper evaluates gender biases in vision-language models by assessing the alignment of face image embeddings with occupational and activity-related text embeddings.


<details>
  <summary>Details</summary>
Motivation: To examine whether vision-language models exhibit gender-associated stereotypes in embedding alignment.

Method: A dataset of 220 face images categorized by gender and 150 occupation/activity-related statements was used. Image and text embeddings were calculated, and associations between genders and statements were analyzed using cosine similarity and statistical evaluations.

Result: The study provides a detailed map of gender associations in vision-language embeddings along with a bias evaluation framework.

Conclusion: The framework highlights gender-based alignment biases in vision-language models, underscoring the importance of evaluating these models' societal impacts.

Abstract: Vision-language models (VLM) align images and text in a shared representation
space that is useful for retrieval and zero-shot transfer. Yet, this alignment
can encode and amplify social stereotypes in subtle ways that are not obvious
from standard accuracy metrics. In this study, we test whether the contrastive
vision-language encoder exhibits gender-linked associations when it places
embeddings of face images near embeddings of short phrases that describe
occupations and activities. We assemble a dataset of 220 face photographs split
by perceived binary gender and a set of 150 unique statements distributed
across six categories covering emotional labor, cognitive labor, domestic
labor, technical labor, professional roles, and physical labor. We compute
unit-norm image embeddings for every face and unit-norm text embeddings for
every statement, then define a statement-level association score as the
difference between the mean cosine similarity to the male set and the mean
cosine similarity to the female set, where positive values indicate stronger
association with the male set and negative values indicate stronger association
with the female set. We attach bootstrap confidence intervals by resampling
images within each gender group, aggregate by category with a separate
bootstrap over statements, and run a label-swap null model that estimates the
level of mean absolute association we would expect if no gender structure were
present. The outcome is a statement-wise and category-wise map of gender
associations in a contrastive vision-language space, accompanied by
uncertainty, simple sanity checks, and a robust gender bias evaluation
framework.

</details>


### [102] [Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds](https://arxiv.org/abs/2508.11265)
*Pei He,Lingling Li,Licheng Jiao,Ronghua Shang,Fang Liu,Shuang Wang,Xu Liu,Wenping Ma*

Main category: cs.CV

TL;DR: This paper addresses domain generalization for 3D segmentation tasks using category-level geometry to improve segmentation accuracy in unseen environments.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of deploying segmentation models in unseen environments where domain shifts hamper performance.

Method: The paper introduces two frameworks: Category-level Geometry Embedding (CGE) for fine-grained geometric feature perception and Geometric Consistent Learning (GCL) for aligning category-level embeddings and focusing on invariant geometric information.

Result: Experimental evaluation shows that the proposed method achieves competitive segmentation accuracy compared to state-of-the-art domain generalization methods for 3D point clouds.

Conclusion: The proposed approach effectively improves domain generalization in 3D segmentation, enabling robust performance in unseen environments.

Abstract: Domain generalization in 3D segmentation is a critical challenge in deploying
models to unseen environments. Current methods mitigate the domain shift by
augmenting the data distribution of point clouds. However, the model learns
global geometric patterns in point clouds while ignoring the category-level
distribution and alignment. In this paper, a category-level geometry learning
framework is proposed to explore the domain-invariant geometric features for
domain generalized 3D semantic segmentation. Specifically, Category-level
Geometry Embedding (CGE) is proposed to perceive the fine-grained geometric
properties of point cloud features, which constructs the geometric properties
of each class and couples geometric embedding to semantic learning. Secondly,
Geometric Consistent Learning (GCL) is proposed to simulate the latent 3D
distribution and align the category-level geometric embeddings, allowing the
model to focus on the geometric invariant information to improve
generalization. Experimental results verify the effectiveness of the proposed
method, which has very competitive segmentation accuracy compared with the
state-of-the-art domain generalized point cloud methods.

</details>


### [103] [Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering](https://arxiv.org/abs/2508.11272)
*Jun Li,Kai Li,Shaoguo Liu,Tingting Gao*

Main category: cs.CV

TL;DR: The paper tackles the challenge of Composed Image Retrieval (CIR) by introducing a new framework, PMTFR, which incorporates a training-free refinement model to improve retrieval results without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing CIR methods often rely on additional ranking models and complex textual reasoning, which increases training cost and complexity. The paper aims to simplify and improve CIR tasks, particularly in supervised settings.

Method: The authors introduced the Pyramid Matching Model with Training-Free Refinement (PMTFR), leveraging a module called Pyramid Patcher to enhance visual understanding. Representations from Chain-of-Thought (CoT) data were injected into Large Vision-Language Models (LVLMs) for refined scores.

Result: PMTFR achieved superior performance over state-of-the-art methods in supervised CIR tasks without requiring extra training, as validated by extensive experiments on CIR benchmarks.

Conclusion: The framework effectively improves CIR tasks by simplifying the refinement paradigm and integrating visual-textual understanding. The approach reduces dependency on explicit textual reasoning and additional training, delivering better results in a supervised setting.

Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it
requires jointly understanding a reference image and a modified textual
instruction to find relevant target images. Some existing methods attempt to
use a two-stage approach to further refine retrieval results. However, this
often requires additional training of a ranking model. Despite the success of
Chain-of-Thought (CoT) techniques in reducing training costs for language
models, their application in CIR tasks remains limited -- compressing visual
information into text or relying on elaborate prompt designs. Besides, existing
works only utilize it for zero-shot CIR, as it is challenging to achieve
satisfactory results in supervised CIR with a well-trained model. In this work,
we proposed a framework that includes the Pyramid Matching Model with
Training-Free Refinement (PMTFR) to address these challenges. Through a simple
but effective module called Pyramid Patcher, we enhanced the Pyramid Matching
Model's understanding of visual information at different granularities.
Inspired by representation engineering, we extracted representations from COT
data and injected them into the LVLMs. This approach allowed us to obtain
refined retrieval scores in the Training-Free Refinement paradigm without
relying on explicit textual reasoning, further enhancing performance. Extensive
experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art
methods in supervised CIR tasks. The code will be made public.

</details>


### [104] [Probing the Representational Power of Sparse Autoencoders in Vision Models](https://arxiv.org/abs/2508.11277)
*Matthew Lyle Olson,Musashi Hinck,Neale Ratzlaff,Changbai Li,Phillip Howard,Vasudev Lal,Shao-Yen Tseng*

Main category: cs.CV

TL;DR: This paper evaluates Sparse Autoencoders (SAEs) in vision models, showing their utility for improving interpretability, generalization, and controllable generation.


<details>
  <summary>Details</summary>
Motivation: SAEs are popular for interpreting large language models, but their application to vision models is less studied, despite potential benefits.

Method: The authors extensively evaluate SAEs in vision models by applying them to diverse image-based tasks across three model types: vision embedding models, multi-modal models, and diffusion models.

Result: SAE features were found to be semantically meaningful, enhancing out-of-distribution generalization and enabling semantic steering and human-interpretable attribute discovery.

Conclusion: SAEs show strong potential in improving interpretability, generalization, and steerability in vision models, providing a foundation for future research in this area.

Abstract: Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting
the hidden states of large language models (LLMs). By learning to reconstruct
activations from a sparse bottleneck layer, SAEs discover interpretable
features from the high-dimensional internal representations of LLMs. Despite
their popularity with language models, SAEs remain understudied in the visual
domain. In this work, we provide an extensive evaluation the representational
power of SAEs for vision models using a broad range of image-based tasks. Our
experimental results demonstrate that SAE features are semantically meaningful,
improve out-of-distribution generalization, and enable controllable generation
across three vision model architectures: vision embedding models, multi-modal
LMMs and diffusion models. In vision embedding models, we find that learned SAE
features can be used for OOD detection and provide evidence that they recover
the ontological structure of the underlying model. For diffusion models, we
demonstrate that SAEs enable semantic steering through text encoder
manipulation and develop an automated pipeline for discovering
human-interpretable attributes. Finally, we conduct exploratory experiments on
multi-modal LLMs, finding evidence that SAE features reveal shared
representations across vision and language modalities. Our study provides a
foundation for SAE evaluation in vision models, highlighting their strong
potential improving interpretability, generalization, and steerability in the
visual domain.

</details>


### [105] [Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction](https://arxiv.org/abs/2508.11282)
*Muzammil Khan,Enzo Kerkhof,Matteo Fusaglia,Koert Kuhlmann,Theo Ruers,Françoise J. Siepel*

Main category: cs.CV

TL;DR: This paper introduces a unified framework for monocular endoscopic tissue reconstruction using innovative modules for depth prediction and motion correction, showing improved outcomes compared to existing techniques.


<details>
  <summary>Details</summary>
Motivation: Monocular endoscope pose estimation and tissue reconstruction face challenges such as depth ambiguity, tissue deformation, inconsistent motion, and limited field of view, which hinder navigation and spatial awareness.

Method: The framework integrates the MAPIS-Depth module for depth prediction using Depth Pro and Depth Anything, employs RAFT for temporal refinement, and incorporates the WEMA-RTDL module for pose registration. Truncated signed distance function and marching cubes methods enable 3D surface mesh extraction.

Result: Evaluations on HEVD and SCARED datasets, along with ablation and comparative analyses, demonstrate the framework's robustness and superior performance compared to state-of-the-art methods.

Conclusion: By addressing depth ambiguity and motion artifacts, the framework significantly improves 3D tissue reconstruction and endoscopic pose estimation for minimally invasive surgeries.

Abstract: Accurate endoscope pose estimation and 3D tissue surface reconstruction
significantly enhances monocular minimally invasive surgical procedures by
enabling accurate navigation and improved spatial awareness. However, monocular
endoscope pose estimation and tissue reconstruction face persistent challenges,
including depth ambiguity, physiological tissue deformation, inconsistent
endoscope motion, limited texture fidelity, and a restricted field of view. To
overcome these limitations, a unified framework for monocular endoscopic tissue
reconstruction that integrates scale-aware depth prediction with
temporally-constrained perceptual refinement is presented. This framework
incorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust
initialisation and Depth Anything for efficient per-frame depth prediction, in
conjunction with L-BFGS-B optimisation, to generate pseudo-metric depth
estimates. These estimates are temporally refined by computing pixel
correspondences using RAFT and adaptively blending flow-warped frames based on
LPIPS perceptual similarity, thereby reducing artefacts arising from
physiological tissue deformation and motion. To ensure accurate registration of
the synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module
is integrated, optimising both rotation and translation. Finally, truncated
signed distance function-based volumetric fusion and marching cubes are applied
to extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,
with ablation and comparative analyses, demonstrate the framework's robustness
and superiority over state-of-the-art methods.

</details>


### [106] [TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation](https://arxiv.org/abs/2508.11284)
*Yilin Mi,Qixin Yan,Zheng-Peng Duan,Chunle Guo,Hubery Yin,Hao Liu,Chen Li,Chongyi Li*

Main category: cs.CV

TL;DR: TimeMachine introduces a diffusion-based framework for accurate facial age editing while maintaining identity consistency.


<details>
  <summary>Details</summary>
Motivation: Fine-grained age editing in facial images is difficult due to the challenge of preserving identity features.

Method: TimeMachine leverages a diffusion model with a multi-cross attention module and introduces an Age Classifier Guidance (ACG) module alongside building the HFFA dataset.

Result: The proposed framework demonstrates state-of-the-art performance in precise age editing and identity preservation.

Conclusion: TimeMachine provides accurate, controllable facial age editing, addressing key challenges in disentangling features and enhancing dataset availability.

Abstract: With the advancement of generative models, facial image editing has made
significant progress. However, achieving fine-grained age editing while
preserving personal identity remains a challenging task.In this paper, we
propose TimeMachine, a novel diffusion-based framework that achieves accurate
age editing while keeping identity features unchanged. To enable fine-grained
age editing, we inject high-precision age information into the multi-cross
attention module, which explicitly separates age-related and identity-related
features. This design facilitates more accurate disentanglement of age
attributes, thereby allowing precise and controllable manipulation of facial
aging.Furthermore, we propose an Age Classifier Guidance (ACG) module that
predicts age directly in the latent space, instead of performing denoising
image reconstruction during training. By employing a lightweight module to
incorporate age constraints, this design enhances age editing accuracy by
modest increasing training cost. Additionally, to address the lack of
large-scale, high-quality facial age datasets, we construct a HFFA dataset
(High-quality Fine-grained Facial-Age dataset) which contains one million
high-resolution images labeled with identity and facial attributes.
Experimental results demonstrate that TimeMachine achieves state-of-the-art
performance in fine-grained age editing while preserving identity consistency.

</details>


### [107] [Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study](https://arxiv.org/abs/2508.11301)
*Jiarong Li,Imad Ali Shah,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: This study explores hyperspectral imaging for improved pedestrian segmentation in urban automotive systems, leveraging the Hyperspectral City v2 dataset and dimensionality-reduction techniques such as PCA and CSNR-JMIM.


<details>
  <summary>Details</summary>
Motivation: To address safety challenges in automotive perception caused by pedestrians blending with backgrounds in RGB imaging, improving segmentation through enhanced spectral discrimination.

Method: Compared RGB data with two methods for reducing 128-channel HSI data to 3-channel representations: Principal Component Analysis (PCA) and CSNR-JMIM. Evaluated three segmentation models: U-Net, DeepLabV3+, and SegFormer.

Result: CSNR-JMIM outperformed RGB, improving IoU by 1.44% and F1-score by 2.18% for pedestrians and showing similar gains for riders with IoU up by 1.43% and F1-score up by 2.25%.

Conclusion: Optimal HSI band selection significantly enhances pedestrian segmentation accuracy, highlighting HSI's value for safety-critical automotive applications.

Abstract: Pedestrian segmentation in automotive perception systems faces critical
safety challenges due to metamerism in RGB imaging, where pedestrians and
backgrounds appear visually indistinguishable.. This study investigates the
potential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation
in urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We
compared standard RGB against two dimensionality-reduction approaches by
converting 128-channel HSI data into three-channel representations: Principal
Component Analysis (PCA) and optimal band selection using Contrast
Signal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).
Three semantic segmentation models were evaluated: U-Net, DeepLabV3+, and
SegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements
of 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian
segmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%
F1-score improvements. These improved performance results from enhanced
spectral discrimination of optimally selected HSI bands effectively reducing
false positives. This study demonstrates robust pedestrian segmentation through
optimal HSI band selection, showing significant potential for safety-critical
automotive applications.

</details>


### [108] [Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval](https://arxiv.org/abs/2508.11313)
*Weijia Liu,Jiuxin Cao,Bo Miao,Zhiheng Fu,Xuelin Zhu,Jiawei Ge,Bo Liu,Mehwish Nasim,Ajmal Mian*

Main category: cs.CV

TL;DR: The paper introduces DRNet, a denoise-then-retrieve approach to improve text-driven Video Moment Retrieval by filtering irrelevant clips and enhancing multimodal alignment, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current VMR methods struggle with irrelevant video clips disrupting multimodal alignment and optimization, motivating the need for a method to filter out such noise.

Method: The proposed DRNet utilizes Text-Conditioned Denoising (TCD) to filter irrelevant video clips using cross-attention and structured state space blocks, and Text-Reconstruction Feedback (TRF) to align purified video representations with text embeddings for retrieval.

Result: DRNet outperforms state-of-the-art methods on Charades-STA and QVHighlights, demonstrating superior metrics across the board.

Conclusion: The denoise-then-retrieve paradigm effectively improves VMR performance and is adaptable for integration into other advanced VMR models to enhance results.

Abstract: Current text-driven Video Moment Retrieval (VMR) methods encode all video
clips, including irrelevant ones, disrupting multimodal alignment and hindering
optimization. To this end, we propose a denoise-then-retrieve paradigm that
explicitly filters text-irrelevant clips from videos and then retrieves the
target moment using purified multimodal representations. Following this
paradigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising
Text-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)
modules. TCD integrates cross-attention and structured state space blocks to
dynamically identify noisy clips and produce a noise mask to purify multimodal
video representations. TRF further distills a single query embedding from
purified video representations and aligns it with the text embedding, serving
as auxiliary supervision for denoising during training. Finally, we perform
conditional retrieval using text embeddings on purified video representations
for accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that
our approach surpasses state-of-the-art methods on all metrics. Furthermore,
our denoise-then-retrieve paradigm is adaptable and can be seamlessly
integrated into advanced VMR models to boost performance.

</details>


### [109] [Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models](https://arxiv.org/abs/2508.11317)
*Yuchen Zhou,Jiayu Tang,Shuo Yang,Xiaoyan Xiao,Yuqin Dai,Wenhao Yang,Chao Gou,Xiaobo Xia,Tat-Seng Chua*

Main category: cs.CV

TL;DR: The paper introduces LogicBench, a benchmark to evaluate logical understanding in Vision-Language Models (VLMs), and proposes LogicCLIP, a training framework to enhance their logical reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: VLMs like CLIP are foundational for multimodal intelligence but lack robust logical understanding, limiting their practical reliability. The authors aim to address these logical deficiencies.

Method: The authors created LogicBench with 50,000 vision-language pairs across diverse scenarios for performance evaluation. They also developed LogicCLIP, which uses logic-aware data generation, contrastive learning, and novel objectives for training VLMs.

Result: LogicCLIP demonstrated significant improvements in logical tasks across all LogicBench domains, outperforming existing baselines, while maintaining high performance in general vision-language benchmarks.

Conclusion: LogicBench and LogicCLIP are proposed as key resources for improving the logical reasoning capabilities of Vision-Language Models and advancing multimodal intelligence.

Abstract: Vision-Language Models (VLMs), exemplified by CLIP, have emerged as
foundational for multimodal intelligence. However, their capacity for logical
understanding remains significantly underexplored, resulting in critical
''logical blindspots'' that limit their reliability in practical applications.
To systematically diagnose this, we introduce LogicBench, a comprehensive
benchmark with over 50,000 vision-language pairs across 9 logical categories
and 4 diverse scenarios: images, videos, anomaly detection, and medical
diagnostics. Our evaluation reveals that existing VLMs, even the
state-of-the-art ones, fall at over 40 accuracy points below human performance,
particularly in challenging tasks like Causality and Conditionality,
highlighting their reliance on surface semantics over critical logical
structures. To bridge this gap, we propose LogicCLIP, a novel training
framework designed to boost VLMs' logical sensitivity through advancements in
both data generation and optimization objectives. LogicCLIP utilizes
logic-aware data generation and a contrastive learning strategy that combines
coarse-grained alignment, a fine-grained multiple-choice objective, and a novel
logical structure-aware objective. Extensive experiments demonstrate
LogicCLIP's substantial improvements in logical comprehension across all
LogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP
retains, and often surpasses, competitive performance on general
vision-language benchmarks, demonstrating that the enhanced logical
understanding does not come at the expense of general alignment. We believe
that LogicBench and LogicCLIP will be important resources for advancing VLM
logical capabilities.

</details>


### [110] [Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking](https://arxiv.org/abs/2508.11323)
*Haonan Zhang,Xinyao Wang,Boxi Wu,Tu Zheng,Wang Yunhua,Zheng Yang*

Main category: cs.CV

TL;DR: The paper introduces DSC-Track, a 3D multi-object tracking method leveraging cue-consistency in spatial geometry, achieving state-of-the-art results on datasets.


<details>
  <summary>Details</summary>
Motivation: Existing 3D tracking paradigms fail in complex scenarios due to reliance on individual object motions, ignoring spatial relationships.

Method: The authors propose DSC-Track, combining spatiotemporal encoders, cue-consistency modules, and dynamic updates to enhance tracking performance.

Result: DSC-Track achieves 73.2% and 70.3% AMOTA on the nuScenes validation and test sets respectively, demonstrating state-of-the-art performance.

Conclusion: The study presents a robust approach to overcome interference in multi-object tracking by focusing on spatial cue-consistency, achieving competitive results.

Abstract: 3D multi-object tracking is a critical and challenging task in the field of
autonomous driving. A common paradigm relies on modeling individual object
motion, e.g., Kalman filters, to predict trajectories. While effective in
simple scenarios, this approach often struggles in crowded environments or with
inaccurate detections, as it overlooks the rich geometric relationships between
objects. This highlights the need to leverage spatial cues. However, existing
geometry-aware methods can be susceptible to interference from irrelevant
objects, leading to ambiguous features and incorrect associations. To address
this, we propose focusing on cue-consistency: identifying and matching stable
spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency
Tracker (DSC-Track) to implement this principle. Firstly, we design a unified
spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative
trajectory embeddings while suppressing interference. Secondly, our
cue-consistency transformer module explicitly aligns consistent feature
representations between historical tracks and current detections. Finally, a
dynamic update mechanism preserves salient spatiotemporal information for
stable online tracking. Extensive experiments on the nuScenes and Waymo Open
Datasets validate the effectiveness and robustness of our approach. On the
nuScenes benchmark, for instance, our method achieves state-of-the-art
performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,
respectively.

</details>


### [111] [Noise Matters: Optimizing Matching Noise for Diffusion Classifiers](https://arxiv.org/abs/2508.11330)
*Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: The paper proposes a novel method to improve diffusion classifiers (DCs) by addressing the instability caused by random noise through optimized noise strategies.


<details>
  <summary>Details</summary>
Motivation: To address the inherent noise instability of diffusion classifiers, which leads to variable classification performance and necessitates computationally expensive noise ensembling.

Method: The study introduces 'Noise Optimization' (NoOp) to create 'good noises' by optimizing dataset-specific noise and learning image-specific noise offsets with a meta-network, replacing random noise in diffusion classifiers.

Result: Experiments on multiple datasets show that the proposed NoOp significantly reduces noise instability and improves classification performance.

Conclusion: NoOp enhances the stability and efficiency of diffusion classifiers by ensuring the alignment of optimized noise with both frequency and spatial characteristics of the data.

Abstract: Although today's pretrained discriminative vision-language models (e.g.,
CLIP) have demonstrated strong perception abilities, such as zero-shot image
classification, they also suffer from the bag-of-words problem and spurious
bias. To mitigate these problems, some pioneering studies leverage powerful
generative models (e.g., pretrained diffusion models) to realize generalizable
image classification, dubbed Diffusion Classifier (DC). Specifically, by
randomly sampling a Gaussian noise, DC utilizes the differences of denoising
effects with different category conditions to classify categories.
Unfortunately, an inherent and notorious weakness of existing DCs is noise
instability: different random sampled noises lead to significant performance
changes. To achieve stable classification performance, existing DCs always
ensemble the results of hundreds of sampled noises, which significantly reduces
the classification speed. To this end, we firstly explore the role of noise in
DC, and conclude that: there are some ``good noises'' that can relieve the
instability. Meanwhile, we argue that these good noises should meet two
principles: Frequency Matching and Spatial Matching. Regarding both principles,
we propose a novel Noise Optimization method to learn matching (i.e., good)
noise for DCs: NoOp. For frequency matching, NoOp first optimizes a
dataset-specific noise: Given a dataset and a timestep t, optimize one randomly
initialized parameterized noise. For Spatial Matching, NoOp trains a
Meta-Network that adopts an image as input and outputs image-specific noise
offset. The sum of optimized noise and noise offset will be used in DC to
replace random noise. Extensive ablations on various datasets demonstrated the
effectiveness of NoOp.

</details>


### [112] [GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition](https://arxiv.org/abs/2508.11334)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Md Jawadul Hasan,Tze Hui Liew*

Main category: cs.CV

TL;DR: GANDiff FR, a synthetic framework combining StyleGAN3 and diffusion-based controls, measures and reduces bias in face recognition models with demographic and environmental factors.


<details>
  <summary>Details</summary>
Motivation: Bias in face recognition models stemming from demographic and environmental factors needs a rigorous, reproducible framework for evaluation and mitigation.

Method: GANDiff FR combines StyleGAN3 for identity-preserving generation with diffusion methods for controlling pose, illumination, and expressions, synthesizing balanced faces for bias evaluation.

Result: AdaFace showed a 60% reduction in bias disparity compared to others, and synthetic-to-real bias transfer effectiveness correlated strongly at r=0.85.

Conclusion: GANDiff FR establishes a scalable, reproducible standard for fairness evaluation in AI systems while aligning with regulatory requirements and supports transparency through released code and datasets.

Abstract: We introduce GANDiff FR, the first synthetic framework that precisely
controls demographic and environmental factors to measure, explain, and reduce
bias with reproducible rigor. GANDiff FR unifies StyleGAN3-based
identity-preserving generation with diffusion-based attribute control, enabling
fine-grained manipulation of pose around 30 degrees, illumination (four
directions), and expression (five levels) under ceteris paribus conditions. We
synthesize 10,000 demographically balanced faces across five cohorts validated
for realism via automated detection (98.2%) and human review (89%) to isolate
and quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under
matched operating points shows AdaFace reduces inter-group TPR disparity by 60%
(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.
Cross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong
synthetic-to-real transfer (r 0.85). Despite around 20% computational overhead
relative to pure GANs, GANDiff FR yields three times more attribute-conditioned
variants, establishing a reproducible, regulation-aligned (EU AI Act) standard
for fairness auditing. Code and data are released to support transparent,
scalable bias evaluation.

</details>


### [113] [Index-Aligned Query Distillation for Transformer-based Incremental Object Detection](https://arxiv.org/abs/2508.11339)
*Mingxiao Ma,Shunyao Zhu,Guoliang Kang*

Main category: cs.CV

TL;DR: The paper addresses catastrophic forgetting in incremental object detection (IOD) and introduces Index-Aligned Query Distillation (IAQD) to outperform traditional Hungarian Matching-based knowledge distillation methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle catastrophic forgetting in transformer-based incremental object detection models, especially because Hungarian Matching causes mismatched knowledge during the knowledge distillation process.

Method: The proposed Index-Aligned Query Distillation (IAQD) aligns queries of past and present models based on their indices rather than Hungarian Matching, and selectively distills knowledge from critical queries relevant to old categories.

Result: IAQD minimizes knowledge forgetting in IOD and achieves state-of-the-art detection performance as demonstrated on benchmarks.

Conclusion: IAQD is an effective approach for preserving encoded knowledge from previously learned categories while allowing for the expansion of novel category detection in transformer-based models.

Abstract: Incremental object detection (IOD) aims to continuously expand the capability
of a model to detect novel categories while preserving its performance on
previously learned ones. When adopting a transformer-based detection model to
perform IOD, catastrophic knowledge forgetting may inevitably occur, meaning
the detection performance on previously learned categories may severely
degenerate. Previous typical methods mainly rely on knowledge distillation (KD)
to mitigate the catastrophic knowledge forgetting of transformer-based
detection models. Specifically, they utilize Hungarian Matching to build a
correspondence between the queries of the last-phase and current-phase
detection models and align the classifier and regressor outputs between matched
queries to avoid knowledge forgetting. However, we observe that in IOD task,
Hungarian Matching is not a good choice. With Hungarian Matching, the query of
the current-phase model may match different queries of the last-phase model at
different iterations during KD. As a result, the knowledge encoded in each
query may be reshaped towards new categories, leading to the forgetting of
previously encoded knowledge of old categories. Based on our observations, we
propose a new distillation approach named Index-Aligned Query Distillation
(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD
establishes a correspondence between queries of the previous and current phase
models that have the same index. Moreover, we perform index-aligned
distillation only on partial queries which are critical for the detection of
previous categories. In this way, IAQD largely preserves the previous semantic
and spatial encoding capabilities without interfering with the learning of new
categories. Extensive experiments on representative benchmarks demonstrate that
IAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art
performance.

</details>


### [114] [Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification](https://arxiv.org/abs/2508.11340)
*Yuanlin Liu,Zhihan Zhou,Mingqiang Wei,Youyi Song*

Main category: cs.CV

TL;DR: The paper proposes an active labeling approach that reduces human cost for creating representative training datasets in cervical cell classification.


<details>
  <summary>Details</summary>
Motivation: Existing classification methods for cervical cancer diagnosis require large, representative datasets, which are costly to produce.

Method: Active labeling is introduced, using classifier uncertainty to select beneficial images for labeling, minimizing human effort.

Result: The algorithm efficiently enhances dataset representativity with reduced human effort, validated through extensive empirical tests.

Conclusion: The proposed method demonstrates potential for cost-effective, data-efficient cervical cell classification, improving accessibility without compromising accuracy.

Abstract: Information on the number and category of cervical cells is crucial for the
diagnosis of cervical cancer. However, existing classification methods capable
of automatically measuring this information require the training dataset to be
representative, which consumes an expensive or even unaffordable human cost. We
herein propose active labeling that enables us to construct a representative
training dataset using a much smaller human cost for data-efficient cervical
cell classification. This cost-effective method efficiently leverages the
classifier's uncertainty on the unlabeled cervical cell images to accurately
select images that are most beneficial to label. With a fast estimation of the
uncertainty, this new algorithm exhibits its validity and effectiveness in
enhancing the representative ability of the constructed training dataset. The
extensive empirical results confirm its efficacy again in navigating the usage
of human cost, opening the avenue for data-efficient cervical cell
classification.

</details>


### [115] [Semantically Guided Adversarial Testing of Vision Models Using Language Models](https://arxiv.org/abs/2508.11341)
*Katarzyna Filus,Jorge M. Cruz-Duarte*

Main category: cs.CV

TL;DR: The paper proposes a semantics-guided framework for adversarial target selection in vision models using pretrained language and vision-language models.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial target selection strategies rely on random or static methods, causing limitations in interpretability, reproducibility, and flexibility.

Method: The framework uses cross-modal knowledge transfer from pretrained models (BERT, TinyLLAMA, CLIP) to evaluate semantic relationships and select target labels.

Result: Experiments across multiple vision models and attack methods show improved target selection and effectiveness, outperforming static resources like WordNet.

Conclusion: Pretrained models enable scalable and interpretable adversarial benchmarks, supporting better evaluation of attack strategies in vision models.

Abstract: In targeted adversarial attacks on vision models, the selection of the target
label is a critical yet often overlooked determinant of attack success. This
target label corresponds to the class that the attacker aims to force the model
to predict. Now, existing strategies typically rely on randomness, model
predictions, or static semantic resources, limiting interpretability,
reproducibility, or flexibility. This paper then proposes a semantics-guided
framework for adversarial target selection using the cross-modal knowledge
transfer from pretrained language and vision-language models. We evaluate
several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity
sources to select the most and least semantically related labels with respect
to the ground truth, forming best- and worst-case adversarial scenarios. Our
experiments on three vision models and five attack methods reveal that these
models consistently render practical adversarial targets and surpass static
lexical databases, such as WordNet, particularly for distant class
relationships. We also observe that static testing of target labels offers a
preliminary assessment of the effectiveness of similarity sources, \textit{a
priori} testing. Our results corroborate the suitability of pretrained models
for constructing interpretable, standardized, and scalable adversarial
benchmarks across architectures and datasets.

</details>


### [116] [HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model](https://arxiv.org/abs/2508.11350)
*Zhenhao Zhang,Hanqing Wang,Xiangyu Zeng,Ziyu Cheng,Jiaxin Liu,Haoyu Yan,Zhirui Liu,Kaiyang Ji,Tianxiang Gui,Ke Hu,Kangyi Chen,Yahao Fan,Mokai Pan*

Main category: cs.CV

TL;DR: This paper introduces HOID-R1, a novel framework combining chain-of-thought-guided fine-tuning, reinforcement learning, and a mechanism to improve reasoning for improved human-object interaction (HOI) detection.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of open-vocabulary HOI detection methods, which overly rely on language models and lack 3D spatial understanding.

Method: The framework applies chain-of-thought supervised fine-tuning for reasoning, group relative policy optimization (GRPO) in reinforcement learning, and employs an 'MLLM-as-a-judge' mechanism to supervise generated reasoning outputs.

Result: HOID-R1 achieves state-of-the-art results on HOI detection benchmarks and exhibits strong performance in generalizing to open-world scenarios.

Conclusion: Integrating multi-reward reinforcement learning, reasoning supervision, and CoT fine-tuning improves HOI detection, enabling better understanding and alignment across diverse modalities.

Abstract: Understanding and recognizing human-object interaction (HOI) is a pivotal
application in AR/VR and robotics. Recent open-vocabulary HOI detection
approaches depend exclusively on large language models for richer textual
prompts, neglecting their inherent 3D spatial understanding capabilities. To
address this shortcoming, we introduce HOID-R1, the first HOI detection
framework that integrates chain-of-thought (CoT) guided supervised fine-tuning
(SFT) with group relative policy optimization (GRPO) within a reinforcement
learning (RL) paradigm. Specifically, we initially apply SFT to imbue the model
with essential reasoning capabilities, forcing the model to articulate its
thought process in the output. Subsequently, we integrate GRPO to leverage
multi-reward signals for policy optimization, thereby enhancing alignment
across diverse modalities. To mitigate hallucinations in the CoT reasoning, we
introduce an "MLLM-as-a-judge" mechanism that supervises the CoT outputs,
further improving generalization. Extensive experiments show that HOID-R1
achieves state-of-the-art performance on HOI detection benchmarks and
outperforms existing methods in open-world generalization to novel scenarios.

</details>


### [117] [Leveraging the RETFound foundation model for optic disc segmentation in retinal images](https://arxiv.org/abs/2508.11354)
*Zhenyi Zhao,Muthu Rama Krishnan Mookiah,Emanuele Trucco*

Main category: cs.CV

TL;DR: The paper adapts RETFound, a foundation model for retinal images, for optic disc segmentation, achieving superior performance compared to task-specific networks.


<details>
  <summary>Details</summary>
Motivation: RETFound demonstrated excellence in diagnosing diseases from retinal images, but its application in tasks like optic disc segmentation hasn't been explored yet.

Method: The researchers trained RETFound by adding a segmentation head with minimal task-specific examples, then tested it across multiple datasets.

Result: The model achieved approximately 96% Dice score consistently across public and private datasets, surpassing state-of-the-art task-specific models.

Conclusion: RETFound's adaptation for optic disc segmentation highlights the potential of foundation models as versatile alternatives to traditional, task-specific architectures for retinal image analysis.

Abstract: RETFound is a well-known foundation model (FM) developed for fundus camera
and optical coherence tomography images. It has shown promising performance
across multiple datasets in diagnosing diseases, both eye-specific and
systemic, from retinal images. However, to our best knowledge, it has not been
used for other tasks. We present the first adaptation of RETFound for optic
disc segmentation, a ubiquitous and foundational task in retinal image
analysis. The resulting segmentation system outperforms state-of-the-art,
segmentation-specific baseline networks after training a head with only a very
modest number of task-specific examples. We report and discuss results with
four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private
dataset, GoDARTS, achieving about 96% Dice consistently across all datasets.
Overall, our method obtains excellent performance in internal verification,
domain generalization and domain adaptation, and exceeds most of the
state-of-the-art baseline results. We discuss the results in the framework of
the debate about FMs as alternatives to task-specific architectures. The code
is available at: [link to be added after the paper is accepted]

</details>


### [118] [Does the Skeleton-Recall Loss Really Work?](https://arxiv.org/abs/2508.11374)
*Devansh Arora,Nitin Kumar,Sukrit Gupta*

Main category: cs.CV

TL;DR: The paper critically evaluates the Skeleton Recall Loss (SRL), a topology-based loss function for segmenting thin tubular structures, finding its performance does not surpass traditional models.


<details>
  <summary>Details</summary>
Motivation: SRL Loss was claimed to achieve state-of-the-art performance for segmenting tubular structures, and the study aims to verify and critically analyze its effectiveness.

Method: The authors use theoretical gradient analysis and performance comparison of SRL against traditional baselines on both benchmark and additional datasets.

Result: They found that SRL-based models do not outperform traditional segmentation models, contradicting previous claims.

Conclusion: Through theoretical and empirical analysis, the paper identifies limitations in topology-based loss functions, guiding future improvements in segmentation models for tubular structures.

Abstract: Image segmentation is an important and widely performed task in computer
vision. Accomplishing effective image segmentation in diverse settings often
requires custom model architectures and loss functions. A set of models that
specialize in segmenting thin tubular structures are topology
preservation-based loss functions. These models often utilize a pixel
skeletonization process claimed to generate more precise segmentation masks of
thin tubes and better capture the structures that other models often miss. One
such model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\cite
{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark
tubular datasets. In this work, we performed a theoretical analysis of the
gradients for the SRL loss. Upon comparing the performance of the proposed
method on some of the tubular datasets (used in the original work, along with
some additional datasets), we found that the performance of SRL-based
segmentation models did not exceed traditional baseline models. By providing
both a theoretical explanation and empirical evidence, this work critically
evaluates the limitations of topology-based loss functions, offering valuable
insights for researchers aiming to develop more effective segmentation models
for complex tubular structures.

</details>


### [119] [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
*Oscar Mañas,Pierluca D'Oro,Koustuv Sinha,Adriana Romero-Soriano,Michal Drozdzal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: This paper introduces a new method for adapting Multimodal Large Language Models (MLLMs) through controlled reward-guided decoding to improve visual grounding.


<details>
  <summary>Details</summary>
Motivation: To address the need for adapting Multimodal Large Language Models (MLLMs) to diverse user needs and improve their visual grounding capabilities.

Method: The authors develop reward-guided decoding for MLLMs using two reward models to control object precision and recall independently. These models enable dynamic adjustments during inference for better image captioning and visual grounding.

Result: The method demonstrates significant controllability in MLLM inference and consistently outperforms existing hallucination mitigation methods when evaluated on standard benchmarks.

Conclusion: The research highlights the potential of reward-guided decoding to provide effective controllability for MLLMs, achieving enhanced performance and adaptability in tasks like image captioning.

Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

</details>


### [120] [Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition](https://arxiv.org/abs/2508.11376)
*Durgesh Mishra,Rishabh Uikey*

Main category: cs.CV

TL;DR: An advanced knowledge distillation framework for improving face recognition on resource-limited devices surpasses state-of-the-art methods and sometimes enables students to outperform their teachers.


<details>
  <summary>Details</summary>
Motivation: Optimize face recognition models for deployment on edge devices by addressing traditional KD methods' limitations in capturing fine-grained details and relational structures.

Method: Integrates two loss functions: Instance-Level Embedding Distillation with dynamic hard mining and Relation-Based Pairwise Similarity Distillation with a memory bank mechanism and sample mining.

Result: Achieves superior performance over existing distillation methods on benchmark datasets, with cases where the student surpasses the teacher's accuracy.

Conclusion: Combining instance alignment with relational geometric preservation leads to effective and comprehensive distillation, suitable for face recognition in constrained settings.

Abstract: Knowledge Distillation is crucial for optimizing face recognition models for
deployment in computationally limited settings, such as edge devices.
Traditional KD methods, such as Raw L2 Feature Distillation or Feature
Consistency loss, often fail to capture both fine-grained instance-level
details and complex relational structures, leading to suboptimal performance.
We propose a unified approach that integrates two novel loss functions,
Instance-Level Embedding Distillation and Relation-Based Pairwise Similarity
Distillation. Instance-Level Embedding Distillation focuses on aligning
individual feature embeddings by leveraging a dynamic hard mining strategy,
thereby enhancing learning from challenging examples. Relation-Based Pairwise
Similarity Distillation captures relational information through pairwise
similarity relationships, employing a memory bank mechanism and a sample mining
strategy. This unified framework ensures both effective instance-level
alignment and preservation of geometric relationships between samples, leading
to a more comprehensive distillation process. Our unified framework outperforms
state-of-the-art distillation methods across multiple benchmark face
recognition datasets, as demonstrated by extensive experimental evaluations.
Interestingly, when using strong teacher networks compared to the student, our
unified KD enables the student to even surpass the teacher's accuracy.

</details>


### [121] [G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration](https://arxiv.org/abs/2508.11379)
*Ramil Khafizov,Artem Komarichev,Ruslan Rakhimov,Peter Wonka,Evgeny Burnaev*

Main category: cs.CV

TL;DR: The paper introduces G-CUT3R, an enhanced 3D reconstruction model incorporating prior information like depth and camera calibrations to outperform conventional feed-forward methods.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward approaches for 3D scene reconstruction often underutilize auxiliary data, limiting their performance and adaptability to real-world scenarios.

Method: The authors propose a lightweight modification to the CUT3R model, adding dedicated encoders for each type of auxiliary data and fusing their features with RGB tokens through zero convolution, enabling flexible integration of multiple modalities.

Result: G-CUT3R achieves significant improvements in 3D reconstruction and multi-view tasks across diverse benchmarks by effectively leveraging auxiliary priors.

Conclusion: G-CUT3R offers a robust solution for guided 3D scene reconstruction, adaptable to various input modalities and enhanced through the use of prior information.

Abstract: We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene
reconstruction that enhances the CUT3R model by integrating prior information.
Unlike existing feed-forward methods that rely solely on input images, our
method leverages auxiliary data, such as depth, camera calibrations, or camera
positions, commonly available in real-world scenarios. We propose a lightweight
modification to CUT3R, incorporating a dedicated encoder for each modality to
extract features, which are fused with RGB image tokens via zero convolution.
This flexible design enables seamless integration of any combination of prior
information during inference. Evaluated across multiple benchmarks, including
3D reconstruction and other multi-view tasks, our approach demonstrates
significant performance improvements, showing its ability to effectively
utilize available priors while maintaining compatibility with varying input
modalities.

</details>


### [122] [RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator](https://arxiv.org/abs/2508.11409)
*Zhiming Liu,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: This paper introduces RMFAT, a lightweight recurrent framework for mitigating atmospheric turbulence effects in videos, achieving superior clarity, temporal consistency, and efficient real-time performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for handling atmospheric turbulence in videos are computationally heavy, limiting real-time deployment in resource-constrained scenarios.

Method: The proposed RMFAT framework uses a recurrent architecture with multi-scale feature encoding, decoding, and temporal warping modules, processing frames with only two inputs at a time to reduce computational complexity.

Result: RMFAT demonstrated better performance than previous methods, achieving nearly a 9% improvement in SSIM and reducing runtime by more than fourfold.

Conclusion: RMFAT is effective and efficient for video restoration under atmospheric turbulence, making it well-suited for real-time applications.

Abstract: Atmospheric turbulence severely degrades video quality by introducing
distortions such as geometric warping, blur, and temporal flickering, posing
significant challenges to both visual clarity and temporal consistency. Current
state-of-the-art methods are based on transformer and 3D architectures and
require multi-frame input, but their large computational cost and memory usage
limit real-time deployment, especially in resource-constrained scenarios. In
this work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric
Turbulence Mitigator, designed for efficient and temporally consistent video
restoration under AT conditions. RMFAT adopts a lightweight recurrent framework
that restores each frame using only two inputs at a time, significantly
reducing temporal window size and computational burden. It further integrates
multi-scale feature encoding and decoding with temporal warping modules at both
encoder and decoder stages to enhance spatial detail and temporal coherence.
Extensive experiments on synthetic and real-world atmospheric turbulence
datasets demonstrate that RMFAT not only outperforms existing methods in terms
of clarity restoration (with nearly a 9\% improvement in SSIM) but also
achieves significantly improved inference speed (more than a fourfold reduction
in runtime), making it particularly suitable for real-time atmospheric
turbulence suppression tasks.

</details>


### [123] [SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models](https://arxiv.org/abs/2508.11411)
*Fabian H. Reith,Jannik Franzen,Dinesh R. Palli,J. Lorenz Rumberger,Dagmar Kainmueller*

Main category: cs.CV

TL;DR: The paper introduces SelfAdapt, an unsupervised method for adapting cell segmentation models without requiring labels, showing significant performance improvement compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing models for biomedical instance segmentation like Cellpose face limitations when applied to domains different from their training data, and acquiring labeled data for supervised fine-tuning is challenging.

Method: SelfAdapt leverages student-teacher augmentation consistency training, L2-SP regularization, and label-free stopping criteria for unsupervised adaptation of pre-trained models.

Result: On LiveCell and TissueNet datasets, SelfAdapt showed improvements in AP0.5 of up to 29.64% compared to baseline Cellpose, and enhanced models that were already fine-tuned with supervision.

Conclusion: SelfAdapt demonstrates the potential of label-free adaptation methods, offering significant advancements in biomedical segmentation by improving domain generalization without the need for manual annotations.

Abstract: Deep neural networks have become the go-to method for biomedical instance
segmentation. Generalist models like Cellpose demonstrate state-of-the-art
performance across diverse cellular data, though their effectiveness often
degrades on domains that differ from their training data. While supervised
fine-tuning can address this limitation, it requires annotated data that may
not be readily available. We propose SelfAdapt, a method that enables the
adaptation of pre-trained cell segmentation models without the need for labels.
Our approach builds upon student-teacher augmentation consistency training,
introducing L2-SP regularization and label-free stopping criteria. We evaluate
our method on the LiveCell and TissueNet datasets, demonstrating relative
improvements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we
show that our unsupervised adaptation can further improve models that were
previously fine-tuned with supervision. We release SelfAdapt as an easy-to-use
extension of the Cellpose framework. The code for our method is publicly
available at https: //github.com/Kainmueller-Lab/self_adapt.

</details>


### [124] [Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems](https://arxiv.org/abs/2508.11419)
*Florian Bayer,Maximilian Russo,Christian Rathgeb*

Main category: cs.CV

TL;DR: This paper explores using dimensionality reduction and multi-modal fusion in biometric templates to lower Homomorphic Encryption computational costs while maintaining security and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the computational challenges of biometric template protection using Homomorphic Encryption and leverage multi-modal fusion to improve security and efficiency.

Method: The authors perform experiments on an in-house virtual multi-biometric database using DNN-extracted features from face, fingerprint, and iris modalities, employing dimensionality reduction and feature fusion schemes.

Result: Template size can be reduced by 67% without compromising biometric accuracy, achieving the same or better Equal Error Rate (EER) compared to single-biometric recognition.

Conclusion: Multi-biometric feature fusion and dimensionality reduction offer computational efficiency and improved template security without losing accuracy, supporting practical encrypted biometric systems.

Abstract: Biometric recognition is widely used, making the privacy and security of
extracted templates a critical concern. Biometric Template Protection schemes,
especially those utilizing Homomorphic Encryption, introduce significant
computational challenges due to increased workload. Recent advances in deep
neural networks have enabled state-of-the-art feature extraction for face,
fingerprint, and iris modalities. The ubiquity and affordability of biometric
sensors further facilitate multi-modal fusion, which can enhance security by
combining features from different modalities. This work investigates the
biometric performance of reduced multi-biometric template sizes. Experiments
are conducted on an in-house virtual multi-biometric database, derived from
DNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,
and CASIA databases. The evaluated approaches are (i) explainable and
straightforward to implement under encryption, (ii) training-free, and (iii)
capable of generalization. Dimensionality reduction of feature vectors leads to
fewer operations in the Homomorphic Encryption (HE) domain, enabling more
efficient encrypted processing while maintaining biometric accuracy and
security at a level equivalent to or exceeding single-biometric recognition.
Our results demonstrate that, by fusing feature vectors from multiple
modalities, template size can be reduced by 67 % with no loss in Equal Error
Rate (EER) compared to the best-performing single modality.

</details>


### [125] [ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving](https://arxiv.org/abs/2508.11428)
*Jingyu Li,Bozhou Zhang,Xin Jin,Jiankang Deng,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: The paper introduces ImagiDrive, an autonomous driving framework combining Vision-Language Models (VLMs) with Driving World Models (DWMs) to enhance contextual understanding and scenario planning capabilities.


<details>
  <summary>Details</summary>
Motivation: To improve autonomous driving systems by leveraging the complementary strengths of VLMs and DWMs for more accurate behavioral predictions and realistic scene generation, overcoming challenges in integration and computational efficiency.

Method: ImagiDrive integrates a VLM-driven driving agent with a DWM-based scene generator, forming an imagination-and-planning loop where imagined future scenarios refine planning decisions. Additionally, it employs efficiency enhancements like early stopping and trajectory selection.

Result: Experiments conducted on the nuScenes and NAVSIM datasets demonstrate that ImagiDrive is more robust and superior to existing alternatives in both open-loop and closed-loop evaluations.

Conclusion: ImagiDrive effectively addresses contextual comprehension and predictive reasoning challenges in autonomous driving, showcasing its potential benefits through extensive validation and innovative integration strategies.

Abstract: Autonomous driving requires rich contextual comprehension and precise
predictive reasoning to navigate dynamic and complex environments safely.
Vision-Language Models (VLMs) and Driving World Models (DWMs) have
independently emerged as powerful recipes addressing different aspects of this
challenge. VLMs provide interpretability and robust action prediction through
their ability to understand multi-modal context, while DWMs excel in generating
detailed and plausible future driving scenarios essential for proactive
planning. Integrating VLMs with DWMs is an intuitive, promising, yet
understudied strategy to exploit the complementary strengths of accurate
behavioral prediction and realistic scene generation. Nevertheless, this
integration presents notable challenges, particularly in effectively connecting
action-level decisions with high-fidelity pixel-level predictions and
maintaining computational efficiency. In this paper, we propose ImagiDrive, a
novel end-to-end autonomous driving framework that integrates a VLM-based
driving agent with a DWM-based scene imaginer to form a unified
imagination-and-planning loop. The driving agent predicts initial driving
trajectories based on multi-modal inputs, guiding the scene imaginer to
generate corresponding future scenarios. These imagined scenarios are
subsequently utilized to iteratively refine the driving agent's planning
decisions. To address efficiency and predictive accuracy challenges inherent in
this integration, we introduce an early stopping mechanism and a trajectory
selection strategy. Extensive experimental validation on the nuScenes and
NAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over
previous alternatives under both open-loop and closed-loop conditions.

</details>


### [126] [Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting](https://arxiv.org/abs/2508.11431)
*Simona Kocour,Assia Benbihi,Torsten Sattler*

Main category: cs.CV

TL;DR: This paper studies the unintended semantic traces left behind after object removal in 3D scene representations. It introduces a benchmark, dataset, and evaluation framework for semantic residuals.


<details>
  <summary>Details</summary>
Motivation: To understand and address the challenge of unintended semantic traces that persist after object removal, critical for privacy-preserving 3D reconstructions and editable scene representations.

Method: The researchers introduce a benchmark and the Remove360 dataset, conducting experiments on indoor and outdoor scenes to evaluate the persistence of semantic information post object removal using 3D Gaussian Splatting.

Result: Experiments reveal that current 3D object removal methods fail to eliminate semantic information entirely, allowing downstream models to infer removed objects.

Conclusion: Current 3D object removal methods have significant limitations, necessitating more robust solutions to ensure semantic information removal in real-world scenarios.

Abstract: Understanding what semantic information persists after object removal is
critical for privacy-preserving 3D reconstruction and editable scene
representations. In this work, we introduce a novel benchmark and evaluation
framework to measure semantic residuals, the unintended semantic traces left
behind, after object removal in 3D Gaussian Splatting. We conduct experiments
across a diverse set of indoor and outdoor scenes, showing that current methods
can preserve semantic information despite the absence of visual geometry. We
also release Remove360, a dataset of pre/post-removal RGB images and
object-level masks captured in real-world environments. While prior datasets
have focused on isolated object instances, Remove360 covers a broader and more
complex range of indoor and outdoor scenes, enabling evaluation of object
removal in the context of full-scene representations. Given ground truth images
of a scene before and after object removal, we assess whether we can truly
eliminate semantic presence, and if downstream models can still infer what was
removed. Our findings reveal critical limitations in current 3D object removal
techniques and underscore the need for more robust solutions capable of
handling real-world complexity. The evaluation framework is available at
github.com/spatial-intelligence-ai/Remove360.git. Data are available at
huggingface.co/datasets/simkoc/Remove360.

</details>


### [127] [MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation](https://arxiv.org/abs/2508.11433)
*Qian Liang,Yujia Wu,Kuncheng Li,Jiwei Wei,Shiyuan He,Jinyu Guo,Ning Xie*

Main category: cs.CV

TL;DR: This paper introduces MM-R1, a model enabling personalized image generation using a cross-modal Chain-of-Thought reasoning and requiring no fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address the scalability challenges and subject-specific requirements in existing MLLMs for personalized image generation.

Method: MM-R1 integrates cross-modal Chain-of-Thought reasoning and employs Grouped Reward Proximal Policy Optimization (GRPO) to align personalized image generation with extracted subject representations and user prompts.

Result: Experiments show MM-R1 generates personalized images with high subject accuracy and strong alignment to text-based prompts in a zero-shot setting.

Conclusion: MM-R1 demonstrates the potential for scalable and effective personalized image generation within unified multimodal models.

Abstract: Multimodal Large Language Models (MLLMs) with unified architectures excel
across a wide range of vision-language tasks, yet aligning them with
personalized image generation remains a significant challenge. Existing methods
for MLLMs are frequently subject-specific, demanding a data-intensive
fine-tuning process for every new subject, which limits their scalability. In
this paper, we introduce MM-R1, a framework that integrates a cross-modal
Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of
unified MLLMs for personalized image generation. Specifically, we structure
personalization as an integrated visual reasoning and generation process: (1)
grounding subject concepts by interpreting and understanding user-provided
images and contextual cues, and (2) generating personalized images conditioned
on both the extracted subject representations and user prompts. To further
enhance the reasoning capability, we adopt Grouped Reward Proximal Policy
Optimization (GRPO) to explicitly align the generation. Experiments demonstrate
that MM-R1 unleashes the personalization capability of unified MLLMs to
generate images with high subject fidelity and strong text alignment in a
zero-shot manner.

</details>


### [128] [Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation](https://arxiv.org/abs/2508.11446)
*Daniel Airinei,Elena Burceanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: The paper introduces a vision-based deep learning approach for real-time indoor navigation, avoiding reliance on GPS or special sensors.


<details>
  <summary>Details</summary>
Motivation: Current indoor navigation solutions face deployment challenges due to complex requirements and reliance on external sources such as GPS or sensors.

Method: The approach employs visual input, a novel graph-based path generation method, explainable data augmentation, and curriculum learning for efficient training and data handling.

Result: The introduced method successfully predicts navigation directions from visual data, and is validated on a novel large-scale annotated dataset collected in a shopping mall.

Conclusion: This visually-driven navigation solution is efficient, robust, and practical, enabling easy deployment via an Android app and offering open access to datasets and code.

Abstract: Indoor navigation is a difficult task, as it generally comes with poor GPS
access, forcing solutions to rely on other sources of information. While
significant progress continues to be made in this area, deployment to
production applications is still lacking, given the complexity and additional
requirements of current solutions. Here, we introduce an efficient, real-time
and easily deployable deep learning approach, based on visual input only, that
can predict the direction towards a target from images captured by a mobile
device. Our technical approach, based on a novel graph-based path generation
method, combined with explainable data augmentation and curriculum learning,
includes contributions that make the process of data collection, annotation and
training, as automatic as possible, efficient and robust. On the practical
side, we introduce a novel largescale dataset, with video footage inside a
relatively large shopping mall, in which each frame is annotated with the
correct next direction towards different specific target destinations.
Different from current methods, ours relies solely on vision, avoiding the need
of special sensors, additional markers placed along the path, knowledge of the
scene map or internet access. We also created an easy to use application for
Android, which we plan to make publicly available. We make all our data and
code available along with visual demos on our project site

</details>


### [129] [Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge](https://arxiv.org/abs/2508.11464)
*Xiaoya Zhu,Yibing Nan,Shiguo Lian*

Main category: cs.CV

TL;DR: This paper discusses using Swin Transformer V2-B with advanced data handling techniques to detect Deepfake images, achieving award-winning results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by Deepfake technology to digital security by improving methods to detect Deepfake images effectively.

Method: The method involves using the Swin Transformer V2-B classification network, combined with online data augmentation and offline sample generation, to enhance training sample diversity and model generalization.

Result: The approach earned the award of excellence in a Deepfake image detection competition, indicating strong performance.

Conclusion: The proposed strategy with Swin Transformer V2-B and data handling techniques proved effective in identifying Deepfakes and advancing digital security efforts.

Abstract: With the rapid development of technology in the field of AI, deepfake
technology has emerged as a double-edged sword. It has not only created a large
amount of AI-generated content but also posed unprecedented challenges to
digital security. The task of the competition is to determine whether a face
image is a Deepfake image and output its probability score of being a Deepfake
image. In the image track competition, our approach is based on the Swin
Transformer V2-B classification network. And online data augmentation and
offline sample generation methods are employed to enrich the diversity of
training samples and increase the generalization ability of the model. Finally,
we got the award of excellence in Deepfake image detection.

</details>


### [130] [CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation](https://arxiv.org/abs/2508.11469)
*Hongjin Fang,Daniel Reisenbüchler,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: This study introduces CoFi, a coarse-to-fine few-shot segmentation pipeline for glomerular basement membrane analysis in electron microscopy images. It requires minimal pixel annotation and achieves high segmentation performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately segmenting the glomerular basement membrane in electron microscopy images while reducing the burden of extensive pixel-level annotations.

Method: The CoFi pipeline trains a lightweight neural network using three annotated images for coarse segmentation. Then, it generates point prompts using morphology-aware pruning to guide SAM for a refined segmentation.

Result: The proposed method achieved a Dice coefficient of 74.54% for GBM segmentation and an inference speed of 1.9 FPS, demonstrating high accuracy and efficiency.

Conclusion: CoFi reduces annotation and computational demands while achieving reliable segmentation, making it suitable for research and potential clinical applications in renal pathology.

Abstract: Accurate segmentation of the glomerular basement membrane (GBM) in electron
microscopy (EM) images is fundamental for quantifying membrane thickness and
supporting the diagnosis of various kidney diseases. While supervised deep
learning approaches achieve high segmentation accuracy, their reliance on
extensive pixel-level annotation renders them impractical for clinical
workflows. Few-shot learning can reduce this annotation burden but often
struggles to capture the fine structural details necessary for GBM analysis. In
this study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot
segmentation pipeline designed for GBM delineation in EM images. CoFi first
trains a lightweight neural network using only three annotated images to
produce an initial coarse segmentation mask. This mask is then automatically
processed to generate high-quality point prompts with morphology-aware pruning,
which are subsequently used to guide SAM in refining the segmentation. The
proposed method achieved exceptional GBM segmentation performance, with a Dice
coefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that
CoFi not only alleviates the annotation and computational burdens associated
with conventional methods, but also achieves accurate and reliable segmentation
results. The pipeline's speed and annotation efficiency make it well-suited for
research and hold strong potential for clinical applications in renal
pathology. The pipeline is publicly available at:
https://github.com/ddrrnn123/CoFi.

</details>


### [131] [TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations](https://arxiv.org/abs/2508.11478)
*Xinyi Yin,Wenbo Yuan,Xuecheng Wu,Liangyu Fu,Danlei Huang*

Main category: cs.CV

TL;DR: The paper introduces TACR-YOLO, an enhanced YOLO-based framework for abnormal human behavior detection (AHBD), addressing issues like small object detection, classification-regression conflicts, and multi-scale fusion, achieving high performance on a new dataset.


<details>
  <summary>Details</summary>
Motivation: The increasing need for effective abnormal human behavior detection (AHBD) under specialized scenarios has been hindered by challenges like poor small object detection, task conflicts, and inefficient multi-scale fusion in existing YOLO-based methods.

Method: The paper presents TACR-YOLO, incorporating a Coordinate Attention Module, Task-Aware Attention Module, and Strengthen Neck Network. Optimization strategies like K-means based Anchor Box sizing and DIoU-loss are applied to enhance detection accuracy and robustness.

Result: TACR-YOLO achieves 91.92% mAP on the newly developed PABD dataset, demonstrating competitive speed, robustness, and effectiveness in abnormal behavior detection.

Conclusion: This research provides a high-performance, real-time framework for AHBD, overcoming key detection challenges and setting a precedent for future improvements in AHBD under special scenarios.

Abstract: Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming
increasingly crucial. While YOLO-based detection methods excel in real-time
tasks, they remain hindered by challenges including small objects, task
conflicts, and multi-scale fusion in AHBD. To tackle them, we propose
TACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate
Attention Module to enhance small object detection, a Task-Aware Attention
Module to deal with classification-regression conflicts, and a Strengthen Neck
Network for refined multi-scale fusion, respectively. In addition, we optimize
Anchor Box sizes using K-means clustering and deploy DIoU-Loss to improve
bounding box regression. The Personnel Anomalous Behavior Detection (PABD)
dataset, which includes 8,529 samples across four behavior categories, is also
presented. Extensive experimental results indicate that TACR-YOLO achieves
91.92% mAP on PABD, with competitive speed and robustness. Ablation studies
highlight the contribution of each improvement. This work provides new insights
for abnormal behavior detection under special scenarios, advancing its
progress.

</details>


### [132] [OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring](https://arxiv.org/abs/2508.11482)
*Ruoxin Xiong,Yanyu Wang,Jiannan Cai,Kaijian Liu,Yuansheng Zhu,Pingbo Tang,Nora El-Gohary*

Main category: cs.CV

TL;DR: This paper provides a systematic review of 51 publicly available visual datasets for AI applications in construction, categorizing them by size, modality, annotations, and applications to identify gaps and create the OpenConstruction catalog.


<details>
  <summary>Details</summary>
Motivation: Address the lack of systematic reviews on visual datasets for the construction industry, which limits the development of scalable and reliable AI applications.

Method: The authors conducted an extensive search of academic databases and open-data platforms for datasets (2005-2024), categorized them using a structured schema, and synthesized the findings into the OpenConstruction catalog.

Result: The study identified 51 diverse datasets and highlighted critical limitations in the dataset landscape, offering a roadmap for data infrastructure aligned with FAIR principles.

Conclusion: By categorizing datasets and identifying gaps, this study helps guide future research and development of data infrastructures to enable advanced AI-driven solutions in construction.

Abstract: The construction industry increasingly relies on visual data to support
Artificial Intelligence (AI) and Machine Learning (ML) applications for site
monitoring. High-quality, domain-specific datasets, comprising images, videos,
and point clouds, capture site geometry and spatiotemporal dynamics, including
the location and interaction of objects, workers, and materials. However,
despite growing interest in leveraging visual datasets, existing resources vary
widely in sizes, data modalities, annotation quality, and representativeness of
real-world construction conditions. A systematic review to categorize their
data characteristics and application contexts is still lacking, limiting the
community's ability to fully understand the dataset landscape, identify
critical gaps, and guide future directions toward more effective, reliable, and
scalable AI applications in construction. To address this gap, this study
conducts an extensive search of academic databases and open-data platforms,
yielding 51 publicly available visual datasets that span the 2005-2024 period.
These datasets are categorized using a structured data schema covering (i) data
fundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and
point cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)
downstream application domains (e.g., progress tracking). This study
synthesizes these findings into an open-source catalog, OpenConstruction,
supporting data-driven method development. Furthermore, the study discusses
several critical limitations in the existing construction dataset landscape and
presents a roadmap for future data infrastructure anchored in the Findability,
Accessibility, Interoperability, and Reusability (FAIR) principles. By
reviewing the current landscape and outlining strategic priorities, this study
supports the advancement of data-centric solutions in the construction sector.

</details>


### [133] [CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models](https://arxiv.org/abs/2508.11484)
*Xiaoxue Wu,Bingjie Gao,Yu Qiao,Yaohui Wang,Xinyuan Chen*

Main category: cs.CV

TL;DR: CineTrans introduces a method for generating coherent multi-shot videos with cinematic transitions using novel datasets and mask mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current video synthesis models struggle to produce stable transitions in multi-shot video generation, limiting outputs to single-shot sequences.

Method: The approach involves designing CineTrans, leveraging a new dataset (Cine250K) with shot annotations, and implementing a mask-based control mechanism to enable smooth transitions.

Result: CineTrans produces stable and cinematic multi-shot video sequences surpassing existing models, backed by improved evaluation metrics.

Conclusion: This framework marks a significant advancement in multi-shot video generation, enhancing coherence and adhering to film editing styles.

Abstract: Despite significant advances in video synthesis, research into multi-shot
video generation remains in its infancy. Even with scaled-up models and massive
datasets, the shot transition capabilities remain rudimentary and unstable,
largely confining generated videos to single-shot sequences. In this work, we
introduce CineTrans, a novel framework for generating coherent multi-shot
videos with cinematic, film-style transitions. To facilitate insights into the
film editing style, we construct a multi-shot video-text dataset Cine250K with
detailed shot annotations. Furthermore, our analysis of existing video
diffusion models uncovers a correspondence between attention maps in the
diffusion model and shot boundaries, which we leverage to design a mask-based
control mechanism that enables transitions at arbitrary positions and transfers
effectively in a training-free setting. After fine-tuning on our dataset with
the mask mechanism, CineTrans produces cinematic multi-shot sequences while
adhering to the film editing style, avoiding unstable transitions or naive
concatenations. Finally, we propose specialized evaluation metrics for
transition control, temporal consistency and overall quality, and demonstrate
through extensive experiments that CineTrans significantly outperforms existing
baselines across all criteria.

</details>


### [134] [Automated Building Heritage Assessment Using Street-Level Imagery](https://arxiv.org/abs/2508.11486)
*Kristina Dabrock,Tim Johansson,Anna Donarelli,Mikael Mangold,Noah Pflugradt,Jann Michael Weinand,Jochen Linßen*

Main category: cs.CV

TL;DR: The paper explores AI, specifically GPT, to identify and classify cultural heritage values in building façade images, achieving an F1-score of 0.71 when combining model features.


<details>
  <summary>Details</summary>
Motivation: The need for precise data to balance energy conservation in buildings without compromising cultural heritage, while overcoming inefficiencies of traditional methods.

Method: Large language model GPT was employed to extract cultural heritage value from façade images and used alongside building register data as inputs for machine learning models.

Result: The analysis achieved a validation F1-score of 0.71 using combined register data and GPT-derived features, and 0.60 using GPT data alone.

Conclusion: The approach can enhance databases, aiding large-scale energy efficiency upgrades while integrating heritage values effectively.

Abstract: Detailed data is required to quantify energy conservation measures in
buildings, such as envelop retrofits, without compromising cultural heritage.
Novel artificial intelligence tools may improve efficiency in identifying
heritage values in buildings compared to costly and time-consuming traditional
inventories. In this study, the large language model GPT was used to detect
various aspects of cultural heritage value in fa\c{c}ade images. Using this
data and building register data as features, machine learning models were
trained to classify multi-family and non-residential buildings in Stockholm,
Sweden. Validation against an expert-created inventory shows a macro F1-score
of 0.71 using a combination of register data and features retrieved from GPT,
and a score of 0.60 using only GPT-derived data. The presented methodology can
contribute to a higher-quality database and thus support careful energy
efficiency measures and integrated consideration of heritage value in
large-scale energetic refurbishment scenarios.

</details>


### [135] [Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.11488)
*Bozhou Zhang,Jingyu Li,Nan Song,Li Zhang*

Main category: cs.CV

TL;DR: The paper presents VeteranAD, an advanced coupled perception and planning framework for end-to-end autonomous driving that integrates perception into the planning process, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing autonomous driving methods by better integrating perception and planning to enhance driving accuracy and reliability.

Method: The framework introduces a perception-in-plan design using multi-mode anchored trajectories to guide perception, and implements an autoregressive strategy for progressive trajectory prediction and targeted perception.

Result: VeteranAD demonstrates superior performance and reliability in autonomous driving tasks compared to existing methods, validated through experiments on NAVSIM and Bench2Drive datasets.

Conclusion: Integrating perception with planning enables targeted and more effective driving behavior. VeteranAD sets a new benchmark for end-to-end autonomous driving frameworks.

Abstract: End-to-end autonomous driving has achieved remarkable advancements in recent
years. Existing methods primarily follow a perception-planning paradigm, where
perception and planning are executed sequentially within a fully differentiable
framework for planning-oriented optimization. We further advance this paradigm
through a perception-in-plan framework design, which integrates perception into
the planning process. This design facilitates targeted perception guided by
evolving planning objectives over time, ultimately enhancing planning
performance. Building on this insight, we introduce VeteranAD, a coupled
perception and planning framework for end-to-end autonomous driving. By
incorporating multi-mode anchored trajectories as planning priors, the
perception module is specifically designed to gather traffic elements along
these trajectories, enabling comprehensive and targeted perception. Planning
trajectories are then generated based on both the perception results and the
planning priors. To make perception fully serve planning, we adopt an
autoregressive strategy that progressively predicts future trajectories while
focusing on relevant regions for targeted perception at each step. With this
simple yet effective design, VeteranAD fully unleashes the potential of
planning-oriented end-to-end methods, leading to more accurate and reliable
driving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets
demonstrate that our VeteranAD achieves state-of-the-art performance.

</details>


### [136] [Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition](https://arxiv.org/abs/2508.11497)
*Feiyue Zhao,Zhichao Zhang*

Main category: cs.CV

TL;DR: The paper addresses the limitations of CNNs in modeling complex and non-local relationships in images by proposing the Hierarchical Graph Feature Enhancement (HGFE), which combines graph-based reasoning with CNNs.


<details>
  <summary>Details</summary>
Motivation: CNNs are limited by their reliance on regular grid structures, which hampers their ability to capture complex topological and semantic relationships in images.

Method: HGFE employs hierarchical graph structures: intra-window graph convolution for local dependencies and inter-window supernode interactions for global semantics. It also introduces adaptive frequency modulation to balance signal propagation while reducing over-smoothing.

Result: Experiments on benchmark datasets like CIFAR-100, PASCAL VOC, and specialized segmentation tasks demonstrate HGFE's effectiveness in improving structural representation and recognition performance.

Conclusion: HGFE is a lightweight, trainable module that enhances the capability of CNNs by integrating graph-based reasoning, showing promise in diverse visual recognition tasks.

Abstract: Convolutional neural networks (CNNs) have
  demonstrated strong performance in visual recognition tasks,
  but their inherent reliance on regular grid structures limits
  their capacity to model complex topological relationships and
  non-local semantics within images. To address this limita tion, we propose
the hierarchical graph feature enhancement
  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to
enhance both structural awareness and
  feature representation. HGFE builds two complementary levels
  of graph structures: intra-window graph convolution to cap ture local spatial
dependencies and inter-window supernode
  interactions to model global semantic relationships. Moreover,
  we introduce an adaptive frequency modulation module that
  dynamically balances low-frequency and high-frequency signal
  propagation, preserving critical edge and texture information
  while mitigating over-smoothing. The proposed HGFE module
  is lightweight, end-to-end trainable, and can be seamlessly
  integrated into standard CNN backbone networks. Extensive
  experiments on CIFAR-100 (classification), PASCAL VOC,
  and VisDrone (detection), as well as CrackSeg and CarParts
  (segmentation), validated the effectiveness of the HGFE in
  improving structural representation and enhancing overall
  recognition performance.

</details>


### [137] [Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models](https://arxiv.org/abs/2508.11499)
*Erez Meoded*

Main category: cs.CV

TL;DR: This study applies the TrOCR HTR model to 16th-century Latin manuscripts, introducing novel augmentations and achieving significant error rate reductions.


<details>
  <summary>Details</summary>
Motivation: To improve handwritten text recognition for historical manuscripts, which face challenges like scarce transcriptions, linguistic variation, and diverse handwriting.

Method: The study uses TrOCR, applies domain-specific image preprocessing, introduces four new data augmentation techniques, and evaluates ensemble learning approaches.

Result: Achieved a Character Error Rate (CER) of 1.86 with the best single model (Elastic) and 1.60 with a voting ensemble, marking substantial improvements over previous methods.

Conclusion: Domain-specific augmentations combined with ensemble strategies significantly enhance HTR performance for historical texts.

Abstract: Historical handwritten text recognition (HTR) is essential for unlocking the
cultural and scholarly value of archival documents, yet digitization is often
hindered by scarce transcriptions, linguistic variation, and highly diverse
handwriting styles. In this study, we apply TrOCR, a state-of-the-art
transformer-based HTR model, to 16th-century Latin manuscripts authored by
Rudolf Gwalther. We investigate targeted image preprocessing and a broad suite
of data augmentation techniques, introducing four novel augmentation methods
designed specifically for historical handwriting characteristics. We also
evaluate ensemble learning approaches to leverage the complementary strengths
of augmentation-trained models. On the Gwalther dataset, our best single-model
augmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a
top-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative
improvement over the best reported TrOCR_BASE result and a 42% improvement over
the previous state of the art. These results highlight the impact of
domain-specific augmentations and ensemble strategies in advancing HTR
performance for historical manuscripts.

</details>


### [138] [AIM: Amending Inherent Interpretability via Self-Supervised Masking](https://arxiv.org/abs/2508.11502)
*Eyad Alshami,Shashank Agnihotri,Bernt Schiele,Margret Keuper*

Main category: cs.CV

TL;DR: The paper proposes AIM, a method that improves model interpretability and accuracy by promoting the use of genuine features over spurious ones using self-supervised feature masking.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often rely on spurious features that compromise interpretability and generalization. The authors aim to address this issue and enhance models' faithfulness to meaningful features.

Method: AIM uses self-supervised feature masking guided by features across multiple encoding stages to favor genuine features in the decision process, without requiring additional annotations.

Result: AIM improves model accuracy and interpretability, as measured by the Energy Pointing Game (EPG) score, across diverse datasets, architectures, and classification tasks.

Conclusion: AIM presents a promising approach to build interpretable and high-performing models by inherently promoting the use of true features, enhancing both generalization and human-aligned interpretability.

Abstract: It has been observed that deep neural networks (DNNs) often use both genuine
as well as spurious features. In this work, we propose "Amending Inherent
Interpretability via Self-Supervised Masking" (AIM), a simple yet interestingly
effective method that promotes the network's utilization of genuine features
over spurious alternatives without requiring additional annotations. In
particular, AIM uses features at multiple encoding stages to guide a
self-supervised, sample-specific feature-masking process. As a result, AIM
enables the training of well-performing and inherently interpretable models
that faithfully summarize the decision process. We validate AIM across a
diverse range of challenging datasets that test both out-of-distribution
generalization and fine-grained visual understanding. These include
general-purpose classification benchmarks such as ImageNet100, HardImageNet,
and ImageWoof, as well as fine-grained classification datasets such as
Waterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual
benefits: interpretability improvements, as measured by the Energy Pointing
Game (EPG) score, and accuracy gains over strong baselines. These consistent
gains across domains and architectures provide compelling evidence that AIM
promotes the use of genuine and meaningful features that directly contribute to
improved generalization and human-aligned interpretability.

</details>


### [139] [A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11](https://arxiv.org/abs/2508.11517)
*Shaoze Huang,Qi Liu,Chao Chen,Yuhang Chen*

Main category: cs.CV

TL;DR: The paper develops YOLOv11-KW-TA-FP, a multi-task model for concrete crack detection, achieving high precision and robustness under challenging environments.


<details>
  <summary>Details</summary>
Motivation: Aging transportation infrastructure in the Yangtze River Delta region creates an urgent need for efficient concrete crack detection to ensure structural integrity and economic stability.

Method: The model YOLOv11-KW-TA-FP employs a three-step optimization: KWConv for feature enhancement, triple attention for better spatial-channel interaction, and FP-IoU for improved bounding box regression.

Result: The enhanced model achieved 91.3% precision, 76.6% recall, and 86.4% mAP@50 in experiments, with strong performance even under scarce or noisy data.

Conclusion: The research provides an effective and robust computer vision model for automated infrastructure inspections, with significant engineering value.

Abstract: Accelerated aging of transportation infrastructure in the rapidly developing
Yangtze River Delta region necessitates efficient concrete crack detection, as
crack deterioration critically compromises structural integrity and regional
economic growth. To overcome the limitations of inefficient manual inspection
and the suboptimal performance of existing deep learning models, particularly
for small-target crack detection within complex backgrounds, this paper
proposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and
segmentation model based on the YOLOv11n architecture. The proposed model
integrates a three-stage optimization framework: (1) Embedding dynamic
KernelWarehouse convolution (KWConv) within the backbone network to enhance
feature representation through a dynamic kernel sharing mechanism; (2)
Incorporating a triple attention mechanism (TA) into the feature pyramid to
strengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU
loss function to facilitate adaptive bounding box regression penalization.
Experimental validation demonstrates that the enhanced model achieves
significant performance improvements over the baseline, attaining 91.3%
precision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the
synergistic efficacy of the proposed modules. Furthermore, robustness tests
indicate stable performance under conditions of data scarcity and noise
interference. This research delivers an efficient computer vision solution for
automated infrastructure inspection, exhibiting substantial practical
engineering value.

</details>


### [140] [Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction](https://arxiv.org/abs/2508.11531)
*Shilei Wang,Gong Cheng,Pujian Lai,Dong Gao,Junwei Han*

Main category: cs.CV

TL;DR: The paper introduces Multi-State Tracker (MST), a lightweight tracking model combining state-specific enhancements and multi-state interactions to boost feature representation and tracking robustness with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the trade-off in efficient trackers between reduced computational complexity and weakened feature representation capacity, limiting their tracking accuracy in complex environments.

Method: MST employs Multi-State Generation (MSG) for feature extraction, State-Specific Enhancement (SSE) for highlighting target-specific features, and Cross-State Interaction (CSI) for adaptive feature integration, using a lightweight hidden state adaptation-based state space duality (HSA-SSD) design.

Result: Experiments show MST surpasses previous efficient trackers in accuracy, robustness, and runtime, with a notable AO score improvement of 4.5% over HCAT on GOT-10K while maintaining computational efficiency.

Conclusion: MST significantly improves tracking performance and robustness, demonstrating the potential of state-specific and adaptive feature integration with minimal computational cost.

Abstract: Efficient trackers achieve faster runtime by reducing computational
complexity and model parameters. However, this efficiency often compromises the
expense of weakened feature representation capacity, thus limiting their
ability to accurately capture target states using single-layer features. To
overcome this limitation, we propose Multi-State Tracker (MST), which utilizes
highly lightweight state-specific enhancement (SSE) to perform specialized
enhancement on multi-state features produced by multi-state generation (MSG)
and aggregates them in an interactive and adaptive manner using cross-state
interaction (CSI). This design greatly enhances feature representation while
incurring minimal computational overhead, leading to improved tracking
robustness in complex environments. Specifically, the MSG generates multiple
state representations at multiple stages during feature extraction, while SSE
refines them to highlight target-specific features. The CSI module facilitates
information exchange between these states and ensures the integration of
complementary features. Notably, the introduced SSE and CSI modules adopt a
highly lightweight hidden state adaptation-based state space duality (HSA-SSD)
design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.
Experimental results demonstrate that MST outperforms all previous efficient
trackers across multiple datasets, significantly improving tracking accuracy
and robustness. In particular, it shows excellent runtime performance, with an
AO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT on
the GOT-10K dataset. The code is available at https://github.com/wsumel/MST.

</details>


### [141] [An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture](https://arxiv.org/abs/2508.11532)
*Jingsong Xia,Yue Yin,Xiuhan Li*

Main category: cs.CV

TL;DR: This study proposes an efficient medical image classification method using an improved ConvNeXt-Tiny architecture for high accuracy in resource-limited environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving accurate medical image classification in computationally constrained settings for clinical diagnosis.

Method: The method improves the ConvNeXt-Tiny architecture by adding dual global pooling, a lightweight channel attention module (SEVector), and incorporating feature smoothing into the loss function.

Result: Achieved a classification accuracy of 89.10% in a CPU-only environment (8 threads) with stable convergence within 10 training epochs.

Conclusion: The proposed method demonstrates improved medical image classification performance and offers a viable solution for resource-constrained computational environments.

Abstract: Intelligent analysis of medical imaging plays a crucial role in assisting
clinical diagnosis. However, achieving efficient and high-accuracy image
classification in resource-constrained computational environments remains
challenging. This study proposes a medical image classification method based on
an improved ConvNeXt-Tiny architecture. Through structural optimization and
loss function design, the proposed method enhances feature extraction
capability and classification performance while reducing computational
complexity. Specifically, the method introduces a dual global pooling (Global
Average Pooling and Global Max Pooling) feature fusion strategy into the
ConvNeXt-Tiny backbone to simultaneously preserve global statistical features
and salient response information. A lightweight channel attention module,
termed Squeeze-and-Excitation Vector (SEVector), is designed to improve the
adaptive allocation of channel weights while minimizing parameter overhead.
Additionally, a Feature Smoothing Loss is incorporated into the loss function
to enhance intra-class feature consistency and suppress intra-class variance.
Under CPU-only conditions (8 threads), the method achieves a maximum
classification accuracy of 89.10% on the test set within 10 training epochs,
exhibiting a stable convergence trend in loss values. Experimental results
demonstrate that the proposed method effectively improves medical image
classification performance in resource-limited settings, providing a feasible
and efficient solution for the deployment and promotion of medical imaging
analysis models.

</details>


### [142] [Reinforcing Video Reasoning Segmentation to Think Before It Segments](https://arxiv.org/abs/2508.11538)
*Sitong Gong,Lu Zhang,Yunzhi Zhuge,Xu Jia,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: The paper introduces Veason-R1, a video reasoning segmentation (VRS) model that outperforms previous methods by leveraging structured reasoning and specialized training strategies like Group Relative Policy Optimization (GRPO) and Chain-of-Thought (CoT) initialization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of interpretability and performance in video reasoning segmentation (VRS) tasks, which are hindered by insufficient spatiotemporal reasoning and ineffective prior approaches that rely on large vision language models (LVLMs).

Method: Veason-R1 uses a two-stage training approach: supervised fine-tuning (Veason-SFT) with curated Chain-of-Thought (CoT) data for structured reasoning, followed by optimization with Group Relative Policy Optimization (GRPO) incorporating a holistic reward system to enhance spatial and temporal reasoning.

Result: Veason-R1 achieves state-of-the-art performance on various VRS benchmarks, demonstrating significant improvements such as +1.3 J&F on ReVOS and +10.0 J&F on ReasonVOS, while also showing robustness against hallucination errors (+8.8 R).

Conclusion: The proposed Veason-R1 model successfully integrates structured reasoning and innovative training techniques to advance the state-of-the-art in VRS, offering both superior performance and interpretability in video segmentation tasks.

Abstract: Video reasoning segmentation (VRS) endeavors to delineate referred objects in
videos guided by implicit instructions that encapsulate human intent and
temporal logic. Previous approaches leverage large vision language models
(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.
However, this paradigm suffers from limited interpretability during inference
and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing
inspiration from seminal breakthroughs in reinforcement learning, we introduce
Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in
segmentation. Veason-R1 is trained through Group Relative Policy Optimization
(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we
curate high-quality CoT training data to instill structured reasoning
trajectories, bridging video-level semantics and frame-level spatial grounding,
yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO
fine-tuning encourages efficient exploration of the reasoning space by
optimizing reasoning chains. To this end, we incorporate a holistic reward
mechanism that synergistically enhances spatial alignment and temporal
consistency, bolstering keyframe localization and fine-grained grounding.
Comprehensive empirical evaluations demonstrate that Veason-R1 achieves
state-of-the-art performance on multiple benchmarks, surpassing prior art by
significant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),
while exhibiting robustness to hallucinations (+8.8 R). Our code and model
weights will be available at Veason-R1.

</details>


### [143] [Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model](https://arxiv.org/abs/2508.11550)
*Zuo Zuo,Jiahao Dong,Yanyun Qu,Zongze Wu*

Main category: cs.CV

TL;DR: The paper introduces AAG, a training-free anomaly generation framework using Stable Diffusion to produce realistic anomalies in specific image regions, addressing data scarcity issues in industrial anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of data scarcity in industrial anomaly detection by developing a method for generating realistic anomaly data without additional training or extra data.

Method: Leverages Stable Diffusion with proposed Cross-Attention Enhancement (CAE) and Self-Attention Enhancement (SAE) techniques to refine anomaly generation in specified image regions while maintaining realism and fidelity.

Result: Demonstrated the effectiveness of AAG in generating realistic anomalies through experiments on MVTec AD and VisA datasets. The generated anomalies were also shown to enhance performance in downstream anomaly inspection tasks.

Conclusion: AAG provides an efficient, training-free approach to anomaly generation that effectively supports industrial anomaly detection by improving data diversity and aiding downstream tasks.

Abstract: Industrial anomaly detection (AD) plays a significant role in manufacturing
where a long-standing challenge is data scarcity. A growing body of works have
emerged to address insufficient anomaly data via anomaly generation. However,
these anomaly generation methods suffer from lack of fidelity or need to be
trained with extra data. To this end, we propose a training-free anomaly
generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s
strong generation ability for effective anomaly image generation. Given a
normal image, mask and a simple text prompt, AAG can generate realistic and
natural anomalies in the specific regions and simultaneously keep contents in
other regions unchanged. In particular, we propose Cross-Attention Enhancement
(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion
based on the given mask. CAE increases the similarity between visual tokens in
specific regions and text embeddings, which guides these generated visual
tokens in accordance with the text description. Besides, generated anomalies
need to be more natural and plausible with object in given image. We propose
Self-Attention Enhancement (SAE) which improves similarity between each normal
visual token and anomaly visual tokens. SAE ensures that generated anomalies
are coherent with original pattern. Extensive experiments on MVTec AD and VisA
datasets demonstrate effectiveness of AAG in anomaly generation and its
utility. Furthermore, anomaly images generated by AAG can bolster performance
of various downstream anomaly inspection tasks.

</details>


### [144] [TrajSV: A Trajectory-based Model for Sports Video Representations and Applications](https://arxiv.org/abs/2508.11569)
*Zheng Wang,Shihao Xu,Wei Shi*

Main category: cs.CV

TL;DR: This paper presents TrajSV, a trajectory-based framework to analyze sports videos, showing state-of-the-art results across multiple applications like video retrieval, action spotting, and video captioning.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from unresolved issues in sports analytics, such as data unavailability, lack of effective trajectory-based frameworks, and need for sufficient supervision labels.

Method: The proposed framework, TrajSV, includes data preprocessing to extract trajectories from videos, a Clip Representation Network (CRNet) enhanced by trajectory data, a Video Representation Network (VRNet) for aggregating representations, and a triple contrastive loss for self-supervised learning.

Result: Experiments on three datasets covering soccer, basketball, and volleyball demonstrate state-of-the-art results in video retrieval (70% improvement), superior action spotting in 9 of 17 categories, and nearly 20% improvement in video captioning.

Conclusion: TrajSV effectively tackles key challenges in sports analytics, leading to remarkable improvements in various applications, and has been implemented as a deployable system.

Abstract: Sports analytics has received significant attention from both academia and
industry in recent years. Despite the growing interest and efforts in this
field, several issues remain unresolved, including (1) data unavailability, (2)
lack of an effective trajectory-based framework, and (3) requirement for
sufficient supervision labels. In this paper, we present TrajSV, a
trajectory-based framework that addresses various issues in existing studies.
TrajSV comprises three components: data preprocessing, Clip Representation
Network (CRNet), and Video Representation Network (VRNet). The data
preprocessing module extracts player and ball trajectories from sports
broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to
learn clip representations based on these trajectories. Additionally, VRNet
learns video representations by aggregating clip representations and visual
features with an encoder-decoder architecture. Finally, a triple contrastive
loss is introduced to optimize both video and clip representations in an
unsupervised manner. The experiments are conducted on three broadcast video
datasets to verify the effectiveness of TrajSV for three types of sports (i.e.,
soccer, basketball, and volleyball) with three downstream applications (i.e.,
sports video retrieval, action spotting, and video captioning). The results
demonstrate that TrajSV achieves state-of-the-art performance in sports video
retrieval, showcasing a nearly 70% improvement. It outperforms baselines in
action spotting, achieving state-of-the-art results in 9 out of 17 action
categories, and demonstrates a nearly 20% improvement in video captioning.
Additionally, we introduce a deployed system along with the three applications
based on TrajSV.

</details>


### [145] [Causality Matters: How Temporal Information Emerges in Video Language Models](https://arxiv.org/abs/2508.11576)
*Yumeng Shi,Quanyu Long,Yin Wu,Wenya Wang*

Main category: cs.CV

TL;DR: This paper investigates temporal understanding in Video Language Models (VideoLMs) and finds that removing positional encodings has minimal effect on performance, while reversed frame sequences degrade understanding. Temporal cues are synthesized through inter-frame attention. The proposed strategies improve model efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Temporal understanding, which involves decoding relationships across time in video, remains a core challenge for VideoLMs despite their advancements in multimodal understanding.

Method: The paper conducts analysis experiments to trace temporal information integration within the model, revealing a mechanism where temporal cues are synthesized through inter-frame attention and aggregated in query tokens. Two strategies (staged cross-modal attention and temporal exit mechanisms) are proposed for efficiency.

Result: The proposed strategies are validated with experiments on two benchmarks, demonstrating improved efficiency and effectiveness in temporal understanding.

Conclusion: This work systematically explores temporal understanding in VideoLMs, revealing key temporal reasoning mechanisms and proposing strategies for model improvement.

Abstract: Video language models (VideoLMs) have made significant progress in multimodal
understanding. However, temporal understanding, which involves identifying
event order, duration, and relationships across time, still remains a core
challenge. Prior works emphasize positional encodings (PEs) as a key mechanism
for encoding temporal structure. Surprisingly, we find that removing or
modifying PEs in video inputs yields minimal degradation in the performance of
temporal understanding. In contrast, reversing the frame sequence while
preserving the original PEs causes a substantial drop. To explain this
behavior, we conduct substantial analysis experiments to trace how temporal
information is integrated within the model. We uncover a causal information
pathway: temporal cues are progressively synthesized through inter-frame
attention, aggregated in the final frame, and subsequently integrated into the
query tokens. This emergent mechanism shows that temporal reasoning emerges
from inter-visual token interactions under the constraints of causal attention,
which implicitly encodes temporal structure. Based on these insights, we
propose two efficiency-oriented strategies: staged cross-modal attention and a
temporal exit mechanism for early token truncation. Experiments on two
benchmarks validate the effectiveness of both approaches. To the best of our
knowledge, this is the first work to systematically investigate video temporal
understanding in VideoLMs, offering insights for future model improvement.

</details>


### [146] [DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring](https://arxiv.org/abs/2508.11591)
*Durga Joshi,Chandi Witharana,Robert Fahey,Thomas Worthley,Zhe Zhu,Diego Cerrai*

Main category: cs.CV

TL;DR: This research presents a framework using dashcam videos for real-time monitoring and geolocation of urban roadside vegetation and infrastructure by leveraging monocular depth estimation and GPS triangulation for spatial accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for a cost-effective and scalable solution for structural monitoring of urban roadside vegetation and infrastructure, especially relevant for utility companies and urban planners.

Method: The proposed method integrates monocular depth estimation, depth error correction using gradient-boosted regression, and GPS-based geometric triangulation. It measures object heights using pinhole camera geometry and evaluates data under varied vehicle and camera conditions.

Result: Strong predictive depth model performance (R2 = 0.92, MAE transformed scale = 0.31), accurate geolocation error of 2.83 m, and reliable height estimation errors of 2.09 m (trees) and 0.88 m (poles) in optimal conditions.

Conclusion: The framework offers a novel, real-time, and cost-effective monitoring solution that complements traditional Remote Sensing methods, making it viable for extensive, frequent assessments of dynamic urban environments.

Abstract: Our study introduces a novel, low-cost, and reproducible framework for
real-time, object-level structural assessment and geolocation of roadside
vegetation and infrastructure with commonly available but underutilized
dashboard camera (dashcam) video data. We developed an end-to-end pipeline that
combines monocular depth estimation, depth error correction, and geometric
triangulation to generate accurate spatial and structural data from
street-level video streams from vehicle-mounted dashcams. Depth maps were first
estimated using a state-of-the-art monocular depth model, then refined via a
gradient-boosted regression framework to correct underestimations, particularly
for distant objects. The depth correction model achieved strong predictive
performance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly
reducing bias beyond 15 m. Further, object locations were estimated using
GPS-based triangulation, while object heights were calculated using pin hole
camera geometry. Our method was evaluated under varying conditions of camera
placement and vehicle speed. Low-speed vehicle with inside camera gave the
highest accuracy, with mean geolocation error of 2.83 m, and mean absolute
error (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To
the best of our knowledge, it is the first framework to combine monocular depth
modeling, triangulated GPS-based geolocation, and real-time structural
assessment for urban vegetation and infrastructure using consumer-grade video
data. Our approach complements conventional RS methods, such as LiDAR and image
by offering a fast, real-time, and cost-effective solution for object-level
monitoring of vegetation risks and infrastructure exposure, making it
especially valuable for utility companies, and urban planners aiming for
scalable and frequent assessments in dynamic urban environments.

</details>


### [147] [CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion](https://arxiv.org/abs/2508.11603)
*Zhe Zhu,Honghua Chen,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: The paper introduces CoreEditor, a novel framework for text-driven 3D editing that enhances multi-view consistency and detail quality.


<details>
  <summary>Details</summary>
Motivation: Existing text-driven approaches for 3D editing struggle to maintain cross-view consistency, resulting in suboptimal edits with blurry details.

Method: CoreEditor employs a correspondence-constrained attention mechanism combined with geometric alignment and semantic similarity during denoising for reliable multi-view editing. It also includes a selective editing pipeline for user-controlled outcomes.

Result: CoreEditor achieves sharper, high-quality 3D edits with consistent details across multiple views, significantly surpassing previous methods.

Conclusion: CoreEditor demonstrates the importance of multi-view consistency and user control in advancing text-to-3D editing, establishing a new benchmark in quality and flexibility.

Abstract: Text-driven 3D editing seeks to modify 3D scenes according to textual
descriptions, and most existing approaches tackle this by adapting pre-trained
2D image editors to multi-view inputs. However, without explicit control over
multi-view information exchange, they often fail to maintain cross-view
consistency, leading to insufficient edits and blurry details. We introduce
CoreEditor, a novel framework for consistent text-to-3D editing. The key
innovation is a correspondence-constrained attention mechanism that enforces
precise interactions between pixels expected to remain consistent throughout
the diffusion denoising process. Beyond relying solely on geometric alignment,
we further incorporate semantic similarity estimated during denoising, enabling
more reliable correspondence modeling and robust multi-view editing. In
addition, we design a selective editing pipeline that allows users to choose
preferred results from multiple candidates, offering greater flexibility and
user control. Extensive experiments show that CoreEditor produces high-quality,
3D-consistent edits with sharper details, significantly outperforming prior
methods.

</details>


### [148] [LoRAtorio: An intrinsic approach to LoRA Skill Composition](https://arxiv.org/abs/2508.11624)
*Niki Foteinopoulou,Ignas Budvytis,Stephan Liwicki*

Main category: cs.CV

TL;DR: LoRAtorio is a new framework for integrating multiple LoRA adapters without retraining, showing improved performance in text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Existing Low-Rank Adaptation (LoRA) approaches struggle with the composition of multiple adapters, especially when the number and type of adapters are unknown in advance.

Method: LoRAtorio computes cosine similarity in latent space to dynamically merge multiple LoRA adapters' outputs and incorporates modifications in classifier-free guidance to address domain drift.

Result: LoRAtorio achieves up to a 1.3% improvement in ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, demonstrating state-of-the-art performance across multiple latent diffusion models.

Conclusion: LoRAtorio effectively integrates multiple LoRA adapters and generalizes well, advancing the personalization capabilities in text-to-image diffusion models.

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted technique in
text-to-image diffusion models, enabling the personalisation of visual concepts
such as characters, styles, and objects. However, existing approaches struggle
to effectively compose multiple LoRA adapters, particularly in open-ended
settings where the number and nature of required skills are not known in
advance. In this work, we present LoRAtorio, a novel train-free framework for
multi-LoRA composition that leverages intrinsic model behaviour. Our method is
motivated by two key observations: (1) LoRA adapters trained on narrow domains
produce denoised outputs that diverge from the base model, and (2) when
operating out-of-distribution, LoRA outputs show behaviour closer to the base
model than when conditioned in distribution. The balance between these two
observations allows for exceptional performance in the single LoRA scenario,
which nevertheless deteriorates when multiple LoRAs are loaded. Our method
operates in the latent space by dividing it into spatial patches and computing
cosine similarity between each patch's predicted noise and that of the base
model. These similarities are used to construct a spatially-aware weight
matrix, which guides a weighted aggregation of LoRA outputs. To address domain
drift, we further propose a modification to classifier-free guidance that
incorporates the base model's unconditional score into the composition. We
extend this formulation to a dynamic module selection setting, enabling
inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio
achieves state-of-the-art performance, showing up to a 1.3% improvement in
ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises
effectively to multiple latent diffusion models.

</details>


### [149] [Is ChatGPT-5 Ready for Mammogram VQA?](https://arxiv.org/abs/2508.11628)
*Qiang Li,Shansong Wang,Mingzhe Hu,Mojtaba Safari,Zachary Eidex,Xiaofeng Yang*

Main category: cs.CV

TL;DR: This paper evaluates GPT-5 and GPT-4o on mammogram visual question answering tasks using four public datasets. GPT-5 performs better than GPT-4o but lags behind human experts and fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: To explore the ability of general large language models, specifically GPT-5 and GPT-4o, to perform mammogram visual question answering, aiming to integrate image interpretation with clinical reasoning for breast cancer screening.

Method: Systematic evaluation of GPT-5 and GPT-4o on four datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for tasks like BI-RADS assessment, abnormality detection, and malignancy classification, comparing their performance to human experts and domain-specific models.

Result: GPT-5 consistently outperforms GPT-4o across datasets and tasks but still underperforms compared to human experts and fine-tuned models. For example, on CBIS-DDSM, it achieved 69.3% BI-RADS accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy.

Conclusion: While GPT-5 shows potential for screening tasks and exhibits significant improvement over GPT-4o, it currently lacks reliability for high-stakes clinical applications without further domain-specific optimization.

Abstract: Mammogram visual question answering (VQA) integrates image interpretation
with clinical reasoning and has potential to support breast cancer screening.
We systematically evaluated the GPT-5 family and GPT-4o model on four public
mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,
abnormality detection, and malignancy classification tasks. GPT-5 consistently
was the best performing model but lagged behind both human experts and
domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores
among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),
calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it
attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%
malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection
and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS
accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared
with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and
specificity (52.3%). While GPT-5 exhibits promising capabilities for screening
tasks, its performance remains insufficient for high-stakes clinical imaging
applications without targeted domain adaptation and optimization. However, the
tremendous improvements in performance from GPT-4o to GPT-5 show a promising
trend in the potential for general large language models (LLMs) to assist with
mammography VQA tasks.

</details>


### [150] [Thyme: Think Beyond Images](https://arxiv.org/abs/2508.11630)
*Yi-Fan Zhang,Xingyu Lu,Shukang Yin,Chaoyou Fu,Wei Chen,Xiao Hu,Bin Wen,Kaiyu Jiang,Changyi Liu,Tianke Zhang,Haonan Fan,Kaibing Chen,Jiankang Chen,Haojie Ding,Kaiyu Tang,Zhang Zhang,Liang Wang,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.CV

TL;DR: Thyme proposes a paradigm to enhance multimodal language models (MLLMs) by enabling them to autonomously generate and execute diverse image manipulations and computations using executable code.


<details>
  <summary>Details</summary>
Motivation: Existing models lack an open-source solution combining rich image manipulation capabilities with enhanced logical reasoning. OpenAI's concept of "thinking with images" inspired this work.

Method: Thyme employs a novel two-stage training approach: first, supervised fine-tuning with 500K samples to teach code generation, and then reinforcement learning with high-resolution question-answer pairs using the proposed GRPO-ATS algorithm.

Result: Extensive experiments across 20 benchmarks show Thyme outperforms in high-resolution perception and complex reasoning tasks, achieving consistent and significant gains.

Conclusion: Thyme demonstrates the feasibility and effectiveness of integrating autonomous manipulation of images and computation into reasoning models through enhanced training strategies.

Abstract: Following OpenAI's introduction of the ``thinking with images'' concept,
recent efforts have explored stimulating the use of visual information in the
reasoning process to enhance model performance in perception and reasoning
tasks. However, to the best of our knowledge, no open-source work currently
offers a feature set as rich as proprietary models (O3), which can perform
diverse image manipulations and simultaneously enhance logical reasoning
capabilities through code. In this paper, we make a preliminary attempt in this
direction by introducing Thyme (Think Beyond Images), a novel paradigm for
enabling MLLMs to transcend existing ``think with images'' approaches by
autonomously generating and executing diverse image processing and
computational operations via executable code. This approach not only
facilitates a rich, on-the-fly set of image manipulations (e.g., cropping,
rotation, contrast enhancement) but also allows for mathematical computations,
all while maintaining high autonomy in deciding when and how to apply these
operations. We activate this capability through a two-stage training strategy:
an initial SFT on a curated dataset of 500K samples to teach code generation,
followed by a RL phase to refine decision-making. For the RL stage, we manually
collect and design high-resolution question-answer pairs to increase the
learning difficulty, and we propose GRPO-ATS (Group Relative Policy
Optimization with Adaptive Temperature Sampling), an algorithm that applies
distinct temperatures to text and code generation to balance reasoning
exploration with code execution precision. We conduct extensive experimental
analysis and ablation studies. Comprehensive evaluations on nearly 20
benchmarks show that Thyme yields significant and consistent performance gains,
particularly in challenging high-resolution perception and complex reasoning
tasks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [151] [EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training](https://arxiv.org/abs/2508.11035)
*Hasibul Jamil,MD S Q Zulkar Nine,Tevfik Kosar*

Main category: cs.DC

TL;DR: The paper introduces EMLIO, a service optimizing data-loading latency and I/O energy consumption in large-scale machine learning workloads, achieving better performance and energy efficiency than current systems.


<details>
  <summary>Details</summary>
Motivation: To address the growing I/O bottlenecks in large-scale deep learning workloads caused by increasing dataset sizes and network/disk latencies, while accounting for the significant energy costs at scale.

Method: The authors propose EMLIO, which integrates a lightweight data-serving daemon on storage nodes to batch, serialize, and prefetch data, stream it over TCP, and utilize GPU-accelerated preprocessing (NVIDIA DALI) on client-side nodes.

Result: EMLIO achieves up to 8.6X faster I/O speeds and 10.9X lower energy consumption compared to existing approaches, maintaining consistent performance regardless of network distance across local, LAN, and WAN environments.

Conclusion: EMLIO provides an efficient and scalable I/O solution for machine learning workloads, optimizing both performance and energy consumption. It offers a framework that can support energy-aware data handling in AI cloud environments.

Abstract: Large-scale deep learning workloads increasingly suffer from I/O bottlenecks
as datasets grow beyond local storage capacities and GPU compute outpaces
network and disk latencies. While recent systems optimize data-loading time,
they overlook the energy cost of I/O - a critical factor at large scale. We
introduce EMLIO, an Efficient Machine Learning I/O service that jointly
minimizes end-to-end data-loading latency T and I/O energy consumption E across
variable-latency networked storage. EMLIO deploys a lightweight data-serving
daemon on storage nodes that serializes and batches raw samples, streams them
over TCP with out-of-order prefetching, and integrates seamlessly with
GPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive
evaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)
environments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use
compared to state-of-the-art loaders, while maintaining constant performance
and energy profiles irrespective of network distance. EMLIO's service-based
architecture offers a scalable blueprint for energy-aware I/O in
next-generation AI clouds.

</details>


### [152] [Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets](https://arxiv.org/abs/2508.11266)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.DC

TL;DR: The paper introduces a two-tier tokenization system (Element Tokens and Everything Tokens) to improve liquidity and transparency of complex assets.


<details>
  <summary>Details</summary>
Motivation: Many alternative assets are illiquid and difficult to fractionalize under traditional frameworks, necessitating new methods for better trading and ownership.

Method: The proposed architecture uses Element Tokens to represent asset components and Everything Tokens to represent the whole asset, enabling fractional ownership and full ownership through two-way convertibility and an arbitrage mechanism.

Result: Illustrative examples show that the approach facilitates the fractionalization and trading of previously illiquid high-value projects, akin to stocks or ETFs.

Conclusion: This system offers benefits like lower entry barriers, better price discovery, and flexible financing, while requiring attention to implementation and regulatory challenges.

Abstract: Alternative assets such as mines, power plants, or infrastructure projects
are often large, heterogeneous bundles of resources, rights, and outputs whose
value is difficult to trade or fractionalize under traditional frameworks. This
paper proposes a novel two-tier tokenization architecture to enhance the
liquidity and transparency of such complex assets. We introduce the concepts of
Element Tokens and Everything Tokens: elemental tokens represent standardized,
fully collateralized components of an asset (e.g., outputs, rights, or
credits), while an everything token represents the entire asset as a fixed
combination of those elements. The architecture enables both fine-grained
partial ownership and integrated whole-asset ownership through a system of
two-way convertibility. We detail the design and mechanics of this system,
including an arbitrage mechanism that keeps the price of the composite token
aligned with the net asset value of its constituents. Through illustrative
examples in the energy and industrial sectors, we demonstrate that our approach
allows previously illiquid, high-value projects to be fractionalized and traded
akin to stocks or exchange-traded funds (ETFs). We discuss the benefits for
investors and asset owners, such as lower entry barriers, improved price
discovery, and flexible financing, as well as the considerations for
implementation and regulation.

</details>


### [153] [Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method](https://arxiv.org/abs/2508.11467)
*Shifang Liu,Huiyuan Li,Hongjiao Sheng,Haoyuan Gui,Xiaoyu Zhang*

Main category: cs.DC

TL;DR: This paper introduces a GPU-centric SVD algorithm that eliminates CPU-GPU data transfers, features a new GPU-based BDC algorithm, and significantly speeds up computations on GPUs.


<details>
  <summary>Details</summary>
Motivation: Traditional SVD algorithms face inefficiencies due to slow panel factorization and frequent CPU-GPU data transfers, limiting their performance on heterogeneous systems despite advancements in GPU technology.

Method: The researchers reformulated the SVD algorithm to perform all panel-level computations and matrix updates entirely on the GPU. They introduced a GPU-based BDC method, optimized BLAS utilization for increased arithmetic intensity, and enabled asynchronous execution between CPU and GPU by restructuring workflows.

Result: The proposed method achieved remarkable speedups of up to 1293.64x/7.47x and 14.10x/12.38x on AMD MI210 and NVIDIA V100 GPUs compared to existing solutions like rocSOLVER/cuSOLVER and MAGMA.

Conclusion: The GPU-centered SVD algorithm solves the inefficiencies of traditional methods by maximizing GPU utilization and eliminating CPU-GPU data transfers, significantly improving computational performance and efficiency.

Abstract: Singular Value Decomposition (SVD) is a fundamental matrix factorization
technique in linear algebra, widely applied in numerous matrix-related
problems. However, traditional SVD approaches are hindered by slow panel
factorization and frequent CPU-GPU data transfers in heterogeneous systems,
despite advancements in GPU computational capabilities. In this paper, we
introduce a GPU-centered SVD algorithm, incorporating a novel GPU-based
bidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and
data layout of different steps for SVD computation, performing all panel-level
computations and trailing matrix updates entirely on GPU to eliminate CPU-GPU
data transfers. Furthermore, we integrate related computations to optimize BLAS
utilization, thereby increasing arithmetic intensity and fully leveraging the
computational capabilities of GPUs. Additionally, we introduce a newly
developed GPU-based BDC algorithm that restructures the workflow to eliminate
matrix-level CPU-GPU data transfers and enable asynchronous execution between
the CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs
demonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x
and 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.

</details>


### [154] [Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive](https://arxiv.org/abs/2508.11298)
*Gabin Schieffer,Jacob Wahlgren,Ruimin Shi,Edgar A. León,Roger Pearce,Maya Gokhale,Ivy Peng*

Main category: cs.DC

TL;DR: The paper investigates data movement optimization for HPC applications using AMD MI300A APUs with integrated CPU, GPU, and high-bandwidth memory (HBM).


<details>
  <summary>Details</summary>
Motivation: To address inefficiency in CPU-GPU data movement and improve HPC application performance using AMD MI300A APUs.

Method: Benchmarks were designed to evaluate direct memory access, inter-APU data movement, and communication across multiple APUs.

Result: Key optimization strategies for inter-APU communication were identified, and real HPC applications were optimized on the AMD MI300A system.

Conclusion: Efficient data movement using specific programming interfaces and allocators is crucial for HPC performance on AMD MI300A systems.

Abstract: The ever-increasing compute performance of GPU accelerators drives up the
need for efficient data movements within HPC applications to sustain
performance. Proposed as a solution to alleviate CPU-GPU data movement, AMD
MI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth
memory (HBM) within a single physical package. Leadership supercomputers, such
as El Capitan, group four APUs within a single compute node, using Infinity
Fabric Interconnect. In this work, we design specific benchmarks to evaluate
direct memory access from the GPU, explicit inter-APU data movement, and
collective multi-APU communication. We also compare the efficiency of HIP APIs,
MPI routines, and the GPU-specialized RCCL library. Our results highlight key
design choices for optimizing inter-APU communication on multi-APU AMD MI300A
systems with Infinity Fabric, including programming interfaces, allocators, and
data movement. Finally, we optimize two real HPC applications, Quicksilver and
CloverLeaf, and evaluate them on a four MI100A APU system.

</details>


### [155] [Space-efficient population protocols for exact majority in general graphs](https://arxiv.org/abs/2508.11384)
*Joel Rybicki,Jakob Solnerzik,Olivier Stietel,Robin Vacus*

Main category: cs.DC

TL;DR: This paper examines the exact majority consensus problem in the population protocol model using random interactions between nodes in a graph. It establishes tight time bounds for protocols, proposes a scalable protocol, and provides results for stabilization in regular expander graphs.


<details>
  <summary>Details</summary>
Motivation: To develop theoretical bounds and efficient protocols for exact majority consensus in graphs, ensuring nodes stabilize to the majority input value with precision and reduced complexity.

Method: The study analyzes general protocols and derives bounds using graph metrics like degree imbalance and relaxation time of random walks. It introduces a protocol suitable for arbitrary graphs and discusses its scalability.

Result: The protocol stabilizes in $O\left( \tfrac{\Delta}{\delta} \tau_{\mathsf{rel}} \log^2 n \right)$ steps with $O(\log n)$ space complexity in regular expander graphs, approaching near-optimal results from prior research.

Conclusion: Efficient exact majority consensus protocols can be achieved with precise time and space bounds for different types of graphs, advancing both theoretical understanding and practical applicability in distributed systems.

Abstract: We study exact majority consensus in the population protocol model. In this
model, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in
each time step, a scheduler samples uniformly at random a pair of adjacent
nodes to interact. In the exact majority consensus task, each node is given a
binary input, and the goal is to design a protocol that almost surely reaches a
stable configuration, where all nodes output the majority input value.
  We give improved upper and lower bounds for the exact majority in general
graphs. First, we give asymptotically tight time lower bounds for general
(unbounded space) protocols. Second, we obtain new upper bounds parameterized
by the relaxation time $\tau_{\mathsf{rel}}$ of the random walk on $G$ induced
by the scheduler and the degree imbalance $\Delta/\delta$ of $G$. Specifically,
we give a protocol that stabilizes in $O\left( \tfrac{\Delta}{\delta}
\tau_{\mathsf{rel}} \log^2 n \right)$ steps in expectation and with high
probability and uses $O\left( \log n \cdot \left(
\log\left(\tfrac{\Delta}{\delta}\right) + \log
\left(\tfrac{\tau_{\mathsf{rel}}}{n}\right) \right) \right)$ states in any
graph with minimum degree at least $\delta$ and maximum degree at most
$\Delta$.
  For regular expander graphs, this matches the optimal space complexity of
$\Theta(\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA
2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of
$O(n \log^2 n)$ steps. Finally, we give a new upper bound of
$O(\tau_{\mathsf{rel}} \cdot n \log n)$ for the stabilization time of a
constant-state protocol.

</details>


### [156] [Time, Fences and the Ordering of Events in TSO](https://arxiv.org/abs/2508.11415)
*Raïssa Nataf,Yoram Moses*

Main category: cs.DC

TL;DR: This paper investigates the Total Store Order (TSO) memory model, defining when synchronization primitives are necessary for correctness, and introduces a new "occurs-before" relation tailored to TSO.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in reasoning about correctness under TSO, a relaxed memory model widely used for multiprocessors, where executions often violate sequential consistency.

Method: The paper introduces a semantic framework with a new TSO-specific occurs-before relation, adapted from Lamport's happens-before concept. It provides a theorem detailing conditions for proper event ordering under TSO.

Result: The framework identifies cases where synchronization operations (like memory fences or atomic operations) are unavoidable for maintaining correct behavior under TSO.

Conclusion: This study advances the theoretical understanding of the TSO memory model, extending causality and information flow analysis to better address correctness and performance trade-offs.

Abstract: The Total Store Order (TSO) is arguably the most widely used relaxed memory
model in multiprocessor architectures, widely implemented, for example in
Intel's x86 and x64 platforms. It allows processes to delay the visibility of
writes through store buffering. While this supports hardware-level
optimizations and makes a significant contribution to multiprocessor
efficiency, it complicates reasoning about correctness, as executions may
violate sequential consistency. Ensuring correct behavior often requires
inserting synchronization primitives such as memory fences ($F$) or atomic
read-modify-write ($RMW$) operations, but this approach can incur significant
performance costs. In this work, we develop a semantic framework that precisely
characterizes when such synchronization is necessary under TSO. We introduce a
novel TSO-specific occurs-before relation, which adapts Lamport's celebrated
happens-before relation from asynchronous message-passing systems to the TSO
setting. Our main result is a theorem that proves that the only way to ensure
that two events that take place at different sites are temporally ordered is by
having the execution create an occurs-before chain between the events. By
studying the role of fences and $RMW$s in creating occurs-before chains, we are
then able to capture cases in which these costly synchronization operations are
unavoidable. Since proper real-time ordering of events is a fundamental aspect
of consistency conditions such as Linearizability, our analysis provides a
sound theoretical understanding of essential aspects of the TSO model. In
particular, we are able to generalize prior lower bounds for linearizable
implementations of shared memory objects. Our results capture the structure of
information flow and causality in the TSO model by extending the standard
communication-based reasoning from asynchronous systems to the TSO memory
model.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [157] [A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification](https://arxiv.org/abs/2508.10926)
*DongSeong-Yoon*

Main category: cs.LG

TL;DR: The paper addresses the limitations of traditional ensemble learning weighting methods that overly rely on one evaluation criterion. It introduces a new decision-making method based on cooperative games to integrate multiple criteria for better performance.


<details>
  <summary>Details</summary>
Motivation: Current ensemble learning methods often rely on a single evaluation criterion for weighting classifiers, limiting their ability to incorporate diverse pre-information realistically.

Method: Introduce a cooperative game-based decision-making approach to ensemble weighting in multi-criteria situations, ensuring comprehensive information integration for classifiers.

Result: Testing on the Open-ML-CC18 dataset, the proposed method demonstrated superior performance compared with existing ensemble weighting techniques.

Conclusion: The cooperative game-based approach enhances representation, optimizes weight distribution, and alleviates key AI model challenges, providing promising advancements for ensemble learning.

Abstract: Since the Fourth Industrial Revolution, AI technology has been widely used in
many fields, but there are several limitations that need to be overcome,
including overfitting/underfitting, class imbalance, and the limitations of
representation (hypothesis space) due to the characteristics of different
models. As a method to overcome these problems, ensemble, commonly known as
model combining, is being extensively used in the field of machine learning.
Among ensemble learning methods, voting ensembles have been studied with
various weighting methods, showing performance improvements. However, the
existing methods that reflect the pre-information of classifiers in weights
consider only one evaluation criterion, which limits the reflection of various
information that should be considered in a model realistically. Therefore, this
paper proposes a method of making decisions considering various information
through cooperative games in multi-criteria situations. Using this method,
various types of information known beforehand in classifiers can be
simultaneously considered and reflected, leading to appropriate weight
distribution and performance improvement. The machine learning algorithms were
applied to the Open-ML-CC18 dataset and compared with existing ensemble
weighting methods. The experimental results showed superior performance
compared to other weighting methods.

</details>


### [158] [Apriel-Nemotron-15B-Thinker](https://arxiv.org/abs/2508.10948)
*Shruthan Radhakrishna,Soham Parikh,Gopal Sarda,Anil Turkkan,Quaizar Vohra,Raymond Li,Dhruv Jhamb,Kelechi Ogueji,Aanjaneya Shukla,Oluwanifemi Bamgbose,Toby Liang,Luke Kumar,Oleksiy Ostapenko,Shiva Krishna Reddy Malay,Aman Tiwari,Tara Bogavelli,Vikas Yadav,Jash Mehta,Saloni Mittal,Akshay Kalkunte,Pulkit Pattnaik,Khalil Slimi,Anirudh Sreeram,Jishnu Nair,Akintunde Oladipo,Shashank Maiya,Khyati Mahajan,Rishabh Maheshwary,Masoud Hashemi,Sai Rajeswar Mudumba,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sebastien Paquet,Sagar Davasam,Srinivas Sunkara*

Main category: cs.LG

TL;DR: The paper introduces Apriel-Nemotron-15B-Thinker, a 15-billion parameter model optimized for performance and lower memory footprint, comparing favorably to larger models.


<details>
  <summary>Details</summary>
Motivation: Large language models often face limitations due to high memory and computational demands, hindering enterprise applicability.

Method: The model is trained via a four-stage pipeline: Base Model upscaling, Continual Pre-training, Supervised Fine-tuning, and Reinforcement Learning using GRPO.

Result: Apriel-Nemotron-15B-Thinker achieves performance on par with or better than much larger models, while using only half the memory.

Conclusion: This model demonstrates the possibility of combining efficiency with high performance, aiding practical deployment in enterprise scenarios.

Abstract: While large language models (LLMs) have achieved remarkable reasoning
capabilities across domains like code, math and other enterprise tasks, their
significant memory and computational costs often preclude their use in
practical enterprise settings. To this end, we introduce
Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow
Apriel SLM series that achieves performance against medium sized
state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while
maintaining only half the memory footprint of those alternatives.
Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline
including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised
Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive
evaluations across a diverse suite of benchmarks consistently demonstrate that
our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its
32-billion parameter counterparts, despite being less than half their size.

</details>


### [159] [Towards Efficient Prompt-based Continual Learning in Distributed Medical AI](https://arxiv.org/abs/2508.10954)
*Gyutae Oh,Jitae Shin*

Main category: cs.LG

TL;DR: The study introduces a prompt-based continual learning (PCL) framework to address challenges in medical AI caused by distributed and shifting data.


<details>
  <summary>Details</summary>
Motivation: Medical domain constraints limit centralized AI model training, leading to overfitting and forgetting issues in traditional methods.

Method: The method uses a unified prompt pool with selective prompt expansion and freezing, combined with regularization for retention and adaptation.

Result: The proposed model improves classification accuracy by 10% and F1-score by 9 points on diabetic retinopathy datasets, while reducing inference costs.

Conclusion: This approach aims to advance sustainable medical AI practices for distributed healthcare applications like telemedicine and diagnosis.

Abstract: Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

</details>


### [160] [Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis](https://arxiv.org/abs/2508.10967)
*Xinyi Li,Sai Wang,Yutian Lin,Yu Wu,Yi Yang*

Main category: cs.LG

TL;DR: Retro-Expert is a retrosynthesis prediction framework that combines large language models (LLMs) and specialized models using reinforcement learning, leading to enhanced performance and interpretable explanations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing retrosynthesis models which rely on static pattern-matching and result in black-box decision-making.

Method: Retro-Expert integrates LLMs and specialized models for collaborative reasoning, using three key components: specialized models for shallow reasoning, LLMs for generating predictions with interpretable reasoning paths, and reinforcement learning to optimize decisions.

Result: Retro-Expert surpasses the performance of both LLM-based and specialized models across metrics and generates expert-consistent, interpretable explanations.

Conclusion: Retro-Expert bridges the gap between AI-based retrosynthesis predictions and actionable chemical logic, improving performance and interpretability.

Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a
given product molecule, which is a fundamental task in chemical synthesis.
However, existing models rely on static pattern-matching paradigm, which limits
their ability to perform effective logic decision-making, leading to black-box
decision-making. Building on this, we propose Retro-Expert, an interpretable
retrosynthesis framework that performs collaborative reasoning by combining the
complementary reasoning strengths of Large Language Models and specialized
models via reinforcement learning. It outputs natural language explanations
grounded in chemical logic through three components: (1) specialized models
perform shallow reasoning to construct high-quality chemical decision space,
(2) LLM-driven critical reasoning to generate predictions and corresponding
interpretable reasoning path, and (3) reinforcement learning optimizing
interpretable decision policy. Experiments show that Retro-Expert not only
surpasses both LLM-based and specialized models across different metrics but
also provides expert-aligned explanations that bridge the gap between AI
predictions and actionable chemical insights.

</details>


### [161] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: The paper introduces BeyondWeb, a synthetic data generation framework that produces higher-quality synthetic data for pretraining large language models compared to state-of-the-art methods, with improvements in both training speed and benchmark performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the diminishing returns in large language model performance that arise from simply scaling data size, exploring synthetic data as a way to overcome this limitation.

Method: The methodology involves designing BeyondWeb, a framework that carefully optimizes synthetic data generation by identifying and balancing critical factors contributing to data quality.

Result: BeyondWeb outperforms existing synthetic datasets like Cosmopedia and Nemotron-Synth on various benchmarks. The framework enables faster training and shows superior performance even with a smaller model size and token budget.

Conclusion: The study highlights the complexity of producing superior synthetic data for pretraining, stressing that success requires the careful optimization of multiple factors. When well-executed, synthetic data generation frameworks like BeyondWeb can deliver transformative performance improvements.

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [162] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: This paper proposes a framework named M&C to efficiently select the best pretrained text-to-image (T2I) model for fine-tuning on a target dataset.


<details>
  <summary>Details</summary>
Motivation: Many pretrained T2I models are widely available, yet users struggle to identify the optimal model for fine-tuning on their specific data domain.

Method: The M&C framework employs a matching graph consisting of nodes (models and datasets) and edges capturing model-data fine-tuning performance and data similarity. A predictive model leverages these features to recommend the best T2I model for the target domain.

Result: M&C was tested across ten T2I models and 32 datasets, outperforming three baselines. It accurately predicts the best model for fine-tuning in 61.3% of cases and a closely performing alternative in the remainder.

Conclusion: The M&C framework streamlines model selection for T2I applications, addressing a significant gap in efficiently matching models to domain-specific datasets without exhaustive fine-tuning.

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [163] [CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention](https://arxiv.org/abs/2508.11016)
*Qingbin Li,Rongkun Xue,Jie Wang,Ming Zhou,Zhi Li,Xiaofeng Ji,Yongqi Wang,Miao Liu,Zheming Yang,Minghui Qiu,Jing Yang*

Main category: cs.LG

TL;DR: The paper proposes CURE, a novel framework for improving reasoning tasks in Reinforcement Learning with Verified Reward (RLVR) by preventing entropy collapse through a two-stage strategy involving dynamic token regeneration and static state sampling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of prior RLVR pipelines, which used static initial-state sampling leading to low-diversity model behaviors, rapid entropy collapse, and hindered prolonged performance improvements.

Method: CURE introduces a two-stage process: (1) High-entropy critical-token regeneration to explore novel yet coherent contexts, optimizing both original and branched trajectories, and (2) Conventional training using static initial-state sampling to strengthen exploitation progressively.

Result: The framework demonstrates a 5% performance gain on six math benchmarks using Qwen-2.5-Math-7B, achieving state-of-the-art results in both entropy maintenance and task accuracy.

Conclusion: CURE successfully balances exploration and exploitation during training, offering a robust alternative to existing RLVR methods and establishing new benchmarks in reinforcement learning-driven reasoning tasks.

Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have
driven the emergence of more sophisticated cognitive behaviors in large
language models (LLMs), thereby enhancing their reasoning capabilities.
However, in prior RLVR pipelines, the repeated use of static initial-state
sampling drawn exactly from the dataset distribution during each sampling phase
produced overly deterministic, low diversity model behavior, which manifested
as rapid entropy collapse and hindered sustained performance gains during
prolonged training. To address this issue, we introduce CURE
(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a
two-stage framework that balances exploration and exploitation. Specifically,
in the first stage, to deliberately steer the model toward novel yet coherent
contexts, we re-generate at high-entropy critical tokens and jointly optimize
the original and the branched trajectories. The further comparison with vanilla
DAPO shows that the regeneration process achieves a better performance on math
reasoning tasks while sustaining a high-level entropy degree for exploration.
In the second stage, we continue training with static initial-state sampling by
DAPO, intentionally placing the model in a familiar state to gradually
strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,
compared to other RLVR methods, CURE achieves a 5% performance gain across six
math benchmarks, establishing state-of-the-art performance in both entropy and
accuracy. A series of experiments further validate the effectiveness of our
approach. Code is available at https://github.com/CURE-Project/CURE.

</details>


### [164] [Conditional Independence Estimates for the Generalized Nonparanormal](https://arxiv.org/abs/2508.11050)
*Ujas Shah,Manuel Lladser,Rebecca Morrison*

Main category: cs.LG

TL;DR: This paper extends conditional independence analysis to non-Gaussian distributions derived from diagonal transformations of Gaussians, termed generalized nonparanormal, proposing a computational algorithm to extract structures.


<details>
  <summary>Details</summary>
Motivation: Understanding conditional independence structures in non-Gaussian distributions is critical but challenging since covariance and precision matrices are not effective indicators of independence for these distributions.

Method: The authors examined generalized nonparanormal data, developing a computationally efficient algorithm to deduce independence structures directly from the precision matrix under specific conditions.

Result: The algorithm demonstrated effective recovery of conditional independence structures in generalized nonparanormal distributions, validated through synthetic and real-world datasets.

Conclusion: Generalized nonparanormal distributions allow inference of conditional independence from precision matrices, analogous to multivariate Gaussian cases, and the proposed algorithm performs accurately and efficiently on such data.

Abstract: For general non-Gaussian distributions, the covariance and precision matrices
do not encode the independence structure of the variables, as they do for the
multivariate Gaussian. This paper builds on previous work to show that for a
class of non-Gaussian distributions -- those derived from diagonal
transformations of a Gaussian -- information about the conditional independence
structure can still be inferred from the precision matrix, provided the data
meet certain criteria, analogous to the Gaussian case. We call such
transformations of the Gaussian as the generalized nonparanormal. The functions
that define these transformations are, in a broad sense, arbitrary. We also
provide a simple and computationally efficient algorithm that leverages this
theory to recover conditional independence structure from the generalized
nonparanormal data. The effectiveness of the proposed algorithm is demonstrated
via synthetic experiments and applications to real-world data.

</details>


### [165] [Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis](https://arxiv.org/abs/2508.11020)
*Aakash Kumar,Emanuele Natale*

Main category: cs.LG

TL;DR: This paper extends the Strong Lottery Ticket Hypothesis (SLTH) framework to finite-precision networks, establishing theoretical foundations for quantized neural networks to represent target discrete networks exactly.


<details>
  <summary>Details</summary>
Motivation: Quantization in neural networks offers efficiency but lacks deep theoretical understanding, particularly in extremely low-precision scenarios like binary networks.

Method: The authors build upon foundational results from Borgs et al. concerning the Number Partitioning Problem to derive insights into the Random Subset Sum Problem in the quantized domain, and then adapt the SLTH framework for finite-precision networks.

Result: The paper proves that target discrete neural networks in the quantized setting can be represented exactly, along with establishing optimal bounds on overparameterization relative to the network's precision.

Conclusion: Quantized networks can achieve exact representation of specific discrete networks, providing optimal overparameterization bounds and contributing to theoretical advancements in neural network quantization.

Abstract: Quantization is an essential technique for making neural networks more
efficient, yet our theoretical understanding of it remains limited. Previous
works demonstrated that extremely low-precision networks, such as binary
networks, can be constructed by pruning large, randomly-initialized networks,
and showed that the ratio between the size of the original and the pruned
networks is at most polylogarithmic.
  The specific pruning method they employed inspired a line of theoretical work
known as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights
from the Random Subset Sum Problem. However, these results primarily address
the continuous setting and cannot be applied to extend SLTH results to the
quantized setting.
  In this work, we build on foundational results by Borgs et al. on the Number
Partitioning Problem to derive new theoretical results for the Random Subset
Sum Problem in a quantized setting.
  Using these results, we then extend the SLTH framework to finite-precision
networks. While prior work on SLTH showed that pruning allows approximation of
a certain class of neural networks, we demonstrate that, in the quantized
setting, the analogous class of target discrete neural networks can be
represented exactly, and we prove optimal bounds on the necessary
overparameterization of the initial network as a function of the precision of
the target network.

</details>


### [166] [Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees](https://arxiv.org/abs/2508.11112)
*Jianhao Ma,Lin Xiao*

Main category: cs.LG

TL;DR: This paper investigates Piecewise-Affine Regularization (PAR) for handling optimization problems with discrete variables, focusing on supervised learning. It provides theoretical insights and practical computational methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimization over discrete or quantized variables, particularly for supervised learning tasks, by leveraging a continuous optimization framework through PAR.

Method: The study establishes theoretical properties of PAR in overparameterized regimes, develops proximal mappings for solving PAR-regularized problems using advanced optimization methods (e.g., proximal gradient and ADMM), and examines their statistical guarantees for linear regression.

Result: The findings show that PAR-regularized loss functions result in highly quantized critical points in overparameterized settings. Additionally, the approach provides practical algorithms to solve PAR-regularized optimization problems and demonstrates how PAR can approximate traditional regularizations while ensuring statistical guarantees.

Conclusion: PAR offers a robust framework for addressing discrete optimization problems in supervised learning, combining theoretical rigor, computational algorithms, and practical statistical assurances for quantized solutions.

Abstract: Optimization problems over discrete or quantized variables are very
challenging in general due to the combinatorial nature of their search space.
Piecewise-affine regularization (PAR) provides a flexible modeling and
computational framework for quantization based on continuous optimization. In
this work, we focus on the setting of supervised learning and investigate the
theoretical foundations of PAR from optimization and statistical perspectives.
First, we show that in the overparameterized regime, where the number of
parameters exceeds the number of samples, every critical point of the
PAR-regularized loss function exhibits a high degree of quantization. Second,
we derive closed-form proximal mappings for various (convex, quasi-convex, and
non-convex) PARs and show how to solve PAR-regularized problems using the
proximal gradient method, its accelerated variant, and the Alternating
Direction Method of Multipliers. Third, we study statistical guarantees of
PAR-regularized linear regression problems; specifically, we can approximate
classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex
regularizations using PAR and obtain similar statistical guarantees with
quantized solutions.

</details>


### [167] [Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks](https://arxiv.org/abs/2508.11025)
*Laura Lützow,Michael Eichelbeck,Mykel J. Kochenderfer,Matthias Althoff*

Main category: cs.LG

TL;DR: The paper introduces zono-conformal prediction, a novel method for constructing prediction sets with coverage guarantees, addressing inefficiencies and limitations of current conformal prediction methods.


<details>
  <summary>Details</summary>
Motivation: Existing conformal prediction methods are computationally expensive, data-intensive, and primarily use intervals, limiting their ability to capture dependencies in multi-dimensional outputs.

Method: The authors propose using zonotopes for prediction sets instead of intervals. Zonotopic uncertainty sets are integrated into the base predictor's model through a single linear program, focusing on feed-forward neural networks and extending to regression and classification tasks.

Result: Numerical experiments demonstrate that zono-conformal predictors are less conservative than existing methods while maintaining similar coverage over test data.

Conclusion: Zono-conformal prediction improves computational efficiency and flexibility in capturing multi-dimensional dependencies, providing a promising alternative to traditional conformal prediction techniques.

Abstract: Conformal prediction is a popular uncertainty quantification method that
augments a base predictor with prediction sets with statistically valid
coverage guarantees. However, current methods are often computationally
expensive and data-intensive, as they require constructing an uncertainty model
before calibration. Moreover, existing approaches typically represent the
prediction sets with intervals, which limits their ability to capture
dependencies in multi-dimensional outputs. We address these limitations by
introducing zono-conformal prediction, a novel approach inspired by interval
predictor models and reachset-conformant identification that constructs
prediction zonotopes with assured coverage. By placing zonotopic uncertainty
sets directly into the model of the base predictor, zono-conformal predictors
can be identified via a single, data-efficient linear program. While we can
apply zono-conformal prediction to arbitrary nonlinear base predictors, we
focus on feed-forward neural networks in this work. Aside from regression
tasks, we also construct optimal zono-conformal predictors in classification
settings where the output of an uncertain predictor is a set of possible
classes. We provide probabilistic coverage guarantees and present methods for
detecting outliers in the identification data. In extensive numerical
experiments, we show that zono-conformal predictors are less conservative than
interval predictor models and standard conformal prediction methods, while
achieving a similar coverage over the test data.

</details>


### [168] [Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning](https://arxiv.org/abs/2508.11210)
*Minghui Sun,Matthew M. Engelhard,Benjamin A. Goldstein*

Main category: cs.LG

TL;DR: This paper introduces a method called Borrowing From the Future (BFF), a contrastive multi-modal framework, to enhance early-stage pediatric risk assessments by leveraging data from later stages.


<details>
  <summary>Details</summary>
Motivation: Risk assessments for pediatric populations are important but challenging, especially when conducted in early stages where less information is available.

Method: The method uses a contrastive multi-modal training framework that treats each time stage (prenatal, birth, Well-Child visits) as a unique modality, allowing the model to learn from later-stage data while assessing early-stage risks.

Result: The framework was validated on two real-world pediatric prediction tasks and showed consistent improvement in early-stage risk predictions.

Conclusion: BFF successfully improves early-stage pediatric risk assessments by leveraging later-stage data through its contrastive multi-modal approach.

Abstract: Risk assessments for a pediatric population are often conducted across
multiple stages. For example, clinicians may evaluate risks prenatally, at
birth, and during Well-Child visits. Although predictions made at later stages
typically achieve higher precision, it is clinically desirable to make reliable
risk assessments as early as possible. Therefore, this study focuses on
improving prediction performance in early-stage risk assessments. Our solution,
\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal
framework that treats each time window as a distinct modality. In BFF, a model
is trained on all available data throughout the time while performing a risk
assessment using up-to-date information. This contrastive framework allows the
model to ``borrow'' informative signals from later stages (e.g., Well-Child
visits) to implicitly supervise the learning at earlier stages (e.g.,
prenatal/birth stages). We validate BFF on two real-world pediatric outcome
prediction tasks, demonstrating consistent improvements in early risk
assessments. The code is available at https://github.com/scotsun/bff.

</details>


### [169] [Learning with Confidence](https://arxiv.org/abs/2508.11037)
*Oliver Ethan Richardson*

Main category: cs.LG

TL;DR: This paper introduces a formal framework for understanding 'confidence' in learning, distinguishing it from probability, and linking it to concepts like learning rates, weight of evidence, and Kalman gain.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in the formal representation of 'confidence' in learning processes, a key yet often overlooked aspect that impacts how beliefs are updated.

Method: The authors axiomatize confidence, present two canonical measurement approaches, and derive more compact representations using vector fields and loss functions.

Result: They prove that confidence can always be represented on a measurable continuum and provide representations that simplify understanding and computation in confidence-based learning.

Conclusion: Bayes Rule emerges as a specific case within this extended framework, offering insights into optimizing learners and their loss-based behavior.

Abstract: We characterize a notion of confidence that arises in learning or updating
beliefs: the amount of trust one has in incoming information and its impact on
the belief state. This learner's confidence can be used alongside (and is
easily mistaken for) probability or likelihood, but it is fundamentally a
different concept -- one that captures many familiar concepts in the
literature, including learning rates and number of training epochs, Shafer's
weight of evidence, and Kalman gain. We formally axiomatize what it means to
learn with confidence, give two canonical ways of measuring confidence on a
continuum, and prove that confidence can always be represented in this way.
Under additional assumptions, we derive more compact representations of
confidence-based learning in terms of vector fields and loss functions. These
representations induce an extended language of compound "parallel"
observations. We characterize Bayes Rule as the special case of an optimizing
learner whose loss representation is a linear expectation.

</details>


### [170] [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](https://arxiv.org/abs/2508.11460)
*Aurora Grefsrud,Nello Blaser,Trygve Buanes*

Main category: cs.LG

TL;DR: The study evaluates six probabilistic machine learning algorithms for uncertainty estimation using approximate Bayesian inference and synthetic datasets. While all methods are well-calibrated, none of the deep learning methods consistently handle out-of-distribution uncertainty.


<details>
  <summary>Details</summary>
Motivation: With increasingly complex data models, particularly deep learning, quantifying uncertainty has become challenging, necessitating robust evaluations of various techniques.

Method: The researchers used approximate Bayesian inference and synthetic classification datasets to test six probabilistic machine learning algorithms for their performance in class probability and uncertainty estimation.

Result: All tested algorithms were well-calibrated; however, the deep learning-based methods failed to consistently reflect a lack of experimental evidence for out-of-distribution data points.

Conclusion: While current methods achieve calibration, there is a need for improved techniques to quantify uncertainties, particularly for handling out-of-distribution data reliably.

Abstract: Rigorous statistical methods, including parameter estimation with
accompanying uncertainties, underpin the validity of scientific discovery,
especially in the natural sciences. With increasingly complex data models such
as deep learning techniques, uncertainty quantification has become exceedingly
difficult and a plethora of techniques have been proposed. In this case study,
we use the unifying framework of approximate Bayesian inference combined with
empirical tests on carefully created synthetic classification datasets to
investigate qualitative properties of six different probabilistic machine
learning algorithms for class probability and uncertainty estimation: (i) a
neural network ensemble, (ii) neural network ensemble with conflictual loss,
(iii) evidential deep learning, (iv) a single neural network with Monte Carlo
Dropout, (v) Gaussian process classification and (vi) a Dirichlet process
mixture model. We check if the algorithms produce uncertainty estimates which
reflect commonly desired properties, such as being well calibrated and
exhibiting an increase in uncertainty for out-of-distribution data points. Our
results indicate that all algorithms are well calibrated, but none of the deep
learning based algorithms provide uncertainties that consistently reflect lack
of experimental evidence for out-of-distribution data points. We hope our study
may serve as a clarifying example for researchers developing new methods of
uncertainty estimation for scientific data-driven modeling.

</details>


### [171] [SHLIME: Foiling adversarial attacks fooling SHAP and LIME](https://arxiv.org/abs/2508.11053)
*Sam Chauhan,Estelle Duguet,Karthik Ramakrishnan,Hugh Van Deventer,Jack Kruger,Ranjan Subbaraman*

Main category: cs.LG

TL;DR: The paper evaluates the robustness of LIME and SHAP, widely-used explanation methods for black-box models, against adversarial manipulation aimed at concealing biases. It proposes improved configurations to better detect biases.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the vulnerability of post hoc explanation methods, like LIME and SHAP, in detecting biases in machine learning models, which can lead to harmful consequences if concealed.

Method: The authors replicate experiments to validate prior findings, establish a baseline, and introduce a modular testing framework to systematically assess augmented and ensemble explanation methods against biased models.

Result: The study identifies specific configurations of LIME/SHAP ensemble methods that notably enhance the ability to detect concealed biases in black-box classifiers.

Conclusion: Improved configurations of explanation methods offer a promising avenue to enhance transparency and bias detection in high-stakes machine learning systems.

Abstract: Post hoc explanation methods, such as LIME and SHAP, provide interpretable
insights into black-box classifiers and are increasingly used to assess model
biases and generalizability. However, these methods are vulnerable to
adversarial manipulation, potentially concealing harmful biases. Building on
the work of Slack et al. (2020), we investigate the susceptibility of LIME and
SHAP to biased models and evaluate strategies for improving robustness. We
first replicate the original COMPAS experiment to validate prior findings and
establish a baseline. We then introduce a modular testing framework enabling
systematic evaluation of augmented and ensemble explanation approaches across
classifiers of varying performance. Using this framework, we assess multiple
LIME/SHAP ensemble configurations on out-of-distribution models, comparing
their resistance to bias concealment against the original methods. Our results
identify configurations that substantially improve bias detection, highlighting
their potential for enhancing transparency in the deployment of high-stakes
machine learning systems.

</details>


### [172] [Abundance-Aware Set Transformer for Microbiome Sample Embedding](https://arxiv.org/abs/2508.11075)
*Hyunwoo Yoo,Gail Rosen*

Main category: cs.LG

TL;DR: The authors propose an abundance-aware Set Transformer to improve input representations of microbiome samples for classification tasks, outperforming previous pooling methods.


<details>
  <summary>Details</summary>
Motivation: Prior research in microbiome sample representation for tasks like phenotype and environmental classification often neglect the biological importance of taxa abundance while embedding samples.

Method: The paper introduces a method that uses an abundance-aware Set Transformer, replicating embedding vectors proportional to taxa abundance and aggregating them using self-attention, without altering the model architecture.

Result: The proposed method demonstrates superior performance compared to average pooling and unweighted Set Transformers in real-world microbiome classification tasks, even achieving perfect performance in some cases.

Conclusion: This work highlights the value of incorporating abundance into sample embeddings, representing robust and biologically relevant microbiome samples, and fills a gap in integrating abundance-awareness into LLM inputs.

Abstract: Microbiome sample representation to input into LLMs is essential for
downstream tasks such as phenotype prediction and environmental classification.
While prior studies have explored embedding-based representations of each
microbiome sample, most rely on simple averaging over sequence embeddings,
often overlooking the biological importance of taxa abundance. In this work, we
propose an abundance-aware variant of the Set Transformer to construct
fixed-size sample-level embeddings by weighting sequence embeddings according
to their relative abundance. Without modifying the model architecture, we
replicate embedding vectors proportional to their abundance and apply
self-attention-based aggregation. Our method outperforms average pooling and
unweighted Set Transformers on real-world microbiome classification tasks,
achieving perfect performance in some cases. These results demonstrate the
utility of abundance-aware aggregation for robust and biologically informed
microbiome representation. To the best of our knowledge, this is one of the
first approaches to integrate sequence-level abundance into Transformer-based
sample embeddings.

</details>


### [173] [A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora](https://arxiv.org/abs/2508.11084)
*Thanasis Schoinas,Ghulam Qadir*

Main category: cs.LG

TL;DR: The paper addresses predictive coding challenges for informal, smaller datasets like instant messages and offers a cost-effective solution using logistic regression and dimensionality reduction.


<details>
  <summary>Details</summary>
Motivation: Predictive coding for legal document classification with instant messages is challenging due to their informal nature and smaller size, necessitating economical solutions.

Method: The study involves grouping messages into day chats, feature selection, logistic regression, dimensionality reduction focused on quantitative features, and testing with the Instant Bloomberg dataset.

Result: Improved baseline model performance and demonstrated economic feasibility for predictive coding on informal instant message datasets.

Conclusion: The methodology provides an efficient machine learning solution for predictive coding in legal contexts, especially for instant messages, with proven cost savings and enhanced performance.

Abstract: Predictive coding, the term used in the legal industry for document
classification using machine learning, presents additional challenges when the
dataset comprises instant messages, due to their informal nature and smaller
sizes. In this paper, we exploit a data management workflow to group messages
into day chats, followed by feature selection and a logistic regression
classifier to provide an economically feasible predictive coding solution. We
also improve the solution's baseline model performance by dimensionality
reduction, with focus on quantitative features. We test our methodology on an
Instant Bloomberg dataset, rich in quantitative information. In parallel, we
provide an example of the cost savings of our approach.

</details>


### [174] [Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation](https://arxiv.org/abs/2508.11086)
*Emily Liu,Kuan Han,Minfeng Zhan,Bocheng Zhao,Guanyu Mu,Yang Song*

Main category: cs.LG

TL;DR: The paper addresses biases in video recommendations derived from raw watch times by introducing a debiasing framework that improves recommendation accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Raw watch times, widely used in video recommendations, are influenced by factors like video duration and popularity, which distort user preference signals and lead to biased models.

Method: The authors propose a relative advantage debiasing framework that compares watch times against reference distributions and employs a two-stage architecture for separating distribution estimation and preference learning, along with distributional embeddings for efficient quantile parameterization.

Result: Experiments, both offline and online, show that the proposed framework significantly enhances recommendation accuracy and robustness compared to existing methods.

Conclusion: Debiasing raw watch time signals can mitigate distortions in video recommendations, resulting in more accurate and unbiased preference-based recommendation systems.

Abstract: Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

</details>


### [175] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Giró-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: The paper introduces a meta-learning framework for compressive learning, optimizing both encoding and decoding stages for fast and efficient data processing.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies in traditional compressive learning methods that fail to leverage data structure and struggle with growing datasets.

Method: The paper uses neural networks to meta-learn the encoding and decoding steps in compressive learning, optimizing them for data structure exploitation.

Result: The proposed framework demonstrated faster and more accurate performance compared to traditional approaches across various applications.

Conclusion: The meta-learning approach enhances compressive learning, showcasing its versatility and efficiency on tasks like compressive PCA and autoencoders.

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


### [176] [Predictive Multimodal Modeling of Diagnoses and Treatments in EHR](https://arxiv.org/abs/2508.11092)
*Cindy Shih-Ting Huang,Clarence Boon Liang Ng,Marek Rei*

Main category: cs.LG

TL;DR: This paper introduces a multimodal system for early ICD code prediction by integrating clinical notes and tabular EHRs using cross-modal attention and temporal loss.


<details>
  <summary>Details</summary>
Motivation: To enable early ICD code prediction for risk identification, treatment suggestion, and resource optimization based on limited patient data at the beginning of a hospital stay.

Method: The paper proposes a system that fuses clinical notes and EHR tabular events using pre-trained encoders, feature pooling, cross-modal attention, and a weighted temporal loss to optimize predictive modeling.

Result: The proposed strategies improve early prediction performance and outperform state-of-the-art methods in ICD code forecasting.

Conclusion: The multimodal approach and temporal strategies presented in the paper provide significant advantages in early prediction capabilities for healthcare applications.

Abstract: While the ICD code assignment problem has been widely studied, most works
have focused on post-discharge document classification. Models for early
forecasting of this information could be used for identifying health risks,
suggesting effective treatments, or optimizing resource allocation. To address
the challenge of predictive modeling using the limited information at the
beginning of a patient stay, we propose a multimodal system to fuse clinical
notes and tabular events captured in electronic health records. The model
integrates pre-trained encoders, feature pooling, and cross-modal attention to
learn optimal representations across modalities and balance their presence at
every temporal point. Moreover, we present a weighted temporal loss that
adjusts its contribution at each point in time. Experiments show that these
strategies enhance the early prediction model, outperforming the current
state-of-the-art systems.

</details>


### [177] [Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation](https://arxiv.org/abs/2508.11105)
*Sajjad Saed,Babak Teimourpour*

Main category: cs.LG

TL;DR: The paper introduces FGAT, a novel framework using hierarchical graph structures and graph attention mechanisms to improve personalized fashion recommendation systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing challenge in e-commerce platforms of simultaneously ensuring outfit compatibility and personalized recommendations.

Method: The study proposes FGAT, which leverages a hierarchical graph of users, outfits, and items, integrating visual and textual features. A graph attention mechanism dynamically weights node importance during representation propagation.

Result: FGAT outperforms baseline models like HFGN across metrics such as precision, HR, recall, NDCG, and accuracy on the POG dataset.

Conclusion: The integration of multimodal visual-textual features with hierarchical graph structures and attention mechanisms significantly enhances personalized fashion recommendations.

Abstract: The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

</details>


### [178] [CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets](https://arxiv.org/abs/2508.11144)
*Gauri Jain,Dominik Rothenhäusler,Kirk Bansak,Elisabeth Paulson*

Main category: cs.LG

TL;DR: The paper presents CTRL, a meta-learning method designed for improving predictions across data from multiple sources, addressing challenges such as distributional shifts and sample size variation.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for machine learning models to deliver not just overall accuracy, but reliable, source-specific predictions in scenarios where data comes from diverse sources.

Method: The proposed method, CTRL, merges cross-domain residual learning with adaptive pooling or clustering to enhance overall prediction accuracy while maintaining source-specific differences.

Result: CTRL demonstrates consistent outperformance over state-of-the-art benchmarks across multiple datasets, including a real-world application in the Swiss asylum program.

Conclusion: CTRL offers a reliable and effective framework for addressing prediction challenges in multi-source, heterogeneous data environments.

Abstract: Machine learning (ML) tasks often utilize large-scale data that is drawn from
several distinct sources, such as different locations, treatment arms, or
groups. In such settings, practitioners often desire predictions that not only
exhibit good overall accuracy, but also remain reliable within each source and
preserve the differences that matter across sources. For instance, several
asylum and refugee resettlement programs now use ML-based employment
predictions to guide where newly arriving families are placed within a host
country, which requires generating informative and differentiated predictions
for many and often small source locations. However, this task is made
challenging by several common characteristics of the data in these settings:
the presence of numerous distinct data sources, distributional shifts between
them, and substantial variation in sample sizes across sources. This paper
introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method
that combines the strengths of cross-domain residual learning and adaptive
pooling/clustering in order to simultaneously improve overall accuracy and
preserve source-level heterogeneity. We provide theoretical results that
clarify how our objective navigates the trade-off between data quantity and
data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5
large-scale datasets. This includes a dataset from the national asylum program
in Switzerland, where the algorithmic geographic assignment of asylum seekers
is currently being piloted. CTRL consistently outperforms the benchmarks across
several key metrics and when using a range of different base learners.

</details>


### [179] [Towards the Next-generation Bayesian Network Classifiers](https://arxiv.org/abs/2508.11145)
*Huan Zhang,Daokun Zhang,Kexin Meng,Geoffrey I. Webb*

Main category: cs.LG

TL;DR: The paper introduces NeuralKDB, a high-order Bayesian network classifier using neural architectures and distributional representations for more accurate data classification.


<details>
  <summary>Details</summary>
Motivation: Bayesian network classifiers face challenges like parameter explosion and data sparsity, limiting their ability to model complex feature dependencies.

Method: The authors propose NeuralKDB, a technique where distributional representations of feature values are learned via a novel neural network architecture, and conditional probabilities are efficiently parameterized using stochastic gradient descent.

Result: Experiments on 60 UCI datasets show NeuralKDB effectively captures high-order feature dependencies and outperforms traditional Bayesian classifiers and other competitive models.

Conclusion: NeuralKDB demonstrates the potential for combining neural networks with Bayesian methods to improve classification performance, particularly in settings requiring complex feature dependency modeling.

Abstract: Bayesian network classifiers provide a feasible solution to tabular data
classification, with a number of merits like high time and memory efficiency,
and great explainability. However, due to the parameter explosion and data
sparsity issues, Bayesian network classifiers are restricted to low-order
feature dependency modeling, making them struggle in extrapolating the
occurrence probabilities of complex real-world data. In this paper, we propose
a novel paradigm to design high-order Bayesian network classifiers, by learning
distributional representations for feature values, as what has been done in
word embedding and graph representation learning. The learned distributional
representations are encoded with the semantic relatedness between different
features through their observed co-occurrence patterns in training data, which
then serve as a hallmark to extrapolate the occurrence probabilities of new
test samples. As a classifier design realization, we remake the K-dependence
Bayesian classifier (KDB) by extending it into a neural version, i.e.,
NeuralKDB, where a novel neural network architecture is designed to learn
distributional representations of feature values and parameterize the
conditional probabilities between interdependent features. A stochastic
gradient descent based algorithm is designed to train the NeuralKDB model
efficiently. Extensive classification experiments on 60 UCI datasets
demonstrate that the proposed NeuralKDB classifier excels in capturing
high-order feature dependencies and significantly outperforms the conventional
Bayesian network classifiers, as well as other competitive classifiers,
including two neural network based classifiers without distributional
representation learning.

</details>


### [180] [Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning](https://arxiv.org/abs/2508.11159)
*Heqiang Wang,Weihong Yang,Xiaoxiong Zhong,Jia Zhou,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: The paper addresses modality imbalance challenges in multimodal online federated learning for IoT ecosystems, presenting a rebalancing algorithm for enhanced performance.


<details>
  <summary>Details</summary>
Motivation: The field requires robust solutions to handle modality quantity and quality disparities in multimodal online federated learning for data-intensive IoT devices.

Method: The paper introduces the Modality Quantity and Quality Rebalanced (QQR) algorithm, leveraging prototype learning that operates parallelly with training.

Result: The QQR algorithm demonstrated superior performance in experiments on two multimodal datasets under challenging imbalance conditions.

Conclusion: The proposed QQR algorithm effectively manages modality imbalances, making it a significant advancement for IoT-based distributed learning systems.

Abstract: The Internet of Things (IoT) ecosystem produces massive volumes of multimodal
data from diverse sources, including sensors, cameras, and microphones. With
advances in edge intelligence, IoT devices have evolved from simple data
acquisition units into computationally capable nodes, enabling localized
processing of heterogeneous multimodal data. This evolution necessitates
distributed learning paradigms that can efficiently handle such data.
Furthermore, the continuous nature of data generation and the limited storage
capacity of edge devices demand an online learning framework. Multimodal Online
Federated Learning (MMO-FL) has emerged as a promising approach to meet these
requirements. However, MMO-FL faces new challenges due to the inherent
instability of IoT devices, which often results in modality quantity and
quality imbalance (QQI) during data collection. In this work, we systematically
investigate the impact of QQI within the MMO-FL framework and present a
comprehensive theoretical analysis quantifying how both types of imbalance
degrade learning performance. To address these challenges, we propose the
Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning
based method designed to operate in parallel with the training process.
Extensive experiments on two real-world multimodal datasets show that the
proposed QQR algorithm consistently outperforms benchmarks under modality
imbalance conditions with promising learning performance.

</details>


### [181] [A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels](https://arxiv.org/abs/2508.11180)
*Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: This paper addresses multi-view learning with missing data by proposing a semi-supervised generative model that improves classification and imputation for datasets with incomplete views and limited labels.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the challenges of applying multi-view learning to datasets, such as missing views and labels in real-life applications like multiple omics biological data.

Method: The authors propose a semi-supervised generative model that incorporates labeled and unlabeled data. The method involves maximizing the likelihood of unlabeled samples and uses cross-view mutual information in the latent space to extract shared information.

Result: The proposed model outperforms existing approaches in both predictive accuracy and imputation quality on datasets with missing views and limited labeled samples.

Conclusion: The model effectively handles semi-supervised multi-view learning scenarios, leveraging shared latent spaces and demonstrating improvements in datasets like image and multi-omics data.

Abstract: Multi-view learning is widely applied to real-life datasets, such as multiple
omics biological data, but it often suffers from both missing views and missing
labels. Prior probabilistic approaches addressed the missing view problem by
using a product-of-experts scheme to aggregate representations from present
views and achieved superior performance over deterministic classifiers, using
the information bottleneck (IB) principle. However, the IB framework is
inherently fully supervised and cannot leverage unlabeled data. In this work,
we propose a semi-supervised generative model that utilizes both labeled and
unlabeled samples in a unified framework. Our method maximizes the likelihood
of unlabeled samples to learn a latent space shared with the IB on labeled
data. We also perform cross-view mutual information maximization in the latent
space to enhance the extraction of shared information across views. Compared to
existing approaches, our model achieves better predictive and imputation
performance on both image and multi-omics data with missing views and limited
labeled samples.

</details>


### [182] [Quantum-Boosted High-Fidelity Deep Learning](https://arxiv.org/abs/2508.11190)
*Feng-ao Wang,Shaobo Chen,Yao Xuan,Junwei Liu,Qi Gao,Hongdong Zhu,Junjie Hou,Lixin Yuan,Jinyu Cheng,Chenxin Yi,Hai Wei,Yin Ma,Tao Xu,Kai Wen,Yixue Li*

Main category: cs.LG

TL;DR: The paper introduces the Quantum Boltzmann Machine-Variational Autoencoder (QBM-VAE), a hybrid quantum-classical architecture. It demonstrates its superiority in biological data modeling by employing a quantum-based Boltzmann prior instead of conventional Gaussian priors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of Gaussian priors in probabilistic deep learning which fail to capture complex, non-Gaussian landscapes in biological data. Classical methods are computationally inefficient, and quantum methods have been limited by technology constraints.

Method: The authors propose the QBM-VAE framework, leveraging quantum processors for Boltzmann distribution sampling and combining this with a deep generative model in a hybrid quantum-classical architecture.

Result: The QBM-VAE outperforms traditional models like VAE and SCVI in tasks such as omics data integration, cell-type classification, and trajectory inference, preserving complex biological structures better.

Conclusion: The study demonstrates the feasibility of leveraging quantum computing for deep learning in large-scale scientific problems, showcasing quantum advantage and offering a blueprint for future hybrid quantum AI models.

Abstract: A fundamental limitation of probabilistic deep learning is its predominant
reliance on Gaussian priors. This simplistic assumption prevents models from
accurately capturing the complex, non-Gaussian landscapes of natural data,
particularly in demanding domains like complex biological data, severely
hindering the fidelity of the model for scientific discovery. The
physically-grounded Boltzmann distribution offers a more expressive
alternative, but it is computationally intractable on classical computers. To
date, quantum approaches have been hampered by the insufficient qubit scale and
operational stability required for the iterative demands of deep learning.
Here, we bridge this gap by introducing the Quantum Boltzmann
Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable
hybrid quantum-classical architecture. Our framework leverages a quantum
processor for efficient sampling from the Boltzmann distribution, enabling its
use as a powerful prior within a deep generative model. Applied to
million-scale single-cell datasets from multiple sources, the QBM-VAE generates
a latent space that better preserves complex biological structures,
consistently outperforming conventional Gaussian-based deep learning models
like VAE and SCVI in essential tasks such as omics data integration, cell-type
classification, and trajectory inference. It also provides a typical example of
introducing a physics priori into deep learning to drive the model to acquire
scientific discovery capabilities that breaks through data limitations. This
work provides the demonstration of a practical quantum advantage in deep
learning on a large-scale scientific problem and offers a transferable
blueprint for developing hybrid quantum AI models.

</details>


### [183] [Meta-learning Structure-Preserving Dynamics](https://arxiv.org/abs/2508.11205)
*Cheng Jing,Uvini Balasuriya Mudiyanselage,Woojin Cho,Minju Jo,Anthony Gruber,Kookjin Lee*

Main category: cs.LG

TL;DR: This paper presents a modulation-based meta-learning framework for modeling dynamical systems, enabling scalable and parameter-generalizable learning while preserving physical constraints.


<details>
  <summary>Details</summary>
Motivation: Many structure-preserving models for dynamical systems require retraining for new system parameters, which is computationally costly and limits generalization to parameter-varying scenarios.

Method: The authors propose a modulation-based meta-learning framework that directly conditions structure-preserving models on compact latent representations of system parameters, using novel modulation strategies for scalability and adaptability.

Result: Experiments show that the proposed framework achieves accurate predictions in few-shot learning settings while maintaining physical constraints and effective generalization across a wide parameter space.

Conclusion: The proposed method overcomes the limits of existing meta-learning approaches and structure-preserving modeling, offering a scalable and stable solution for learning across parametric dynamical systems.

Abstract: Structure-preserving approaches to dynamics modeling have demonstrated great
potential for modeling physical systems due to their strong inductive biases
that enforce conservation laws and dissipative behavior. However, the resulting
models are typically trained for fixed system configurations, requiring
explicit knowledge of system parameters as well as costly retraining for each
new set of parameters -- a major limitation in many-query or parameter-varying
scenarios. Meta-learning offers a potential solution, but existing approaches
like optimization-based meta-learning often suffer from training instability or
limited generalization capability. Inspired by ideas from computer vision, we
introduce a modulation-based meta-learning framework that directly conditions
structure-preserving models on compact latent representations of potentially
unknown system parameters, avoiding the need for gray-box system knowledge and
explicit optimization during adaptation. Through the application of novel
modulation strategies to parametric energy-conserving and dissipative systems,
we enable scalable and generalizable learning across parametric families of
dynamical systems. Experiments on standard benchmark problems demonstrate that
our approach achieves accurate predictions in few-shot learning settings,
without compromising on the essential physical constraints necessary for
dynamical stability and effective generalization performance across parameter
space.

</details>


### [184] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: This paper discusses how causal abstraction can explain cognitive computation and its implementation, bridging ideas from philosophy, cognition, and machine learning.


<details>
  <summary>Details</summary>
Motivation: To address how systems can implement computations over representational vehicles using the framework of causal abstraction.

Method: The authors connect classical philosophical themes of computation with modern machine learning discussions, presenting a framework for computational implementation grounded in causal abstraction.

Result: The discussion draws on both deep learning and philosophy, providing insights into generalization, prediction, and the role of representation in computational systems.

Conclusion: Causal abstraction offers a compelling perspective on computational implementation, linking cognitive theories and machine learning, with representation playing a significant role in generalization and prediction.

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [185] [Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM](https://arxiv.org/abs/2508.11215)
*Zicheng Guo,Shuqi Wu,Meixing Zhu,He Guandi*

Main category: cs.LG

TL;DR: The study introduces a hybrid CNN-LSTM model predicting PM2.5 concentrations, achieving superior accuracy and generalization compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The need for accurate PM2.5 prediction against climate change's impact on environment, health, and urban management.

Method: The paper uses a CNN-LSTM hybrid model employing spatial feature extraction (CNN) and temporal dependency modeling (LSTM) on a multivariate dataset from Beijing.

Result: Achieved a competitive RMSE of 5.236, showcasing improved performance over traditional models.

Conclusion: The model exhibits potential for real-world air quality prediction, though demands high computational resources and requires optimization for diverse atmospheric inputs.

Abstract: With the intensification of global climate change, accurate prediction of air
quality indicators, especially PM2.5 concentration, has become increasingly
important in fields such as environmental protection, public health, and urban
management. To address this, we propose an air quality PM2.5 index prediction
model based on a hybrid CNN-LSTM architecture. The model effectively combines
Convolutional Neural Networks (CNN) for local spatial feature extraction and
Long Short-Term Memory (LSTM) networks for modeling temporal dependencies in
time series data. Using a multivariate dataset collected from an industrial
area in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5
concentration, temperature, dew point, pressure, wind direction, wind speed,
and precipitation -- the model predicts the average PM2.5 concentration over
6-hour intervals. Experimental results show that the model achieves a root mean
square error (RMSE) of 5.236, outperforming traditional time series models in
both accuracy and generalization. This demonstrates its strong potential in
real-world applications such as air pollution early warning systems. However,
due to the complexity of multivariate inputs, the model demands high
computational resources, and its ability to handle diverse atmospheric factors
still requires optimization. Future work will focus on enhancing scalability
and expanding support for more complex multivariate weather prediction tasks.

</details>


### [186] [Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories](https://arxiv.org/abs/2508.11235)
*William Alemanni,Arianna Burzacchi,Davide Colombi,Elena Giarratano*

Main category: cs.LG

TL;DR: The paper enhances the Interactive Voting-Based Map Matching algorithm by improving its accuracy, computational efficiency, and adaptability to diverse GPS trajectories and missing road network data.


<details>
  <summary>Details</summary>
Motivation: The research addresses the need for accurate GPS trajectory reconstruction, regardless of data quality, while expanding the algorithm’s applicability to more scenarios.

Method: The paper extends an existing algorithm by adding trajectory imputation, incorporating a distance-bounded interactive voting strategy, and integrating a custom OpenStreetMap asset for enhanced functionality.

Result: The enhanced algorithm maintains the core strengths of the original while improving accuracy for diverse scenarios, reducing computational complexity, and addressing missing road network issues.

Conclusion: The advancements make the algorithm more robust and versatile, suitable for real-world applications with varying data quality and geographic coverage.

Abstract: This paper presents an enhanced version of the Interactive Voting-Based Map
Matching algorithm, designed to efficiently process trajectories with varying
sampling rates. The main aim is to reconstruct GPS trajectories with high
accuracy, independent of input data quality. Building upon the original
algorithm, developed exclusively for aligning GPS signals to road networks, we
extend its capabilities by integrating trajectory imputation. Our improvements
also include the implementation of a distance-bounded interactive voting
strategy to reduce computational complexity, as well as modifications to
address missing data in the road network. Furthermore, we incorporate a
custom-built asset derived from OpenStreetMap, enabling this approach to be
smoothly applied in any geographic region covered by OpenStreetMap's road
network. These advancements preserve the core strengths of the original
algorithm while significantly extending its applicability to diverse real-world
scenarios.

</details>


### [187] [Graph Neural Diffusion via Generalized Opinion Dynamics](https://arxiv.org/abs/2508.11249)
*Asela Hevapathige,Asiri Wijesinghe,Ahad N. Zehmakan*

Main category: cs.LG

TL;DR: The paper introduces GODNF, a novel Generalized Opinion Dynamics Neural Framework for Graph Neural Networks (GNNs), addressing limitations in current diffusion-based GNNs such as homogeneous diffusion and limited theoretical understanding.


<details>
  <summary>Details</summary>
Motivation: To address the critical limitations in existing diffusion-based GNNs, including their static dynamics, computational depth constraints, and limited theoretical insights.

Method: GODNF unifies opinion dynamics models into a trainable diffusion mechanism, modeling heterogeneous and temporal diffusion patterns using node-specific behavior and dynamic neighborhood influence.

Result: Theoretical analysis supports GODNF's capacity for diverse convergence, and empirical evaluations on node classification and influence estimation tasks demonstrate its superiority over existing methods.

Conclusion: GODNF overcomes the limitations of previous GNNs by offering a flexible and interpretable framework, improving both performance and theoretical understanding.

Abstract: There has been a growing interest in developing diffusion-based Graph Neural
Networks (GNNs), building on the connections between message passing mechanisms
in GNNs and physical diffusion processes. However, existing methods suffer from
three critical limitations: (1) they rely on homogeneous diffusion with static
dynamics, limiting adaptability to diverse graph structures; (2) their depth is
constrained by computational overhead and diminishing interpretability; and (3)
theoretical understanding of their convergence behavior remains limited. To
address these challenges, we propose GODNF, a Generalized Opinion Dynamics
Neural Framework, which unifies multiple opinion dynamics models into a
principled, trainable diffusion mechanism. Our framework captures heterogeneous
diffusion patterns and temporal dynamics via node-specific behavior modeling
and dynamic neighborhood influence, while ensuring efficient and interpretable
message propagation even at deep layers. We provide a rigorous theoretical
analysis demonstrating GODNF's ability to model diverse convergence
configurations. Extensive empirical evaluations of node classification and
influence estimation tasks confirm GODNF's superiority over state-of-the-art
GNNs.

</details>


### [188] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: The paper introduces a framework to derive fair classifiers from closed-weight LLMs via prompting, enabling post-hoc fair classification for high-stakes applications.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring group fairness in high-stakes applications using closed-weight LLMs under the in-context learning paradigm, given the limitations of traditional fairness approaches.

Method: The authors propose a framework where prompts are designed to extract features (e.g., token log probabilities) from closed-weight LLMs to train lightweight fair classifiers using a fairness criterion.

Result: The framework demonstrates strong accuracy-fairness tradeoffs on five datasets, outperforming classifiers trained via embedding-based head-tuning or raw features.

Conclusion: The proposed approach is effective, data-efficient, and suitable for deriving fair classifiers from both open and closed-weight LLMs, leveraging their probabilistic predictions.

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [189] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: The paper explores adversarial robustness in Spiking Neural Networks (SNNs) using a temporal ensembling approach, introducing Robust Temporal self-Ensemble (RTE) to mitigate vulnerabilities. Extensive experiments demonstrate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient and brain-inspired computing models, but their vulnerability to adversarial attacks remains an open challenge.

Method: The study uses temporal ensembling to view SNNs as evolving sub-networks across timesteps, proposing RTE, a unified training framework that combines robustness improvement for sub-networks and reduction of temporal transferability for adversarial attacks.

Result: Experimental results show that RTE improves the robust-accuracy trade-off across benchmarks and reshapes decision boundaries to make them more resilient.

Conclusion: Temporal structure plays a critical role in adversarial learning, and the RTE framework establishes a principled approach for enhancing adversarial robustness in SNNs.

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [190] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: The paper introduces HS-GPPT, a model for graph pre-training and prompt-tuning that effectively transfers spectral knowledge under varying graph homophily using a hybrid spectral filter and local-global contrastive learning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing graph pre-training methods that struggle with diverse spectral distributions in real-world graphs, especially under limited supervision.

Method: The HS-GPPT uses a hybrid spectral filter, local-global contrastive learning to acquire spectral knowledge, and prompt graphs to align spectral distributions for effective transfer.

Result: Experimental validation shows that HS-GPPT achieves effective knowledge transfer in both transductive and inductive learning scenarios.

Conclusion: Aligning spectral filters with the intrinsic spectrum of downstream graphs is crucial for optimal knowledge transfer. HS-GPPT addresses this and shows effectiveness in diverse tasks.

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [191] [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](https://arxiv.org/abs/2508.11338)
*Prathamesh Devadiga,Yashmitha Shailesh*

Main category: cs.LG

TL;DR: RegimeNAS is a novel architecture search framework that improves cryptocurrency trading performance by incorporating market regime awareness.


<details>
  <summary>Details</summary>
Motivation: Static deep learning models are insufficient for dynamic financial environments, prompting the need for adaptive, regime-aware models.

Method: RegimeNAS introduces regime-aware neural modules, a Bayesian search space, a multi-objective loss function, and multi-head attention for regime identification.

Result: Rigorous evaluation on cryptocurrency data shows RegimeNAS outperforms benchmarks, reducing Mean Absolute Error by 80.3% and converging significantly faster.

Conclusion: Embedding domain-specific knowledge like market regimes within the NAS process is essential for robust and adaptive financial models.

Abstract: We introduce RegimeNAS, a novel differentiable architecture search framework
specifically designed to enhance cryptocurrency trading performance by
explicitly integrating market regime awareness. Addressing the limitations of
static deep learning models in highly dynamic financial environments, RegimeNAS
features three core innovations: (1) a theoretically grounded Bayesian search
space optimizing architectures with provable convergence properties; (2)
specialized, dynamically activated neural modules (Volatility, Trend, and Range
blocks) tailored for distinct market conditions; and (3) a multi-objective loss
function incorporating market-specific penalties (e.g., volatility matching,
transition smoothness) alongside mathematically enforced Lipschitz stability
constraints. Regime identification leverages multi-head attention across
multiple timeframes for improved accuracy and uncertainty estimation. Rigorous
empirical evaluation on extensive real-world cryptocurrency data demonstrates
that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving
an 80.3% Mean Absolute Error reduction compared to the best traditional
recurrent baseline and converging substantially faster (9 vs. 50+ epochs).
Ablation studies and regime-specific analysis confirm the critical contribution
of each component, particularly the regime-aware adaptation mechanism. This
work underscores the imperative of embedding domain-specific knowledge, such as
market regimes, directly within the NAS process to develop robust and adaptive
models for challenging financial applications.

</details>


### [192] [Conformal Prediction Meets Long-tail Classification](https://arxiv.org/abs/2508.11345)
*Shuqi Liu,Jianguo Huang,Luke Ong*

Main category: cs.LG

TL;DR: This paper introduces Tail-Aware Conformal Prediction (TACP) and its extension, soft TACP (sTACP), to address imbalanced class coverage in Conformal Prediction under long-tail label distributions.


<details>
  <summary>Details</summary>
Motivation: Existing Conformal Prediction methods fail to provide reliable prediction coverage for minority tail classes under long-tail distributions, which jeopardizes the utility of prediction sets for those classes.

Method: The authors propose TACP to reduce the head-tail coverage gap by leveraging long-tail structures and extend it with sTACP using a reweighting mechanism for improved class-wise coverage balance.

Result: Theoretical analysis confirms TACP reduces head-tail coverage gap, while experiments on various long-tail benchmark datasets validate the effectiveness of both TACP and sTACP methods.

Conclusion: TACP and sTACP enhance the reliability and balance of coverage in Conformal Prediction, addressing issues posed by long-tail label distributions.

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
that converts a pretrained model's point prediction into a prediction set, with
the set size reflecting the model's confidence. Although existing CP methods
are guaranteed to achieve marginal coverage, they often exhibit imbalanced
coverage across classes under long-tail label distributions, tending to over
cover the head classes at the expense of under covering the remaining tail
classes. This under coverage is particularly concerning, as it undermines the
reliability of the prediction sets for minority classes, even with coverage
ensured on average. In this paper, we propose the Tail-Aware Conformal
Prediction (TACP) method to mitigate the under coverage of the tail classes by
utilizing the long-tail structure and narrowing the head-tail coverage gap.
Theoretical analysis shows that it consistently achieves a smaller head-tail
coverage gap than standard methods. To further improve coverage balance across
all classes, we introduce an extension of TACP: soft TACP (sTACP) via a
reweighting mechanism. The proposed framework can be combined with various
non-conformity scores, and experiments on multiple long-tail benchmark datasets
demonstrate the effectiveness of our methods.

</details>


### [193] [NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models](https://arxiv.org/abs/2508.11348)
*Xiaohan Bi,Binhang Qi,Hailong Sun,Xiang Gao,Yue Yu,Xiaojun Liang*

Main category: cs.LG

TL;DR: NeMo is a new approach for neural network modularization focusing on training at the neuron level, achieving better efficiency and scalability, especially for large-scale Transformer models.


<details>
  <summary>Details</summary>
Motivation: The inefficiencies of indiscriminate model reuse and limitations of current modularizing-while-training methods prompted the need for scalable, generalizable modularization methods applicable to diverse neural networks.

Method: NeMo applies a neuron-level modularization using a contrastive learning-based training method and a composite loss function, making it suitable for both CNNs and Transformers.

Result: Experiments reveal NeMo achieves an average accuracy increase of 1.72% in module classification, a 58.10% reduction in module size, and demonstrates efficacy across multiple model types and datasets.

Conclusion: NeMo offers a scalable, generalizable solution for DNN modularization, showing promise in both research and practical applications, particularly for large-scale networks like Transformers.

Abstract: With the growing incorporation of deep neural network (DNN) models into
modern software systems, the prohibitive construction costs have become a
significant challenge. Model reuse has been widely applied to reduce training
costs, but indiscriminately reusing entire models may incur significant
inference overhead. Consequently, DNN modularization has gained attention,
enabling module reuse by decomposing DNN models. The emerging
modularizing-while-training (MwT) paradigm, which incorporates modularization
into training, outperforms modularizing-after-training approaches. However,
existing MwT methods focus on small-scale CNN models at the convolutional
kernel level and struggle with diverse DNNs and large-scale models,
particularly Transformer-based models. To address these limitations, we propose
NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron
level fundamental component common to all DNNs-ensuring applicability to
Transformers and various architectures. We design a contrastive learning-based
modular training method with an effective composite loss function, enabling
scalability to large-scale models. Comprehensive experiments on two
Transformer-based models and four CNN models across two classification datasets
demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show
average gains of 1.72% in module classification accuracy and 58.10% reduction
in module size, demonstrating efficacy across both CNN and large-scale
Transformer-based models. A case study on open-source projects shows NeMo's
potential benefits in practical scenarios, offering a promising approach for
scalable and generalizable DNN modularization.

</details>


### [194] [A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts](https://arxiv.org/abs/2508.11349)
*Angela John,Selvyn Allotey,Till Koebe,Alexandra Tyukavina,Ingmar Weber*

Main category: cs.LG

TL;DR: The paper discusses a global dataset for afforestation and reforestation projects validated with satellite imagery, introducing a Location Data Integrity Score (LDIS) to evaluate data integrity.


<details>
  <summary>Details</summary>
Motivation: Address issues of data reliability and project integrity in afforestation and reforestation projects, as their effectiveness is often self-reported or inadequately validated.

Method: Created a global dataset of 1,289,068 planting sites from 45,628 projects over 33 years, leveraging satellite imagery and secondary data to assess data integrity.

Result: Introduced the LDIS indicator, revealing that 79% of georeferenced sites fail in location integrity and 15% of projects lack machine-readable geospatial data.

Conclusion: The dataset contributes to greater accountability in carbon offset markets and provides valuable data for computational applications such as computer vision.

Abstract: Afforestation and reforestation are popular strategies for mitigating climate
change by enhancing carbon sequestration. However, the effectiveness of these
efforts is often self-reported by project developers, or certified through
processes with limited external validation. This leads to concerns about data
reliability and project integrity. In response to increasing scrutiny of
voluntary carbon markets, this study presents a dataset on global afforestation
and reforestation efforts compiled from primary (meta-)information and
augmented with time-series satellite imagery and other secondary data. Our
dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.
Since any remote sensing-based validation effort relies on the integrity of a
planting site's geographic boundary, this dataset introduces a standardized
assessment of the provided site-level location information, which we summarize
in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity
Score. We find that approximately 79\% of the georeferenced planting sites
monitored fail on at least 1 out of 10 LDIS indicators, while 15\% of the
monitored projects lack machine-readable georeferenced data in the first place.
In addition to enhancing accountability in the voluntary carbon market, the
presented dataset also holds value as training data for e.g. computer
vision-related tasks with millions of linked Sentinel-2 and Planetscope
satellite images.

</details>


### [195] [Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning](https://arxiv.org/abs/2508.11353)
*Han Zhou,Hongpeng Yin,Xuanhong Deng,Yuyu Huang,Hao Ren*

Main category: cs.LG

TL;DR: The paper presents the Harmonized Gradient Descent (HGD) algorithm for online learning of imbalanced data streams through gradient norm balancing.


<details>
  <summary>Details</summary>
Motivation: Imbalanced data streams, common in real-world sequential datasets, pose learning challenges by causing under-fitting in minor classes.

Method: The authors propose the Harmonized Gradient Descent (HGD), a training modification method that balances gradient norms across classes during gradient descent.

Result: HGD achieves a sub-linear regret bound in theoretical analysis and outperforms existing online imbalance learning methods in experimental evaluations.

Conclusion: HGD offers an efficient, effective, and broadly applicable solution to imbalanced data stream learning without requiring extra parameters, data buffering, or prior knowledge.

Abstract: Many real-world data are sequentially collected over time and often exhibit
skewed class distributions, resulting in imbalanced data streams. While
existing approaches have explored several strategies, such as resampling and
reweighting, for imbalanced data stream learning, our work distinguishes itself
by addressing the imbalance problem through training modification, particularly
focusing on gradient descent techniques. We introduce the harmonized gradient
descent (HGD) algorithm, which aims to equalize the norms of gradients across
different classes. By ensuring the gradient norm balance, HGD mitigates
under-fitting for minor classes and achieves balanced online learning. Notably,
HGD operates in a streamlined implementation process, requiring no data-buffer,
extra parameters, or prior knowledge, making it applicable to any learning
models utilizing gradient descent for optimization. Theoretical analysis, based
on a few common and mild assumptions, shows that HGD achieves a satisfied
sub-linear regret bound. The proposed algorithm are compared with the commonly
used online imbalance learning methods under several imbalanced data stream
scenarios. Extensive experimental evaluations demonstrate the efficiency and
effectiveness of HGD in learning imbalanced data streams.

</details>


### [196] [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)
*Jia Liu,ChangYi He,YingQiao Lin,MingMin Yang,FeiYang Shen,ShaoGuo Liu,TingTing Gao*

Main category: cs.LG

TL;DR: This paper introduces entropy-based strategies (ETMR and EAR) to boost test-time reinforcement learning efficiency and performance, achieving significant improvement on open-domain reasoning tasks while reducing computational cost.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with unsupervised adaptability, and test-time reinforcement learning aims to address this but faces issues like high inference costs and overconfidence.

Method: The authors propose entropy-enhancing strategies, namely Entropy-fork Tree Majority Rollout (ETMR) and Entropy-based Advantage Reshaping (EAR), to improve exploration-exploitation balance during test-time reinforcement learning.

Result: Their method achieves a 68% relative improvement on the AIME 2024 benchmark in the Pass at 1 metric while using only 60% of the rollout token budget.

Conclusion: The proposed approach significantly enhances test-time reinforcement learning by optimizing efficiency, diversity, and robustness, pushing the boundaries of unsupervised learning for reasoning tasks.

Abstract: Recent advancements in Large Language Models have yielded significant
improvements in complex reasoning tasks such as mathematics and programming.
However, these models remain heavily dependent on annotated data and exhibit
limited adaptability in unsupervised scenarios. To address these limitations,
test-time reinforcement learning (TTRL) has been proposed, which enables
self-optimization by leveraging model-generated pseudo-labels. Despite its
promise, TTRL faces several key challenges, including high inference costs due
to parallel rollouts and early-stage estimation bias that fosters
overconfidence, reducing output diversity and causing performance plateaus. To
address these challenges, we introduce an entropy-based mechanism to enhance
the exploration-exploitation balance in test-time reinforcement learning
through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and
Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our
approach enables Llama3.1-8B to achieve a 68 percent relative improvement in
Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of
the rollout tokens budget. This highlights our method's ability to effectively
optimize the trade-off between inference efficiency, diversity, and estimation
robustness, thereby advancing unsupervised reinforcement learning for
open-domain reasoning tasks.

</details>


### [197] [PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding](https://arxiv.org/abs/2508.11357)
*Changhong Jing,Yan Liu,Shuqiang Wang,Bruce X. B. Yu,Gong Chen,Zhejing Hu,Zhi Zhang,Yanyan Shen*

Main category: cs.LG

TL;DR: This paper introduces PTSM, a framework for robust cross-subject EEG decoding using disentangled neural representations and dual-branch spatio-temporal masking.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of variability between EEG data from different subjects in brain-computer interface research and create subject-invariant representations for EEG decoding.

Method: PTSM uses a dual-branch masking mechanism to learn spatio-temporal patterns for personalized and shared representations, factorizes masks to modulate EEG patterns, and applies information-theoretic constraints to enforce disentanglement of task-related and subject-related embeddings.

Result: PTSM achieves strong zero-shot generalization and outperforms existing approaches in cross-subject motor imagery EEG decoding without requiring subject-specific calibration.

Conclusion: Disentangled neural representations offer an effective solution for personalized and transferable EEG decoding in non-stationary neurophysiological contexts.

Abstract: Cross-subject electroencephalography (EEG) decoding remains a fundamental
challenge in brain-computer interface (BCI) research due to substantial
inter-subject variability and the scarcity of subject-invariant
representations. This paper proposed PTSM (Physiology-aware and Task-invariant
Spatio-temporal Modeling), a novel framework for interpretable and robust EEG
decoding across unseen subjects. PTSM employs a dual-branch masking mechanism
that independently learns personalized and shared spatio-temporal patterns,
enabling the model to preserve individual-specific neural characteristics while
extracting task-relevant, population-shared features. The masks are factorized
across temporal and spatial dimensions, allowing fine-grained modulation of
dynamic EEG patterns with low computational overhead. To further address
representational entanglement, PTSM enforces information-theoretic constraints
that decompose latent embeddings into orthogonal task-related and
subject-related subspaces. The model is trained end-to-end via a
multi-objective loss integrating classification, contrastive, and
disentanglement objectives. Extensive experiments on cross-subject motor
imagery datasets demonstrate that PTSM achieves strong zero-shot
generalization, outperforming state-of-the-art baselines without
subject-specific calibration. Results highlight the efficacy of disentangled
neural representations for achieving both personalized and transferable
decoding in non-stationary neurophysiological settings.

</details>


### [198] [Fusing Rewards and Preferences in Reinforcement Learning](https://arxiv.org/abs/2508.11363)
*Sadegh Khorasani,Saber Salehkaleybar,Negar Kiyavash,Matthias Grossglauser*

Main category: cs.LG

TL;DR: The paper introduces the Dual-Feedback Actor (DFA) algorithm for reinforcement learning, combining individual rewards and pairwise preferences to improve learning and training stability.


<details>
  <summary>Details</summary>
Motivation: The research aims to address limitations in reinforcement learning by incorporating human feedback and preferences, achieving better stability and efficiency compared to standard methods.

Method: DFA fuses rewards and preferences into a unified update rule, directly modeling preference probability using policy log-probabilities, without requiring a separate reward model. Preferences can be derived from human annotation or synthesized from Q-values in a replay buffer.

Result: DFA exhibits comparable or superior performance to Soft Actor-Critic (SAC) in six control environments, demonstrates stability in training, and surpasses RLHF baselines in a GridWorld setting using semi-synthetic preference data.

Conclusion: The algorithm offers a promising alternative to reward-modeling approaches by effectively leveraging preferences and achieving stable and high-performing reinforcement learning outcomes.

Abstract: We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that
fuses both individual rewards and pairwise preferences (if available) into a
single update rule. DFA uses the policy's log-probabilities directly to model
the preference probability, avoiding a separate reward-modeling step.
Preferences can be provided by human-annotators (at state-level or
trajectory-level) or be synthesized online from Q-values stored in an
off-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing
DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)
policy. Our simulation results show that DFA trained on generated preferences
matches or exceeds SAC on six control environments and demonstrates a more
stable training process. With only a semi-synthetic preference dataset under
Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement
learning from human feedback (RLHF) baselines in a stochastic GridWorld and
approaches the performance of an oracle with true rewards.

</details>


### [199] [Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization](https://arxiv.org/abs/2508.11365)
*Jayanta Mandi,Ali İrfan Mahmutoğulları,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: Existing decision-focused learning (DFL) methods for linear programs (LPs) face issues with zero gradients during optimization. This study proposes using surrogate losses alongside differentiable optimization techniques to improve regret minimization and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge in DFL methods where zero gradients make it difficult to optimize linear program-based problems effectively.

Method: The authors recommend minimizing surrogate losses instead of directly minimizing regret, even when using differentiable optimization layers like DYS-Net that approximate solutions and compute gradients efficiently.

Result: Minimizing surrogate losses enables differentiable optimization layers like DYS-Net to achieve competitive or superior decision regret, while significantly reducing training time.

Conclusion: Surrogate losses can enhance the performance and efficiency of decision-focused learning approaches for linear programs, making them a practical alternative for achieving low regret.

Abstract: Decision-focused learning (DFL) trains a machine learning (ML) model to
predict parameters of an optimization problem, to directly minimize decision
regret, i.e., maximize decision quality. Gradient-based DFL requires computing
the derivative of the solution to the optimization problem with respect to the
predicted parameters. However, for many optimization problems, such as linear
programs (LPs), the gradient of the regret with respect to the predicted
parameters is zero almost everywhere. Existing gradient-based DFL approaches
for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP
into a differentiable optimization problem by adding a quadratic regularizer
and then minimizing the regret directly or (b) minimizing surrogate losses that
have informative (sub)gradients. In this paper, we show that the former
approach still results in zero gradients, because even after smoothing the
regret remains constant across large regions of the parameter space. To address
this, we propose minimizing surrogate losses -- even when a differentiable
optimization layer is used and regret can be minimized directly. Our
experiments demonstrate that minimizing surrogate losses allows differentiable
optimization layers to achieve regret comparable to or better than
surrogate-loss based DFL methods. Further, we demonstrate that this also holds
for DYS-Net, a recently proposed differentiable optimization technique for LPs,
that computes approximate solutions and gradients through operations that can
be performed using feedforward neural network layers. Because DYS-Net executes
the forward and the backward pass very efficiently, by minimizing surrogate
losses using DYS-Net, we are able to attain regret on par with the
state-of-the-art while reducing training time by a significant margin.

</details>


### [200] [A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting](https://arxiv.org/abs/2508.11390)
*Michael Banf,Dominik Filipiak,Max Schattauer,Liliya Imasheva*

Main category: cs.LG

TL;DR: The paper introduces a new structural lifting strategy leveraging Forman-Ricci curvature to address information distortion in long-distance message passing within graph neural networks.


<details>
  <summary>Details</summary>
Motivation: Graph learning often struggles with over-squashing when dealing with long-distance interactions. The paper aims to use higher-order topological domains to capture more complex relationships.

Method: The proposed method applies a lifting strategy based on Forman-Ricci curvature, to transform basic graph representations into higher-order structures like hyperedges.

Result: The approach identifies and leverages graph backbones to more effectively model long-range information flows and clusters.

Conclusion: Forman-Ricci curvature enables improved modeling of graph geometries, mitigating the over-squashing issue and enhancing GNN's ability to handle long-distance communication.

Abstract: Graph Neural Networks are highly effective at learning from relational data,
leveraging node and edge features while maintaining the symmetries inherent to
graph structures. However, many real-world systems, such as social or
biological networks, exhibit complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Geometric and Topological Deep Learning addresses this challenge by introducing
methods that utilize and benefit from higher-order structures. Central to TDL
is the concept of lifting, which transforms data representations from basic
graph forms to more expressive topologies before the application of GNN models
for learning. In this work, we propose a structural lifting strategy using
Forman-Ricci curvature, which defines an edge-based network characteristic
based on Riemannian geometry. Curvature reveals local and global properties of
a graph, such as a network's backbones, i.e. coarse, structure-preserving graph
geometries that form connections between major communities - most suitably
represented as hyperedges to model information flows between clusters across
large distances in the network. To this end, our approach provides a remedy to
the problem of information distortion in message passing across long distances
and graph bottlenecks - a phenomenon known in graph learning as over-squashing.

</details>


### [201] [On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting](https://arxiv.org/abs/2508.11408)
*Wenhao Zhang,Yuexiang Xie,Yuchang Sun,Yanxi Chen,Guoyin Wang,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: The paper introduces CHORD, a framework to integrate supervised fine-tuning (SFT) dynamically with on-policy reinforcement learning (RL) to avoid overfitting and model disruption, yielding significant improvements in training efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Current techniques combining supervised fine-tuning (SFT) and reinforcement learning (RL) risk disrupting model patterns and overfitting to expert data. This necessitates better integration for balancing pre-trained expertise with on-policy exploration.

Method: The CHORD framework reframes SFT as an auxiliary objective integrated into on-policy RL through dynamic weighting. It uses a dual-control mechanism: a global coefficient for balancing off-policy and on-policy data, and token-wise weighting for precise expert data learning.

Result: CHORD demonstrates significant improvements in stability and efficiency of learning, showing superiority over baseline methods according to extensive experimentation on commonly used benchmarks.

Conclusion: By effectively integrating off-policy data with on-policy training dynamically, CHORD advances the field with meaningful empirical enhancements and offers a promising framework for refining large language models. Its open-source implementation invites further exploration.

Abstract: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established model
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for the Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data's influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from expert
tokens, which preserves on-policy exploration and mitigates disruption from
off-policy data. We conduct extensive experiments on widely used benchmarks,
providing empirical evidence that CHORD achieves a stable and efficient
learning process. By effectively harmonizing off-policy expert data with
on-policy exploration, CHORD demonstrates significant improvements over
baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.

</details>


### [202] [Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space](https://arxiv.org/abs/2508.11424)
*Yinghua Yao,Yuangang Pan,Xixian Chen*

Main category: cs.LG

TL;DR: LEAD introduces a framework for efficient sequence-structure co-design optimization in antibody modeling, outperforming existing methods in developability property improvement.


<details>
  <summary>Details</summary>
Motivation: Current methods of optimizing antibody complementarity-determining regions (CDRs) involve inefficient searches in raw data space, leading to high evaluation costs.

Method: LEAD uses a shared latent space approach for co-optimization of sequence and structure, ensuring design synchronization across modalities and employing a black-box guidance strategy for non-differentiable property evaluators.

Result: LEAD demonstrates improved performance in single and multi-property objectives, reducing query consumption by 50%, and exceeding baseline optimization methods.

Conclusion: LEAD provides a more efficient and synchronized approach to sequence-structure optimization in antibody modeling, making it suitable for real-world, non-differentiable evaluators.

Abstract: Advancements in deep generative models have enabled the joint modeling of
antibody sequence and structure, given the antigen-antibody complex as context.
However, existing approaches for optimizing complementarity-determining regions
(CDRs) to improve developability properties operate in the raw data space,
leading to excessively costly evaluations due to the inefficient search
process. To address this, we propose LatEnt blAck-box Design (LEAD), a
sequence-structure co-design framework that optimizes both sequence and
structure within their shared latent space. Optimizing shared latent codes can
not only break through the limitations of existing methods, but also ensure
synchronization of different modality designs. Particularly, we design a
black-box guidance strategy to accommodate real-world scenarios where many
property evaluators are non-differentiable. Experimental results demonstrate
that our LEAD achieves superior optimization performance for both single and
multi-property objectives. Notably, LEAD reduces query consumption by a half
while surpassing baseline methods in property optimization. The code is
available at https://github.com/EvaFlower/LatEnt-blAck-box-Design.

</details>


### [203] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: The paper proposes leveraging contraction theory in Convolutional Neural ODEs (NODEs) to improve robustness against noise and adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: To address the fragility of neural networks to noise and adversarial attacks by exploring Convolutional Neural ODEs and contraction theory.

Method: Introduced contractivity by adding regularization terms involving the Jacobian or specific weight regularizations for dynamical systems with slope-restricted activation functions.

Result: Demonstrated improved robustness of the model on MNIST and FashionMNIST classification tasks under various noise and attacks.

Conclusion: Contraction-based regularization enhances the robustness of Convolutional NODEs while maintaining operational efficiency.

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


### [204] [Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity](https://arxiv.org/abs/2508.11436)
*Mayssa Soussia,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: The paper introduces mCOCO, a framework using Reservoir Computing (RC) to create functional Connectional Brain Templates (CBTs) that incorporate multi-sensory inputs for cognitive modeling, outperforming existing methods in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating CBTs face challenges like poor interpretability, high computational costs, and limited focus on structural and topological aspects, without incorporating cognitive processing.

Method: The mCOCO framework operates in two phases: mapping BOLD signals to create individual functional connectomes aggregated into group CBTs, and integrating multi-sensory inputs to infuse cognitive traits using a cognitive reservoir.

Result: mCOCO significantly outperforms GNN-based CBTs in centeredness, discriminativeness, topological soundness, and multi-sensory memory retention.

Conclusion: mCOCO provides an interpretable, efficient approach to CBT creation, successfully modeling cognitive brain dynamics and outperforming existing techniques in various critical metrics.

Abstract: The generation of connectional brain templates (CBTs) has recently garnered
significant attention for its potential to identify unique connectivity
patterns shared across individuals. However, existing methods for CBT learning
such as conventional machine learning and graph neural networks (GNNs) are
hindered by several limitations. These include: (i) poor interpretability due
to their black-box nature, (ii) high computational cost, and (iii) an exclusive
focus on structure and topology, overlooking the cognitive capacity of the
generated CBT. To address these challenges, we introduce mCOCO (multi-sensory
COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)
to learn population-level functional CBT from BOLD
(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow
for tracking state changes over time, enhancing interpretability and enabling
the modeling of brain-like dynamics, as demonstrated in prior literature. By
integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO
captures not only structure and topology but also how brain regions process
information and adapt to cognitive tasks such as sensory processing, all in a
computationally efficient manner. Our mCOCO framework consists of two phases:
(1) mapping BOLD signals into the reservoir to derive individual functional
connectomes, which are then aggregated into a group-level CBT - an approach, to
the best of our knowledge, not previously explored in functional connectivity
studies - and (2) incorporating multi-sensory inputs through a cognitive
reservoir, endowing the CBT with cognitive traits. Extensive evaluations show
that our mCOCO-based template significantly outperforms GNN-based CBT in terms
of centeredness, discriminativeness, topological soundness, and multi-sensory
memory retention. Our source code is available at
https://github.com/basiralab/mCOCO.

</details>


### [205] [Informative Post-Hoc Explanations Only Exist for Simple Functions](https://arxiv.org/abs/2508.11441)
*Eric Günther,Balázs Szabados,Robi Bhattacharjee,Sebastian Bordt,Ulrike von Luxburg*

Main category: cs.LG

TL;DR: This paper establishes that many popular post-hoc explanation algorithms lack informativeness when applied to complex machine learning models and proposes a framework to assess the informativeness of explanations.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the uncertainty around the theoretical guarantees of post-hoc explanation algorithms for complex machine learning models and establish conditions under which these algorithms can be useful.

Method: The paper introduces a learning-theory-based framework to determine the informativeness of explanations, defining informativeness in terms of reducing the complexity of plausible decision functions.

Result: The study demonstrates that popular explanation algorithms (gradient, counterfactual, SHAP, anchor explanations) fail to be informative for certain classes of decision functions and derives stricter conditions for them to become informative.

Conclusion: The findings challenge the assumption of universal applicability of explanation algorithms, providing guidance on how to improve their utility for auditing, regulation, and high-risk AI applications.

Abstract: Many researchers have suggested that local post-hoc explanation algorithms
can be used to gain insights into the behavior of complex machine learning
models. However, theoretical guarantees about such algorithms only exist for
simple decision functions, and it is unclear whether and under which
assumptions similar results might exist for complex models. In this paper, we
introduce a general, learning-theory-based framework for what it means for an
explanation to provide information about a decision function. We call an
explanation informative if it serves to reduce the complexity of the space of
plausible decision functions. With this approach, we show that many popular
explanation algorithms are not informative when applied to complex decision
functions, providing a rigorous mathematical rejection of the idea that it
should be possible to explain any model. We then derive conditions under which
different explanation algorithms become informative. These are often stronger
than what one might expect. For example, gradient explanations and
counterfactual explanations are non-informative with respect to the space of
differentiable functions, and SHAP and anchor explanations are not informative
with respect to the space of decision trees. Based on these results, we discuss
how explanation algorithms can be modified to become informative. While the
proposed analysis of explanation algorithms is mathematical, we argue that it
holds strong implications for the practical applicability of these algorithms,
particularly for auditing, regulation, and high-risk applications of AI.

</details>


### [206] [Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection](https://arxiv.org/abs/2508.11504)
*Andrea Castellani,Zacharias Papadovasilakis,Giorgos Papoutsoglou,Mary Cole,Brian Bautsch,Tobias Rodemann,Ioannis Tsamardinos,Angela Harden*

Main category: cs.LG

TL;DR: This study analyzed over 3 million vehicle occupants involved in accidents in Ohio from 2017-2022, using AutoML and explainable AI to predict crash severity and identify key risk factors.


<details>
  <summary>Details</summary>
Motivation: The study aims to reduce severe motor vehicle crash injuries and deaths using data-driven insights into crash severity.

Method: The research employed JADBio AutoML to create predictive models for crash severity, using SHAP for interpretability, and applying Ridge Logistic Regression for final predictions.

Result: The final model achieved 85.6% AUC-ROC on training and 84.9% on test datasets. It identified 17 influential crash severity predictors, with environmental and contextual factors being more significant than drug/alcohol impairment.

Conclusion: This study provides a scalable, interpretable framework for traffic policy, emphasizing data-informed interventions to improve road safety under Vision Zero goals.

Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide,
necessitating data-driven approaches to understand and mitigate crash severity.
This study introduces a curated dataset of more than 3 million people involved
in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3
million vehicle-level records for predictive analysis. The primary contribution
is a transparent and reproducible methodology that combines Automated Machine
Learning (AutoML) and explainable artificial intelligence (AI) to identify and
interpret key risk factors associated with severe crashes. Using the JADBio
AutoML platform, predictive models were constructed to distinguish between
severe and non-severe crash outcomes. The models underwent rigorous feature
selection across stratified training subsets, and their outputs were
interpreted using SHapley Additive exPlanations (SHAP) to quantify the
contribution of individual features. A final Ridge Logistic Regression model
achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test
set, with 17 features consistently identified as the most influential
predictors. Key features spanned demographic, environmental, vehicle, human,
and operational categories, including location type, posted speed, minimum
occupant age, and pre-crash action. Notably, certain traditionally emphasized
factors, such as alcohol or drug impairment, were less influential in the final
model compared to environmental and contextual variables. Emphasizing
methodological rigor and interpretability over mere predictive performance,
this study offers a scalable framework to support Vision Zero with aligned
interventions and advanced data-informed traffic safety policy.

</details>


### [207] [Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies](https://arxiv.org/abs/2508.11513)
*Fanzhen Liu,Xiaoxiao Ma,Jian Yang,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Quan Z. Sheng,Jia Wu*

Main category: cs.LG

TL;DR: The paper introduces GraphOracle, a self-explainable GNN framework, addressing the lack of effective class-level explanations seen in prior GNN models.


<details>
  <summary>Details</summary>
Motivation: To enhance the interpretability of GNNs for practical and fair deployment, particularly at the class-level, which existing models have not adequately addressed.

Method: GraphOracle incorporates joint learning of a classifier and discriminative structured subgraphs per class, utilizes novel integrated training approaches, and employs entropy-regularized subgraph selection for scalability.

Result: GraphOracle demonstrated superior fidelity, explainability, and scalability over previous methods across multiple graph classification tasks while retroactively evaluating older methods' shortcomings.

Conclusion: GraphOracle represents a practical, efficient, and scalable solution for generating faithful class-level explanations in GNNs, bridging crucial gaps in interpretability.

Abstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to
ensure their safe and fair deployment. Recent work has introduced
self-explainable GNNs that generate explanations as part of training, improving
both faithfulness and efficiency. Some of these models, such as ProtGNN and
PGIB, learn class-specific prototypes, offering a potential pathway toward
class-level explanations. However, their evaluations focus solely on
instance-level explanations, leaving open the question of whether these
prototypes meaningfully generalize across instances of the same class. In this
paper, we introduce GraphOracle, a novel self-explainable GNN framework
designed to generate and evaluate class-level explanations for GNNs. Our model
jointly learns a GNN classifier and a set of structured, sparse subgraphs that
are discriminative for each class. We propose a novel integrated training that
captures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependencies
efficiently and faithfully, validated through a masking-based evaluation
strategy. This strategy enables us to retroactively assess whether prior
methods like ProtGNN and PGIB deliver effective class-level explanations. Our
results show that they do not. In contrast, GraphOracle achieves superior
fidelity, explainability, and scalability across a range of graph
classification tasks. We further demonstrate that GraphOracle avoids the
computational bottlenecks of previous methods$\unicode{x2014}$like Monte Carlo
Tree Search$\unicode{x2014}$by using entropy-regularized subgraph selection and
lightweight random walk extraction, enabling faster and more scalable training.
These findings position GraphOracle as a practical and principled solution for
faithful class-level self-explainability in GNNs.

</details>


### [208] [DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality](https://arxiv.org/abs/2508.11514)
*Qitong Chu,Yufeng Yue,Danya Yao,Huaxin Pei*

Main category: cs.LG

TL;DR: The paper introduces a framework for generating diverse and critical testing scenarios for decision-making agents by integrating scenario parameter space and agent behavior space analyses.


<details>
  <summary>Details</summary>
Motivation: The increasing deployment of decision-making agents in dynamic settings demands effective safety verification tools, specifically testing scenarios that balance both diversity and criticality.

Method: A dual-space guided testing framework was developed, using hierarchical dimensionality reduction, local perturbation, global exploration, and agent-environment interaction to generate critical and diverse testing scenarios while proactively avoiding local optima traps.

Result: Experimental results demonstrated a 56.23% improvement in critical scenario generation and superior diversity over five decision-making agent tests compared to current state-of-the-art methods.

Conclusion: The proposed framework is effective for safety verification in dynamic environments, achieving a balanced generation of diverse and critical scenarios while outperforming baseline methods.

Abstract: The growing deployment of decision-making agents in dynamic environments
increases the demand for safety verification. While critical testing scenario
generation has emerged as an appealing verification methodology, effectively
balancing diversity and criticality remains a key challenge for existing
methods, particularly due to local optima entrapment in high-dimensional
scenario spaces. To address this limitation, we propose a dual-space guided
testing framework that coordinates scenario parameter space and agent behavior
space, aiming to generate testing scenarios considering diversity and
criticality. Specifically, in the scenario parameter space, a hierarchical
representation framework combines dimensionality reduction and
multi-dimensional subspace evaluation to efficiently localize diverse and
critical subspaces. This guides dynamic coordination between two generation
modes: local perturbation and global exploration, optimizing critical scenario
quantity and diversity. Complementarily, in the agent behavior space,
agent-environment interaction data are leveraged to quantify behavioral
criticality/diversity and adaptively support generation mode switching, forming
a closed feedback loop that continuously enhances scenario characterization and
exploration within the parameter space. Experiments show our framework improves
critical scenario generation by an average of 56.23\% and demonstrates greater
diversity under novel parameter-behavior co-driven metrics when tested on five
decision-making agents, outperforming state-of-the-art baselines.

</details>


### [209] [Finite-Width Neural Tangent Kernels from Feynman Diagrams](https://arxiv.org/abs/2508.11522)
*Max Guillen,Philipp Misof,Jan E. Gerken*

Main category: cs.LG

TL;DR: This paper introduces Feynman diagrams to simplify computations of finite-width corrections in neural tangent kernels (NTKs), making the analysis of training dynamics more feasible and extending findings for deep networks.


<details>
  <summary>Details</summary>
Motivation: Understanding finite-width corrections is crucial since infinite-width NTKs lack key training properties such as feature learning and NTK evolution.

Method: The authors use Feynman diagrams to compute layer-wise recursive relations for NTK-related statistics (e.g., dNTK and ddNTK), simplifying algebraic manipulations and enabling precise predictions.

Result: The framework extends stability analyses from preactivations to NTKs and proves the absence of certain corrections for scale-invariant nonlinearities like ReLU. Numerical experiments validate theoretical findings.

Conclusion: Feynman diagrams are a viable tool for analyzing finite-width effects in NTKs, offering deeper insight into training dynamics and feature evolution in deep networks.

Abstract: Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,
non-linear neural networks. In the infinite-width limit, NTKs can easily be
computed for most common architectures, yielding full analytic control over the
training dynamics. However, at infinite width, important properties of training
such as NTK evolution or feature learning are absent. Nevertheless, finite
width effects can be included by computing corrections to the Gaussian
statistics at infinite width. We introduce Feynman diagrams for computing
finite-width corrections to NTK statistics. These dramatically simplify the
necessary algebraic manipulations and enable the computation of layer-wise
recursive relations for arbitrary statistics involving preactivations, NTKs and
certain higher-derivative tensors (dNTK and ddNTK) required to predict the
training dynamics at leading order. We demonstrate the feasibility of our
framework by extending stability results for deep networks from preactivations
to NTKs and proving the absence of finite-width corrections for scale-invariant
nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We
validate our results with numerical experiments.

</details>


### [210] [Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2508.11528)
*Juhi Soni,Markus Lange-Hegermann,Stefan Windmann*

Main category: cs.LG

TL;DR: This paper proposes an unsupervised anomaly detection method using a physics-informed diffusion model for multivariate time series data. The approach enhances anomaly detection performance by integrating a weighted physics-informed training loss.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to improve unsupervised anomaly detection in multivariate time series data by leveraging physics-informed insights with advanced machine learning techniques like diffusion models.

Method: The method involves training a diffusion model using a weighted physics-informed loss constructed with a static weight schedule. This enables the model to better capture the temporal distribution of multivariate time series data.

Result: Experiments demonstrate that the physics-informed training improves F1 scores for anomaly detection, enhances data diversity, and optimizes log-likelihood. The model consistently outperformed baseline approaches and prior physics-informed methods on various datasets.

Conclusion: Physics-informed training enables diffusion models to more accurately approximate data distributions, leading to improved unsupervised anomaly detection results in multivariate time series scenarios.

Abstract: We propose an unsupervised anomaly detection approach based on a
physics-informed diffusion model for multivariate time series data. Over the
past years, diffusion model has demonstrated its effectiveness in forecasting,
imputation, generation, and anomaly detection in the time series domain. In
this paper, we present a new approach for learning the physics-dependent
temporal distribution of multivariate time series data using a weighted
physics-informed loss during diffusion model training. A weighted
physics-informed loss is constructed using a static weight schedule. This
approach enables a diffusion model to accurately approximate underlying data
distribution, which can influence the unsupervised anomaly detection
performance. Our experiments on synthetic and real-world datasets show that
physics-informed training improves the F1 score in anomaly detection; it
generates better data diversity and log-likelihood. Our model outperforms
baseline approaches, additionally, it surpasses prior physics-informed work and
purely data-driven diffusion models on a synthetic dataset and one real-world
dataset while remaining competitive on others.

</details>


### [211] [A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow](https://arxiv.org/abs/2508.11529)
*George Paterakis,Andrea Castellani,George Papoutsoglou,Tobias Rodemann,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: This paper introduces Holistic Explainable Artificial Intelligence (HXAI), a framework designed to integrate user-centric explanations throughout the entire AI data-analysis process.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of conventional explainable AI methods, which primarily focus on individual predictions and neglect end-to-end transparency in AI workflows.

Method: The paper proposes a taxonomy unifying six components of the AI workflow, backed by a 112-item question bank, user studies, and large-language-model-enabled explanation techniques tailored to stakeholders.

Result: HXAI identifies gaps in existing explanation tools and provides actionable guidelines for enhancing explainability and trust by aligning with user needs across the entire process.

Conclusion: Through an interdisciplinary approach, HXAI redefines explainability to encompass transparency and trustworthiness, fostering responsible AI deployment and bridging the gap between developers and domain experts.

Abstract: Artificial intelligence is reshaping science and industry, yet many users
still regard its models as opaque "black boxes". Conventional explainable
artificial-intelligence methods clarify individual predictions but overlook the
upstream decisions and downstream quality checks that determine whether
insights can be trusted. In this work, we present Holistic Explainable
Artificial Intelligence (HXAI), a user-centric framework that embeds
explanation into every stage of the data-analysis workflow and tailors those
explanations to users. HXAI unifies six components (data, analysis set-up,
learning process, model output, model quality, communication channel) into a
single taxonomy and aligns each component with the needs of domain experts,
data analysts and data scientists. A 112-item question bank covers these needs;
our survey of contemporary tools highlights critical coverage gaps. Grounded in
theories of human explanation, principles from human-computer interaction and
findings from empirical user studies, HXAI identifies the characteristics that
make explanations clear, actionable and cognitively manageable. A comprehensive
taxonomy operationalises these insights, reducing terminological ambiguity and
enabling rigorous coverage analysis of existing toolchains. We further
demonstrate how AI agents that embed large-language models can orchestrate
diverse explanation techniques, translating technical artifacts into
stakeholder-specific narratives that bridge the gap between AI developers and
domain experts. Departing from traditional surveys or perspective articles,
this work melds concepts from multiple disciplines, lessons from real-world
projects and a critical synthesis of the literature to advance a novel,
end-to-end viewpoint on transparency, trustworthiness and responsible AI
deployment.

</details>


### [212] [DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning](https://arxiv.org/abs/2508.11530)
*Lianshuai Guo,Zhongzheng Yuan,Xunkai Li,Yinlin Zhu,Meixia Qu,Wenyu Wang*

Main category: cs.LG

TL;DR: DFed-SST introduces a decentralized federated graph learning framework aimed at improving communication and model aggregation by leveraging local subgraph topologies.


<details>
  <summary>Details</summary>
Motivation: Existing DFL strategies fail to address the unique topological information in local subgraphs, and centralized implementations of FGL limit decentralization benefits.

Method: The paper introduces DFed-SST with a dual-topology adaptive communication mechanism to dynamically construct inter-client communication models based on local subgraph topologies.

Result: DFed-SST consistently demonstrated superior performance, achieving a significant overall improvement of 3.26% in accuracy across tests on eight real-world datasets.

Conclusion: DFed-SST efficiently bridges the gap between decentralized learning and graph-based tasks, addressing heterogeneity challenges while improving accuracy.

Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed
paradigm that circumvents the single-point-of-failure and communication
bottleneck risks of centralized architectures. However, a significant challenge
arises as existing DFL optimization strategies, primarily designed for tasks
such as computer vision, fail to address the unique topological information
inherent in the local subgraph. Notably, while Federated Graph Learning (FGL)
is tailored for graph data, it is predominantly implemented in a centralized
server-client model, failing to leverage the benefits of decentralization.To
bridge this gap, we propose DFed-SST, a decentralized federated graph learning
framework with adaptive communication. The core of our method is a
dual-topology adaptive communication mechanism that leverages the unique
topological features of each client's local subgraph to dynamically construct
and optimize the inter-client communication topology. This allows our framework
to guide model aggregation efficiently in the face of heterogeneity. Extensive
experiments on eight real-world datasets consistently demonstrate the
superiority of DFed-SST, achieving 3.26% improvement in average accuracy over
baseline methods.

</details>


### [213] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: The paper introduces a nested Operator Inference (OpInf) method to improve reduced-order models (ROMs), achieving lower reconstruction errors compared to standard OpInf across diverse applications.


<details>
  <summary>Details</summary>
Motivation: To solve high-dimensional dynamical systems more efficiently by enhancing reduced-order modeling techniques.

Method: Utilizing a nested approach within OpInf to sequentially optimize interactions among dominant modes, with capabilities for warm-starting based on prior models.

Result: Demonstrated improvement in ROM performance, with 4x error reduction for a heat conduction problem and significant speed-up (above 19,000) for Greenland ice sheet application.

Conclusion: The nested OpInf algorithm is effective for physics-informed ROMs, showcasing lower errors and increased computational efficiency, making it versatile for large-scale applications with dynamic updates.

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


### [214] [SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling](https://arxiv.org/abs/2508.11553)
*Jinghui Wang,Shaojie Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Xiaojiang Zhang,Minglei Zhang,Jiarong Zhang,Wenhao Zhuang,Yuchen Cao,Wankang Bao,Haimo Li,Zheng Lin,Huiming Wang,Haoyang Huang,Zongxian Feng,Zizheng Zhan,Ken Deng,Wen Xiang,Huaixi Tang,Kun Wu,Mengtong Li,Mengfei Xie,Junyi Peng,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SeamlessFlow is a server-based RL framework that optimizes industrial-scale RL training by decoupling training from agent execution and maximizing GPU utilization through scheduling innovations.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies and challenges in industrial-scale RL training, including complex agent execution flows and underutilization of GPU resources.

Method: Introduce a data plane to decouple RL training from agent implementation while maintaining throughput. Utilize tag-driven scheduling to optimize resource utilization across diverse hardware setups.

Result: SeamlessFlow achieves high stability and performance suitable for multi-agent, long-horizon RL tasks by eliminating pipeline bottlenecks and maximizing cluster resource usage.

Conclusion: The framework is scalable and efficient, offering solutions to key RL training challenges and ensuring suitability for complex RL applications.

Abstract: We introduce SeamlessFlow, a server based reinforcement learning (RL)
framework that addresses two core challenges in industrial scale RL: (1)
decoupling RL training from the complex execution flow of agents; (2)
maximizing GPU utilization with minimal idle time while preserving the
stability and scalability required for large-scale deployments. First,
SeamlessFlow introduces a data plane that decouples the RL trainer from
diverse, complex agent implementations while sustaining high throughput. A
central trajectory manager maintains complete interaction histories and
supports partial rollout, allowing rollout to pause for weight updates and
resume seamlessly, keeping agents unaware of service interruptions. Second, we
propose a tag driven scheduling paradigm that abstracts hardware into
capability tagged resources, unifying colocated and disaggregated
architectures. Based on this, SeamlessFlow introduces a spatiotemporal
multiplexing pipeline that dynamically reassigns idle training nodes to rollout
in a train rollout separated setup, eliminating pipeline bubbles and fully
exploiting heterogeneous cluster resources. By combining these innovations,
SeamlessFlow delivers both stability and high performance, making it well
suited for multi agent, long horizon, and other complex RL tasks.

</details>


### [215] [Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective](https://arxiv.org/abs/2508.11618)
*Jungang Chen,Seyyed A. Hosseini*

Main category: cs.LG

TL;DR: This paper introduces a Markov game framework to manage stakeholder collaboration in CCS projects, especially under geologically complex and interconnected conditions.


<details>
  <summary>Details</summary>
Motivation: To address competitive behaviors and the complexity of managing CCS operations involving multiple stakeholders in geologically linked sites.

Method: The paper employs Markov games combined with multi-agent reinforcement learning with safety constraints, and uses the Embed-to-Control (E2C) surrogate model for optimization.

Result: The framework effectively manages CO2 storage optimization and aligns stakeholder strategies with safety regulations.

Conclusion: Collaborative approaches using advanced modeling can streamline stakeholder coordination and optimize CCS operations in complex geological scenarios.

Abstract: Carbon capture and storage (CCS) projects typically involve a diverse array
of stakeholders or players from public, private, and regulatory sectors, each
with different objectives and responsibilities. Given the complexity, scale,
and long-term nature of CCS operations, determining whether individual
stakeholders can independently maximize their interests or whether
collaborative coalition agreements are needed remains a central question for
effective CCS project planning and management. CCS projects are often
implemented in geologically connected sites, where shared geological features
such as pressure space and reservoir pore capacity can lead to competitive
behavior among stakeholders. Furthermore, CO2 storage sites are often located
in geologically mature basins that previously served as sites for hydrocarbon
extraction or wastewater disposal in order to leverage existing
infrastructures, which makes unilateral optimization even more complicated and
unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively
investigate how different coalition structures affect the goals of
stakeholders. We frame this multi-stakeholder multi-site problem as a
multi-agent reinforcement learning problem with safety constraints. Our
approach enables agents to learn optimal strategies while compliant with safety
regulations. We present an example where multiple operators are injecting CO2
into their respective project areas in a geologically connected basin. To
address the high computational cost of repeated simulations of high-fidelity
models, a previously developed surrogate model based on the Embed-to-Control
(E2C) framework is employed. Our results demonstrate the effectiveness of the
proposed framework in addressing optimal management of CO2 storage when
multiple stakeholders with various objectives and goals are involved.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [216] [SDSNN: A Single-Timestep Spiking Neural Network with Self-Dropping Neuron and Bayesian Optimization](https://arxiv.org/abs/2508.10913)
*Changqing Xu,Buxuan Song,Yi Liu,Xinfang Liao,Wenbin Zheng,Yintang Yang*

Main category: cs.NE

TL;DR: This paper introduces a single-timestep Spiking Neural Network (SNN) that enhances accuracy and energy efficiency by optimizing spike generation and using a Self-Dropping Neuron mechanism, reducing energy consumption significantly compared to multi-timestep SNNs.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of conventional SNNs, particularly the high inference latency and computational energy in multi-timestep models, which hinder their suitability for edge computing scenarios.

Method: The authors propose a single-timestep SNN incorporating a Self-Dropping Neuron mechanism for dynamic threshold adjustment and selective spike suppression. Bayesian optimization is used to globally optimize temporal parameters for efficient inference.

Result: The proposed SNN achieved high classification accuracies on the Fashion-MNIST (93.72%), CIFAR-10 (92.20%), and CIFAR-100 (69.45%) datasets with single-timestep inference, while reducing energy consumption by 56%, 21%, and 22%, respectively.

Conclusion: The single-timestep SNN design offers a significant improvement in both energy efficiency and inference speed without compromising accuracy, marking a step forward in applying SNNs in energy-critical scenarios like edge computing.

Abstract: Spiking Neural Networks (SNNs), as an emerging biologically inspired
computational model, demonstrate significant energy efficiency advantages due
to their event-driven information processing mechanism. Compared to traditional
Artificial Neural Networks (ANNs), SNNs transmit information through discrete
spike signals, which substantially reduces computational energy consumption
through their sparse encoding approach. However, the multi-timestep computation
model significantly increases inference latency and energy, limiting the
applicability of SNNs in edge computing scenarios. We propose a single-timestep
SNN, which enhances accuracy and reduces computational energy consumption in a
single timestep by optimizing spike generation and temporal parameters. We
design a Self-Dropping Neuron mechanism, which enhances information-carrying
capacity through dynamic threshold adjustment and selective spike suppression.
Furthermore, we employ Bayesian optimization to globally search for time
parameters and obtain an efficient inference mode with a single time step.
Experimental results on the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets
demonstrate that, compared to traditional multi-timestep SNNs employing the
Leaky Integrate-and-Fire (LIF) model, our method achieves classification
accuracies of 93.72%, 92.20%, and 69.45%, respectively, using only
single-timestep spikes, while maintaining comparable or even superior accuracy.
Additionally, it reduces energy consumption by 56%, 21%, and 22%, respectively.

</details>


### [217] [Insect-Wing Structured Microfluidic System for Reservoir Computing](https://arxiv.org/abs/2508.10915)
*Jacob Clouse,Thomas Ramsey,Samitha Somathilaka,Nicholas Kleinsasser,Sangjin Ryu,Sasitharan Balasubramaniam*

Main category: cs.NE

TL;DR: This paper presents a hybrid microfluidic reservoir computing system inspired by dragonfly wings, achieving up to 91% classification accuracy for spatiotemporal patterns.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the growing need for adaptive, low-power computing systems suitable for environments where conventional electronics are not practical.

Method: The system employs a microfluidic chip with three dye-based inlet channels and detection areas monitored by cameras. Reservoir outputs are used for pattern classification via a trainable readout layer.

Result: The system, incorporating raw and synthetic outputs, achieves classification accuracies up to 91%, even with coarse resolution and limited training data.

Conclusion: Dragonfly-wing-inspired microfluidic reservoir computing is a promising alternative for efficient and resilient computing in environments unsuitable for electronics.

Abstract: As the demand for more efficient and adaptive computing grows,
nature-inspired architectures offer promising alternatives to conventional
electronic designs. Microfluidic platforms, drawing on biological forms and
fluid dynamics, present a compelling foundation for low-power, high-resilience
computing in environments where electronics are unsuitable. This study explores
a hybrid reservoir computing system based on a dragonfly-wing inspired
microfluidic chip, which encodes temporal input patterns as fluid interactions
within the micro channel network.
  The system operates with three dye-based inlet channels and three
camera-monitored detection areas, transforming discrete spatial patterns into
dynamic color output signals. These reservoir output signals are then modified
and passed to a simple and trainable readout layer for pattern classification.
Using a combination of raw reservoir outputs and synthetically generated
outputs, we evaluated system performance, system clarity, and data efficiency.
The results demonstrate consistent classification accuracies up to $91\%$, even
with coarse resolution and limited training data, highlighting the viability of
the microfluidic reservoir computing.

</details>


### [218] [Use of a genetic algorithm to find solutions to introductory physics problems](https://arxiv.org/abs/2508.10920)
*Tom Bensky,Justin Kopcinski*

Main category: cs.NE

TL;DR: This paper introduces a genetic algorithm (GA) to assist students in solving introductory physics problems by generating equation sequences based on known and unknown quantities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assist students with problem-solving in physics by leveraging computational techniques to guide them in identifying and applying the correct sequence of equations.

Method: The GA sequentially evaluates equations by measuring the difference between knowns and unknowns. It matches problem text with available data and generates questions to obtain knowns. Less-fit equations are refined using intermediate results from more-fit equations.

Result: The proposed GA approach successfully guides students toward solutions for one-dimensional kinematics problems in introductory physics.

Conclusion: The research demonstrates the interpretability and effectiveness of GA in aiding step-by-step problem solving in physics and validating its applicability for educational purposes.

Abstract: In this work, we show how a genetic algorithm (GA) can be used to find
step-by-step solutions to introductory physics problems. Our perspective is
that the underlying task for this is one of finding a sequence of equations
that will lead to the needed answer. Here a GA is used to find an appropriate
equation sequence by minimizing a fitness function that measures the difference
between the number of unknowns versus knowns in a set of equations. Information
about knowns comes from the GA posing questions to the student about what
quantities exist in the text of their problem. The questions are generated from
enumerations pulled from the chromosomes that drive the GA. Equations with
smaller known vs. unknown differences are considered more fit and are used to
produce intermediate results that feed less fit equations. We show that this
technique can guide a student to an answer to any introductory physics problem
involving one-dimensional kinematics. Interpretability findings are discussed.

</details>


### [219] [SO-PIFRNN: Self-optimization physics-informed Fourier-features randomized neural network for solving partial differential equations](https://arxiv.org/abs/2508.10921)
*Jiale Linghu,Weifeng Gao,Hao Dong,Yufeng Nie*

Main category: cs.NE

TL;DR: The study develops a self-optimizing neural network framework to improve solving partial differential equations (PDEs) using Fourier features and advanced optimization strategies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of improving numerical accuracy in solving PDEs, leveraging hyperparameter optimization to enhance neural network performance.

Method: The proposed framework uses a bi-level optimization approach with a particle swarm optimization (MSC-PSO) algorithm at the outer level and a least squares method for neural network weight optimization at the inner level.

Result: Through experiments on various complex equations, the framework demonstrated superior accuracy and the ability to capture frequency components effectively.

Conclusion: The SO-PIFRNN framework is shown to significantly enhance numerical solutions of PDEs, proving its validity and potential through extensive tests.

Abstract: This study proposes a self-optimization physics-informed Fourier-features
randomized neural network (SO-PIFRNN) framework, which significantly improves
the numerical solving accuracy of PDEs through hyperparameter optimization
mechanism. The framework employs a bi-level optimization architecture: the
outer-level optimization utilizes a multi-strategy collaborated particle swarm
optimization (MSC-PSO) algorithm to search for optimal hyperparameters of
physics-informed Fourier-features randomized neural network, while the
inner-level optimization determines the output layer weights of the neural
network via the least squares method. The core innovation of this study is
embodied in the following three aspects: First, the Fourier basis function
activation mechanism is introduced in the hidden layer of neural network, which
significantly enhances the ability of the network to capture multi-frequency
components of the solution. Secondly, a novel derivative neural network method
is proposed, which improves the calculation accuracy and efficiency of PIFRNN
method. Finally, the MSC-PSO algorithm of the hybrid optimization strategy is
designed to improve the global search ability and convergence accuracy through
the synergistic effect of dynamic parameter adjustment, elitist and mutation
strategies. Through a series of numerical experiments, including multiscale
equations in complex regions, high-order equations, high-dimensional equations
and nonlinear equations, the validity of SO-PIFRNN is verified. The
experimental results affirm that SO-PIFRNN exhibits superior approximation
accuracy and frequency capture capability.

</details>


### [220] [Allee Synaptic Plasticity and Memory](https://arxiv.org/abs/2508.10929)
*Eddy Kwessi*

Main category: cs.NE

TL;DR: The paper proposes an Allee-based nonlinear plasticity model to tackle challenges in synaptic weight growth and noise sensitivity and improves memory retention and retrieval compared to traditional models.


<details>
  <summary>Details</summary>
Motivation: Existing models for memory storage and retrieval in neural systems face limitations like sensitivity to noise and uncontrolled synaptic weight growth.

Method: The paper introduces a biologically inspired Allee-based nonlinear plasticity model and extends it with time-dependent dynamics, incorporating eligibility traces and oscillatory inputs.

Result: The model demonstrates enhanced noise robustness, weight stabilization mechanisms, and improved memory retention and retrieval capacity when compared to classical models such as Hebbian and Oja's rules.

Conclusion: The work offers a robust framework for neural adaptation while presenting practical implications for neuroscience and advancements in artificial intelligence.

Abstract: Neural plasticity is fundamental to memory storage and retrieval in
biological systems, yet existing models often fall short in addressing noise
sensitivity and unbounded synaptic weight growth. This paper investigates the
Allee-based nonlinear plasticity model, emphasizing its biologically inspired
weight stabilization mechanisms, enhanced noise robustness, and critical
thresholds for synaptic regulation. We analyze its performance in memory
retention and pattern retrieval, demonstrating increased capacity and
reliability compared to classical models like Hebbian and Oja's rules. To
address temporal limitations, we extend the model by integrating time-dependent
dynamics, including eligibility traces and oscillatory inputs, resulting in
improved retrieval accuracy and resilience in dynamic environments. This work
bridges theoretical insights with practical implications, offering a robust
framework for modeling neural adaptation and informing advances in artificial
intelligence and neuroscience.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [221] [Inference performance evaluation for LLMs on edge devices with a novel benchmarking framework and metric](https://arxiv.org/abs/2508.11269)
*Hao Chen,Cong Tian,Zixuan He,Bin Yu,Yepang Liu,Jialun Cao*

Main category: cs.PF

TL;DR: The paper introduces ELIB, a benchmarking tool for evaluating large language models’ (LLMs) inference performance on edge devices and proposes a metric, MBU, to assess memory bandwidth utilization efficiency. The paper evaluates ELIB across different edge platforms and models.


<details>
  <summary>Details</summary>
Motivation: The motivation is driven by the need for edge computing-based LLM inference services to ensure data privacy while confronting the challenges posed by different hardware characteristics and high memory requirements on edge devices.

Method: The authors developed a benchmarking tool, ELIB, introduced a custom metric called MBU for memory bandwidth utilization assessment, and tested five quantized models on three edge platforms to optimize multiple performance metrics, including FLOPS, throughput, latency, and accuracy.

Result: ELIB successfully evaluates edge platforms, highlighting factors and constraints impacting memory bandwidth utilization efficiency (MBU). The analysis revealed insights beneficial for LLM optimization on edge platforms.

Conclusion: With ELIB and the proposed MBU metric, the study provides a systematic way to evaluate and optimize LLM inference performance on edge devices, addressing key deployment challenges and improving future implementations.

Abstract: With the significant success achieved by large language models (LLMs) like
LLaMA, edge computing-based LLM inference services for mobile and PC are in
high demand for data privacy. However, different edge platforms have different
hardware characteristics and the large demand for memory capacity and bandwidth
makes it very challenging to deploy and benchmark LLMs on edge devices. In this
paper, we introduce a benchmarking tool named ELIB (edge LLM inference
benchmarking) to evaluate LLM inference performance of different edge
platforms, and propose a novel metric named MBU to indicate the percentage of
the theoretically efficient use of available memory bandwidth for a specific
model running on edge hardware to optimize memory usage. We deploy ELIB on
three edge platforms and benchmark using five quantized models to optimize MBU
in combination with other metrics such as FLOPS, throughput, latency and
accuracy. And we analyze the results to derive the key factors, constraints,
unpredictability in optimizing MBU that can guide deploying LLMs on more edge
platforms.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [222] [Generic Reduction-Based Interpreters (Extended Version)](https://arxiv.org/abs/2508.11297)
*Casper Bach*

Main category: cs.PL

TL;DR: The paper proposes using generic programming techniques to minimize boilerplate code in reduction-based interpreters.


<details>
  <summary>Details</summary>
Motivation: Reduction-based interpreters require significant boilerplate code, and there is a need to streamline their implementation process.

Method: Applying techniques from generic programming to the construction of reduction-based interpreters.

Result: The approach successfully reduces the amount of boilerplate code required for writing reduction-based interpreters.

Conclusion: Generic programming techniques can effectively simplify the implementation process for reduction-based interpreters, making it more efficient.

Abstract: Reduction-based interpreters are traditionally defined in terms of a one-step
reduction function which systematically decomposes a term into a potential
redex and context, contracts the redex, and recomposes it to construct the new
term to be further reduced. While implementing such interpreters follows a
systematic recipe, they often require interpreter engineers to write a
substantial amount of code -- much of it boilerplate. In this paper, we apply
well-known techniques from generic programming to reduce boilerplate code in
reduction-based interpreters.

</details>


### [223] [Towards Efficient Hash Maps in Functional Array Languages](https://arxiv.org/abs/2508.11443)
*William Henrich Due,Martin Elsman,Troels Henriksen*

Main category: cs.PL

TL;DR: The paper derives a data-parallel implementation for two-level, collision-free hash maps in a functional array language (Futhark), comparing its GPU performance to tree-based implementations and NVIDIA's cuCollections library.


<details>
  <summary>Details</summary>
Motivation: To create a flexible, collision-free hash map implementation that accommodates dynamically sized keys in a data-parallel functional language, addressing limitations of tree/search-based methods.

Method: The authors reformulated the Fredman et al. hash map construction functionally, implemented it in Futhark, and compared GPU performance with tree-based approaches and NVIDIA cuCollections.

Result: The functional implementation outperformed tree-based methods but was slower than cuCollections, especially in hash map construction, due to Futhark's low-level and vocabulary limitations.

Conclusion: The authors reflect on extending the functional array language programming model to overcome its limitations, making it closer to low-level algorithmic efficiency.

Abstract: We present a systematic derivation of a data-parallel implementation of
two-level, static and collision-free hash maps, by giving a functional
formulation of the Fredman et al. construction, and then flattening it. We
discuss the challenges of providing a flexible, polymorphic, and abstract
interface to hash maps in a functional array language, with particular
attention paid to the problem of dynamically sized keys, which we address by
associating each hash map with an arbitrary context. The algorithm is
implemented in Futhark, and the achieved GPU execution performance is compared
on simple benchmark problems. We find that our hash maps outperform
conventional tree/search-based approaches. Furthermore, our implementation is
compared against the state-of-the-art cuCollections library, which is
significantly faster for hash map construction, and to a lesser degree for
lookups. We explain to which extent the performance difference is due to
low-level code generation limitation in the Futhark compiler, and to which
extent it can be attributed to the data-parallel programming vocabulary not
providing the constructs necessary to express the equivalent of the algorithms
used by cuCollections. We end by reflecting to which extent the functional
array language programming model could, or should, be extended to address these
weaknesses.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [224] [Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes](https://arxiv.org/abs/2508.10973)
*Hongchen Wang,Sima Zeinali Danalou,Jiahao Zhu,Kenneth Sulimro,Chaewon Lim,Smita Basak,Aimee Tai,Usan Siriwardana,Jason Hattrick-Simpers,Jay Werber*

Main category: cs.RO

TL;DR: The paper introduces an automated platform for fabricating and characterizing porous polymeric membranes through nonsolvent-induced phase separation (NIPS), enhancing reproducibility and reducing experimental time.


<details>
  <summary>Details</summary>
Motivation: To address the labor-intensive and trial-and-error nature of identifying fabrication parameters for porous polymeric membranes, which currently lack scalability and consistency.

Method: The system automates solution preparation, blade casting, immersion, and compression testing, while controlling fabrication variables such as polymer concentration and ambient humidity.

Result: The system successfully demonstrated reproducible results, showing familiar effects of polymer concentration and ambient humidity on membrane stiffness and morphology, validated using a proof-of-concept experiment with polysulfone and green solvents.

Conclusion: The platform supports high-throughput, data-driven optimization, and is ideal for scalable, reproducible experimentation, making it suitable for integration into self-driving laboratories.

Abstract: The development of porous polymeric membranes remains a labor-intensive
process, often requiring extensive trial and error to identify optimal
fabrication parameters. In this study, we present a fully automated platform
for membrane fabrication and characterization via nonsolvent-induced phase
separation (NIPS). The system integrates automated solution preparation, blade
casting, controlled immersion, and compression testing, allowing precise
control over fabrication parameters such as polymer concentration and ambient
humidity. The modular design allows parallel processing and reproducible
handling of samples, reducing experimental time and increasing consistency.
Compression testing is introduced as a sensitive mechanical characterization
method for estimating membrane stiffness and as a proxy to infer porosity and
intra-sample uniformity through automated analysis of stress-strain curves. As
a proof of concept to demonstrate the effectiveness of the system, NIPS was
carried out with polysulfone, the green solvent PolarClean, and water as the
polymer, solvent, and nonsolvent, respectively. Experiments conducted with the
automated system reproduced expected effects of polymer concentration and
ambient humidity on membrane properties, namely increased stiffness and
uniformity with increasing polymer concentration and humidity variations in
pore morphology and mechanical response. The developed automated platform
supports high-throughput experimentation and is well-suited for integration
into self-driving laboratory workflows, offering a scalable and reproducible
foundation for data-driven optimization of porous polymeric membranes through
NIPS.

</details>


### [225] [Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction](https://arxiv.org/abs/2508.10999)
*Yizhi Zhou,Jie Xu,Jiawei Xia,Zechen Hu,Weizi Li,Xuan Wang*

Main category: cs.RO

TL;DR: The paper introduces a robust online calibration framework for UWB anchors in UWB-aided Visual-Inertial Navigation Systems, improving accuracy and practicality compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: Existing UWB calibration methods suffer from issues like sensitivity to initial guess and localization errors during initialization, limiting their practical use.

Method: The authors explicitly account for localization uncertainties in the calibration process and implement a tightly-coupled Schmidt Kalman Filter (SKF) for online refinement.

Result: Simulations and real-world experiments demonstrated improved accuracy and robustness in UWB anchor positioning.

Conclusion: The proposed framework ensures robust UWB calibration, making it more suitable for real-world applications.

Abstract: This paper presents a novel robust online calibration framework for
Ultra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems
(VINS). Accurate anchor positioning, a process known as calibration, is crucial
for integrating UWB ranging measurements into state estimation. While several
prior works have demonstrated satisfactory results by using robot-aided systems
to autonomously calibrate UWB systems, there are still some limitations: 1)
these approaches assume accurate robot localization during the initialization
step, ignoring localization errors that can compromise calibration robustness,
and 2) the calibration results are highly sensitive to the initial guess of the
UWB anchors' positions, reducing the practical applicability of these methods
in real-world scenarios. Our approach addresses these challenges by explicitly
incorporating the impact of robot localization uncertainties into the
calibration process, ensuring robust initialization. To further enhance the
robustness of the calibration results against initialization errors, we propose
a tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,
making the system suitable for practical applications. Simulations and
real-world experiments validate the improved accuracy and robustness of our
approach.

</details>


### [226] [3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation](https://arxiv.org/abs/2508.11002)
*Nikolaos Gkanatsios,Jiahe Xu,Matthew Bronars,Arsalan Mousavian,Tsung-Wei Ke,Katerina Fragkiadaki*

Main category: cs.RO

TL;DR: The paper introduces 3DFA, a 3D policy architecture for faster and efficient robotic manipulation with impressive performance in both bimanual and unimanual tasks.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and performance of robotic manipulation policies, leveraging 3D visual scene representations and trajectory prediction techniques.

Method: They integrate flow matching with 3D pretrained visual scene representations and optimize system-level processes, introducing 3D relative attention during action denoising.

Result: 3DFA achieves over 30x faster training/inference and outperforms prior methods significantly, establishing new state-of-the-art results in bimanual and unimanual benchmarks.

Conclusion: 3DFA showcases the feasibility of efficient and advanced robot manipulation learning with substantial gains in both efficiency and accuracy over previous methods.

Abstract: We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot
manipulation that combines flow matching for trajectory prediction with 3D
pretrained visual scene representations for learning from demonstration. 3DFA
leverages 3D relative attention between action and visual tokens during action
denoising, building on prior work in 3D diffusion-based single-arm policy
learning. Through a combination of flow matching and targeted system-level and
architectural optimizations, 3DFA achieves over 30x faster training and
inference than previous 3D diffusion-based policies, without sacrificing
performance. On the bimanual PerAct2 benchmark, it establishes a new state of
the art, outperforming the next-best method by an absolute margin of 41.4%. In
extensive real-world evaluations, it surpasses strong baselines with up to
1000x more parameters and significantly more pretraining. In unimanual
settings, it sets a new state of the art on 74 RLBench tasks by directly
predicting dense end-effector trajectories, eliminating the need for motion
planning. Comprehensive ablation studies underscore the importance of our
design choices for both policy effectiveness and efficiency.

</details>


### [227] [GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning](https://arxiv.org/abs/2508.11049)
*Kelin Yu,Sheng Zhang,Harshit Soora,Furong Huang,Heng Huang,Pratap Tokekar,Ruohan Gao*

Main category: cs.RO

TL;DR: This paper proposes GenFlowRL, a novel framework that effectively derives robust robot policies using generated object-centric flow for diverse manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in video generation systems for robot learning, namely the reliance on data quality and the lack of fine-grained environment feedback in manipulation tasks.

Method: Introduce GenFlowRL, a method based on shaped rewards derived from generated object-centric flow trained with diverse cross-embodiment datasets.

Result: GenFlowRL demonstrated superior performance in 10 manipulation tasks across both simulation and real-world cross-embodiment scenarios.

Conclusion: GenFlowRL enhances the generalization and robustness of robot learning from demonstrations using object-centric flow, excelling in a variety of challenging manipulation contexts.

Abstract: Recent advances have shown that video generation models can enhance robot
learning by deriving effective robot actions through inverse dynamics. However,
these methods heavily depend on the quality of generated data and struggle with
fine-grained manipulation due to the lack of environment feedback. While
video-based reinforcement learning improves policy robustness, it remains
constrained by the uncertainty of video generation and the challenges of
collecting large-scale robot datasets for training diffusion models. To address
these limitations, we propose GenFlowRL, which derives shaped rewards from
generated flow trained from diverse cross-embodiment datasets. This enables
learning generalizable and robust policies from diverse demonstrations using
low-dimensional, object-centric features. Experiments on 10 manipulation tasks,
both in simulation and real-world cross-embodiment evaluations, demonstrate
that GenFlowRL effectively leverages manipulation features extracted from
generated object-centric flow, consistently achieving superior performance
across diverse and challenging scenarios. Our Project Page:
https://colinyu1.github.io/genflowrl

</details>


### [228] [Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance](https://arxiv.org/abs/2508.11093)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: The paper introduces a method to enhance human-robot collaboration by integrating vision-language and text-language models into GUIDER framework for better intent inference, real-time reasoning, and goal-oriented assistance.


<details>
  <summary>Details</summary>
Motivation: To improve human-robot collaboration by enabling robots to infer user intent efficiently, provide transparent reasoning, and assist users effectively in navigation and manipulation tasks.

Method: They augment the GUIDER framework with a vision-language model (VLM) and a text-only language model (LLM), which create semantic priors for filtering objects and locations based on a mission prompt. This combines object detection, segmentation, and scoring of relevance to rank context-relevant objects to assist robot decision-making.

Result: The combined belief system allows the autonomy layer to trigger actions, enabling robots to navigate to specific areas and retrieve desired objects adaptively based on intent changes.

Conclusion: Integrating multi-modal models into GUIDER improves real-time adaptability and efficiency in human-robot collaboration, setting the stage for further evaluations in simulated environments with physical robotic setups for seamless assistance.

Abstract: Human-robot collaboration requires robots to quickly infer user intent,
provide transparent reasoning, and assist users in achieving their goals. Our
recent work introduced GUIDER, our framework for inferring navigation and
manipulation intents. We propose augmenting GUIDER with a vision-language model
(VLM) and a text-only language model (LLM) to form a semantic prior that
filters objects and locations based on the mission prompt. A vision pipeline
(YOLO for object detection and the Segment Anything Model for instance
segmentation) feeds candidate object crops into the VLM, which scores their
relevance given an operator prompt; in addition, the list of detected object
labels is ranked by a text-only LLM. These scores weight the existing
navigation and manipulation layers of GUIDER, selecting context-relevant
targets while suppressing unrelated objects. Once the combined belief exceeds a
threshold, autonomy changes occur, enabling the robot to navigate to the
desired area and retrieve the desired object, while adapting to any changes in
the operator's intent. Future work will evaluate the system on Isaac Sim using
a Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.

</details>


### [229] [Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective](https://arxiv.org/abs/2508.11117)
*Xuning Yang,Clemens Eppner,Jonathan Tremblay,Dieter Fox,Stan Birchfield,Fabio Ramos*

Main category: cs.RO

TL;DR: This paper addresses the need for better real-world evaluation methods for generalist robotic manipulation policies, focusing on sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between simulation and real-world applications in robotic manipulation, as current benchmarks focus heavily on simulation.

Method: The authors propose using high visual-fidelity simulation, systematic evaluations of task complexity, and the quantification of simulation-real-world performance alignment.

Result: The paper introduces a structured approach to assess generalist policies' robustness and real-world applicability.

Conclusion: Effective benchmarks for sim-to-real policy transfer require considering task complexity, simulation fidelity, and alignment metrics to ensure robustness in real-world applications.

Abstract: Current vision-based robotics simulation benchmarks have significantly
advanced robotic manipulation research. However, robotics is fundamentally a
real-world problem, and evaluation for real-world applications has lagged
behind in evaluating generalist policies. In this paper, we discuss challenges
and desiderata in designing benchmarks for generalist robotic manipulation
policies for the goal of sim-to-real policy transfer. We propose 1) utilizing
high visual-fidelity simulation for improved sim-to-real transfer, 2)
evaluating policies by systematically increasing task complexity and scenario
perturbation to assess robustness, and 3) quantifying performance alignment
between real-world performance and its simulation counterparts.

</details>


### [230] [Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC](https://arxiv.org/abs/2508.11129)
*Ryan M. Bena,Gilbert Bahati,Blake Werner,Ryan K. Cosner,Lizhi Yang,Aaron D. Ames*

Main category: cs.RO

TL;DR: The paper introduces a predictive safety filter using nonlinear model predictive control (MPC) and control barrier functions (CBFs) for safe trajectory planning of legged robots in unstructured, dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of safe and efficient trajectory planning for legged robots in complex and dynamic environments, considering the asymmetry of their geometries.

Method: The method combines nonlinear MPC with geometry-aware safety constraints derived from Poisson safety functions. It uses control barrier functions (CBFs) generated via perception data and reformulates Poisson's equation as a moving boundary value problem, incorporating robot geometry via Minkowski set operations.

Result: The approach is successfully implemented in real-time on humanoid and quadruped robots across various safety-critical scenarios, demonstrating its efficacy and versatility.

Conclusion: The study shows that combining predictive safety filters with geometry-aware CBFs enhances the ability of legged robots to navigate complex, dynamic environments safely, highlighting the adaptability of Poisson safety functions.

Abstract: Autonomous navigation through unstructured and dynamically-changing
environments is a complex task that continues to present many challenges for
modern roboticists. In particular, legged robots typically possess manipulable
asymmetric geometries which must be considered during safety-critical
trajectory planning. This work proposes a predictive safety filter: a nonlinear
model predictive control (MPC) algorithm for online trajectory generation with
geometry-aware safety constraints based on control barrier functions (CBFs).
Critically, our method leverages Poisson safety functions to numerically
synthesize CBF constraints directly from perception data. We extend the
theoretical framework for Poisson safety functions to incorporate temporal
changes in the domain by reformulating the static Dirichlet problem for
Poisson's equation as a parameterized moving boundary value problem.
Furthermore, we employ Minkowski set operations to lift the domain into a
configuration space that accounts for robot geometry. Finally, we implement our
real-time predictive safety filter on humanoid and quadruped robots in various
safety-critical scenarios. The results highlight the versatility of Poisson
safety functions, as well as the benefit of CBF constrained model predictive
safety-critical controllers.

</details>


### [231] [Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward](https://arxiv.org/abs/2508.11143)
*Jiarui Yang,Bin Zhu,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: AC3 is a reinforcement learning (RL) framework designed to overcome challenges in robotic manipulation tasks, especially those with sparse rewards, by learning continuous action sequences efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods struggle with long-horizon robotic manipulation tasks due to sparse rewards and difficulties in learning stable action chunking paradigms.

Method: AC3 introduces stabilization mechanisms in both the actor (asymmetric update from successful trajectories) and the critic (intra-chunk n-step returns and self-supervised intrinsic rewards at anchor points).

Result: AC3 demonstrates superior success rates on 25 tasks from BiGym and RLBench benchmarks, requiring only a few demonstrations and utilizing a simple model architecture.

Conclusion: AC3 effectively stabilizes the learning of continuous action sequences, improving data efficiency and performance in sparse reward robotic manipulation tasks.

Abstract: Existing reinforcement learning (RL) methods struggle with long-horizon
robotic manipulation tasks, particularly those involving sparse rewards. While
action chunking is a promising paradigm for robotic manipulation, using RL to
directly learn continuous action chunks in a stable and data-efficient manner
remains a critical challenge. This paper introduces AC3 (Actor-Critic for
Continuous Chunks), a novel RL framework that learns to generate
high-dimensional, continuous action sequences. To make this learning process
stable and data-efficient, AC3 incorporates targeted stabilization mechanisms
for both the actor and the critic. First, to ensure reliable policy
improvement, the actor is trained with an asymmetric update rule, learning
exclusively from successful trajectories. Second, to enable effective value
learning despite sparse rewards, the critic's update is stabilized using
intra-chunk $n$-step returns and further enriched by a self-supervised module
providing intrinsic rewards at anchor points aligned with each action chunk. We
conducted extensive experiments on 25 tasks from the BiGym and RLBench
benchmarks. Results show that by using only a few demonstrations and a simple
model architecture, AC3 achieves superior success rates on most tasks,
validating its effective design.

</details>


### [232] [Visuomotor Grasping with World Models for Surgical Robots](https://arxiv.org/abs/2508.11200)
*Hongbin Lin,Bin Li,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: The paper presents a novel framework, GASv2, for automated surgical grasping using a single stereo camera, achieving 65% success rates and generalization to unseen objects in real-world surgical contexts.


<details>
  <summary>Details</summary>
Motivation: To reduce surgeon workload and enhance efficiency and consistency in robot-assisted surgeries by automating the grasping process, which currently suffers from dependence on handcrafted features and poor generalization.

Method: Introduces GASv2, a visuomotor learning framework that combines world-model-based architecture, a surgical perception pipeline, domain randomization for training, and a hybrid control system for safe execution. It is evaluated in real-world surgical setups with single-pair endoscopic cameras.

Result: GASv2 achieves a 65% success rate in phantom-based and ex vivo surgical settings. It generalizes to unseen objects and grippers and adapts to varying disturbances, showcasing robustness and effectiveness.

Conclusion: GASv2 demonstrates promising potential for improving the reliability and generalizability of robotic systems in varied surgical environments by automating grasping tasks.

Abstract: Grasping is a fundamental task in robot-assisted surgery (RAS), and
automating it can reduce surgeon workload while enhancing efficiency, safety,
and consistency beyond teleoperated systems. Most prior approaches rely on
explicit object pose tracking or handcrafted visual features, limiting their
generalization to novel objects, robustness to visual disturbances, and the
ability to handle deformable objects. Visuomotor learning offers a promising
alternative, but deploying it in RAS presents unique challenges, such as low
signal-to-noise ratio in visual observations, demands for high safety and
millimeter-level precision, as well as the complex surgical environment. This
paper addresses three key challenges: (i) sim-to-real transfer of visuomotor
policies to ex vivo surgical scenes, (ii) visuomotor learning using only a
single stereo camera pair -- the standard RAS setup, and (iii) object-agnostic
grasping with a single policy that generalizes to diverse, unseen surgical
objects without retraining or task-specific models. We introduce Grasp Anything
for Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.
GASv2 leverages a world-model-based architecture and a surgical perception
pipeline for visual observations, combined with a hybrid control system for
safe execution. We train the policy in simulation using domain randomization
for sim-to-real transfer and deploy it on a real robot in both phantom-based
and ex vivo surgical settings, using only a single pair of endoscopic cameras.
Extensive experiments show our policy achieves a 65% success rate in both
settings, generalizes to unseen objects and grippers, and adapts to diverse
disturbances, demonstrating strong performance, generality, and robustness.

</details>


### [233] [Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation](https://arxiv.org/abs/2508.11204)
*Hongbin Lin,Juan Rojas,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: This paper proposes a data augmentation method, MEA, utilizing non-isometric task symmetries to improve sampling efficiency in robotic manipulation learning.


<details>
  <summary>Details</summary>
Motivation: Prior works in robotic manipulation learning are limited by their reliance on isometric symmetries, which apply uniform transformations to all task objects across all timesteps, restricting flexibility.

Method: The paper introduces MEA, a novel approach integrating non-isometric symmetry structures via a reformulated POMDP and voxel-based visual representation to enhance offline reinforcement learning.

Result: Experiments conducted in simulations and real-world robot manipulation tasks show significant improvements in sampling efficiency using MEA over conventional methods.

Conclusion: Incorporating non-isometric symmetries in visuomotor learning and leveraging MEA substantially enhances sampling efficiency, paving the way for more adaptive robotic manipulation systems.

Abstract: Sampling efficiency is critical for deploying visuomotor learning in
real-world robotic manipulation. While task symmetry has emerged as a promising
inductive bias to improve efficiency, most prior work is limited to isometric
symmetries -- applying the same group transformation to all task objects across
all timesteps. In this work, we explore non-isometric symmetries, applying
multiple independent group transformations across spatial and temporal
dimensions to relax these constraints. We introduce a novel formulation of the
partially observable Markov decision process (POMDP) that incorporates the
non-isometric symmetry structures, and propose a simple yet effective data
augmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate
MEA with offline reinforcement learning to enhance sampling efficiency, and
introduce a voxel-based visual representation that preserves translational
equivariance. Extensive simulation and real-robot experiments across two
manipulation domains demonstrate the effectiveness of our approach.

</details>


### [234] [Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification](https://arxiv.org/abs/2508.11232)
*Guoliang Li,Xibin Jin,Yujie Wan,Chenxuan Liu,Tong Zhang,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: This paper introduces the NEEI paradigm, integrating embodied edge intelligence (EEI) with near-field communication (NFC), and proposes techniques for joint optimization of their functionalities.


<details>
  <summary>Details</summary>
Motivation: Achieving embodied artificial intelligence with real-time performance is hindered by the high computational demands of large models, necessitating advanced edge computing and communication solutions.

Method: The paper advocates the integration of EEI with NFC to form the NEEI paradigm and proposes techniques like radio-friendly embodied planning and view-guided beam-focusing to address challenges in their joint optimization.

Result: The experimental results verify the superiority of the proposed techniques compared to various benchmark methods.

Conclusion: The integration of EEI with NFC in the NEEI paradigm is effective for addressing computational and communication challenges in embodied artificial intelligence, opening new research avenues for resource-efficient designs.

Abstract: Realizing embodied artificial intelligence is challenging due to the huge
computation demands of large models (LMs). To support LMs while ensuring
real-time inference, embodied edge intelligence (EEI) is a promising paradigm,
which leverages an LM edge to provide computing powers in close proximity to
embodied robots. Due to embodied data exchange, EEI requires higher spectral
efficiency, enhanced communication security, and reduced inter-user
interference. To meet these requirements, near-field communication (NFC), which
leverages extremely large antenna arrays as its hardware foundation, is an
ideal solution. Therefore, this paper advocates the integration of EEI and NFC,
resulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces
new challenges that cannot be adequately addressed by isolated EEI or NFC
designs, creating research opportunities for joint optimization of both
functionalities. To this end, we propose radio-friendly embodied planning for
EEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI
scenarios. We also elaborate how to realize resource-efficient NEEI through
opportunistic collaborative navigation. Experimental results are provided to
confirm the superiority of the proposed techniques compared with various
benchmarks.

</details>


### [235] [Tactile Robotics: An Outlook](https://arxiv.org/abs/2508.11261)
*Shan Luo,Nathan F. Lepora,Wenzhen Yuan,Kaspar Althoefer,Gordon Cheng,Ravinder Dahiya*

Main category: cs.RO

TL;DR: The paper emphasizes challenges and solutions for enhancing robots' tactile sensing capabilities, essential for better human-robot coexistence and interactions.


<details>
  <summary>Details</summary>
Motivation: To equip robots with tactile sensing similar to biological systems for improved interaction with humans across multiple domains.

Method: Analysis of current tactile sensing technologies such as capacitive and optical sensors and evaluation of advances in simulation tools and integrated modalities like vision.

Result: Identification of challenges in tactile robotics and exploration of ways to address them for application in fields like healthcare and manufacturing.

Conclusion: A holistic approach integrating tactile sensing with multimodal strategies is crucial for the transformative impact of tactile robotics on various industries.

Abstract: Robotics research has long sought to give robots the ability to perceive the
physical world through touch in an analogous manner to many biological systems.
Developing such tactile capabilities is important for numerous emerging
applications that require robots to co-exist and interact closely with humans.
Consequently, there has been growing interest in tactile sensing, leading to
the development of various technologies, including piezoresistive and
piezoelectric sensors, capacitive sensors, magnetic sensors, and optical
tactile sensors. These diverse approaches utilise different transduction
methods and materials to equip robots with distributed sensing capabilities,
enabling more effective physical interactions. These advances have been
supported in recent years by simulation tools that generate large-scale tactile
datasets to support sensor designs and algorithms to interpret and improve the
utility of tactile data. The integration of tactile sensing with other
modalities, such as vision, as well as with action strategies for active
tactile perception highlights the growing scope of this field. To further the
transformative progress in tactile robotics, a holistic approach is essential.
In this outlook article, we examine several challenges associated with the
current state of the art in tactile robotics and explore potential solutions to
inspire innovations across multiple domains, including manufacturing,
healthcare, recycling and agriculture.

</details>


### [236] [Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation](https://arxiv.org/abs/2508.11275)
*Masaki Murooka,Iori Kumagai,Mitsuharu Morisawa,Fumio Kanehiro*

Main category: cs.RO

TL;DR: The paper introduces a differentiable reachability map to streamline humanoid motion planning, representing robot kinematic reachability as a scalar map usable in optimization.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational burden of humanoid robot motion planning by creating a more efficient kinematic reachability representation.

Method: A differentiable reachability map is learned using neural networks or support vector machines from end-effector poses, enabling its use as constraints in continuous optimization.

Result: The approach demonstrated efficiency in solving various humanoid motion planning problems such as footstep planning, multi-contact motion, and loco-manipulation planning.

Conclusion: Differentiable reachability maps provide a continuous and efficient solution for humanoid motion generation through optimization techniques, reducing computational costs.

Abstract: To reduce the computational cost of humanoid motion generation, we introduce
a new approach to representing robot kinematic reachability: the differentiable
reachability map. This map is a scalar-valued function defined in the task
space that takes positive values only in regions reachable by the robot's
end-effector. A key feature of this representation is that it is continuous and
differentiable with respect to task-space coordinates, enabling its direct use
as constraints in continuous optimization for humanoid motion planning. We
describe a method to learn such differentiable reachability maps from a set of
end-effector poses generated using a robot's kinematic model, using either a
neural network or a support vector machine as the learning model. By
incorporating the learned reachability map as a constraint, we formulate
humanoid motion generation as a continuous optimization problem. We demonstrate
that the proposed approach efficiently solves various motion planning problems,
including footstep planning, multi-contact motion planning, and
loco-manipulation planning for humanoid robots.

</details>


### [237] [Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent](https://arxiv.org/abs/2508.11286)
*Che Rin Yu,Daewon Chae,Dabin Seo,Sangwon Lee,Hyeongwoo Im,Jinkyu Kim*

Main category: cs.RO

TL;DR: The paper introduces a proactive replanning framework for robots, comparing scene graphs from current observations to graphs from demonstrations to prevent task execution failures.


<details>
  <summary>Details</summary>
Motivation: Many autonomous robots struggle to adapt to subtle environmental changes, leading to failures that could be prevented with proactive replanning.

Method: Using RGB-D observations, the system creates and compares scene graphs with reference graphs from successful demonstrations, activating a reasoning module to address mismatches.

Result: Experiments on AI2-THOR show improved detection of mismatches before failure, enhancing task success and robustness.

Conclusion: Proactive replanning can significantly bolster robot autonomy by diagnosing and adjusting plans in real-time, before encountering execution failures.

Abstract: When humans perform everyday tasks, we naturally adjust our actions based on
the current state of the environment. For instance, if we intend to put
something into a drawer but notice it is closed, we open it first. However,
many autonomous robots lack this adaptive awareness. They often follow
pre-planned actions that may overlook subtle yet critical changes in the scene,
which can result in actions being executed under outdated assumptions and
eventual failure. While replanning is critical for robust autonomy, most
existing methods respond only after failures occur, when recovery may be
inefficient or infeasible. While proactive replanning holds promise for
preventing failures in advance, current solutions often rely on manually
designed rules and extensive supervision. In this work, we present a proactive
replanning framework that detects and corrects failures at subtask boundaries
by comparing scene graphs constructed from current RGB-D observations against
reference graphs extracted from successful demonstrations. When the current
scene fails to align with reference trajectories, a lightweight reasoning
module is activated to diagnose the mismatch and adjust the plan. Experiments
in the AI2-THOR simulator demonstrate that our approach detects semantic and
spatial mismatches before execution failures occur, significantly improving
task success and robustness.

</details>


### [238] [A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation](https://arxiv.org/abs/2508.11289)
*Lin Li,Xueming Liu,Zhoujingzi Qiu,Tianjiang Hu,Qingrui Zhang*

Main category: cs.RO

TL;DR: This paper proposes a Recursive Total Least Squares (RTLS) method for bearing-only Target Motion Analysis (TMA), addressing biases and computational inefficiencies in position estimation. It also introduces a circumnavigation controller to improve observability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in bearing-only TMA, such as nonlinearity and lack of range information, which reduce observability and impede estimator convergence.

Method: The authors developed an RTLS method inspired by Total Least Squares for online target localization, and a circumnavigation controller for enhancing observability and estimator convergence.

Result: Extensive simulations and experiments showed that the RTLS method improves accuracy and efficiency over pseudo-linear Kalman filter-based methods, and the circumnavigation controller strengthens system observability.

Conclusion: The proposed RTLS method and circumnavigation controller outperform state-of-the-art approaches, offering superior accuracy and stability in bearing-only TMA applications.

Abstract: Bearing-only Target Motion Analysis (TMA) is a promising technique for
passive tracking in various applications as a bearing angle is easy to measure.
Despite its advantages, bearing-only TMA is challenging due to the nonlinearity
of the bearing measurement model and the lack of range information, which
impairs observability and estimator convergence. This paper addresses these
issues by proposing a Recursive Total Least Squares (RTLS) method for online
target localization and tracking using mobile observers. The RTLS approach,
inspired by previous results on Total Least Squares (TLS), mitigates biases in
position estimation and improves computational efficiency compared to
pseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a
circumnavigation controller to enhance system observability and estimator
convergence by guiding the mobile observer in orbit around the target.
Extensive simulations and experiments are performed to demonstrate the
effectiveness and robustness of the proposed method. The proposed algorithm is
also compared with the state-of-the-art approaches, which confirms its superior
performance in terms of both accuracy and stability.

</details>


### [239] [Pedestrian Dead Reckoning using Invariant Extended Kalman Filter](https://arxiv.org/abs/2508.11396)
*Jingran Zhang,Zhengzhang Yan,Yiming Chen,Zeqiang He,Jiahao Chen*

Main category: cs.RO

TL;DR: This paper introduces an inertial pedestrian dead reckoning method using invariant extended Kalman filters (InEKF) for bipedal robots in GPS-denied environments.


<details>
  <summary>Details</summary>
Motivation: To provide a reliable and cost-effective navigation solution for bipedal robots in environments where GPS is unavailable.

Method: The method employs an InEKF using stationary pseudo-measurements while the IMU is on the stance foot, leveraging matrix Lie group-based theoretical development.

Result: Experimental evaluation includes a motion capture benchmark, multi-floor walking, and a real bipedal robot experiment, demonstrating feasibility and ease of parameter tuning for InEKF.

Conclusion: InEKF outperforms standard EKF in tuning simplicity and shows practicality in real-world robotics applications.

Abstract: This paper presents a cost-effective inertial pedestrian dead reckoning
method for the bipedal robot in the GPS-denied environment. Each time when the
inertial measurement unit (IMU) is on the stance foot, a stationary
pseudo-measurement can be executed to provide innovation to the IMU measurement
based prediction. The matrix Lie group based theoretical development of the
adopted invariant extended Kalman filter (InEKF) is set forth for tutorial
purpose. Three experiments are conducted to compare between InEKF and standard
EKF, including motion capture benchmark experiment, large-scale multi-floor
walking experiment, and bipedal robot experiment, as an effort to show our
method's feasibility in real-world robot system. In addition, a sensitivity
analysis is included to show that InEKF is much easier to tune than EKF.

</details>


### [240] [An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration](https://arxiv.org/abs/2508.11404)
*Junyeon Kim,Tianshu Ruan,Cesar Alan Contreras,Manolis Chiou*

Main category: cs.RO

TL;DR: The paper investigates AI-powered visual crack detection in nuclear facility inspections using a mobile robot to enhance accuracy and reduce operator workload.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety and integrity in nuclear facility inspections while addressing limitations of traditional manual methods, such as safety risks and inaccuracies.

Method: The paper integrates AI-assisted visual crack detection algorithms into a mobile Jackal robot platform as part of Human-Robot Collaboration (HRC) for inspections.

Result: Experimental results show improved accuracy in crack detection and reduced workload for operators when compared to traditional manual inspections.

Conclusion: AI-assisted inspection methods, combined with robotic platforms, provide safer, more efficient, and better-performing alternatives to traditional approaches for nuclear facility inspections.

Abstract: Structural inspection in nuclear facilities is vital for maintaining
operational safety and integrity. Traditional methods of manual inspection pose
significant challenges, including safety risks, high cognitive demands, and
potential inaccuracies due to human limitations. Recent advancements in
Artificial Intelligence (AI) and robotic technologies have opened new
possibilities for safer, more efficient, and accurate inspection methodologies.
Specifically, Human-Robot Collaboration (HRC), leveraging robotic platforms
equipped with advanced detection algorithms, promises significant improvements
in inspection outcomes and reductions in human workload. This study explores
the effectiveness of AI-assisted visual crack detection integrated into a
mobile Jackal robot platform. The experiment results indicate that HRC enhances
inspection accuracy and reduces operator workload, resulting in potential
superior performance outcomes compared to traditional manual methods.

</details>


### [241] [Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing](https://arxiv.org/abs/2508.11406)
*Benjamin Alt,Mareike Picklum,Sorin Arion,Franklin Kenghagho Kenfack,Michael Beetz*

Main category: cs.RO

TL;DR: The paper introduces tools for autonomous robots to conduct scientific experiments in a transparent and replicable manner.


<details>
  <summary>Details</summary>
Motivation: To ensure scientific experiments by autonomous robots are precise, repeatable, transparent, and trustworthy.

Method: A semantic execution tracing framework for logging data and annotated robot states, and a cloud-based AICOR Virtual Research Building (VRB) for sharing and validating task executions.

Result: These tools facilitate reproducible robot-driven science with deterministic execution and open knowledge sharing.

Conclusion: The tools lay the groundwork for autonomous systems to contribute to scientific discovery transparently and at scale.

Abstract: We envision a future in which autonomous robots conduct scientific
experiments in ways that are not only precise and repeatable, but also open,
trustworthy, and transparent. To realize this vision, we present two key
contributions: a semantic execution tracing framework that logs sensor data
together with semantically annotated robot belief states, ensuring that
automated experimentation is transparent and replicable; and the AICOR Virtual
Research Building (VRB), a cloud-based platform for sharing, replicating, and
validating robot task executions at scale. Together, these tools enable
reproducible, robot-driven science by integrating deterministic execution,
semantic memory, and open knowledge representation, laying the foundation for
autonomous systems to participate in scientific discovery.

</details>


### [242] [EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback](https://arxiv.org/abs/2508.11453)
*Jiayue Jin,Lang Qian,Jingyu Zhang,Chuanyu Ju,Liang Song*

Main category: cs.RO

TL;DR: EvoPSF is a novel online evolutionary framework designed to adapt autonomous driving systems during deployment by addressing planning failures through targeted self-supervised learning.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems lack adaptability during deployment and show diminished performance in unseen real-world environments.

Method: EvoPSF detects planner uncertainty caused by inaccurate motion predictions and adapts the model online by computing targeted self-supervised loss for critical objects responsible for failures.

Result: EvoPSF improves the robustness of autonomous driving systems to environmental changes and enhances motion prediction accuracy, which leads to better planning under challenging conditions.

Conclusion: EvoPSF demonstrates consistent performance improvements across cross-region and corrupted datasets, proving its effectiveness in elevating autonomous driving system reliability.

Abstract: Recent years have witnessed remarkable progress in autonomous driving, with
systems evolving from modular pipelines to end-to-end architectures. However,
most existing methods are trained offline and lack mechanisms to adapt to new
environments during deployment. As a result, their generalization ability
diminishes when faced with unseen variations in real-world driving scenarios.
In this paper, we break away from the conventional "train once, deploy forever"
paradigm and propose EvoPSF, a novel online Evolution framework for autonomous
driving based on Planning-State Feedback. We argue that planning failures are
primarily caused by inaccurate object-level motion predictions, and such
failures are often reflected in the form of increased planner uncertainty. To
address this, we treat planner uncertainty as a trigger for online evolution,
using it as a diagnostic signal to initiate targeted model updates. Rather than
performing blind updates, we leverage the planner's agent-agent attention to
identify the specific objects that the ego vehicle attends to most, which are
primarily responsible for the planning failures. For these critical objects, we
compute a targeted self-supervised loss by comparing their predicted waypoints
from the prediction module with their actual future positions, selected from
the perception module's outputs with high confidence scores. This loss is then
backpropagated to adapt the model online. As a result, our method improves the
model's robustness to environmental changes, leads to more precise motion
predictions, and therefore enables more accurate and stable planning behaviors.
Experiments on both cross-region and corrupted variants of the nuScenes dataset
demonstrate that EvoPSF consistently improves planning performance under
challenging conditions.

</details>


### [243] [OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation](https://arxiv.org/abs/2508.11479)
*Tatiana Zemskova,Aleksei Staroverov,Dmitry Yudin,Aleksandr Panov*

Main category: cs.RO

TL;DR: OVSegDT introduces a lightweight transformer policy for open-vocabulary object navigation, significantly improving generalization to unseen categories and reducing unsafe collisions.


<details>
  <summary>Details</summary>
Motivation: Existing models for object goal navigation overfit to simulator datasets, struggle with generalizing to unseen categories, and exhibit unsafe behaviors like frequent collisions.

Method: OVSegDT proposes two components: a semantic branch with encoder and auxiliary segmentation loss for spatial grounding of textual goals, and Entropy-Adaptive Loss Modulation to balance imitation and reinforcement signals continuously.

Result: The model reduces training sample complexity by 33%, cuts collision count by half, and achieves state-of-the-art performance on unseen categories with RGB-only input while avoiding reliance on depth or large vision-language models.

Conclusion: OVSegDT enhances safety and efficiency in object navigation tasks, setting a new benchmark for handling unseen categories without requiring extra sensory inputs.

Abstract: Open-vocabulary Object Goal Navigation requires an embodied agent to reach
objects described by free-form language, including categories never seen during
training. Existing end-to-end policies overfit small simulator datasets,
achieving high success on training scenes but failing to generalize and
exhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a
lightweight transformer policy that tackles these issues with two synergistic
components. The first component is the semantic branch, which includes an
encoder for the target binary mask and an auxiliary segmentation loss function,
grounding the textual goal and providing precise spatial cues. The second
component consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample
scheduler that continuously balances imitation and reinforcement signals
according to the policy entropy, eliminating brittle manual phase switches.
These additions cut the sample complexity of training by 33%, and reduce
collision count in two times while keeping inference cost low (130M parameters,
RGB-only input). On HM3D-OVON, our model matches the performance on unseen
categories to that on seen ones and establishes state-of-the-art results (40.1%
SR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language
models. Code is available at https://github.com/CognitiveAISystems/OVSegDT.

</details>


### [244] [i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping](https://arxiv.org/abs/2508.11485)
*Hailiang Tang,Tisheng Zhang,Liqiang Wang,Xin Ding,Man Yuan,Zhiyu Xiang,Jujin Chen,Yuhan Bian,Shuangyan Liu,Yuqing Wang,Guan Wang,Xiaoji Niu*

Main category: cs.RO

TL;DR: The paper introduces i2Nav-Robot, a multi-sensor fusion navigation dataset designed specifically for unmanned ground vehicles (UGVs) in diverse indoor-outdoor scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing UGV datasets lack adequate sensor configuration, time synchronization, ground truth, and scenario diversity required for improving navigation and mapping techniques.

Method: A dataset was constructed using an omnidirectional wheeled robot equipped with advanced multi-modal sensors (LiDARs, 4D radar, stereo cameras, GNSS, IMU, etc.) and precise time synchronization mechanisms. It covers a distance of 17060 meters under diverse operating scenarios.

Result: The dataset demonstrated high-quality data standards and was evaluated successfully using over ten open-source multi-sensor fusion systems.

Conclusion: The i2Nav-Robot dataset fills the existing gaps in UGV research, offering a reliable resource for advancing navigation and mapping in complex environments.

Abstract: Accurate and reliable navigation is crucial for autonomous unmanned ground
vehicle (UGV). However, current UGV datasets fall short in meeting the demands
for advancing navigation and mapping techniques due to limitations in sensor
configuration, time synchronization, ground truth, and scenario diversity. To
address these challenges, we present i2Nav-Robot, a large-scale dataset
designed for multi-sensor fusion navigation and mapping in indoor-outdoor
environments. We integrate multi-modal sensors, including the newest front-view
and 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,
odometer, global navigation satellite system (GNSS) receiver, and inertial
measurement units (IMU) on an omnidirectional wheeled robot. Accurate
timestamps are obtained through both online hardware synchronization and
offline calibration for all sensors. The dataset comprises ten larger-scale
sequences covering diverse UGV operating scenarios, such as outdoor streets,
and indoor parking lots, with a total length of about 17060 meters.
High-frequency ground truth, with centimeter-level accuracy for position, is
derived from post-processing integrated navigation methods using a
navigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more
than ten open-sourced multi-sensor fusion systems, and it has proven to have
superior data quality.

</details>


### [245] [Relative Position Matters: Trajectory Prediction and Planning with Polar Representation](https://arxiv.org/abs/2508.11492)
*Bozhou Zhang,Nan Song,Bingzhao Gao,Li Zhang*

Main category: cs.RO

TL;DR: The paper introduces Polaris, a model for autonomous driving trajectory prediction and planning that uses Polar coordinates instead of Cartesian coordinates to capture spatial relationships more effectively.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory prediction and planning methods using Cartesian coordinates are suboptimal for modeling relationships between the ego vehicle and surrounding elements in dynamic environments.

Method: Polaris operates entirely in Polar coordinates, encoding positions with radius and angle, leveraging distance and directional relationships, and employing dedicated encoding and refinement modules.

Result: Polaris achieves state-of-the-art performance on challenging benchmarks like Argoverse 2 for trajectory prediction and nuPlan for planning.

Conclusion: Using Polar coordinates improves spatial awareness and the modeling of relationships between traffic elements, making Polaris more effective for autonomous driving systems.

Abstract: Trajectory prediction and planning in autonomous driving are highly
challenging due to the complexity of predicting surrounding agents' movements
and planning the ego agent's actions in dynamic environments. Existing methods
encode map and agent positions and decode future trajectories in Cartesian
coordinates. However, modeling the relationships between the ego vehicle and
surrounding traffic elements in Cartesian space can be suboptimal, as it does
not naturally capture the varying influence of different elements based on
their relative distances and directions. To address this limitation, we adopt
the Polar coordinate system, where positions are represented by radius and
angle. This representation provides a more intuitive and effective way to model
spatial changes and relative relationships, especially in terms of distance and
directional influence. Based on this insight, we propose Polaris, a novel
method that operates entirely in Polar coordinates, distinguishing itself from
conventional Cartesian-based approaches. By leveraging the Polar
representation, this method explicitly models distance and direction variations
and captures relative relationships through dedicated encoding and refinement
modules, enabling more structured and spatially aware trajectory prediction and
planning. Extensive experiments on the challenging prediction (Argoverse 2) and
planning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art
performance.

</details>


### [246] [Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language](https://arxiv.org/abs/2508.11498)
*Agnes Bressan de Almeida,Joao Aires Correa Fernandes Marsicano*

Main category: cs.RO

TL;DR: Swarm in Blocks simplifies drone programming with a block-based interface, and version 2.0 streamlines swarm management further.


<details>
  <summary>Details</summary>
Motivation: The need for accessible tools to manage increasingly complex drone swarms, especially for beginners.

Method: Developed a block-based high-level interface built on the Clover platform to simplify drone swarm programming.

Result: Created Swarm in Blocks 2.0, making swarm programming more user-friendly and accessible for educational purposes.

Conclusion: The tool enhances accessibility and education in drone programming and swarm management without requiring extensive expertise.

Abstract: Swarm in Blocks, originally developed for CopterHack 2022, is a high-level
interface that simplifies drone swarm programming using a block-based language.
Building on the Clover platform, this tool enables users to create
functionalities like loops and conditional structures by assembling code
blocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the
platform to address the complexities of swarm management in a user-friendly
way. As drone swarm applications grow in areas like delivery, agriculture, and
surveillance, the challenge of managing them, especially for beginners, has
also increased. The Atena team developed this interface to make swarm handling
accessible without requiring extensive knowledge of ROS or programming. The
block-based approach not only simplifies swarm control but also expands
educational opportunities in programming.

</details>


### [247] [Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media](https://arxiv.org/abs/2508.11503)
*Andrej Orsula,Matthieu Geist,Miguel Olivares-Mendez,Carol Martinez*

Main category: cs.RO

TL;DR: This paper proposes a sim-to-real framework to develop robust control policies for autonomous navigation of wheeled rovers over unstructured terrains, validated on a lunar-analogue setup.


<details>
  <summary>Details</summary>
Motivation: The need for reliable autonomous navigation on planetary terrains amidst challenges posed by the sim-to-real gap, primarily due to complex terrain and wheel interactions.

Method: Use parallel simulation to train reinforcement learning agents in randomized environments, followed by zero-shot deployment and systematic evaluation on a physical rover in a lunar-analogue setting.

Result: Reinforcement learning agents trained with procedural environment diversity outperform those trained on static setups; highlights trade-offs of fine-tuning with high-fidelity physics.

Conclusion: This workflow enables developing reliable navigation policies, progressing autonomous robotic deployment for planetary exploration.

Abstract: Reliable autonomous navigation across the unstructured terrains of distant
planetary surfaces is a critical enabler for future space exploration. However,
the deployment of learning-based controllers is hindered by the inherent
sim-to-real gap, particularly for the complex dynamics of wheel interactions
with granular media. This work presents a complete sim-to-real framework for
developing and validating robust control policies for dynamic waypoint tracking
on such challenging surfaces. We leverage massively parallel simulation to
train reinforcement learning agents across a vast distribution of procedurally
generated environments with randomized physics. These policies are then
transferred zero-shot to a physical wheeled rover operating in a lunar-analogue
facility. Our experiments systematically compare multiple reinforcement
learning algorithms and action smoothing filters to identify the most effective
combinations for real-world deployment. Crucially, we provide strong empirical
evidence that agents trained with procedural diversity achieve superior
zero-shot performance compared to those trained on static scenarios. We also
analyze the trade-offs of fine-tuning with high-fidelity particle physics,
which offers minor gains in low-speed precision at a significant computational
cost. Together, these contributions establish a validated workflow for creating
reliable learning-based navigation systems, marking a critical step towards
deploying autonomous robots in the final frontier.

</details>


### [248] [A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning](https://arxiv.org/abs/2508.11520)
*Evangelos Tsiatsianas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: This paper compares different floating-base parameterizations for trajectory optimization in legged robots, introducing a novel formulation leveraging the SE(3) tangent space.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of understanding on how the choice of floating-base space parameterization affects agile motion generation, especially in complex contact dynamics.

Method: The study conducts a systematic evaluation of several parameterizations under identical optimization conditions, and proposes a novel SE(3) tangent space formulation that avoids specialized manifold optimization.

Result: The proposed formulation allows the use of existing solvers and provides insights into selecting parameterizations for agile robotic motion.

Conclusion: The paper contributes tools and insights for optimal parameterization choice in trajectory optimization for agile legged robots, advancing the field.

Abstract: Automatically generating agile whole-body motions for legged and humanoid
robots remains a fundamental challenge in robotics. While numerous trajectory
optimization approaches have been proposed, there is no clear guideline on how
the choice of floating-base space parameterization affects performance,
especially for agile behaviors involving complex contact dynamics. In this
paper, we present a comparative study of different parameterizations for direct
transcription-based trajectory optimization of agile motions in legged systems.
We systematically evaluate several common choices under identical optimization
settings to ensure a fair comparison. Furthermore, we introduce a novel
formulation based on the tangent space of SE(3) for representing the robot's
floating-base pose, which, to our knowledge, has not received attention from
the literature. This approach enables the use of mature off-the-shelf numerical
solvers without requiring specialized manifold optimization techniques. We hope
that our experiments and analysis will provide meaningful insights for
selecting the appropriate floating-based representation for agile whole-body
motion generation.

</details>


### [249] [MultiPark: Multimodal Parking Transformer with Next-Segment Prediction](https://arxiv.org/abs/2508.11537)
*Han Zheng,Zikang Zhou,Guli Zhang,Zhepei Wang,Kaixuan Wang,Peiliang Li,Shaojie Shen,Ming Yang,Tong Qin*

Main category: cs.RO

TL;DR: MultiPark introduces an autoregressive transformer framework enabling multimodal and robust parking behavior in constrained spaces, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current parking systems struggle with multimodal solutions and generalization across diverse scenarios due to causal confusion in imitation learning approaches.

Method: MultiPark leverages a data-efficient next-segment prediction paradigm and learnable parking queries divided into specific components. It incorporates target-centric pose and collision losses to address causal confusion beyond imitation losses.

Result: Evaluations on real-world datasets confirm MultiPark's state-of-the-art efficacy and robustness across various parking scenarios.

Conclusion: MultiPark successfully handles complex parking challenges with multimodal reasoning and effective generalization, confirmed by both simulation and deployment in production vehicles.

Abstract: Parking accurately and safely in highly constrained spaces remains a critical
challenge. Unlike structured driving environments, parking requires executing
complex maneuvers such as frequent gear shifts and steering saturation. Recent
attempts to employ imitation learning (IL) for parking have achieved promising
results. However, existing works ignore the multimodal nature of parking
behavior in lane-free open space, failing to derive multiple plausible
solutions under the same situation. Notably, IL-based methods encompass
inherent causal confusion, so enabling a neural network to generalize across
diverse parking scenarios is particularly difficult. To address these
challenges, we propose MultiPark, an autoregressive transformer for multimodal
parking. To handle paths filled with abrupt turning points, we introduce a
data-efficient next-segment prediction paradigm, enabling spatial
generalization and temporal extrapolation. Furthermore, we design learnable
parking queries factorized into gear, longitudinal, and lateral components,
parallelly decoding diverse parking behaviors. To mitigate causal confusion in
IL, our method employs target-centric pose and ego-centric collision as
outcome-oriented loss across all modalities beyond pure imitation loss.
Evaluations on real-world datasets demonstrate that MultiPark achieves
state-of-the-art performance across various scenarios. We deploy MultiPark on a
production vehicle, further confirming our approach's robustness in real-world
parking environments.

</details>


### [250] [Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads](https://arxiv.org/abs/2508.11547)
*Martin Jiroušek,Tomáš Báča,Martin Saska*

Main category: cs.RO

TL;DR: The paper presents a method to track and control cable-suspended payloads carried by UAVs using only standard onboard sensors, achieving performance comparable to systems with advanced hardware.


<details>
  <summary>Details</summary>
Motivation: Current methods for tracking UAV payloads often rely on specialized hardware like motion-capture systems or external sensors, which limit real-world deployment. This paper seeks to develop a practical solution using minimal hardware.

Method: The system models the full coupled dynamics of the UAV and payload and integrates a linear Kalman filter, a model predictive contouring control planner, and an incremental model predictive controller, relying only on standard onboard sensors.

Result: Simulations show the system performs nearly as well (< 6% degradation) as control strategies using ground-truth measurements. Field experiments confirm robust performance and adaptability to payload parameter variations.

Conclusion: The proposed framework is practical and reliable, demonstrating effective UAV payload tracking and control using minimal, off-the-shelf hardware.

Abstract: This paper addresses the problem of tracking the position of a
cable-suspended payload carried by an unmanned aerial vehicle, with a focus on
real-world deployment and minimal hardware requirements. In contrast to many
existing approaches that rely on motion-capture systems, additional onboard
cameras, or instrumented payloads, we propose a framework that uses only
standard onboard sensors--specifically, real-time kinematic global navigation
satellite system measurements and data from the onboard inertial measurement
unit--to estimate and control the payload's position. The system models the
full coupled dynamics of the aerial vehicle and payload, and integrates a
linear Kalman filter for state estimation, a model predictive contouring
control planner, and an incremental model predictive controller. The control
architecture is designed to remain effective despite sensing limitations and
estimation uncertainty. Extensive simulations demonstrate that the proposed
system achieves performance comparable to control based on ground-truth
measurements, with only minor degradation (< 6%). The system also shows strong
robustness to variations in payload parameters. Field experiments further
validate the framework, confirming its practical applicability and reliable
performance in outdoor environments using only off-the-shelf aerial vehicle
hardware.

</details>


### [251] [Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching](https://arxiv.org/abs/2508.11573)
*Mogens Plessen*

Main category: cs.RO

TL;DR: The paper compares a complex Automatic Section Control (ASC) method for agricultural spraying to a simpler sensor-free alternative that minimizes overlap and cost.


<details>
  <summary>Details</summary>
Motivation: The study seeks a simpler and cost-effective alternative for spraying in agriculture, where current methods like ASC require dynamic sensors and are complex.

Method: The approach evaluates combinations of path planning and spray switching logics with three section-setups: controlling 48 sections, 2 sections, or all uniformly. Real-world fields with diverse layouts are used.

Result: Findings show that the simpler method minimizes path length, offers acceptable overlap, works for manual driving, and eliminates the need for costly sensors.

Conclusion: A simpler predictive spray switching logic is suggested as a viable low-cost, sensor-free alternative to ASC for agricultural spraying.

Abstract: Automatic Section Control (ASC) is a long-standing trend for spraying in
agriculture. It promises to minimise spray overlap areas. The core idea is to
(i) switch off spray nozzles on areas that have already been sprayed, and (ii)
to dynamically adjust nozzle flow rates along the boom bar that holds the spray
nozzles when velocities of boom sections vary during turn maneuvers. ASC is not
possible without sensors, in particular for accurate positioning data. Spraying
and the movement of modern wide boom bars are highly dynamic processes. In
addition, many uncertainty factors have an effect such as cross wind drift,
boom height, nozzle clogging in open-field conditions, and so forth. In view of
this complexity, the natural question arises if a simpler alternative exist.
Therefore, an Automatic Multi-Sections Control method is compared to a proposed
simpler one- or two-sections alternative that uses predictive spray switching.
The comparison is provided under nominal conditions. Agricultural spraying is
intrinsically linked to area coverage path planning and spray switching logic.
Combinations of two area coverage path planning and switching logics as well as
three sections-setups are compared. The three sections-setups differ by
controlling 48 sections, 2 sections or controlling all nozzles uniformly with
the same control signal as one single section. Methods are evaluated on 10
diverse real-world field examples, including non-convex field contours,
freeform mainfield lanes and multiple obstacle areas. A preferred method is
suggested that (i) minimises area coverage pathlength, (ii) offers intermediate
overlap, (iii) is suitable for manual driving by following a pre-planned
predictive spray switching logic for an area coverage path plan, and (iv) and
in contrast to ASC can be implemented sensor-free and therefore at low cost.

</details>


### [252] [Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks](https://arxiv.org/abs/2508.11584)
*Jakub Łucki,Jonathan Becktor,Georgios Georgakis,Robert Royce,Shehryar Khattak*

Main category: cs.RO

TL;DR: The paper introduces VPEngine, a modular framework for efficient GPU usage in multitasking visual perception on resource-constrained robotics platforms, achieving up to 3x speedup and real-time performance.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the inefficiencies and integration complexities of deploying multiple ML models for perception tasks on limited-resource robotic platforms.

Method: VPEngine employs a shared foundation model backbone to extract and share image representations across multiple specialized task heads without unnecessary memory transfers, leveraging CUDA Multi-Process Service for GPU efficiency.

Result: The framework demonstrated up to 3x speedup over sequential model execution and real-time performance (≥50 Hz) with TensorRT optimization on NVIDIA Jetson Orin AGX.

Conclusion: VPEngine enhances visual multitasking efficiency in robotics by reducing computational redundancy and memory usage, while being accessible for diverse platforms through Python and ROS2 C++ bindings.

Abstract: Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.

</details>


### [253] [Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation](https://arxiv.org/abs/2508.11588)
*Benjamin Walt,Jordan Westphal,Girish Krishnan*

Main category: cs.RO

TL;DR: The paper explores the use of multiple sensors integrated into a gripper for accurate grasp state classification during agricultural harvesting, emphasizing the importance of reliable feedback to enhance efficiency and reliability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in agricultural harvesting, such as complex environments and the need for precise fruit separation, by improving grasp state identification through sensor technology.

Method: The study integrates multiple sensors—IMUs, IR reflectance, tension, tactile sensors, and RGB cameras—into a gripper. It evaluates the classification performance of Random Forest and LSTM models and identifies the contributions of individual sensors.

Result: The Random Forest classifier, trained in a lab and tested on real cherry tomato plants, achieved 100% accuracy in identifying grasp states. The IMU and tension sensors were highlighted as an effective minimal sensor combination.

Conclusion: The findings suggest that integrating a minimal viable sensor set with a Random Forest classifier significantly improves grasp state classification, enabling effective real-time corrective actions and enhancing harvesting efficiency.

Abstract: Effective and efficient agricultural manipulation and harvesting depend on
accurately understanding the current state of the grasp. The agricultural
environment presents unique challenges due to its complexity, clutter, and
occlusion. Additionally, fruit is physically attached to the plant, requiring
precise separation during harvesting. Selecting appropriate sensors and
modeling techniques is critical for obtaining reliable feedback and correctly
identifying grasp states. This work investigates a set of key sensors, namely
inertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile
sensors, and RGB cameras, integrated into a compliant gripper to classify grasp
states. We evaluate the individual contribution of each sensor and compare the
performance of two widely used classification models: Random Forest and Long
Short-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest
classifier, trained in a controlled lab environment and tested on real cherry
tomato plants, achieved 100% accuracy in identifying slip, grasp failure, and
successful picks, marking a substantial improvement over baseline performance.
Furthermore, we identify a minimal viable sensor combination, namely IMU and
tension sensors that effectively classifies grasp states. This classifier
enables the planning of corrective actions based on real-time feedback, thereby
enhancing the efficiency and reliability of fruit harvesting operations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [254] [The Impact of Large Language Models (LLMs) on Code Review Process](https://arxiv.org/abs/2508.11034)
*Antonio Collante,Samuel Abedu,SayedHassan Khatoonabadi,Ahmad Abdellatif,Ebube Alor,Emad Shihab*

Main category: cs.SE

TL;DR: This study investigates the impact of GPT on GitHub pull request workflows, focusing on resolution time, review efficiency, and phase-specific improvements. It reveals that GPT assistance significantly boosts productivity across code review phases by reducing resolution, review, and waiting times.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to explore how large language models like GPT can improve efficiency in software development tasks, specifically focusing on unexamined phase-specific effects on pull request workflows.

Method: The research involved curating a dataset of 25,473 pull requests from 9,254 GitHub projects, identifying GPT-assisted PRs via a semi-automated approach, and applying statistical models such as multiple linear regression and Mann-Whitney U tests to evaluate differences.

Result: The study shows that GPT-assisted pull requests significantly reduce resolution time by over 60% (median 9 hours vs. 23 hours without assistance), review time by 33%, and waiting time for acceptance by 87%. Developers mainly use GPT for code optimization (60%), bug fixing (26%), and documentation updates (12%).

Conclusion: Early adoption of GPT can markedly enhance the pull request process, delivering considerable time savings and improved productivity in code reviews. The findings provide actionable strategies for software teams aiming to improve workflows and collaboration.

Abstract: Large language models (LLMs) have recently gained prominence in the field of
software development, significantly boosting productivity and simplifying
teamwork. Although prior studies have examined task-specific applications, the
phase-specific effects of LLM assistance on the efficiency of code review
processes remain underexplored. This research investigates the effect of GPT on
GitHub pull request (PR) workflows, with a focus on reducing resolution time,
optimizing phase-specific performance, and assisting developers. We curated a
dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted
PRs using a semi-automated heuristic approach that combines keyword-based
detection, regular expression filtering, and manual verification until
achieving 95% labeling accuracy. We then applied statistical modeling,
including multiple linear regression and Mann-Whitney U test, to evaluate
differences between GPT-assisted and non-assisted PRs, both at the overall
resolution level and across distinct review phases. Our research has revealed
that early adoption of GPT can substantially boost the effectiveness of the PR
process, leading to considerable time savings at various stages. Our findings
suggest that GPT-assisted PRs reduced median resolution time by more than 60%
(9 hours compared to 23 hours for non-assisted PRs). We discovered that
utilizing GPT can reduce the review time by 33% and the waiting time before
acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we
discovered that developers predominantly use GPT for code optimization (60%),
bug fixing (26%), and documentation updates (12%). This research sheds light on
the impact of the GPT model on the code review process, offering actionable
insights for software teams seeking to enhance workflows and promote seamless
collaboration.

</details>


### [255] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: This paper explores leveraging code diffusion models for the task of repairing broken or incomplete code and for generating training data for similar tasks in Python, Excel, and PowerShell domains.


<details>
  <summary>Details</summary>
Motivation: Existing code diffusion models excel at generating code, but their potential in the realm of fine-tuning and repairing broken or incomplete code in a last-mile repair scenario is underexplored.

Method: The paper proposes two approaches: (1) adding noise to broken code snippets and resuming the diffusion process for repair, and (2) generating training data by sampling intermediate and final programs during the diffusion process.

Result: The experiments conducted on Python, Excel, and PowerShell demonstrate the feasibility and potential of applying code diffusion models for last-mile repair and data generation tasks.

Conclusion: Code diffusion models offer a promising solution for both repairing broken code and creating training datasets for such tasks, expanding their potential applications.

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [256] [AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities](https://arxiv.org/abs/2508.11126)
*Huanting Wang,Jingzhi Gong,Huawei Zhang,Zheng Wang*

Main category: cs.SE

TL;DR: AI agentic programming leverages large language models (LLMs) to autonomously perform complex software tasks by interacting with tools and adapting to feedback. A survey outlines its taxonomy, techniques, and challenges.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to consolidate the rapidly evolving field of AI agentic programming, define its scope, review technical foundations, and identify research challenges.

Method: The authors conducted a comprehensive survey of AI agentic systems, creating a taxonomy of behaviors and architectures, and analyzed techniques such as planning, memory management, tool integration, and execution monitoring.

Result: The paper highlights current challenges like handling long contexts, lack of persistent memory, safety concerns, and alignment issues, while also identifying opportunities to improve reliability and adaptability.

Conclusion: This survey provides a foundation for research and development in next-generation intelligent and trustworthy AI coding agents by synthesizing advances and outlining future directions.

Abstract: AI agentic programming is an emerging paradigm in which large language models
(LLMs) autonomously plan, execute, and interact with external tools like
compilers, debuggers, and version control systems to iteratively perform
complex software development tasks. Unlike conventional code generation tools,
agentic systems are capable of decomposing high-level goals, coordinating
multi-step processes, and adapting their behavior based on intermediate
feedback. These capabilities are transforming the software development
practice. As this emerging field evolves rapidly, there is a need to define its
scope, consolidate its technical foundations, and identify open research
challenges. This survey provides a comprehensive and timely review of AI
agentic programming. We introduce a taxonomy of agent behaviors and system
architectures, and examine core techniques including planning, memory and
context management, tool integration, and execution monitoring. We also analyze
existing benchmarks and evaluation methodologies used to assess coding agent
performance. Our study identifies several key challenges, including limitations
in handling long context, a lack of persistent memory across tasks, and
concerns around safety, alignment with user intent, and collaboration with
human developers. We discuss emerging opportunities to improve the reliability,
adaptability, and transparency of agentic systems. By synthesizing recent
advances and outlining future directions, this survey aims to provide a
foundation for research and development in building the next generation of
intelligent and trustworthy AI coding agents.

</details>


### [257] [From Feedback to Failure: Automated Android Performance Issue Reproduction](https://arxiv.org/abs/2508.11147)
*Zhengquan Li,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: This paper introduces RevPerf, a tool to reproduce mobile app performance issues using Google Play reviews, achieving a 70% success rate.


<details>
  <summary>Details</summary>
Motivation: Mobile app performance issues are hard to detect during development, as they're often less apparent and challenging to diagnose in controlled environments.

Method: RevPerf extracts performance issue details from app reviews, refines them with prompt engineering, generates commands for issue reproduction, and uses Android logs, GUI monitoring, and resource tracking to analyze the issue.

Result: RevPerf successfully reproduced 70% of performance issues in a manually validated dataset.

Conclusion: RevPerf enhances performance testing processes by reliably reproducing and detecting mobile app performance problems using real-world user reviews.

Abstract: Mobile application performance is a vital factor for user experience. Yet,
performance issues are notoriously difficult to detect within development
environments, where their manifestations are often less conspicuous and
diagnosis proves more challenging. To address this limitation, we propose
RevPerf, an advanced performance issue reproduction tool that leverages app
reviews from Google Play to acquire pertinent information. RevPerf employs
relevant reviews and prompt engineering to enrich the original review with
performance issue details. An execution agent is then employed to generate and
execute commands to reproduce the issue. After executing all necessary steps,
the system incorporates multifaceted detection methods to identify performance
issues by monitoring Android logs, GUI changes, and system resource utilization
during the reproduction process. Experimental results demonstrate that our
proposed framework achieves a 70\% success rate in reproducing performance
issues on the dataset we constructed and manually validated.

</details>


### [258] [PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers](https://arxiv.org/abs/2508.11179)
*Pei Liu,Terry Zhuo,Jiawei Deng,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhan*

Main category: cs.SE

TL;DR: The paper introduces PTMPicker, a tool designed to facilitate the search and selection of pretrained models (PTMs) by using a structured template and advanced similarity and prompt-based evaluation techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the limitations of keyword-based searches for pretrained models, which often fail to capture user intent and overlook considerations like bias mitigation, hardware requirements, or license compliance.

Method: The authors developed PTMPicker, which uses a structured template to represent both candidate models and user search requests. It computes embedding similarities for function-related attributes and employs prompts for evaluating non-standard requirements like licenses or hardware compatibility.

Result: PTMPicker, using a dataset of 543,949 pretrained models, achieved a success rate of 85% in locating suitable PTMs for sampled model search requests among the top-10 ranked models.

Conclusion: PTMPicker effectively enhances the search process for pretrained models by incorporating structured representations and tailored evaluation methods, addressing user requirements beyond basic functionality.

Abstract: The rapid emergence of pretrained models (PTMs) has attracted significant
attention from both Deep Learning (DL) researchers and downstream application
developers. However, selecting appropriate PTMs remains challenging because
existing methods typically rely on keyword-based searches in which the keywords
are often derived directly from function descriptions. This often fails to
fully capture user intent and makes it difficult to identify suitable models
when developers also consider factors such as bias mitigation, hardware
requirements, or license compliance. To address the limitations of
keyword-based model search, we propose PTMPicker to accurately identify
suitable PTMs. We first define a structured template composed of common and
essential attributes for PTMs and then PTMPicker represents both candidate
models and user-intended features (i.e., model search requests) in this unified
format. To determine whether candidate models satisfy user requirements, it
computes embedding similarities for function-related attributes and uses
well-crafted prompts to evaluate special constraints such as license compliance
and hardware requirements. We scraped a total of 543,949 pretrained models from
Hugging Face to prepare valid candidates for selection. PTMPicker then
represented them in the predefined structured format by extracting their
associated descriptions. Guided by the extracted metadata, we synthesized a
total of 15,207 model search requests with carefully designed prompts, as no
such search requests are readily available. Experiments on the curated PTM
dataset and the synthesized model search requests show that PTMPicker can help
users effectively identify models,with 85% of the sampled requests successfully
locating appropriate PTMs within the top-10 ranked candidates.

</details>


### [259] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: The paper introduces ORFuzz, an evolutionary testing framework to detect over-refusals in LLMs, showcasing improved detection rates and creating a robust new testing dataset.


<details>
  <summary>Details</summary>
Motivation: Large Language Models often reject benign queries due to over-cautious safety measures, affecting their usability and reliability.

Method: The authors propose ORFuzz, which includes seed selection for safety category coverage, adaptive mutator optimization for creating test cases, and a human-aligned judge model validated for user perception.

Result: ORFuzz doubles the detection rate of over-refusals compared to baselines, producing 1,855 test cases forming a benchmark (ORFuzzSet) that achieves a 63.56% average over-refusal detection rate across 10 LLMs.

Conclusion: ORFuzz establishes a new standard for detecting vulnerabilities in LLMs' refusal behavior, paving the way for safer and more reliable systems while providing valuable tools and datasets for the research community.

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>


### [260] [Hallucination in LLM-Based Code Generation: An Automotive Case Study](https://arxiv.org/abs/2508.11257)
*Marc Pavel,Nenad Petrovic,Lukasz Mazur,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: This paper examines hallucinations in code generation by Large Language Models (LLMs), particularly in the automotive domain. It shows that even state-of-the-art models like GPT-4.1 can struggle without rich prompts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of hallucinations in code generation by LLMs, which limit their practical application, especially in safety-critical areas like automotive software.

Method: The study evaluated code generation by LLMs using three levels of prompting complexity: simple prompts, context-enhanced prompts with Covesa VSS, and prompts with code skeletons.

Result: The evaluation uncovered frequent syntax violations and API errors. Only the most context-rich prompts enabled models like GPT-4.1 to generate correct solutions.

Conclusion: LLM-generated code requires effective mitigation techniques to ensure safety and reliability, especially in domains like automotive software development.

Abstract: Large Language Models (LLMs) have shown significant potential in automating
code generation tasks offering new opportunities across software engineering
domains. However, their practical application remains limited due to
hallucinations - outputs that appear plausible but are factually incorrect,
unverifiable or nonsensical. This paper investigates hallucination phenomena in
the context of code generation with a specific focus on the automotive domain.
A case study is presented that evaluates multiple code LLMs for three different
prompting complexities ranging from a minimal one-liner prompt to a prompt with
Covesa Vehicle Signal Specifications (VSS) as additional context and finally to
a prompt with an additional code skeleton. The evaluation reveals a high
frequency of syntax violations, invalid reference errors and API knowledge
conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the
evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct
solution when given the most context-rich prompt. Simpler prompting strategies
failed to yield a working result, even after multiple refinement iterations.
These findings highlight the need for effective mitigation techniques to ensure
the safe and reliable use of LLM generated code, especially in safety-critical
domains such as automotive software systems.

</details>


### [261] [Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning](https://arxiv.org/abs/2508.11305)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: The paper discusses logging code defects and evaluates the ability of large language models (LLMs) to detect them, proposing strategies for improved detection.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic and comprehensive analysis of logging code defects and explore LLMs' capability in detecting such defects.

Method: They derive a taxonomy of defects, construct a dataset of verified logging defects, and propose a framework using LLMs with various strategies.

Result: LLMs struggle with defect detection using source code alone; incorporating knowledge improves accuracy by 10.9%.

Conclusion: The study highlights areas for improvement in LLM-based logging defect detection and provides actionable guidance to developers.

Abstract: Logging code is written by developers to capture system runtime behavior and
plays a vital role in debugging, performance analysis, and system monitoring.
However, defects in logging code can undermine the usefulness of logs and lead
to misinterpretations. Although prior work has identified several logging
defect patterns and provided valuable insights into logging practices, these
studies often focus on a narrow range of defect patterns derived from limited
sources (e.g., commit histories) and lack a systematic and comprehensive
analysis. Moreover, large language models (LLMs) have demonstrated promising
generalization and reasoning capabilities across a variety of code-related
tasks, yet their potential for detecting logging code defects remains largely
unexplored.
  In this paper, we derive a comprehensive taxonomy of logging code defects,
which encompasses seven logging code defect patterns with 14 detailed
scenarios. We further construct a benchmark dataset, \dataset, consisting of
164 developer-verified real-world logging defects. Then we propose an automated
framework that leverages various prompting strategies and contextual
information to evaluate LLMs' capability in detecting and reasoning logging
code defects. Experimental results reveal that LLMs generally struggle to
accurately detect and reason logging code defects based on the source code
only. However, incorporating proper knowledge (e.g., detailed scenarios of
defect patterns) can lead to 10.9\% improvement in detection accuracy. Overall,
our findings provide actionable guidance for practitioners to avoid common
defect patterns and establish a foundation for improving LLM-based reasoning in
logging code defect detection.

</details>


### [262] [TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation](https://arxiv.org/abs/2508.11468)
*Zhihao Gong,Zeyu Sun,Dong Huang,Qingyuan Liang,Jie M. Zhang,Dan Hao*

Main category: cs.SE

TL;DR: TRACY introduces a benchmark to measure execution efficiency in LLM-translated code. Current models excel in correctness but lack optimization for runtime and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs focus heavily on the correctness of code translation but neglect execution efficiency, which is crucial for practical use.

Method: The TRACY benchmark was developed via a two-stage LLM-driven pipeline: stress-test generation for highlighting performance differences and pruning tasks to focus on efficiency-distinguishing challenges.

Result: TRACY evaluated 26 LLMs over 1,011 code translation tasks, showing top models falter on execution efficiency, with notable slowdowns and memory impacts.

Conclusion: Addressing execution efficiency alongside correctness is vital for advancing LLM-based code translation effectively.

Abstract: Automatic code translation is a fundamental task in modern software
development. While the advent of Large Language Models (LLMs) has significantly
improved the correctness of code translation, the critical dimension of
execution efficiency remains overlooked. To address this gap, we introduce
TRACY, the first comprehensive benchmark designed to evaluate the execution
efficiency of LLM-translated code. TRACY is constructed through an LLM-driven
two-stage pipeline: an initial stage generates a suite of stress tests to
amplify performance differences, followed by an efficiency-oriented task
pruning stage that isolates the efficiency-distinguishing tasks. The resulting
benchmark comprises 1,011 code translation tasks across C++, Java, and Python,
each accompanied by an average of 22.1 verified reference translations and 10
computationally demanding tests. Our extensive evaluation of 26 representative
LLMs reveals that even top-tier LLMs struggle to consistently produce efficient
code translations. For instance, Claude-4-think, the leading model for
correctness, ranks eighth overall when time efficiency is taken into account,
surpassed by several smaller open-source models. We further pinpoint that
algorithmic flaws and improper resource handling are the most detrimental,
causing a median time slowdown of 5.6$\times$ and memory increase of
12.0$\times$, respectively. Our work underscores the necessity of jointly
optimizing for correctness and efficiency in future LLM-based code translation.

</details>


### [263] [Temporal Network Analysis of Microservice Architectural Degradation](https://arxiv.org/abs/2508.11571)
*Alexander Bakhtin*

Main category: cs.SE

TL;DR: The paper discusses using temporal network analysis to study microservice architectures across time, focusing on the challenges in obtaining and analyzing such networks.


<details>
  <summary>Details</summary>
Motivation: To explore how temporal network analysis can enhance the understanding of evolving microservice architectures within digital systems.

Method: Temporal network analysis was applied to microservice system architectures, using a limited dataset of 7 time instances and 42 microservices to study system evolution.

Result: The research uncovers challenges in obtaining comprehensive temporal networks for analysis and demonstrates limited scalability due to the small dataset size.

Conclusion: Temporal network analysis has potential for studying microservice systems, but its application is constrained by data availability and scale limitations.

Abstract: Microservice architecture can be modeled as a network of microservices making
calls to each other, commonly known as the service dependency graph. Network
Science can provide methods to study such networks. In particular, temporal
network analysis is a branch of Network Science that analyzes networks evolving
with time. In microservice systems, temporal networks can arise if we examine
the architecture of the system across releases or monitor a deployed system
using tracing.
  In this research summary paper, I discuss the challenges in obtaining
temporal networks from microservice systems and analyzing them with the
temporal network methods. In particular, the most complete temporal network
that we could obtain contains 7 time instances and 42 microservices, which
limits the potential analysis that could be applied.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [264] [Repetitive TMS-based Identification of Methamphetamine-Dependent Individuals Using EEG Spectra](https://arxiv.org/abs/2508.11312)
*Ziyi Zeng,Yun-Hsuan Chen,Xurong Gao,Wenyao Zheng,Hemmings Wu,Zhoule Zhu,Jie Yang,Chengkai Wang,Lihua Zhong,Weiwei Cheng,Mohamad Sawan*

Main category: q-bio.NC

TL;DR: The study investigates gamma relative band power (RBP) in EEG signals as a more objective measure to assess the effectiveness of rTMS treatment in methamphetamine (METH) addiction.


<details>
  <summary>Details</summary>
Motivation: Current methods to measure craving levels of METH users rely on questionnaires, which may lack objectivity. Neural signals are explored as an alternative for precise, real-time assessment.

Method: EEG signals were collected from 20 methamphetamine-addicted participants before and after rTMS treatment and 20 healthy participants. Gamma RBP across brain regions was analyzed; random forest classification was employed to validate the findings.

Result: Gamma RBP, especially recorded via TP10 and CP2 channels, shows significant changes post rTMS. Using random forest, the study achieved a 90% accuracy in distinguishing pre-treatment from healthy subjects. Post-treatment results were closer to healthy subjects but still less distinct.

Conclusion: Gamma RBP from specific EEG channels, particularly during exposure to METH-related cues, can serve as a biomarker for evaluating rTMS effectiveness. This makes it suitable for real-time monitoring in neuromodulation systems for METH addiction treatment.

Abstract: The impact of repetitive transcranial magnetic stimulation (rTMS) on
methamphetamine (METH) users' craving levels is often assessed using
questionnaires. This study explores the feasibility of using neural signals to
obtain more objective results. EEG signals recorded from 20 METH-addicted
participants Before and After rTMS (MBT and MAT) and from 20 healthy
participants (HC) are analyzed. In each EEG paradigm, participants are shown 15
METH-related and 15 neutral pictures randomly, and the relative band power
(RBP) of each EEG sub-band frequency is derived. The average RBP across all 31
channels, as well as individual brain regions, is analyzed. Statistically,
MAT's alpha, beta, and gamma RBPs are more like those of HC compared to MBT, as
indicated by the power topographies. Utilizing a random forest (RF), the gamma
RBP is identified as the optimal frequency band for distinguishing between MBT
and HC with a 90% accuracy. The performance of classifying MAT versus HC is
lower than that of MBT versus HC, suggesting that the efficacy of rTMS can be
validated using RF with gamma RBP. Furthermore, the gamma RBP recorded by the
TP10 and CP2 channels dominates the classification task of MBT versus HC when
receiving METH-related image cues. The gamma RBP during exposure to
METH-related cues can serve as a biomarker for distinguishing between MBT and
HC and for evaluating the effectiveness of rTMS. Therefore, real-time
monitoring of gamma RBP variations holds promise as a parameter for
implementing a customized closed-loop neuromodulation system for treating METH
addiction.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [265] [Non-asymptotic convergence bound of conditional diffusion models](https://arxiv.org/abs/2508.10944)
*Mengze Li*

Main category: stat.ML

TL;DR: This paper introduces a conditional diffusion model called CARD for learning original distributions with inputs, integrating pre-trained models to enhance precision and theoretical foundations.


<details>
  <summary>Details</summary>
Motivation: Conditional diffusion models have progressed in acceleration and generation quality but lack robust non-asymptotic theoretical properties, hindering further research.

Method: The CARD model integrates pre-trained models into diffusion frameworks to approximate conditional distributions, develops stochastic differential equations for theoretical backing, and uses Wasserstein distance metrics to assess error bounds.

Result: The paper establishes error and convergence upper bounds for the CARD model under Lipschitz and light-tailed distribution assumptions, validating the proposed framework’s precision against the original distribution.

Conclusion: The study provides theoretical advancements for conditional diffusion models, solidifying their foundation and offering reliable tools for enhanced data learning and generation.

Abstract: Learning and generating various types of data based on conditional diffusion
models has been a research hotspot in recent years. Although conditional
diffusion models have made considerable progress in improving acceleration
algorithms and enhancing generation quality, the lack of non-asymptotic
properties has hindered theoretical research. To address this gap, we focus on
a conditional diffusion model within the domains of classification and
regression (CARD), which aims to learn the original distribution with given
input x (denoted as Y|X). It innovatively integrates a pre-trained model
f_{\phi}(x) into the original diffusion model framework, allowing it to
precisely capture the original conditional distribution given f (expressed as
Y|f_{\phi}(x)). Remarkably, when f_{\phi}(x) performs satisfactorily,
Y|f_{\phi}(x) closely approximates Y|X. Theoretically, we deduce the stochastic
differential equations of CARD and establish its generalized form predicated on
the Fokker-Planck equation, thereby erecting a firm theoretical foundation for
analysis. Mainly under the Lipschitz assumptions, we utilize the second-order
Wasserstein distance to demonstrate the upper error bound between the original
and the generated conditional distributions. Additionally, by appending
assumptions such as light-tailedness to the original distribution, we derive
the convergence upper bound between the true value analogous to the score
function and the corresponding network-estimated value.

</details>


### [266] [Counterfactual Survival Q Learning for Longitudinal Randomized Trials via Buckley James Boosting](https://arxiv.org/abs/2508.11060)
*Jeongjin Lee,Jong-Min Kim*

Main category: stat.ML

TL;DR: The paper presents a Buckley James (BJ) Boost Q learning framework to estimate optimal dynamic treatment regimes for right-censored survival data, offering a more robust alternative to Cox-based Q learning.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations in current methodologies for estimating treatment regimes under right-censored survival data, particularly in the context of multistage settings and biases arising from model misspecifications.

Method: The proposed method uses accelerated failure time models combined with iterative boosting techniques, like componentwise least squares and regression trees, within a counterfactual Q learning framework, avoiding proportional hazards assumptions.

Result: Simulation studies and the analysis of the ACTG175 HIV trial show that BJ Boost Q learning enhances treatment decision accuracy, particularly in multistage contexts prone to bias.

Conclusion: The BJ Boost Q learning framework offers a more flexible and unbiased alternative to Cox-based methods for treatment regime estimation, ensuring robustness even in complex, multistage scenarios.

Abstract: We propose a Buckley James (BJ) Boost Q learning framework for estimating
optimal dynamic treatment regimes under right censored survival data, tailored
for longitudinal randomized clinical trial settings. The method integrates
accelerated failure time models with iterative boosting techniques, including
componentwise least squares and regression trees, within a counterfactual Q
learning framework. By directly modeling conditional survival time, BJ Boost Q
learning avoids the restrictive proportional hazards assumption and enables
unbiased estimation of stage specific Q functions. Grounded in potential
outcomes, this framework ensures identifiability of the optimal treatment
regime under standard causal assumptions. Compared to Cox based Q learning,
which relies on hazard modeling and may suffer from bias under
misspecification, our approach provides robust and flexible estimation.
Simulation studies and analysis of the ACTG175 HIV trial demonstrate that BJ
Boost Q learning yields higher accuracy in treatment decision making,
especially in multistage settings where bias can accumulate.

</details>


### [267] [Uniform convergence for Gaussian kernel ridge regression](https://arxiv.org/abs/2508.11274)
*Paul Dommel,Rajmadan Lakshmanan*

Main category: stat.ML

TL;DR: This paper provides the first polynomial convergence rates for Gaussian kernel ridge regression (KRR) with fixed hyperparameters in both uniform and $L^2$-norm metrics.


<details>
  <summary>Details</summary>
Motivation: To address the gap in theoretical understanding of Gaussian kernel ridge regression, particularly in providing convergence rates for fixed hyperparameters.

Method: The authors derive theoretical polynomial convergence bounds for Gaussian kernel ridge regression, analyzing the uniform metric and $L^2$-norm under fixed hyperparameters and kernel width.

Result: Polynomial convergence rates are established for both uniform and $L^2$-norms in Gaussian KRR, overcoming prior limitations where such rates were either unknown or sub-polynomial.

Conclusion: The findings broaden the theoretical justification for employing Gaussian kernel ridge regression with fixed hyperparameters, supporting its use in nonparametric regression applications.

Abstract: This paper establishes the first polynomial convergence rates for Gaussian
kernel ridge regression (KRR) with a fixed hyperparameter in both the uniform
and the $L^{2}$-norm. The uniform convergence result closes a gap in the
theoretical understanding of KRR with the Gaussian kernel, where no such rates
were previously known. In addition, we prove a polynomial $L^{2}$-convergence
rate in the case, where the Gaussian kernel's width parameter is fixed. This
also contributes to the broader understanding of smooth kernels, for which
previously only sub-polynomial $L^{2}$-rates were known in similar settings.
Together, these results provide new theoretical justification for the use of
Gaussian KRR with fixed hyperparameters in nonparametric regression.

</details>


### [268] [ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization](https://arxiv.org/abs/2508.11551)
*Shengzhuang Chen,Xu Ouyang,Michael Arthur Leopold Pearce,Thomas Hartvigsen,Jonathan Richard Schwarz*

Main category: stat.ML

TL;DR: This paper introduces a learning-based optimization method for selecting optimal data mixtures in large language model training using Multi-fidelity Bayesian Optimization, yielding significant efficiency improvements and releasing a valuable dataset.


<details>
  <summary>Details</summary>
Motivation: Determining the best data mixture for training large language models impacts performance significantly, but current approaches rely on heuristics due to the lack of reliable learning-based methods.

Method: The paper frames data mixture selection as a black-box hyperparameter optimization problem and proposes Multi-fidelity Bayesian Optimization to balance computational cost and model fit, with experiments across model sizes and architectures.

Result: The proposed approach delivered speed-ups exceeding 500% compared to recent methods for determining optimal data mixtures and produced strong results across benchmarks and datasets.

Conclusion: Multi-fidelity Bayesian Optimization effectively optimizes data mixtures in large language model training, reducing experimentation costs and improving practical accessibility with the release of a comprehensive training dataset.

Abstract: Determining the optimal data mixture for large language model training
remains a challenging problem with an outsized impact on performance. In
practice, language model developers continue to rely on heuristic exploration
since no learning-based approach has emerged as a reliable solution. In this
work, we propose to view the selection of training data mixtures as a black-box
hyperparameter optimization problem, for which Bayesian Optimization is a
well-established class of appropriate algorithms. Firstly, we cast data mixture
learning as a sequential decision-making problem, in which we aim to find a
suitable trade-off between the computational cost of training exploratory
(proxy-) models and final mixture performance. Secondly, we systematically
explore the properties of transferring mixtures learned at a small scale to
larger-scale experiments, providing insights and highlighting opportunities for
research at a modest scale. By proposing Multi-fidelity Bayesian Optimization
as a suitable method in this common scenario, we introduce a natural framework
to balance experiment cost with model fit, avoiding the risks of overfitting to
smaller scales while minimizing the number of experiments at high cost. We
present results for pre-training and instruction finetuning across models
ranging from 1 million to 7 billion parameters, varying from simple
architectures to state-of-the-art models and benchmarks spanning dozens of
datasets. We demonstrate consistently strong results relative to a wide range
of benchmarks, showingspeed-ups of over 500% in determining the best data
mixture on our largest experiments relative to recent baselines. In addition,
we broaden access to research by sharing ADMIRE IFT Runs, a dataset of 460 full
training & evaluation runs across various model sizes worth over 13,000 GPU
hours, greatly reducing the cost of conducting research in this area.

</details>


### [269] [Nonparametric learning of stochastic differential equations from sparse and noisy data](https://arxiv.org/abs/2508.11597)
*Arnab Ganguly,Riten Mitra,Jinpu Zhou*

Main category: stat.ML

TL;DR: This paper introduces a novel framework to model complex scientific dynamics using sparse and noisy data through stochastic differential equations, focusing on learning the drift function directly without relying on predefined assumptions.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address challenges in scientific modeling where system dynamics are either partially understood or overly complex, and traditional parametric methods may fail due to their reliance on predefined assumptions about dynamic structures.

Method: The authors utilize penalized negative log-likelihood optimization within a reproducing kernel Hilbert space (RKHS), employing an Expectation-Maximization (EM) algorithm with a Sequential Monte Carlo (SMC) approach. They enhance the algorithm with a hybrid Bayesian variant for complexity control.

Result: The proposed method, EM-SMC-RKHS, accurately estimates the drift function in stochastic models with sparse observational data, demonstrating effective performance through numerical experiments.

Conclusion: The framework provides a robust solution for continuous-time modeling under observational constraints, with theoretical guarantees for convergence and wide applicability across scientific domains.

Abstract: The paper proposes a systematic framework for building data-driven stochastic
differential equation (SDE) models from sparse, noisy observations. Unlike
traditional parametric approaches, which assume a known functional form for the
drift, our goal here is to learn the entire drift function directly from data
without strong structural assumptions, making it especially relevant in
scientific disciplines where system dynamics are partially understood or highly
complex. We cast the estimation problem as minimization of the penalized
negative log-likelihood functional over a reproducing kernel Hilbert space
(RKHS). In the sparse observation regime, the presence of unobserved trajectory
segments makes the SDE likelihood intractable. To address this, we develop an
Expectation-Maximization (EM) algorithm that employs a novel Sequential Monte
Carlo (SMC) method to approximate the filtering distribution and generate Monte
Carlo estimates of the E-step objective. The M-step then reduces to a penalized
empirical risk minimization problem in the RKHS, whose minimizer is given by a
finite linear combination of kernel functions via a generalized representer
theorem. To control model complexity across EM iterations, we also develop a
hybrid Bayesian variant of the algorithm that uses shrinkage priors to identify
significant coefficients in the kernel expansion. We establish important
theoretical convergence results for both the exact and approximate EM
sequences. The resulting EM-SMC-RKHS procedure enables accurate estimation of
the drift function of stochastic dynamical systems in low-data regimes and is
broadly applicable across domains requiring continuous-time modeling under
observational constraints. We demonstrate the effectiveness of our method
through a series of numerical experiments.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [270] [Weighted First Order Model Counting for Two-variable Logic with Axioms on Two Relations](https://arxiv.org/abs/2508.11515)
*Qipeng Kuang,Václav Kůla,Ondřej Kuželka,Yuanhong Wang,Yuyi Wang*

Main category: cs.LO

TL;DR: This paper investigates Weighted First-Order Model Counting (WFOMC) and explores the computational complexity of extending the two-variable logic fragment ($\text{FO}^2$) with certain axioms on two relations. It presents both negative and positive results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand the complexity boundary of WFOMC when extending $\text{FO}^2$ with axioms involving multiple relations, addressing a gap in earlier studies which focused on single-relation axioms.

Method: The authors analyze the computational complexity of specific axiom extensions to $\text{FO}^2$, considering cases like two linear order relations, two acyclic relations, and a combination of linear and successor relations.

Result: The paper shows $\mathsf{\#P_1}$-hardness for $\text{FO}^2$ extended with two linear order relations and two acyclic relations. It also provides a polynomial-time algorithm for WFOMC of $\text{C}^2$ with specific linear and successor relations.

Conclusion: Extending $\text{FO}^2$ with axioms on multiple relations can lead to significant computational complexity, varying between intractable and efficiently solvable cases. These findings clarify the boundary for tractable WFOMC in such scenarios.

Abstract: The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the
weighted sum of models of a given first-order logic sentence over a given
domain. The boundary between fragments for which WFOMC can be computed in
polynomial time relative to the domain size lies between the two-variable
fragment ($\text{FO}^2$) and the three-variable fragment ($\text{FO}^3$). It is
known that WFOMC for \FOthree{} is $\mathsf{\#P_1}$-hard while polynomial-time
algorithms exist for computing WFOMC for $\text{FO}^2$ and $\text{C}^2$,
possibly extended by certain axioms such as the linear order axiom, the
acyclicity axiom, and the connectedness axiom. All existing research has
concentrated on extending the fragment with axioms on a single distinguished
relation, leaving a gap in understanding the complexity boundary of axioms on
multiple relations. In this study, we explore the extension of the two-variable
fragment by axioms on two relations, presenting both negative and positive
results. We show that WFOMC for $\text{FO}^2$ with two linear order relations
and $\text{FO}^2$ with two acyclic relations are $\mathsf{\#P_1}$-hard.
Conversely, we provide an algorithm in time polynomial in the domain size for
WFOMC of $\text{C}^2$ with a linear order relation, its successor relation and
another successor relation.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [271] [Data-driven global ocean model resolving ocean-atmosphere coupling dynamics](https://arxiv.org/abs/2508.10908)
*Jeong-Hwan Kim,Daehyun Kang,Young-Min Yang,Jae-Heung Park,Yoo-Geun Ham*

Main category: physics.ao-ph

TL;DR: The study introduces KIST-Ocean, a deep learning-based global 3D ocean circulation model that outperforms traditional methods in predicting ocean-atmosphere interactions and can represent key climate phenomena like El Niño.


<details>
  <summary>Details</summary>
Motivation: To extend weather forecasting to subseasonal and longer timescales by developing a deep learning-based coupled ocean-atmosphere model capable of simulating complex oceanic responses to atmospheric drivers.

Method: The researchers developed KIST-Ocean, leveraging a U-shaped visual attention adversarial network architecture integrated with partial convolution, adversarial training, and transfer learning to improve predictive accuracy and handle challenges in coastal areas.

Result: The model demonstrated strong predictive accuracy, computational efficiency, and the capability to simulate realistic ocean phenomena, including wave propagation and vertical motions influenced by wind stress.

Conclusion: KIST-Ocean highlights the potential of deep learning for advancing global weather and climate prediction, specifically in improving Earth system models and supporting long-term climate forecasting.

Abstract: Artificial intelligence has advanced global weather forecasting,
outperforming traditional numerical models in both accuracy and computational
efficiency. Nevertheless, extending predictions beyond subseasonal timescales
requires the development of deep learning (DL)-based ocean-atmosphere coupled
models that can realistically simulate complex oceanic responses to atmospheric
forcing. This study presents KIST-Ocean, a DL-based global three-dimensional
ocean general circulation model using a U-shaped visual attention adversarial
network architecture. KIST-Ocean integrates partial convolution, adversarial
training, and transfer learning to address coastal complexity and predictive
distribution drift in auto-regressive models. Comprehensive evaluations
confirmed the model's robust ocean predictive skill and efficiency. Moreover,
it accurately captures realistic ocean response, such as Kelvin and Rossby wave
propagation in the tropical Pacific, and vertical motions induced by cyclonic
and anticyclonic wind stress, demonstrating its ability to represent key
ocean-atmosphere coupling mechanisms underlying climate phenomena, including
the El Nino-Southern Oscillation. These findings reinforce confidence in
DL-based global weather and climate models and their extending DL-based
approaches to broader Earth system modeling, offering potential for enhancing
climate prediction capabilities.

</details>


### [272] [Approximating the universal thermal climate index using sparse regression with orthogonal polynomials](https://arxiv.org/abs/2508.11307)
*Sabin Roman,Gregor Skok,Ljupco Todorovski,Saso Dzeroski*

Main category: physics.ao-ph

TL;DR: The paper presents novel data-driven approaches using symbolic and sparse regression to approximate the Universal Thermal Climate Index (UTCI) more accurately and efficiently.


<details>
  <summary>Details</summary>
Motivation: To improve the interpretability and accuracy of modeling the Universal Thermal Climate Index (UTCI), a complex, multivariable metric used for assessing thermal comfort.

Method: Symbolic and sparse regression techniques were utilized with orthogonal polynomial bases, particularly Legendre polynomials, to create function approximations of UTCI. These models aimed to enhance efficiency and maintain interpretability.

Result: The proposed models outperformed the state-of-the-art sixth-degree polynomial benchmark, delivering lower root-mean-squared losses with fewer parameters and achieving robust generalization even with limited training data.

Conclusion: Combining sparsity, orthogonality, and symbolic structure enables interpretable and efficient modeling of complex environmental indices like UTCI, surpassing traditional methods in accuracy and stability.

Abstract: This article explores novel data-driven modeling approaches for analyzing and
approximating the Universal Thermal Climate Index (UTCI), a
physiologically-based metric integrating multiple atmospheric variables to
assess thermal comfort. Given the nonlinear, multivariate structure of UTCI, we
investigate symbolic and sparse regression techniques as tools for
interpretable and efficient function approximation. In particular, we highlight
the benefits of using orthogonal polynomial bases-such as Legendre
polynomials-in sparse regression frameworks, demonstrating their advantages in
stability, convergence, and hierarchical interpretability compared to standard
polynomial expansions. We demonstrate that our models achieve significantly
lower root-mean squared losses than the widely used sixth-degree polynomial
benchmark-while using the same or fewer parameters. By leveraging Legendre
polynomial bases, we construct models that efficiently populate a Pareto front
of accuracy versus complexity and exhibit stable, hierarchical coefficient
structures across varying model capacities. Training on just 20% of the data,
our models generalize robustly to the remaining 80%, with consistent
performance under bootstrapping. The decomposition effectively approximates the
UTCI as a Fourier-like expansion in an orthogonal basis, yielding results near
the theoretical optimum in the L2 (least squares) sense. We also connect these
findings to the broader context of equation discovery in environmental
modeling, referencing probabilistic grammar-based methods that enforce domain
consistency and compactness in symbolic expressions. Taken together, these
results illustrate how combining sparsity, orthogonality, and symbolic
structure enables robust, interpretable modeling of complex environmental
indices like UTCI - and significantly outperforms the state-of-the-art
approximation in both accuracy and efficiency.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [273] [StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation](https://arxiv.org/abs/2508.11203)
*Seungmi Lee,Kwan Yun,Junyong Noh*

Main category: cs.GR

TL;DR: StyleMM is a framework for creating stylized 3D Morphable Models (3DMM) based on user-defined text descriptions, using mesh deformation and texture generation enhanced by diffusion models.


<details>
  <summary>Details</summary>
Motivation: To provide a method for creating stylized 3D models guided by text, while maintaining crucial facial attributes and allowing explicit control over facial parameters.

Method: The framework fine-tunes pre-trained mesh deformation and texture generator models using text-guided image stylization from a diffusion model. It maintains facial attribute consistency during stylization.

Result: StyleMM generates consistent, animatable stylized 3D face meshes that excel in identity-level diversity and stylization, outperforming existing methods.

Conclusion: StyleMM successfully achieves high-quality stylized 3D mesh generation with explicit parameter control, advancing the state of the art in 3D facial stylization.

Abstract: We introduce StyleMM, a novel framework that can construct a stylized 3D
Morphable Model (3DMM) based on user-defined text descriptions specifying a
target style. Building upon a pre-trained mesh deformation network and a
texture generator for original 3DMM-based realistic human faces, our approach
fine-tunes these models using stylized facial images generated via text-guided
image-to-image (i2i) translation with a diffusion model, which serve as
stylization targets for the rendered mesh. To prevent undesired changes in
identity, facial alignment, or expressions during i2i translation, we introduce
a stylization method that explicitly preserves the facial attributes of the
source image. By maintaining these critical attributes during image
stylization, the proposed approach ensures consistent 3D style transfer across
the 3DMM parameter space through image-based training. Once trained, StyleMM
enables feed-forward generation of stylized face meshes with explicit control
over shape, expression, and texture parameters, producing meshes with
consistent vertex connectivity and animatability. Quantitative and qualitative
evaluations demonstrate that our approach outperforms state-of-the-art methods
in terms of identity-level facial diversity and stylization capability. The
code and videos are available at
[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).

</details>


### [274] [SPG: Style-Prompting Guidance for Style-Specific Content Creation](https://arxiv.org/abs/2508.11476)
*Qian Liang,Zichong Chen,Yang Zhou,Hui Huang*

Main category: cs.GR

TL;DR: The paper introduces Style-Prompting Guidance (SPG), a new method for controlling visual styles during text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image diffusion models lack effective tools to control the visual style of generated images while maintaining alignment with prompts.

Method: The proposed Style-Prompting Guidance creates a style noise vector and adjusts the diffusion process with its directional deviation to align the generated images with the desired style.

Result: SPG demonstrates semantic fidelity and style consistency, outperforming state-of-the-art methods in extensive experiments.

Conclusion: SPG is a practical and versatile approach that combines style control with semantic alignment, compatible with frameworks like ControlNet and IPAdapter.

Abstract: Although recent text-to-image (T2I) diffusion models excel at aligning
generated images with textual prompts, controlling the visual style of the
output remains a challenging task. In this work, we propose Style-Prompting
Guidance (SPG), a novel sampling strategy for style-specific image generation.
SPG constructs a style noise vector and leverages its directional deviation
from unconditional noise to guide the diffusion process toward the target style
distribution. By integrating SPG with Classifier-Free Guidance (CFG), our
method achieves both semantic fidelity and style consistency. SPG is simple,
robust, and compatible with controllable frameworks like ControlNet and
IPAdapter, making it practical and widely applicable. Extensive experiments
demonstrate the effectiveness and generality of our approach compared to
state-of-the-art methods. Code is available at
https://github.com/Rumbling281441/SPG.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [275] [FLUID: Flow-Latent Unified Integration via Token Distillation for Expert Specialization in Multimodal Learning](https://arxiv.org/abs/2508.07264)
*Van Duc Cuong,Ta Dinh Tam,Tran Duc Chinh,Nguyen Thi Hanh*

Main category: cs.SI

TL;DR: The paper introduces FLUID, a token-level multimodal classification method that achieves state-of-the-art accuracy while addressing issues like noise and inefficiency in modality integration.


<details>
  <summary>Details</summary>
Motivation: Improving cross-modal robustness and scalability for multimodal classification, which remains vulnerable to modality-specific noise and lacks efficiency.

Method: FLUID introduces three key components: (1) Q-transforms for token-level feature distillation; (2) a two-stage fusion approach for cross-modal consistency and task-aware integration; and (3) a Mixture-of-Experts technique for specialized prediction.

Result: FLUID achieves 91% accuracy on the GLAMI-1M benchmark, surpassing prior models, and demonstrates resilience to noise, class imbalance, and semantic diversity.

Conclusion: FLUID is a scalable and robust solution for multimodal classification capable of addressing modality-specific challenges and achieving high performance.

Abstract: Multimodal classification requires robust integration of visual and textual
signals, yet common fusion strategies are brittle and vulnerable to
modality-specific noise. In this paper, we present \textsc{FLUID}-Flow-Latent
Unified Integration via Token Distillation for Expert Specialization, a
principled token-level pipeline that improves cross-modal robustness and
scalability. \textsc{FLUID} contributes three core elements: (1)
\emph{Q-transforms}, learnable query tokens that distill and retain salient
token-level features from modality-specific backbones; (2) a two-stage fusion
scheme that enforces cross-modal consistency via contrastive alignment and then
performs adaptive, task-aware fusion through a gating mechanism and a
\emph{Q-bottleneck} that selectively compresses information for downstream
reasoning; and (3) a lightweight, load-balanced Mixture-of-Experts at
prediction time that enables efficient specialization to diverse semantic
patterns. Extensive experiments demonstrate that \textsc{FLUID} attains
\(91\%\) accuracy on the GLAMI-1M benchmark, significantly outperforming prior
baselines and exhibiting strong resilience to label noise, long-tail class
imbalance, and semantic heterogeneity. Targeted ablation studies corroborate
both the individual and synergistic benefits of the proposed components,
positioning \textsc{FLUID} as a scalable, noise-resilient solution for
multimodal product classification.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [276] [The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers](https://arxiv.org/abs/2506.20844)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: Current scientific fact-checking systems are limited by their reliance on simplified datasets and need improvements in evidence retrieval, structured parsing, and credibility assessment to handle full research papers.


<details>
  <summary>Details</summary>
Motivation: Scientific fact-checking is essential for verifying claims by analyzing evidence from complex, evolving research literature.

Method: The paper identifies key challenges in evidence retrieval, including semantic limitations, citation tracking, structured parsing, and handling multimodal scientific expressions.

Result: Preliminary experiments highlight the challenges in scientific fact-checking and suggest potential pathways to improvement.

Conclusion: The paper advocates for specialized information retrieval systems to address challenges in evidence retrieval and processing to improve scientific fact-checking capabilities.

Abstract: Scientific fact-checking aims to determine the veracity of scientific claims
by retrieving and analysing evidence from research literature. The problem is
inherently more complex than general fact-checking since it must accommodate
the evolving nature of scientific knowledge, the structural complexity of
academic literature and the challenges posed by long-form, multimodal
scientific expression. However, existing approaches focus on simplified
versions of the problem based on small-scale datasets consisting of abstracts
rather than full papers, thereby avoiding the distinct challenges associated
with processing complete documents. This paper examines the limitations of
current scientific fact-checking systems and reveals the many potential
features and resources that could be exploited to advance their performance. It
identifies key research challenges within evidence retrieval, including (1)
evidence-driven retrieval that addresses semantic limitations and topic
imbalance (2) time-aware evidence retrieval with citation tracking to mitigate
outdated information, (3) structured document parsing to leverage long-range
context, (4) handling complex scientific expressions, including tables,
figures, and domain-specific terminology and (5) assessing the credibility of
scientific literature. Preliminary experiments were conducted to substantiate
these challenges and identify potential solutions. This perspective paper aims
to advance scientific fact-checking with a specialised IR system tailored for
real-world applications.

</details>


### [277] [Role-Augmented Intent-Driven Generative Search Engine Optimization](https://arxiv.org/abs/2508.11158)
*Xiaolu Chen,Haojie Wu,Jie Bao,Zhen Chen,Yong Liao,Hu Huang*

Main category: cs.IR

TL;DR: This paper introduces a novel Role-Augmented Intent-Driven Generative SEO method to optimize content for Generative Search Engines (GSEs), addressing the challenges posed by traditional SEO misalignments with new generative retrieval systems.


<details>
  <summary>Details</summary>
Motivation: The emergence of Generative Search Engines (GSEs) powered by LLMs and RAG disrupts traditional SEO practices, creating challenges for content creators in maintaining visibility.

Method: The authors propose a structured optimization method called Role-Augmented Intent-Driven Generative SEO (G-SEO), which models search intent through reflective refinement and evaluates it using an enhanced GEO dataset and G-Eval 2.0 rubric.

Result: Experiments demonstrate that the proposed approach improves content optimization, achieving better subjective impressions and content visibility within GSE responses compared to traditional approaches.

Conclusion: Search intent is a powerful signal for optimizing content in GSE contexts, and the proposed G-SEO method offers an effective framework for addressing the limitations of traditional SEO in generative systems.

Abstract: Generative Search Engines (GSEs), powered by Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG), are reshaping information retrieval.
While commercial systems (e.g., BingChat, Perplexity.ai) demonstrate impressive
semantic synthesis capabilities, their black-box nature fundamentally
undermines established Search Engine Optimization (SEO) practices. Content
creators face a critical challenge: their optimization strategies, effective in
traditional search engines, are misaligned with generative retrieval contexts,
resulting in diminished visibility. To bridge this gap, we propose a
Role-Augmented Intent-Driven Generative Search Engine Optimization (G-SEO)
method, providing a structured optimization pathway tailored for GSE scenarios.
Our method models search intent through reflective refinement across diverse
informational roles, enabling targeted content enhancement. To better evaluate
the method under realistic settings, we address the benchmarking limitations of
prior work by: (1) extending the GEO dataset with diversified query variations
reflecting real-world search scenarios and (2) introducing G-Eval 2.0, a
6-level LLM-augmented evaluation rubric for fine-grained human-aligned
assessment. Experimental results demonstrate that search intent serves as an
effective signal for guiding content optimization, yielding significant
improvements over single-aspect baseline approaches in both subjective
impressions and objective content visibility within GSE responses.

</details>


### [278] [PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing](https://arxiv.org/abs/2508.11116)
*Zhuoqun Li,Xuanang Chen,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun*

Main category: cs.IR

TL;DR: This paper presents a new system, PaperRegister, to enhance paper search by supporting flexible-grained queries through a hierarchical indexing system. It achieves state-of-the-art performance, especially in fine-grained scenarios.


<details>
  <summary>Details</summary>
Motivation: Conventional paper search systems lack the ability to process flexible-grained queries due to their reliance on indexing based on abstracts, which insufficiently cover detailed information.

Method: The proposed system, PaperRegister, features offline hierarchical indexing and online adaptive retrieval, transforming traditional abstract-based indices into hierarchical structures to support search at varying granularities.

Result: Experiments demonstrate that PaperRegister achieves state-of-the-art performance in paper search tasks, especially excelling in fine-grained scenarios.

Conclusion: PaperRegister is a promising solution for more flexible and detailed paper search requirements, addressing limitations of existing systems and showing strong real-world application suitability.

Abstract: Paper search is an important activity for researchers, typically involving
using a query with description of a topic to find relevant papers. As research
deepens, paper search requirements may become more flexible, sometimes
involving specific details such as module configuration rather than being
limited to coarse-grained topics. However, previous paper search systems are
unable to meet these flexible-grained requirements, as these systems mainly
collect paper abstracts to construct index of corpus, which lack detailed
information to support retrieval by finer-grained queries. In this work, we
propose PaperRegister, consisted of offline hierarchical indexing and online
adaptive retrieval, transforming traditional abstract-based index into
hierarchical index tree for paper search, thereby supporting queries at
flexible granularity. Experiments on paper search tasks across a range of
granularity demonstrate that PaperRegister achieves the state-of-the-art
performance, and particularly excels in fine-grained scenarios, highlighting
the good potential as an effective solution for flexible-grained paper search
in real-world applications. Code for this work is in
https://github.com/Li-Z-Q/PaperRegister.

</details>


### [279] [+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking](https://arxiv.org/abs/2508.11122)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: +VeriRel improves scientific fact-checking by incorporating verification success into document ranking, showing superior results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods for scientific fact-checking depend on generic Information Retrieval systems, focusing on document relevance rather than supporting/refuting evidence for claims.

Method: +VeriRel incorporates verification success as a factor in ranking documents during evidence retrieval.

Result: +VeriRel achieved superior evidence retrieval performance on three datasets (SciFact, SciFact-Open, and Check-Covid) and enhanced downstream verification.

Conclusion: Integrating verification feedback into document relevance assessment is effective and offers potential for advancing scientific fact-checking systems.

Abstract: Identification of appropriate supporting evidence is critical to the success
of scientific fact checking. However, existing approaches rely on off-the-shelf
Information Retrieval algorithms that rank documents based on relevance rather
than the evidence they provide to support or refute the claim being checked.
This paper proposes +VeriRel which includes verification success in the
document ranking. Experimental results on three scientific fact checking
datasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently
leading performance by +VeriRel for document evidence retrieval and a positive
impact on downstream verification. This study highlights the potential of
integrating verification feedback to document relevance assessment for
effective scientific fact checking systems. It shows promising future work to
evaluate fine-grained relevance when examining complex documents for advanced
scientific fact checking.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [280] [Failures to Surface Harmful Contents in Video Large Language Models](https://arxiv.org/abs/2508.10974)
*Yuxin Cao,Wei Song,Derui Wang,Jingling Xue,Jin Song Dong*

Main category: cs.MM

TL;DR: Current Video Large Language Models (VideoLLMs) fail to detect harmful content in videos due to design flaws, such as poor temporal and spatial sampling alongside encoder-decoder disconnection.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the critical safety gap where harmful video content escapes detection and mention in outputs of VideoLLMs, posing risks for users relying on auto-generated summaries.

Method: The authors identified three design flaws in VideoLLMs (insufficient temporal coverage, spatial loss due to token downsampling, and weak encoder-decoder integration) and conducted large-scale evaluations using crafted black-box attacks.

Result: The research found a harmfulness omission rate exceeding 90% across five leading VideoLLMs under evaluation, demonstrating the models' inability to reliably detect harmful content.

Conclusion: The study highlights a fundamental vulnerability in VideoLLMs and calls for new improvements in sampling, compression, and decoding strategies to ensure safety and semantic accuracy.

Abstract: Video Large Language Models (VideoLLMs) are increasingly deployed on numerous
critical applications, where users rely on auto-generated summaries while
casually skimming the video stream. We show that this interaction hides a
critical safety gap: if harmful content is embedded in a video, either as
full-frame inserts or as small corner patches, state-of-the-art VideoLLMs
rarely mention the harmful content in the output, despite its clear visibility
to human viewers. A root-cause analysis reveals three compounding design flaws:
(1) insufficient temporal coverage resulting from the sparse, uniformly spaced
frame sampling used by most leading VideoLLMs, (2) spatial information loss
introduced by aggressive token downsampling within sampled frames, and (3)
encoder-decoder disconnection, whereby visual cues are only weakly utilized
during text generation. Leveraging these insights, we craft three zero-query
black-box attacks, aligning with these flaws in the processing pipeline. Our
large-scale evaluation across five leading VideoLLMs shows that the harmfulness
omission rate exceeds 90% in most cases. Even when harmful content is clearly
present in all frames, these models consistently fail to identify it. These
results underscore a fundamental vulnerability in current VideoLLMs' designs
and highlight the urgent need for sampling strategies, token compression, and
decoding mechanisms that guarantee semantic coverage rather than speed alone.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [281] [Temporally-Similar Structure-Aware Spatiotemporal Fusion of Satellite Images](https://arxiv.org/abs/2508.11259)
*Ryosuke Isono,Shunsuke Ono*

Main category: eess.SP

TL;DR: This paper introduces TSSTF, a novel satellite image fusion framework that improves noise robustness while maintaining spatial structure details.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between spatial and temporal resolution in noisy satellite images while preserving fine spatial structures.

Method: TSSTF uses Temporally-Guided Total Variation (TGTV) and Temporally-Guided Edge Constraint (TGEC) within a constrained optimization problem, solved with a preconditioned primal-dual splitting method.

Result: The framework performs comparably to state-of-the-art methods in noise-free conditions and better under noisy conditions, with consistent parameter recommendations.

Conclusion: TSSTF enhances image fusion robustness and structural fidelity, offering practical utility and reproducibility for diverse scenarios.

Abstract: This paper proposes a novel spatiotemporal (ST) fusion framework for
satellite images, named Temporally-Similar Structure-Aware ST fusion (TSSTF).
ST fusion is a promising approach to address the trade-off between the spatial
and temporal resolution of satellite images. In real-world scenarios, observed
satellite images are severely degraded by noise due to measurement equipment
and environmental conditions. Consequently, some recent studies have focused on
enhancing the robustness of ST fusion methods against noise. However, existing
noise-robust ST fusion approaches often fail to capture fine spatial structure,
leading to oversmoothing and artifacts. To address this issue, TSSTF introduces
two key mechanisms: Temporally-Guided Total Variation (TGTV) and
Temporally-Guided Edge Constraint (TGEC). TGTV is a novel regularization
function that promotes spatial piecewise smoothness while preserving structural
details, guided by a reference high spatial resolution image acquired on a
nearby date. TGEC enforces consistency in edge locations between two temporally
adjacent images, while allowing for spectral variations. We formulate the ST
fusion task as a constrained optimization problem incorporating TGTV and TGEC,
and develop an efficient algorithm based on a preconditioned primal-dual
splitting method. Experimental results demonstrate that TSSTF performs
comparably to state-of-the-art methods under noise-free conditions and
outperforms them under noisy conditions. Additionally, we provide a
comprehensive set of recommended parameter values that consistently yield high
performance across diverse target regions and noise conditions, aiming to
enhance reproducibility and practical utility.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [282] [LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters](https://arxiv.org/abs/2508.11074)
*Haomin Zhang,Kristin Qi,Shuxin Yang,Zihao Chen,Chaofan Ding,Xinhan Di*

Main category: cs.SD

TL;DR: This paper introduces LD-LAudio-V1, a model for long-form video-to-audio generation, featuring improved synchronization, reduced artifacts, and high efficiency, alongside releasing a new clean dataset.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in generating synchronized and high-quality audio for long-form videos, which current methods fail to achieve effectively.

Method: A state-of-the-art video-to-audio model enhanced with dual lightweight adapters, accompanied by a noise-free, human-annotated dataset tailored for long-form synthesis.

Result: The method achieves substantial improvements across multiple metrics for quality and synchronization, highlighting its effectiveness compared to short video training approaches.

Conclusion: LD-LAudio-V1 sets a new benchmark for long-form video-to-audio generation, offering improved results and a clean dataset to support further research in this domain.

Abstract: Generating high-quality and temporally synchronized audio from video content
is essential for video editing and post-production tasks, enabling the creation
of semantically aligned audio for silent videos. However, most existing
approaches focus on short-form audio generation for video segments under 10
seconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To
address these limitations, we introduce LD-LAudio-V1, an extension of
state-of-the-art video-to-audio models and it incorporates dual lightweight
adapters to enable long-form audio generation. In addition, we release a clean
and human-annotated video-to-audio dataset that contains pure sound effects
without noise or artifacts. Our method significantly reduces splicing artifacts
and temporal inconsistencies while maintaining computational efficiency.
Compared to direct fine-tuning with short training videos, LD-LAudio-V1
achieves significant improvements across multiple metrics: $FD_{\text{passt}}$
450.00 $\rightarrow$ 327.29 (+27.27%), $FD_{\text{panns}}$ 34.88 $\rightarrow$
22.68 (+34.98%), $FD_{\text{vgg}}$ 3.75 $\rightarrow$ 1.28 (+65.87%),
$KL_{\text{panns}}$ 2.49 $\rightarrow$ 2.07 (+16.87%), $KL_{\text{passt}}$ 1.78
$\rightarrow$ 1.53 (+14.04%), $IS_{\text{panns}}$ 4.17 $\rightarrow$ 4.30
(+3.12%), $IB_{\text{score}}$ 0.25 $\rightarrow$ 0.28 (+12.00%),
$Energy\Delta10\text{ms}$ 0.3013 $\rightarrow$ 0.1349 (+55.23%),
$Energy\Delta10\text{ms(vs.GT)}$ 0.0531 $\rightarrow$ 0.0288 (+45.76%), and
$Sem.\,Rel.$ 2.73 $\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate
further research in long-form video-to-audio generation and is available at
https://github.com/deepreasonings/long-form-video2audio.

</details>


### [283] [Benchmarking Prosody Encoding in Discrete Speech Tokens](https://arxiv.org/abs/2508.11224)
*Kentaro Onda,Satoru Fukayama,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.SD

TL;DR: This paper examines the effectiveness of discrete tokens derived from self-supervised learning for capturing prosodic information and aims to provide practical design guidelines.


<details>
  <summary>Details</summary>
Motivation: To explore and address the limitations of discrete tokens in capturing both semantic and prosodic information, which are important for speech language models.

Method: The study conducts a comprehensive analysis by examining the sensitivity of discrete tokens to artificially modified prosody.

Result: The research highlights how well discrete tokens capture prosodic features and provides insights into their design choices.

Conclusion: Practical guidelines for designing discrete tokens are proposed, focusing on improving their ability to encode prosody in speech language models.

Abstract: Recently, discrete tokens derived from self-supervised learning (SSL) models
via k-means clustering have been actively studied as pseudo-text in speech
language models and as efficient intermediate representations for various
tasks. However, these discrete tokens are typically learned in advance,
separately from the training of language models or downstream tasks. As a
result, choices related to discretization, such as the SSL model used or the
number of clusters, must be made heuristically. In particular, speech language
models are expected to understand and generate responses that reflect not only
the semantic content but also prosodic features. Yet, there has been limited
research on the ability of discrete tokens to capture prosodic information. To
address this gap, this study conducts a comprehensive analysis focusing on
prosodic encoding based on their sensitivity to the artificially modified
prosody, aiming to provide practical guidelines for designing discrete tokens.

</details>


### [284] [Pretrained Conformers for Audio Fingerprinting and Retrieval](https://arxiv.org/abs/2508.11609)
*Kemal Altwlkany,Elmedin Selmanovic,Sead Delalic*

Main category: cs.SD

TL;DR: This paper proposes using conformer-based encoders with self-supervised contrastive learning for robust audio embeddings, excelling in retrieval tasks with short inputs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage the strengths of conformers in capturing local and global interactions to create embeddings for audio that generalize well to unseen data and function effectively even under distortions such as noise and temporal misalignments.

Method: The authors use a self-supervised contrastive learning framework to train conformer-based encoders that generate unique embeddings for small audio segments, enabling robust performance across various tasks.

Result: The results demonstrate state-of-the-art performance in audio retrieval tasks, achieving robustness against temporal misalignment, noise, reverb, and extreme temporal stretching while using only 3 seconds of audio for embedding generation.

Conclusion: The proposed method provides a robust and effective solution for generating embeddings for audio retrieval tasks, and the publicly available code ensures reproducibility and real-world applicability.

Abstract: Conformers have shown great results in speech processing due to their ability
to capture both local and global interactions. In this work, we utilize a
self-supervised contrastive learning framework to train conformer-based
encoders that are capable of generating unique embeddings for small segments of
audio, generalizing well to previously unseen data. We achieve state-of-the-art
results for audio retrieval tasks while using only 3 seconds of audio to
generate embeddings. Our models are almost completely immune to temporal
misalignments and achieve state-of-the-art results in cases of other audio
distortions such as noise, reverb or extreme temporal stretching. Code and
models are made publicly available and the results are easy to reproduce as we
train and test using popular and freely available datasets of different sizes.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [285] [Note on Selection Bias in Observational Estimates of Algorithmic Progress](https://arxiv.org/abs/2508.11033)
*Parker Whitfill*

Main category: econ.GN

TL;DR: Ho et al. (2024) study the algorithmic progress of language models, finding increased efficiency over time, but their methods may face bias issues.


<details>
  <summary>Details</summary>
Motivation: To understand how language models' algorithmic efficiency has evolved, particularly the trend where less compute achieves better performance (lower loss).

Method: Collecting observational data on language models' loss and compute over time to evaluate trends in algorithmic efficiency.

Result: The study suggests that the algorithmic efficiency of language models has been improving as models achieve lower loss with fixed compute over time.

Conclusion: There may be methodological concerns, such as bias, if algorithmic quality is partially latent and compute endogenously influences quality. This could affect the validity of efficiency estimates.

Abstract: Ho et. al (2024) is an interesting paper that attempts to estimate the degree
of algorithmic progress from language models. They collect observational data
on language models' loss and compute over time, and argue that as time has
passed, language models' algorithmic efficiency has been rising. That is, the
loss achieved for fixed compute has been dropping over time. In this note, I
want to raise one potential methodological problem with the estimation
strategy. Intuitively, if part of algorithmic quality is latent, and compute
choices are endogenous to algorithmic quality, then resulting estimates of
algorithmic quality will be biased.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [286] [Principles of Physiological Closed-Loop Controllers in Neuromodulation](https://arxiv.org/abs/2508.11422)
*Victoria S. Marks,Joram vanRheede,Dean Karantonis,Rosana Esteller,David Dinsmoor,John Fleming,Barrett Larson,Lane Desborough,Peter Single,Robert Raike,Pierre-Francois DHaese,Dario J. Englot,Scott Lempka,Richard North,Lawrence Poree,Marom Bikson,Tim J. Denison*

Main category: eess.SY

TL;DR: This paper proposes a common framework to map neuromodulation-based physiological closed-loop controllers (PCLCs), integrating FDA guidance and control systems theory.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of closed-loop neurostimulation devices demands better risk management and design optimization to enhance benefits.

Method: A framework is developed incorporating FDA guidance, biomarker classification, and control systems theory, illustrated through examples.

Result: The framework explains risk management implications and applies it to three neuromodulation technologies.

Conclusion: This work provides standardized guidance for developing PCLCs, promoting risk mitigation and systematic device implementation.

Abstract: As neurostimulation devices increasingly incorporate closed-loop
functionality, the greater design complexity brings additional requirements for
risk management and special considerations to optimise benefit. This manuscript
creates a common framework upon which all current and planned
neuromodulation-based physiological closed-loop controllers (PCLCs) can be
mapped including integration of the Technical Considerations of Medical Devices
with Physiologic Closed-Loop Control Technology guidance published in 2023 by
the United States Food and Drug Administration (FDA), a classification of
feedback (reactive) and feedforward (predictive) biomarkers, and control
systems theory. We explain risk management in the context of this framework and
illustrate its applications for three exemplary technologies. This manuscript
serves as guidance to the emerging field of PCLCs in neuromodulation,
mitigating risk through standardized nomenclature and a systematic outline for
rigorous device development, testing, and implementation.

</details>


### [287] [Risk-Based Prognostics and Health Management](https://arxiv.org/abs/2508.11031)
*John W. Sheppard*

Main category: eess.SY

TL;DR: The paper discusses a risk-based approach to prognostics utilizing continuous-time Bayesian networks for tighter integration between risk assessment and fault prediction.


<details>
  <summary>Details</summary>
Motivation: Prognostics and risk assessment are often separate tasks; the paper aims to integrate them for improved decision-making and operational logistics.

Method: The study employs continuous-time Bayesian networks as a modeling framework, with techniques to derive models from data for practical applications.

Result: The approach facilitates risk assessment and fault prediction while providing decision support and enhancing performance-based logistics.

Conclusion: The paper highlights advancements in risk-based prognostics and offers guidance for adopting these techniques in practice.

Abstract: It is often the case that risk assessment and prognostics are viewed as
related but separate tasks. This chapter describes a risk-based approach to
prognostics that seeks to provide a tighter coupling between risk assessment
and fault prediction. We show how this can be achieved using the
continuous-time Bayesian network as the underlying modeling framework.
Furthermore, we provide an overview of the techniques that are available to
derive these models from data and show how they might be used in practice to
achieve tasks like decision support and performance-based logistics. This work
is intended to provide an overview of the recent developments related to
risk-based prognostics, and we hope that it will serve as a tutorial of sorts
that will assist others in adopting these techniques.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [288] [Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style](https://arxiv.org/abs/2508.11187)
*Wonjune Kang,Deb Roy*

Main category: eess.AS

TL;DR: This paper introduces expressive speech retrieval using natural language descriptions as queries to retrieve speech based on emotion or style. The system embeds speech and text in a shared latent space.


<details>
  <summary>Details</summary>
Motivation: Address the limitation in existing speech retrieval systems by targeting the retrieval of speech based on style or emotion, rather than content.

Method: Jointly train speech and text encoders to align speech and text style descriptions in a shared latent space. Analyzed encoder architectures, alignment criteria, and prompt augmentation for improved generalization.

Result: The proposed framework demonstrates strong retrieval performance across multiple datasets with 22 speaking styles, evaluated with Recall@k.

Conclusion: This approach is effective for expressive speech retrieval, showcasing its potential for style-based querying using free-form text prompts.

Abstract: We introduce the task of expressive speech retrieval, where the goal is to
retrieve speech utterances spoken in a given style based on a natural language
description of that style. While prior work has primarily focused on performing
speech retrieval based on what was said in an utterance, we aim to do so based
on how something was said. We train speech and text encoders to embed speech
and text descriptions of speaking styles into a joint latent space, which
enables using free-form text prompts describing emotions or styles as queries
to retrieve matching expressive speech segments. We perform detailed analyses
of various aspects of our proposed framework, including encoder architectures,
training criteria for effective cross-modal alignment, and prompt augmentation
for improved generalization to arbitrary text queries. Experiments on multiple
datasets encompassing 22 speaking styles demonstrate that our approach achieves
strong retrieval performance as measured by Recall@k.

</details>


### [289] [Emphasis Sensitivity in Speech Representations](https://arxiv.org/abs/2508.11566)
*Shaun Cassini,Thomas Hain,Anton Ragni*

Main category: eess.AS

TL;DR: This paper studies the sensitivity of modern speech models to prosodic emphasis, proposing a residual-based framework to analyze emphasis encoding.


<details>
  <summary>Details</summary>
Motivation: To determine if speech models systematically differentiate between emphasized and neutral words, addressing limitations of prior methods that overlook relational structure.

Method: A residual-based framework measures emphasis as differences between paired word representations, analyzing self-supervised and ASR fine-tuned speech models.

Result: Residuals correlate with duration changes and show structured, relational emphasis encoding. Fine-tuned models demonstrate more compact residual subspaces.

Conclusion: Emphasis is encoded as a low-dimensional, consistent transformation, with structure refined through task-specific learning.

Abstract: This work investigates whether modern speech models are sensitive to prosodic
emphasis - whether they encode emphasized and neutral words in systematically
different ways. Prior work typically relies on isolated acoustic correlates
(e.g., pitch, duration) or label prediction, both of which miss the relational
structure of emphasis. This paper proposes a residual-based framework, defining
emphasis as the difference between paired neutral and emphasized word
representations. Analysis on self-supervised speech models shows that these
residuals correlate strongly with duration changes and perform poorly at word
identity prediction, indicating a structured, relational encoding of prosodic
emphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more
compact than in pre-trained models, further suggesting that emphasis is encoded
as a consistent, low-dimensional transformation that becomes more structured
with task-specific learning.

</details>


### [290] [CleanCTG: A Deep Learning Model for Multi-Artefact Detection and Reconstruction in Cardiotocography](https://arxiv.org/abs/2508.10928)
*Sheng Wong,Beth Albert,Gabriel Davis Jones*

Main category: eess.AS

TL;DR: CleanCTG is a dual-stage deep-learning model designed for identifying and correcting diverse artefacts in cardiotocography (CTG) fetal monitoring data, outperforming traditional and existing approaches in artefact detection and signal reconstruction.


<details>
  <summary>Details</summary>
Motivation: Improving CTG analysis by addressing issues with existing methods that fail to handle complex artefacts effectively, which can result in misdiagnosis or delayed medical decisions.

Method: CleanCTG employs a dual-stage approach combining multi-scale convolution and cross-attention for artefact detection and artefact-specific correction branches for reconstruction. Training is performed using simulated corrupted data derived from clean recordings.

Result: CleanCTG achieved superior artefact detection (AU-ROC = 1.00 on synthetic data and 0.95 on clinical data) and significant improvements in signal reconstruction accuracy compared to other methods, enhancing clinical decision-making performance.

Conclusion: Explicit artefact removal combined with reconstruction not only maintains diagnostic accuracy but also shortens decision-making time, showing promise for more effective and reliable CTG monitoring systems.

Abstract: Cardiotocography (CTG) is essential for fetal monitoring but is frequently
compromised by diverse artefacts which obscure true fetal heart rate (FHR)
patterns and can lead to misdiagnosis or delayed intervention. Current
deep-learning approaches typically bypass comprehensive noise handling,
applying minimal preprocessing or focusing solely on downstream classification,
while traditional methods rely on simple interpolation or rule-based filtering
that addresses only missing samples and fail to correct complex artefact types.
We present CleanCTG, an end-to-end dual-stage model that first identifies
multiple artefact types via multi-scale convolution and context-aware
cross-attention, then reconstructs corrupted segments through artefact-specific
correction branches. Training utilised over 800,000 minutes of physiologically
realistic, synthetically corrupted CTGs derived from expert-verified "clean"
recordings. On synthetic data, CleanCTG achieved perfect artefact detection
(AU-ROC = 1.00) and reduced mean squared error (MSE) on corrupted segments to
2.74 x 10^-4 (clean-segment MSE = 2.40 x 10^-6), outperforming the next best
method by more than 60%. External validation on 10,190 minutes of
clinician-annotated segments yielded AU-ROC = 0.95 (sensitivity = 83.44%,
specificity 94.22%), surpassing six comparator classifiers. Finally, when
integrated with the Dawes-Redman system on 933 clinical CTG recordings,
denoised traces increased specificity (from 80.70% to 82.70%) and shortened
median time to decision by 33%. These findings suggest that explicit artefact
removal and signal reconstruction can both maintain diagnostic accuracy and
enable shorter monitoring sessions, offering a practical route to more reliable
CTG interpretation.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [291] [AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions](https://arxiv.org/abs/2508.11152)
*Tianjiao Zhao,Jingrao Lyu,Stokes Jones,Harrison Garber,Stefano Pasquali,Dhagash Mehta*

Main category: q-fin.ST

TL;DR: The study explores how role-based multi-agent AI systems can support stock selection in equity research and portfolio management, analyzing their performance under different risk tolerances.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the rapid advancements in AI and multi-agent systems, specifically focusing on their potential to enhance stock selection in equity analysis.

Method: This paper uses specialized role-based multi-agent systems to perform equity analysis and evaluate stock-picking performance compared to industry benchmarks.

Result: The research demonstrates the performance of multi-agent frameworks for stock selection and identifies both their advantages and limitations in practical application.

Conclusion: Role-based multi-agent systems show promise for equity research, but there are notable challenges and constraints in their implementation for portfolio management.

Abstract: The field of artificial intelligence (AI) agents is evolving rapidly, driven
by the capabilities of Large Language Models (LLMs) to autonomously perform and
refine tasks with human-like efficiency and adaptability. In this context,
multi-agent collaboration has emerged as a promising approach, enabling
multiple AI agents to work together to solve complex challenges. This study
investigates the application of role-based multi-agent systems to support stock
selection in equity research and portfolio management. We present a
comprehensive analysis performed by a team of specialized agents and evaluate
their stock-picking performance against established benchmarks under varying
levels of risk tolerance. Furthermore, we examine the advantages and
limitations of employing multi-agent frameworks in equity analysis, offering
critical insights into their practical efficacy and implementation challenges.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [292] [Allen: Rethinking MAS Design through Step-Level Policy Autonomy](https://arxiv.org/abs/2508.11294)
*Qiangong Zhou,Zhiting Wang,Mingyou Yao,Zongyang Liu*

Main category: cs.MA

TL;DR: The paper introduces a new Multi-Agent System called Allen, aiming to improve policy autonomy and optimize collaborative efficiency while maintaining controllability.


<details>
  <summary>Details</summary>
Motivation: Existing Multi-Agent Systems often struggle with dynamic strategy adaptation and achieving a balance between collaboration, supervision, and oversight.

Method: The authors implemented a four-tier state architecture that redefines the basic execution unit in MAS, enabling agents to self-organize and adaptively form execution patterns.

Result: Allen successfully balances policy autonomy with controllable collaborative structures, offering efficient and adaptable performance in complex networks.

Conclusion: Allen is a breakthrough in MAS design, improving dynamic response capabilities and achieving efficient collaboration while maintaining oversight.

Abstract: We introduce a new Multi-Agent System (MAS) - Allen, designed to address two
core challenges in current MAS design: (1) improve system's policy autonomy,
empowering agents to dynamically adapt their behavioral strategies, and (2)
achieving the trade-off between collaborative efficiency, task supervision, and
human oversight in complex network topologies.
  Our core insight is to redefine the basic execution unit in the MAS, allowing
agents to autonomously form different patterns by combining these units. We
have constructed a four-tier state architecture (Task, Stage, Agent, Step) to
constrain system behavior from both task-oriented and execution-oriented
perspectives. This achieves a unification of topological optimization and
controllable progress.
  Allen grants unprecedented Policy Autonomy, while making a trade-off for the
controllability of the collaborative structure. The project code has been open
source at: https://github.com/motern88/Allen

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [293] [The Role of Entanglement in Quantum Reservoir Computing with Coupled Kerr Nonlinear Oscillators](https://arxiv.org/abs/2508.11175)
*Ali Karimi,Hadi Zadeh-Haghighi,Youssef Kora,Christoph Simon*

Main category: quant-ph

TL;DR: This paper investigates the use of two coupled Kerr nonlinear oscillators in Quantum Reservoir Computing (QRC) for time-series prediction tasks, focusing on the relationship between system parameters, entanglement, and predictive performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how physical parameters and entanglement influence the performance of quantum reservoirs, specifically in time-series prediction tasks.

Method: The study uses a QRC system with two coupled Kerr nonlinear oscillators and quantifies entanglement using logarithmic negativity while assessing predictive accuracy with normalized root mean square error (NRMSE).

Result: Entanglement was found to provide a computational advantage for time-series prediction up to a threshold input frequency, even under certain dissipation and dephasing levels. Interestingly, higher dissipation rates could increase performance, and the entanglement advantage improved average and worst-case—but not best-case—errors.

Conclusion: The findings enhance the understanding of quantum reservoirs for quantum machine learning and time-series forecasting, highlighting the potential computational benefits of entanglement in quantum systems.

Abstract: Quantum Reservoir Computing (QRC) uses quantum dynamics to efficiently
process temporal data. In this work, we investigate a QRC framework based on
two coupled Kerr nonlinear oscillators, a system well-suited for time-series
prediction tasks due to its complex nonlinear interactions and potentially
high-dimensional state space. We explore how its performance in time-series
prediction depends on key physical parameters: input drive strength, Kerr
nonlinearity, and oscillator coupling, and analyze the role of entanglement in
improving the reservoir's computational performance, focusing on its effect on
predicting non-trivial time series. Using logarithmic negativity to quantify
entanglement and normalized root mean square error (NRMSE) to evaluate
predictive accuracy, our results suggest that entanglement provides a
computational advantage on average-up to a threshold in the input
frequency-that persists under some levels of dissipation and dephasing. In
particular, we find that higher dissipation rates can enhance performance.
While the entanglement advantage manifests as improvements in both average and
worst-case performance, it does not lead to improvements in the best-case
error. These findings contribute to the broader understanding of quantum
reservoirs for high performance, efficient quantum machine learning and
time-series forecasting.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [294] [MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications](https://arxiv.org/abs/2508.10991)
*Wenpeng Xing,Zhonghao Qi,Yupeng Qin,Yilin Li,Caini Chang,Jiahui Yu,Changting Lin,Zhenzhen Xie,Meng Han*

Main category: cs.CR

TL;DR: This paper introduces MCP-Guard, a defense system against security vulnerabilities in LLM-tool interactions, and MCP-AttackBench for evaluation.


<details>
  <summary>Details</summary>
Motivation: LLMs integrated with tools face security threats like prompt injection and data exfiltration, which necessitate robust defense mechanisms.

Method: The authors propose MCP-Guard, a three-layer defense system including static threat scans, deep neural detection, and a lightweight LLM arbitrator, and develop the MCP-AttackBench dataset with 70,000 samples for training and testing.

Result: The fine-tuned E5-based model in MCP-Guard achieved 96.01% accuracy in adversarial prompt detection.

Conclusion: MCP-Guard effectively balances accuracy with efficient detection of attacks on LLM-tool interactions, offering significant advancements in securing these systems.

Abstract: The integration of Large Language Models (LLMs) with external tools via
protocols such as the Model Context Protocol (MCP) introduces critical security
vulnerabilities, including prompt injection, data exfiltration, and other
threats. To counter these challenges, we propose MCP-Guard, a robust, layered
defense architecture designed for LLM--tool interactions. MCP-Guard employs a
three-stage detection pipeline that balances efficiency with accuracy: it
progresses from lightweight static scanning for overt threats and a deep neural
detector for semantic attacks, to our fine-tuned E5-based model achieves
(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM
arbitrator synthesizes these signals to deliver the final decision while
minimizing false positives. To facilitate rigorous training and evaluation, we
also introduce MCP-AttackBench, a comprehensive benchmark of over 70,000
samples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench
simulates diverse, real-world attack vectors in the MCP format, providing a
foundation for future research into securing LLM-tool ecosystems.

</details>


### [295] [RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning](https://arxiv.org/abs/2508.11472)
*Yang Wang,Yaxin Zhao,Xinyu Jiao,Sihan Xu,Xiangrui Cai,Ying Zhang,Xiaojie Yuan*

Main category: cs.CR

TL;DR: This paper introduces a framework called Robust Multi-sphere Learning (RMSL) to detect insider threats by analyzing user behavior sequences with weak sequence-level labels.


<details>
  <summary>Details</summary>
Motivation: The researchers aimed to address the challenge of detecting behavior-level anomalies in user behavior logs, particularly when fine-grained behavior-level annotations are unavailable, and to reduce the high false positive and miss rates of existing unsupervised methods.

Method: The proposed RMSL framework uses a combination of a one-class classifier, multiple hyper-spheres to capture normal behavior patterns, and a multiple instance learning approach with self-training for refining representations based on weak sequence-level labels.

Result: RMSL significantly improves detection performance of behavior-level anomalies in insider threat detection, demonstrating its effectiveness through extensive experiments.

Conclusion: Weak sequence-level labels and multi-sphere learning effectively enhance the discriminative capability of anomaly detection systems, improving insider threat detection outcomes.

Abstract: Insider threat detection aims to identify malicious user behavior by
analyzing logs that record user interactions. Due to the lack of fine-grained
behavior-level annotations, detecting specific behavior-level anomalies within
user behavior sequences is challenging. Unsupervised methods face high false
positive rates and miss rates due to the inherent ambiguity between normal and
anomalous behaviors. In this work, we instead introduce weak labels of behavior
sequences, which have lower annotation costs, i.e., the training labels
(anomalous or normal) are at sequence-level instead of behavior-level, to
enhance the detection capability for behavior-level anomalies by learning
discriminative features. To achieve this, we propose a novel framework called
Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to
represent the normal patterns of behaviors. Initially, a one-class classifier
is constructed as a good anomaly-supervision-free starting point. Building on
this, using multiple instance learning and adaptive behavior-level
self-training debiasing based on model prediction confidence, the framework
further refines hyper-spheres and feature representations using weak
sequence-level labels. This approach enhances the model's ability to
distinguish between normal and anomalous behaviors. Extensive experiments
demonstrate that RMSL significantly improves the performance of behavior-level
insider threat detection.

</details>


### [296] [CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection](https://arxiv.org/abs/2508.11599)
*Zhihao Li,Zimo Ji,Tao Zheng,Hao Ren,Xiao Lan*

Main category: cs.CR

TL;DR: The paper introduces CryptoScope, a framework combining Chain-of-Thought (CoT) prompting, Retrieval-Augmented Generation (RAG), and a cryptographic knowledge base for detecting vulnerabilities using Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Detecting subtle logic flaws in cryptographic implementations is challenging, necessitating tools that leverage advanced automated techniques.

Method: CryptoScope combines Chain-of-Thought prompting, Retrieval-Augmented Generation, and a curated cryptographic knowledge base. It is tested on the LLM-CLVA benchmark comprising real-world CVEs, CTF challenges, and synthetic examples.

Result: CryptoScope improves vulnerability detection performance significantly compared to strong baselines, with notable gains across multiple models. It also discovers 9 previously unknown flaws in open-source cryptographic projects.

Conclusion: CryptoScope demonstrates the potential of leveraging LLMs and cryptographic knowledge bases for improving automated vulnerability detection, surpassing existing methods in accuracy and breadth of application.

Abstract: Cryptographic algorithms are fundamental to modern security, yet their
implementations frequently harbor subtle logic flaws that are hard to detect.
We introduce CryptoScope, a novel framework for automated cryptographic
vulnerability detection powered by Large Language Models (LLMs). CryptoScope
combines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation
(RAG), guided by a curated cryptographic knowledge base containing over 12,000
entries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily
derived from real-world CVE vulnerabilities, complemented by cryptographic
challenges from major Capture The Flag (CTF) competitions and synthetic
examples across 11 programming languages. CryptoScope consistently improves
performance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,
GPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9
previously undisclosed flaws in widely used open-source cryptographic projects.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [297] [Holistic Bioprocess Development Across Scales Using Multi-Fidelity Batch Bayesian Optimization](https://arxiv.org/abs/2508.10970)
*Adrian Martens,Mathias Neufang,Alessandro Butté,Moritz von Stosch,Antonio del Rio Chanona,Laura Marie Helleckes*

Main category: q-bio.QM

TL;DR: The paper introduces a multi-fidelity batch Bayesian optimization framework to optimize bioprocess development more effectively, outperforming conventional methods in cost reduction and yield improvement.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies in bioprocess development, particularly the joint optimization of reaction conditions and biocatalyst selection, which conventional methods struggle with.

Method: The paper employs multi-fidelity Bayesian optimization, integrating Gaussian Processes tailored for multi-fidelity modeling and mixed-variable optimization. It also benchmarks performance using a simulated bioprocess and industrial DoE baselines.

Result: Case studies demonstrate reductions in experimental costs and increased production yield using the proposed framework, surpassing performance of traditional approaches.

Conclusion: The research presents a data-efficient and cost-effective approach to bioprocess optimization, suggesting further potential in transfer learning and uncertainty-aware designs for sustainable biotechnology.

Abstract: Bioprocesses are central to modern biotechnology, enabling sustainable
production in pharmaceuticals, specialty chemicals, cosmetics, and food.
However, developing high-performing processes is costly and complex, requiring
iterative, multi-scale experimentation from microtiter plates to pilot
reactors. Conventional Design of Experiments (DoE) approaches often struggle to
address process scale-up and the joint optimization of reaction conditions and
biocatalyst selection.
  We propose a multi-fidelity batch Bayesian optimization framework to
accelerate bioprocess development and reduce experimental costs. The method
integrates Gaussian Processes tailored for multi-fidelity modeling and
mixed-variable optimization, guiding experiment selection across scales and
biocatalysts. A custom simulation of a Chinese Hamster Ovary bioprocess,
capturing non-linear and coupled scale-up dynamics, is used for benchmarking
against multiple simulated industrial DoE baselines. Multiple case studies show
how the proposed workflow can achieve a reduction in experimental costs and
increased yield.
  This work provides a data-efficient strategy for bioprocess optimization and
highlights future opportunities in transfer learning and uncertainty-aware
design for sustainable biotechnology.

</details>


### [298] [Trees Assembling Mann Whitney Approach for Detecting Genome-wide Joint Association among Low Marginal Effect loci](https://arxiv.org/abs/1505.01206)
*Changshuai Wei,Daniel J. Schaid,Qing Lu*

Main category: q-bio.QM

TL;DR: The paper introduces TAMW, a computational approach for analyzing the interplay of genetic variants with low marginal effects in complex diseases, demonstrating its superior accuracy and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying and analyzing genetic variants with low marginal effects (LME) and their joint association in complex diseases using high-dimensional data like genome-wide association studies (GWAS).

Method: Developed a statistical approach called Trees Assembling Mann Whitney (TAMW) to analyze LME genetic variants. Its efficiency and power were evaluated through simulations, empirical studies using Crohn's disease (CD) loci, and a large-scale genome-wide analysis with parallel computing.

Result: TAMW outperformed existing methods (e.g., MDR, LRMW) in identifying associations for diseases with multiple interacting LME loci. It demonstrated higher power in simulations, stronger joint associations in an empirical CD study, and successfully processed 459K SNPs in 40 hours, identifying key genes potentially linked to CD.

Conclusion: TAMW is a computationally efficient and robust approach for studying LME genetic variants in complex diseases, offering improved power and reliability over other methodologies. Its findings could enhance understanding of genetic contributions to diseases like Crohn's disease.

Abstract: Common complex diseases are likely influenced by the interplay of hundreds,
or even thousands, of genetic variants. Converging evidence shows that genetic
variants with low marginal effects (LME) play an important role in disease
development. Despite their potential significance, discovering LME genetic
variants and assessing their joint association on high dimensional data (e.g.,
genome wide association studies) remain a great challenge. To facilitate joint
association analysis among a large ensemble of LME genetic variants, we
proposed a computationally efficient and powerful approach, which we call Trees
Assembling Mann whitney (TAMW). Through simulation studies and an empirical
data application, we found that TAMW outperformed multifactor dimensionality
reduction (MDR) and the likelihood ratio based Mann whitney approach (LRMW)
when the underlying complex disease involves multiple LME loci and their
interactions. For instance, in a simulation with 20 interacting LME loci, TAMW
attained a higher power (power=0.931) than both MDR (power=0.599) and LRMW
(power=0.704). In an empirical study of 29 known Crohn's disease (CD) loci,
TAMW also identified a stronger joint association with CD than those detected
by MDR and LRMW. Finally, we applied TAMW to Wellcome Trust CD GWAS to conduct
a genome wide analysis. The analysis of 459K single nucleotide polymorphisms
was completed in 40 hours using parallel computing, and revealed a joint
association predisposing to CD (p-value=2.763e-19). Further analysis of the
newly discovered association suggested that 13 genes, such as ATG16L1 and
LACC1, may play an important role in CD pathophysiological and etiological
processes.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [299] [Deep Learning-Based Automated Segmentation of Uterine Myomas](https://arxiv.org/abs/2508.11010)
*Tausifa Jan Saleem,Mohammad Yaqub*

Main category: eess.IV

TL;DR: This paper addresses the challenge of automated uterine fibroid segmentation using deep learning on publicly available MRI data.


<details>
  <summary>Details</summary>
Motivation: Uterine fibroids are common, affecting over 70% of women and often require precise assessment via MRI for effective treatment, but current manual methods are time-consuming and variable.

Method: The study employs deep learning algorithms with the publicly available Uterine Myoma MRI Dataset to achieve automated fibroid segmentation.

Result: The research establishes a baseline for automated segmentation of uterine fibroids using publicly accessible data.

Conclusion: This work not only enhances accuracy in segmentation but also enables standardized evaluations, driving future advancements in uterine fibroid research.

Abstract: Uterine fibroids (myomas) are the most common benign tumors of the female
reproductive system, particularly among women of childbearing age. With a
prevalence exceeding 70%, they pose a significant burden on female reproductive
health. Clinical symptoms such as abnormal uterine bleeding, infertility,
pelvic pain, and pressure-related discomfort play a crucial role in guiding
treatment decisions, which are largely influenced by the size, number, and
anatomical location of the fibroids. Magnetic Resonance Imaging (MRI) is a
non-invasive and highly accurate imaging modality commonly used by clinicians
for the diagnosis of uterine fibroids. Segmenting uterine fibroids requires a
precise assessment of both the uterus and fibroids on MRI scans, including
measurements of volume, shape, and spatial location. However, this process is
labor intensive and time consuming and subjected to variability due to intra-
and inter-expert differences at both pre- and post-treatment stages. As a
result, there is a critical need for an accurate and automated segmentation
method for uterine fibroids. In recent years, deep learning algorithms have
shown re-markable improvements in medical image segmentation, outperforming
traditional methods. These approaches offer the potential for fully automated
segmentation. Several studies have explored the use of deep learning models to
achieve automated segmentation of uterine fibroids. However, most of the
previous work has been conducted using private datasets, which poses challenges
for validation and comparison between studies. In this study, we leverage the
publicly available Uterine Myoma MRI Dataset (UMD) to establish a baseline for
automated segmentation of uterine fibroids, enabling standardized evaluation
and facilitating future research in this domain.

</details>


### [300] [HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis](https://arxiv.org/abs/2508.11181)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: The paper proposes a Vision Transformer (ViT)-based system for automating cancer diagnosis using histopathological images, showing high accuracy and AUC scores across multiple cancer types.


<details>
  <summary>Details</summary>
Motivation: Cancer diagnosis using histopathological images is challenging due to significant variability across tissue types. There is a need for robust, scalable, and accurate diagnostic methods.

Method: The authors used a fine-tuned Vision Transformer (ViT) model, combined with a simplified preprocessing pipeline, to classify tumor types. They converted whole-slide images into tensors, standardized the data, and optimized the pipeline for better performance.

Result: The proposed method achieved high classification accuracies for breast (99.32%), prostate (96.92%), bone (95.28%), and cervical (96.94%) cancers, with AUC scores exceeding 99% on benchmark datasets.

Conclusion: The ViT-based framework demonstrates superior performance, robustness, and scalability for cancer diagnosis, highlighting its clinical feasibility and potential to improve automated diagnostics in pathology.

Abstract: Accurate and scalable cancer diagnosis remains a critical challenge in modern
pathology, particularly for malignancies such as breast, prostate, bone, and
cervical, which exhibit complex histological variability. In this study, we
propose a transformer-based deep learning framework for multi-class tumor
classification in histopathological images. Leveraging a fine-tuned Vision
Transformer (ViT) architecture, our method addresses key limitations of
conventional convolutional neural networks, offering improved performance,
reduced preprocessing requirements, and enhanced scalability across tissue
types. To adapt the model for histopathological cancer images, we implement a
streamlined preprocessing pipeline that converts tiled whole-slide images into
PyTorch tensors and standardizes them through data normalization. This ensures
compatibility with the ViT architecture and enhances both convergence stability
and overall classification performance. We evaluate our model on four benchmark
datasets: ICIAR2018 (breast), SICAPv2 (prostate), UT-Osteosarcoma (bone), and
SipakMed (cervical) dataset -- demonstrating consistent outperformance over
existing deep learning methods. Our approach achieves classification accuracies
of 99.32%, 96.92%, 95.28%, and 96.94% for breast, prostate, bone, and cervical
cancers respectively, with area under the ROC curve (AUC) scores exceeding 99%
across all datasets. These results confirm the robustness, generalizability,
and clinical potential of transformer-based architectures in digital pathology.
Our work represents a significant advancement toward reliable, automated, and
interpretable cancer diagnosis systems that can alleviate diagnostic burdens
and improve healthcare outcomes.

</details>


### [301] [The Role of Radiographic Knee Alignment in Knee Replacement Outcomes and Opportunities for Artificial Intelligence-Driven Assessment](https://arxiv.org/abs/2508.10941)
*Zhisen Hu,David S. Johnson,Aleksei Tiulpin,Timothy F. Cootes,Claudia Lindner*

Main category: eess.IV

TL;DR: This review focuses on knee alignment biomarkers and their role in predicting Total Knee Replacement (TKR) outcomes, emphasizing AI's role in evaluating these biomarkers from radiographs.


<details>
  <summary>Details</summary>
Motivation: To address the need for better outcome prediction in Total Knee Replacement (TKR) surgeries by assessing knee alignment biomarkers through AI, given the high burden of knee osteoarthritis (OA) and challenges in predicting complications.

Method: The paper reviews current scoring protocols for TKR outcomes, identifies potential knee alignment biomarkers, discusses AI approaches for analyzing knee radiographs, and proposes future research directions.

Result: Compiles existing knowledge on scoring protocols, biomarkers, and AI techniques for TKR outcomes; highlights the potential of AI in automating knee alignment assessments.

Conclusion: AI-driven knee alignment evaluation holds promise for improving TKR outcome predictions, but further research is needed to refine biomarkers and develop AI methods.

Abstract: Prevalent knee osteoarthritis (OA) imposes substantial burden on health
systems with no cure available. Its ultimate treatment is total knee
replacement (TKR). Complications from surgery and recovery are difficult to
predict in advance, and numerous factors may affect them. Radiographic knee
alignment is one of the key factors that impacts TKR outcomes, affecting
outcomes such as postoperative pain or function. Recently, artificial
intelligence (AI) has been introduced to the automatic analysis of knee
radiographs, for example, to automate knee alignment measurements. Existing
review articles tend to focus on knee OA diagnosis and segmentation of bones or
cartilages in MRI rather than exploring knee alignment biomarkers for TKR
outcomes and their assessment. In this review, we first examine the current
scoring protocols for evaluating TKR outcomes and potential knee alignment
biomarkers associated with these outcomes. We then discuss existing AI-based
approaches for generating knee alignment biomarkers from knee radiographs, and
explore future directions for knee alignment assessment and TKR outcome
prediction.

</details>


### [302] [Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification](https://arxiv.org/abs/2508.11511)
*Siyamalan Manivannan*

Main category: eess.IV

TL;DR: This paper presents a semi-supervised deep learning method for skin lesion classification, combining ensemble learning with online knowledge distillation to reduce dependency on extensive labeled data and achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of obtaining extensive labeled skin lesion data required for fully supervised learning methods, which are costly and difficult to obtain.

Method: The method involves training an ensemble of convolutional neural network models integrated with online knowledge distillation, allowing insights from the ensemble to enhance the performance of individual models.

Result: Experimental results show that the distillation-enhanced models outperform independently trained models and achieve state-of-the-art performance on ISIC 2018 and 2019 datasets.

Conclusion: The proposed approach not only reduces the annotation burden but also provides a resource-efficient solution for skin lesion classification, offering comparable performance in resource-constrained scenarios.

Abstract: Deep Learning has emerged as a promising approach for skin lesion analysis.
However, existing methods mostly rely on fully supervised learning, requiring
extensive labeled data, which is challenging and costly to obtain. To alleviate
this annotation burden, this study introduces a novel semi-supervised deep
learning approach that integrates ensemble learning with online knowledge
distillation for enhanced skin lesion classification. Our methodology involves
training an ensemble of convolutional neural network models, using online
knowledge distillation to transfer insights from the ensemble to its members.
This process aims to enhance the performance of each model within the ensemble,
thereby elevating the overall performance of the ensemble itself.
Post-training, any individual model within the ensemble can be deployed at test
time, as each member is trained to deliver comparable performance to the
ensemble. This is particularly beneficial in resource-constrained environments.
Experimental results demonstrate that the knowledge-distilled individual model
performs better than independently trained models. Our approach demonstrates
superior performance on both the \emph{International Skin Imaging
Collaboration} 2018 and 2019 public benchmark datasets, surpassing current
state-of-the-art results. By leveraging ensemble learning and online knowledge
distillation, our method reduces the need for extensive labeled data while
providing a more resource-efficient solution for skin lesion classification in
real-world scenarios.

</details>


### [303] [Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension](https://arxiv.org/abs/2508.11211)
*Zhenhao Li,Long Yang,Xiaojie Yin,Haijun Yu,Jiazhou Wang,Hongbin Han,Weigang Hu,Yixing Huang*

Main category: eess.IV

TL;DR: A new diffusion model called I$^2$SB is proposed to extend CT field-of-view (FOV) efficiently and accurately, addressing computational limitations of traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Computed tomography (CT) has issues with imaging quality when the scanned object is outside the scanner's FOV, making reconstructions incomplete and clinical reliability limited.

Method: The I$^2$SB framework uses a direct stochastic mapping between limited-FOV and extended-FOV images, bypassing traditional Gaussian noise-based synthesis methods for faster and more accurate results.

Result: I$^2$SB achieved superior performance with lower RMSE values and faster inference speeds (0.19s per 2D slice), outperforming existing state-of-the-art methods like cDDPM and diffusionGAN.

Conclusion: The proposed I$^2$SB model offers both high accuracy and exceptional computational efficiency, making it ideal for real-time and clinical applications in CT imaging.

Abstract: Computed tomography (CT) is a cornerstone imaging modality for non-invasive,
high-resolution visualization of internal anatomical structures. However, when
the scanned object exceeds the scanner's field of view (FOV), projection data
are truncated, resulting in incomplete reconstructions and pronounced artifacts
near FOV boundaries. Conventional reconstruction algorithms struggle to recover
accurate anatomy from such data, limiting clinical reliability. Deep learning
approaches have been explored for FOV extension, with diffusion generative
models representing the latest advances in image synthesis. Yet, conventional
diffusion models are computationally demanding and slow at inference due to
their iterative sampling process. To address these limitations, we propose an
efficient CT FOV extension framework based on the image-to-image Schr\"odinger
Bridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that
synthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic
mapping between paired limited-FOV and extended-FOV images. This direct
correspondence yields a more interpretable and traceable generative process,
enhancing anatomical consistency and structural fidelity in reconstructions.
I$^2$SB achieves superior quantitative performance, with root-mean-square error
(RMSE) values of 49.8\,HU on simulated noisy data and 152.0HU on real data,
outperforming state-of-the-art diffusion models such as conditional denoising
diffusion probabilistic models (cDDPM) and patch-based diffusion methods.
Moreover, its one-step inference enables reconstruction in just 0.19s per 2D
slice, representing over a 700-fold speedup compared to cDDPM (135s) and
surpassing diffusionGAN (0.58s), the second fastest. This combination of
accuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical
deployment.

</details>


### [304] [Guiding WaveMamba with Frequency Maps for Image Debanding](https://arxiv.org/abs/2508.11331)
*Xinyi Wang,Smaranda Tasmoc,Nantheera Anantrasirichai,Angeliki Katsenou*

Main category: eess.IV

TL;DR: The paper addresses banding artifacts in compressed images, proposing a restoration method using the Wavelet State Space Model and frequency masking.


<details>
  <summary>Details</summary>
Motivation: To tackle the quality degradation in images due to banding artifacts caused by low-bitrate compression and repeated transcoding.

Method: A banding restoration approach combining the Wavelet State Space Model and a frequency masking map, along with a benchmarking of existing methods using public datasets.

Result: The method demonstrates better suppression of banding artifacts compared to state-of-the-art methods, achieving a DBI value of 0.082 on BAND-2k dataset.

Conclusion: The proposed method effectively reduces banding artifacts while maintaining image textures, confirmed by both quantitative and visual evaluations.

Abstract: Compression at low bitrates in modern codecs often introduces banding
artifacts, especially in smooth regions such as skies. These artifacts degrade
visual quality and are common in user-generated content due to repeated
transcoding. We propose a banding restoration method that employs the Wavelet
State Space Model and a frequency masking map to preserve high-frequency
details. Furthermore, we provide a benchmark of open-source banding restoration
methods and evaluate their performance on two public banding image datasets.
Experimentation on the available datasets suggests that the proposed
post-processing approach effectively suppresses banding compared to the
state-of-the-art method (a DBI value of 0.082 on BAND-2k) while preserving
image textures. Visual inspections of the results confirm this. Code and
supplementary material are available at:
https://github.com/xinyiW915/Debanding-PCS2025.

</details>


### [305] [AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis](https://arxiv.org/abs/2508.11375)
*Zonglin Wu,Yule Xue,Qianxiang Hu,Yaoyao Feng,Yuqi Ma,Shanxiong Chen*

Main category: eess.IV

TL;DR: AnatoMaskGAN is a novel GAN-based framework for medical semantic-mask image synthesis, effectively addressing spatial inconsistency issues with enhanced methods and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current GAN-based approaches fail to handle spatial consistency in complex medical image synthesis, limiting their utility for augmenting and analyzing data.

Method: AnatoMaskGAN employs a GNN-based slice-feature fusion module, a 3D spatial noise-injection strategy, and a grayscale-texture classifier to enhance anatomical and contextual representation in generated images.

Result: The framework showed improved PSNR and SSIM scores over current state-of-the-art methods on public datasets, indicating higher reconstruction accuracy and perceptual quality.

Conclusion: The proposed modules and methods in AnatoMaskGAN independently and significantly contribute to medical image synthesis, setting a new benchmark for future frameworks.

Abstract: Medical semantic-mask synthesis boosts data augmentation and analysis, yet
most GAN-based approaches still produce one-to-one images and lack spatial
consistency in complex scans. To address this, we propose AnatoMaskGAN, a novel
synthesis framework that embeds slice-related spatial features to precisely
aggregate inter-slice contextual dependencies, introduces diverse
image-augmentation strategies, and optimizes deep feature learning to improve
performance on complex medical images. Specifically, we design a GNN-based
strongly correlated slice-feature fusion module to model spatial relationships
between slices and integrate contextual information from neighboring slices,
thereby capturing anatomical details more comprehensively; we introduce a
three-dimensional spatial noise-injection strategy that weights and fuses
spatial features with noise to enhance modeling of structural diversity; and we
incorporate a grayscale-texture classifier to optimize grayscale distribution
and texture representation during generation. Extensive experiments on the
public L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR
on L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and
achieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain over
the best model, demonstrating its superiority in reconstruction accuracy and
perceptual quality. Ablation studies that successively remove the slice-feature
fusion module, spatial 3D noise-injection strategy, and grayscale-texture
classifier reveal that each component contributes significantly to PSNR, SSIM,
and LPIPS, further confirming the independent value of each core design in
enhancing reconstruction accuracy and perceptual quality.

</details>


### [306] [LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11391)
*Yinggan Tang,Quanwei Hu*

Main category: eess.IV

TL;DR: The paper introduces LKFMixer, a lightweight convolutional model using large kernels to replace self-attention for image super-resolution, achieving higher performance and faster inference.


<details>
  <summary>Details</summary>
Motivation: Self-attention excels at capturing non-local information crucial for image super-resolution but poses computational challenges for lightweight models.

Method: The authors propose LKFMixer, a CNN model using large kernels (up to 31) for a larger receptive field, coordinate decomposition to reduce computations, a spatial feature modulation block for spatial and channel focus, and a feature selection block to balance local and non-local features.

Result: LKFMixer outperforms current SOTA methods in super-resolution accuracy (notably a 0.6dB PSNR improvement over SwinIR-light on Manga109) and is 5x faster in inference on certain datasets.

Conclusion: The LKFMixer demonstrates the feasibility of using large-kernel CNNs to efficiently replace self-attention in lightweight image super-resolution tasks, offering superior accuracy and speed.

Abstract: The success of self-attention (SA) in Transformer demonstrates the importance
of non-local information to image super-resolution (SR), but the huge computing
power required makes it difficult to implement lightweight models. To solve
this problem, we propose a pure convolutional neural network (CNN) model,
LKFMixer, which utilizes large convolutional kernel to simulate the ability of
self-attention to capture non-local features. Specifically, we increase the
kernel size to 31 to obtain the larger receptive field as possible, and reduce
the parameters and computations by coordinate decomposition. Meanwhile, a
spatial feature modulation block (SFMB) is designed to enhance the focus of
feature information on both spatial and channel dimension. In addition, by
introducing feature selection block (FSB), the model can adaptively adjust the
weights between local features and non-local features. Extensive experiments
show that the proposed LKFMixer family outperform other state-of-the-art (SOTA)
methods in terms of SR performance and reconstruction quality. In particular,
compared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR
improvement at $\times$4 scale, while the inference speed is $\times$5 times
faster. The code is available at https://github.com/Supereeeee/LKFMixer.

</details>


### [307] [Subcortical Masks Generation in CT Images via Ensemble-Based Cross-Domain Label Transfer](https://arxiv.org/abs/2508.11450)
*Augustine X. W. Lee,Pak-Hei Yeung,Jagath C. Rajapakse*

Main category: eess.IV

TL;DR: The paper presents an automatic ensemble framework that generates high-quality subcortical segmentation labels for CT scans using MRI-based models with paired MRI-CT data, enabling improved segmentation performance.


<details>
  <summary>Details</summary>
Motivation: There is a lack of publicly available datasets for subcortical segmentation in CT imaging, even though it is crucial for understanding brain anatomy and diagnosing various disorders.

Method: The study proposes a robust ensembling pipeline to combine MRI-based models and applies this pipeline to unannotated paired MRI-CT data, thereby creating a CT segmentation dataset.

Result: Extensive experiments demonstrate improved performance for segmentation tasks on CT data using the proposed approach, alongside generating a public CT segmentation dataset.

Conclusion: This paper introduces a novel solution for CT subcortical segmentation, provides open-source resources, and facilitates future advancements in CT-based neuroimaging research.

Abstract: Subcortical segmentation in neuroimages plays an important role in
understanding brain anatomy and facilitating computer-aided diagnosis of
traumatic brain injuries and neurodegenerative disorders. However, training
accurate automatic models requires large amounts of labelled data. Despite the
availability of publicly available subcortical segmentation datasets for
Magnetic Resonance Imaging (MRI), a significant gap exists for Computed
Tomography (CT). This paper proposes an automatic ensemble framework to
generate high-quality subcortical segmentation labels for CT scans by
leveraging existing MRI-based models. We introduce a robust ensembling pipeline
to integrate them and apply it to unannotated paired MRI-CT data, resulting in
a comprehensive CT subcortical segmentation dataset. Extensive experiments on
multiple public datasets demonstrate the superior performance of our proposed
framework. Furthermore, using our generated CT dataset, we train segmentation
models that achieve improved performance on related segmentation tasks. To
facilitate future research, we make our source code, generated dataset, and
trained models publicly available at
https://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation,
marking the first open-source release for CT subcortical segmentation to the
best of our knowledge.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [308] [Fluid Dynamics and Domain Reconstruction from Noisy Flow Images Using Physics-Informed Neural Networks and Quasi-Conformal Mapping](https://arxiv.org/abs/2508.11216)
*Han Zhang,Xue-Cheng Tai,Jean-Michel Morel,Raymond H. Chan*

Main category: math.NA

TL;DR: This paper proposes a framework to reconstruct high-quality blood flow images by addressing noise-induced artifacts using a combined optimization approach with physics-based modeling.


<details>
  <summary>Details</summary>
Motivation: High-quality blood flow imaging is crucial for medical diagnostics and is often compromised by artifacts from short acquisition times or device errors, necessitating denoising advancements.

Method: The authors frame the denoising task as an optimization problem with subproblems: (1) A Physics-Informed Neural Network solves the fluid subproblem to reconstruct the velocity field. (2) A quasi-conformal mapping deals with the geometry subproblem to infer the flow region. Both are solved iteratively using a Gauss-Seidel approach.

Result: Experiments on synthetic flow data in a converging channel and real-like aortic flow data with noise validate the approach. Robust reconstruction of high-quality flow images and effective parameter analysis are demonstrated.

Conclusion: The method is effective in reconstructing high-quality blood flow images and offers robustness against noise through an iterative, physics-driven optimization framework.

Abstract: Blood flow imaging provides important information for hemodynamic behavior
within the vascular system and plays an essential role in medical diagnosis and
treatment planning. However, obtaining high-quality flow images remains a
significant challenge. In this work, we address the problem of denoising flow
images that may suffer from artifacts due to short acquisition times or
device-induced errors. We formulate this task as an optimization problem, where
the objective is to minimize the discrepancy between the modeled velocity
field, constrained to satisfy the Navier-Stokes equations, and the observed
noisy velocity data. To solve this problem, we decompose it into two
subproblems: a fluid subproblem and a geometry subproblem. The fluid subproblem
leverages a Physics-Informed Neural Network to reconstruct the velocity field
from noisy observations, assuming a fixed domain. The geometry subproblem aims
to infer the underlying flow region by optimizing a quasi-conformal mapping
that deforms a reference domain. These two subproblems are solved in an
alternating Gauss-Seidel fashion, iteratively refining both the velocity field
and the domain. Upon convergence, the framework yields a high-quality
reconstruction of the flow image. We validate the proposed method through
experiments on synthetic flow data in a converging channel geometry under
varying levels of Gaussian noise, and on real-like flow data in an aortic
geometry with signal-dependent noise. The results demonstrate the effectiveness
and robustness of the approach. Additionally, ablation studies are conducted to
assess the influence of key hyperparameters.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [309] [Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas](https://arxiv.org/abs/2508.11278)
*Francesco Sovrano,Gabriele Dominici,Rita Sevastjanova,Alessandra Stramiglio,Alberto Bacchelli*

Main category: cs.HC

TL;DR: The paper investigates whether general-purpose AI (GPAI) systems exhibit human cognitive biases in software engineering, presenting a benchmarking framework that uncovers bias-related behaviors and risks.


<details>
  <summary>Details</summary>
Motivation: Cognitive biases in human decision-making can cause errors in software engineering. The paper aims to determine if AI systems trained on human data inherit and exhibit similar biases.

Method: The authors use a dynamic benchmarking framework starting with 16 crafted tasks, incorporating cognitive biases and unbiased variants. An augmentation pipeline generates task variants, employing GPAI systems to ensure accuracy and diversity.

Result: Evaluating systems like GPT and LLaMA revealed biases influenced by shallow linguistic cues. The biases ranged from 5.9% to 35% across types, spiking to 49% with higher task complexity.

Conclusion: GPAI systems show critical vulnerabilities to cognitive biases, which increase with task complexity, underscoring risks for software engineering applications.

Abstract: Human cognitive biases in software engineering can lead to costly errors.
While general-purpose AI (GPAI) systems may help mitigate these biases due to
their non-human nature, their training on human-generated data raises a
critical question: Do GPAI systems themselves exhibit cognitive biases?
  To investigate this, we present the first dynamic benchmarking framework to
evaluate data-induced cognitive biases in GPAI within software engineering
workflows. Starting with a seed set of 16 hand-crafted realistic tasks, each
featuring one of 8 cognitive biases (e.g., anchoring, framing) and
corresponding unbiased variants, we test whether bias-inducing linguistic cues
unrelated to task logic can lead GPAI systems from correct to incorrect
conclusions.
  To scale the benchmark and ensure realism, we develop an on-demand
augmentation pipeline relying on GPAI systems to generate task variants that
preserve bias-inducing cues while varying surface details. This pipeline
ensures correctness (88--99% on average, according to human evaluation),
promotes diversity, and controls reasoning complexity by leveraging
Prolog-based reasoning and LLM-as-a-judge validation. It also verifies that the
embedded biases are both harmful and undetectable by logic-based, unbiased
reasoners.
  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent
tendency to rely on shallow linguistic heuristics over deep reasoning. All
systems exhibit cognitive biases (ranging from 5.9% to 35% across types), with
bias sensitivity increasing sharply with task complexity (up to 49%),
highlighting critical risks in real-world software engineering deployments.

</details>


### [310] [Multimodal Quantitative Measures for Multiparty Behaviour Evaluation](https://arxiv.org/abs/2508.10916)
*Ojas Shirekar,Wim Pouw,Chenxu Hao,Vrushank Phadnis,Thabo Beeler,Chirag Raman*

Main category: cs.HC

TL;DR: This paper introduces a framework for evaluating social behavior in multiparty interactions based on skeletal motion data with three novel metrics targeting synchrony, temporal alignment, and structural similarity.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations in existing evaluation methods for autonomous digital humans, which largely miss coordination dynamics in multiparty social interactions.

Method: The paper proposes three metrics (CRQA for synchrony, Beat Consistency for alignment, Soft-DTW for structural similarity) and tests them against controlled perturbations and a perception study using data from group interactions in the DnD dataset.

Result: The three metrics successfully measure predictable shifts in interaction dynamics under perturbations, and the perception study reveals representation effects on observer judgments.

Conclusion: The proposed measures provide a robust framework for analyzing contextual and dynamic social interactions, aiding in the evaluation and refinement of socially intelligent digital agents.

Abstract: Digital humans are emerging as autonomous agents in multiparty interactions,
yet existing evaluation metrics largely ignore contextual coordination
dynamics. We introduce a unified, intervention-driven framework for objective
assessment of multiparty social behaviour in skeletal motion data, spanning
three complementary dimensions: (1) synchrony via Cross-Recurrence
Quantification Analysis, (2) temporal alignment via Multiscale Empirical Mode
Decompositionbased Beat Consistency, and (3) structural similarity via Soft
Dynamic Time Warping. We validate metric sensitivity through three
theory-driven perturbations -- gesture kinematic dampening, uniform
speech-gesture delays, and prosodic pitch-variance reduction-applied to
$\approx 145$ 30-second thin slices of group interactions from the DnD dataset.
Mixed-effects analyses reveal predictable, joint-independent shifts: dampening
increases CRQA determinism and reduces beat consistency, delays weaken
cross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A
complementary perception study ($N=27$) compares judgments of full-video and
skeleton-only renderings to quantify representation effects. Our three measures
deliver orthogonal insights into spatial structure, timing alignment, and
behavioural variability. Thereby forming a robust toolkit for evaluating and
refining socially intelligent agents. Code available on
\href{https://github.com/tapri-lab/gig-interveners}{GitHub}.

</details>


### [311] [Managing the unexpected: Operator behavioural data and its value in predicting correct alarm responses](https://arxiv.org/abs/2508.10917)
*Chidera W. Amazu,Joseph Mietkiewicz,Ammar N. Abbas,Gabriele Baldissone,Davide Fissore,Micaela Demichela,Anders L. Madsen,Maria Chiara Leva*

Main category: cs.HC

TL;DR: The paper investigates using non-intrusive real-time data from system logs to analyze operator behavior and predict performance during critical scenarios in control rooms.


<details>
  <summary>Details</summary>
Motivation: To address the intrusive nature of wearable physiological tools, the study explores alternative real-time data sources from system interactions to monitor operator behavior.

Method: Using a formaldehyde plant simulator, experiments with different configurations were conducted. Analysis involved step-wise logistic regression and Bayesian network modeling.

Result: The study identified predictive metrics for operator behavior and system performance during alarm scenarios.

Conclusion: Real-time behavioral metrics accessible from system logs can be valuable in improving anticipatory decision-making and operator support in critical scenarios.

Abstract: Data from psychophysiological measures can offer new insight into control
room operators' behaviour, cognition, and mental workload status. This can be
particularly helpful when combined with appraisal of capacity to respond to
possible critical plant conditions (i.e. critical alarms response scenarios).
However, wearable physiological measurement tools such as eye tracking and EEG
caps can be perceived as intrusive and not suitable for usage in daily
operations. Therefore, this article examines the potential of using real-time
data from process and operator-system interactions during abnormal scenarios
that can be recorded and retrieved from the distributed control system's
historian or process log, and their capacity to provide insight into operator
behavior and predict their response outcomes, without intruding on daily tasks.
Data for this study were obtained from a design of experiment using a
formaldehyde production plant simulator and four human-in-the-loop experimental
support configurations. A comparison between the different configurations in
terms of both behaviour and performance is presented in this paper. A step-wise
logistic regression and a Bayesian network models were used to achieve this
objective. The results identified some predictive metrics and the paper discuss
their value as precursor or predictor of overall system performance in alarm
response scenarios. Knowledge of relevant and predictive behavioural metrics
accessible in real time can better equip decision-makers to predict outcomes
and provide timely support measures for operators.

</details>


### [312] [Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?](https://arxiv.org/abs/2508.10919)
*Mohammed Saqr,Kamila Misiejuk,Sonsoles López-Pernas*

Main category: cs.HC

TL;DR: The paper examines human-AI collaboration during complex problem-solving, finding dominant instructive interactions with limited cognitive synergy and alignment between prompts and AI output.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the lack of focus on evolving patterns and dynamics in human-AI collaboration during cognitively demanding tasks.

Method: A mixed-method approach involving qualitative coding, network analysis, sequence analysis, chi-square tests, and Mosaic plot visualizations was employed to analyze student-AI interactions.

Result: Findings reveal instructive rather than collaborative interaction patterns, misalignment between prompts and AI outputs, and no significant link between task complexity, prompt length, and student performance.

Conclusion: The study concludes that current large language models (LLMs) optimized for instruction-following hinder their potential as cognitively-aligned collaborative partners, recommending future AI design focus on enhancing cognitive partnership.

Abstract: While research on human-AI collaboration exists, it mainly examined language
learning and used traditional counting methods with little attention to
evolution and dynamics of collaboration on cognitively demanding tasks. This
study examines human-AI interactions while solving a complex problem.
Student-AI interactions were qualitatively coded and analyzed with transition
network analysis, sequence analysis and partial correlation networks as well as
comparison of frequencies using chi-square and Person-residual shaded Mosaic
plots to map interaction patterns, their evolution, and their relationship to
problem complexity and student performance. Findings reveal a dominant
Instructive pattern with interactions characterized by iterative ordering
rather than collaborative negotiation. Oftentimes, students engaged in long
threads that showed misalignment between their prompts and AI output that
exemplified a lack of synergy that challenges the prevailing assumptions about
LLMs as collaborative partners. We also found no significant correlations
between assignment complexity, prompt length, and student grades suggesting a
lack of cognitive depth, or effect of problem difficulty. Our study indicates
that the current LLMs, optimized for instruction-following rather than
cognitive partnership, compound their capability to act as cognitively
stimulating or aligned collaborators. Implications for designing AI systems
that prioritize cognitive alignment and collaboration are discussed.

</details>


### [313] [GhostObjects: Instructing Robots by Manipulating Spatially Aligned Virtual Twins in Augmented Reality](https://arxiv.org/abs/2508.11022)
*Lauren W. Wang,Parastoo Abtahi*

Main category: cs.HC

TL;DR: This paper proposes using interactive AR "GhostObjects" to give precise robot instructions for complex tasks rather than relying on traditional control methods like teleoperation.


<details>
  <summary>Details</summary>
Motivation: Enhancing human-robot interaction by enabling users to intuitively give instructions for complex robotic operations without direct manual control.

Method: Introducing GhostObjects as AR virtual twins for real-world objects and enabling physical goal specification through AR manipulation, allowing advanced task execution.

Result: GhostObjects facilitate precise physical goals and spatial parameters including multisobject interaction and advanced task setups beyond basic operations.

Conclusion: The approach provides an intuitive and effective alternative to direct robot control by leveraging AR-based manipulation with GhostObjects for complex instruction.

Abstract: Robots are increasingly capable of autonomous operations, yet human
interaction remains essential for issuing personalized instructions. Instead of
directly controlling robots through Programming by Demonstration (PbD) or
teleoperation, we propose giving instructions by interacting with
GhostObjects-world-aligned, life-size virtual twins of physical objects-in
augmented reality (AR). By direct manipulation of GhostObjects, users can
precisely specify physical goals and spatial parameters, with features
including real-world lasso selection of multiple objects and snapping back to
default positions, enabling tasks beyond simple pick-and-place.

</details>


### [314] [ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality](https://arxiv.org/abs/2508.11426)
*Steffen Hauck,Diar Abdlkarim,John Dudley,Per Ola Kristensson,Eyal Ofek,Jens Grubert*

Main category: cs.HC

TL;DR: The paper explores the effectiveness of a visualization tool called ReachVox for enhancing human-robot collaboration in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address challenges in planning and understanding robot movements for collaborative tasks, especially in dynamic environments requiring frequent motion path adaptations.

Method: The study introduces ReachVox, a minimalistic encoding for visualizing point reachability near an object, and validates its effectiveness through VR-based user experiments with 20 subjects.

Result: The user study demonstrates the potential of ReachVox for improving operator collaboration with a robotic arm compared to traditional point-based reachability methods.

Conclusion: ReachVox provides significant advantages in aiding human-robot collaboration, showcasing better reachability visualization and usability in VR setups compared to conventional approaches.

Abstract: Human-Robot-Collaboration can enhance workflows by leveraging the mutual
strengths of human operators and robots. Planning and understanding robot
movements remain major challenges in this domain. This problem is prevalent in
dynamic environments that might need constant robot motion path adaptation. In
this paper, we investigate whether a minimalistic encoding of the reachability
of a point near an object of interest, which we call ReachVox, can aid the
collaboration between a remote operator and a robotic arm in VR. Through a user
study (n=20), we indicate the strength of the visualization relative to a
point-based reachability check-up.

</details>


### [315] [AI That Helps Us Help Each Other: A Proactive System for Scaffolding Mentor-Novice Collaboration in Entrepreneurship Coaching](https://arxiv.org/abs/2508.11052)
*Evey Jiaxin Huang,Matthew Easterday,Elizabeth Gerber*

Main category: cs.HC

TL;DR: The paper introduces a human-AI coaching system to support entrepreneurial metacognition and mentor collaboration by combining a cognitive model with a large language model, improving meeting quality and decision-making under uncertainty.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the challenges novice entrepreneurs face with metacognitive tasks, and the limited time and visibility mentors have to offer personalized support.

Method: A human-AI coaching system was developed that combines a cognitive model of entrepreneurial risk with a large language model (LLM). It proactively assists novices with diagnostic questions and supports mentors with planning emotionally attuned and focused meetings.

Result: The system was field-tested and found to enhance novice metacognition, improve mentor strategies, and deepen meeting intent and focus, while revealing tensions like trust and misdiagnosis in AI.

Conclusion: Proactive AI systems can scaffold metacognition and foster human collaboration in ill-defined domains, with applications extending to healthcare, education, and knowledge work.

Abstract: Entrepreneurship requires navigating open-ended, ill-defined problems:
identifying risks, challenging assumptions, and making strategic decisions
under deep uncertainty. Novice founders often struggle with these metacognitive
demands, while mentors face limited time and visibility to provide tailored
support. We present a human-AI coaching system that combines a domain-specific
cognitive model of entrepreneurial risk with a large language model (LLM) to
proactively scaffold both novice and mentor thinking. The system proactively
poses diagnostic questions that challenge novices' thinking and helps both
novices and mentors plan for more focused and emotionally attuned meetings.
Critically, mentors can inspect and modify the underlying cognitive model,
shaping the logic of the system to reflect their evolving needs. Through an
exploratory field deployment, we found that using the system supported novice
metacognition, helped mentors plan emotionally attuned strategies, and improved
meeting depth, intentionality, and focus--while also surfaced key tensions
around trust, misdiagnosis, and expectations of AI. We contribute design
principles for proactive AI systems that scaffold metacognition and human-human
collaboration in complex, ill-defined domains, offering implications for
similar domains like healthcare, education, and knowledge work.

</details>


### [316] [Uncovering Latent Connections in Indigenous Heritage: Semantic Pipelines for Cultural Preservation in Brazil](https://arxiv.org/abs/2508.10911)
*Luis Vitor Zerkowski,Nina S. T. Hirata*

Main category: cs.HC

TL;DR: This paper explores how AI can assist in the preservation of Brazilian Indigenous heritage by creating visualization and exploration tools using semantic pipelines.


<details>
  <summary>Details</summary>
Motivation: To address the challenges Indigenous communities face in preserving cultural heritage, particularly amid systemic marginalization and urban development, while leveraging modern technology.

Method: The authors used AI to develop two semantic pipelines: a visual pipeline for image similarity and a textual pipeline for item descriptions, both projected in two dimensions and integrated into an interactive visualization tool.

Result: The system enhances user navigation through similarity, temporal, and geographic views, supports curatorial tasks, boosts public engagement, and uncovers latent cultural connections.

Conclusion: AI-driven tools can ethically contribute to preserving cultural heritage by improving accessibility, interpretation, and exploration for Indigenous artifacts in Brazil.

Abstract: Indigenous communities face ongoing challenges in preserving their cultural
heritage, particularly in the face of systemic marginalization and urban
development. In Brazil, the Museu Nacional dos Povos Indigenas through the
Tainacan platform hosts the country's largest online collection of Indigenous
objects and iconographies, providing a critical resource for cultural
engagement. Using publicly available data from this repository, we present a
data-driven initiative that applies artificial intelligence to enhance
accessibility, interpretation, and exploration. We develop two semantic
pipelines: a visual pipeline that models image-based similarity and a textual
pipeline that captures semantic relationships from item descriptions. These
embedding spaces are projected into two dimensions and integrated into an
interactive visualization tool we also developed. In addition to
similarity-based navigation, users can explore the collection through temporal
and geographic lenses, enabling both semantic and contextualized perspectives.
The system supports curatorial tasks, aids public engagement, and reveals
latent connections within the collection. This work demonstrates how AI can
ethically contribute to cultural preservation practices.

</details>


### [317] [Human-in-the-Loop Systems for Adaptive Learning Using Generative AI](https://arxiv.org/abs/2508.11062)
*Bhavishya Tarun,Haoze Du,Dinesh Kannan,Edward F. Gehringer*

Main category: cs.HC

TL;DR: The paper presents a Human-in-the-Loop (HITL) learning model leveraging generative AI for personalized learning by integrating student feedback.


<details>
  <summary>Details</summary>
Motivation: To improve student engagement, retention, and confidence in academic subjects like STEM by introducing an adaptive, feedback-driven learning environment.

Method: Combines tagging, prompt engineering, and a Retrieval-Augmented Generation (RAG) system, allowing students to critique and modify AI-generated solutions in real time.

Result: Preliminary study results suggest improved learning outcomes and student confidence when compared to traditional AI learning tools.

Conclusion: The HITL approach demonstrates potential for creating interactive, personalized, and effective learning experiences, emphasizing student-driven feedback.

Abstract: A Human-in-the-Loop (HITL) approach leverages generative AI to enhance
personalized learning by directly integrating student feedback into
AI-generated solutions. Students critique and modify AI responses using
predefined feedback tags, fostering deeper engagement and understanding. This
empowers students to actively shape their learning, with AI serving as an
adaptive partner. The system uses a tagging technique and prompt engineering to
personalize content, informing a Retrieval-Augmented Generation (RAG) system to
retrieve relevant educational material and adjust explanations in real time.
This builds on existing research in adaptive learning, demonstrating how
student-driven feedback loops can modify AI-generated responses for improved
student retention and engagement, particularly in STEM education. Preliminary
findings from a study with STEM students indicate improved learning outcomes
and confidence compared to traditional AI tools. This work highlights AI's
potential to create dynamic, feedback-driven, and personalized learning
environments through iterative refinement.

</details>


### [318] [Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis](https://arxiv.org/abs/2508.11398)
*Mithat Can Ozgun,Jiahuan Pei,Koen Hindriks,Lucia Donatelli,Qingzhi Liu,Xin Sun,Junxiao Wang*

Main category: cs.HC

TL;DR: DSM5AgentFlow, a specialized LLM-based workflow, autonomously generates DSM-5 diagnostic questionnaires to address gaps in mental health diagnosis, emphasizing realism, accuracy, and explainability.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the limitations of LLMs in specialized domains like mental health diagnosis, where they fail to match general application performance due to challenges like limited datasets and lack of clinical reasoning alignment.

Method: The proposed DSM5AgentFlow simulates therapist-client dialogues to autonomously generate DSM-5 Level-1 diagnostic questionnaires, delivering explainable, step-by-step disorder predictions while adhering to ethical standards.

Result: The framework was comprehensively evaluated across conversational realism, diagnostic accuracy, and explainability, showcasing its potential and offering open-sourced datasets and implementations.

Conclusion: DSM5AgentFlow serves as a complementary, transparent, and ethical tool for mental health diagnosis, bridging the gap between current LLM capabilities and clinical needs in this specialized domain.

Abstract: LLM-based agents have emerged as transformative tools capable of executing
complex tasks through iterative planning and action, achieving significant
advancements in understanding and addressing user needs. Yet, their
effectiveness remains limited in specialized domains such as mental health
diagnosis, where they underperform compared to general applications. Current
approaches to integrating diagnostic capabilities into LLMs rely on scarce,
highly sensitive mental health datasets, which are challenging to acquire.
These methods also fail to emulate clinicians' proactive inquiry skills, lack
multi-turn conversational comprehension, and struggle to align outputs with
expert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the
first LLM-based agent workflow designed to autonomously generate DSM-5 Level-1
diagnostic questionnaires. By simulating therapist-client dialogues with
specific client profiles, the framework delivers transparent, step-by-step
disorder predictions, producing explainable and trustworthy results. This
workflow serves as a complementary tool for mental health diagnosis, ensuring
adherence to ethical and legal standards. Through comprehensive experiments, we
evaluate leading LLMs across three critical dimensions: conversational realism,
diagnostic accuracy, and explainability. Our datasets and implementations are
fully open-sourced.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [319] [Tabularis Formatus: Predictive Formatting for Tables](https://arxiv.org/abs/2508.11121)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Gust Verbruggen*

Main category: cs.DB

TL;DR: TaFo is a neuro-symbolic approach that automatically generates conditional formatting (CF) suggestions for spreadsheets, outperforming existing methods by a significant margin.


<details>
  <summary>Details</summary>
Motivation: Creating CF rules in spreadsheets is challenging due to the need for technical expertise, platform-specific knowledge, and limited interfaces.

Method: TaFo combines semantic language model knowledge with component-based synthesis systems and diversity-preserving rule ranking to produce predictive and automated CF suggestions.

Result: TaFo achieved 15.6%–26.5% improvement over existing systems in matching user-added ground truth rules using a corpus of 1.8 million public workbooks.

Conclusion: TaFo simplifies and enhances spreadsheet formatting by automating CF rule generation, making the process more accessible and effective for users.

Abstract: Spreadsheet manipulation software are widely used for data management and
analysis of tabular data, yet the creation of conditional formatting (CF) rules
remains a complex task requiring technical knowledge and experience with
specific platforms. In this paper we present TaFo, a neuro-symbolic approach to
generating CF suggestions for tables, addressing common challenges such as user
unawareness, difficulty in rule creation, and inadequate user interfaces. TaFo
takes inspiration from component based synthesis systems and extends them with
semantic knowledge of language models and a diversity preserving rule
ranking.Unlike previous methods focused on structural formatting, TaFo uniquely
incorporates value-based formatting, automatically learning both the rule
trigger and the associated visual formatting properties for CF rules. By
removing the dependency on user specification used by existing techniques in
the form of formatted examples or natural language instruction, TaFo makes
formatting completely predictive and automated for the user. To evaluate TaFo,
we use a corpus of 1.8 Million public workbooks with CF and manual formatting.
We compare TaFo against a diverse set of symbolic and neural systems designed
for or adapted for the task of table formatting. Our results show that TaFo
generates more accurate, diverse and complete formatting suggestions than
current systems and outperforms these by 15.6\%--26.5\% on matching user added
ground truth rules in tables.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [320] [Functional Analysis of Variance for Association Studies](https://arxiv.org/abs/2508.11069)
*Olga A. Vsevolozhskaya,Dmitri V. Zaykin,Mark C. Greenwood,Changshuai Wei,Qing Lu*

Main category: stat.AP

TL;DR: The paper introduces FANOVA, a statistical method designed to more effectively identify genetic sequence variants associated with diseases, outperforming existing tools SKAT and FLM.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of uncovering unknown genetic variants associated with complex diseases, as current methods explain only a small portion of genetic heritability.

Method: The FANOVA method tests the joint effect of genetic variants using linkage disequilibrium, genetic position information, and accommodates both protective and risk-increasing variants.

Result: Simulations demonstrate FANOVA’s superior performance over SKAT and FLM, particularly for small sample sizes or variants with low to moderate effects. Empirical analysis using the Dallas Heart Study data confirmed FANOVA’s capability by identifying multiple obesity-associated genes.

Conclusion: FANOVA provides a computationally efficient way to enhance discovery of disease-associated variants, offering improvements over existing methods in both theoretical and practical settings.

Abstract: While progress has been made in identifying common genetic variants
associated with human diseases, for most of common complex diseases, the
identified genetic variants only account for a small proportion of
heritability. Challenges remain in finding additional unknown genetic variants
predisposing to complex diseases. With the advance in next-generation
sequencing technologies, sequencing studies have become commonplace in genetic
research. The ongoing exome-sequencing and whole-genome-sequencing studies
generate a massive amount of sequencing variants and allow researchers to
comprehensively investigate their role in human diseases. The discovery of new
disease-associated variants can be enhanced by utilizing powerful and
computationally efficient statistical methods. In this paper, we propose a
functional analysis of variance (FANOVA) method for testing an association of
sequence variants in a genomic region with a qualitative trait. The FANOVA has
a number of advantages: (1) it tests for a joint effect of gene variants,
including both common and rare; (2) it fully utilizes linkage disequilibrium
and genetic position information; and (3) allows for either protective or
risk-increasing causal variants. Through simulations, we show that FANOVA
outperform two popularly used methods - SKAT and a previously proposed method
based on functional linear models (FLM), - especially if a sample size of a
study is small and/or sequence variants have low to moderate effects. We
conduct an empirical study by applying three methods (FANOVA, SKAT and FLM) to
sequencing data from Dallas Heart Study. While SKAT and FLM respectively
detected ANGPTL 4 and ANGPTL 3 associated with obesity, FANOVA was able to
identify both genes associated with obesity.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [321] [A weighted U statistic for association analysis considering genetic heterogeneity](https://arxiv.org/abs/1504.08319)
*Changshuai Wei,Robert C. Elston,Qing Lu*

Main category: stat.ME

TL;DR: The paper introduces the HWU method to analyze genetic heterogeneity in complex diseases, demonstrating its advantages through simulations and a nicotine dependence genome-wide study.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with detecting genetic associations in diseases with heterogeneous etiologies, limiting the power of genetic studies.

Method: The study introduces the HWU method, designed for association analyses across diverse phenotypes and high-dimensional genetic data, showcasing enhanced efficiency and robustness.

Result: Simulations confirm HWU's effectiveness for heterogeneous genetic etiologies. Using HWU, analysis of nearly 1 million genetic markers identified two genes (CYP3A5 and IKBKB) linked to nicotine dependence.

Conclusion: HWU addresses genetic heterogeneity in complex diseases, improving detection in diverse scenarios and offering robust genome-wide association analysis capabilities.

Abstract: Converging evidence suggests that common complex diseases with the same or
similar clinical manifestations could have different underlying genetic
etiologies. While current research interests have shifted toward uncovering
rare variants and structural variations predisposing to human diseases, the
impact of heterogeneity in genetic studies of complex diseases has been largely
overlooked. Most of the existing statistical methods assume the disease under
investigation has a homogeneous genetic effect and could, therefore, have low
power if the disease undergoes heterogeneous pathophysiological and etiological
processes. In this paper, we propose a heterogeneity weighted U (HWU) method
for association analyses considering genetic heterogeneity. HWU can be applied
to various types of phenotypes (e.g., binary and continuous) and is
computationally effcient for high- dimensional genetic data. Through
simulations, we showed the advantage of HWU when the underlying genetic
etiology of a disease was heterogeneous, as well as the robustness of HWU
against different model assumptions (e.g., phenotype distributions). Using HWU,
we conducted a genome-wide analysis of nicotine dependence from the Study of
Addiction: Genetics and Environments (SAGE) dataset. The genome-wide analysis
of nearly one million genetic markers took 7 hours, identifying heterogeneous
effects of two new genes (i.e., CYP3A5 and IKBKB) on nicotine dependence.

</details>


### [322] [A Generalized Similarity U Test for Multivariate Analysis of Sequencing Data](https://arxiv.org/abs/1505.01179)
*Changshuai Wei,Qing Lu*

Main category: stat.ME

TL;DR: The paper introduces GSU, a test method for analyzing high-dimensional genetic data in complex diseases, which is effective for multiple phenotypes with diverse distributions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by high-dimensional genetic data and low variant frequency in traditional statistical methods, as well as the need for tools that can handle associations with multiple phenotypes.

Method: The authors proposed the Generalized Similarity U test (GSU), a similarity-based method capable of handling high-dimensional genotype and phenotype data. They evaluated its theoretical properties, developed efficient p-value and power calculations, and conducted simulations to test its robustness.

Result: The GSU method demonstrated superior power and robustness compared to existing methods. It could identify associations even with phenotypes following different distributions, as validated in simulations and real datasets.

Conclusion: GSU offers a robust and powerful solution for genetic association studies involving complex diseases, high-dimensional data, and multi-phenotype analysis.

Abstract: Sequencing-based studies are emerging as a major tool for genetic association
studies of complex diseases. These studies pose great challenges to the
traditional statistical methods (e.g., single-locus analyses based on
regression methods) because of the high-dimensionality of data and the low
frequency of genetic variants. In addition, there is a great interest in
biology and epidemiology to identify genetic risk factors contributed to
multiple disease phenotypes. The multiple phenotypes can often follow different
distributions, which violates the assumptions of most current methods. In this
paper, we propose a generalized similarity U test, referred to as GSU. GSU is a
similarity-based test and can handle high-dimensional genotypes and phenotypes.
We studied the theoretical properties of GSU, and provided the efficient
p-value calculation for association test as well as the sample size and power
calculation for the study design. Through simulation, we found that GSU had
advantages over existing methods in terms of power and robustness to phenotype
distributions. Finally, we used GSU to perform a multivariate analysis of
sequencing data in the Dallas Heart Study and identified a joint association of
4 genes with 5 metabolic related phenotypes.

</details>


### [323] [A Weighted U Statistic for Genetic Association Analyses of Sequencing Data](https://arxiv.org/abs/1505.01204)
*Changshuai Wei,Ming Li,Zihuai He,Olga Vsevolozhskaya,Daniel J. Schaid,Qing Lu*

Main category: stat.ME

TL;DR: The paper introduces WU-seq, a weighted U statistic method for analyzing high-dimensional sequencing data, outperforming traditional SKAT in specific scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional methods in analyzing high-dimensional sequencing data for rare variants and complex diseases.

Method: Usage of a weighted U statistic (WU-seq) that makes no assumptions about disease models or phenotype distributions.

Result: Through simulations and empirical studies, WU-seq demonstrated better performance compared to SKAT under restrictive conditions while maintaining comparable efficiency otherwise.

Conclusion: WU-seq provides a robust and efficient solution for association analyses, confirming its utility through real-world application and outperforming traditional methods in challenging scenarios.

Abstract: With advancements in next generation sequencing technology, a massive amount
of sequencing data are generated, offering a great opportunity to
comprehensively investigate the role of rare variants in the genetic etiology
of complex diseases. Nevertheless, this poses a great challenge for the
statistical analysis of high-dimensional sequencing data. The association
analyses based on traditional statistical methods suffer substantial power loss
because of the low frequency of genetic variants and the extremely high
dimensionality of the data. We developed a weighted U statistic, referred to as
WU-seq, for the high-dimensional association analysis of sequencing data. Based
on a non-parametric U statistic, WU-SEQ makes no assumption of the underlying
disease model and phenotype distribution, and can be applied to a variety of
phenotypes. Through simulation studies and an empirical study, we showed that
WU-SEQ outperformed a commonly used SKAT method when the underlying assumptions
were violated (e.g., the phenotype followed a heavy-tailed distribution). Even
when the assumptions were satisfied, WU-SEQ still attained comparable
performance to SKAT. Finally, we applied WU-seq to sequencing data from the
Dallas Heart Study (DHS), and detected an association between ANGPTL 4 and very
low density lipoprotein cholesterol.

</details>


### [324] [Generalized Similarity U: A Non-parametric Test of Association Based on Similarity](https://arxiv.org/abs/1801.01220)
*Changshuai Wei,Qing Lu*

Main category: stat.ME

TL;DR: This paper introduces a method called Generalized Similarity U (GSU) to test associations between complex genetic and phenotypic datasets, showing it outperforms existing methods and identifying specific genes related to Alzheimer's Disease.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of testing associations between complex genotypic and phenotypic objects, which are common in sequencing-based genetic studies.

Method: The paper proposes GSU, a similarity-based statistical test employing Laplacian kernel-based similarity for enhanced power and robustness. It was validated through simulations and applied to Alzheimer's Disease data.

Result: GSU demonstrated better power and robustness compared to existing methods, and was successfully used to identify genetic associations (APOE, APOC1, TOMM40) with Alzheimer's imaging phenotypes.

Conclusion: The GSU framework is both a theoretically and practically effective tool for genetic association studies, supporting its utility in complex datasets, and a C++ package for its implementation is made available.

Abstract: Second generation sequencing technologies are being increasingly used for
genetic association studies, where the main research interest is to identify
sets of genetic variants that contribute to various phenotype. The phenotype
can be univariate disease status, multivariate responses and even
high-dimensional outcomes. Considering the genotype and phenotype as two
complex objects, this also poses a general statistical problem of testing
association between complex objects. We here proposed a similarity-based test,
generalized similarity U (GSU), that can test the association between complex
objects. We first studied the theoretical properties of the test in a general
setting and then focused on the application of the test to sequencing
association studies. Based on theoretical analysis, we proposed to use
Laplacian kernel based similarity for GSU to boost power and enhance
robustness. Through simulation, we found that GSU did have advantages over
existing methods in terms of power and robustness. We further performed a whole
genome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative
(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with
imaging phenotype. We developed a C++ package for analysis of whole genome
sequencing data using GSU. The source codes can be downloaded at
https://github.com/changshuaiwei/gsu.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [325] [CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems](https://arxiv.org/abs/2508.11287)
*Xuran Liu,Nan Xue,Rui Bao,Yaping Sun,Zhiyong Chen,Meixia Tao,Xiaodong Xu,Shuguang Cui*

Main category: cs.IT

TL;DR: The paper addresses the challenge of deploying large language models on edge devices by proposing a latency-aware scheduling framework that minimizes inference latency during initial model loading.


<details>
  <summary>Details</summary>
Motivation: Deploying large language models on edge devices is crucial for low-latency and privacy-preserving AI services. However, limited device resources and cold-start latency during model loading create bottlenecks.

Method: The framework dynamically adjusts layer partitioning and allocation to overlap model loading with computation and communication, minimizing idle periods. The problem is formulated as a Mixed-Integer Non-Linear Program, with a dynamic programming algorithm for optimization.

Result: Experimental results demonstrate significant reductions in cold-start latency using the proposed method compared to baseline strategies.

Conclusion: The proposed latency-aware scheduling framework effectively addresses key bottlenecks in deploying large language models on edge devices, enabling more efficient AI services.

Abstract: While deploying large language models on edge devices promises low-latency
and privacy-preserving AI services, it is hindered by limited device resources.
Although pipeline parallelism facilitates distributed inference, existing
approaches often ignore the cold-start latency caused by on-demand model
loading. In this paper, we propose a latency-aware scheduling framework that
overlaps model loading with computation and communication to minimize total
inference latency. Based on device and model parameters, the framework
dynamically adjusts layer partitioning and allocation to effectively hide
loading time, thereby eliminating as many idle periods as possible. We
formulate the problem as a Mixed-Integer Non-Linear Program and design an
efficient dynamic programming algorithm to optimize model partitioning and
device assignment. Experimental results show that the proposed method
significantly reduces cold-start latency compared to baseline strategies.

</details>


### [326] [Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks](https://arxiv.org/abs/2508.11291)
*Rui Bao,Nan Xue,Yaping Sun,Zhiyong Chen*

Main category: cs.IT

TL;DR: The paper addresses the challenges of integrating LLMs with wireless edge devices by proposing a dynamic routing framework that balances quality and latency in inference.


<details>
  <summary>Details</summary>
Motivation: To enable ubiquitous intelligent services by integrating wireless communications and LLMs, while addressing the trade-offs between inference quality and latency in resource-constrained edge environments.

Method: The proposed framework dynamically routes inference between lightweight models on mobile devices and powerful models on edge servers using cost models based on task complexity, resource constraints, and context-aware factors.

Result: The framework reduces response latency by 5-15% and large model invocations by 10-20% while maintaining full inference quality, as validated on benchmarks MMLU, GSM8K, and MT-Bench-101.

Conclusion: The proposed framework effectively balances the trade-off between latency and quality in LLM deployments for edge-device collaborative environments, optimizing performance and resource utilization.

Abstract: The integration of wireless communications and Large Language Models (LLMs)
is poised to unlock ubiquitous intelligent services, yet deploying them in
wireless edge-device collaborative environments presents a critical trade-off
between inference quality and end-to-end latency. A fundamental mismatch exists
between task complexity and resource allocation: offloading simple queries
invites prohibitive latency, while on-device models lack the capacity for
demanding computations. To address this challenge, we propose a dynamic,
quality-latency aware routing framework that orchestrates inference between a
lightweight model on the mobile device and a powerful model on the edge server.
Our framework employs two distinct cost models: for single-turn queries, it
fuses a BERT-predicted semantic score with communication and computation
overheads; for multi-turn dialogues, it further quantifies context-aware costs
arising from model switching and KV-cache management. While maintaining full
inference quality, extensive experiments demonstrate that our framework cuts
average response latency by 5-15% and reduces large model invocations by 10-20%
against competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [327] [Optimizing ROS 2 Communication for Wireless Robotic Systems](https://arxiv.org/abs/2508.11366)
*Sanghoon Lee,Taehun Kim,Jiyeong Chae,Kyung-Joon Park*

Main category: cs.NI

TL;DR: ROS 2 faces wireless transmission challenges for large payloads. The paper presents a DDS optimization framework improving communication efficiency over lossy links.


<details>
  <summary>Details</summary>
Motivation: Wireless transmission of large payloads, like images and LiDAR data, in ROS 2 triggers performance issues over lossy networks. A deep analysis and solution is needed to address this.

Method: The authors analyzed network-layer performance issues in ROS 2 DDS communication stack under wireless conditions. They proposed a lightweight, XML-configurable DDS optimization framework tailored to link and payload needs.

Result: The optimization framework effectively delivers large payloads in challenging wireless scenarios with low latency, surpassing standard DDS modes.

Conclusion: The proposed optimization framework improves wireless communication for large payloads in ROS 2 without requiring protocol changes, additional components, or complex integrations.

Abstract: Wireless transmission of large payloads, such as high-resolution images and
LiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source
robotics middleware. The default Data Distribution Service (DDS) communication
stack in ROS 2 exhibits significant performance degradation over lossy wireless
links. Despite the widespread use of ROS 2, the underlying causes of these
wireless communication challenges remain unexplored. In this paper, we present
the first in-depth network-layer analysis of ROS 2's DDS stack under wireless
conditions with large payloads. We identify the following three key issues:
excessive IP fragmentation, inefficient retransmission timing, and congestive
buffer bursts. To address these issues, we propose a lightweight and fully
compatible DDS optimization framework that tunes communication parameters based
on link and payload characteristics. Our solution can be seamlessly applied
through the standard ROS 2 application interface via simple XML-based QoS
configuration, requiring no protocol modifications, no additional components,
and virtually no integration efforts. Extensive experiments across various
wireless scenarios demonstrate that our framework successfully delivers large
payloads in conditions where existing DDS modes fail, while maintaining low
end-to-end latency.

</details>
