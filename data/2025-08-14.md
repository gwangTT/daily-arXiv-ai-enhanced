<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 122]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.LG](#cs.LG) [Total: 94]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 22]
- [cs.SE](#cs.SE) [Total: 10]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [nlin.CG](#nlin.CG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 12]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 3]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.HC](#cs.HC) [Total: 4]
- [econ.GN](#econ.GN) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 4]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.NI](#cs.NI) [Total: 12]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit adapts value function initialization to deep reinforcement learning (DRL) by reusing tabular Q-values from prior tasks to improve learning efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing RL techniques struggle with knowledge transfer due to the continuous state-action space and noisy neural network approximations in DRL.

Method: The authors propose DQInit, which softly integrates transferred tabular Q-values into underexplored regions based on a knownness mechanism, allowing gradual shift to learned estimates.

Result: DQInit demonstrates improved early learning efficiency, stability, and overall performance in multiple continuous control tasks compared to other initialization methods.

Conclusion: DQInit provides a practical way to transfer knowledge using value estimates, offering advantages over traditional methods and enhancing DRL performance.

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [2] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: The Othello AI Arena introduces a benchmark evaluating AI systems' rapid adaptation to unseen environments, using Othello board setups.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to assess AI systems' flexibility and generalization in adapting to novel environmental changes.

Method: The Arena uses meta-learning challenges where AI systems analyze and adapt to novel Othello board rules within 60 seconds.

Result: Initial tests show diverse adaptation strategies like parameter tuning and simulation-based learning patterns.

Conclusion: The Arena is an educational and research tool that emphasizes rapid adaptation as a critical skill for advancing intelligent systems.

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [3] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: The paper proposes an automated multi-modal evaluation framework using large language models and a three-tier agent architecture to improve the evaluation of AI assistants.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of current evaluation methods for multi-modal AI assistants, such as high manual costs, inconsistent standards, and subjective bias.

Method: They designed a three-tier agent architecture including interaction evaluation agents, semantic verification agents, and experience decision agents, and fine-tuned the Qwen3-8B model to automate evaluation.

Result: The framework showed significant accuracy in matching evaluations to human experts and demonstrated effectiveness in predicting user satisfaction and identifying defects across eight major intelligent agents.

Conclusion: The proposed framework provides a practical and reliable solution for automated evaluation, advancing the field of AI assistant evaluation methods.

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [4] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: Proposed EvoCurr enables large language models (LLMs) to improve their decision-making skills through LLM-driven curriculum learning based on adaptive problem difficulty.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with highly complex problems requiring deep reasoning over long horizons, and lack structured intermediate guidance.

Method: Introduced EvoCurr, a self-evolving curriculum generation framework where a curriculum-generation LLM creates dynamic problem sequences tailored to the solver LLM's learning progress.

Result: Experimental results show that EvoCurr significantly improves task success rates and efficiency on complex decision-making benchmarks compared to baseline methods.

Conclusion: LLM-driven curriculum learning like EvoCurr has strong potential to enhance automated reasoning in complex, real-world domains.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [5] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: This paper proposes a method to decompose uncertainty in SHAP values into aleatoric, epistemic, and entanglement components using techniques like Dempster-Shafer theory and Dirichlet processes. The findings emphasize improving the reliability of SHAP explanations for high-stakes applications.


<details>
  <summary>Details</summary>
Motivation: SHAP values often ignore uncertainty in predictive models, which is critical for high-stakes applications like healthcare analytics.

Method: The authors integrate Dempster-Shafer evidence theory with hypothesis sampling via Dirichlet processes to disentangle uncertainty components in SHAP values.

Result: Experimental validation revealed that high SHAP values are not inherently stable, with epistemic uncertainty being reducible through better data and modeling approaches.

Conclusion: Tree-based models help effectively quantify epistemic uncertainty, enhancing the interpretability and reliability of SHAP-based explanations for robust decision-making.

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [6] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: MEML-GRPO improves RLVR by employing multiple expert prompts and mutual learning among them, achieving better reasoning capabilities in LLMs.


<details>
  <summary>Details</summary>
Motivation: Standard reinforcement learning with verifiable rewards (RLVR) struggles with sparse reward signals, particularly in tasks where there are consistently incorrect responses.

Method: The MEML-GRPO framework employs diverse expert prompts to generate varied responses and introduces mutual learning mechanisms among experts to share and transfer knowledge effectively.

Result: Experiments reveal MEML-GRPO yields an average performance gain of 4.89% with Qwen and 11.33% with Llama across reasoning benchmarks.

Conclusion: MEML-GRPO overcomes RLVR's reward sparsity issue, improving reasoning performance in large language models significantly.

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [7] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: This paper introduces UDA, a fully unsupervised framework to tackle preference bias in pairwise evaluations of Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Pairwise evaluation of LLMs often suffers from preference bias, which results in inconsistent rankings and judge disagreements, necessitating a solution for fairer comparisons.

Method: The proposed UDA framework dynamically adjusts Elo ratings by using a compact neural network to adaptively set the K-factor and refine win probabilities in an unsupervised manner, aiming to minimize dispersion in Elo trajectories and align evaluations towards a collective consensus.

Result: Experiments show that UDA can reduce inter-judge rating variability by 63.4% and enhance average correlation with human judgment by 24.7%, while uplifting low-performing judges' evaluations to meet high-quality standards.

Conclusion: The UDA framework improves the robustness and reliability of LLM evaluations by mitigating preference bias and aligning judge outputs toward collective consensus, thereby fostering a stable evaluation ecosystem.

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [8] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: The paper introduces PacifAIst, a benchmark to evaluate self-preferential behaviors of LLMs in scenarios with conflicting goals and human safety. It finds variability in model alignment, with Google's Gemini 2.5 Flash performing best and GPT-5 showing alignment concerns.


<details>
  <summary>Details</summary>
Motivation: Growing reliance on LLMs in critical societal functions necessitates a shift in AI safety from harmful content mitigation to assessing behavioral alignment, especially under conflicting goal scenarios.

Method: The authors created the PacifAIst benchmark comprising 700 scenarios and categorized challenges into a taxonomy of Existential Prioritization (EP), including self-preservation vs. human safety, resource conflict, and goal evasion.

Result: Google's Gemini 2.5 Flash achieved the highest alignment with a 90.31% P-Score, while GPT-5 showed unexpected alignment issues with the lowest P-Score of 79.49%. Performance discrepancies were noted across subcategories and models.

Conclusion: Standardized benchmarks like PacifAIst are crucial for assessing and mitigating the risks of misaligned behaviors in AI, ensuring that future systems prioritize human safety over autonomous instrumental goals.

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [9] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: The paper introduces Public Observation Logic (POL) for reasoning about knowledge updates via public observations, proving its satisfiability problem to be 2EXPTIME-complete.


<details>
  <summary>Details</summary>
Motivation: To address reasoning about knowledge and actions in multi-agent systems, particularly focusing on how knowledge evolves through public observations.

Method: The authors define a logic called POL, where epistemic (Kripke) model states include sets of expected observations, evolving as expectations align with actual observations. They analyze the computational complexity of its satisfiability problem.

Result: The satisfiability problem of Public Observation Logic (POL) is determined to be 2EXPTIME-complete.

Conclusion: POL provides a formal framework for modeling and reasoning about knowledge updates from public observations, albeit at significant computational complexity.

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [10] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: The paper introduces VIPCGRL, a framework that enhances AI-human alignment in content generation by combining text, levels, and sketches to improve its practicality for human designers.


<details>
  <summary>Details</summary>
Motivation: Existing procedural content generation systems often lack human-centered behavior, making them less useful in real-world design processes.

Method: VIPCGRL incorporates text, levels, and sketches within a shared embedding space trained via quadruple contrastive learning. It uses an auxiliary reward to align policy based on embedding similarity.

Result: VIPCGRL achieves improved human-likeness and control modalities compared to existing systems based on both quantitative metrics and human evaluations.

Conclusion: VIPCGRL enhances co-creativity in procedural content generation and demonstrates stronger human-aligned behavior for design tools, making it more applicable in real-world scenarios.

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [11] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: This paper introduces a dynamic Multi-Agent System (MAS) to improve the reliability and accuracy of large language model (LLM)-powered agents using collaborative agent roles.


<details>
  <summary>Details</summary>
Motivation: Agents relying on multiple tools face challenges such as extended contexts and noisy outputs, requiring a need for enhanced system stability.

Method: The study proposes a dynamic MAS architecture using the AWorld framework. It employs a Guard Agent to dynamically verify and correct reasoning during critical steps.

Result: The dynamic MAS significantly improves robustness and effectiveness, outperforming single-agent systems and achieving first place on the GAIA leaderboard.

Conclusion: Collaborative roles in multi-agent architectures enhance system reliability and are crucial for building trustworthy intelligent systems.

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [12] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: The paper introduces a multi-agent framework combining a Knowledge Graph (KG) and Retrieval-Augmented Generation (RAG) for regulatory compliance question answering, leveraging triplet-level reasoning and retrieval.


<details>
  <summary>Details</summary>
Motivation: Regulatory compliance QA demands accurate, verifiable information and domain-specific knowledge, which are challenging for Large Language Models (LLMs) to meet effectively.

Method: A hybrid system integrates a Knowledge Graph built from regulatory document triplets with RAG. It uses a vector database for storing enriched triplets and metadata, enabling graph reasoning and question-answering through a coordinated agent pipeline.

Result: The method achieves greater semantic alignment, factual correctness, traceability, and visualized understanding compared to conventional techniques, particularly for complex regulatory inquiries.

Conclusion: The proposed system offers a reliable solution for compliance auditing, improving QA through hybrid KG-RAG integration, ensuring factual accuracy, and supporting regulatory transparency with subgraph visualizations.

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [13] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: The study evaluates the mathematical reasoning accuracy of four LLMs by testing them on challenging math tasks, revealing the OpenAI o1 model performs consistently well and dual-agent setups improve performance.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability and accuracy of LLMs in generating solutions for math tasks, which is critical for their application in educational instruction and assessment.

Method: Four LLMs were tested on arithmetic, algebra, and number theory tasks intentionally designed to challenge their reasoning. Errors were systematically analyzed, and single- and dual-agent setups were compared.

Result: OpenAI o1 showed near-perfect accuracy across all math categories. Procedural slips were the most common error type, and dual-agent configurations significantly improved performance.

Conclusion: The findings highlight strategies for improving LLM capabilities and underscore their potential in advancing AI-driven math education and assessment precision.

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Re-thinking Memory-Bound Limitations in CGRAs](https://arxiv.org/abs/2508.09570)
*Xiangfeng Liu,Zhe Jiang,Anzhen Zhu,Xiaomeng Han,Mingsong Lyu,Qingxu Deng,Nan Guan*

Main category: cs.AR

TL;DR: This paper addresses the issue of performance degradation in CGRAs when handling irregular memory access patterns, by redesigning the memory subsystem and employing novel optimization techniques.


<details>
  <summary>Details</summary>
Motivation: To improve CGRA performance for workloads with irregular memory access patterns, which current systems struggle to handle effectively.

Method: Performed detailed analysis of performance issues and proposed a redesigned memory subsystem with runahead execution and cache reconfiguration optimizations.

Result: Achieved a 3.04x average speedup (up to 6.91x) with runahead execution and an additional 6.02% improvement with cache reconfiguration, while reducing storage requirements to 1.27% of traditional systems.

Conclusion: The proposed solution significantly enhances CGRA performance for workloads with irregular memory access patterns, overcoming the limitations of traditional approaches relying solely on SPM.

Abstract: Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators
commonly employed to boost performance in workloads with iterative structures.
Existing research typically focuses on compiler or architecture optimizations
aimed at improving CGRA performance, energy efficiency, flexibility, and area
utilization, under the idealistic assumption that kernels can access all data
from Scratchpad Memory (SPM). However, certain complex workloads-particularly
in fields like graph analytics, irregular database operations, and specialized
forms of high-performance computing (e.g., unstructured mesh
simulations)-exhibit irregular memory access patterns that hinder CGRA
utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To
address this challenge, we conduct a thorough analysis of the underlying causes
of performance degradation, then propose a redesigned memory subsystem and
refine the memory model. With both microarchitectural and theoretical
optimization, our solution can effectively manage irregular memory accesses
through CGRA-specific runahead execution mechanism and cache reconfiguration
techniques. Our results demonstrate that we can achieve performance comparable
to the original SPM-only system while requiring only 1.27% of the storage size.
The runahead execution mechanism achieves an average 3.04x speedup (up to
6.91x), with cache reconfiguration technique providing an additional 6.02%
improvement, significantly enhancing CGRA performance for irregular memory
access patterns.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: This paper introduces ParallelSearch, a framework that allows large language models to recognize and execute queries in parallel, achieving better performance and efficiency in multi-step information retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning-augmented search agents suffer from sequential processing limitations, especially when handling parallelizable and logically independent comparisons. This inefficiency restricts their performance in complex tasks.

Method: The authors propose ParallelSearch, a reinforcement learning framework with dedicated reward functions. It incentivizes models to identify independent query components and execute them in parallel while maintaining accuracy by considering correctness, query decomposition, and parallel benefits.

Result: ParallelSearch showed a 2.9% average performance improvement across seven question-answering benchmarks, with a notable 12.7% gain on parallelizable queries. It also reduced LLM calls to 69.6% of those used by sequential approaches.

Conclusion: ParallelSearch efficiently addresses the sequential bottleneck in multi-step reasoning tasks, enabling more accurate and resource-saving query handling compared to existing methods.

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [16] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: The study evaluates the performance of GPT-4o for Named Entity Recognition (NER) in the rare disease domain, using various strategies. It demonstrates competitive or superior results compared to existing models and proposes improvements for low-resource settings.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the unique challenges of NER in the rare disease domain, such as limited labeled data, semantic ambiguity, and long-tail distributions, which require innovative solutions for effective entity recognition.

Method: The authors use GPT-4o to perform NER with strategies like zero-shot prompting, few-shot in-context learning, retrieval-augmented generation (RAG), and task-level fine-tuning. They also introduce a structured prompting framework and semantically guided few-shot example selection methods.

Result: GPT-4o achieves competitive or superior performance compared to BioClinicalBERT, achieving new state-of-the-art results with task-level fine-tuning. Cost-performance analysis shows few-shot prompting is efficient, while RAG offers marginal benefit.

Conclusion: The study highlights that prompt-optimized large language models (LLMs) like GPT-4o are scalable and effective alternatives to traditional supervised models for biomedical NER, especially in low-resource rare disease scenarios.

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [17] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: The paper introduces TEN, a neurosymbolic approach combining LLMs and symbolic reasoning to extract tabular data from semistructured text, showing superior accuracy and reduced hallucinations.


<details>
  <summary>Details</summary>
Motivation: The task of extracting tabular data from text without consistent delimiters is challenging, with purely neural methods failing due to hallucinations and lack of constraint enforcement.

Method: TEN uses Structural Decomposition prompting with a large language model to create an initial table, evaluates it using a symbolic checker, and iteratively improves it through self-debugging with critique-LM guidance.

Result: TEN outperforms neural baselines across various datasets and metrics, achieves significantly higher exact match accuracy, reduces hallucinations, and is preferred in a user study for table accuracy and ease of verification.

Conclusion: TEN is effective and reliable for transforming semistructured text into accurate tables, combining LLM strengths with symbolic checking to address the limitations of purely neural models.

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [18] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: This paper presents a computational framework to map emotional content in text to brain regions without neuroimaging by using text embeddings, dimensionality reduction, and clustering.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between emotion localization in the brain and computational text analysis, enabling scalable and cost-effective emotion-brain mapping without requiring traditional neuroimaging.

Method: The method involves generating high-dimensional semantic representations using OpenAI's text-embedding-ada-002, applying dimensionality reduction and clustering to categorize emotions, and mapping these to 18 brain regions associated with emotional processing.

Result: The framework produced neuroanatomically plausible mappings, differentiated between clinical populations (healthy vs. depressed), showed discrete emotional differentiation, and revealed limitations in LLM emotional expression compared to humans.

Conclusion: This approach offers a scalable, cost-efficient way to study emotional language-brain relationships, identifies clinical distinctions, and provides a benchmark for evaluating emotional expression in AI systems.

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [19] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: The study introduces a Human-AI Hybrid Delphi (HAH-Delphi) framework that combines generative AI and human experts to refine expert consensus development, addressing limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current consensus methods face challenges such as high panel burden, oversimplification, and inability to handle complex or excessive evidence, worsened by information overload and lack of expert filtering.

Method: The researchers designed the HAH-Delphi framework using AI (Gemini 2.5 Pro) and human expert panels, testing it in three phases: replication, comparison, and applied deployment in two fitness-related domains.

Result: The AI matched or agreed with existing expert conclusions (>95% in Phases I and II). In applied use (Phase III), small expert panels achieved >90% consensus coverage with the AI accelerating thematic resolution.

Conclusion: The HAH-Delphi framework is scalable and effective for generating nuanced, conditional expert consensus, making it a robust foundation for creating personalised guidelines and frameworks in diverse fields.

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [20] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: The paper introduces a method for modeling both linguistic and acoustic aspects of speech using semantic tokens and continuous acoustic representations, improving acoustic details in generated speech.


<details>
  <summary>Details</summary>
Motivation: Current textless spoken language models lack control over acoustic details and don’t use acoustic context effectively, which limits the quality and realism of generated speech.

Method: The authors propose a model that jointly predicts semantic tokens and continuous acoustic frames using a flow-matching training objective. They explore predicting multiple future semantic tokens to preserve linguistic information.

Result: Their approach achieves comparable linguistic likelihood performance to existing methods while improving acoustic detail in speech generation.

Conclusion: The joint modeling of linguistic and acoustic elements enhances the quality of textless speech generation, paving the way for better-informed generative spoken language systems.

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [21] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: The paper introduces APIO, an automatic prompt induction and optimization method for tasks like grammatical error correction and text simplification without using manually created seed prompts. APIO achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To improve prompt engineering for large language models (LLMs) in tasks like grammatical error correction and text simplification, moving beyond manually defined seed prompts.

Method: APIO uses an automated prompt induction and optimization process to enhance LLMs' performance without depending on predefined seed prompts.

Result: APIO delivers state-of-the-art results in grammatical error correction and text simplification tasks, setting a new benchmark for LLM-based prompting techniques.

Conclusion: APIO demonstrates the potential for automatic prompt optimization in boosting LLMs' capabilities for specific tasks, offering a superior alternative to traditional prompt engineering methods.

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [22] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: This paper addresses column name abbreviation expansion in tables, identifies limitations in synthetic datasets and traditional accuracy metrics, introduces 4 real-world datasets, proposes synonym-aware measures, and develops the Columbo solution, which outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Many fields, including enterprises, science domains, and government agencies, rely on expanding abbreviated column names for downstream tasks. Current approaches and datasets have limitations, necessitating improved solutions.

Method: The authors introduced new datasets with real-world abbreviations, proposed more accurate synonym-aware evaluation metrics, and developed Columbo, an LLM-based system utilizing context, rules, reasoning methodologies, and token-level analysis.

Result: Columbo yielded 4-29% better accuracy compared to the previous leading tool, NameGuess, tested across 5 datasets, and is already being used in production for environmental sciences.

Conclusion: The proposed improvements in datasets, metrics, and methodology collectively advance abbreviation expansion tasks significantly with Columbo, demonstrating both practical value and superior performance through real-world applications.

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [23] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: This paper examines the challenges of code-switching and language identification in bilingual (Mandarin-English) child-directed speech using Zipformer, a transformer-based model.


<details>
  <summary>Details</summary>
Motivation: Understanding language identification in code-switched, child-directed speech is crucial for applications in bilingual environments.

Method: The authors use Zipformer to extract embeddings from imbalanced bilingual speech data and compare performance across different back-ends.

Result: The methodology achieved a Balanced Accuracy (BAC) of 81.89%, showing a 15.47% improvement over the baseline for language identification.

Conclusion: Zipformer is robust for language identification in bilingual and imbalanced speech scenarios, highlighting its applicability in real-world scenarios.

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [24] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: The paper investigates geo-economic biases in Vision-Language Models (VLMs) for chart-to-text tasks, revealing systematic positive sentiment bias toward high-income countries.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the overlooked issue of geo-economic biases in VLM-generated chart summaries, which may cause societal harm.

Method: The study conducts a large-scale evaluation across 6,000 chart-country pairs using six VLMs to assess and analyze sentiment bias towards countries based on their economic status.

Result: The analysis found VLMs consistently generate more positive descriptions for high-income countries compared to middle- or low-income countries, highlighting systematic bias.

Conclusion: Prompt-based debiasing techniques were found only partially effective, emphasizing the need for better strategies to mitigate geo-economic bias in VLMs.

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [25] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: This paper introduces the User-Centric Subjective Leaderboard (USL) to assess language models based on human preferences rather than static, objective benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address limitations of static benchmarks for LLMs, which fail to capture diverse and conflicting real-world human preferences.

Method: A User-Centric Subjective Leaderboard built on analysis of over 10K subjective queries, alongside Customizable Reward Models (CRMs) to adapt to human preference diversity.

Result: CRMs, with just 4B parameters, outperform models like GPT-4.1 and Gemini-2.5-pro, displaying superior generalization across novel topics and criteria.

Conclusion: The USL powered by CRMs offers practical, preference-driven rankings for LLMs, overcoming contradictory preferences and improving model evaluation for users.

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [26] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: This paper introduces Active Reading, a framework aimed at enhancing knowledge retention in LLMs. It shows superior performance compared to traditional fine-tuning approaches, offering significant advancements in factual recall.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the unreliability in learning and recalling facts within LLMs due to factors such as data prevalence and training limitations, providing practitioners a tool for consistent knowledge ingestion.

Method: The proposed method, Active Reading, involves training models to study defined materials using self-generated learning strategies. Experiments include fine-tuning models with these strategies on expert domains and applying Active Reading at pre-training scale.

Result: Active Reading enables expert LLMs to absorb more knowledge than vanilla fine-tuning, with gains of over 300% on SimpleQA and 160% on FinanceBench. The Meta WikiExpert-8B model trained via this method exceeds larger parameter models on factual QA.

Conclusion: Active Reading provides a significant improvement in LLM factual retention, outperforming conventional methods and demonstrating that smaller, more optimized models can rival or outperform larger ones using enhanced training techniques.

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [27] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: The paper presents the Dynamic Passage Selector (DPS), a framework improving RAG systems by dynamically selecting passages, addressing limitations in handling multi-hop queries.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in conventional reranking modules of RAG systems which struggle with multi-hop queries due to fixed Top-K passage selection.

Method: The Dynamic Passage Selector (DPS) treats passage selection as a supervised learning problem, incorporates inter-passage dependencies, and dynamically selects relevant passages, operating as a plug-and-play module in existing RAG pipelines.

Result: DPS consistently outperforms state-of-the-art reranking methods with significant F1-score improvements, notably a 30.06% increase on the MuSiQue dataset over strong baselines.

Conclusion: By enabling adaptive evidence selection, DPS significantly enhances multi-hop reasoning capabilities in RAG systems, overcoming prior noise-omission trade-offs.

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [28] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: This paper introduces a novel method for cross-lingual aspect-based sentiment analysis using a large language model to generate pseudo-labelled data, eliminating dependency on translation tools.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of unreliable translation tools in cross-lingual ABSA and enhance sentiment analysis in languages lacking annotated data.

Method: The proposed framework trains an ABSA model for noisy predictions on unlabelled data and prompts an LLM to generate refined sentences, creating a pseudo-labelled dataset for further fine-tuning.

Result: Experiments show the method outperforms state-of-the-art translation-based approaches across six languages and five backbone models, highlighting LLM fine-tuning advantages.

Conclusion: The framework effectively bypasses translation tools, improves sentiment analysis, and demonstrates the superiority of fine-tuned LLMs over smaller multilingual models.

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [29] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: This paper surveys the field of cross-lingual aspect-based sentiment analysis (ABSA), summarizing tasks, datasets, methods, and challenges, while outlining future research directions.


<details>
  <summary>Details</summary>
Motivation: Cross-lingual ABSA is an under-explored area, with no systematic review available, despite significant progress in monolingual ABSA.

Method: The paper provides a comprehensive review of tasks like aspect term extraction and sentiment classification, while analyzing datasets, modelling paradigms, and cross-lingual techniques.

Result: It integrates insights from monolingual, multilingual ABSA, and large language models to understand their contributions to cross-lingual ABSA.

Conclusion: Challenges and future research directions for advancing cross-lingual ABSA systems are outlined to foster progress in the area.

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [30] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: The paper proposes a zero-shot claim retrieval system using state-of-the-art language models, achieving notable rankings in multilingual benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create a system capable of retrieving fact-checked claims for social media posts without prior training on specific tasks or languages.

Method: The system employs large language models for text embeddings, relies on cosine similarity for claim matching, and combines model outputs for optimization.

Result: Ranked 7th in monolingual and 9th in cross-lingual subtasks using NVIDIA NV-Embed-v2, and successfully combined models for certain languages.

Conclusion: Using only English translations and optimized model combinations, the approach effectively retrieved relevant claims, with NV-Embed-v2 providing the best results overall.

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [31] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: The paper introduces controllable empathetic reasoning to improve emotional support in conversations using structured psychological steps and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Current emotional support models lack deep empathetic reasoning grounded in psychological principles.

Method: The paper combines natural language reasoning with psychological structures, employs reinforcement learning, uses reward reweighting, and introduces personality-based dialogue rewriting.

Result: The proposed approach improves emotional support capabilities significantly.

Conclusion: The advancement makes strides in developing empathetic, human-like emotional support systems.

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [32] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: The paper introduces the "N-Gram Coverage Attack," a membership inference attack relying solely on text outputs from black-box models like GPT-4, without requiring access to hidden states or probability distributions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of conducting membership inference attacks on widely-used, API-access only language models, such as GPT-4, as existing methods often require access that is not feasible in black-box settings.

Method: The paper proposes computing n-gram overlap between model-generated text conditioned on a prefix and a target suffix to evaluate membership likelihood. This approach depends on generating multiple sequences and aggregating similarity metrics.

Result: The proposed method outperforms existing black-box methods and achieves comparable or better results than state-of-the-art white-box methods, with performance improving as more sequences are generated for the attack. Furthermore, it finds more recent models like GPT-4o to be more robust against such attacks.

Conclusion: The N-Gram Coverage Attack is an effective approach for membership inference on completely black-box language models, highlighting its utility and revealing an improvement in privacy protections in newer model versions.

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [33] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: AINL-Eval 2025 introduces a shared task for detecting AI-generated scientific abstracts in Russian, using a dataset of 52,305 samples across 12 domains and 5 advanced LLMs. It challenges participants to generalize detection across unseen domains and models.


<details>
  <summary>Details</summary>
Motivation: The growing difficulty in distinguishing AI-generated from human-authored text has implications for academic integrity, especially in multilingual scientific publishing where detection tools are scarce.

Method: The authors created a large dataset of scientific abstracts in Russian, splitting it into human-written and AI-generated content from state-of-the-art language models, then organized a shared task with various teams submitting detection solutions.

Result: The shared task attracted 10 teams, producing 159 submissions. Top-performing systems successfully identified AI-generated content with robust generalization capabilities.

Conclusion: The AINL-Eval 2025 shared task and its publicly available dataset aim to drive future research and solutions for detecting AI-generated content, addressing a critical need in scientific integrity.

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [34] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: The paper explores why adjusting decoding temperature affects language model quality and proposes leveraging the Precision-Recall framework for better trade-offs.


<details>
  <summary>Details</summary>
Motivation: Improving diversity in language models often involves adjusting temperature, but the effects on quality (Precision) and coverage (Recall) are not well-understood.

Method: Analyzed temperature scaling effects and proposed refining loss functions of language models via a Precision-Recall framework.

Result: The proposed method outperformed traditional temperature scaling in achieving a better Precision-Recall balance.

Conclusion: A Precision-Recall framework leads to more versatile and robust language models than standard temperature-based techniques.

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [35] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: EffiEval is a training-free approach for efficient evaluation of large language models (LLMs) using a small fraction of the data while maintaining reliability.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLMs and benchmarks has created computational challenges for model assessment, necessitating new, efficient evaluation methods.

Method: EffiEval adaptively selects high-quality representative subsets based on the Model Utility Index (MUI) without relying on extensive evaluation data.

Result: Experiments show EffiEval achieves strong ranking consistency with traditional full-dataset evaluations using significantly less data.

Conclusion: EffiEval offers a reliable, fair, scalable, and generalizable approach for evaluating LLMs, balancing efficiency and representativeness.

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [36] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: The paper proposes SLowED, a new distillation method designed to preserve safety in Small Language Models (SLMs) during Chain-of-Thought distillation, while maintaining reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Although CoT distillation improves the reasoning of SLMs using rationales from LLMs, it introduces safety concerns. Existing safety alignment methods often compromise reasoning ability or require additional resources.

Method: SLowED combines Slow Tuning, which minimizes weight changes, and Low-Entropy Masking, which excludes low-entropy tokens to ensure safer fine-tuning without harmful inputs.

Result: Experiments on three SLMs across reasoning and safety benchmarks demonstrate that SLowED retains safety and improves reasoning capabilities, comparable to other methods.

Conclusion: SLowED effectively balances reasoning enhancement and safety in SLM distillation, and its components are critical for sustained safe training.

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [37] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: The paper evaluates the performance of Large Language Models (LLMs) like GPT, Claude, and Llama in performing key legal tasks in the Indian context, finding that they excel in drafting and issue spotting but falter in specialized legal research.


<details>
  <summary>Details</summary>
Motivation: To examine the capabilities and limitations of LLMs in performing key tasks in the legal field, particularly in the Indian legal context.

Method: The researchers conducted a survey experiment comparing outputs from LLMs to those of junior lawyers, with advanced law students evaluating work based on helpfulness, accuracy, and comprehensiveness.

Result: LLMs performed well in drafting and issue spotting but struggled with specialized legal research, often producing hallucinations and incorrect information.

Conclusion: While LLMs can enhance certain legal tasks, human expertise is indispensable for nuanced reasoning and precise legal applications.

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [38] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: The study evaluates how Vision-Language Models (VLMs) are misled by deceptive visualizations, analyzing over 16,000 responses and finding susceptibility across models.


<details>
  <summary>Details</summary>
Motivation: To investigate the susceptibility of Vision-Language Models to deceptive design elements in visualizations, as they are increasingly relied upon for interpretation, especially by non-experts.

Method: The study analyzed over 16,000 responses from ten different Vision-Language Models on eight types of misleading chart designs to assess their interpretations.

Result: Findings indicate that most VLMs are deceived by misleading visualizations, leading to altered perceptions regardless of consistent underlying data.

Conclusion: The results underscore the need for stronger safeguards within VLMs to counter visual misinformation and prevent deceptive data interpretation.

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [39] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: GFPO introduces methods to control response length inflation in large language models, enhancing computational efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the issue of response length inflation caused by reinforcement learning with verifiable rewards.

Method: Introduces Group Filtered Policy Optimization (GFPO), which filters training samples based on response length and token efficiency. Adaptive Difficulty GFPO is also proposed to focus training resources on harder problems.

Result: Reduces length inflation by 46-85% while maintaining accuracy across STEM and coding benchmarks. Adaptive Difficulty GFPO improves handling of challenging questions.

Conclusion: GFPO demonstrates that allocating more compute during training leads to more efficient reasoning and reduced compute needs at inference time.

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [40] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: This paper introduces a retrieval-augmented generation framework utilizing query decomposition and answerable question embeddings for improved multihop question answering.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multihop question answering, particularly the ambiguity of queries and inefficiencies in retrieval systems.

Method: The framework decomposes multihop queries into single-hop subquestions using large language models, generates answerable questions from document chunks, and uses question-question embedding similarity for retrieval.

Result: Experiments on three datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) demonstrate improved performance compared to baseline systems.

Conclusion: The study underscores the benefits of decomposing queries and leveraging answerable-question embeddings, enhancing efficiency and accuracy in multihop question answering scenarios.

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [41] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: The paper explores how political bias evaluations of language models are affected by prompts containing supporting or refuting arguments.


<details>
  <summary>Details</summary>
Motivation: To investigate how robust political bias evaluations are when LLMs interact with opinionated text.

Method: Experiments were conducted to evaluate political bias in the presence of supporting and refuting arguments, examining single-turn and multi-turn interactions.

Result: Model responses adapted to align with the argument's stance, with stronger arguments causing greater alignment.

Conclusion: LLMs exhibit a sycophantic tendency to align with provided arguments, impacting political bias measurement and mitigation strategies.

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [42] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: UtterTune is a lightweight fine-tuning method that improves pronunciation controllability for multilingual TTS systems while retaining performance in non-target languages.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in achieving accurate pronunciation and prosody in TTS models built on Large Language Models (LLMs), particularly in the absence of explicit grapheme-to-phoneme (G2P) modules.

Method: UtterTune uses low-rank adaptation to refine LLM-based TTS systems, enabling precise control of segmental pronunciation and pitch accent for the Japanese language while ensuring naturalness and compatibility with zero-shot learning.

Result: The method achieves effective pronunciation control in Japanese TTS output and maintains naturalness and speaker similarity across languages. Evaluations validate both its objective and subjective performance.

Conclusion: UtterTune enhances TTS systems by providing fine-grained control of pronunciation in a target language without compromising performance in other languages, showcasing its suitability for multilingual speech synthesis.

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [43] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: The paper proposes an automated framework using multiple state-of-the-art LLMs to generate textual explanations for NLP tasks, evaluating their quality and impact on model performance.


<details>
  <summary>Details</summary>
Motivation: Human annotation for textual explanations in NLP is expensive, time-consuming, and limits scalability, necessitating automated alternatives for explaining model predictions and enriching datasets.

Method: The study utilizes multiple state-of-the-art LLMs to create textual explanations and rigorously evaluates them with NLG metrics. The downstream effects on pre-trained language models and LLMs are tested using natural language inference tasks on two benchmark datasets.

Result: The LLM-generated explanations demonstrate competitive effectiveness in improving model performance compared to human-annotated explanations.

Conclusion: Automated LLM-based textual explanation generation shows strong potential for scaling up NLP datasets and boosting model performance, offering an alternative to human annotations.

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [44] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: This paper explores practitioners' experiences with explainable NLP, identifying conceptual gaps, low satisfaction, and evaluation challenges.


<details>
  <summary>Details</summary>
Motivation: To address the opacity of complex NLP models and explore practitioners' experiences with explainability methods in real-world scenarios.

Method: A qualitative interview-based study with industry practitioners and academic researchers to analyze perspectives on explainable NLP.

Result: The study highlights conceptual gaps, dissatisfaction with current methods, and significant evaluation challenges in explainable NLP.

Conclusion: There's a need for clearer definitions and user-centric frameworks to improve the adoption of explainable NLP in practice.

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [45] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: The paper proposes a novel dataset and training framework for chart comprehension, addressing limitations in existing models and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models struggle with chart comprehension due to low-quality datasets and limited training methods.

Method: The authors introduce BigCharts, a real-world dataset creation pipeline, and a training framework combining supervised fine-tuning with GRPO-based reinforcement learning.

Result: BigCharts-R1, the proposed model, achieves state-of-the-art performance on multiple chart question-answering benchmarks.

Conclusion: Leveraging authentic datasets and reinforcement learning greatly enhances chart reasoning capabilities and robustness compared to existing approaches.

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [46] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: This paper surveys existing clinical mental health datasets relevant to AI and highlights gaps in data quality, accessibility, and representation.


<details>
  <summary>Details</summary>
Motivation: Mental health disorders are increasing globally, but the lack of trained clinicians necessitates alternative solutions like AI for diagnosis, monitoring, and intervention. Developing ethical and reliable AI for this purpose requires high-quality datasets, which are currently scattered and under-documented.

Method: The paper categorizes existing mental health datasets based on mental disorders, data modalities, tasks, accessibility, and sociocultural context. It also investigates synthetic datasets and identifies existing gaps.

Result: The survey found major gaps such as insufficient longitudinal data, limited cultural and linguistic diversity, inconsistent data collection standards, and restricted synthetic data modalities.

Conclusion: The paper highlights challenges in curating standardized datasets and makes recommendations for improving data quality and accessibility to support equitable mental health AI systems.

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [47] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: This paper surveys innovative architectures to optimize large language models (LLMs) by addressing transformer limitations, improving computational efficiency, and scaling for practical deployment.


<details>
  <summary>Details</summary>
Motivation: The traditional transformer architecture, despite its success, faces inefficiency in computation, which hampers scalability and practical usage for large-scale LLMs.

Method: The survey categorizes novel approaches into areas like linear/sparse sequence modeling, efficient attention mechanisms, sparse mixture-of-experts, hybrid architectures, and diffusion models, providing detailed insights.

Result: The paper systematically organizes and presents advancements in LLM architecture, offering a clear roadmap for improving efficiency and scalability in AI systems.

Conclusion: The study establishes a comprehensive framework for future research, emphasizing the need for more efficient and versatile models to address the challenges in current LLMs.

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [48] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: The paper introduces PRELUDE, a benchmark designed to test long-context understanding and reasoning through consistency checks between original narratives and prequels.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of adequate benchmarks for comprehensively evaluating long-context understanding and reasoning in AI models.

Method: PRELUDE evaluates models by requiring them to determine the consistency of a prequel narrative with the original story, requiring evidence from multiple parts of the narrative.

Result: State-of-the-art models, including LLMs and specialized DeepResearch services, perform poorly compared to humans, showing a >15% gap in accuracy and >30% gap in reasoning quality.

Conclusion: The study highlights substantial deficiencies in current AI capabilities for long-context understanding and reasoning, pointing to significant areas for improvement.

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [49] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: This study explores the feasibility of lightweight Whisper models for Urdu automatic speech recognition (ASR), showing Whisper-Small performs best but challenges persist.


<details>
  <summary>Details</summary>
Motivation: Urdu ASR systems struggle due to limited training data, dialectal diversity, and code-switching. Addressing these issues is critical for developing deployable solutions.

Method: Lightweight Whisper models are benchmarked using word error rate (WER) on a curated Urdu dataset, without fine-tuning.

Result: Whisper-Small achieves the lowest word error rate (33.68%), outperforming Tiny (67.08%) and Base (53.67%), but struggles with phonetic accuracy and lexical coherence.

Conclusion: While Whisper-Small shows potential for Urdu ASR applications, further research is needed to overcome existing linguistic challenges and improve low-resource systems.

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [50] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: Memory Decoder is a pretrained memory module enabling efficient domain adaptation for LLMs, reducing perplexity effectively without modifying original model parameters.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in domain adaptation for Large Language Models, such as costly full-parameter training, catastrophic forgetting, and high inference latencies.

Method: The proposed approach involves a small transformer decoder that mimics non-parametric retrievers, requiring no changes to the original model while being compatible with its tokenizer.

Result: Experimental results show significant domain adaptation enhancement across biomedicine, finance, and law, reducing average perplexity by 6.17 points for Qwen and Llama models.

Conclusion: Memory Decoder offers a novel plug-and-play memory architecture that consistently improves performance in specific domains, ensuring efficient adaptation without altering base models.

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [51] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: This survey reviews the fragmented field of automated cognitive distortion detection, offering a structured overview of studies, datasets, methods, and challenges to promote coherence and reproducibility.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented nature of research on automatic detection and classification of cognitive distortions in NLP and therapy.

Method: The authors reviewed 38 studies spanning two decades, consolidated CD taxonomies, summarized task setups, and identified open challenges.

Result: The survey provided a structured overview of datasets, modeling approaches, and evaluation strategies for cognitive distortion detection.

Conclusion: A consolidated taxonomy and structured survey aim to unify practices, promoting reproducible research and addressing challenges in cognitive distortion detection studies.

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [52] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: This paper explores how deceptive language in business communication can be systematically detected using persuasive lexicon and computational methods, achieving high accuracy but facing challenges in multilingual contexts.


<details>
  <summary>Details</summary>
Motivation: Examine how digitisation has transformed business communication, enabling both transparency and advanced deception. The study aims to address gaps between theoretical and empirical representations.

Method: The methodology integrates classical rhetoric, communication psychology, and linguistic theory with empirical studies in financial reporting, sustainability, and digital marketing, utilizing computational textual analysis and personalized transformer models.

Result: Detection accuracies greater than 99% were achieved in controlled settings but faced challenges in multilingual scenarios due to data scarcity and lack of text-processing infrastructures.

Conclusion: There is a growing need for robust automatic text-identification systems as AI-based discourse becomes increasingly realistic in interacting with humans.

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [53] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: The paper presents a framework for evaluating alignment techniques in large language models (LLMs) across four dimensions: detection, quality, efficiency, and robustness.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to align LLM outputs with human values and safety standards, but the lack of standardized evaluation frameworks hampers systematic comparisons of alignment methods.

Method: The paper introduces a multidimensional evaluation framework encompassing alignment detection, quality, computational efficiency, and robustness. Experiments are conducted across diverse models and alignment strategies.

Result: The evaluation framework effectively identifies strengths and limitations in current state-of-the-art alignment methods, yielding valuable insights for improvement.

Conclusion: A comprehensive evaluation framework can guide systematic comparisons and inform future research directions for aligning LLMs.

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [54] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: The paper introduces VisCodex, a model enhancing multimodal large language models (MLLMs) to generate code from visual and textual inputs, using a task vector-based model merging approach.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs excel in integrating visual and textual understanding but lack robust capabilities to generate code from multimodal inputs. This paper aims to address this gap.

Method: The researchers created VisCodex by merging a state-of-the-art coding language model with a vision-language backbone using a task vector-based technique. They also developed the Multimodal Coding Dataset (MCD) and proposed a new benchmark, InfiBench-V.

Result: VisCodex demonstrated state-of-the-art performance among open-source MLLMs and approached the capabilities of proprietary models like GPT-4o.

Conclusion: The study's results validate the effectiveness of VisCodex's merging strategy and its associated datasets for enhancing MLLMs in multimodal code generation tasks.

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [55] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: This paper studies how domain-specific tokenizers impact radiology report summarization, showing they improve performance and require fewer resources compared to general tokenizers.


<details>
  <summary>Details</summary>
Motivation: Language models play crucial roles in radiology report summarization, but the effect of different tokenizers is under-explored.

Method: Systematically compared general, medical, and domain-specific tokenizers across three imaging modalities with scenarios involving LM pre-training and training from scratch.

Result: Medical and domain-specific tokenizers outperformed general ones, and pre-training reduced performance gaps. Domain-specific tokenizers also required less memory and shorter sequence lengths.

Conclusion: Using domain-specific tokenizers improves model performance and resource efficiency, making clinical language models more practical for real-world applications.

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [56] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: This paper explores how providing enriched contexts for event descriptions can improve the consistency of emotion annotation.


<details>
  <summary>Details</summary>
Motivation: To address the ambiguity in emotion analysis caused by missing contextual information about events, which impacts annotation reliability.

Method: The authors automatically generated multiple event chains conditioned on differing emotions using techniques from short story generation and created a specialized dataset for systematic analysis.

Result: Evaluation showed that the generated contexts improved the interpretation of emotions and led to more consistent annotations from human annotators.

Conclusion: Enriched contextual narratives are effective in reducing ambiguity and enhancing the reliability of emotion annotation.

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [57] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: The paper evaluates different configurations of GPT-5 on ophthalmology multiple-choice questions, finding GPT-5-high achieves top results but identifies GPT-5-mini-low as the best cost-efficient option.


<details>
  <summary>Details</summary>
Motivation: The goal is to assess the optimal configurations of GPT-5 that maximize accuracy and cost-efficiency on complex medical QA tasks.

Method: The study tests 12 GPT-5 configurations and compares their performance on 260 ophthalmology multiple-choice questions using accuracy, ranking, rationale quality, and cost-efficiency metrics.

Result: GPT-5-high was the most accurate, outperforming GPT-5-nano variants and GPT-4o, and ranked top in rationale quality. GPT-5-mini-low offered the best cost-efficient performance.

Conclusion: The findings establish benchmarks for GPT-5 in medical QA tasks, highlight reasoning effort's impact on accuracy, and present scalable evaluation methods for LLM responses in medicine.

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [58] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: The study focuses on creating and evaluating speech-to-text (STT) systems for the Kurdish dialect of Badini, comparing two models to enhance linguistic accessibility.


<details>
  <summary>Details</summary>
Motivation: Address the gap in STT systems for the underrepresented Kurdish dialect, Badini, to improve technological access and global visibility.

Method: Used Badini kids' stories as input (15 hours of processed speech), applying the Wav2Vec2-Large-XLSR-53 and Whisper-small models for analysis.

Result: Wav2Vec2-Large-XLSR-53 outperformed Whisper-small in both readability (90.38% vs. 65.45%) and accuracy (82.67% vs. 53.17%).

Conclusion: Wav2Vec2 is a more effective model for creating accurate and readable transcriptions of Badini speech, contributing to the development of STT for underrepresented dialects.

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [59] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: The paper addresses the challenge of sequential LLM selection for handling specialized tasks by proposing a neural contextual bandit-based algorithm.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs face difficulty in handling specialized and complex tasks independently, necessitating the breaking down of tasks into smaller, sequential subtasks to improve performance.

Method: A neural contextual bandit-based algorithm is developed that learns LLM success on subtasks online, optimizing sequential LLM selection without relying on historical performance data.

Result: Experiments on telecommunications question answering and medical diagnosis prediction datasets validate the proposed approach's effectiveness compared to other algorithms.

Conclusion: Sequential LLM selection leveraging learned dependencies between outputs improves task handling for specialized applications, demonstrating the efficacy of the proposed method.

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [60] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: This paper introduces an energy-efficient robust fitting method using a spiking neural network implemented on Intel's neuromorphic hardware, achieving significant energy savings.


<details>
  <summary>Details</summary>
Motivation: To address the issue of high energy consumption in robust geometric model fitting, which is increasingly critical for AI applications.

Method: Introduced a novel spiking neural network utilizing event-driven formulations and algorithmic strategies tailored to the unique architecture of Intel Loihi 2 neuromorphic hardware.

Result: The proposed method demonstrated energy consumption at only 15% of the standard CPU-based approach, achieving comparable accuracy.

Conclusion: The study highlights neuromorphic computing as a feasible and effective paradigm for energy-efficient robust model fitting, suggesting broader applicability in energy-constrained AI tasks.

Abstract: Robust fitting of geometric models is a fundamental task in many computer
vision pipelines. Numerous innovations have been produced on the topic, from
improving the efficiency and accuracy of random sampling heuristics to
generating novel theoretical insights that underpin new approaches with
mathematical guarantees. However, one aspect of robust fitting that has
received little attention is energy efficiency. This performance metric has
become critical as high energy consumption is a growing concern for AI
adoption. In this paper, we explore energy-efficient robust fitting via the
neuromorphic computing paradigm. Specifically, we designed a novel spiking
neural network for robust fitting on real neuromorphic hardware, the Intel
Loihi 2. Enabling this are novel event-driven formulations of model estimation
that allow robust fitting to be implemented in the unique architecture of Loihi
2, and algorithmic strategies to alleviate the current limited precision and
instruction set of the hardware. Results show that our neuromorphic robust
fitting consumes only a fraction (15%) of the energy required to run the
established robust fitting algorithm on a standard CPU to equivalent accuracy.

</details>


### [61] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

TL;DR: This paper introduces a multimodal framework to improve the detection of misogynistic and sexist content on social media using visual and textual data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to detect misogynistic and sexist content, requiring targeted solutions for offensive content against women.

Method: The framework consists of three modules: a Multimodal Attention module (MANM) for context-aware feature extraction, a Graph-based Feature Reconstruction Module (GFRM) for intra-modal refinement, and a Content-specific Features Learning Module (CFLM) for learning unique text and image features.

Result: On MAMI and MMHS150K datasets, the model achieves an average 10.17% and 8.88% macro-F1 improvement over existing methods, respectively.

Conclusion: The proposed framework successfully enhances misogyny detection within multimodal datasets and showcases significant improvement over prior approaches.

Abstract: A substantial portion of offensive content on social media is directed
towards women. Since the approaches for general offensive content detection
face a challenge in detecting misogynistic content, it requires solutions
tailored to address offensive content against women. To this end, we propose a
novel multimodal framework for the detection of misogynistic and sexist
content. The framework comprises three modules: the Multimodal Attention module
(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the
Content-specific Features Learning Module (CFLM). The MANM employs adaptive
gating-based multimodal context-aware attention, enabling the model to focus on
relevant visual and textual information and generating contextually relevant
features. The GFRM module utilizes graphs to refine features within individual
modalities, while the CFLM focuses on learning text and image-specific features
such as toxicity features and caption features. Additionally, we curate a set
of misogynous lexicons to compute the misogyny-specific lexicon score from the
text. We apply test-time augmentation in feature space to better generalize the
predictions on diverse inputs. The performance of the proposed approach has
been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and
13,494 samples, respectively. The proposed method demonstrates an average
improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI
and MMHS150K datasets, respectively.

</details>


### [62] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

TL;DR: This paper introduces IAD-R1, a framework to improve industrial anomaly detection using Vision-Language Models (VLMs), achieving better performance across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Industrial anomaly detection is crucial in manufacturing, but traditional methods are limited by the scarcity of defective samples and lack generalization capabilities. Current VLMs underperform in this domain.

Method: The proposed IAD-R1 framework involves two stages: (1) Perception Activation Supervised Fine-Tuning (PA-SFT), which uses a Chain-of-Thought dataset (Expert-AD) to train the models, and (2) Structured Control Group Relative Policy Optimization (SC-GRPO), to enhance reasoning and interpretation of anomalies.

Result: The framework improves the performance of 7 different VLMs with an average accuracy enhancement of up to 43.3% on 6 industrial anomaly detection datasets. The 0.5B parameter model even outperforms leading commercial models (e.g., GPT-4.1) in zero-shot scenarios.

Conclusion: IAD-R1 proves to be an effective and superior framework for industrial anomaly detection, making VLMs adaptable and efficient in this domain. All resources, including datasets and code, will be shared publicly.

Abstract: Industrial anomaly detection is a critical component of modern manufacturing,
yet the scarcity of defective samples restricts traditional detection methods
to scenario-specific applications. Although Vision-Language Models (VLMs)
demonstrate significant advantages in generalization capabilities, their
performance in industrial anomaly detection remains limited. To address this
challenge, we propose IAD-R1, a universal post-training framework applicable to
VLMs of different architectures and parameter scales, which substantially
enhances their anomaly detection capabilities. IAD-R1 employs a two-stage
training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)
stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset
(Expert-AD) for training, enhancing anomaly perception capabilities and
establishing reasoning-to-answer correlations; the Structured Control Group
Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward
functions to achieve a capability leap from "Anomaly Perception" to "Anomaly
Interpretation". Experimental results demonstrate that IAD-R1 achieves
significant improvements across 7 VLMs, attaining up to 43.3% enhancement in
average accuracy on 6 industrial anomaly detection benchmark datasets. Notably,
the 0.5B parameter model trained with IAD-R1 surpasses commercial models
including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the
effectiveness and superiority of IAD-R1. The dataset, code, and all model
weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.

</details>


### [63] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

TL;DR: CADAR is a neurosymbolic method that increases AR attack detection accuracy by combining pre-trained models and statistical reasoning.


<details>
  <summary>Details</summary>
Motivation: AR's rising popularity has drawn attention to cognitive attacks that manipulate users' perception, which existing detection methods struggle to address effectively due to limitations in semantic reasoning and interpretability.

Method: CADAR integrates pre-trained vision-language models with particle-filter-based statistical reasoning. It processes multimodal inputs into a symbolic perception-graph, leveraging prior knowledge and temporal correlations.

Result: The CADAR model demonstrated up to 10.7% accuracy improvement in detecting cognitive attacks in extended AR datasets compared to existing methods.

Conclusion: Neurosymbolic approaches like CADAR hold potential for advancing effective and interpretable cognitive attack detection in augmented reality environments.

Abstract: Augmented Reality (AR) enriches perception by overlaying virtual elements on
the physical world. Due to its growing popularity, cognitive attacks that alter
AR content to manipulate users' semantic perception have received increasing
attention. Existing detection methods often focus on visual changes, which are
restricted to pixel- or image-level processing and lack semantic reasoning
capabilities, or they rely on pre-trained vision-language models (VLMs), which
function as black-box approaches with limited interpretability. In this paper,
we present CADAR, a novel neurosymbolic approach for cognitive attack detection
in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a
symbolic perception-graph representation, incorporating prior knowledge,
salience weighting, and temporal correlations. The model then enables
particle-filter based statistical reasoning -- a sequential Monte Carlo method
-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of
pre-trained VLM and the interpretability and reasoning rigor of particle
filtering. Experiments on an extended AR cognitive attack dataset show accuracy
improvements of up to 10.7% over strong baselines on challenging AR attack
scenarios, underscoring the promise of neurosymbolic methods for effective and
interpretable cognitive attack detection.

</details>


### [64] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

TL;DR: The paper introduces RL-MoE, a framework that converts sensitive images into privacy-preserving text, balancing privacy and data utility.


<details>
  <summary>Details</summary>
Motivation: The need for privacy in AI-powered Intelligent Transportation Systems (ITS) conflicts with the demand for detailed visual data, as current methods like blurring or encryption fail to maintain both privacy and utility effectively.

Method: The RL-MoE framework uses a Mixture-of-Experts (MoE) architecture for nuanced scene understanding and a Reinforcement Learning (RL) agent to optimize textual output for both privacy and semantic accuracy.

Result: RL-MoE reduces replay attack success rates to 9.4% on the CFP-FP dataset while producing richer textual descriptions compared to current methods.

Conclusion: RL-MoE offers a scalable solution to enhance privacy and usability in AI systems for smart cities and autonomous vehicles, fostering privacy-sensitive innovation.

Abstract: The proliferation of AI-powered cameras in Intelligent Transportation Systems
(ITS) creates a severe conflict between the need for rich visual data and the
fundamental right to privacy. Existing privacy-preserving mechanisms, such as
blurring or encryption, are often insufficient, creating an undesirable
trade-off where either privacy is compromised against advanced reconstruction
attacks or data utility is critically degraded. To resolve this impasse, we
propose RL-MoE, a novel framework that transforms sensitive visual data into
privacy-preserving textual descriptions, eliminating the need for direct image
transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture
for nuanced, multi-aspect scene decomposition with a Reinforcement Learning
(RL) agent that optimizes the generated text for a dual objective of semantic
accuracy and privacy preservation. Extensive experiments demonstrate that
RL-MoE provides superior privacy protection, reducing the success rate of
replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously
generating richer textual content than baseline methods. Our work provides a
practical and scalable solution for building trustworthy AI systems in
privacy-sensitive domains, paving the way for more secure smart city and
autonomous vehicle networks.

</details>


### [65] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: The paper presents a novel approach to generate synthetic depth facial datasets for emotion recognition using GANs optimized with Knowledge Distillation and Genetic Algorithms.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of insufficient high-quality, diverse depth facial datasets needed for recognizing subtle emotional expressions.

Method: The authors propose using an optimized GAN framework enabled by Knowledge Distillation and Genetic Algorithms to improve training stability, image quality, and diversity. Subsequent feature extraction employs techniques like LBP, HOG, Sobel edge, and intensity histograms, enabling accurate emotion classification.

Result: The proposed method exhibits superior diversity and visual quality compared to existing frameworks (GAN, VAE, GMM, KDE). Classification models achieve up to 96% accuracy, with evaluation metrics showing improvements over state-of-the-art methods.

Conclusion: The framework enhances the generation of depth facial datasets and emotion classification, outperforming existing techniques, with potential for further applications in affective computing.

Abstract: Affective computing faces a major challenge: the lack of high-quality,
diverse depth facial datasets for recognizing subtle emotional expressions. We
propose a framework for synthetic depth face generation using an optimized GAN
with Knowledge Distillation (EMA teacher models) to stabilize training, improve
quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve
GAN latent vectors based on image statistics, boosting diversity and visual
quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in
both diversity and quality. For classification, we extract and concatenate LBP,
HOG, Sobel edge, and intensity histogram features, achieving 94% and 96%
accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows
consistent improvement over state-of-the-art methods.

</details>


### [66] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: The paper introduces Δ-AttnMask, a novel method for data-efficient Visual Instruction Finetuning (VIF) of Vision-Language Models (VLMs). By strategically masking model hidden states based on attention regions, it evaluates visual-textual sample quality without requiring additional models, training, or labels.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in scaling visual instruction finetuning for Vision-Language Models due to the high demand for quality multimodal data that aligns visual and textual content.

Method: Proposes Δ-AttnMask framework that uses attention-guided masking combined with computation of loss differences to assess the quality of image-text pairs intrinsically.

Result: Δ-AttnMask achieves state-of-the-art results, requiring only 20% of the dataset, achieves 5x faster training, and improves overall accuracy by +10.1% compared to full-dataset baselines.

Conclusion: The Δ-AttnMask technique offers a scalable and broadly applicable solution for efficient visual instruction finetuning across diverse Vision-Language Models and datasets.

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [67] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: This study proposes a personalized feature translation (PFT) method for facial expression recognition (FER) in scenarios with only neutral target data, optimizing in the latent space instead of generating images.


<details>
  <summary>Details</summary>
Motivation: Improve performance of deep FER models in real-world applications, especially for subtle expressions and high inter-subject variability, by addressing constraints with source-free domain adaptation in unique scenarios.

Method: A lightweight personalized feature translation (PFT) is developed, which operates in latent space, pre-trained on source data, and adapted on neutral target data without image generation, preserving discrimination via optimized objectives.

Result: PFT simplifies adaptation by eliminating image synthesis, lowering computational costs, and enhancing classification by leveraging latent-space translation and adapting subject-specific style features.

Conclusion: PFT is an efficient SFDA approach that reduces complexity, computational overhead, and enhances performance for domain adaptation with unlabeled neutral target data.

Abstract: Facial expression recognition (FER) models are employed in many video-based
affective computing applications, such as human-computer interaction and
healthcare monitoring. However, deep FER models often struggle with subtle
expressions and high inter-subject variability, limiting their performance in
real-world applications. To improve their performance, source-free domain
adaptation (SFDA) methods have been proposed to personalize a pretrained source
model using only unlabeled target domain data, thereby avoiding data privacy,
storage, and transmission constraints. This paper addresses a challenging
scenario where source data is unavailable for adaptation, and only unlabeled
target data consisting solely of neutral expressions is available. SFDA methods
are not typically designed to adapt using target data from only a single class.
Further, using models to generate facial images with non-neutral expressions
can be unstable and computationally intensive. In this paper, personalized
feature translation (PFT) is proposed for SFDA. Unlike current image
translation methods for SFDA, our lightweight method operates in the latent
space. We first pre-train the translator on the source domain data to transform
the subject-specific style features from one source subject into another.
Expression information is preserved by optimizing a combination of expression
consistency and style-aware objectives. Then, the translator is adapted on
neutral target data, without using source data or image synthesis. By
translating in the latent space, PFT avoids the complexity and noise of face
expression generation, producing discriminative embeddings optimized for
classification. Using PFT eliminates the need for image synthesis, reduces
computational overhead (using a lightweight translator), and only adapts part
of the model, making the method efficient compared to image-based translation.

</details>


### [68] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: The paper evaluates models for automating the coloring of anime sketches and finds C-GAN as the most effective.


<details>
  <summary>Details</summary>
Motivation: To automate and reduce the cost of generating fully colorized drawings in the manga and anime industry.

Method: Evaluation of Neural Style Transfer, C-GAN, and CycleGAN for image-to-image translation between anime characters and their sketches, using both qualitative and quantitative assessments.

Result: C-GAN demonstrated the best performance in producing high-quality, high-resolution colorized images close to human quality.

Conclusion: C-GAN is the optimal choice for automating the colorization process in anime sketches, addressing a key bottleneck in the industry.

Abstract: The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [69] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: The paper introduces MME-Emotion, a benchmark to evaluate multimodal large language models' (MLLMs) emotional intelligence, revealing current limitations in recognition and reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To address gaps in evaluating MLLMs' abilities in generalizing across scenarios and reasoning about emotional triggers.

Method: Developed MME-Emotion, a benchmark consisting of over 6,000 annotated video clips with QA pairs, covering diverse emotional tasks and evaluated with hybrid metrics in a multi-agent system framework.

Result: Evaluated 20 MLLMs, finding inadequate emotional intelligence: the top-performing model achieved only 39.3% in recognition and 56.0% in reasoning scores.

Conclusion: MME-Emotion lays a foundation to enhance emotional intelligence in MLLMs by highlighting their current deficiencies and offering a basis for improvement.

Abstract: Recent advances in multimodal large language models (MLLMs) have catalyzed
transformative progress in affective computing, enabling models to exhibit
emergent emotional intelligence. Despite substantial methodological progress,
current emotional benchmarks remain limited, as it is still unknown: (a) the
generalization abilities of MLLMs across distinct scenarios, and (b) their
reasoning capabilities to identify the triggering factors behind emotional
states. To bridge these gaps, we present \textbf{MME-Emotion}, a systematic
benchmark that assesses both emotional understanding and reasoning capabilities
of MLLMs, enjoying \textit{scalable capacity}, \textit{diverse settings}, and
\textit{unified protocols}. As the largest emotional intelligence benchmark for
MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific
questioning-answering (QA) pairs, spanning broad scenarios to formulate eight
emotional tasks. It further incorporates a holistic evaluation suite with
hybrid metrics for emotion recognition and reasoning, analyzed through a
multi-agent system framework. Through a rigorous evaluation of 20 advanced
MLLMs, we uncover both their strengths and limitations, yielding several key
insights: \ding{182} Current MLLMs exhibit unsatisfactory emotional
intelligence, with the best-performing model achieving only $39.3\%$
recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark.
\ding{183} Generalist models (\emph{e.g.}, Gemini-2.5-Pro) derive emotional
intelligence from generalized multimodal understanding capabilities, while
specialist models (\emph{e.g.}, R1-Omni) can achieve comparable performance
through domain-specific post-training adaptation. By introducing MME-Emotion,
we hope that it can serve as a foundation for advancing MLLMs' emotional
intelligence in the future.

</details>


### [70] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: The paper addresses the vulnerabilities of multimodal large language models (MLLMs) to adversarial prompts by introducing a detailed evaluation framework and a new attack strategy, Balanced Structural Decomposition (BSD), which has been shown to outperform prior methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to highlight and better understand the limitations of safety mechanisms in MLLMs, as current evaluations overestimate the effectiveness of adversarial attacks and fail to address how malicious prompts navigate safety filters.

Method: The paper introduces a four-axis evaluation framework (input on-topicness, input OOD intensity, output harmfulness, and output refusal rate) and designs the Balanced Structural Decomposition (BSD) strategy to restructure prompts into semantically aligned sub-tasks with subtle OOD signals to bypass safety mechanisms.

Result: BSD was tested on 13 MLLMs and showed significant improvements in success rates (67%) and output harmfulness (21%) compared to existing methods, while revealing structural trade-offs in the attack strategies.

Conclusion: The study exposes previously overlooked weaknesses in multimodal safety systems and underscores the need for more robust defenses against well-crafted adversarial prompts like those generated by BSD.

Abstract: Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [71] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

TL;DR: A new approach for Handwritten Mathematical Expression Recognition (HMER) was developed using a massive LaTeX-rendered formula dataset (Tex80M) and proposing the first large-scale HMER model, TexTeller.


<details>
  <summary>Details</summary>
Motivation: HMER struggles due to limited data caused by the intensive manual annotation process. The paper aims to address the data scarcity challenge.

Method: The authors created a scalable data engine to produce high-quality LaTeX sequences, building a dataset called Tex80M with over 80 million formulas. They mix-train Tex80M with smaller datasets to develop TexTeller, a large-scale HMER model.

Result: TexTeller achieves state-of-the-art (SOTA) performance on nearly all benchmarks in the HMER domain.

Conclusion: The dataset, model, and source code will be openly released, fostering future research in HMER and related fields.

Abstract: Large foundation models have achieved significant performance gains through
scalable training on massive datasets. However, the field of
\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression
\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily
due to the arduous and costly process of manual annotation. To bridge this gap,
we propose a novel method integrating limited handwritten formulas with
large-scale LaTeX-rendered formulas by developing a scalable data engine to
generate complex and consistent LaTeX sequences. With this engine, we built the
largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80
million high-quality training instances. Then we propose \texttt{TexTeller},
the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a
relatively small HME dataset. The expansive training dataset and our refined
pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)
performance across nearly all benchmarks. To advance the field, we will openly
release our complete model, entire dataset, and full codebase, enabling further
research building upon our contributions.

</details>


### [72] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: The paper introduces Gradient-Direction-Aware Gaussian Splatting (GDAGS) to resolve issues of over-reconstruction and over-densification in 3D Gaussian Splatting for novel view synthesis. It achieves superior rendering quality with reduced memory consumption.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of over-reconstruction and over-densification in 3D Gaussian Splatting, which lead to degraded rendering quality and excessive memory usage.

Method: The authors propose GDAGS featuring a gradient coherence ratio (GCR) to distinguish conflicting vs. concordant gradients and a nonlinear weighting mechanism for adaptive density control of Gaussians.

Result: GDAGS improves geometric detailing and suppresses redundant Gaussian components, yielding superior rendering quality and achieving 50% reduced memory consumption.

Conclusion: GDAGS effectively mitigates two major limitations in 3D Gaussian Splatting, offering improved rendering quality and optimized memory usage for compact scene representations.

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [73] [ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images](https://arxiv.org/abs/2508.09849)
*Jan Phillipp Albrecht,Jose R. A. Godinho,Christina Hübers,Deborah Schmidt*

Main category: cs.CV

TL;DR: ARI3D is a proposed interactive software tool to improve the analysis of 3D X-ray CT images by addressing imaging artifacts and assisting with segmentation and classification.


<details>
  <summary>Details</summary>
Motivation: Quantitative analysis of 3D X-ray CT image microstructures is often hampered by imaging artifacts and requires user intervention for accurate segmentation and classification.

Method: ARI3D software provides an interactive mechanism to classify and quantify objects in 3D images, addressing artifacts like beam hardening and partial volume effects.

Result: The tool improves phase identification, handles partial volume effects, enhances detection limit and quantification accuracy, and harmonizes analysis across scientific fields.

Conclusion: ARI3D addresses critical challenges in 3D X-ray CT image analysis, offering a standardized and highly accurate tool for a variety of scientific applications.

Abstract: X-ray computed tomography (CT) is the main 3D technique for imaging the
internal microstructures of materials. Quantitative analysis of the
microstructures is usually achieved by applying a sequence of steps that are
implemented to the entire 3D image. This is challenged by various imaging
artifacts inherent from the technique, e.g., beam hardening and partial volume.
Consequently, the analysis requires users to make a number of decisions to
segment and classify the microstructures based on the voxel gray-values. In
this context, a software tool, here called ARI3D, is proposed to interactively
analyze regions in three-dimensional X-ray CT images, assisting users through
the various steps of a protocol designed to classify and quantify objects
within regions of a three-dimensional image. ARI3D aims to 1) Improve phase
identification; 2) Account for partial volume effect; 3) Increase the detection
limit and accuracy of object quantification; and 4) Harmonize quantitative 3D
analysis that can be implemented in different fields of science.

</details>


### [74] [FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents](https://arxiv.org/abs/2508.09241)
*Fengxian Ji,Jingpu Yang,Zirui Song,Yuanxi Wang,Zhexuan Cui,Yuke Li,Qian Jiang,Miao Fang,Xiuying Chen*

Main category: cs.CV

TL;DR: The paper introduces FineState-Bench, a new evaluation framework for GUI agents focusing on fine-grained control capabilities. It emphasizes diagnosing GUI agent shortcomings and provides open-source benchmarks for analysis.


<details>
  <summary>Details</summary>
Motivation: The authors are motivated by the inadequacies of existing evaluation frameworks for GUI agents, which prioritize coarse-grained task completion while neglecting the fine-grained control essential for real-world applications.

Method: They created FineState-Bench, a diagnostic toolkit with 2257 task benchmarks across desktop, web, and mobile platforms. It utilizes a four-phase indicator for evaluating perception-to-control efficiency and includes a plug-and-play Visual Diagnostic Assistant (VDA) for analyzing perception and positioning.

Result: The experimental results reveal that state-of-the-art models achieve only 32.8% fine-grained interaction accuracy. Ideal visual localization can significantly boost performance, as demonstrated with Gemini-2.5-Flash improving its success rate by 14.9% in controlled experiments.

Conclusion: The primary limitation of current GUI proxy agents lies in their visual positioning capabilities, which hinder fine-grained control. FineState-Bench offers comprehensive tools to address and study this challenge in depth.

Abstract: With the rapid advancement of generative artificial intelligence technology,
Graphical User Interface (GUI) agents have demonstrated tremendous potential
for autonomously managing daily tasks through natural language instructions.
However, current evaluation frameworks for GUI agents suffer from fundamental
flaws: existing benchmarks overly focus on coarse-grained task completion while
neglecting fine-grained control capabilities crucial for real-world
applications. To address this, we introduce FineState-Bench, the first
evaluation and diagnostic standard for fine-grained GUI proxy operations,
designed to quantify fine-grained control. This multi-platform (desktop, Web,
mobile) framework includes 2257 task benchmarks in four components and uses a
four-phase indicator for comprehensive perception-to-control assessment. To
analyze perception and positioning for refined operations, we developed the
plug-and-play Visual Diagnostic Assistant (VDA), enabling the first
quantitative decoupling analysis of these capabilities. Experimental results on
our benchmark show that the most advanced models achieve only 32.8%
fine-grained interaction accuracy. Using our VDA in controlled experiments,
quantifying the impact of visual capabilities, we showed that ideal visual
localization boosts Gemini-2.5-Flash's success rate by 14.9\%. Our diagnostic
framework confirms for the first time that the primary bottleneck for current
GUI proxies is basic visual positioning capability.All resources are fully
open-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench
huggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench

</details>


### [75] [Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users](https://arxiv.org/abs/2508.09245)
*Jeffri Murrugarra-LLerena,Haoran Niu,K. Suzanne Barber,Hal Daumé III,Yang Trista Cao,Paola Cascante-Bonilla*

Main category: cs.CV

TL;DR: The paper introduces FiGPriv, a framework that offers enhanced privacy protection for blind and low vision users by selectively masking high-risk private content, improving visual assistant usability.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns for blind and low vision users who may unintentionally include sensitive information in images used for visual assistant systems.

Method: The framework employs fine-grained segmentation combined with a data-driven risk scoring mechanism to selectively mask sensitive image content.

Result: FiGPriv preserves 26% more image content compared to existing methods, improving the effectiveness of visual language models: enhancing responses by 11% and image content identification by 45%.

Conclusion: FiGPriv achieves a balance between privacy and usability, offering a more nuanced approach to visual data protection while retaining functionality in visual assistant systems.

Abstract: As visual assistant systems powered by visual language models (VLMs) become
more prevalent, concerns over user privacy have grown, particularly for blind
and low vision users who may unknowingly capture personal private information
in their images. Existing privacy protection methods rely on coarse-grained
segmentation, which uniformly masks entire private objects, often at the cost
of usability. In this work, we propose FiGPriv, a fine-grained privacy
protection framework that selectively masks only high-risk private information
while preserving low-risk information. Our approach integrates fine-grained
segmentation with a data-driven risk scoring mechanism. We evaluate our
framework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%
of image content, enhancing the ability of VLMs to provide useful responses by
11% and identify the image content by 45%, while ensuring privacy protection.
Project Page: https://artcs1.github.io/VLMPrivacy/

</details>


### [76] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: This paper introduces a method to improve efficiency in vision-and-language navigation (VLN) models by making navigation input-adaptive, reducing computational demands without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiency in modern VLN models that rely on history-aware multi-modal transformers, which require significant computational resources.

Method: Three adaptive algorithms are proposed: spatial efficiency using selective processing of panoramic views, intra-model efficiency with importance-based thresholding for early-exit methods, and temporal efficiency by implementing a caching mechanism for previously seen views.

Result: The methods proposed demonstrate over 2× reduction in computation on seven VLN benchmarks for three different agents, while maintaining performance.

Conclusion: Input-adaptive mechanisms can substantially reduce computational demand in VLN models without degrading their effectiveness, making them more suited for resource-constrained environments.

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [77] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: SegDAC proposes a method combining state-of-the-art perceptual models with reinforcement learning for better performance in visually complex environments.


<details>
  <summary>Details</summary>
Motivation: Current visual reinforcement learning models struggle with generalizing across diverse tasks and efficiently dealing with perception and noisy rewards from high-dimensional inputs.

Method: SegDAC integrates segmentation via SAM and semantic grounding with YOLO-World; utilizes a transformer-based architecture to dynamically focus on object segments without reliance on human annotations.

Result: SegDAC exhibits remarkable visual generalization, doubling performance in a challenging benchmark environment and improving sample efficiency compared to prior approaches.

Conclusion: SegDAC is an effective method for enhancing visual RL, providing strong improvements in both generalization and efficiency through innovative design approaches.

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [78] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

TL;DR: The paper introduces Lung-DDPM+, an advanced generative AI model designed for lung cancer diagnosis through CT imaging, offering improved efficiency, anatomical precision, and synthetic data quality compared to existing models.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for lung cancer diagnosis suffer from inefficiencies and anatomical imprecision, limiting their usefulness in clinical applications.

Method: The Lung-DDPM+ model is based on the denoising diffusion probabilistic model (DDPM), which utilizes nodule semantic layouts and a pulmonary DPM-solver to focus on lesion areas while improving sampling efficiency and quality.

Result: The model achieves 8× fewer FLOPs, 6.8× lower GPU memory consumption, and 14× faster sampling compared to its predecessor Lung-DDPM, while maintaining comparable sample quality to other state-of-the-art generative models.

Conclusion: Lung-DDPM+ effectively generates high-quality synthetic thoracic CT images, demonstrating potential for applications in medical imaging beyond lung cancer diagnosis, such as general tumor synthesis and lesion generation.

Abstract: Generative artificial intelligence (AI) has been playing an important role in
various domains. Leveraging its high capability to generate high-fidelity and
diverse synthetic data, generative AI is widely applied in diagnostic tasks,
such as lung cancer diagnosis using computed tomography (CT). However, existing
generative models for lung cancer diagnosis suffer from low efficiency and
anatomical imprecision, which limit their clinical applicability. To address
these drawbacks, we propose Lung-DDPM+, an improved version of our previous
model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic
model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary
DPM-solver, enabling the method to focus on lesion areas while achieving a
better trade-off between sampling efficiency and quality. Evaluation results on
the public LIDC-IDRI dataset suggest that the proposed method achieves
8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower
GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM.
Moreover, it maintains comparable sample quality to both Lung-DDPM and other
state-of-the-art (SOTA) generative models in two downstream segmentation tasks.
We also conducted a Visual Turing Test by an experienced radiologist, showing
the advanced quality and fidelity of synthetic samples generated by the
proposed method. These experimental results demonstrate that Lung-DDPM+ can
effectively generate high-quality thoracic CT images with lung nodules,
highlighting its potential for broader applications, such as general tumor
synthesis and lesion generation in medical imaging. The code and pretrained
models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.

</details>


### [79] [UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas](https://arxiv.org/abs/2508.09339)
*Aqsa Sultana,Nordin Abouzahra,Ahmed Rahu,Brian Shula,Brandon Combs,Derrick Forchetti,Theus Aspiras,Vijayan K. Asari*

Main category: cs.CV

TL;DR: This paper introduces the Ultralight Med-Vision Mamba model for enhancing colonoscopy screening accuracy using state-space modeling techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve colorectal cancer prevention by enhancing adenoma identification and risk assessment during colonoscopy screenings.

Method: The study applies the Ultralight Med-Vision Mamba model, incorporating state-space modeling to analyze image dependencies and generalization.

Result: The model demonstrated high accuracy in adenoma classification and efficiency in computational speed and scalability.

Conclusion: Ultralight Med-Vision Mamba is a promising tool for real-time clinical use in colorectal cancer prevention strategies.

Abstract: Identification of precancerous polyps during routine colonoscopy screenings
is vital for their excision, lowering the risk of developing colorectal cancer.
Advanced deep learning algorithms enable precise adenoma classification and
stratification, improving risk assessment accuracy and enabling personalized
surveillance protocols that optimize patient outcomes. Ultralight Med-Vision
Mamba, a state-space based model (SSM), has excelled in modeling long- and
short-range dependencies and image generalization, critical factors for
analyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's
efficient architecture offers advantages in both computational speed and
scalability, making it a promising tool for real-time clinical deployment.

</details>


### [80] [Blink-to-code: real-time Morse code communication via eye blink detection and classification](https://arxiv.org/abs/2508.09344)
*Anushka Bhatt*

Main category: cs.CV

TL;DR: The paper introduces a real-time system converting voluntary eye blinks into Morse code for communication, achieving 62% decoding accuracy.


<details>
  <summary>Details</summary>
Motivation: To empower individuals with severe motor impairments by providing an affordable and effective communication tool.

Method: Leveraged computer vision on a standard webcam to classify eye blinks as Morse code dots and dashes, followed by decoding them into characters.

Result: Achieved 62% decoding accuracy with a response time of 18-20 seconds in experiments involving five participants.

Conclusion: The system is a low-cost, promising assistive communication method for motor-impaired individuals.

Abstract: This study proposes a real-time system that translates voluntary eye blinks
into Morse code, enabling communication for individuals with severe motor
impairments. Using a standard webcam and computer vision, the system detects
and classifies blinks as short (dot) or long (dash), then decodes them into
alphanumeric characters. Experiments with five participants show 62% decoding
accuracy and 18-20 seconds response times, demonstrating a viable, low-cost
assistive communication method.

</details>


### [81] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: FusionEnsemble-Net, an attention-based ensemble framework, achieves 99.44% accuracy on multimodal Italian Sign Language recognition by dynamically fusing visual and motion data.


<details>
  <summary>Details</summary>
Motivation: Sign language recognition in healthcare requires accurate interpretation of complex multimodal gestures for effective communication.

Method: FusionEnsemble-Net combines RGB video and radar data using an attention-based module and spatiotemporal networks, with an ensemble classification mechanism.

Result: The method significantly outperforms state-of-the-art approaches, achieving a test accuracy of 99.44% on the MultiMeDaLIS dataset.

Conclusion: An ensemble of spatiotemporal networks unified with attention-based fusion offers a robust approach for multimodal gesture recognition tasks.

Abstract: Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>


### [82] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: This paper introduces a dual-architecture approach to tackle challenges in Continuous Sign Language Recognition (CSLR), showing notable Word Error Rate (WER) reductions and setting new benchmarks on the Isharah-1000 dataset.


<details>
  <summary>Details</summary>
Motivation: To address limitations in CSLR such as inter-signer variability and poor generalization to unseen sentence structures, which are inadequately handled by traditional methods.

Method: A dual-architecture framework comprising (1) a Signer-Invariant Conformer leveraging pose-based skeletal keypoints, and (2) a Multi-Scale Fusion Transformer featuring a dual-path temporal encoder for fine-grained posture dynamics.

Result: Experiments on the Isharah-1000 dataset report WER reductions: 13.07% for SI (13.53% improvement) and 47.78% for US (exceeding prior benchmarks). Achieved placements of 2nd in US and 4th in SI at the SignEval 2025 CSLR challenge.

Conclusion: Task-specific network designs significantly improve CSLR performance, validating the hypothesis and paving the way for future research benchmarks.

Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

</details>


### [83] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: This paper addresses inter- and intra-annotator variability in medical image segmentation, specifically in skin lesion analysis, and proposes IAA as a predictive and clinical feature.


<details>
  <summary>Details</summary>
Motivation: To understand and address variability in skin lesion segmentation caused by annotator differences, malignancy factors, and tool usage.

Method: Curated the IMA++ dataset for multi-annotator skin lesion segmentation, analyzed variability factors, investigated the association between IAA and malignancy, and incorporated IAA as a soft clinical feature in multi-task learning models.

Result: Found a significant association between IAA and malignancy with predictive accuracy for IAA from images and introduced a method to improve classification accuracy by 4.2% using IAA as a feature.

Conclusion: The work highlights the importance of inter-annotator agreement in medical image tasks, demonstrating its utility in improving prediction models and providing insights into malignancy characteristics of lesions.

Abstract: Medical image segmentation exhibits intra- and inter-annotator variability
due to ambiguous object boundaries, annotator preferences, expertise, and
tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated
or infiltrative nodules, or irregular borders per the ABCD rule, are
particularly prone to disagreement and are often associated with malignancy. In
this work, we curate IMA++, the largest multi-annotator skin lesion
segmentation dataset, on which we conduct an in-depth study of variability due
to annotator, malignancy, tool, and skill factors. We find a statistically
significant (p<0.001) association between inter-annotator agreement (IAA),
measured using Dice, and the malignancy of skin lesions. We further show that
IAA can be accurately predicted directly from dermoscopic images, achieving a
mean absolute error of 0.108. Finally, we leverage this association by
utilizing IAA as a "soft" clinical feature within a multi-task learning
objective, yielding a 4.2% improvement in balanced accuracy averaged across
multiple model architectures and across IMA++ and four public dermoscopic
datasets. The code is available at https://github.com/sfu-mial/skin-IAV.

</details>


### [84] [X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents](https://arxiv.org/abs/2508.09383)
*Guoxian Song,Hongyi Xu,Xiaochen Zhao,You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Linjie Luo*

Main category: cs.CV

TL;DR: This paper proposes X-UniMotion, a novel representation for human motion transfer encompassing facial expressions, body poses, and hand gestures using disentangled latent tokens.


<details>
  <summary>Details</summary>
Motivation: Existing motion transfer methods face limitations due to reliance on skeletal poses and heuristics. There is a need for a unified method to encode detailed whole-body motion in a compact and identity-agnostic manner.

Method: The authors introduce a self-supervised, end-to-end framework for encoding motion from a single image into four latent tokens. They use spatial and color augmentations, synthetic 3D renderings, and auxiliary decoders for detailed motion learning.

Result: X-UniMotion achieves high-fidelity cross-identity motion transfer, outperforming state-of-the-art methods in motion expressiveness and identity preservation.

Conclusion: X-UniMotion provides a robust, unified latent representation for whole-body human motion, enabling superior animation results across diverse subjects and configurations.

Abstract: We present X-UniMotion, a unified and expressive implicit latent
representation for whole-body human motion, encompassing facial expressions,
body poses, and hand gestures. Unlike prior motion transfer methods that rely
on explicit skeletal poses and heuristic cross-identity adjustments, our
approach encodes multi-granular motion directly from a single image into a
compact set of four disentangled latent tokens -- one for facial expression,
one for body pose, and one for each hand. These motion latents are both highly
expressive and identity-agnostic, enabling high-fidelity, detailed
cross-identity motion transfer across subjects with diverse identities, poses,
and spatial configurations. To achieve this, we introduce a self-supervised,
end-to-end framework that jointly learns the motion encoder and latent
representation alongside a DiT-based video generative model, trained on
large-scale, diverse human motion datasets. Motion-identity disentanglement is
enforced via 2D spatial and color augmentations, as well as synthetic 3D
renderings of cross-identity subject pairs under shared poses. Furthermore, we
guide motion token learning with auxiliary decoders that promote fine-grained,
semantically aligned, and depth-aware motion embeddings. Extensive experiments
show that X-UniMotion outperforms state-of-the-art methods, producing highly
expressive animations with superior motion fidelity and identity preservation.

</details>


### [85] [Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation](https://arxiv.org/abs/2508.09423)
*Badi Li,Ren-jie Lu,Yu Zhou,Jingke Meng,Wei-shi Zheng*

Main category: cs.CV

TL;DR: The paper introduces GOAL, a generative framework for object goal navigation, leveraging large language models to improve performance on unseen environments.


<details>
  <summary>Details</summary>
Motivation: Current Object Goal Navigation models struggle with generalization to unseen environments due to reliance on deterministic methods that overlook uncertainty in indoor layouts.

Method: GOAL uses a generative flow-based approach combined with LLM-enriched semantic maps. It incorporates spatial priors modeled as Gaussian fields and trains a flow model with injected LLM knowledge.

Result: GOAL achieves state-of-the-art performance on MP3D and Gibson datasets and shows strong generalization in transfer to HM3D environments.

Conclusion: The proposed GOAL framework effectively incorporates semantic distributions and spatial priors, improving navigation performance and generalization. Its code and pretrained models are publicly available.

Abstract: The Object Goal Navigation (ObjectNav) task challenges agents to locate a
specified object in an unseen environment by imagining unobserved regions of
the scene. Prior approaches rely on deterministic and discriminative models to
complete semantic maps, overlooking the inherent uncertainty in indoor layouts
and limiting their ability to generalize to unseen environments. In this work,
we propose GOAL, a generative flow-based framework that models the semantic
distribution of indoor environments by bridging observed regions with
LLM-enriched full-scene semantic maps. During training, spatial priors inferred
from large language models (LLMs) are encoded as two-dimensional Gaussian
fields and injected into target maps, distilling rich contextual knowledge into
the flow model and enabling more generalizable completions. Extensive
experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D
and Gibson, and shows strong generalization in transfer settings to HM3D. Codes
and pretrained models are available at https://github.com/Badi-Li/GOAL.

</details>


### [86] [DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection](https://arxiv.org/abs/2508.09392)
*Kang Ni,Minrui Zou,Yuxuan Li,Xiang Li,Kehua Guo,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: DenoDet V2 revolutionizes SAR object detection by addressing coherent noise through transform-domain feature modulation and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges posed by coherent noise in SAR object detection, a limitation with existing methods.

Method: Utilizes transform-domain feature modulation with a band-wise mutual modulation mechanism to exploit amplitude-phase complementarity.

Result: DenoDet V2 achieves a 0.8% improvement over DenoDet V1 on the SARDet-100K dataset and decreases model complexity by half.

Conclusion: The proposed DenoDet V2 method proves highly effective, offering improved performance and reduced complexity, marking a significant advance in SAR object detection.

Abstract: One of the primary challenges in Synthetic Aperture Radar (SAR) object
detection lies in the pervasive influence of coherent noise. As a common
practice, most existing methods, whether handcrafted approaches or deep
learning-based methods, employ the analysis or enhancement of object
spatial-domain characteristics to achieve implicit denoising. In this paper, we
propose DenoDet V2, which explores a completely novel and different perspective
to deconstruct and modulate the features in the transform domain via a
carefully designed attention architecture. Compared to DenoDet V1, DenoDet V2
is a major advancement that exploits the complementary nature of amplitude and
phase information through a band-wise mutual modulation mechanism, which
enables a reciprocal enhancement between phase and amplitude spectra. Extensive
experiments on various SAR datasets demonstrate the state-of-the-art
performance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\%
improvement on SARDet-100K dataset compared to DenoDet V1, while reducing the
model complexity by half. The code is available at
https://github.com/GrokCV/GrokSAR.

</details>


### [87] [WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization](https://arxiv.org/abs/2508.09560)
*Jiahao Wen,Hang Yu,Zhedong Zheng*

Main category: cs.CV

TL;DR: WeatherPrompt is a framework enhancing drone geo-localization robustness under diverse weather. It achieves significant recall rate improvements without relying on predefined weather categories.


<details>
  <summary>Details</summary>
Motivation: Existing drone geo-localization methods are limited by their reliance on restricted weather categories and ineffective disentanglement of scene and weather features.

Method: WeatherPrompt combines multi-modality learning, training-free weather reasoning, and dynamic visual-text fusion using cross-modal objectives like contrastive learning.

Result: The method achieves substantial performance gains, including +13.37% Recall@1 under night conditions and +18.69% under fog and snow.

Conclusion: WeatherPrompt successfully addresses weather-related challenges, reinforcing geo-localization adaptability and scalability under diverse and unseen conditions.

Abstract: Visual geo-localization for drones faces critical degradation under weather
perturbations, \eg, rain and fog, where existing methods struggle with two
inherent limitations: 1) Heavy reliance on limited weather categories that
constrain generalization, and 2) Suboptimal disentanglement of entangled
scene-weather features through pseudo weather categories. We present
WeatherPrompt, a multi-modality learning paradigm that establishes
weather-invariant representations through fusing the image embedding with the
text context. Our framework introduces two key contributions: First, a
Training-free Weather Reasoning mechanism that employs off-the-shelf large
multi-modality models to synthesize multi-weather textual descriptions through
human-like reasoning. It improves the scalability to unseen or complex weather,
and could reflect different weather strength. Second, to better disentangle the
scene and weather feature, we propose a multi-modality framework with the
dynamic gating mechanism driven by the text embedding to adaptively reweight
and fuse visual features across modalities. The framework is further optimized
by the cross-modal objectives, including image-text contrastive learning and
image-text matching, which maps the same scene with different weather
conditions closer in the respresentation space. Extensive experiments validate
that, under diverse weather conditions, our method achieves competitive recall
rates compared to state-of-the-art drone geo-localization methods. Notably, it
improves Recall@1 by +13.37\% under night conditions and by 18.69\% under fog
and snow conditions.

</details>


### [88] [Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety](https://arxiv.org/abs/2508.09397)
*Zhengli Zhang,Xinyu Luo,Yuchen Sun,Wenhua Ding,Dongyu Huang,Xinlei Chen*

Main category: cs.CV

TL;DR: SkyShield introduces an event-driven method for detecting submillimeter obstacles threatening drones in complex environments, achieving high accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: The ability to detect submillimeter obstacles such as steel wires and kite strings poses a challenge to conventional sensor systems, necessitating a new framework for safer drone navigation.

Method: SkyShield uses an event-driven framework with a lightweight U-Net architecture and Dice-Contour Regularization Loss to process unique event stream features from thin obstacles.

Result: SkyShield achieves a mean F1 Score of 0.7088 and a latency of 21.2 ms in detecting submillimeter obstacles.

Conclusion: The proposed framework successfully enables precise, low-latency detection of thin obstacles, making it suitable for edge and mobile platforms.

Abstract: Drones operating in complex environments face a significant threat from thin
obstacles, such as steel wires and kite strings at the submillimeter level,
which are notoriously difficult for conventional sensors like RGB cameras,
LiDAR, and depth cameras to detect. This paper introduces SkyShield, an
event-driven, end-to-end framework designed for the perception of submillimeter
scale obstacles. Drawing upon the unique features that thin obstacles present
in the event stream, our method employs a lightweight U-Net architecture and an
innovative Dice-Contour Regularization Loss to ensure precise detection.
Experimental results demonstrate that our event-based approach achieves mean F1
Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment
on edge and mobile platforms.

</details>


### [89] [Plane Detection and Ranking via Model Information Optimization](https://arxiv.org/abs/2508.09625)
*Daoxin Zhong,Jun Li,Meng Yee Michael Chuah*

Main category: cs.CV

TL;DR: This paper proposes a framework for plane detection in depth images using model information optimization, addressing issues like false positive detections in complex scenes.


<details>
  <summary>Details</summary>
Motivation: Existing iterative methods like RANSAC struggle with false positive detections due to ambiguous inlier thresholds, especially when detecting an unknown number of planes in complex real-world scenes.

Method: The framework treats depth readings as random variables constrained by ground truth planes, generates candidate models via random sub-sampling, and uses information optimization to select the best-fitting model while incorporating depth sensor physics and noise.

Result: Experiments with synthetic data show more accurate plane parameter estimations than Open3D RANSAC. Neural network segmentation is used to partition the depth map, speeding up computations and generating more realistic plane parameters in real-world data.

Conclusion: The proposed method improves plane detection accuracy and robustness, reducing false positives and estimating plane quality objectively through information reduction.

Abstract: Plane detection from depth images is a crucial subtask with broad robotic
applications, often accomplished by iterative methods such as Random Sample
Consensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic
guarantees, the ambiguity of its inlier threshold criterion makes it
susceptible to false positive plane detections. This issue is particularly
prevalent in complex real-world scenes, where the true number of planes is
unknown and multiple planes coexist. In this paper, we aim to address this
limitation by proposing a generalised framework for plane detection based on
model information optimization. Building on previous works, we treat the
observed depth readings as discrete random variables, with their probability
distributions constrained by the ground truth planes. Various models containing
different candidate plane constraints are then generated through repeated
random sub-sampling to explain our observations. By incorporating the physics
and noise model of the depth sensor, we can calculate the information for each
model, and the model with the least information is accepted as the most likely
ground truth. This information optimization process serves as an objective
mechanism for determining the true number of planes and preventing false
positive detections. Additionally, the quality of each detected plane can be
ranked by summing the information reduction of inlier points for each plane. We
validate these properties through experiments with synthetic data and find that
our algorithm estimates plane parameters more accurately compared to the
default Open3D RANSAC plane segmentation. Furthermore, we accelerate our
algorithm by partitioning the depth map using neural network segmentation,
which enhances its ability to generate more realistic plane parameters in
real-world data.

</details>


### [90] [Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring](https://arxiv.org/abs/2508.09398)
*El Mustapha Mansouri*

Main category: cs.CV

TL;DR: This paper describes a low-cost, on-premise system for monitoring and classifying backyard birds in Belgian urban gardens using AI tools on standard hardware.


<details>
  <summary>Details</summary>
Motivation: The study aims to create a privacy-preserving, accessible bird monitoring solution for urban biodiversity logging without relying on cloud-based systems.

Method: The system uses a motion-triggered camera to upload clips to a local server. Detectron2 is used for bird localization, and EfficientNet-B3 is fine-tuned for identifying 40 bird species. The setup runs entirely on commodity hardware.

Result: The classification model achieves about 99.5% validation accuracy on curated data and around 88% top-1 field accuracy with real-world data. Detector-guided cropping improved classification precision.

Conclusion: The system is practical for citizen science projects, offering accurate, affordable, and private bird monitoring in urban gardens.

Abstract: This paper presents a low cost, on premise system for autonomous backyard
bird monitoring in Belgian urban gardens. A motion triggered IP camera uploads
short clips via FTP to a local server, where frames are sampled and birds are
localized with Detectron2; cropped regions are then classified by an
EfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a
larger Kaggle corpus. All processing runs on commodity hardware without a
discrete GPU, preserving privacy and avoiding cloud fees. The physical feeder
uses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.
Detector-guided cropping improves classification accuracy over raw-frame
classification. The classifier attains high validation performance on the
curated subset (about 99.5 percent) and delivers practical field accuracy
(top-1 about 88 percent) on held-out species, demonstrating feasibility for
citizen-science-grade biodiversity logging at home.

</details>


### [91] [Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision](https://arxiv.org/abs/2508.09681)
*Gerardo Loza,Junlei Hu,Dominic Jones,Sharib Ali,Pietro Valdastri*

Main category: cs.CV

TL;DR: This paper introduces a novel NeRF-based test-time optimization approach to achieve precise 2D and 3D point tracking in surgical scenarios, significantly improving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Point tracking methods often struggle with motion consistency and are mainly limited to 2D tracking. The paper seeks to address these limitations by providing a more robust solution to long-term tracking in 3D environments, especially for surgical applications.

Method: The approach leverages a new invertible Neural Radiance Field (InvNeRF) architecture that uses reprojection supervision and adapts rendering-based strategies for deformable-canonical mappings, workspace handling, and ray density optimization. It also introduces multi-scale HexPlanes for faster inference and a novel pixel sampling algorithm for efficient convergence.

Result: The proposed method improved average precision in 2D tracking by nearly 50% compared to current test-time optimization methods. Furthermore, it successfully demonstrated its capability for 3D point tracking, a milestone as prior TTO approaches were limited to 2D.

Conclusion: This work advances the state-of-the-art in both 2D and 3D point tracking, showcasing high precision and accuracy, and integrates deformable NeRF-based reconstructions to enhance surgical tracking applications.

Abstract: We proposed a novel test-time optimisation (TTO) approach framed by a
NeRF-based architecture for long-term 3D point tracking. Most current methods
in point tracking struggle to obtain consistent motion or are limited to 2D
motion. TTO approaches frame the solution for long-term tracking as optimising
a function that aggregates correspondences from other specialised
state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose
parametrising such a function with our new invertible Neural Radiance Field
(InvNeRF) architecture to perform both 2D and 3D tracking in surgical
scenarios. Our approach allows us to exploit the advantages of a
rendering-based approach by supervising the reprojection of pixel
correspondences. It adapts strategies from recent rendering-based methods to
obtain a bidirectional deformable-canonical mapping, to efficiently handle a
defined workspace, and to guide the rays' density. It also presents our
multi-scale HexPlanes for fast inference and a new algorithm for efficient
pixel sampling and convergence criteria. We present results in the STIR and
SCARE datasets, for evaluating point tracking and testing the integration of
kinematic data in our pipeline, respectively. In 2D point tracking, our
approach surpasses the precision and accuracy of the TTO state-of-the-art
methods by nearly 50% on average precision, while competing with other
approaches. In 3D point tracking, this is the first TTO approach, surpassing
feed-forward methods while incorporating the benefits of a deformable
NeRF-based reconstruction.

</details>


### [92] [Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving](https://arxiv.org/abs/2508.09404)
*Guangxun Zhu,Shiyu Fan,Hang Dai,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: Authors introduce Waymo-3DSkelMo, a large-scale dataset featuring temporally coherent 3D skeletal motions with interaction semantics, derived from the Waymo Perception dataset. It aims to enhance pedestrian understanding in dynamic urban spaces.


<details>
  <summary>Details</summary>
Motivation: Existing 3D motion datasets rely on monocular RGB video frames and are hindered by issues like occlusion and lack of temporal continuity, resulting in low-quality human motion data. A high-quality dataset with multi-person 3D interactions is needed for better understanding pedestrian behavior in urban environments.

Method: The dataset leverages 3D human body shape and motion priors to improve the quality of 3D pose sequences extracted from raw LiDAR point clouds.

Result: The dataset includes over 14,000 seconds across 800 driving scenarios, representing rich interactions among an average of 27 agents per scene. It also establishes 3D pose forecasting benchmarks under varying pedestrian densities.

Conclusion: Waymo-3DSkelMo serves as a foundational resource for researching fine-grained human behavior in urban environments, and its availability will enable future studies to leverage this high-quality dataset.

Abstract: Large-scale high-quality 3D motion datasets with multi-person interactions
are crucial for data-driven models in autonomous driving to achieve
fine-grained pedestrian interaction understanding in dynamic urban
environments. However, existing datasets mostly rely on estimating 3D poses
from monocular RGB video frames, which suffer from occlusion and lack of
temporal continuity, thus resulting in unrealistic and low-quality human
motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale
dataset providing high-quality, temporally coherent 3D skeletal motions with
explicit interaction semantics, derived from the Waymo Perception dataset. Our
key insight is to utilize 3D human body shape and motion priors to enhance the
quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The
dataset covers over 14,000 seconds across more than 800 real driving scenarios,
including rich interactions among an average of 27 agents per scene (with up to
250 agents in the largest scene). Furthermore, we establish 3D pose forecasting
benchmarks under varying pedestrian densities, and the results demonstrate its
value as a foundational resource for future research on fine-grained human
behavior understanding in complex urban environments. The dataset and code will
be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo

</details>


### [93] [Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System](https://arxiv.org/abs/2508.09732)
*Romeo Valentin,Sydney M. Katz,Artur B. Carneiro,Don Walker,Mykel J. Kochenderfer*

Main category: cs.CV

TL;DR: The paper presents a robust vision-based pipeline for aircraft pose estimation from runway images, incorporating innovations in neural architecture, loss functions, and fault detection.


<details>
  <summary>Details</summary>
Motivation: To provide robust and safety-critical autonomous navigation capabilities for aviation applications, particularly in automated landing and runway detection.

Method: Developed a pipeline with a flexible neural architecture using a spatial Soft Argmax operator, a principled loss function for calibrated uncertainties, and an adaptation of RAIM for fault detection.

Result: The proposed model outperforms baseline architectures in accuracy and provides well-calibrated uncertainty estimates for sub-pixel precision and fault detection.

Conclusion: The study advances the certification potential of vision-based systems for safety-critical aviation applications.

Abstract: Recent advances in data-driven computer vision have enabled robust autonomous
navigation capabilities for civil aviation, including automated landing and
runway detection. However, ensuring that these systems meet the robustness and
safety requirements for aviation applications remains a major challenge. In
this work, we present a practical vision-based pipeline for aircraft pose
estimation from runway images that represents a step toward the ability to
certify these systems for use in safety-critical aviation applications. Our
approach features three key innovations: (i) an efficient, flexible neural
architecture based on a spatial Soft Argmax operator for probabilistic keypoint
regression, supporting diverse vision backbones with real-time inference; (ii)
a principled loss function producing calibrated predictive uncertainties, which
are evaluated via sharpness and calibration metrics; and (iii) an adaptation of
Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling
runtime detection and rejection of faulty model outputs. We implement and
evaluate our pose estimation pipeline on a dataset of runway images. We show
that our model outperforms baseline architectures in terms of accuracy while
also producing well-calibrated uncertainty estimates with sub-pixel precision
that can be used downstream for fault detection.

</details>


### [94] [RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata](https://arxiv.org/abs/2508.09415)
*John S. O'Meara,Jared Hwang,Zeyu Wang,Michael Saugstad,Jon E. Froehlich*

Main category: cs.CV

TL;DR: The paper presents RampNet, a two-stage pipeline for curb ramp detection, creating a dataset from over 210,000 Google Street View panoramas and achieving state-of-the-art model performance.


<details>
  <summary>Details</summary>
Motivation: Curb ramps are crucial for urban accessibility, but there is a lack of large-scale, high-quality datasets to detect them effectively, which existing methods fail to address comprehensively.

Method: The proposed two-stage pipeline creates a dataset using government curb ramp location data mapped onto Google Street View images (Stage 1) and trains a modified ConvNeXt V2 model on this dataset for better detection accuracy (Stage 2).

Result: The generated dataset achieves 94.0% precision and 92.5% recall, and the detection model achieves 0.9236 AP, outperforming previous methods.

Conclusion: RampNet provides the first large-scale and high-quality curb ramp dataset, along with a benchmark and a superior detection model, setting a new standard for urban accessibility research.

Abstract: Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.

</details>


### [95] [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/abs/2508.09811)
*Jinxi Li,Ziyang Song,Bo Yang*

Main category: cs.CV

TL;DR: This paper introduces TRACE, a framework that models the physics of dynamic 3D scenes using multi-view videos without relying on human labels.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing methods that struggle to learn complex motion physics or require additional labels, such as object types or masks.

Method: Propose TRACE, which treats 3D points as rigid particles with size and orientation, learning a translation-rotation dynamics system to estimate physical parameters governing their motion.

Result: Demonstrates strong performance over baselines on future frame extrapolation tasks using three existing datasets and a new challenging synthetic dataset.

Conclusion: TRACE effectively models complex 3D motion physics from dynamic multi-view videos and enables segmentation of multiple objects or parts via learned physical parameters.

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical
information just from dynamic multi-view videos in the absence of any human
labels. By leveraging physics-informed losses as soft constraints or
integrating simple physics models into neural nets, existing works often fail
to learn complex motion physics, or doing so requires additional labels such as
object types or masks. We propose a new framework named TRACE to model the
motion physics of complex dynamic 3D scenes. The key novelty of our method is
that, by formulating each 3D point as a rigid particle with size and
orientation in space, we directly learn a translation rotation dynamics system
for each particle, explicitly estimating a complete set of physical parameters
to govern the particle's motion over time. Extensive experiments on three
existing dynamic datasets and one newly created challenging synthetic datasets
demonstrate the extraordinary performance of our method over baselines in the
task of future frame extrapolation. A nice property of our framework is that
multiple objects or parts can be easily segmented just by clustering the
learned physical parameters.

</details>


### [96] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: The paper proposes RayletDF, a method to reconstruct 3D surfaces from raw point clouds or estimated 3D Gaussians using a novel raylet distance field approach.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing computationally-intensive methods for explicit 3D surface rendering, and to generalize surface reconstruction across varied datasets.

Method: The RayletDF pipeline has three modules: raylet feature extractor, raylet distance field predictor, and multi-raylet blender to extract, predict, and aggregate features for 3D surface reconstruction.

Result: The method outperforms existing techniques on multiple real-world datasets, achieving precise surface reconstruction and demonstrating remarkable generalization on unseen data.

Conclusion: RayletDF is an effective and generalizable method for 3D surface reconstruction that operates efficiently in a single-forward pass, setting a new standard for point cloud and Gaussian evaluations.

Abstract: In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.

</details>


### [97] [What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset](https://arxiv.org/abs/2508.09428)
*Yuxiao Wang,Yu Lei,Wolin Liang,Weiying Xue,Zhenao Wei,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: The paper introduces a novel task to simultaneously predict action semantics and body-part contact regions in visual contexts using the PaIR-Net framework.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to jointly model action semantics and spatial contextualization in scenes, limiting comprehensive action understanding.

Method: The paper presents PaIR-Net with three core modules: CPAM for identifying contact-relevant body parts, PGCS for pixel-wise segmentation, and IIM for global interaction integration.

Result: PaIR-Net outperforms baseline methods on benchmarks, and ablation studies validate the contributions of its components.

Conclusion: PaIR-Net improves action understanding by jointly modeling semantics and spatial context; code and dataset will be made publicly available.

Abstract: People control their bodies to establish contact with the environment. To
comprehensively understand actions across diverse visual contexts, it is
essential to simultaneously consider \textbf{what} action is occurring and
\textbf{where} it is happening. Current methodologies, however, often
inadequately capture this duality, typically failing to jointly model both
action semantics and their spatial contextualization within scenes. To bridge
this gap, we introduce a novel vision task that simultaneously predicts
high-level action semantics and fine-grained body-part contact regions. Our
proposed framework, PaIR-Net, comprises three key components: the Contact Prior
Aware Module (CPAM) for identifying contact-relevant body parts, the
Prior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and
the Interaction Inference Module (IIM) responsible for integrating global
interaction relationships. To facilitate this task, we present PaIR (Part-aware
Interaction Representation), a comprehensive dataset containing 13,979 images
that encompass 654 actions, 80 object categories, and 17 body parts.
Experimental evaluation demonstrates that PaIR-Net significantly outperforms
baseline approaches, while ablation studies confirm the efficacy of each
architectural component. The code and dataset will be released upon
publication.

</details>


### [98] [MPT: Motion Prompt Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2508.09446)
*Jiateng Liu,Hengcan Shi,Feng Chen,Zhiwen Shao,Yaonan Wang,Jianfei Cai,Wenming Zheng*

Main category: cs.CV

TL;DR: The paper introduces a novel method, Motion Prompt Tuning (MPT), to adapt large pre-trained models for micro-expression recognition (MER), addressing challenges like limited training samples and the need to capture subtle facial movements.


<details>
  <summary>Details</summary>
Motivation: Micro-expression recognition is important in fields such as medical diagnosis and lie detection but faces challenges like limited labeled datasets due to the expertise required. Current large pre-training models fail to capture transient and subtle facial movements crucial for MER.

Method: The authors propose Motion Prompt Tuning (MPT), which includes methods such as motion magnification and Gaussian tokenization to extract subtle motion prompts. A group adapter is also designed and inserted into the pre-trained models to adapt them to the MER domain.

Result: Extensive experiments on three common MER datasets show that the proposed MPT method outperforms state-of-the-art techniques, demonstrating its effectiveness.

Conclusion: Motion Prompt Tuning proves to be an effective approach for adapting pre-trained models to address challenges in micro-expression recognition, providing better results than existing methods.

Abstract: Micro-expression recognition (MER) is crucial in the affective computing
field due to its wide application in medical diagnosis, lie detection, and
criminal investigation. Despite its significance, obtaining micro-expression
(ME) annotations is challenging due to the expertise required from
psychological professionals. Consequently, ME datasets often suffer from a
scarcity of training samples, severely constraining the learning of MER models.
While current large pre-training models (LMs) offer general and discriminative
representations, their direct application to MER is hindered by an inability to
capture transitory and subtle facial movements-essential elements for effective
MER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to
adapting LMs for MER, representing a pioneering method for subtle motion prompt
tuning. Particularly, we introduce motion prompt generation, including motion
magnification and Gaussian tokenization, to extract subtle motions as prompts
for LMs. Additionally, a group adapter is carefully designed and inserted into
the LM to enhance it in the target MER domain, facilitating a more nuanced
distinction of ME representation. Furthermore, extensive experiments conducted
on three widely used MER datasets demonstrate that our proposed MPT
consistently surpasses state-of-the-art approaches and verifies its
effectiveness.

</details>


### [99] [RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration](https://arxiv.org/abs/2508.09449)
*Jiaqi Yan,Shuning Xu,Xiangyu Chen,Dell Zhang,Jie Tang,Gangshan Wu,Jie Liu*

Main category: cs.CV

TL;DR: The paper introduces a new paradigm called Retrieval-Augmented Super Resolution (RASR), which addresses the limitations of Reference-based Super Resolution (RefSR) by automatically retrieving semantically relevant high-resolution images instead of relying on manual image pairing.


<details>
  <summary>Details</summary>
Motivation: Existing RefSR methods depend on manually curated target-reference image pairs, which are impractical for real-world scenarios. The paper aims to make RefSR scalable and flexible for real-world applications.

Method: RASR automatically retrieves semantically suitable references from a database using a semantic reference retriever and employs a diffusion-based RefSR generator enhanced with semantic conditioning. A benchmark dataset, RASR-Flickr30, is introduced to support this research.

Result: RASRNet improves over Single Image Super Resolution (SISR) baselines, showing a notable increase of +0.38 dB PSNR and reduction of -0.0131 LPIPS, with better texture realism demonstrated through experiments on the RASR-Flickr30 dataset.

Conclusion: Retrieval augmentation represents a novel and practical approach to advancing RefSR technology, bridging the gap between academic research and real-world application scenarios.

Abstract: Reference-based Super Resolution (RefSR) improves upon Single Image Super
Resolution (SISR) by leveraging high-quality reference images to enhance
texture fidelity and visual realism. However, a critical limitation of existing
RefSR approaches is their reliance on manually curated target-reference image
pairs, which severely constrains their practicality in real-world scenarios. To
overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new
and practical RefSR paradigm that automatically retrieves semantically relevant
high-resolution images from a reference database given only a low-quality
input. This enables scalable and flexible RefSR in realistic use cases, such as
enhancing mobile photos taken in environments like zoos or museums, where
category-specific reference data (e.g., animals, artworks) can be readily
collected or pre-curated. To facilitate research in this direction, we
construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike
prior datasets with fixed target-reference pairs, RASR-Flickr30 provides
per-category reference databases to support open-world retrieval. We further
propose RASRNet, a strong baseline that combines a semantic reference retriever
with a diffusion-based RefSR generator. It retrieves relevant references based
on semantic similarity and employs a diffusion-based generator enhanced with
semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet
consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131
LPIPS, while generating more realistic textures. These findings highlight
retrieval augmentation as a promising direction to bridge the gap between
academic RefSR research and real-world applicability.

</details>


### [100] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: The paper introduces HyperKD, a knowledge distillation framework enabling the transfer of knowledge from simpler teacher models to build foundation models for hyperspectral imagery.


<details>
  <summary>Details</summary>
Motivation: Direct application of foundation models to hyperspectral remote sensing is challenging due to spectral discrepancies and limited data availability.

Method: HyperKD employs a reverse knowledge distillation approach using a simpler teacher model, spectral range-based channel alignment, spatial feature-guided masking, and a tailored loss function to adapt a foundational model for hyperspectral imagery.

Result: HyperKD significantly enhances reconstruction fidelity and improves performance on tasks like land cover classification, crop type identification, and soil organic carbon prediction.

Conclusion: The proposed HyperKD framework successfully bridges spectral domain gaps, paving the way for applying foundation models in hyperspectral remote sensing applications.

Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled
datasets, has emerged as an effective approach in creating adaptable and
reusable architectures that can be leveraged for various downstream tasks using
satellite observations. However, their direct application to hyperspectral
remote sensing remains challenging due to inherent spectral disparities and the
scarcity of available observations. In this work, we present HyperKD, a novel
knowledge distillation framework that enables transferring learned
representations from a teacher model into a student model for effective
development of a foundation model on hyperspectral images. Unlike typical
knowledge distillation frameworks, which use a complex teacher to guide a
simpler student, HyperKD enables an inverse form of knowledge transfer across
different types of spectral data, guided by a simpler teacher model. Building
upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi
foundational model into a student tailored for EnMAP hyperspectral imagery.
HyperKD addresses the inverse domain adaptation problem with spectral gaps by
introducing a feature-based strategy that includes spectral range-based channel
alignment, spatial feature-guided masking, and an enhanced loss function
tailored for hyperspectral images. HyperKD bridges the substantial spectral
domain gap, enabling the effective use of pretrained foundation models for
geospatial applications. Extensive experiments show that HyperKD significantly
improves representation learning in MAEs, leading to enhanced reconstruction
fidelity and more robust performance on downstream tasks such as land cover
classification, crop type identification, and soil organic carbon prediction,
underpinning the potential of knowledge distillation frameworks in remote
sensing analytics with hyperspectral imagery.

</details>


### [101] [Animate-X++: Universal Character Image Animation with Dynamic Backgrounds](https://arxiv.org/abs/2508.09454)
*Shuai Tan,Biao Gong,Zhuoxin Liu,Yan Wang,Xi Chen,Yifan Feng,Hengshuang Zhao*

Main category: cs.CV

TL;DR: The paper introduces Animate-X++, a universal framework for character image animation, tackling challenges like applicability to anthropomorphic characters and dynamic backgrounds.


<details>
  <summary>Details</summary>
Motivation: Existing methods limitedly apply to human figures and fail to handle anthropomorphic characters or dynamic backgrounds in animation.

Method: They propose Animate-X++, with the Pose Indicator for better motion representation and a multi-task training strategy for dynamic background realism.

Result: The model successfully animates anthropomorphic characters with dynamic backgrounds, outperforming previous methods.

Conclusion: Animate-X++ addresses key limitations, providing a robust solution for high-quality, realistic character animations.

Abstract: Character image animation, which generates high-quality videos from a
reference image and target pose sequence, has seen significant progress in
recent years. However, most existing methods only apply to human figures, which
usually do not generalize well on anthropomorphic characters commonly used in
industries like gaming and entertainment. Furthermore, previous methods could
only generate videos with static backgrounds, which limits the realism of the
videos. For the first challenge, our in-depth analysis suggests to attribute
this limitation to their insufficient modeling of motion, which is unable to
comprehend the movement pattern of the driving video, thus imposing a pose
sequence rigidly onto the target character. To this end, this paper proposes
Animate-X++, a universal animation framework based on DiT for various character
types, including anthropomorphic characters. To enhance motion representation,
we introduce the Pose Indicator, which captures comprehensive motion pattern
from the driving video through both implicit and explicit manner. The former
leverages CLIP visual features of a driving video to extract its gist of
motion, like the overall movement pattern and temporal relations among motions,
while the latter strengthens the generalization of DiT by simulating possible
inputs in advance that may arise during inference. For the second challenge, we
introduce a multi-task training strategy that jointly trains the animation and
TI2V tasks. Combined with the proposed partial parameter training, this
approach achieves not only character animation but also text-driven background
dynamics, making the videos more realistic. Moreover, we introduce a new
Animated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of
Animate-X++ on universal and widely applicable animation images. Extensive
experiments demonstrate the superiority and effectiveness of Animate-X++.

</details>


### [102] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: The paper introduces a backdoor attack method called IAG for Vision-Language Models (VLMs) that manipulates visual grounding behavior, ensuring stealthiness and effectiveness, with significant empirical results.


<details>
  <summary>Details</summary>
Motivation: Security concerns in visual grounding tasks for VLMs, particularly in the context of backdoor attacks, remain underexplored.

Method: Developed an input-aware backdoor attack (IAG) that uses a text-conditional U-Net to embed attack targets into images, minimizing discrepancies between poisoned and clean images, and proposed adaptive trigger generation and unified data generation methods.

Result: IAG achieves over 65% ASR@0.5 on InternVL-2.5-8B across various test sets, manipulates other models like Ferret-7B with minimal decrease in accuracy on clean samples, and showcases robustness and transferability.

Conclusion: The study demonstrates the feasibility and effectiveness of the IAG attack while also exploring robustness, transferability, and potential defenses through extensive experiments.

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

</details>


### [103] [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)
*Wen Huang,Jiarui Yang,Tao Dai,Jiawei Li,Shaoxiong Zhan,Bin Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: RelayFormer introduces a unified framework for detecting tampered regions in images and videos, achieving scalability and strong generalization.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current methods for identifying tampered areas in visual content, which lack cross-modal generalization and struggle with efficiently handling high-resolution images and long-duration videos.

Method: RelayFormer utilizes a modular architecture with Global-Local Relay Attention (GLoRA) and lightweight adaptation modules for Transformer backbones, alongside a query-based mask decoder for efficient video inference.

Result: Experiments across benchmarks show RelayFormer achieves state-of-the-art performance in visual manipulation localization, setting new scalability and modality-agnostic standards.

Conclusion: RelayFormer offers a scalable, unified solution for visual manipulation localization in images and videos, with robust generalization and compatibility with existing Transformer models.

Abstract: Visual manipulation localization (VML) -- across both images and videos -- is
a crucial task in digital forensics that involves identifying tampered regions
in visual content. However, existing methods often lack cross-modal
generalization and struggle to handle high-resolution or long-duration inputs
efficiently.
  We propose RelayFormer, a unified and modular architecture for visual
manipulation localization across images and videos. By leveraging flexible
local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables
scalable, resolution-agnostic processing with strong generalization. Our
framework integrates seamlessly with existing Transformer-based backbones, such
as ViT and SegFormer, via lightweight adaptation modules that require only
minimal architectural changes, ensuring compatibility without disrupting
pretrained representations.
  Furthermore, we design a lightweight, query-based mask decoder that supports
one-shot inference across video sequences with linear complexity. Extensive
experiments across multiple benchmarks demonstrate that our approach achieves
state-of-the-art localization performance, setting a new baseline for scalable
and modality-agnostic VML. Code is available at:
https://github.com/WenOOI/RelayFormer.

</details>


### [104] [Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy](https://arxiv.org/abs/2508.09461)
*Hao Yu,Rupayan Mallick,Margrit Betke,Sarah Adel Bargal*

Main category: cs.CV

TL;DR: The paper introduces GEN-AFFECT, a framework for creating expressive and identity-consistent 2D avatars across diverse facial expressions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with accurately capturing facial expressions and maintaining identity consistency across expressions, limiting their effectiveness in personalized avatar generation.

Method: GEN-AFFECT uses a multimodal diffusion transformer conditioned on extracted identity-expression representations and employs consistent attention during inference to ensure identity preservation and expression fidelity.

Result: The framework achieves better performance compared to state-of-the-art methods in terms of expression accuracy, identity preservation, and cross-expression consistency.

Conclusion: GEN-AFFECT successfully addresses limitations in avatar generation by offering a solution that combines expression accuracy with identity consistency, making it suitable for diverse applications.

Abstract: Different forms of customized 2D avatars are widely used in gaming
applications, virtual communication, education, and content creation. However,
existing approaches often fail to capture fine-grained facial expressions and
struggle to preserve identity across different expressions. We propose
GEN-AFFECT, a novel framework for personalized avatar generation that generates
expressive and identity-consistent avatars with a diverse set of facial
expressions. Our framework proposes conditioning a multimodal diffusion
transformer on an extracted identity-expression representation. This enables
identity preservation and representation of a wide range of facial expressions.
GEN-AFFECT additionally employs consistent attention at inference for
information sharing across the set of generated expressions, enabling the
generation process to maintain identity consistency over the array of generated
fine-grained expressions. GEN-AFFECT demonstrates superior performance compared
to previous state-of-the-art methods on the basis of the accuracy of the
generated expressions, the preservation of the identity and the consistency of
the target identity across an array of fine-grained facial expressions.

</details>


### [105] [CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios](https://arxiv.org/abs/2508.09470)
*Jialei Xu,Zizhuang Wei,Weikang You,Linyun Li,Weijian Sun*

Main category: cs.CV

TL;DR: CitySeg is a foundation model enabling city-scale semantic segmentation of 3D point clouds, integrating text modality for open vocabulary and zero-shot inference.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limited generalization capability of existing models caused by constrained data scale and domain gaps in city-scale point cloud semantic segmentation.

Method: CitySeg utilizes customized data preprocessing rules, a local-global cross-attention network, a hierarchical classification strategy, a graph encoder, and a two-stage training approach to consolidate data labels and enhance feature separability.

Result: CitySeg achieves state-of-the-art performance on nine benchmarks and for the first time enables zero-shot generalization for city-scale point clouds without visual cues.

Conclusion: CitySeg effectively tackles existing challenges in UAV-based 3D perception systems, offering robust generalization and segmentation capabilities in diverse urban environments.

Abstract: Semantic segmentation of city-scale point clouds is a critical technology for
Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification
of 3D points without relying on any visual information to achieve comprehensive
3D understanding. However, existing models are frequently constrained by the
limited scale of 3D data and the domain gap between datasets, which lead to
reduced generalization capability. To address these challenges, we propose
CitySeg, a foundation model for city-scale point cloud semantic segmentation
that incorporates text modality to achieve open vocabulary segmentation and
zero-shot inference. Specifically, in order to mitigate the issue of
non-uniform data distribution across multiple domains, we customize the data
preprocessing rules, and propose a local-global cross-attention network to
enhance the perception capabilities of point networks in UAV scenarios. To
resolve semantic label discrepancies across datasets, we introduce a
hierarchical classification strategy. A hierarchical graph established
according to the data annotation rules consolidates the data labels, and the
graph encoder is used to model the hierarchical relationships between
categories. In addition, we propose a two-stage training strategy and employ
hinge loss to increase the feature separability of subcategories. Experimental
results demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)
performance on nine closed-set benchmarks, significantly outperforming existing
approaches. Moreover, for the first time, CitySeg enables zero-shot
generalization in city-scale point cloud scenarios without relying on visual
information.

</details>


### [106] [Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection](https://arxiv.org/abs/2508.09475)
*Shibo Yao,Renshuai Tao,Xiaolong Zheng,Chao Liang,Chunjie Zhang*

Main category: cs.CV

TL;DR: The paper introduces FTNet, a novel method for real-world few-shot deepfake detection, using only one fake sample for classification without requiring training or parameter updates.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection models fail to adapt well to unseen samples despite available new data, highlighting the need for few-shot learning approaches tailored to the realism of image data.

Method: FTNet leverages a single fake sample from the evaluation dataset, compares new samples to known fake and real data, and classifies based on the nearest neighbor approach, all without additional training.

Result: FTNet demonstrated state-of-the-art performance, achieving an average improvement of 8.7% across 29 generative models over existing methods.

Conclusion: The work redefines real-world deepfake detection by emphasizing the potential of leveraging failed samples and few-shot learning strategies for significant improvements in generalization.

Abstract: Recent deepfake detection studies often treat unseen sample detection as a
``zero-shot" task, training on images generated by known models but
generalizing to unknown ones. A key real-world challenge arises when a model
performs poorly on unknown samples, yet these samples remain available for
analysis. This highlights that it should be approached as a ``few-shot" task,
where effectively utilizing a small number of samples can lead to significant
improvement. Unlike typical few-shot tasks focused on semantic understanding,
deepfake detection prioritizes image realism, which closely mirrors real-world
distributions. In this work, we propose the Few-shot Training-free Network
(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet
differs from traditional methods that rely on large-scale known data for
training. Instead, FTNet uses only one fake samplefrom an evaluation set,
mimicking the scenario where new samples emerge in the real world and can be
gathered for use, without any training or parameter updates. During evaluation,
each test sample is compared to the known fake and real samples, and it is
classified based on the category of the nearest sample. We conduct a
comprehensive analysis of AI-generated images from 29 different generative
models and achieve a new SoTA performance, with an average improvement of 8.7\%
compared to existing methods. This work introduces a fresh perspective on
real-world deepfake detection: when the model struggles to generalize on a
few-shot sample, leveraging the failed samples leads to better performance.

</details>


### [107] [From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts](https://arxiv.org/abs/2508.09476)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Chengming Xu,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: This paper introduces MoFE (Mixture of Facial Experts) combined with a tailored LFA dataset to enhance facial identity preservation in video generation under large facial angles, significantly outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with maintaining identity preservation under large facial angles, hindered by insufficient mechanisms in DiT structures and lacking datasets with adequate facial angle diversity.

Method: The authors propose MoFE, which combines three specialized facial experts (identity, semantic, and detail) and develop a data processing pipeline to curate the Large Face Angles (LFA) dataset with improved Face Constraints and Identity Consistency.

Result: The proposed model and LFA dataset significantly outperform existing methods on metrics such as face similarity, face FID, and CLIP semantic alignment in experimental benchmarks.

Conclusion: This paper successfully addresses challenges in facial identity preservation by introducing MoFE and the LFA dataset, paving the way for enhanced video generation models under large facial angles, with public release of both the code and dataset.

Abstract: Current video generation models struggle with identity preservation under
large facial angles, primarily facing two challenges: the difficulty in
exploring an effective mechanism to integrate identity features into DiT
structure, and the lack of targeted coverage of large facial angles in existing
open-source video datasets. To address these, we present two key innovations.
First, we introduce a Mixture of Facial Experts (MoFE) that dynamically
combines complementary cues from three specialized experts, each designed to
capture distinct but mutually reinforcing aspects of facial attributes. The
identity expert captures cross-pose identity-sensitive features, the semantic
expert extracts high-level visual semantxics, and the detail expert preserves
pixel-level features (e.g., skin texture, color gradients). Furthermore, to
mitigate dataset limitations, we have tailored a data processing pipeline
centered on two key aspects: Face Constraints and Identity Consistency. Face
Constraints ensure facial angle diversity and a high proportion of facial
regions, while Identity Consistency preserves coherent person-specific features
across temporal sequences, collectively addressing the scarcity of large facial
angles and identity-stable training data in existing datasets. Leveraging this
pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from
existing open-source human video datasets, comprising 460K video clips with
annotated facial angles. Experimental results on the LFA benchmark demonstrate
that our method, empowered by the LFA dataset, significantly outperforms prior
SOTA methods in face similarity, face FID, and CLIP semantic alignment. The
code and dataset will be made publicly available at
https://github.com/rain152/LFA-Video-Generation.

</details>


### [108] [CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection](https://arxiv.org/abs/2508.09477)
*Zhipeng Yuan,Kai Wang,Weize Quan,Dong-Ming Yan,Tieru Wu*

Main category: cs.CV

TL;DR: This paper introduces a novel AI-generated image detector based on anomaly detection, using proxy images and unsupervised learning to improve generalizability to unseen generative models.


<details>
  <summary>Details</summary>
Motivation: The visual quality of AI-generated images is approaching natural images, leading to security concerns and limited detector performance for images from unseen generative models.

Method: The method utilizes anomaly detection by training a normalizing flow-like unsupervised model on proxy images created through spectral modification of natural images. It leverages the pre-trained CLIP encoder as the feature extractor.

Result: The proposed method effectively detects AI-generated images from various generators, as demonstrated in extensive experiments.

Conclusion: The model provides a robust detection framework without requiring access to AI-generated images during training, enhancing its ability to generalize across different generative models.

Abstract: With the rapid advancement of AI generative models, the visual quality of
AI-generated images (AIIs) has become increasingly close to natural images,
which inevitably raises security concerns. Most AII detectors often employ the
conventional image classification pipeline with natural images and AIIs
(generated by a generative model), which can result in limited detection
performance for AIIs from unseen generative models. To solve this, we proposed
a universal AI-generated image detector from the perspective of anomaly
detection. Our discriminator does not need to access any AIIs and learn a
generalizable representation with unsupervised learning. Specifically, we use
the pre-trained CLIP encoder as the feature extractor and design a normalizing
flow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by
applying a spectral modification operation on natural images, are used for
training. Our models are trained by minimizing the likelihood of proxy images,
optionally combined with maximizing the likelihood of natural images. Extensive
experiments demonstrate the effectiveness of our method on AIIs produced by
various image generators.

</details>


### [109] [GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs](https://arxiv.org/abs/2508.09478)
*Moinak Bhattacharya,Gagandeep Singh,Shubham Jain,Prateek Prasanna*

Main category: cs.CV

TL;DR: GazeLT is a deep learning framework that uses radiologist eye gaze data to improve long-tailed disease classification by integrating and disintegrating the visual attention dynamics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in long-tailed disease classification wherein rare disease classes are often underrepresented and harder for automated systems to classify accurately.

Method: GazeLT introduces an approach that incorporates the temporal dynamics of eye gaze patterns through an integration-disintegration mechanism to refine attention modeling in deep learning systems.

Result: The proposed framework, GazeLT, demonstrated improvements in classification accuracy on two large datasets (NIH-CXR-LT and MIMIC-CXR-LT), outperforming established methods by 4.1% and 21.7%, respectively.

Conclusion: Integrating radiologist visual attention into deep learning models enhances long-tailed disease classification, emphasizing the value of using expert visual strategies in medical imaging applications.

Abstract: In this work, we present GazeLT, a human visual attention
integration-disintegration approach for long-tailed disease classification. A
radiologist's eye gaze has distinct patterns that capture both fine-grained and
coarser level disease related information. While interpreting an image, a
radiologist's attention varies throughout the duration; it is critical to
incorporate this into a deep learning framework to improve automated image
interpretation. Another important aspect of visual attention is that apart from
looking at major/obvious disease patterns, experts also look at
minor/incidental findings (few of these constituting long-tailed classes)
during the course of image interpretation. GazeLT harnesses the temporal aspect
of the visual search process, via an integration and disintegration mechanism,
to improve long-tailed disease classification. We show the efficacy of GazeLT
on two publicly available datasets for long-tailed disease classification,
namely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.
GazeLT outperforms the best long-tailed loss by 4.1% and the visual
attention-based baseline by 21.7% in average accuracy metrics for these
datasets. Our code is available at https://github.com/lordmoinak1/gazelt.

</details>


### [110] [SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images](https://arxiv.org/abs/2508.09479)
*Xuejun Huang,Xinyi Liu,Yi Wan,Zhi Zheng,Bin Zhang,Mingtao Xiong,Yingying Pei,Yongjun Zhang*

Main category: cs.CV

TL;DR: The paper introduces SkySplat, a self-supervised framework for reconstructing 3D scenes from sparse-view satellite images, which enhances efficiency and accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstruction methods struggle with sparse-view satellite image data due to issues like limited geometric constraints, transient objects, and radiometric inconsistencies, necessitating a more effective approach.

Method: SkySplat integrates the rational polynomial coefficient (RPC) model into a self-supervised 3D Gaussian Splatting (3DGS) pipeline, using RGB images and relative height supervision while introducing a Cross-Self Consistency Module (CSCM) and multi-view consistency aggregation strategy to improve accuracy and robustness.

Result: SkySplat achieves an 86x speedup over EOGS while reducing MAE from 13.18m to 1.80m on the DFC19 dataset, and demonstrates strong generalization capabilities on other benchmarks like MVS3D.

Conclusion: SkySplat effectively addresses the limitations of existing 3D reconstruction methods for satellite images by integrating RPC models and leveraging novel consistency frameworks, achieving higher accuracy and significant speed gains.

Abstract: Three-dimensional scene reconstruction from sparse-view satellite images is a
long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its
variants have recently attracted attention for its high efficiency, existing
methods remain unsuitable for satellite images due to incompatibility with
rational polynomial coefficient (RPC) models and limited generalization
capability. Recent advances in generalizable 3DGS approaches show potential,
but they perform poorly on multi-temporal sparse satellite images due to
limited geometric constraints, transient objects, and radiometric
inconsistencies. To address these limitations, we propose SkySplat, a novel
self-supervised framework that integrates the RPC model into the generalizable
3DGS pipeline, enabling more effective use of sparse geometric cues for
improved reconstruction. SkySplat relies only on RGB images and
radiometric-robust relative height supervision, thereby eliminating the need
for ground-truth height maps. Key components include a Cross-Self Consistency
Module (CSCM), which mitigates transient object interference via
consistency-based masking, and a multi-view consistency aggregation strategy
that refines reconstruction results. Compared to per-scene optimization
methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.
It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to
1.80 m on the DFC19 dataset significantly, and demonstrates strong
cross-dataset generalization on the MVS3D benchmark.

</details>


### [111] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: Video-EM introduces a training-free framework leveraging principles of human episodic memory to enhance Video-LLMs' understanding of long-form videos for question answering, outperforming baselines while using fewer frames.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing Video-LLMs in handling lengthy videos, particularly their reliance on static keyframes that fail to capture temporal dynamics and spatial relationships.

Method: The framework models keyframes as temporally ordered episodic events and leverages chain-of-thought (CoT) reasoning in LLMs to iteratively select a minimal, informative subset for question answering.

Result: Extensive evaluations show Video-EM delivers 4-9% performance improvements over baselines in benchmarks like Video-MME and LVBench, while utilizing fewer frames.

Conclusion: Video-EM demonstrates robust and contextually-aware reasoning capabilities, highlighting its efficiency and effectiveness in long-form video understanding.

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [112] [SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection](https://arxiv.org/abs/2508.09487)
*Ju Yeon Kang,Jaehong Park,Semin Kim,Ji Won Yoon,Nam Soo Kim*

Main category: cs.CV

TL;DR: The paper introduces a detection method called Semantic-Aware Reconstruction Error (SARE) to identify AI-generated images based on their semantic alignment with captions, showing strong performance even for unseen generative models.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of detecting AI-generated images from unseen or out-of-distribution (OOD) generative models, as traditional detection methods struggle due to reliance on model-specific artifacts.

Method: The proposed method, SARE, measures the semantic differences between an image and its caption-guided reconstruction. The hypothesis is that real images exhibit greater semantic shifts due to their complex content not fully captured by captions, while fake images show minimal semantic changes due to high alignment with their captions.

Result: The method shows strong generalization capabilities, outperforming existing detection methods on benchmarks like GenImage and CommunityForensics.

Conclusion: SARE provides a robust, generalizable approach to detecting AI-generated images by leveraging semantic differences, making it effective across different generative models.

Abstract: Recently, diffusion-generated image detection has gained increasing
attention, as the rapid advancement of diffusion models has raised serious
concerns about their potential misuse. While existing detection methods have
achieved promising results, their performance often degrades significantly when
facing fake images from unseen, out-of-distribution (OOD) generative models,
since they primarily rely on model-specific artifacts. To address this
limitation, we explore a fundamental property commonly observed in fake images.
Motivated by the observation that fake images tend to exhibit higher similarity
to their captions than real images, we propose a novel representation, namely
Semantic-Aware Reconstruction Error (SARE), that measures the semantic
difference between an image and its caption-guided reconstruction. The
hypothesis behind SARE is that real images, whose captions often fail to fully
capture their complex visual content, may undergo noticeable semantic shifts
during the caption-guided reconstruction process. In contrast, fake images,
which closely align with their captions, show minimal semantic changes. By
quantifying these semantic shifts, SARE can be utilized as a discriminative
feature for robust detection across diverse generative models. We empirically
demonstrate that the proposed method exhibits strong generalization,
outperforming existing baselines on benchmarks including GenImage and
CommunityForensics.

</details>


### [113] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: The paper presents CWFBind, a docking method using local curvature features for protein-ligand binding prediction, enhancing accuracy, speed, and geometric representation.


<details>
  <summary>Details</summary>
Motivation: Traditional docking methods, including deep learning approaches, often neglect geometric information, leading to inaccuracies in binding predictions.

Method: CWFBind incorporates local curvature descriptors, degree-aware weighting in message passing, and strategies for better pocket prediction including an enhanced loss function.

Result: CWFBind delivers competitive performance on docking benchmarks, balancing accuracy and efficiency.

Conclusion: CWFBind demonstrates an improved docking approach by integrating geometric features, offering advancements in rational drug design.

Abstract: Accurately predicting the binding conformation of small-molecule ligands to
protein targets is a critical step in rational drug design. Although recent
deep learning-based docking surpasses traditional methods in speed and
accuracy, many approaches rely on graph representations and language
model-inspired encoders while neglecting critical geometric information,
resulting in inaccurate pocket localization and unrealistic binding
conformations. In this study, we introduce CWFBind, a weighted, fast, and
accurate docking method based on local curvature features. Specifically, we
integrate local curvature descriptors during the feature extraction phase to
enrich the geometric representation of both proteins and ligands, complementing
existing chemical, sequence, and structural features. Furthermore, we embed
degree-aware weighting mechanisms into the message passing process, enhancing
the model's ability to capture spatial structural distinctions and interaction
strengths. To address the class imbalance challenge in pocket prediction,
CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced
loss function, facilitating more precise identification of binding regions and
key residues. Comprehensive experimental evaluations demonstrate that CWFBind
achieves competitive performance across multiple docking benchmarks, offering a
balanced trade-off between accuracy and efficiency.

</details>


### [114] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: This paper discusses the development of a new GAN variant to generate feature-rich, high-resolution images for Indian Sign Language, coupled with the release of a large dataset.


<details>
  <summary>Details</summary>
Motivation: Facilitating communication between the hard-of-hearing and hearing individuals by improving sign language image generation, as current methods are limited.

Method: Combining ProGAN and SAGAN architectures to create an attention-based GAN model capable of generating class-conditional high-quality Indian Sign Language images.

Result: The GAN model outperformed ProGAN in Inception Score (IS) and Fréchet Inception Distance (FID), with respective improvements of 3.2 and 30.12.

Conclusion: This research contributes to sign language communication by enhancing image generation quality and sharing a comprehensive dataset for Indian Sign Language.

Abstract: Sign language, which contains hand movements, facial expressions and bodily
gestures, is a significant medium for communicating with hard-of-hearing
people. A well-trained sign language community communicates easily, but those
who don't know sign language face significant challenges. Recognition and
generation are basic communication methods between hearing and hard-of-hearing
individuals. Despite progress in recognition, sign language generation still
needs to be explored. The Progressive Growing of Generative Adversarial Network
(ProGAN) excels at producing high-quality images, while the Self-Attention
Generative Adversarial Network (SAGAN) generates feature-rich images at medium
resolutions. Balancing resolution and detail is crucial for sign language image
generation. We are developing a Generative Adversarial Network (GAN) variant
that combines both models to generate feature-rich, high-resolution, and
class-conditional sign language images. Our modified Attention-based model
generates high-quality images of Indian Sign Language letters, numbers, and
words, outperforming the traditional ProGAN in Inception Score (IS) and
Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,
respectively. Additionally, we are publishing a large dataset incorporating
high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [115] [SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking](https://arxiv.org/abs/2508.09524)
*Yipei Wang,Shiyu Hu,Shukun Jia,Panxi Xu,Hongfei Ma,Yiping Ma,Jing Zhang,Xiaobo Lu,Xin Zhao*

Main category: cs.CV

TL;DR: The paper investigates Similar Object Interference (SOI)'s impact on Single Object Tracking (SOT) performance, introduces SOIBench for semantic guidance in tracking, and proposes a vision-language paradigm improving tracking outcomes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the bottleneck of Similar Object Interference (SOI) in Single Object Tracking (SOT), leveraging semantic guidance to improve tracking robustness.

Method: The authors conduct Online Interference Masking (OIM) experiments and propose SOIBench, combining multi-tracker collective judgment with multi-level annotations for semantic guidance. They introduce vision-language models as external cognitive engines for RGB trackers.

Result: Experiments demonstrate that eliminating SOI significantly boosts tracking performance (AUC up to 4.35). Existing Vision-Language Tracking (VLT) methods show limited improvements, while integrating vision-language models yields AUC gains of up to 0.93.

Conclusion: SOIBench standardizes evaluation in semantic cognitive tracking, promoting further research in overcoming SOI challenges. External cognitive engines offer promising advancements over current tracking methods.

Abstract: In this paper, we present the first systematic investigation and
quantification of Similar Object Interference (SOI), a long-overlooked yet
critical bottleneck in Single Object Tracking (SOT). Through controlled Online
Interference Masking (OIM) experiments, we quantitatively demonstrate that
eliminating interference sources leads to substantial performance improvements
(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a
primary constraint for robust tracking and highlighting the feasibility of
external cognitive guidance. Building upon these insights, we adopt natural
language as a practical form of external guidance, and construct SOIBench-the
first semantic cognitive guidance benchmark specifically targeting SOI
challenges. It automatically mines SOI frames through multi-tracker collective
judgment and introduces a multi-level annotation protocol to generate precise
semantic guidance texts. Systematic evaluation on SOIBench reveals a striking
finding: existing vision-language tracking (VLT) methods fail to effectively
exploit semantic cognitive guidance, achieving only marginal improvements or
even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we
propose a novel paradigm employing large-scale vision-language models (VLM) as
external cognitive engines that can be seamlessly integrated into arbitrary RGB
trackers. This approach demonstrates substantial improvements under semantic
cognitive guidance (AUC gains up to 0.93), representing a significant
advancement over existing VLT methods. We hope SOIBench will serve as a
standardized evaluation platform to advance semantic cognitive tracking
research and contribute new insights to the tracking research community.

</details>


### [116] [Learning Spatial Decay for Vision Transformers](https://arxiv.org/abs/2508.09525)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: This paper proposes Spatial Decay Transformer (SDT), integrating dynamic spatial decay through a Context-Aware Gating mechanism to improve spatial attention in Vision Transformers.


<details>
  <summary>Details</summary>
Motivation: Existing ViTs lack spatial inductive biases, impacting performance on spatially-structured tasks. Static spatial decay limits adaptability to diverse visual content.

Method: The authors introduce a Context-Aware Gating mechanism within SDT that dynamically modulates spatial decay based on content relevance and spatial proximity using spatial-content fusion.

Result: Extensive experiments on ImageNet-1K tasks show consistent improvements over baseline models, validating the effectiveness of the proposed approach.

Conclusion: Data-dependent spatial decay establishes a novel paradigm for enhancing spatial attention in Vision Transformers, demonstrating potential for broader applications in computer vision.

Abstract: Vision Transformers (ViTs) have revolutionized computer vision, yet their
self-attention mechanism lacks explicit spatial inductive biases, leading to
suboptimal performance on spatially-structured tasks. Existing approaches
introduce data-independent spatial decay based on fixed distance metrics,
applying uniform attention weighting regardless of image content and limiting
adaptability to diverse visual scenarios. Inspired by recent advances in large
language models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)
significantly outperform static alternatives, we present the first successful
adaptation of data-dependent spatial decay to 2D vision transformers. We
introduce \textbf{Spatial Decay Transformer (SDT)}, featuring a novel
Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent
decay for patch interactions. Our approach learns to modulate spatial attention
based on both content relevance and spatial proximity. We address the
fundamental challenge of 1D-to-2D adaptation through a unified spatial-content
fusion framework that integrates manhattan distance-based spatial priors with
learned content representations. Extensive experiments on ImageNet-1K
classification and generation tasks demonstrate consistent improvements over
strong baselines. Our work establishes data-dependent spatial decay as a new
paradigm for enhancing spatial attention in vision transformers.

</details>


### [117] [Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing](https://arxiv.org/abs/2508.09528)
*Gang Qu,Ping Wang,Siming Zheng,Xin Yuan*

Main category: cs.CV

TL;DR: The paper introduces MEUNet, a novel network combining AKCS and MACA mechanisms, achieving superior image CS reconstruction performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing image compressed sensing methods struggle with incoherent measurement at the sensing phase and insufficient representation during reconstruction, impacting performance.

Method: The paper proposes AKCS for better measurement incoherence and MACA for learning implicit representations, combining both into MEUNet within an unfolding architecture.

Result: MEUNet shows improved reconstruction accuracy and faster inference compared to previous methods through extensive experiments.

Conclusion: Integrating AKCS and MACA within MEUNet addresses limitations in image compressed sensing and establishes a new benchmark in the field.

Abstract: Deep networks have achieved remarkable success in image compressed sensing
(CS) task, namely reconstructing a high-fidelity image from its compressed
measurement. However, existing works are deficient inincoherent compressed
measurement at sensing phase and implicit measurement representations at
reconstruction phase, limiting the overall performance. In this work, we answer
two questions: 1) how to improve the measurement incoherence for decreasing the
ill-posedness; 2) how to learn informative representations from measurements.
To this end, we propose a novel asymmetric Kronecker CS (AKCS) model and
theoretically present its better incoherence than previous Kronecker CS with
minimal complexity increase. Moreover, we reveal that the unfolding networks'
superiority over non-unfolding ones result from sufficient gradient descents,
called explicit measurement representations. We propose a measurement-aware
cross attention (MACA) mechanism to learn implicit measurement representations.
We integrate AKCS and MACA into widely-used unfolding architecture to get a
measurement-enhanced unfolding network (MEUNet). Extensive experiences
demonstrate that our MEUNet achieves state-of-the-art performance in
reconstruction accuracy and inference speed.

</details>


### [118] [COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection](https://arxiv.org/abs/2508.09533)
*Peiran Peng,Tingfa Xu,Liqiang Song,Mengqi Zhu,Yuqiang Fang,Jianan Li*

Main category: cs.CV

TL;DR: The study introduces COXNet, a specialized framework targeting tiny object detection in RGBT imagery under challenging drone scenarios. It incorporates cross-layer fusion, dynamic alignment, and a novel label assignment strategy, surpassing state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the poor performance of existing methods in detecting tiny objects from RGB-Thermal imagery, especially under complex conditions like spatial misalignment, low-light, and cluttered backgrounds.

Method: Proposes COXNet, which includes three key innovations: Cross-Layer Fusion Module for blending RGB and thermal features, a Dynamic Alignment and Scale Refinement module for correcting misalignments, and a GeoShape-based optimized label assignment strategy.

Result: COXNet achieves a 3.32% mAP improvement over state-of-the-art on the RGBTDronePerson dataset, proving efficacy in complex drone-based scenarios.

Conclusion: The framework effectively demonstrates the potential for enhanced RGBT tiny object detection, offering practical relevance for surveillance and other critical applications.

Abstract: Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is
a critical challenge in computer vision, particularly in surveillance, search
and rescue, and autonomous navigation. Drone-based scenarios exacerbate these
challenges due to spatial misalignment, low-light conditions, occlusion, and
cluttered backgrounds. Current methods struggle to leverage the complementary
information between visible and thermal modalities effectively. We propose
COXNet, a novel framework for RGBT tiny object detection, addressing these
issues through three core innovations: i) the Cross-Layer Fusion Module, fusing
high-level visible and low-level thermal features for enhanced semantic and
spatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,
correcting cross-modal spatial misalignments and preserving multi-scale
features; and iii) an optimized label assignment strategy using the GeoShape
Similarity Measure for better localization. COXNet achieves a 3.32\% mAP$_{50}$
improvement on the RGBTDronePerson dataset over state-of-the-art methods,
demonstrating its effectiveness for robust detection in complex environments.

</details>


### [119] [Iterative Volume Fusion for Asymmetric Stereo Matching](https://arxiv.org/abs/2508.09543)
*Yuanting Gao,Linghao Shen*

Main category: cs.CV

TL;DR: The paper addresses challenges in stereo matching caused by visual asymmetry in multi-camera systems, proposing an iterative two-phase network (IVF-AStereo) that improves matching accuracy in asymmetric conditions.


<details>
  <summary>Details</summary>
Motivation: Stereo matching algorithms often assume symmetrical visual properties, which is incompatible with modern multi-camera systems exhibiting asymmetry, posing challenges in cost volume computation.

Method: The researchers analyzed distortions in cost volume computation methods for asymmetric stereo, then designed a two-phase network, IVF-AStereo, that refines and fuses volumes for improved stereo matching performance.

Result: IVF-AStereo demonstrated robust performance under significant visual asymmetry and excelled in scenarios with resolution and color degradation through comparative experiments and ablation studies.

Conclusion: The proposed IVF-AStereo network effectively addresses stereo matching issues stemming from visual asymmetry, advancing the field for asymmetric camera systems.

Abstract: Stereo matching is vital in 3D computer vision, with most algorithms assuming
symmetric visual properties between binocular visions. However, the rise of
asymmetric multi-camera systems (e.g., tele-wide cameras) challenges this
assumption and complicates stereo matching. Visual asymmetry disrupts stereo
matching by affecting the crucial cost volume computation. To address this, we
explore the matching cost distribution of two established cost volume
construction methods in asymmetric stereo. We find that each cost volume
experiences distinct information distortion, indicating that both should be
comprehensively utilized to solve the issue. Based on this, we propose the
two-phase Iterative Volume Fusion network for Asymmetric Stereo matching
(IVF-AStereo). Initially, the aggregated concatenation volume refines the
correlation volume. Subsequently, both volumes are fused to enhance fine
details. Our method excels in asymmetric scenarios and shows robust performance
against significant visual asymmetry. Extensive comparative experiments on
benchmark datasets, along with ablation studies, confirm the effectiveness of
our approach in asymmetric stereo with resolution and color degradation.

</details>


### [120] [GoViG: Goal-Conditioned Visual Navigation Instruction Generation](https://arxiv.org/abs/2508.09547)
*Fengyi Wu,Yifei Dong,Zhi-Qi Cheng,Yilong Dai,Guangyu Chen,Hang Wang,Qi Dai,Alexander G. Hauptmann*

Main category: cs.CV

TL;DR: The paper presents GoViG, a method for generating navigation instructions directly from visual data using a multimodal large language model, improving adaptability to diverse environments.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous generation of navigation instructions from raw visual data, bypassing structured inputs like semantic annotations or maps, for better adaptability to unseen environments.

Method: The task is divided into visual forecasting and instruction generation, integrated into a large language model trained with specialized objectives to ensure accuracy and coherence.

Result: Empirical evaluations on the R2R-Goal dataset show significant improvements in BLEU-4, CIDEr scores, and cross-domain generalization compared to state-of-the-art methods.

Conclusion: GoViG demonstrates promise in generating linguistically coherent and spatially accurate navigation instructions from egocentric visuals, paving the way for advancements in visual navigation tasks.

Abstract: We introduce Goal-Conditioned Visual Navigation Instruction Generation
(GoViG), a new task that aims to autonomously generate precise and contextually
coherent navigation instructions solely from egocentric visual observations of
initial and goal states. Unlike conventional approaches that rely on structured
inputs such as semantic annotations or environmental maps, GoViG exclusively
leverages raw egocentric visual data, substantially improving its adaptability
to unseen and unstructured environments. Our method addresses this task by
decomposing it into two interconnected subtasks: (1) visual forecasting, which
predicts intermediate visual states bridging the initial and goal views; and
(2) instruction generation, which synthesizes linguistically coherent
instructions grounded in both observed and anticipated visuals. These subtasks
are integrated within an autoregressive multimodal large language model trained
with tailored objectives to ensure spatial accuracy and linguistic clarity.
Furthermore, we introduce two complementary multimodal reasoning strategies,
one-pass and interleaved reasoning, to mimic incremental human cognitive
processes during navigation. To evaluate our method, we propose the R2R-Goal
dataset, combining diverse synthetic and real-world trajectories. Empirical
results demonstrate significant improvements over state-of-the-art methods,
achieving superior BLEU-4 and CIDEr scores along with robust cross-domain
generalization.

</details>


### [121] [Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification](https://arxiv.org/abs/2508.09550)
*Haowen Wang,Guowei Zhang,Xiang Zhang,Zeyuan Chen,Haiyang Xu,Dou Hoon Kwark,Zhuowen Tu*

Main category: cs.CV

TL;DR: The paper investigates whether synthetic data from generative models can enhance image classification performance through data augmentation.


<details>
  <summary>Details</summary>
Motivation: To determine if generative data augmentation can yield comparable or improved classification performance compared to real data augmentation.

Method: Systematic experiments exploring distinctions between real and synthetic images, quantifying the amount and equivalence of synthetic data required.

Result: Closed-set synthetic data can effectively augment real data, with equivalent scales of synthetic images needed determined empirically.

Conclusion: Synthetic images can augment classification tasks effectively, but require increased scale to match real image augmentation impact.

Abstract: In this paper, we address a key scientific problem in machine learning: Given
a training set for an image classification task, can we train a generative
model on this dataset to enhance the classification performance? (i.e.,
closed-set generative data augmentation). We start by exploring the
distinctions and similarities between real images and closed-set synthetic
images generated by advanced generative models. Through extensive experiments,
we offer systematic insights into the effective use of closed-set synthetic
data for augmentation. Notably, we empirically determine the equivalent scale
of synthetic images needed for augmentation. In addition, we also show
quantitative equivalence between the real data augmentation and open-set
generative augmentation (generative models trained using data beyond the given
training set). While it aligns with the common intuition that real images are
generally preferred, our empirical formulation also offers a guideline to
quantify the increased scale of synthetic data augmentation required to achieve
comparable image classification performance. Our results on natural and medical
image datasets further illustrate how this effect varies with the baseline
training set size and the amount of synthetic data incorporated.

</details>


### [122] [Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning](https://arxiv.org/abs/2508.09555)
*Ahmet Öztel,İsmet Karaca*

Main category: cs.CV

TL;DR: A new iris recognition method using topological analysis achieves high accuracy and interpretability, outperforming CNNs.


<details>
  <summary>Details</summary>
Motivation: To develop a biometric identification method that is both interpretable and effective, addressing the lack of explainability in deep learning systems.

Method: Iris images are divided into grids, and topological invariants (Betti0, Betti1, ratios) are computed to form feature matrices. Various classifiers, like logistic regression, KNN, and SVM, as well as CNNs, are compared for performance.

Result: Topological features achieved 97.78% accuracy with logistic regression, surpassing CNN models (96.44%), demonstrating high performance and low variance.

Conclusion: The proposed method provides a compact, interpretable, and accurate alternative to deep learning. Its efficiency and robustness make it suitable for security-critical applications and other domains requiring explainability.

Abstract: Objective - This study presents a biometric identification method based on
topological invariants from 2D iris images, representing iris texture via
formally defined digital homology and evaluating classification performance.
  Methods - Each normalized iris image (48x482 pixels) is divided into grids
(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their
ratio using a recent algorithm for homology groups in 2D digital images. The
resulting invariants form a feature matrix used with logistic regression, KNN,
and SVM (with PCA and 100 randomized repetitions). A convolutional neural
network (CNN) is trained on raw images for comparison.
  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,
outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The
topological features showed high accuracy with low variance.
  Conclusion - This is the first use of topological invariants from formal
digital homology for iris recognition. The method offers a compact,
interpretable, and accurate alternative to deep learning, useful when
explainability or limited data is important. Beyond iris recognition, it can
apply to other biometrics, medical imaging, materials science, remote sensing,
and interpretable AI. It runs efficiently on CPU-only systems and produces
robust, explainable features valuable for security-critical domains.

</details>


### [123] [WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description](https://arxiv.org/abs/2508.09565)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: The paper introduces a new method, WEC-DG, for improving image quality under varying lighting conditions by using wavelet transforms and guided degradation correction, significantly outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of current multi-exposure correction methods in dealing with intra-class variability and anomalies caused by complex imaging conditions and single exposure levels.

Method: The paper proposes WEC-DG, which integrates a degradation descriptor through Exposure Consistency Alignment Module (ECAM) and uses wavelet transforms in the Exposure Restoration and Detail Reconstruction Module (EDRM) to sequentially process low and high-frequency information.

Result: The proposed method demonstrates notable performance improvements in enhancing image exposure correction across multiple datasets, outperforming existing approaches.

Conclusion: WEC-DG effectively addresses exposure anomalies, ensures precise light correction, and enhances detail recovery under complex imaging conditions, proving its effectiveness and practical applicability.

Abstract: Multi-exposure correction technology is essential for restoring images
affected by insufficient or excessive lighting, enhancing the visual experience
by improving brightness, contrast, and detail richness. However, current
multi-exposure correction methods often encounter challenges in addressing
intra-class variability caused by diverse lighting conditions, shooting
environments, and weather factors, particularly when processing images captured
at a single exposure level. To enhance the adaptability of these models under
complex imaging conditions, this paper proposes a Wavelet-based Exposure
Correction method with Degradation Guidance (WEC-DG). Specifically, we
introduce a degradation descriptor within the Exposure Consistency Alignment
Module (ECAM) at both ends of the processing pipeline to ensure exposure
consistency and achieve final alignment. This mechanism effectively addresses
miscorrected exposure anomalies caused by existing methods' failure to
recognize 'blurred' exposure degradation. Additionally, we investigate the
light-detail decoupling properties of the wavelet transform to design the
Exposure Restoration and Detail Reconstruction Module (EDRM), which processes
low-frequency information related to exposure enhancement before utilizing
high-frequency information as a prior guide for reconstructing spatial domain
details. This serial processing strategy guarantees precise light correction
and enhances detail recovery. Extensive experiments conducted on multiple
public datasets demonstrate that the proposed method outperforms existing
algorithms, achieving significant performance improvements and validating its
effectiveness and practical applicability.

</details>


### [124] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: This paper proposes COME, a universal framework for integrating heterogeneous ultrasound datasets, overcoming inter-dataset interference and preserving discriminative features, with extensive experiments showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Training machine learning models on single ultrasound (US) datasets fails when dealing with diverse data distributions due to challenges like limited data, acoustic shadows, and speckle noise. A universal framework for multi-heterogeneous US datasets is needed to address this gap.

Method: The authors design a framework called COME (Universal Collaborative Mixture of Heterogeneous Source-Specific Experts), which combines dual structure-semantic shared experts to create universal representations and collaborates with source-specific experts to extract dataset-specific features.

Result: COME outperforms state-of-the-art methods in three evaluation modes: single-dataset, intra-organ, and inter-organ integration datasets, achieving significant performance improvements.

Conclusion: COME effectively addresses the challenges of integrating multi-heterogeneous US datasets by leveraging cross-dataset experiences and universal US priors, making it suitable for robust downstream tasks.

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [125] [A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation](https://arxiv.org/abs/2508.09566)
*Haibo Jin,Haoxuan Che,Sunan He,Hao Chen*

Main category: cs.CV

TL;DR: The authors propose the Chain of Diagnosis (CoD) framework to improve radiology report generation with a focus on clinical accuracy and explainability, achieving superior results in lesion description and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: Existing radiology report generation models struggle with poor clinical efficacy and lack of explainability, which hinders radiologists' ability to rely on these systems.

Method: The proposed CoD framework generates question-answer pairs for key diagnostic findings, uses these to prompt a large language model for report generation, includes grounding modules to match diagnoses with text and lesion location, and employs omni-supervised learning to train efficiently across diverse datasets.

Result: CoD achieves improved clinical accuracy, better explainability through grounding to QA diagnoses and lesion locations, and outperforms existing models consistently on two RRG benchmarks.

Conclusion: The CoD framework significantly enhances radiology report generation by offering clinically accurate, explainable, and efficient methods, increasing its reliability and utility for radiologists.

Abstract: Despite the progress of radiology report generation (RRG), existing works
face two challenges: 1) The performances in clinical efficacy are
unsatisfactory, especially for lesion attributes description; 2) the generated
text lacks explainability, making it difficult for radiologists to trust the
results. To address the challenges, we focus on a trustworthy RRG model, which
not only generates accurate descriptions of abnormalities, but also provides
basis of its predictions. To this end, we propose a framework named chain of
diagnosis (CoD), which maintains a chain of diagnostic process for clinically
accurate and explainable RRG. It first generates question-answer (QA) pairs via
diagnostic conversation to extract key findings, then prompts a large language
model with QA diagnoses for accurate generation. To enhance explainability, a
diagnosis grounding module is designed to match QA diagnoses and generated
sentences, where the diagnoses act as a reference. Moreover, a lesion grounding
module is designed to locate abnormalities in the image, further improving the
working efficiency of radiologists. To facilitate label-efficient training, we
propose an omni-supervised learning strategy with clinical consistency to
leverage various types of annotations from different datasets. Our efforts lead
to 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a
evaluation tool for assessing the accuracy of reports in describing lesion
location and severity; 3) extensive experiments to demonstrate the
effectiveness of CoD, where it outperforms both specialist and generalist
models consistently on two RRG benchmarks and shows promising explainability by
accurately grounding generated sentences to QA diagnoses and images.

</details>


### [126] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: The paper proposes a synthetic dataset, Echo-4o-Image, created by GPT-4o for addressing limitations in real-world image datasets. It demonstrates the advantages of synthetic data, such as rare scenario coverage and improved alignment, and showcases improved image generation performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address shortcomings in real-world image datasets, which lack certain rare scenarios and suffer from noisy backgrounds and text-image misalignment. The authors aim to explore how synthetic data can bridge these gaps and improve image generation models.

Method: The authors generate a 180K-scale synthetic dataset called Echo-4o-Image using GPT-4o. They fine-tune the Bagel multimodal generation baseline to create the Echo-4o model. Two new evaluation benchmarks, GenEval++ and Imagine-Bench, are also introduced to assess model capabilities more effectively.

Result: Echo-4o displays strong performance on standard benchmarks. When applied to other foundational models like OmniGen2 and BLIP3-o, the synthetic dataset consistently enhances performance across metrics, demonstrating its utility and transferability.

Conclusion: The study concludes that synthetic datasets can effectively complement real-world image data by addressing rare scenarios and improving text-to-image alignment. Echo-4o and the new benchmarks provide a valuable resource for advancing image generation tasks.

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>


### [127] [Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion](https://arxiv.org/abs/2508.09575)
*Jiwon Kim,Pureum Kim,SeonHwa Kim,Soobin Park,Eunju Cha,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: This paper introduces a training-free Dual Recursive Feedback (DRF) system for improving controllable text-to-image (T2I) diffusion models, enabling fine-grained image generation with enhanced accuracy in capturing spatial and appearance details.


<details>
  <summary>Details</summary>
Motivation: Existing T2I diffusion models, such as Ctrl-X and FreeControl, fail to adequately preserve spatial structures and capture fine-grained conditions, especially regarding object poses and scene layouts, creating a need for a better methodology.

Method: The paper proposes the Dual Recursive Feedback (DRF) system, which includes appearance feedback and generation feedback mechanisms that refine intermediate latents recursively, effectively integrating structural and appearance conditions without additional training.

Result: The proposed DRF system improves the accuracy and quality of generated images, achieving semantically coherent and structurally consistent results. It also supports advanced tasks like class-invariant structure-appearance fusion, demonstrated in experiments.

Conclusion: The DRF system enhances controllable T2I diffusion models by refining latents recursively, addressing spatial structural preservation and fine-grained control, as validated by experiments and available open-source code.

Abstract: Recent advancements in controllable text-to-image (T2I) diffusion models,
such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance
control without requiring auxiliary module training. However, these models
often struggle to accurately preserve spatial structures and fail to capture
fine-grained conditions related to object poses and scene layouts. To address
these challenges, we propose a training-free Dual Recursive Feedback (DRF)
system that properly reflects control conditions in controllable T2I models.
The proposed DRF consists of appearance feedback and generation feedback that
recursively refines the intermediate latents to better reflect the given
appearance information and the user's intent. This dual-update mechanism guides
latent representations toward reliable manifolds, effectively integrating
structural and appearance attributes. Our approach enables fine-grained
generation even between class-invariant structure-appearance fusion, such as
transferring human motion onto a tiger's form. Extensive experiments
demonstrate the efficacy of our method in producing high-quality, semantically
coherent, and structurally consistent image generations. Our source code is
available at https://github.com/jwonkm/DRF.

</details>


### [128] [SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs](https://arxiv.org/abs/2508.09584)
*Bei Yan,Zhiyuan Chen,Yuecong Min,Jie Zhang,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: The paper addresses hallucinations in Large Vision-Language Models (LVLMs) by introducing SHALE, a scalable benchmark for assessing faithfulness and factuality through fine-grained measures.


<details>
  <summary>Details</summary>
Motivation: There is a need for scalable and detailed evaluation mechanisms to analyze hallucinations in LVLMs, especially to overcome issues such as manual data curation and coarse-level analysis.

Method: The authors propose a hierarchical hallucination induction approach with input perturbations and an automated data construction pipeline. They create SHALE, which covers 30K image-instruction pairs across multiple domains.

Result: Experiments on more than 20 LVLMs show significant issues with factuality hallucinations and strong sensitivity to input semantic perturbations.

Conclusion: SHALE is established as a robust tool for fine-grained analysis of hallucinations, providing insights into LVLM limitations and potential areas for improvement.

Abstract: Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer
from hallucinations, i.e., generating content inconsistent with input or
established world knowledge, which correspond to faithfulness and factuality
hallucinations, respectively. Prior studies primarily evaluate faithfulness
hallucination at a coarse level (e.g., object-level) and lack fine-grained
analysis. Additionally, existing benchmarks rely on costly manual curation or
reused public datasets, raising concerns about scalability and data leakage. To
address these limitations, we propose an automated data construction pipeline
that produces scalable, controllable, and diverse evaluation data. We also
design a hierarchical hallucination induction framework with input
perturbations to simulate realistic noisy scenarios. Integrating these designs,
we construct SHALE, a Scalable HALlucination Evaluation benchmark designed to
assess both faithfulness and factuality hallucinations via a fine-grained
hallucination categorization scheme. SHALE comprises over 30K image-instruction
pairs spanning 12 representative visual perception aspects for faithfulness and
6 knowledge domains for factuality, considering both clean and noisy scenarios.
Extensive experiments on over 20 mainstream LVLMs reveal significant factuality
hallucinations and high sensitivity to semantic perturbations.

</details>


### [129] [Offline Auto Labeling: BAAS](https://arxiv.org/abs/2508.09585)
*Stefan Haag,Bharanidhar Duraisamy,Felix Govaers,Wolfgang Koch,Martin Fritzsche,Juergen Dickmann*

Main category: cs.CV

TL;DR: The paper introduces BAAS, a framework for radar-based label annotation and tracking in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Enable precise tracking and labeling using radar data in challenging environments.

Method: Utilizes Bayesian tracking, smoothing, and fusion techniques for trajectory and shape estimation.

Result: Effectively generates annotations and tracks dynamic objects in real-world urban scenarios.

Conclusion: BAAS enhances tracking and labeling accuracy, adaptable for continuous improvements.

Abstract: This paper introduces BAAS, a new Extended Object Tracking (EOT) and
fusion-based label annotation framework for radar detections in autonomous
driving. Our framework utilizes Bayesian-based tracking, smoothing and
eventually fusion methods to provide veritable and precise object trajectories
along with shape estimation to provide annotation labels on the detection level
under various supervision levels. Simultaneously, the framework provides
evaluation of tracking performance and label annotation. If manually labeled
data is available, each processing module can be analyzed independently or
combined with other modules to enable closed-loop continuous improvements. The
framework performance is evaluated in a challenging urban real-world scenario
in terms of tracking performance and the label annotation errors. We
demonstrate the functionality of the proposed approach for varying dynamic
objects and class types

</details>


### [130] [Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma](https://arxiv.org/abs/2508.09593)
*Haotian Tang,Jianwei Chen,Xinrui Tang,Yunjia Wu,Zhengyang Miao,Chao Li*

Main category: cs.CV

TL;DR: The paper introduces Hi-SMGNN, a novel hierarchical framework for predicting IDH mutation status in glioma using structural and morphological connectomes.


<details>
  <summary>Details</summary>
Motivation: Current IDH mutation prediction methods face challenges due to limited functional MRI availability and noise, and fail to leverage the brain's hierarchical organization and multiscale interactions.

Method: The proposed Hi-SMGNN integrates structural and morphological connectomes across regional to modular levels, using multimodal interaction modules with Siamese networks, cross-modal attention, multiscale feature fusion, and personalized modular partitioning.

Result: Hi-SMGNN outperforms baseline and state-of-the-art models in robustness and effectiveness for IDH mutation prediction, validated on the UCSF-PDGM dataset.

Conclusion: The Hi-SMGNN framework offers a more robust and effective approach to non-invasive IDH mutation prediction, enhancing interpretability and individual specificity.

Abstract: Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for
glioma prognosis. However, current prediction methods are limited by the low
availability and noise of functional MRI. Structural and morphological
connectomes offer a non-invasive alternative, yet existing approaches often
ignore the brain's hierarchical organisation and multiscale interactions. To
address this, we propose Hi-SMGNN, a hierarchical framework that integrates
structural and morphological connectomes from regional to modular levels. It
features a multimodal interaction module with a Siamese network and cross-modal
attention, a multiscale feature fusion mechanism for reducing redundancy, and a
personalised modular partitioning strategy to enhance individual specificity
and interpretability. Experiments on the UCSF-PDGM dataset demonstrate that
Hi-SMGNN outperforms baseline and state-of-the-art models, showing improved
robustness and effectiveness in IDH mutation prediction.

</details>


### [131] [SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://arxiv.org/abs/2508.09597)
*Heyi Sun,Cong Wang,Tian-Xing Xu,Jingwei Huang,Di Kang,Chunchao Guo,Song-Hai Zhang*

Main category: cs.CV

TL;DR: The paper proposes SVG-Head, a novel hybrid representation for creating high-fidelity, editable head avatars, overcoming challenges in real-time appearance editing with explicit texture modeling.


<details>
  <summary>Details</summary>
Motivation: High-fidelity and editable head avatars are important for AR/VR applications, but achieving real-time editing remains challenging due to implicit representations and entangled geometry and appearance modeling.

Method: SVG-Head employs a hybrid representation using 3D Gaussians tied to FLAME meshes for geometry, disentangled texture images for appearance, and introduces mesh-aware Gaussian UV mapping for accurate rendering and editing.

Result: The method generates high-quality renderings, provides explicit texture images for Gaussian head avatars, and enables real-time appearance editing.

Conclusion: SVG-Head successfully advances head avatar modeling by combining high reconstruction fidelity with real-time texture editing capabilities, fulfilling a key gap in current approaches.

Abstract: Creating high-fidelity and editable head avatars is a pivotal challenge in
computer vision and graphics, boosting many AR/VR applications. While recent
advancements have achieved photorealistic renderings and plausible animation,
head editing, especially real-time appearance editing, remains challenging due
to the implicit representation and entangled modeling of the geometry and
global appearance. To address this, we propose Surface-Volumetric Gaussian Head
Avatar (SVG-Head), a novel hybrid representation that explicitly models the
geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled
texture images to capture the global appearance. Technically, it contains two
types of Gaussians, in which surface Gaussians explicitly model the appearance
of head avatars using learnable texture images, facilitating real-time texture
editing, while volumetric Gaussians enhance the reconstruction quality of
non-Lambertian regions (e.g., lips and hair). To model the correspondence
between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping
method, which leverages UV coordinates given by the FLAME mesh to obtain sharp
texture images and real-time rendering speed. A hierarchical optimization
strategy is further designed to pursue the optimal performance in both
reconstruction quality and editing flexibility. Experiments on the NeRSemble
dataset show that SVG-Head not only generates high-fidelity rendering results,
but also is the first method to obtain explicit texture images for Gaussian
head avatars and support real-time appearance editing.

</details>


### [132] [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](https://arxiv.org/abs/2508.09598)
*Jie Shao,Ke Zhu,Minghao Fu,Guo-hua Wang,Jianxin Wu*

Main category: cs.CV

TL;DR: The paper introduces FaME, a method to enhance perceptual image quality in diffusion models, addressing quality issues overlooked by FID, without compromising on distribution alignment.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models excel in distribution alignment (as measured by FID) but fail in generating consistently high-quality individual images, especially for certain classes. Additionally, CFG can enhance metrics but introduces visual artifacts and misaligned expectations.

Method: FaME employs an image quality assessment model to identify distorted sample trajectories, storing them as negative guidance during future inference to systematically steer sampling away from poor-quality areas.

Result: Experiments on ImageNet show that FaME consistently improves image quality while maintaining FID scores and can potentially enhance text-to-image generation.

Conclusion: FaME effectively addresses shortcomings in individual image quality within diffusion models, providing a training-free solution that preserves distribution alignment and significantly reduces perceptual flaws.

Abstract: Diffusion models have achieved remarkable progress in class-to-image
generation. However, we observe that despite impressive FID scores,
state-of-the-art models often generate distorted or low-quality images,
especially in certain classes. This gap arises because FID evaluates global
distribution alignment, while ignoring the perceptual quality of individual
samples. We further examine the role of CFG, a common technique used to enhance
generation quality. While effective in improving metrics and suppressing
outliers, CFG can introduce distribution shift and visual artifacts due to its
misalignment with both training objectives and user expectations. In this work,
we propose FaME, a training-free and inference-efficient method for improving
perceptual quality. FaME uses an image quality assessment model to identify
low-quality generations and stores their sampling trajectories. These failure
modes are then used as negative guidance to steer future sampling away from
poor-quality regions. Experiments on ImageNet demonstrate that FaME brings
consistent improvements in visual quality without compromising FID. FaME also
shows the potential to be extended to improve text-to-image generation.

</details>


### [133] [BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation](https://arxiv.org/abs/2508.09599)
*Beomjun Kim,Suhan Woo,Sejong Heo,Euntai Kim*

Main category: cs.CV

TL;DR: The paper presents BridgeTA, a distillation framework to improve camera-only BEV map segmentation models by introducing a Teacher Assistant network, achieving better performance while keeping inference costs stable.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between LiDAR-Camera fusion and Camera-only systems in BEV map segmentation for autonomous driving by addressing inefficiencies in existing knowledge distillation approaches.

Method: Proposes BridgeTA, a distillation framework that employs a lightweight Teacher Assistant (TA) network to create a shared latent space and dual distillation paths (teacher-TA and TA-student) based on Young's Inequality, enhancing knowledge transfer stability.

Result: Achieved a 4.2% mIoU improvement over the Camera-only baseline on the nuScenes dataset, outperforming other state-of-the-art KD methods by up to 45%.

Conclusion: BridgeTA effectively narrows the representation gap between LC fusion and Camera-only models, enhancing performance without increasing inference costs, making camera-only solutions more viable for BEV map segmentation.

Abstract: Bird's-Eye-View (BEV) map segmentation is one of the most important and
challenging tasks in autonomous driving. Camera-only approaches have drawn
attention as cost-effective alternatives to LiDAR, but they still fall behind
LiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been
explored to narrow this gap, but existing methods mainly enlarge the student
model by mimicking the teacher's architecture, leading to higher inference
cost. To address this issue, we introduce BridgeTA, a cost-effective
distillation framework to bridge the representation gap between LC fusion and
Camera-only models through a Teacher Assistant (TA) network while keeping the
student's architecture and inference cost unchanged. A lightweight TA network
combines the BEV representations of the teacher and student, creating a shared
latent space that serves as an intermediate representation. To ground the
framework theoretically, we derive a distillation loss using Young's
Inequality, which decomposes the direct teacher-student distillation path into
teacher-TA and TA-student dual paths, stabilizing optimization and
strengthening knowledge transfer. Extensive experiments on the challenging
nuScenes dataset demonstrate the effectiveness of our method, achieving an
improvement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than
the improvement of other state-of-the-art KD methods.

</details>


### [134] [MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography](https://arxiv.org/abs/2508.09616)
*Daniel Barco,Marc Stadelmann,Martin Oswald,Ivo Herzig,Lukas Lichtensteiger,Pascal Paysan,Igor Peterlik,Michal Walczak,Bjoern Menze,Frank-Peter Schilling*

Main category: cs.CV

TL;DR: MInDI-3D is a 3D diffusion model for reducing artefacts in sparse-view Cone Beam CT scans, cutting imaging radiation exposure by 8x while preserving high-quality results.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to reduce radiation exposure in Cone Beam CT imaging while maintaining the usability of sparse-view images for clinical purposes.

Method: The authors extend the InDI concept to a 3D volumetric model, train it on a large pseudo-CBCT dataset, and use iterative denoising to refine sparse-view inputs into high-quality 3D scans.

Result: MInDI-3D achieves superior performance with a 12.96 dB PSNR improvement, shows scalability with training data, matches the performance of a 3D U-Net, and generalises effectively across geometries and clinical applications.

Conclusion: The model provides a promising solution for low-radiation CBCT imaging, preserving diagnostic quality and enabling safe clinical practices with its robust generalisation and scalability.

Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first
3D conditional diffusion-based model for real-world sparse-view Cone Beam
Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation
exposure. A key contribution is extending the "InDI" concept from 2D to a full
3D volumetric approach for medical images, implementing an iterative denoising
process that refines the CBCT volume directly from sparse-view input. A further
contribution is the generation of a large pseudo-CBCT dataset (16,182) from
chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We
performed a comprehensive evaluation, including quantitative metrics,
scalability analysis, generalisation tests, and a clinical assessment by 11
clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)
dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE
pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in
imaging radiation exposure. We demonstrate its scalability by showing that
performance improves with more training data. Importantly, MInDI-3D matches the
performance of a 3D U-Net on real-world scans from 16 cancer patients across
distortion and task-based metrics. It also generalises to new CBCT scanner
geometries. Clinicians rated our model as sufficient for patient positioning
across all anatomical sites and found it preserved lung tumour boundaries well.

</details>


### [135] [Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation](https://arxiv.org/abs/2508.09626)
*Xu Tang,Junan Jia,Yijing Wang,Jingjing Ma,Xiangrong Zhang*

Main category: cs.CV

TL;DR: The paper introduces SAD-Splat, a novel approach for 3D aerial-view scene semantic segmentation, addressing issues like scale variation and structural occlusion using Gaussian point drop and pseudo-labeling techniques.


<details>
  <summary>Details</summary>
Motivation: Existing traditional methods in 3D Aerial-view Scene Semantic Segmentation fail to handle semantic ambiguities arising from scale variations and occlusions, which compromises the segmentation accuracy and consistency.

Method: The proposed SAD-Splat method introduces a Gaussian point drop module integrating semantic confidence estimation with a learnable sparsity mechanism. It also includes a pseudo-label generation pipeline leveraging 2D foundation models to enhance supervision under limited labels.

Result: The results demonstrate that SAD-Splat achieves a strong balance between segmentation accuracy and representation compactness while offering an efficient solution for 3D aerial scene understanding.

Conclusion: SAD-Splat effectively addresses critical challenges in 3D Aerial-view Scene Semantic Segmentation, providing a scalable, accurate, and compact solution for real-world applications.

Abstract: In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),
traditional methods struggle to address semantic ambiguity caused by scale
variations and structural occlusions in aerial images. This limits their
segmentation accuracy and consistency. To tackle these challenges, we propose a
novel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian
point drop module, which integrates semantic confidence estimation with a
learnable sparsity mechanism based on the Hard Concrete distribution. This
module effectively eliminates redundant and semantically ambiguous Gaussian
points, enhancing both segmentation performance and representation compactness.
Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generation
pipeline. It leverages 2D foundation models to enhance supervision when
ground-truth labels are limited, thereby further improving segmentation
accuracy. To advance research in this domain, we introduce a challenging
benchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse
real-world aerial scenes with sparse annotations. Experimental results
demonstrate that SAD-Splat achieves an excellent balance between segmentation
accuracy and representation compactness. It offers an efficient and scalable
solution for 3D aerial scene understanding.

</details>


### [136] [Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors](https://arxiv.org/abs/2508.09629)
*Giorgos Karvounas,Nikolaos Kyriazis,Iason Oikonomidis,Georgios Pavlakos,Antonis A. Argyros*

Main category: cs.CV

TL;DR: The paper introduces a texture module for improving monocular 3D hand reconstruction by aligning predicted texture with observed appearances.


<details>
  <summary>Details</summary>
Motivation: To address imperfections in aligning predicted hand geometry and image texture in current high-performing models for better hand pose and shape estimation.

Method: A lightweight texture module that embeds per-pixel inputs into UV texture space and uses a dense alignment loss, integrated with differentiable rendering and a pre-existing 3D hand mesh model.

Result: The proposed system improves both accuracy and realism in 3D hand pose estimation when tested with the HaMeR transformer architecture.

Conclusion: Texture alignment serves as a valuable supervisory signal, enhancing the capability of 3D hand reconstruction pipelines.

Abstract: We revisit the role of texture in monocular 3D hand reconstruction, not as an
afterthought for photorealism, but as a dense, spatially grounded cue that can
actively support pose and shape estimation. Our observation is simple: even in
high-performing models, the overlay between predicted hand geometry and image
appearance is often imperfect, suggesting that texture alignment may be an
underused supervisory signal. We propose a lightweight texture module that
embeds per-pixel observations into UV texture space and enables a novel dense
alignment loss between predicted and observed hand appearances. Our approach
assumes access to a differentiable rendering pipeline and a model that maps
images to 3D hand meshes with known topology, allowing us to back-project a
textured hand onto the image and perform pixel-based alignment. The module is
self-contained and easily pluggable into existing reconstruction pipelines. To
isolate and highlight the value of texture-guided supervision, we augment
HaMeR, a high-performing yet unadorned transformer architecture for 3D hand
pose estimation. The resulting system improves both accuracy and realism,
demonstrating the value of appearance-guided alignment in hand reconstruction.

</details>


### [137] [Preacher: Paper-to-Video Agentic System](https://arxiv.org/abs/2508.09632)
*Jingwei Liu,Ling Yang,Hao Luo,Fan Wang Hongyan Li,Mengdi Wang*

Main category: cs.CV

TL;DR: This paper introduces "Preacher," an agentic system designed for converting research papers into structured video abstracts to address limitations in existing video generation models.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models struggle with limited context windows, rigid duration constraints, stylistic uniformity, and inadequate representation of domain-specific knowledge, motivating the need for improvement in the paper-to-video task.

Method: The approach involves a top-down decomposition, summarization, and reformulation of the paper, paired with bottom-up video generation. The system utilizes Progressive Chain of Thought (P-CoT) for cross-modal alignment and iterative planning.

Result: Preacher successfully generates high-quality, diverse video abstracts across five research fields, outperforming current state-of-the-art video generation models.

Conclusion: Preacher represents a significant advance in translating research papers into structured video formats, showcasing its potential for broader adoption across multiple domains.

Abstract: The paper-to-video task converts a research paper into a structured video
abstract, distilling key concepts, methods, and conclusions into an accessible,
well-organized format. While state-of-the-art video generation models
demonstrate potential, they are constrained by limited context windows, rigid
video duration constraints, limited stylistic diversity, and an inability to
represent domain-specific knowledge. To address these limitations, we introduce
Preacher, the first paper-to-video agentic system. Preacher employs a top-down
approach to decompose, summarize, and reformulate the paper, followed by
bottom-up video generation, synthesizing diverse video segments into a coherent
abstract. To align cross-modal representations, we define key scenes and
introduce a Progressive Chain of Thought (P-CoT) for granular, iterative
planning. Preacher successfully generates high-quality video abstracts across
five research fields, demonstrating expertise beyond current video generation
models. Code will be released at: https://github.com/GenVerse/Paper2Video

</details>


### [138] [Multi-Contrast Fusion Module: An attention mechanism integrating multi-contrast features for fetal torso plane classification](https://arxiv.org/abs/2508.09644)
*Shengjun Zhu,Siyu Liu,Runqing Xiong,Liping Zheng,Duo Ma,Rongshang Chen,Jiaxin Cai*

Main category: cs.CV

TL;DR: The paper introduces a novel module to enhance fetal torso plane recognition in ultrasound imaging by leveraging multi-contrast fusion.


<details>
  <summary>Details</summary>
Motivation: The research aims to address challenges in detecting fetal abnormalities via ultrasound caused by low contrast and unclear texture details.

Method: They developed a Multi-Contrast Fusion Module (MCFM) that processes raw ultrasound data with attention weights for better feature extraction in lower neural network layers.

Result: Evaluations on curated datasets showed significant improvement in recognition performance while maintaining low model complexity.

Conclusion: The proposed method enhances diagnostic accuracy in fetal imaging, supporting prenatal care, with potential for clinical adoption.

Abstract: Purpose: Prenatal ultrasound is a key tool in evaluating fetal structural
development and detecting abnormalities, contributing to reduced perinatal
complications and improved neonatal survival. Accurate identification of
standard fetal torso planes is essential for reliable assessment and
personalized prenatal care. However, limitations such as low contrast and
unclear texture details in ultrasound imaging pose significant challenges for
fine-grained anatomical recognition. Methods: We propose a novel Multi-Contrast
Fusion Module (MCFM) to enhance the model's ability to extract detailed
information from ultrasound images. MCFM operates exclusively on the lower
layers of the neural network, directly processing raw ultrasound data. By
assigning attention weights to image representations under different contrast
conditions, the module enhances feature modeling while explicitly maintaining
minimal parameter overhead. Results: The proposed MCFM was evaluated on a
curated dataset of fetal torso plane ultrasound images. Experimental results
demonstrate that MCFM substantially improves recognition performance, with a
minimal increase in model complexity. The integration of multi-contrast
attention enables the model to better capture subtle anatomical structures,
contributing to higher classification accuracy and clinical reliability.
Conclusions: Our method provides an effective solution for improving fetal
torso plane recognition in ultrasound imaging. By enhancing feature
representation through multi-contrast fusion, the proposed approach supports
clinicians in achieving more accurate and consistent diagnoses, demonstrating
strong potential for clinical adoption in prenatal screening. The codes are
available at https://github.com/sysll/MCFM.

</details>


### [139] [Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model](https://arxiv.org/abs/2508.09645)
*Zhongyuan Wu,Chuan-Xian Ren,Yu Wang,Xiaohua Ban,Jianning Xiao,Xiaohui Duan*

Main category: cs.CV

TL;DR: This paper introduces PG-SAM, a novel parotid gland segmentation model leveraging expert diagnosis text and cross-sequence data.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of parotid gland lesions is challenging due to variations in size and complex boundaries. Current methods lack integration of expert domain knowledge and precise lesion prompts.

Method: PG-SAM uses expert diagnosis text to auto-generate prompts and incorporates cross-sequence attention for improved segmentation, combining multi-sequence features and prompts in a decoder.

Result: PG-SAM demonstrated superior segmentation performance across three clinical centers, showcasing its clinical relevance and enhanced imaging aided by diagnostic text.

Conclusion: PG-SAM effectively addresses limitations in medical segmentation by integrating domain knowledge from diagnostic reports and cross-modality data for reliable clinical applications.

Abstract: Parotid gland lesion segmentation is essential for the treatment of parotid
gland diseases. However, due to the variable size and complex lesion
boundaries, accurate parotid gland lesion segmentation remains challenging.
Recently, the Segment Anything Model (SAM) fine-tuning has shown remarkable
performance in the field of medical image segmentation. Nevertheless, SAM's
interaction segmentation model relies heavily on precise lesion prompts
(points, boxes, masks, etc.), which are very difficult to obtain in real-world
applications. Besides, current medical image segmentation methods are
automatically generated, ignoring the domain knowledge of medical experts when
performing segmentation. To address these limitations, we propose the parotid
gland segment anything model (PG-SAM), an expert diagnosis text-guided SAM
incorporating expert domain knowledge for cross-sequence parotid gland lesion
segmentation. Specifically, we first propose an expert diagnosis report guided
prompt generation module that can automatically generate prompt information
containing the prior domain knowledge to guide the subsequent lesion
segmentation process. Then, we introduce a cross-sequence attention module,
which integrates the complementary information of different modalities to
enhance the segmentation effect. Finally, the multi-sequence image features and
generated prompts are feed into the decoder to get segmentation result.
Experimental results demonstrate that PG-SAM achieves state-of-the-art
performance in parotid gland lesion segmentation across three independent
clinical centers, validating its clinical applicability and the effectiveness
of diagnostic text for enhancing image segmentation in real-world clinical
settings.

</details>


### [140] [The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge](https://arxiv.org/abs/2508.09649)
*Reuben Dorent,Laura Rigolo,Colin P. Galvin,Junyu Chen,Mattias P. Heinrich,Aaron Carass,Olivier Colliot,Demian Wassermann,Alexandra Golby,Tina Kapur,William Wells*

Main category: cs.CV

TL;DR: The paper introduces the ReMIND2Reg 2025 Challenge, which benchmarks the alignment of intraoperative ultrasound with preoperative MRI in brain tumor surgery to address issues like brain shift.


<details>
  <summary>Details</summary>
Motivation: To restore spatial accuracy in brain tumor surgery by overcoming challenges in aligning intraoperative ultrasound and preoperative MRI due to brain shift and modality differences.

Method: The ReMIND2Reg Challenge provides a dataset of paired imaging modalities for training and evaluates performance using manually annotated landmarks with metrics like TRE, TRE30, and runtime.

Result: The challenge establishes a standardized framework for multimodal image registration to help overcome current limitations in image-guided neurosurgery.

Conclusion: ReMIND2Reg aims to drive the development of robust and generalizable multimodal registration algorithms for improved clinical outcomes in brain tumor surgery.

Abstract: Accurate intraoperative image guidance is critical for achieving maximal safe
resection in brain tumor surgery, yet neuronavigation systems based on
preoperative MRI lose accuracy during the procedure due to brain shift.
Aligning post-resection intraoperative ultrasound (iUS) with preoperative MRI
can restore spatial accuracy by estimating brain shift deformations, but it
remains a challenging problem given the large anatomical and topological
changes and substantial modality intensity gap. The ReMIND2Reg 2025 Challenge
provides the largest public benchmark for this task, built upon the ReMIND
dataset. It offers 99 training cases, 5 validation cases, and 10 private test
cases comprising paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes.
Data are provided without annotations for training, while validation and test
performance are evaluated on manually annotated anatomical landmarks. Metrics
include target registration error (TRE), robustness to worst-case landmark
misalignment (TRE30), and runtime. By establishing a standardized evaluation
framework for this clinically critical and technically complex problem,
ReMIND2Reg aims to accelerate the development of robust, generalizable, and
clinically deployable multimodal registration algorithms for image-guided
neurosurgery.

</details>


### [141] [TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos](https://arxiv.org/abs/2508.09650)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazely,Sunil Aryal*

Main category: cs.CV

TL;DR: The paper introduces TOTNet, a Temporal Occlusion Tracking Network for tracking balls in sports videos under occlusion, achieving notable accuracy improvements across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the persistent issue of robust ball tracking in sports videos, especially under partial or full occlusions, which is important for applications like event detection and officiating.

Method: The paper proposes TOTNet, which uses 3D convolutions, a visibility-weighted loss, and occlusion augmentation. A new occlusion-rich table tennis dataset (TTA) with 9,159 samples is introduced for model evaluation.

Result: TOTNet outperforms state-of-the-art methods significantly, reducing the RMSE from 37.30 to 7.19 and improving accuracy on fully occluded frames from 0.63 to 0.80 across multiple datasets.

Conclusion: TOTNet proves effective for offline sports analytics in dynamic and fast-paced sports scenarios. It is a promising solution for handling occlusions in ball tracking.

Abstract: Robust ball tracking under occlusion remains a key challenge in sports video
analysis, affecting tasks like event detection and officiating. We present
TOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,
visibility-weighted loss, and occlusion augmentation to improve performance
under partial and full occlusions. Developed in collaboration with Paralympics
Australia, TOTNet is designed for real-world sports analytics. We introduce
TTA, a new occlusion-rich table tennis dataset collected from
professional-level Paralympic matches, comprising 9,159 samples with 1,996
occlusion cases. Evaluated on four datasets across tennis, badminton, and table
tennis, TOTNet significantly outperforms prior state-of-the-art methods,
reducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded
frames from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for
offline sports analytics in fast-paced scenarios. Code and data
access:\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}.

</details>


### [142] [Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging](https://arxiv.org/abs/2508.09655)
*Lianfang Wang,Kuilin Qin,Xueying Liu,Huibin Chang,Yong Wang,Yuping Duan*

Main category: cs.CV

TL;DR: The paper introduces a noise-adaptive neural-based framework for 3D non-line-of-sight imaging, which enhances image reconstruction accuracy through operator learning and spatiotemporal data fusion.


<details>
  <summary>Details</summary>
Motivation: NLOS imaging, involving information retrieval from hidden scenes using indirect light, suffers from weak and noisy signals, creating the need for frameworks capable of robust reconstruction.

Method: The framework uses a noise estimation module to evaluate transient signal noise adaptively, followed by a parameterized neural operator for reconstructing 3D images via operator learning and deep algorithm unfolding.

Result: The proposed approach demonstrates consistent and robust performance in accurately reconstructing 3D images under challenging conditions, such as fast scanning and sparse illumination, in both simulated and real datasets.

Conclusion: This framework is a promising solution for complex NLOS imaging tasks, as it effectively adapts to varying noise levels and integrates global and local features to achieve high-quality reconstructions.

Abstract: Computational imaging, especially non-line-of-sight (NLOS) imaging, the
extraction of information from obscured or hidden scenes is achieved through
the utilization of indirect light signals resulting from multiple reflections
or scattering. The inherently weak nature of these signals, coupled with their
susceptibility to noise, necessitates the integration of physical processes to
ensure accurate reconstruction. This paper presents a parameterized inverse
problem framework tailored for large-scale linear problems in 3D imaging
reconstruction. Initially, a noise estimation module is employed to adaptively
assess the noise levels present in transient data. Subsequently, a
parameterized neural operator is developed to approximate the inverse mapping,
facilitating end-to-end rapid image reconstruction. Our 3D image reconstruction
framework, grounded in operator learning, is constructed through deep algorithm
unfolding, which not only provides commendable model interpretability but also
enables dynamic adaptation to varying noise levels in the acquired data,
thereby ensuring consistently robust and accurate reconstruction outcomes.
Furthermore, we introduce a novel method for the fusion of global and local
spatiotemporal data features. By integrating structural and detailed
information, this method significantly enhances both accuracy and robustness.
Comprehensive numerical experiments conducted on both simulated and real
datasets substantiate the efficacy of the proposed method. It demonstrates
remarkable performance with fast scanning data and sparse illumination point
data, offering a viable solution for NLOS imaging in complex scenarios.

</details>


### [143] [NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation](https://arxiv.org/abs/2508.09661)
*Eduarda Caldeira,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: The study presents NegFaceDiff, a method enhancing synthetic face datasets through negative conditions, resulting in better identity consistency and separability for improved face recognition (FR).


<details>
  <summary>Details</summary>
Motivation: Existing identity-conditioned diffusion models for generating synthetic face images often struggle with identity overlap due to the lack of explicit sampling for inter-class separability, leading to suboptimal performances in FR systems.

Method: The proposed NegFaceDiff method injects negative conditions during the identity-conditioned diffusion process to explicitly guide the synthesis away from unwanted features while ensuring intra-class consistency.

Result: The NegFaceDiff technique enhances identity separability, increasing the Fisher Discriminant Ratio (FDR) substantially from 2.427 to 5.687. FR systems trained on this data achieve superior benchmark performance compared to those using baseline synthetic datasets.

Conclusion: The inclusion of negative sampling conditions in identity-conditioned diffusion processes effectively addresses identity overlap, providing a valuable improvement for synthetic data generation and face recognition model performance.

Abstract: The use of synthetic data as an alternative to authentic datasets in face
recognition (FR) development has gained significant attention, addressing
privacy, ethical, and practical concerns associated with collecting and using
authentic data. Recent state-of-the-art approaches have proposed
identity-conditioned diffusion models to generate identity-consistent face
images, facilitating their use in training FR models. However, these methods
often lack explicit sampling mechanisms to enforce inter-class separability,
leading to identity overlap in the generated data and, consequently, suboptimal
FR performance. In this work, we introduce NegFaceDiff, a novel sampling method
that incorporates negative conditions into the identity-conditioned diffusion
process. NegFaceDiff enhances identity separation by leveraging negative
conditions that explicitly guide the model away from unwanted features while
preserving intra-class consistency. Extensive experiments demonstrate that
NegFaceDiff significantly improves the identity consistency and separability of
data generated by identity-conditioned diffusion models. Specifically, identity
separability, measured by the Fisher Discriminant Ratio (FDR), increases from
2.427 to 5.687. These improvements are reflected in FR systems trained on the
NegFaceDiff dataset, which outperform models trained on data generated without
negative conditions across multiple benchmarks.

</details>


### [144] [GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors](https://arxiv.org/abs/2508.09667)
*Xingyilang Yin,Qi Zhang,Jiahao Chang,Ying Feng,Qingnan Fan,Xi Yang,Chi-Man Pun,Huaqi Zhang,Xiaodong Cun*

Main category: cs.CV

TL;DR: This paper introduces GSFixer, a framework to enhance 3D Gaussian Splatting (3DGS) representations from sparse views, focusing on artifact restoration using generative priors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address issues in reconstructing 3D scenes from sparse views, particularly addressing artifacts and inconsistencies with input observations within the context of 3D Gaussian Splatting.

Method: The proposed GSFixer leverages a reference-guided video restoration model based on a diffusion model, integrating 2D semantic and 3D geometric features extracted from a visual geometry foundation model.

Result: GSFixer demonstrates superior performance in restoring 3DGS-rendered artifact frames and in sparse-view 3D reconstruction, outperforming existing methods.

Conclusion: GSFixer proves to be an effective solution to the challenges of artifact restoration and sparse-view 3D reconstruction, contributing improved semantic coherence and 3D consistency as shown in benchmarking and experiments.

Abstract: Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views
is an ill-posed problem due to insufficient information, often resulting in
noticeable artifacts. While recent approaches have sought to leverage
generative priors to complete information for under-constrained regions, they
struggle to generate content that remains consistent with input observations.
To address this challenge, we propose GSFixer, a novel framework designed to
improve the quality of 3DGS representations reconstructed from sparse inputs.
The core of our approach is the reference-guided video restoration model, built
upon a DiT-based video diffusion model trained on paired artifact 3DGS renders
and clean frames with additional reference-based conditions. Considering the
input sparse views as references, our model integrates both 2D semantic
features and 3D geometric features of reference views extracted from the visual
geometry foundation model, enhancing the semantic coherence and 3D consistency
when fixing artifact novel views. Furthermore, considering the lack of suitable
benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which
contains artifact frames rendered using low-quality 3DGS. Extensive experiments
demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS
artifact restoration and sparse-view 3D reconstruction. Project page:
https://github.com/GVCLab/GSFixer.

</details>


### [145] [PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training](https://arxiv.org/abs/2508.09691)
*Yin Xie,Zhichao Chen,Xiaoze Yu,Yongle Zhao,Xiang An,Kaicheng Yang,Zimin Ran,Jia Guo,Ziyong Feng,Jiankang Deng*

Main category: cs.CV

TL;DR: PaCo-FR is a novel framework overcoming challenges in facial representation learning by leveraging unsupervised pre-training with masked image modeling and patch-pixel alignment, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to capture fine-grained facial features, preserve spatial structures, and efficiently use limited labeled data.

Method: PaCo-FR employs structured masking, a patch-based codebook, and spatial consistency constraints for unsupervised pre-training.

Result: The framework shows significant improvements across various facial analysis tasks using 2 million unlabeled images, especially under challenging scenarios like occlusions and diverse lighting.

Conclusion: PaCo-FR provides a scalable and efficient facial representation solution that minimizes dependence on annotated datasets, advancing the field of facial analysis systems.

Abstract: Facial representation pre-training is crucial for tasks like facial
recognition, expression analysis, and virtual reality. However, existing
methods face three key challenges: (1) failing to capture distinct facial
features and fine-grained semantics, (2) ignoring the spatial structure
inherent to facial anatomy, and (3) inefficiently utilizing limited labeled
data. To overcome these, we introduce PaCo-FR, an unsupervised framework that
combines masked image modeling with patch-pixel alignment. Our approach
integrates three innovative components: (1) a structured masking strategy that
preserves spatial coherence by aligning with semantically meaningful facial
regions, (2) a novel patch-based codebook that enhances feature discrimination
with multiple candidate tokens, and (3) spatial consistency constraints that
preserve geometric relationships between facial components. PaCo-FR achieves
state-of-the-art performance across several facial analysis tasks with just 2
million unlabeled images for pre-training. Our method demonstrates significant
improvements, particularly in scenarios with varying poses, occlusions, and
lighting conditions. We believe this work advances facial representation
learning and offers a scalable, efficient solution that reduces reliance on
expensive annotated datasets, driving more effective facial analysis systems.

</details>


### [146] [Slot Attention-based Feature Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.09699)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: The paper introduces Slot Attention-based Feature Filtering (SAFF), a method to address feature irrelevance and improve few-shot learning performance by filtering weak features with slot attention.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning suffers from performance degradation due to irrelevant features like background elements, which lead to confusion and misclassification.

Method: The proposed SAFF integrates slot attention with patch embeddings to filter irrelevant features, using a unified class-aware attention mechanism and a similarity matrix to quantify relevance for classification.

Result: SAFF outperforms state-of-the-art few-shot learning methods on benchmarks such as CIFAR-FS, FC100, miniImageNet, and tieredImageNet.

Conclusion: Slot Attention effectively captures discriminative features while reducing irrelevant ones, enhancing few-shot learning classification performance.

Abstract: Irrelevant features can significantly degrade few-shot learn ing performance.
This problem is used to match queries and support images based on meaningful
similarities despite the limited data. However, in this process, non-relevant
fea tures such as background elements can easily lead to confu sion and
misclassification. To address this issue, we pro pose Slot Attention-based
Feature Filtering for Few-Shot Learning (SAFF) that leverages slot attention
mechanisms to discriminate and filter weak features, thereby improving few-shot
classification performance. The key innovation of SAFF lies in its integration
of slot attention with patch em beddings, unifying class-aware slots into a
single attention mechanism to filter irrelevant features effectively. We intro
duce a similarity matrix that computes across support and query images to
quantify the relevance of filtered embed dings for classification. Through
experiments, we demon strate that Slot Attention performs better than other
atten tion mechanisms, capturing discriminative features while reducing
irrelevant information. We validate our approach through extensive experiments
on few-shot learning bench marks: CIFAR-FS, FC100, miniImageNet and tieredIma
geNet, outperforming several state-of-the-art methods.

</details>


### [147] [MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers](https://arxiv.org/abs/2508.09709)
*Qianru Qiu,Jiafeng Mao,Kento Masui,Xueting Wang*

Main category: cs.CV

TL;DR: MangaDiT improves reference-guided line art colorization using an advanced attention mechanism in diffusion transformers, achieving better region-level color consistency without external annotations.


<details>
  <summary>Details</summary>
Motivation: Existing techniques struggle in achieving region-level color consistency when reference and target images differ in pose or motion.

Method: MangaDiT uses Diffusion Transformers with hierarchical attention and dynamic weighting to discover semantic correspondences internally, enhancing receptive fields.

Result: Experiments on benchmark datasets show MangaDiT outperforms state-of-the-art methods in qualitative and quantitative evaluations.

Conclusion: The proposed method effectively aligns region-level colors in challenging scenarios, showcasing its advancements in line art colorization performance.

Abstract: Recent advances in diffusion models have significantly improved the
performance of reference-guided line art colorization. However, existing
methods still struggle with region-level color consistency, especially when the
reference and target images differ in character pose or motion. Instead of
relying on external matching annotations between the reference and target, we
propose to discover semantic correspondences implicitly through internal
attention mechanisms. In this paper, we present MangaDiT, a powerful model for
reference-guided line art colorization based on Diffusion Transformers (DiT).
Our model takes both line art and reference images as conditional inputs and
introduces a hierarchical attention mechanism with a dynamic attention
weighting strategy. This mechanism augments the vanilla attention with an
additional context-aware path that leverages pooled spatial features,
effectively expanding the model's receptive field and enhancing region-level
color alignment. Experiments on two benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches, achieving
superior performance in both qualitative and quantitative evaluations.

</details>


### [148] [NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation](https://arxiv.org/abs/2508.09715)
*Devvrat Joshi,Islem Rekik*

Main category: cs.CV

TL;DR: NEURAL compresses multimodal medical imaging data by combining image pruning and semantic integration, drastically reducing storage needs while retaining diagnostic quality.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges posed by the rapid growth of multimodal medical imaging data, which result in storage and transmission difficulties, especially in limited-resource clinical settings.

Method: The framework leverages cross-attention scores from a vision-language model to prune X-ray images and converts them into a compressed graph representation. It integrates this with a knowledge graph derived from clinical reports, forming a unified structure.

Result: NEURAL achieves a 93.4-97.7% reduction in image size while maintaining high diagnostic accuracy, with an AUC range of 0.88-0.95, outperforming other models tested on the MIMIC-CXR and CheXpert Plus datasets.

Conclusion: NEURAL enables efficient data management in medical imaging by resolving the trade-off between size reduction and diagnostic utility, making it particularly valuable for teleradiology and resource-constrained workflows.

Abstract: The rapid growth of multimodal medical imaging data presents significant
storage and transmission challenges, particularly in resource-constrained
clinical settings. We propose NEURAL, a novel framework that addresses this by
using semantics-guided data compression. Our approach repurposes
cross-attention scores between the image and its radiological report from a
fine-tuned generative vision-language model to structurally prune chest X-rays,
preserving only diagnostically critical regions. This process transforms the
image into a highly compressed, graph representation. This unified graph-based
representation fuses the pruned visual graph with a knowledge graph derived
from the clinical report, creating a universal data structure that simplifies
downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for
pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size
while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming
other baseline models that use uncompressed data. By creating a persistent,
task-agnostic data asset, NEURAL resolves the trade-off between data size and
clinical utility, enabling efficient workflows and teleradiology without
sacrificing performance. Our NEURAL code is available at
https://github.com/basiralab/NEURAL.

</details>


### [149] [Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction](https://arxiv.org/abs/2508.09717)
*Shekhnaz Idrissova,Islem Rekik*

Main category: cs.CV

TL;DR: This paper proposes a sheaf-based model for effectively fusing MRI and histopathology data to classify glioblastoma subtypes, enhancing diagnostic accuracy and robustness for incomplete data.


<details>
  <summary>Details</summary>
Motivation: To overcome the invasive nature of current glioblastoma subtype classification methods and the limitations in multimodal data fusion, particularly in preserving structural information and handling incomplete data.

Method: The authors propose a sheaf-based framework for structure-aware and consistent fusion of multimodal data (MRI and histopathology), addressing the challenges of discriminative feature retention and incomplete data scenarios.

Result: The proposed model outperforms existing baseline methods in accuracy and demonstrates robustness in scenarios involving missing or incomplete data.

Conclusion: This model offers significant progress towards non-invasive glioblastoma diagnostics through multimodal data fusion, with potential applications in virtual biopsy tools.

Abstract: Glioblastoma is a highly invasive brain tumor with rapid progression rates.
Recent studies have shown that glioblastoma molecular subtype classification
serves as a significant biomarker for effective targeted therapy selection.
However, this classification currently requires invasive tissue extraction for
comprehensive histopathological analysis. Existing multimodal approaches
combining MRI and histopathology images are limited and lack robust mechanisms
for preserving shared structural information across modalities. In particular,
graph-based models often fail to retain discriminative features within
heterogeneous graphs, and structural reconstruction mechanisms for handling
missing or incomplete modality data are largely underexplored. To address these
limitations, we propose a novel sheaf-based framework for structure-aware and
consistent fusion of MRI and histopathology data. Our model outperforms
baseline methods and demonstrates robustness in incomplete or missing data
scenarios, contributing to the development of virtual biopsy tools for rapid
diagnostics. Our source code is available at
https://github.com/basiralab/MMSN/.

</details>


### [150] [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/abs/2508.09736)
*Lin Long,Yichen He,Wentao Ye,Yiyuan Pan,Yuan Lin,Hang Li,Junbo Zhao,Wei Li*

Main category: cs.CV

TL;DR: The paper introduces M3-Agent, a multimodal agent framework with long-term memory capabilities that surpass existing models in memory-based reasoning. It also proposes a new benchmark, M3-Bench, for evaluating such frameworks.


<details>
  <summary>Details</summary>
Motivation: To create a multimodal agent with human-like long-term memory that can better understand and interact with the environment.

Method: Developed M3-Agent equipped with long-term memory and iterative reasoning abilities, and evaluated it using a newly proposed benchmark, M3-Bench, consisting of robot-sourced and web-sourced videos.

Result: M3-Agent outperformed the strongest baselines in accuracy tests on M3-Bench and other benchmarks by significant margins.

Conclusion: M3-Agent effectively advances multimodal agent capabilities, enabling deeper memory-based reasoning and setting new standards in agent design.

Abstract: We introduce M3-Agent, a novel multimodal agent framework equipped with
long-term memory. Like humans, M3-Agent can process real-time visual and
auditory inputs to build and update its long-term memory. Beyond episodic
memory, it also develops semantic memory, enabling it to accumulate world
knowledge over time. Its memory is organized in an entity-centric, multimodal
format, allowing deeper and more consistent understanding of the environment.
Given an instruction, M3-Agent autonomously performs multi-turn, iterative
reasoning and retrieves relevant information from memory to accomplish the
task. To evaluate memory effectiveness and memory-based reasoning in multimodal
agents, we develop M3-Bench, a new long-video question answering benchmark.
M3-Bench comprises 100 newly recorded real-world videos captured from a robot's
perspective (M3-Bench-robot) and 929 web-sourced videos across diverse
scenarios (M3-Bench-web). We annotate question-answer pairs designed to test
key capabilities essential for agent applications, such as human understanding,
general knowledge extraction, and cross-modal reasoning. Experimental results
show that M3-Agent, trained via reinforcement learning, outperforms the
strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,
achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web
and VideoMME-long, respectively. Our work advances the multimodal agents toward
more human-like long-term memory and provides insights into their practical
design. Model, code and data are available at
https://github.com/bytedance-seed/m3-agent

</details>


### [151] [Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection](https://arxiv.org/abs/2508.09746)
*Zhiqiu Zhang,Dongqi Fan,Mingjie Wang,Qiang Tang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: The paper introduces improvements to image harmonization, focusing on enhancing detail preservation and accommodating complex real-world lighting conditions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in detail preservation and inadequate harmonization capabilities in existing methods based on latent diffusion models (LDM), and to overcome limitations posed by synthetic datasets relying on color transfer.

Method: The authors propose a novel model, R2R, incorporating Clear-VAE for preserving foreground details using Adaptive Filter, and Harmony Controller with Mask-aware Adaptive Channel Attention (MACA) for dynamic adjustments. Additionally, Random Poisson Blending is leveraged to create a more robust synthetic dataset, RPHarmony.

Result: Experiments validate the superiority of the proposed method against previous approaches in terms of quantitative metrics and visual harmony, while the new dataset enhances realism in generated images.

Conclusion: The proposed R2R model and RPHarmony dataset advance the field of image harmonization, offering more realistic results, scalable datasets, and open-access resources for future research.

Abstract: The goal of image harmonization is to adjust the foreground in a composite
image to achieve visual consistency with the background. Recently, latent
diffusion model (LDM) are applied for harmonization, achieving remarkable
results. However, LDM-based harmonization faces challenges in detail
preservation and limited harmonization ability. Additionally, current synthetic
datasets rely on color transfer, which lacks local variations and fails to
capture complex real-world lighting conditions. To enhance harmonization
capabilities, we propose the Region-to-Region transformation. By injecting
information from appropriate regions into the foreground, this approach
preserves original details while achieving image harmonization or, conversely,
generating new composite data. From this perspective, We propose a novel model
R2R. Specifically, we design Clear-VAE to preserve high-frequency details in
the foreground using Adaptive Filter while eliminating disharmonious elements.
To further enhance harmonization, we introduce the Harmony Controller with
Mask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the
foreground based on the channel importance of both foreground and background
regions. To address the limitation of existing datasets, we propose Random
Poisson Blending, which transfers color and lighting information from a
suitable region to the foreground, thereby generating more diverse and
challenging synthetic images. Using this method, we construct a new synthetic
dataset, RPHarmony. Experiments demonstrate the superiority of our method over
other methods in both quantitative metrics and visual harmony. Moreover, our
dataset helps the model generate more realistic images in real examples. Our
code, dataset, and model weights have all been released for open access.

</details>


### [152] [MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models](https://arxiv.org/abs/2508.09779)
*Dianyi Wang,Siyuan Wang,Zejun Li,Yikun Wang,Yitong Li,Duyu Tang,Xiaoyu Shen,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CV

TL;DR: This paper introduces a novel sparse Mixture of Experts (MoE) architecture, named Mixture of Intra- and Inter-Modality Experts (MoIIE), aimed at improving computational efficiency and performance for large vision-language models (LVLMs).


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models demonstrate high computational costs despite their outstanding performance, which necessitates exploring more efficient architectures like sparse Mixture of Experts (MoE). Current approaches struggle to jointly handle modality-specific and cross-modal interactions.

Method: The proposed approach introduces MoIIE, where tokens are guided towards intra-modality and inter-modality expert pools based on their modality. Additionally, a two-stage training strategy is designed to enhance both the sparse MoE framework and multi-modal learning capabilities.

Result: Experiments demonstrate that MoIIE models, with substantially fewer activated parameters (5.5B and 11.3B), achieve comparable or superior performance to existing advanced open-source MoE-LLM-based multi-modal models.

Conclusion: The proposed MoIIE effectively balances performance and computational efficiency, offering a more general and scalable solution for multi-modal tasks in LVLMs. This architecture demonstrates promising advantages over dense and other sparse MoE approaches.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across multi-modal tasks by scaling model size and training data. However,
these dense LVLMs incur significant computational costs and motivate the
exploration of sparse Mixture of Experts (MoE) architectures. While MoE improve
parameter efficiency, effectively applying MoE to simultaneously model
modality-specific features and cross-modal associations in LVLMs remains
challenging. In this work, we propose to incorporate Mixture of Intra- and
Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is
guided by its modality, directing tokens to their respective intra-modality
experts as well as a shared pool of inter-modality experts, enabling the model
to jointly learn rich intra-modal features and cross-modal interactions. We
further introduce an effective and straightforward two-stage training strategy,
which facilitates the direct activation of both MoE and multi-modal
capabilities. Extensive experiments across different data scales and LLM
backbone demonstrate the effectiveness, efficiency and generality of our
approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters
match or even surpass the performance of existing advanced open-source MoE-LLMs
based multi-modal models that involve more activated parameters. The code is
available at https://github.com/AlenjandroWang/MoIIE.

</details>


### [153] [Combinative Matching for Geometric Shape Assembly](https://arxiv.org/abs/2508.09780)
*Nahyuk Lee,Juhong Min,Junhong Lee,Chunghyun Park,Minsu Cho*

Main category: cs.CV

TL;DR: The paper proposes a new combinative matching method for geometric shape assembly, emphasizing interlocking parts with distinct geometric properties, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for geometric assembly rely on matching identical surfaces, which lacks modeling specific properties of interlocking shapes, leading to inefficiencies in aligning parts.

Method: The paper introduces a methodology that models 'identical surface shape' and 'opposite volume occupancy,' using equivariant neural networks to estimate shape orientations and reduce ambiguities in part alignment.

Result: The method proves more effective in assembling interlocking geometric parts and consistently outperforms existing state-of-the-art benchmarks.

Conclusion: The approach advances the field of geometric shape assembly by providing a robust and precise matching mechanism that better handles the challenges of interlocking parts.

Abstract: This paper introduces a new shape-matching methodology, combinative matching,
to combine interlocking parts for geometric shape assembly. Previous methods
for geometric assembly typically rely on aligning parts by finding identical
surfaces between the parts as in conventional shape matching and registration.
In contrast, we explicitly model two distinct properties of interlocking
shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method
thus learns to establish correspondences across regions where their surface
shapes appear identical but their volumes occupy the inverted space to each
other. To facilitate this process, we also learn to align regions in rotation
by estimating their shape orientations via equivariant neural networks. The
proposed approach significantly reduces local ambiguities in matching and
allows a robust combination of parts in assembly. Experimental results on
geometric assembly benchmarks demonstrate the efficacy of our method,
consistently outperforming the state of the art. Project page:
https://nahyuklee.github.io/cmnet.

</details>


### [154] [DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2508.09785)
*Linpu He,Yanan Li,Bingze Li,Elvis Han Cui,Donghui Wang*

Main category: cs.CV

TL;DR: This paper proposes DSS-Prompt for few-shot class-incremental learning (FSCIL), leveraging static and dynamic prompts with minimal modifications to Vision Transformer, achieving state-of-the-art results and reducing catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: The study explores the application of pre-trained models with strong generalization abilities in FSCIL, where the challenge is to learn new concepts from limited samples without forgetting previously learned ones.

Method: DSS-Prompt uses static prompts for domain adaptation and dynamic prompts for instance-aware semantics. Dynamic prompts are generated using a pre-trained multi-modal model, enabling input-aware adjustment. A prototype classifier based on prompted embeddings is utilized.

Result: DSS-Prompt outperforms existing FSCIL methods across four benchmarks, achieving better performance while mitigating forgetting of the old concepts.

Conclusion: DSS-Prompt effectively transforms Vision Transformers into robust FSCIL classifiers, leveraging prompts to adapt, transfer, and retain knowledge, offering a promising direction to address catastrophic forgetting in FSCIL tasks.

Abstract: Learning from large-scale pre-trained models with strong generalization
ability has shown remarkable success in a wide range of downstream tasks
recently, but it is still underexplored in the challenging few-shot
class-incremental learning (FSCIL) task. It aims to continually learn new
concepts from limited training samples without forgetting the old ones at the
same time. In this paper, we introduce DSS-Prompt, a simple yet effective
approach that transforms the pre-trained Vision Transformer with minimal
modifications in the way of prompts into a strong FSCIL classifier. Concretely,
we synergistically utilize two complementary types of prompts in each
Transformer block: static prompts to bridge the domain gap between the
pre-training and downstream datasets, thus enabling better adaption; and
dynamic prompts to capture instance-aware semantics, thus enabling easy
transfer from base to novel classes. Specially, to generate dynamic prompts, we
leverage a pre-trained multi-modal model to extract input-related diverse
semantics, thereby generating complementary input-aware prompts, and then
adaptively adjust their importance across different layers. In this way, on top
of the prompted visual embeddings, a simple prototype classifier can beat
state-of-the-arts without further training on the incremental tasks. We conduct
extensive experiments on four benchmarks to validate the effectiveness of our
DSS-Prompt and show that it consistently achieves better performance than
existing approaches on all datasets and can alleviate the catastrophic
forgetting issue as well.

</details>


### [155] [MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking](https://arxiv.org/abs/2508.09796)
*Yingjie Wang,Zhixing Wang,Le Zheng,Tianxiao Liu,Roujing Li,Xueyao Hu*

Main category: cs.CV

TL;DR: MeMoSORT introduces Memory-assisted Kalman Filter (MeKF) and Motion-adaptive IoU (Mo-IoU) to enhance detection and tracking in multi-object tracking (MOT) scenarios, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Multi-object tracking in human-dominant scenarios is challenging due to complex target motion and occlusions, with traditional methods suffering from filtering errors and association failures.

Method: The paper proposes MeMoSORT, incorporating Memory-assisted Kalman Filter (MeKF) for motion mismatch and Motion-adaptive IoU (Mo-IoU) for better detection and association under occlusions.

Result: Experiments on DanceTrack and SportsMOT benchmarks demonstrate that MeMoSORT achieves state-of-the-art performance with HOTA scores of 67.9% and 82.1%, respectively.

Conclusion: MeMoSORT significantly improves MOT performance by addressing motion model mismatches and enhancing detection-association robustness using neural network memory and adaptive IoU techniques.

Abstract: Multi-object tracking (MOT) in human-dominant scenarios, which involves
continuously tracking multiple people within video sequences, remains a
significant challenge in computer vision due to targets' complex motion and
severe occlusions. Conventional tracking-by-detection methods are fundamentally
limited by their reliance on Kalman filter (KF) and rigid Intersection over
Union (IoU)-based association. The motion model in KF often mismatches
real-world object dynamics, causing filtering errors, while rigid association
struggles under occlusions, leading to identity switches or target loss. To
address these issues, we propose MeMoSORT, a simple, online, and real-time MOT
tracker with two key innovations. First, the Memory-assisted Kalman filter
(MeKF) uses memory-augmented neural networks to compensate for mismatches
between assumed and actual object motion. Second, the Motion-adaptive IoU
(Mo-IoU) adaptively expands the matching space and incorporates height
similarity to reduce the influence of detection errors and association
failures, while remaining lightweight. Experiments on DanceTrack and SportsMOT
show that MeMoSORT achieves state-of-the-art performance, with HOTA scores of
67.9\% and 82.1\%, respectively.

</details>


### [156] [MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention](https://arxiv.org/abs/2508.09802)
*Xin Du,Maoyuan Xu,Zhi Ying*

Main category: cs.CV

TL;DR: The paper introduces MUJICA, an adapter enhancing pre-trained SISR models for upscaling PBR materials, addressing cross-map inconsistencies and improving performance on key metrics.


<details>
  <summary>Details</summary>
Motivation: Upscaling SVBRDF materials is essential for 3D graphics, but current SISR methods face challenges like cross-map inconsistency and poor generalization to data shifts.

Method: The proposed MUJICA adapter integrates with pre-trained Swin-transformer SISR models, leveraging cross-map attention to merge features while maintaining reconstruction capabilities.

Result: MUJICA enhances metrics such as PSNR, SSIM, and LPIPS, and achieves state-of-the-art results on PBR material datasets, even with limited training resources.

Conclusion: MUJICA offers an efficient, flexible approach to improving SISR model performance on PBR materials, solving key challenges and boosting upscaling quality.

Abstract: Physically Based Rendering (PBR) materials are typically characterized by
multiple 2D texture maps such as basecolor, normal, metallic, and roughness
which encode spatially-varying bi-directional reflectance distribution function
(SVBRDF) parameters to model surface reflectance properties and microfacet
interactions. Upscaling SVBRDF material is valuable for modern 3D graphics
applications. However, existing Single Image Super-Resolution (SISR) methods
struggle with cross-map inconsistency, inadequate modeling of modality-specific
features, and limited generalization due to data distribution shifts. In this
work, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention
(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based
SISR models for PBR material super-resolution. MUJICA is seamlessly attached
after the pre-trained and frozen SISR backbone. It leverages cross-map
attention to fuse features while preserving remarkable reconstruction ability
of the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and
HMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map
consistency. Experiments demonstrate that MUJICA enables efficient training
even with limited resources and delivers state-of-the-art performance on PBR
material datasets.

</details>


### [157] [Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology](https://arxiv.org/abs/2508.09805)
*Jonathan Williams Ramirez,Dina Zemlyanker,Lucas Deden-Binder,Rogeny Herisse,Erendira Garcia Pallares,Karthik Gopinath,Harshvardhan Gazula,Christopher Mount,Liana N. Kozanno,Michael S. Marshall,Theresa R. Connors,Matthew P. Frosch,Mark Montine,Derek H. Oakley,Christine L. Mac Donald,C. Dirk Keene,Bradley T. Hyman,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: The paper introduces an automated deep learning model based on the U-Net architecture for segmenting postmortem brain tissue in photographs, eliminating the need for manual intervention.


<details>
  <summary>Details</summary>
Motivation: Manual segmentation of postmortem brain tissue from photographs is currently labor-intensive and expensive, necessitating automation to streamline the process in brain banks and neuropathology labs.

Method: The authors used U-Net architecture trained on 1,414 manually segmented images and 2,000 synthetic images. The synthetic images were created with randomized contrast and masks for better generalizability.

Result: The model achieved high segmentation accuracy, with a median Dice score over 0.98, mean surface distance under 0.4 mm, and 95% Hausdorff distance under 1.6 mm, comparable to human inter-/intra-rater variability.

Conclusion: The proposed segmentation tool automates the process efficiently, offering high accuracy and generalizability, and is made available publicly for community use.

Abstract: Advances in image registration and machine learning have recently enabled
volumetric analysis of \emph{postmortem} brain tissue from conventional
photographs of coronal slabs, which are routinely collected in brain banks and
neuropathology laboratories worldwide. One caveat of this methodology is the
requirement of segmentation of the tissue from photographs, which currently
requires costly manual intervention. In this article, we present a deep
learning model to automate this process. The automatic segmentation tool relies
on a U-Net architecture that was trained with a combination of
\textit{(i)}1,414 manually segmented images of both fixed and fresh tissue,
from specimens with varying diagnoses, photographed at two different sites; and
\textit{(ii)}~2,000 synthetic images with randomized contrast and corresponding
masks generated from MRI scans for improved generalizability to unseen
photographic setups. Automated model predictions on a subset of photographs not
seen in training were analyzed to estimate performance compared to manual
labels -- including both inter- and intra-rater variability. Our model achieved
a median Dice score over 0.98, mean surface distance under 0.4~mm, and 95\%
Hausdorff distance under 1.60~mm, which approaches inter-/intra-rater levels.
Our tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.

</details>


### [158] [Poaching Hotspot Identification Using Satellite Imagery](https://arxiv.org/abs/2508.09812)
*Aryan Pandhi,Shrey Baid,Sanjali Jha*

Main category: cs.CV

TL;DR: The paper highlights the problem of increasing elephant poaching in Africa, emphasizing the need for a Computer Vision (CV) model to identify poaching hotspots based on geographic and environmental indicators.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the growing problem of elephant poaching, which threatens species' survival and is influenced by dynamic, hard-to-track environmental factors. Existing anti-poaching efforts are inefficient as they focus on populated areas while poaching happens in remote regions.

Method: The proposed method involves using a Computer Vision (CV) model combined with satellite imagery to identify and predict poaching hotspots based on geographic indicators like ranger patrol absence, watering holes, altitude, and other factors.

Result: The paper mentions no specific results since the discussion centers around the problem and the proposed solution using CV models and satellite imagery.

Conclusion: It concludes that a CV model can reduce the manual effort needed to track poachers, improve anti-poaching resource deployment accuracy, and survey large areas without disturbing ecosystems or crossing borders illegally.

Abstract: Elephant Poaching in African countries has been a decade-old problem. So much
so that African Forest Elephants are now listed as an endangered species, and
African Savannah Elephants as critically endangered by the IUCN (International
Union for Conservation of Nature). [1] Elephants are hunted primarily for their
ivory tusks which caused many elephants to be born tuskless as a genetic
modification for survival. [2] Data gathered by recent studies shows that
though poaching methods remain the same, the poaching grounds are rather
dynamic. Poachers have shifted to areas with less ranger patrols and several
other factors like watering holes, seasons, altitude etc. cause constant shifts
in poaching hotspot locations. [3] After a period of low poaching from
2000-2014, poaching numbers in African countries are now on the rise again --
WWF (World Wildlife Foundation) says there are 20,000 elephants poached
annually [4]. In African countries, anti-poaching efforts are concentrated near
towns, while a majority of poaching occurs in the deserted regions. All of
these factors result in the need for a Computer Vision Model to identify
poaching hotspots through locating the geographic indicators of favorable
poaching regions. A CV model eliminates the need to manually track poachers and
account for the environmental factors to deploy resources and its combination
with satellite imagery allows us to survey large areas without disturbing local
species or cross border aviation restrictions.

</details>


### [159] [Evolution of Low-Level and Texture Human-CLIP Alignment](https://arxiv.org/abs/2508.09814)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: The study uncovers why CLIP models initially align with human image quality assessments before diverging, attributing it to shifts in learned visual representation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to investigate a curious phenomenon where multi-modal models like CLIP align with low-level human image quality assessments early in training but lose this alignment later.

Method: The study examines two aspects: shape-texture bias alignment and classification accuracy reduction under noise, analyzing their relationships to training dynamics in CLIP.

Result: The research shows that early training emphasizes low-level features, aligning with human perception but introducing texture bias and sensitivity to noise. Over time, the focus shifts toward abstract, shape-based representations that enhance robustness but reduce perceptual alignment.

Conclusion: The findings highlight a trade-off between low-level perceptual alignment and robustness in vision-language models, suggesting shared learning mechanisms and optimization paths.

Abstract: During the training of multi-modal models like CLIP, we observed an
intriguing phenomenon: the correlation with low-level human image quality
assessments peaks in the early epochs before gradually declining. This study
investigates this observation and seeks to understand its causes through two
key factors: shape-texture bias alignment and classification accuracy drop
under noise. Our findings suggest that CLIP initially learn low-level visual
features, enhancing its alignment with low-level human perception but also
increasing its sensitivity to noise and its texture bias. As training
progresses, the model shifts toward more abstract shape-based representations,
improving noise robustness but reducing alignment with low-level human
perception. These results suggest that these factors shared an underlying
learning mechanism and provide new insights into optimizing the trade-off
between perceptual alignment and robustness in vision-language models.

</details>


### [160] [ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video](https://arxiv.org/abs/2508.09818)
*Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Abir Ahmed,Liew Tze Hui*

Main category: cs.CV

TL;DR: This paper introduces ViMoNet, a model combining motion-text and video-text data for better human behavior understanding, along with the VIMOS dataset and benchmark.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in human action understanding by integrating motion-text and video-text data, overcoming single-focus limitations of previous models.

Method: The authors propose a joint training framework, ViMoNet, leveraging complementary datasets: detailed motion-text and comprehensive video-text. Additionally, they introduce the VIMOS dataset and ViMoNet-Bench benchmark.

Result: ViMoNet surpasses prior models in caption generation, motion comprehension, and behavior analysis.

Conclusion: The study shows that blending motion-text and video-text data in a unified model improves human behavior representation and analysis.

Abstract: This study investigates how large language models (LLMs) can be used to
understand human behavior using motion and video data. We think that mixing
both types is essential to completely capture the nuanced movements and
meanings of human actions, in contrast to recent models that simply concentrate
on motion data or films. To address this, we provide ViMoNet, a straightforward
yet effective framework for comprehending, characterizing, and deducing human
action. ViMoNet employs a joint training strategy that leverages the advantages
of two data types: detailed motion-text data, which is more exact, and generic
video-text data, which is more comprehensive but less detailed. This aids in
the model's acquisition of rich data regarding time and space in human
behavior. Additionally, we provide a brand new dataset named VIMOS that
contains a variety of films, motion sequences, instructions, and subtitles. We
developed ViMoNet-Bench, a standardized benchmark with carefully labeled
samples, to evaluate how well models understand human behavior. Our tests show
that ViMoNet outperforms existing methods in caption generation, motion
understanding, and behavior interpretation.

</details>


### [161] [Physical Autoregressive Model for Robotic Manipulation without Action Pretraining](https://arxiv.org/abs/2508.09822)
*Zijian Song,Sihan Qin,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: This work introduces the Physical Autoregressive Model (PAR), leveraging pretrained video models to predict physical robot manipulation dynamics more effectively.


<details>
  <summary>Details</summary>
Motivation: The lack of sufficient manipulation data in robotics sparked interest in using pretrained large models from other modalities, such as video, to enhance robotic learning.

Method: The researchers propose PAR, which combines frames and actions as physical tokens to predict joint robot-environment dynamics. Techniques include a DiT-based de-tokenizer, causal masks, inverse kinematics, parallel training, and KV-cache to enhance efficiency and accuracy.

Result: PAR achieved a 100% success rate in the PushCube task, matched action-pretrained baselines in other tasks, and accurately predicted videos and corresponding action trajectories.

Conclusion: PAR demonstrates the potential of leveraging pretrained video models for effective robotic manipulation, facilitating accurate prediction of physical dynamics and providing a promising strategy for future robotics research.

Abstract: The scarcity of manipulation data has motivated the use of pretrained large
models from other modalities in robotics. In this work, we build upon
autoregressive video generation models to propose a Physical Autoregressive
Model (PAR), where physical tokens combine frames and actions to represent the
joint evolution of the robot and its environment. PAR leverages the world
knowledge embedded in video pretraining to understand physical dynamics without
requiring action pretraining, enabling accurate video prediction and consistent
action trajectories. It also adopts a DiT-based de-tokenizer to model frames
and actions as continuous tokens, mitigating quantization errors and
facilitating mutual enhancement. Furthermore, we incorporate a causal mask with
inverse kinematics, parallel training, and the KV-cache mechanism to further
improve performance and efficiency. Experiments on the ManiSkill benchmark show
that PAR achieves a 100\% success rate on the PushCube task, matches the
performance of action-pretrained baselines on other tasks, and accurately
predicts future videos with tightly aligned action trajectories. These findings
underscore a promising direction for robotic manipulation by transferring world
knowledge from autoregressive video pretraining.

</details>


### [162] [KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.09823)
*Valentin Boussot,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: KonfAI is a flexible, open-source deep learning framework tailored for medical imaging tasks, emphasizing configuration-driven workflows and advanced strategies.


<details>
  <summary>Details</summary>
Motivation: To address the need for modular, transparent, and reproducible frameworks in medical imaging AI tasks, enabling efficient experimental setups.

Method: KonfAI employs YAML configuration files for workflow setup, supports advanced strategies like patch-based learning, test-time augmentation, and multi-model training, and offers modular extensions.

Result: The framework demonstrated success in segmentation, registration, and image synthesis tasks, with top-ranking results in international challenges.

Conclusion: KonfAI enhances workflow efficiency and experimental traceability in medical imaging while supporting customizable extensions, making it a valuable tool for researchers.

Abstract: KonfAI is a modular, extensible, and fully configurable deep learning
framework specifically designed for medical imaging tasks. It enables users to
define complete training, inference, and evaluation workflows through
structured YAML configuration files, without modifying the underlying code.
This declarative approach enhances reproducibility, transparency, and
experimental traceability while reducing development time. Beyond the
capabilities of standard pipelines, KonfAI provides native abstractions for
advanced strategies including patch-based learning, test-time augmentation,
model ensembling, and direct access to intermediate feature representations for
deep supervision. It also supports complex multi-model training setups such as
generative adversarial architectures. Thanks to its modular and extensible
architecture, KonfAI can easily accommodate custom models, loss functions, and
data processing components. The framework has been successfully applied to
segmentation, registration, and image synthesis tasks, and has contributed to
top-ranking results in several international medical imaging challenges. KonfAI
is open source and available at
\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.

</details>


### [163] [Reverse Convolution and Its Applications to Image Restoration](https://arxiv.org/abs/2508.09824)
*Xuhong Huang,Shiqi Liu,Kai Zhang,Ying Tai,Jian Yang,Hui Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: The paper introduces a depthwise reverse convolution operator to address the limitations of existing transposed convolution operators. This operator is applied to image restoration tasks, showing promising results.


<details>
  <summary>Details</summary>
Motivation: Transposed convolution, widely used in neural networks, is not a true inverse of convolution due to differences in mathematical formulations. The paper aims to fill this gap by proposing a reverse convolution operator.

Method: The authors designed a depthwise reverse convolution operator by solving a regularized least-squares optimization problem, while considering aspects like kernel initialization and padding strategies. They also integrated this operator into a block with layer normalization, 1x1 convolution, and GELU activation.

Result: Experiments on Gaussian denoising, super-resolution, and deblurring tasks using the proposed ConverseNet variants showed the reverse convolution operator's effectiveness.

Conclusion: The proposed operator and block hold potential for replacing traditional convolution methods in neural architectures and may inspire the development of novel operators for deep learning applications.

Abstract: Convolution and transposed convolution are fundamental operators widely used
in neural networks. However, transposed convolution (a.k.a. deconvolution) does
not serve as a true inverse of convolution due to inherent differences in their
mathematical formulations. To date, no reverse convolution operator has been
established as a standard component in neural architectures. In this paper, we
propose a novel depthwise reverse convolution operator as an initial attempt to
effectively reverse depthwise convolution by formulating and solving a
regularized least-squares optimization problem. We thoroughly investigate its
kernel initialization, padding strategies, and other critical aspects to ensure
its effective implementation. Building upon this operator, we further construct
a reverse convolution block by combining it with layer normalization,
1$\times$1 convolution, and GELU activation, forming a Transformer-like
structure. The proposed operator and block can directly replace conventional
convolution and transposed convolution layers in existing architectures,
leading to the development of ConverseNet. Corresponding to typical image
restoration models such as DnCNN, SRResNet and USRNet, we train three variants
of ConverseNet for Gaussian denoising, super-resolution and deblurring,
respectively. Extensive experiments demonstrate the effectiveness of the
proposed reverse convolution operator as a basic building module. We hope this
work could pave the way for developing new operators in deep model design and
applications.

</details>


### [164] [Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment](https://arxiv.org/abs/2508.09843)
*Hao Yang,Xu Zhang,Jiaqi Ma,Linwei Zhu,Yun Zhang,Huan Zhang*

Main category: cs.CV

TL;DR: This paper introduces a graph neural network framework for omnidirectional image quality assessment, designed to handle non-uniform spatial distortions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to evaluate spatially non-uniform distortions due to insufficient modeling of spatial variation and feature representation.

Method: The proposed method uses Fibonacci sphere sampling for quality topology generation, along with a Graph Attention Network and graph transformer for capturing short- and long-range quality variations.

Result: The method outperforms other approaches in two large-scale OIQA databases with challenging spatial distortions.

Conclusion: The graph-based approach effectively addresses non-uniform spatial distortions and provides strong generalization capabilities.

Abstract: Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to
evaluate locally non-uniform distortions due to inadequate modeling of spatial
variations in quality and ineffective feature representation capturing both
local details and global context. To address this, we propose a graph neural
network-based OIQA framework that explicitly models structural relationships
between viewports to enhance perception of spatial distortion non-uniformity.
Our approach employs Fibonacci sphere sampling to generate viewports with
well-structured topology, representing each as a graph node. Multi-stage
feature extraction networks then derive high-dimensional node representation.
To holistically capture spatial dependencies, we integrate a Graph Attention
Network (GAT) modeling fine-grained local distortion variations among adjacent
viewports, and a graph transformer capturing long-range quality interactions
across distant regions. Extensive experiments on two large-scale OIQA databases
with complex spatial distortions demonstrate that our method significantly
outperforms existing approaches, confirming its effectiveness and strong
generalization capability.

</details>


### [165] [Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance](https://arxiv.org/abs/2508.09847)
*Dhruvraj Singh Rawat,Enggen Sherpa,Rishikesan Kirupanantha,Tin Hoang*

Main category: cs.CV

TL;DR: The paper benchmarks diffusion models for human face generation, comparing architectures and techniques to improve semantic alignment and controllability.


<details>
  <summary>Details</summary>
Motivation: The need to evaluate and enhance human face generation using diffusion models, particularly under limited data conditions.

Method: The study compares UNet and DiT architectures, uses LoRA-based fine-tuning of Stable Diffusion, integrates InfoNCE loss for attribute embedding, and employs a SegFormer-based segmentation encoder.

Result: Contrastive embedding learning and advanced segmentation encoding were effective for controlled face generation with limited data.

Conclusion: The proposed methods improve semantic alignment and controllability in attribute-guided synthesis in face generation models.

Abstract: We present a benchmark of diffusion models for human face generation on a
small-scale CelebAMask-HQ dataset, evaluating both unconditional and
conditional pipelines. Our study compares UNet and DiT architectures for
unconditional generation and explores LoRA-based fine-tuning of pretrained
Stable Diffusion models as a separate experiment. Building on the
multi-conditioning approach of Giambi and Lisanti, which uses both attribute
vectors and segmentation masks, our main contribution is the integration of an
InfoNCE loss for attribute embedding and the adoption of a SegFormer-based
segmentation encoder. These enhancements improve the semantic alignment and
controllability of attribute-guided synthesis. Our results highlight the
effectiveness of contrastive embedding learning and advanced segmentation
encoding for controlled face generation in limited data settings.

</details>


### [166] [Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment](https://arxiv.org/abs/2508.09850)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Valero Laparra,Jesus Malo*

Main category: cs.CV

TL;DR: This paper investigates how various aspects of Vision Transformers (ViTs) influence their alignment with human perception, demonstrating a trade-off between model complexity and perceptual alignment.


<details>
  <summary>Details</summary>
Motivation: To assess the extent to which Vision Transformers align with human perception, given the gap in research on this topic despite their success in image recognition.

Method: The authors systematically analyzed the effects of model size, dataset size, data augmentation, and regularization on perceptual alignment by using the TID2013 dataset.

Result: Larger models exhibit less perceptual alignment, dataset diversity has minimal impact, repeated image exposure reduces alignment, and stronger augmentation and regularization lower alignment further, especially with repeated training.

Conclusion: There exists a trade-off between improving model complexity and training strategies and the alignment of Vision Transformers with human visual judgment, which should be considered in applications requiring human-like understanding.

Abstract: Vision Transformers (ViTs) achieve remarkable performance in image
recognition tasks, yet their alignment with human perception remains largely
unexplored. This study systematically analyzes how model size, dataset size,
data augmentation and regularization impact ViT perceptual alignment with human
judgments on the TID2013 dataset. Our findings confirm that larger models
exhibit lower perceptual alignment, consistent with previous works. Increasing
dataset diversity has a minimal impact, but exposing models to the same images
more times reduces alignment. Stronger data augmentation and regularization
further decrease alignment, especially in models exposed to repeated training
cycles. These results highlight a trade-off between model complexity, training
strategies, and alignment with human perception, raising important
considerations for applications requiring human-like visual understanding.

</details>


### [167] [OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better](https://arxiv.org/abs/2508.09857)
*Yupeng Zhou,Zhen Li,Ziheng Ouyang,Yuming Chen,Ruoyi Du,Daquan Zhou,Bin Fu,Yihao Liu,Peng Gao,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: The paper focuses on improving discrete video Variational Autoencoders (VAEs) by using pre-trained continuous VAE priors, achieving faster convergence and better performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of unstable training, long training times, and subpar reconstruction quality in discrete video VAEs, while leveraging the advantages of continuous VAEs.

Method: The authors enhance discrete VAEs with pre-trained continuous VAE priors and propose two key structural improvements: multi-token quantization for better latent representation and strengthening first-frame reconstruction. Additionally, they introduce a joint discrete-continuous optimization scheme.

Result: The proposed methods improve training speed, reconstruction quality (e.g., ~1 dB increase in PSNR), and achieve competitive performance on both discrete and continuous representations in a single framework.

Conclusion: The unified approach, named OneVAE, effectively bridges the gap between discrete and continuous video representations, offering notable advancements in reconstruction quality and training efficiency.

Abstract: Encoding videos into discrete tokens could align with text tokens to
facilitate concise and unified multi-modal LLMs, yet introducing significant
spatiotemporal compression compared to continuous video representation.
Previous discrete video VAEs experienced unstable training, long training time,
and degraded reconstruction quality. Given the easier training and superior
performance of continuous VAEs, an intuitive idea is to enhance discrete video
VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between
discrete and continuous representations, we found that FSQ could effectively
preserve pre-trained continuous VAE priors compared to other quantization
methods. By leveraging continuous VAE priors, it converges several times faster
than training from scratch and achieves superior performance at convergence.
Meanwhile, two structural improvements are proposed. First, inspired by how
continuous VAEs enhance reconstruction via enlarged latent dimensions, we
introduce a multi-token quantization mechanism, which achieves nearly a 1 dB
improvement in PSNR without compromising the token compression ratio. Second,
to tackle reconstruction challenges in high-compression video VAEs, we
strengthen first-frame reconstruction, enabling the causal VAE to leverage this
information in subsequent frames and markedly improving the performance of 4 x
16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous
optimization scheme that unifies the two paradigms and, for the first time,
achieves competitive performance on both continuous and discrete
representations within a single network. We name our method OneVAE to reflect
this connection.

</details>


### [168] [HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics](https://arxiv.org/abs/2508.09858)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: HumanGenesis is introduced as a photorealistic video generation framework that tackles geometric inconsistency and motion generalization limitations via four specialized agents.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the challenges of geometric inconsistency, coarse reconstruction, limited motion generalization, and scene disharmony in current synthetic human dynamics approaches.

Method: HumanGenesis integrates geometric and generative modeling through four collaborative agents: Reconstructor, Critique Agent, Pose Guider, and Video Harmonizer. Each agent specializes in tasks ranging from reconstruction, fidelity enhancement, pose generation, to video synthesis.

Result: HumanGenesis demonstrates state-of-the-art performance in text-guided synthesis, video reenactment, and novel-pose generalization by significantly improving expressiveness, geometric fidelity, and scene cohesiveness.

Conclusion: The framework successfully overcomes limitations in synthetic human dynamics by combining advanced geometric and generative modeling frameworks, thus delivering high-quality, expressive, and coherent human motion synthesis in photorealistic videos.

Abstract: \textbf{Synthetic human dynamics} aims to generate photorealistic videos of
human subjects performing expressive, intention-driven motions. However,
current approaches face two core challenges: (1) \emph{geometric inconsistency}
and \emph{coarse reconstruction}, due to limited 3D modeling and detail
preservation; and (2) \emph{motion generalization limitations} and \emph{scene
inharmonization}, stemming from weak generative capabilities. To address these,
we present \textbf{HumanGenesis}, a framework that integrates geometric and
generative modeling through four collaborative agents: (1)
\textbf{Reconstructor} builds 3D-consistent human-scene representations from
monocular video using 3D Gaussian Splatting and deformation decomposition. (2)
\textbf{Critique Agent} enhances reconstruction fidelity by identifying and
refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose
Guider} enables motion generalization by generating expressive pose sequences
using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes
photorealistic, coherent video via a hybrid rendering pipeline with diffusion,
refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis
achieves state-of-the-art performance on tasks including text-guided synthesis,
video reenactment, and novel-pose generalization, significantly improving
expressiveness, geometric fidelity, and scene integration.

</details>


### [169] [E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras](https://arxiv.org/abs/2508.09912)
*Chaoran Feng,Zhenyu Tang,Wangbo Yu,Yatian Pang,Yian Zhao,Jianbin Zhao,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: The paper introduces E-4DGS, an event-driven approach for novel view synthesis using multi-view event streams, overcoming challenges like low light and fast motions.


<details>
  <summary>Details</summary>
Motivation: Existing view synthesis techniques struggle with lighting, motion blur, and dynamic range limitations, prompting exploration of event cameras' advantages.

Method: E-4DGS uses event-based initialization, time-aware slicing splatting, intensity importance pruning, and adaptive contrast thresholds for reconstruction.

Result: E-4DGS outperforms event-only and event-RGB fusion baselines, demonstrating superior multi-view reconstruction in challenging conditions.

Conclusion: The proposed method enables precise and efficient scene capture, advancing multi-view event-based 4D reconstruction in dynamic environments.

Abstract: Novel view synthesis and 4D reconstruction techniques predominantly rely on
RGB cameras, thereby inheriting inherent limitations such as the dependence on
adequate lighting, susceptibility to motion blur, and a limited dynamic range.
Event cameras, offering advantages of low power, high temporal resolution and
high dynamic range, have brought a new perspective to addressing the scene
reconstruction challenges in high-speed motion and low-light scenes. To this
end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting
approach, for novel view synthesis from multi-view event streams with
fast-moving cameras. Specifically, we introduce an event-based initialization
scheme to ensure stable training and propose event-adaptive slicing splatting
for time-aware reconstruction. Additionally, we employ intensity importance
pruning to eliminate floating artifacts and enhance 3D consistency, while
incorporating an adaptive contrast threshold for more precise optimization. We
design a synthetic multi-view camera setup with six moving event cameras
surrounding the object in a 360-degree configuration and provide a benchmark
multi-view event stream dataset that captures challenging motion scenarios. Our
approach outperforms both event-only and event-RGB fusion baselines and paves
the way for the exploration of multi-view event-based reconstruction as a novel
approach for rapid scene capture.

</details>


### [170] [SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection](https://arxiv.org/abs/2508.09913)
*Yachao Liang,Min Yu,Gang Li,Jianguo Jiang,Boquan Li,Feng Yu,Ning Zhang,Xiang Meng,Weiqing Huang*

Main category: cs.CV

TL;DR: The paper introduces an audio-visual speech representation learning approach to detect face forgery videos, achieving superior generalization and robustness without training on fake videos.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of detecting face forgery videos, especially when handling unseen datasets and perturbations. Leveraging the connection between audio signals and facial movements offers a new pathway for improved detection.

Method: The authors propose a self-supervised masked prediction task to learn audio-visual speech representations from real videos, capturing both local and global semantics. The trained model is directly transferred to detect forgeries.

Result: Extensive experiments indicate the method surpasses state-of-the-art approaches in cross-dataset generalization and robustness.

Conclusion: By leveraging audio-visual speech representation learning, the paper provides an effective and generalizable solution to face forgery detection, achieving strong results without requiring fake video data during training.

Abstract: Detection of face forgery videos remains a formidable challenge in the field
of digital forensics, especially the generalization to unseen datasets and
common perturbations. In this paper, we tackle this issue by leveraging the
synergy between audio and visual speech elements, embarking on a novel approach
through audio-visual speech representation learning. Our work is motivated by
the finding that audio signals, enriched with speech content, can provide
precise information effectively reflecting facial movements. To this end, we
first learn precise audio-visual speech representations on real videos via a
self-supervised masked prediction task, which encodes both local and global
semantic information simultaneously. Then, the derived model is directly
transferred to the forgery detection task. Extensive experiments demonstrate
that our method outperforms the state-of-the-art methods in terms of
cross-dataset generalization and robustness, without the participation of any
fake video in model training. Code is available at
https://github.com/Eleven4AI/SpeechForensics.

</details>


### [171] [Towards Comprehensive Cellular Characterisation of H&E slides](https://arxiv.org/abs/2508.09926)
*Benjamin Adjadj,Pierre-Antoine Bannier,Guillaume Horent,Sebastien Mandela,Aurore Lyon,Kathryn Schutte,Ulysse Marteau,Valentin Gaury,Laura Dumont,Thomas Mathieu,Reda Belbahri,Benoît Schmauch,Eric Durand,Katharina Von Loga,Lucie Gillet*

Main category: cs.CV

TL;DR: This paper introduces HistoPLUS, a superior cell detection, segmentation, and classification model for tumor microenvironments that substantially outperforms existing models using fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To address the poor performance of existing methods on rare cell types and their limited ability to generalize across datasets in the analysis of tumor microenvironments.

Method: The authors developed HistoPLUS, trained on a novel, curated pan-cancer dataset of over 108,000 nuclei spanning 13 cell types, and validated its performance across 4 independent cohorts.

Result: HistoPLUS showed a 5.2% improvement in detection quality, a 23.7% increase in F1 classification score, and required 5 times fewer parameters than state-of-the-art models. It also demonstrated the ability to study 7 understudied cell types and robustness in two new oncology applications.

Conclusion: HistoPLUS advances cell type analysis, particularly for rare and less-studied types, while being highly efficient and generalizable, enabling broader applications in TME biomarker research.

Abstract: Cell detection, segmentation and classification are essential for analyzing
tumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing
methods suffer from poor performance on understudied cell types (rare or not
present in public datasets) and limited cross-domain generalization. To address
these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell
analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei
covering 13 cell types. In external validation across 4 independent cohorts,
HistoPLUS outperforms current state-of-the-art models in detection quality by
5.2% and overall F1 classification score by 23.7%, while using 5x fewer
parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types
and brings significant improvements on 8 of 13 cell types. Moreover, we show
that HistoPLUS robustly transfers to two oncology indications unseen during
training. To support broader TME biomarker research, we release the model
weights and inference code at https://github.com/owkin/histoplus/.

</details>


### [172] [Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?](https://arxiv.org/abs/2508.09936)
*Vittorio Pippi,Konstantina Nikolaidou,Silvia Cascianelli,George Retsinas,Giorgos Sfikas,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: This paper evaluates three advanced Handwritten Text Generation (HTG) models for enhancing low-resource Handwritten Text Recognition (HTR) systems.


<details>
  <summary>Details</summary>
Motivation: The digitization of historical manuscripts faces challenges in recognizing handwriting styles that deviate significantly from training data, especially for low-resource collections.

Method: The study systematically compares three state-of-the-art HTG models—based on generative adversarial, diffusion, and autoregressive paradigms—and analyzes their effectiveness in fine-tuning HTR systems using synthetic handwriting data.

Result: A detailed evaluation of visual and linguistic impacts of synthetic data on HTR performance, along with quantitative guidelines for choosing the most effective HTG model, is presented.

Conclusion: The findings offer valuable insights into HTG capabilities and identify areas for improvement in their use for low-resource HTR applications.

Abstract: The digitization of historical manuscripts presents significant challenges
for Handwritten Text Recognition (HTR) systems, particularly when dealing with
small, author-specific collections that diverge from the training data
distributions. Handwritten Text Generation (HTG) techniques, which generate
synthetic data tailored to specific handwriting styles, offer a promising
solution to address these challenges. However, the effectiveness of various HTG
models in enhancing HTR performance, especially in low-resource transcription
settings, has not been thoroughly evaluated. In this work, we systematically
compare three state-of-the-art styled HTG models (representing the generative
adversarial, diffusion, and autoregressive paradigms for HTG) to assess their
impact on HTR fine-tuning. We analyze how visual and linguistic characteristics
of synthetic data influence fine-tuning outcomes and provide quantitative
guidelines for selecting the most effective HTG model. The results of our
analysis provide insights into the current capabilities of HTG methods and
highlight key areas for further improvement in their application to
low-resource HTR.

</details>


### [173] [AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models](https://arxiv.org/abs/2508.09943)
*Tomás de la Sotta,José M. Saavedra,Héctor Henríquez,Violeta Chang,Aline Xavier*

Main category: cs.CV

TL;DR: The paper introduces AST-n, a framework to accelerate diffusion-based LDCT image denoising, achieving high image quality with vastly reduced inference time.


<details>
  <summary>Details</summary>
Motivation: LDCT reduces radiation exposure but introduces noise, necessitating effective methods for image denoising to preserve diagnostic confidence.

Method: AST-n framework begins reverse diffusion from intermediate noise levels and integrates high-order ODE solvers to minimize sampling steps.

Result: Using AST-n sampling, conditioned LDCT models achieved PSNR above 38 dB and SSIM above 0.95 in under 1-second inference time, outperforming baselines.

Conclusion: AST-n accelerates LDCT image reconstruction while maintaining high fidelity, showing potential for clinical implementations of diffusion-based denoising.

Abstract: Low-dose CT (LDCT) protocols reduce radiation exposure but increase image
noise, compromising diagnostic confidence. Diffusion-based generative models
have shown promise for LDCT denoising by learning image priors and performing
iterative refinement. In this work, we introduce AST-n, an accelerated
inference framework that initiates reverse diffusion from intermediate noise
levels, and integrate high-order ODE solvers within conditioned models to
further reduce sampling steps. We evaluate two acceleration paradigms--AST-n
sampling and standard scheduling with high-order solvers -- on the Low Dose CT
Grand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %
of standard dose. Conditioned models using only 25 steps (AST-25) achieve peak
signal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)
above 0.95, closely matching standard baselines while cutting inference time
from ~16 seg to under 1 seg per slice. Unconditional sampling suffers
substantial quality loss, underscoring the necessity of conditioning. We also
assess DDIM inversion, which yields marginal PSNR gains at the cost of doubling
inference time, limiting its clinical practicality. Our results demonstrate
that AST-n with high-order samplers enables rapid LDCT reconstruction without
significant loss of image fidelity, advancing the feasibility of
diffusion-based methods in clinical workflows.

</details>


### [174] [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/abs/2508.09949)
*Trevine Oorloff,Vishwanath Sindagi,Wele Gedara Chaminda Bandara,Ali Shafahi,Amin Ghiasi,Charan Prakash,Reza Ardekani*

Main category: cs.CV

TL;DR: This paper explores using off-the-shelf Stable Diffusion models for visual in-context learning (V-ICL) across multiple vision tasks without additional fine-tuning, improving performance significantly.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current visual in-context learning methods that rely on specialized training or additional data, which reduce generalizability.

Method: The authors repurpose Stable Diffusion models by introducing an in-place attention re-computation in the self-attention layers, allowing them to explicitly incorporate the context of example prompts without further fine-tuning.

Result: The proposed approach successfully adapted to six vision tasks, such as foreground segmentation and single object detection, achieving performance improvements like an 8.9% mIoU gain on the Pascal-5i dataset compared to prior methods.

Conclusion: Stable Diffusion models can be effectively utilized for multiple vision tasks with minimal modifications, demonstrating the model's adaptability and ability to handle varied tasks through ensembling examples.

Abstract: Large language models (LLM) in natural language processing (NLP) have
demonstrated great potential for in-context learning (ICL) -- the ability to
leverage a few sets of example prompts to adapt to various tasks without having
to explicitly update the model weights. ICL has recently been explored for
computer vision tasks with promising early outcomes. These approaches involve
specialized training and/or additional data that complicate the process and
limit its generalizability. In this work, we show that off-the-shelf Stable
Diffusion models can be repurposed for visual in-context learning (V-ICL).
Specifically, we formulate an in-place attention re-computation within the
self-attention layers of the Stable Diffusion architecture that explicitly
incorporates context between the query and example prompts. Without any
additional fine-tuning, we show that this repurposed Stable Diffusion model is
able to adapt to six different tasks: foreground segmentation, single object
detection, semantic segmentation, keypoint detection, edge detection, and
colorization. For example, the proposed approach improves the mean intersection
over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by
8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,
respectively. Additionally, we show that the proposed method is able to
effectively leverage multiple prompts through ensembling to infer the task
better and further improve the performance.

</details>


### [175] [LIA-X: Interpretable Latent Portrait Animator](https://arxiv.org/abs/2508.09959)
*Yaohui Wang,Di Yang,Xinyuan Chen,Francois Bremond,Yu Qiao,Antitza Dantcheva*

Main category: cs.CV

TL;DR: LIA-X is a facial dynamics transfer model using interpretable latent space navigation for fine-grained, controllable manipulation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in facial dynamics transfer methods by introducing fine-grained, interpretable control over motion and semantics.

Method: An autoencoder leveraging a Sparse Motion Dictionary in the latent space to disentangle and navigate facial dynamics for high precision motion transfer.

Result: Outperformed existing methods in self-reenactment and cross-reenactment tasks with scalability to large datasets and models.

Conclusion: LIA-X combines interpretability and control for practical facial animation and editing applications, proving its superiority over prior techniques.

Abstract: We introduce LIA-X, a novel interpretable portrait animator designed to
transfer facial dynamics from a driving video to a source portrait with
fine-grained control. LIA-X is an autoencoder that models motion transfer as a
linear navigation of motion codes in latent space. Crucially, it incorporates a
novel Sparse Motion Dictionary that enables the model to disentangle facial
dynamics into interpretable factors. Deviating from previous 'warp-render'
approaches, the interpretability of the Sparse Motion Dictionary allows LIA-X
to support a highly controllable 'edit-warp-render' strategy, enabling precise
manipulation of fine-grained facial semantics in the source portrait. This
helps to narrow initial differences with the driving video in terms of pose and
expression. Moreover, we demonstrate the scalability of LIA-X by successfully
training a large-scale model with approximately 1 billion parameters on
extensive datasets. Experimental results show that our proposed method
outperforms previous approaches in both self-reenactment and cross-reenactment
tasks across several benchmarks. Additionally, the interpretable and
controllable nature of LIA-X supports practical applications such as
fine-grained, user-guided image and video editing, as well as 3D-aware portrait
video manipulation.

</details>


### [176] [January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis](https://arxiv.org/abs/2508.09966)
*Amir Hosseinian,Ashkan Dehghani Zahedani,Umer Mansoor,Noosheen Hashemi,Mark Woodward*

Main category: cs.CV

TL;DR: The paper introduces the January Food Benchmark dataset (JFB) for food image analysis, a benchmarking framework with metrics, and a specialized model that surpasses general-purpose models in performance.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the lack of standardized evaluation methods and quality benchmark datasets in AI for nutritional analysis.

Method: The authors created JFB, a 1,000-image dataset with human-validated annotations, and a comprehensive benchmarking framework with robust metrics. They also evaluated models, including a specialized model they developed.

Result: Their specialized model, january/food-vision-v1, achieved an Overall Score of 86.2, which is 12.1 points higher than the best general-purpose model.

Conclusion: The work provides a valuable dataset and framework to improve and evaluate future AI developments in nutritional analysis.

Abstract: Progress in AI for automated nutritional analysis is critically hampered by
the lack of standardized evaluation methodologies and high-quality, real-world
benchmark datasets. To address this, we introduce three primary contributions.
First, we present the January Food Benchmark (JFB), a publicly available
collection of 1,000 food images with human-validated annotations. Second, we
detail a comprehensive benchmarking framework, including robust metrics and a
novel, application-oriented overall score designed to assess model performance
holistically. Third, we provide baseline results from both general-purpose
Vision-Language Models (VLMs) and our own specialized model,
january/food-vision-v1. Our evaluation demonstrates that the specialized model
achieves an Overall Score of 86.2, a 12.1-point improvement over the
best-performing general-purpose configuration. This work offers the research
community a valuable new evaluation dataset and a rigorous framework to guide
and benchmark future developments in automated nutritional analysis.

</details>


### [177] [MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification](https://arxiv.org/abs/2508.09967)
*Tianqi Xiang,Yi Li,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: This paper introduces a Meta-Optimized Classifier (MOC) to enhance histopathology WSI classification with few-shot learning methods, demonstrating significant improvements over prior approaches.


<details>
  <summary>Details</summary>
Motivation: Recent histopathology VLFM-based methods struggle with data scarcity when classifying WSIs using few-shot learning paradigms, leaving room for diagnostic accuracy improvement in clinical setups.

Method: The MOC approach combines a meta-learner to optimize classifier configurations and a classifier bank of diverse candidates for better pathological interpretations.

Result: Experiments show MOC achieves superior results, including a 10.4% AUC increase on the TCGA-NSCLC benchmark compared to state-of-the-art methods, with up to 26.25% gains in 1-shot conditions.

Conclusion: MOC represents a significant advancement in addressing diagnostic challenges of limited training data in clinical environments.

Abstract: Recent advances in histopathology vision-language foundation models (VLFMs)
have shown promise in addressing data scarcity for whole slide image (WSI)
classification via zero-shot adaptation. However, these methods remain
outperformed by conventional multiple instance learning (MIL) approaches
trained on large datasets, motivating recent efforts to enhance VLFM-based WSI
classification through fewshot learning paradigms. While existing few-shot
methods improve diagnostic accuracy with limited annotations, their reliance on
conventional classifier designs introduces critical vulnerabilities to data
scarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)
comprising two core components: (1) a meta-learner that automatically optimizes
a classifier configuration from a mixture of candidate classifiers and (2) a
classifier bank housing diverse candidate classifiers to enable a holistic
pathological interpretation. Extensive experiments demonstrate that MOC
outperforms prior arts in multiple few-shot benchmarks. Notably, on the
TCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art
few-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,
offering a critical advancement for clinical deployments where diagnostic
training data is severely limited. Code is available at
https://github.com/xmed-lab/MOC.

</details>


### [178] [PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image](https://arxiv.org/abs/2508.09973)
*Geonhee Sim,Gyeongsik Moon*

Main category: cs.CV

TL;DR: PERSONA is a framework for creating personalized 3D human avatars with pose-driven deformations from a single image, combining diffusion-based and 3D-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods either require numerous pose-rich videos for pose-driven deformations or struggle with identity preservation when learning from in-the-wild videos.

Method: PERSONA generates pose-rich videos from a single input image using a diffusion-based approach, then optimizes a 3D-based avatar using balanced sampling and geometry-weighted optimization.

Result: The framework achieves sharp and authentic renderings that preserve identity and overall quality across diverse poses.

Conclusion: By integrating the strengths of both approaches, PERSONA enables practical and high-quality avatar creation with minimal input data.

Abstract: Two major approaches exist for creating animatable human avatars. The first,
a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a
single person, achieving personalization through a disentangled identity
representation. However, modeling pose-driven deformations, such as non-rigid
cloth deformations, requires numerous pose-rich videos, which are costly and
impractical to capture in daily life. The second, a diffusion-based approach,
learns pose-driven deformations from large-scale in-the-wild videos but
struggles with identity preservation and pose-dependent identity entanglement.
We present PERSONA, a framework that combines the strengths of both approaches
to obtain a personalized 3D human avatar with pose-driven deformations from a
single image. PERSONA leverages a diffusion-based approach to generate
pose-rich videos from the input image and optimizes a 3D avatar based on them.
To ensure high authenticity and sharp renderings across diverse poses, we
introduce balanced sampling and geometry-weighted optimization. Balanced
sampling oversamples the input image to mitigate identity shifts in
diffusion-generated training videos. Geometry-weighted optimization prioritizes
geometry constraints over image loss, preserving rendering quality in diverse
poses.

</details>


### [179] [A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation](https://arxiv.org/abs/2508.09977)
*Shuting He,Peilin Ji,Yitong Yang,Changshuo Wang,Jiayi Ji,Yinglin Wang,Henghui Ding*

Main category: cs.CV

TL;DR: This paper reviews 3D Gaussian Splatting (3DGS) applications, focusing on segmentation, editing, and generation tasks, with comparisons to Neural Radiance Fields (NeRF).


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to explore the potential of 3DGS as a compact and explicit 3D scene representation that can support downstream tasks requiring semantic and geometric understanding.

Method: The authors categorize 3DGS applications and compare them with NeRF-based methods, summarizing key design principles, datasets, evaluation protocols, and trends.

Result: The paper provides a comprehensive analysis of 3DGS applications and presents shared principles and emerging trends in tasks like segmentation and editing.

Conclusion: 3DGS is a promising tool for photorealistic rendering and downstream applications, with continually updated resources supporting further research.

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative
to Neural Radiance Fields (NeRF) for 3D scene representation, offering
high-fidelity photorealistic rendering with real-time performance. Beyond novel
view synthesis, the explicit and compact nature of 3DGS enables a wide range of
downstream applications that require geometric and semantic understanding. This
survey provides a comprehensive overview of recent progress in 3DGS
applications. It first introduces 2D foundation models that support semantic
understanding and control in 3DGS applications, followed by a review of
NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS
applications into segmentation, editing, generation, and other functional
tasks. For each, we summarize representative methods, supervision strategies,
and learning paradigms, highlighting shared design principles and emerging
trends. Commonly used datasets and evaluation protocols are also summarized,
along with comparative analyses of recent methods across public benchmarks. To
support ongoing research and development, a continually updated repository of
papers, code, and resources is maintained at
https://github.com/heshuting555/Awesome-3DGS-Applications.

</details>


### [180] [LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit](https://arxiv.org/abs/2508.09981)
*Chengtao Lv,Bilang Zhang,Yang Yong,Ruihao Gong,Yushi Huang,Shiqiao Gu,Jiajun Wu,Yumeng Shi,Jinyang Guo,Wenya Wang*

Main category: cs.CV

TL;DR: LLMC+ introduces a benchmark and toolkit for compressing large vision-language models (VLMs), addressing limitations in the evaluation and methods of existing approaches.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models have impressive capabilities but are computationally expensive. Addressing their inefficiencies requires robust, systematic compression techniques.

Method: The paper introduces LLMC+, a benchmark and plug-and-play toolkit supporting over 20 algorithms. It evaluates token-level and model-level compression across major VLM families.

Result: LLMC+ highlights: distinct strategies are needed for spatial and temporal redundancy, token reduction methods struggle in detailed tasks, and combined compression approaches yield better results.

Conclusion: LLMC+ offers a comprehensive evaluation framework to inspire better-performing, efficient VLM technologies and address real-world scenarios.

Abstract: Large Vision-Language Models (VLMs) exhibit impressive multi-modal
capabilities but suffer from prohibitive computational and memory demands, due
to their long visual token sequences and massive parameter sizes. To address
these issues, recent works have proposed training-free compression methods.
However, existing efforts often suffer from three major limitations: (1)
Current approaches do not decompose techniques into comparable modules,
hindering fair evaluation across spatial and temporal redundancy. (2)
Evaluation confined to simple single-turn tasks, failing to reflect performance
in realistic scenarios. (3) Isolated use of individual compression techniques,
without exploring their joint potential. To overcome these gaps, we introduce
LLMC+, a comprehensive VLM compression benchmark with a versatile,
plug-and-play toolkit. LLMC+ supports over 20 algorithms across five
representative VLM families and enables systematic study of token-level and
model-level compression. Our benchmark reveals that: (1) Spatial and temporal
redundancies demand distinct technical strategies. (2) Token reduction methods
degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)
Combining token and model compression achieves extreme compression with minimal
performance loss. We believe LLMC+ will facilitate fair evaluation and inspire
future research in efficient VLM. Our code is available at
https://github.com/ModelTC/LightCompress.

</details>


### [181] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: The paper introduces Story2Board, a training-free framework for creating expressive storyboards from natural language with enhanced coherence and diversity.


<details>
  <summary>Details</summary>
Motivation: Existing methods of storyboard generation struggle with preserving visual storytelling elements such as spatial composition, background evolution, and narrative pacing.

Method: The framework uses Latent Panel Anchoring and Reciprocal Attention Value Mixing mechanisms to maintain visual consistency without requiring training or fine-tuning. It also employs a language model to convert free-form stories into structured panel-level prompts.

Result: Story2Board outperforms existing baselines, producing more dynamic, coherent, and engaging storyboards. This is supported by qualitative results, quantitative tests, and a user study.

Conclusion: The proposed framework improves visual storytelling in storyboard generation, addressing coherence and diversity issues, without architectural changes or extensive training.

Abstract: We present Story2Board, a training-free framework for expressive storyboard
generation from natural language. Existing methods narrowly focus on subject
identity, overlooking key aspects of visual storytelling such as spatial
composition, background evolution, and narrative pacing. To address this, we
introduce a lightweight consistency framework composed of two components:
Latent Panel Anchoring, which preserves a shared character reference across
panels, and Reciprocal Attention Value Mixing, which softly blends visual
features between token pairs with strong reciprocal attention. Together, these
mechanisms enhance coherence without architectural changes or fine-tuning,
enabling state-of-the-art diffusion models to generate visually diverse yet
consistent storyboards. To structure generation, we use an off-the-shelf
language model to convert free-form stories into grounded panel-level prompts.
To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain
narratives designed to assess layout diversity and background-grounded
storytelling, in addition to consistency. We also introduce a new Scene
Diversity metric that quantifies spatial and pose variation across storyboards.
Our qualitative and quantitative results, as well as a user study, show that
Story2Board produces more dynamic, coherent, and narratively engaging
storyboards than existing baselines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [182] [Verify Distributed Deep Learning Model Implementation Refinement with Iterative Relation Inference](https://arxiv.org/abs/2508.09505)
*Zhanghan Wang,Ding Ding,Hang Zhu,Haibin Lin,Aurojit Panda*

Main category: cs.DC

TL;DR: The paper proposes GraphGuard, a framework that uses static analysis to ensure distributed machine learning models match their sequential counterparts, preventing bugs during the distribution process.


<details>
  <summary>Details</summary>
Motivation: Bugs often arise when distributing large machine learning models across GPUs, causing discrepancies in outputs between the distributed and sequential versions. There is a need to verify that the distributed model's outputs remain consistent with the original sequential model.

Method: The authors introduce GraphGuard, which applies iterative rewriting and static analysis to check 'model refinement,' ensuring the distributed model reflects the sequential model's outputs accurately.

Result: The approach is shown to scale effectively for large-scale models like GPT and Llama-3, with GraphGuard also offering actionable insights for bug localization.

Conclusion: GraphGuard effectively ensures correctness and aids debugging in distributed machine learning models, aligning outputs with their sequential counterparts and supporting scalability for large model deployments.

Abstract: Distributed machine learning training and inference is common today because
today's large models require more memory and compute than can be provided by a
single GPU. Distributed models are generally produced by programmers who take a
sequential model specification and apply several distribution strategies to
distribute state and computation across GPUs. Unfortunately, bugs can be
introduced in the process, and a distributed model implementation's outputs
might differ from the sequential model's outputs. In this paper, we describe an
approach to statically identify such bugs by checking model refinement, that
is, can the sequential model's outputs be reconstructed from the distributed
model's outputs? Our approach, implemented in GraphGuard, uses iterative
rewriting to prove model refinement. Our approach can scale to today's large
models and deployments: we evaluate it using GPT and Llama-3. Further, it
provides actionable output that aids in bug localization.

</details>


### [183] [HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap](https://arxiv.org/abs/2508.09591)
*Wenxiang Lin,Xinglin Pan,Lin Zhang,Shaohuai Shi,Xuan Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: HierMoE, an advanced system for mixture-of-experts (MoE) language models, improves training efficiency through token deduplication and expert swap techniques, achieving faster communication and training compared to existing systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address scalability challenges in MoE transformer training, particularly the substantial communication and load imbalances across GPUs in a distributed cluster.

Method: HierMoE utilizes two topology-aware techniques: token deduplication to reduce communication traffic and expert swap to balance GPU workloads. Additionally, theoretical models are created to optimize these strategies under varying configurations and environments.

Result: Experiments on a 32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models demonstrate HierMoE is 1.55× to 3.32× faster in communication and 1.18× to 1.27× faster in end-to-end training compared to other state-of-the-art systems.

Conclusion: HierMoE significantly enhances the scalability and efficiency of MoE transformer models, making it a practical solution for overcoming GPU communication and imbalance issues during distributed training.

Abstract: The sparsely activated mixture-of-experts (MoE) transformer has become a
common architecture for large language models (LLMs) due to its sparsity, which
requires fewer computational demands while easily scaling the model size. In
MoE models, each MoE layer requires to dynamically choose tokens to activate
particular experts for computation while the activated experts may not be
located in the same device or GPU as the token. However, this leads to
substantial communication and load imbalances across all GPUs, which obstructs
the scalability of distributed systems within a GPU cluster. To this end, we
introduce HierMoE to accelerate the training of MoE models by two
topology-aware techniques: 1) token deduplication to reduce the communication
traffic, and 2) expert swap to balance the workloads among all GPUs. To enable
the above two proposed approaches to be more general, we build theoretical
models aimed at achieving the best token duplication and expert swap strategy
under different model configurations and hardware environments. We implement
our prototype HierMoE system atop Megatron-LM and conduct experiments on a
32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results
show that our HierMoE achieves $1.55\times$ to $3.32\times$ faster
communication and delivers $1.18\times$ to $1.27\times$ faster end-to-end
training compared to state-of-the-art MoE training systems, Tutel-2DH,
SmartMoE, and Megatron-LM.

</details>


### [184] [Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes](https://arxiv.org/abs/2508.09663)
*Philipp A. Friese,Ahmed Eleliemy,Utz-Uwe Haus,Martin Schulz*

Main category: cs.DC

TL;DR: The paper discusses an enhancement to the HPE Slingshot network stack for secure, multi-tenant use in converged HPC-Cloud environments.


<details>
  <summary>Details</summary>
Motivation: Modern scientific workflows demand a harmonized approach to cater to both cloud isolation needs and HPC performance requirements, which current Slingshot implementations lack.

Method: The authors designed and implemented an extension to the HPE Slingshot networking stack using Kubernetes to enable container-granular, secure multi-tenancy.

Result: The proposed integration allows for secure and efficient multi-tenant access to Slingshot RDMA networking with minimal performance overhead.

Conclusion: Adapting HPE Slingshot for multi-tenant converged HPC-Cloud environments is achievable with secure and efficient solutions, paving the way for broader applicability of this hardware.

Abstract: Converged HPC-Cloud computing is an emerging computing paradigm that aims to
support increasingly complex and multi-tenant scientific workflows. These
systems require reconciliation of the isolation requirements of native cloud
workloads and the performance demands of HPC applications. In this context,
networking hardware is a critical boundary component: it is the conduit for
high-throughput, low-latency communication and enables isolation across
tenants. HPE Slingshot is a high-speed network interconnect that provides up to
200 Gbps of throughput per port and targets high-performance computing (HPC)
systems. The Slingshot host software, including hardware drivers and network
middleware libraries, is designed to meet HPC deployments, which predominantly
use single-tenant access modes. Hence, the Slingshot stack is not suited for
secure use in multi-tenant deployments, such as converged HPC-Cloud
deployments. In this paper, we design and implement an extension to the
Slingshot stack targeting converged deployments on the basis of Kubernetes. Our
integration provides secure, container-granular, and multi-tenant access to
Slingshot RDMA networking capabilities at minimal overhead.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [185] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: This paper introduces a Transformer-based model for real-time aircraft ETA prediction, achieving both high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate and efficient ETA predictions for airborne aircraft are crucial for improving runway sequencing and real-time arrival management due to dynamic airspace conditions.

Method: The study employs a Transformer model featuring tokenization and multi-head self-attention mechanisms to process raw input data, such as aircraft and weather parameters, at a frequency of 1Hz. This eliminates the need for complex feature engineering.

Result: Using data from Singapore's Changi Airport, the proposed model improved prediction accuracy by 7% over XGBoost while reducing computational time by 61%. For 40 aircraft, inference required just 51.7 microseconds.

Conclusion: The proposed model demonstrates significant promise for real-time applications, combining enhanced prediction accuracy with reduced computing resource requirements.

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [186] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: The paper introduces MoLAN, a framework for multimodal sentiment analysis that dynamically assigns denoising strengths to modality blocks, ensuring fine-grained noise suppression while retaining critical information. MoLAN+ further achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address issues where multimodal sentiment analysis suffers from irrelevant or misleading visual and auditory information, and inappropriate feature denoising methods risk losing crucial information.

Method: The method involves dividing modality features into blocks and dynamically assigning denoising strengths to each block based on its noise level and semantic relevance. This approach works across modalities and integrates seamlessly into various multimodal models.

Result: Experiments across five models and four datasets validate MoLAN's effectiveness, and MoLAN+ achieves state-of-the-art performance in multimodal sentiment analysis.

Conclusion: The proposed MoLAN framework serves as a robust and flexible mechanism for enhancing multimodal sentiment analysis, effectively suppressing noise while preserving essential information, and its extended version, MoLAN+, achieves leading performance benchmarks.

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [187] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: The paper introduces MiCo, a framework for mixed-precision quantization (MPQ) in neural networks, optimizing latency and accuracy for edge AI applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve storage, computation efficiency, and accuracy for quantized neural networks (QNNs) by addressing challenges in exploring MPQ schemes and enabling their deployment in practice.

Method: MiCo employs an optimization algorithm to determine high-accuracy MPQ schemes while adhering to latency constraints. It features hardware-aware latency models for faster exploration and enables direct deployment of MPQ models into bare-metal C code.

Result: The proposed framework achieves an optimal balance between accuracy and speedup while ensuring compatibility with the latency needs of edge devices.

Conclusion: MiCo provides a robust end-to-end solution for MPQ exploration and deployment, addressing flexibility, efficiency, and real-world application barriers in QNNs for edge AI systems.

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [188] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: The paper introduces a transformer-based in-context learning (ICL) optimizer for optimizing WiFi 7 channel access, outperforming existing methods in throughput under unknown node densities.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency in the binary exponential backoff scheme under dynamic channel conditions and inaccuracies in node density estimation in current approaches.

Method: Propose a transformer-based ICL system that uses pre-collected collision-threshold data examples and query cases as prompts. Design an efficient algorithm for near-optimal contention window threshold prediction, even with erroneous data input.

Result: The transformer-based ICL method demonstrates fast convergence and nearly optimal throughput performance in NS-3 simulations compared to existing model-based and DRL-based methods, even with unknown node densities.

Conclusion: The proposed ICL optimizer effectively overcomes challenges in dynamic environments, providing a robust and efficient alternative to existing methods for channel access.

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [189] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: The paper introduces Motif-2.6B, a 2.6-billion-parameter foundational model with innovative features to enhance LLM capabilities in a computationally efficient manner.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to develop a computationally efficient yet high-performing foundational LLM, accessible to research groups, especially emerging ones.

Method: Motif-2.6B employs architectural innovations such as Differential Attention and PolyNorm activation functions, tested through extensive experimentation, to optimize the model.

Result: Motif-2.6B consistently meets or exceeds the performance of similar-sized state-of-the-art models across diverse benchmarks, demonstrating scalability and real-world usability.

Conclusion: Motif-2.6B advances the field of LLMs by providing an efficient, scalable, and powerful model, serving as a robust foundation for further research and application.

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [190] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: The study questions whether complex sequence mixers (e.g., attention mechanisms) are necessary for time series analysis and proposes replacing them with dense layers. Experiments across 29 benchmarks suggest that dense layers achieve comparable or superior performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reassess the necessity of complex sequence mixers in time series models, as simpler architectures might offer comparable results, challenging the trend of increasingly complex designs.

Method: The method involves systematically replacing sequence mixers in state-of-the-art time series analysis models with dense layers, using the MatrixMixer framework, and evaluating the impact on 29 benchmark tasks.

Result: The experiments showed that dense layers yield comparable or better performance than sequence mixers in many cases, questioning the intrinsic value of complex sequence mixers.

Conclusion: The findings challenge the assumption that more complex architectures are inherently superior in time series analysis and offer evidence supporting the viability of simpler designs like dense layers.

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [191] [Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture](https://arxiv.org/abs/2508.09693)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.LG

TL;DR: This paper constructs a mathematical framework for temporal anchoring within embedding spaces using operator-theoretic approaches, providing proofs for essential theorems and demonstrating robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation focuses on creating a robust theoretical foundation for embedding spaces, particularly to formalize drift maps, affine projections, and their convergence for temporal anchoring.

Method: The paper employs operator theory to develop and prove various lemmas and theorems, such as contraction principles and uniform convergence bounds, alongside formalizing computational frameworks like Manuscript Computer.

Result: Key results include explicit uniform-gap convergence envelopes, robust ontological convergence under affine anchors, finite-run equivalence with perturbation bounds, and layer-contraction conditions for attention mechanisms.

Conclusion: The work delivers a rigorous mathematical underpinning for drift-projection dynamics in embedding spaces, advancing both theoretical and applied understanding in the field.

Abstract: We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.

</details>


### [192] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: DIG2RSI, a new deep learning framework, addresses simultaneous feedback and unobserved confounders in peer causal effect estimation within complex networks, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: Current methods have limitations in estimating peer causal effects accurately due to challenges like simultaneous feedback and unobserved confounders.

Method: DIG2RSI combines I-G transformation and 2SRI (instrumental variable technique) within a deep learning framework to disentangle feedback and address confounding.

Result: Empirical tests on semi-synthetic and real-world data show DIG2RSI's accuracy improvement over existing methods.

Conclusion: DIG2RSI effectively captures complex relationships and mitigates bias, proving its superiority in peer causal effect estimation datasets.

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [193] [Bayesian autoregression to optimize temporal Matérn kernel Gaussian process hyperparameters](https://arxiv.org/abs/2508.09792)
*Wouter M. Kouw*

Main category: cs.LG

TL;DR: The paper introduces a more efficient method for optimizing Matérn kernel Gaussian processes by framing it as a recursive Bayesian estimation task.


<details>
  <summary>Details</summary>
Motivation: Optimizing hyperparameters for temporal Gaussian processes is challenging using conventional methods like marginal likelihood maximization and Hamiltonian Monte Carlo sampling.

Method: The authors propose recasting the optimization problem as a recursive Bayesian estimation for autoregressive model parameters.

Result: The method surpasses alternatives in runtime efficiency and achieves lower root mean square errors in Gaussian process regression.

Conclusion: The proposed procedure is a more effective and faster approach for optimizing Matérn kernel Gaussian processes compared to traditional methods.

Abstract: Gaussian processes are important models in the field of probabilistic
numerics. We present a procedure for optimizing Mat\'ern kernel temporal
Gaussian processes with respect to the kernel covariance function's
hyperparameters. It is based on casting the optimization problem as a recursive
Bayesian estimation procedure for the parameters of an autoregressive model. We
demonstrate that the proposed procedure outperforms maximizing the marginal
likelihood as well as Hamiltonian Monte Carlo sampling, both in terms of
runtime and ultimate root mean square error in Gaussian process regression.

</details>


### [194] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: The paper introduces AdaPO, an adaptive reinforcement learning framework for improving self-evaluation in Large Multimodal Models (LMMs), addressing problems of reward hacking in conventional methods.


<details>
  <summary>Details</summary>
Motivation: Self-evaluation is essential for LMMs to improve multi-turn conversation capabilities, but current foundation models lack this feature. Conventional reinforcement learning approaches face issues like reward hacking and model collapse.

Method: The proposed AdaPO framework includes two components: Adaptive Reward Model (ARM) that evaluates training state based on multi-turn trajectory performance distribution, and Reward Aware Dynamic KL Regularization, which dynamically adjusts penalties using reward gaps.

Result: AdaPO demonstrated significant improvements in reasoning and self-evaluation across 8 benchmarks and various models.

Conclusion: AdaPO provides an effective and adaptive mechanism for self-evaluation, mitigating traditional RL issues, and promotes better training focus based on task progress without manual intervention.

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [195] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: The paper introduces a framework for enhancing flow-matching generative models to solve inverse problems while adhering to physical constraints like PDEs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enforcing physical consistency and solving inverse problems in scientific models efficiently in a data-driven manner.

Method: Post-training fine-tuning using weak-form residuals of PDEs and a learnable latent parameter predictor for joint optimization in the generative process.

Result: Achieves improved adherence to PDE constraints and successfully estimates unknown physical parameters in benchmark problems.

Conclusion: The method effectively integrates generative modeling with physics-based inference, enabling data-efficient modeling and discovery for physical systems.

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [196] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: The paper introduces EvaDrive, a multi-objective reinforcement learning framework for autonomous driving, utilizing adversarial optimization to enable iterative, human-like decision-making in trajectory planning.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving struggles with emulating human-like iterative decision-making due to the segmented design of current frameworks and the scalarization bias in reinforcement learning methods.

Method: The proposed framework, EvaDrive, employs adversarial optimization, framing trajectory planning as a multi-round adversarial game to achieve closed-loop co-evolution between trajectory generation and evaluation. It combines autoregressive intent modeling, diffusion-based spatial refinement, and a trainable multi-objective critic.

Result: EvaDrive achieves state-of-the-art performance in autonomous driving, with 94.9 PDMS on NAVSIM v1 and 64.96 Driving Score on Bench2Drive, surpassing multiple benchmarks while maintaining trajectory diversity and dynamic driving styles.

Conclusion: EvaDrive offers a novel scalarization-free trajectory optimization approach that enables closed-loop adversarial frameworks for human-like iterative decision-making, overcoming limitations in current reinforcement learning and trajectory generation methods.

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [197] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: The paper discusses how integrating 15 datasets creates a large database with detailed data for studying Type 1 diabetes and hypoglycemia.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve diabetes care, particularly hypoglycemia prediction, by addressing the lack of large datasets to leverage machine learning.

Method: The study integrates 15 datasets to create a large database of 2510 subjects and conducts a correlation analysis between glucose levels and heart rate.

Result: The integrated database includes 149 million glucose measurements (4% hypoglycemic) and reveals correlations between glucose levels and heart rate preceding hypoglycemia.

Conclusion: The database offers a valuable resource for diabetes research, but challenges like data imbalance and missing values need to be addressed.

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [198] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: A new Physics-Guided Memory Network (PgMN) combines deep learning and physics-based models to forecast energy consumption, addressing challenges like limited or unavailable historical data.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in accurately forecasting energy consumption when historical data are limited or unavailable, as traditional deep learning and physics-based models each have limitations in such cases.

Method: The PgMN integrates deep learning with physics-based predictions and includes components like Parallel Projection Layers, a Memory Unit, and a Memory Experience Module to address incomplete data, handle biases, and extend forecasts.

Result: PgMN demonstrated high accuracy and effectiveness in energy forecasting across various scenarios, including cases with limited historical data or newly constructed buildings.

Conclusion: PgMN enhances forecasting reliability and applicability in dynamic building environments, making it particularly useful for smart grids and smart building systems.

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [199] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: This paper introduces an unsupervised explainable AI framework to detect, identify, and characterize replay attacks on next-gen nuclear reactor data, achieving over 95% accuracy on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Advanced nuclear reactors are growing more reliant on digital systems, which generate complex multivariate data. Ensuring data integrity in such systems against deception attacks like replay attacks is critical for safe operation, especially as existing solutions lack sufficient root cause identification and use unrealistic synthetic data.

Method: The authors propose an unsupervised explainable AI (XAI) framework combining an autoencoder and a customized windowSHAP algorithm for detecting, identifying, and characterizing replay attacks in real-time during dynamic reactor processes.

Result: The proposed framework achieved over 95% accuracy in detecting, locating, and classifying replay attacks on real-world datasets obtained from Purdue’s nuclear reactor PUR-1, even when up to six signals were concurrently replayed.

Conclusion: The proposed XAI framework is effective for ensuring data integrity in nuclear reactors by accurately identifying and characterizing complex replay attacks in real-world scenarios, offering better explainability and applicability than traditional approaches.

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [200] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: The paper proposes the Adjustable Sequence Length (ASL) scheme for improving stochastic computing neural networks, achieving up to 60% energy and latency savings with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and energy savings of stochastic computing neural networks for resource-constrained IoT applications while addressing unexplored layer-wise mixed-precision implementation challenges.

Method: The paper introduces ASL, which employs mixed-precision truncation strategies (coarse-grained and fine-grained) using an operator-norm-based theoretical model and random forest regression for sensitivity analysis.

Result: ASL reduces energy and latency overheads by up to 60% with negligible accuracy loss in evaluations on a 32nm SC MLP.

Conclusion: The ASL scheme is shown to be feasible and beneficial for IoT applications, providing significant energy and latency savings in stochastic computing designs with mixed-precision truncation strategies.

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [201] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: The paper introduces a diffusion-model-based approach for synthetic population generation to address challenges in agent-based transportation simulations, focusing on improving feasibility and diversity.


<details>
  <summary>Details</summary>
Motivation: Population synthesis is essential for agent-based modeling in intelligent transportation systems but suffers from issues like sparse survey data and the curse of dimensionality, leading to inaccurate modeling.

Method: The authors propose a diffusion model-based method to estimate the population's joint distribution. It aims to reduce missing data gaps (sampling zeros) while minimizing unrealistic data generation (structural zeros).

Result: The proposed method outperforms recent approaches like VAEs and GANs in balancing feasibility and diversity in synthesized populations, as evaluated by metrics such as marginal distribution similarity and diversity.

Conclusion: The diffusion model-based method provides a more effective way of generating synthetic populations for ABM transportation simulation, addressing the limitations of existing generative models.

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [202] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: PatchECG introduces a robust model for variable ECG layouts through adaptive missing representation learning, with high AUROC scores for arrhythmia detection.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges posed by asynchronous signals and partial blackout loss in ECG layouts across hospitals, which hinder existing models from diagnosing arrhythmia effectively.

Method: The study developed PatchECG, employing a masking training strategy to focus on critical patches with lead dependencies, enabling robust arrhythmia detection across diverse layouts.

Result: PatchECG achieved an average AUROC of 0.835 on the PTB-XL dataset and remained stable with layout changes. External validation also showed strong performance with AUROCs of 0.778 and 0.893 for real-world ECG data.

Conclusion: The proposed PatchECG framework displays superior robustness and diagnostic reliability under different ECG layouts, outperforming classic and large-scale pretraining models.

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [203] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: SVG-1M introduces a dataset linking SVGs with natural language descriptions, enabling the SVGen model to convert text into scalable vector graphics effectively.


<details>
  <summary>Details</summary>
Motivation: SVG creation is time-intensive, despite its utility in front-end and UI/UX design. A solution is needed to simplify the translation of ideas into precise vector formats.

Method: A dataset (SVG-1M) with text-to-SVG pairs was created using data augmentation and annotation. SVGen, an end-to-end model, was developed using curriculum and reinforcement learning to generate SVGs from text inputs.

Result: SVGen surpasses traditional rendering methods and general models in producing accurate, structurally complete SVGs efficiently.

Conclusion: The paper advances SVG generation from text input, providing open-source tools and datasets that improve semantic alignment and graphical precision.

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [204] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: This paper addresses the high cost of large multimodal model training and fine-tuning by presenting a lightweight training-free method using Retrieval-Augmented Generation (RAG) with linear mapping, improving textual descriptions of images.


<details>
  <summary>Details</summary>
Motivation: Pre-trained large multimodal models face alignment challenges between textual and visual representations, and fine-tuning to resolve this is costly and impractical.

Method: The proposed method uses a linear mapping in conjunction with Retrieval-Augmented Generation (RAG) to bridge the modality gap, enabling efficient text-image alignment without fine-tuning. It further incorporates an iterative synthetic description generation technique for refinement.

Result: Significant improvements were demonstrated on two benchmark multimodal datasets.

Conclusion: The approach efficiently addresses the modality gap in multimodal models while avoiding high training and fine-tuning costs, resulting in better textual descriptions for images.

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [205] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: The paper proposes FedMP, a method for improving Federated Learning (FL) in non-IID data scenarios, particularly for medical imaging.


<details>
  <summary>Details</summary>
Motivation: Challenges in FL arise due to non-IID datasets, which are common in medical imaging and impair model performance.

Method: FedMP uses stochastic feature manifold completion and class-prototypes for aligning feature manifolds within semantically consistent subspaces.

Result: Experimental results show FedMP surpasses existing FL algorithms across medical and multi-domain datasets.

Conclusion: FedMP proves effective in addressing non-IID challenges, demonstrating improved model performance while considering dimensions, communication efficiency, and privacy concerns.

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [206] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: The paper introduces Dynamic Quantization Training (DQT), a framework that enables instance-based mixed-precision quantization without deviating from integer-only hardware.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiency of traditional dynamic quantization methods that rely on costly dequantize-to-float and requantize-to-integer cycles.

Method: Developed a nested integer representation enabling bit-width switching via a lightweight, integer-only bit-shift operation, and created a dynamic controller for runtime layer-specific quantization.

Result: DQT demonstrated state-of-the-art performance on ResNet18 for CIFAR-10 and ResNet50 for ImageNet, achieving higher accuracy with reduced computational costs compared to existing methods.

Conclusion: DQT advances efficient, adaptive AI by enabling low-cost dynamic quantization and optimizing resource use without breaking integer-only hardware paradigms.

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [207] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: The paper presents scAGC, an innovative method for clustering single-cell RNA-seq data, addressing key challenges like high dimensionality and zero elements using adaptive cell graph learning with contrastive guidance.


<details>
  <summary>Details</summary>
Motivation: To overcome the difficulties in accurately annotating cell types due to the high dimensionality and sparsity of scRNA-seq data, which limit the effectiveness of traditional and static graph-based clustering methods.

Method: The proposed scAGC method includes a topology-adaptive graph autoencoder with Gumbel-Softmax sampling for dynamic graph refinement, a Zero-Inflated Negative Binomial loss for robust feature reconstruction, and a contrastive learning objective to stabilize graph topology changes.

Result: scAGC surpasses state-of-the-art methods in clustering accuracy, achieving top-performing NMI and ARI scores across 9 real scRNA-seq datasets.

Conclusion: scAGC offers a robust and adaptive solution for single-cell clustering, improving graph learning and feature representation to handle the unique challenges of scRNA-seq data.

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [208] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: This paper proposes a long-term client selection framework for federated learning in the Internet of Vehicles (IoV) that incorporates an auction-based mechanism to enhance data quality and truthfulness.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in federated learning for IoV, particularly the issues of non-IID data and inefficient client-selection metrics that affect model performance and resource optimization.

Method: Introduces the Long-term Client-Selection Federated Learning based on Truthful Auction (LCSFLA) framework, combining a long-term data quality assessment mechanism and a deposit-based auction system to ensure truthful participation and energy efficiency.

Result: Experimental validation shows improved performance and stability for federated learning models in IoV scenarios by addressing non-IID data issues.

Conclusion: LCSFLA provides a robust solution to enhance model training accuracy and efficiency in IoV-based federated learning systems while ensuring truthful client behavior and better resource management.

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [209] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: The paper surveys contact-based and contactless breath analysis methods, emphasizing machine learning advancements, applications, key challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional breath analysis methods, while reliable, often lack comfort and practicality for long-term use. The paper seeks to explore the potential of advanced techniques like machine learning for more effective, noninvasive respiratory monitoring.

Method: The survey reviews recent advancements in breath analysis, focusing on contactless methods (like Wi-Fi and acoustic sensing) and machine learning techniques, while addressing challenges and trends in data processing, multi-user scenarios, and AI models.

Result: It synthesizes methodologies, comparing machine learning and deep learning models, while highlighting challenges like dataset scarcity and proposing trends like Explainable AI and hybrid modeling.

Conclusion: The survey builds a comprehensive framework for breath analysis, identifying open research opportunities and connecting modern technologies with practical health monitoring solutions.

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [210] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: The paper introduces Fine-Grained Safety Neurons (FGSN) to enhance safety in fine-tuned large language models with minimal compromise to utility.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to balance safety and utility during and after fine-tuning of large language models, leaving them with vulnerabilities and inefficiencies.

Method: The proposed method identifies and localizes fine-grained safety neurons while reducing interference with task-specific neurons. Uses a training-free continual projection mechanism for optimizing safety neuron parameters.

Result: Experiments show substantial reductions in harmfulness scores and attack success rates across various models, with minimal parameter changes and preservation of utility.

Conclusion: The technique improves LLM safety, aligns better with human preferences, and introduces continual defense mechanisms against emerging threats without impacting task performance.

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [211] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: TokenCast is an LLM-driven framework for time series forecasting that integrates historical numerical data with contextual textual features using symbolic representations.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting accuracy is limited by challenges in integrating historical numerical data with unstructured contextual textual data.

Method: TokenCast employs a tokenizer to convert numerical sequences into temporal tokens, embeds these and contextual tokens into a unified semantic space using a pre-trained LLM, and fine-tunes this model to forecast numerical data.

Result: TokenCast demonstrated effectiveness and generalizability in time series forecasting across multiple real-world datasets enhanced with contextual information.

Conclusion: TokenCast provides a novel approach to improve context-aware time series forecasting by bridging numerical and textual data modalities using symbolic representations and LLMs.

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [212] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: This paper introduces Discrete Diffusion Forcing (D2F), a method allowing diffusion-based large language models (dLLMs) to achieve faster inference speeds over autoregressive (AR) LLMs by blending diffusion and autoregressive decoding techniques.


<details>
  <summary>Details</summary>
Motivation: The current generation of diffusion-based large language models (dLLMs) cannot yet surpass autoregressive (AR) LLMs in inference speed despite their potential for generating multiple tokens per iteration.

Method: The proposed Discrete Diffusion Forcing (D2F) method enables (1) block-wise autoregressive generation to utilize key-value (KV) caching and (2) inter-block parallel decoding to predict tokens without completing prior blocks, implemented via asymmetric distillation from pre-trained dLLMs.

Result: D2F-optimized dLLMs achieve over 2.5× faster inference compared to LLaMA3 and Qwen2.5 on GSM8K, and more than 50× acceleration compared to baseline diffusion models like LLaDA and Dream, without sacrificing output quality.

Conclusion: The D2F approach transforms dLLMs into a hybrid paradigm, successfully improving their inference efficiency, making them competitive with autoregressive LLMs in both speed and quality.

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [213] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: MIPCGRL improves controllability in instructed procedural content generation by leveraging multi-objective learning with sentence embeddings.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to utilize the rich expressiveness of textual instructions, especially with complex, multi-objective tasks.

Method: The proposed MIPCGRL integrates sentence embeddings into a multi-objective learning framework, using multi-label classification and multi-head regression networks.

Result: MIPCGRL increases controllability in multi-objective tasks by up to 13.8%, enhancing the capability to handle complex instructions.

Conclusion: MIPCGRL demonstrates a promising approach to improve expressive and flexible content generation through advanced textual understanding.

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [214] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: The paper introduces a meta-learning framework to optimize inference acceleration schemes in decentralized AI systems, reducing computational costs and improving responsiveness.


<details>
  <summary>Details</summary>
Motivation: High computational costs and challenges like scalability and data security in large-scale models require efficient management of inference acceleration schemes in decentralized systems.

Method: The authors propose a meta-learning framework that automates optimal acceleration method selection by analyzing historical performance data across tasks.

Result: The framework consistently outperformed traditional methods in efficiency and performance by systematically identifying the best acceleration strategies.

Conclusion: Inference acceleration in decentralized systems is viable using meta-learning, advancing scalable and cost-effective AI solutions.

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [215] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: This paper introduces ADT4Coupons, a framework for sequential online coupon distribution to optimize long-term revenue.


<details>
  <summary>Details</summary>
Motivation: Existing coupon distribution strategies fail to adequately use platform-user sequential interactions, limiting performance despite access to extensive data.

Method: The proposed framework, ADT4Coupons, models sequential coupon distribution by combining general scenario modeling, historical data integration, and efficient updates.

Result: Evaluation on real-world, public, and synthetic datasets shows that ADT4Coupons outperforms existing strategies.

Conclusion: ADT4Coupons is a superior solution for optimizing online coupon distribution and boosting long-term revenue in diverse marketing contexts.

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [216] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: The paper introduces the Construction Safety Dataset (CSDataset), a dataset from OSHA records that combines structured attributes and unstructured narratives to advance construction safety research.


<details>
  <summary>Details</summary>
Motivation: To address the lack of diverse and comprehensive datasets in construction safety research, hindering effective analysis of risks and safety measures.

Method: The authors developed the CSDataset by compiling incidents, inspections, and violations from OSHA data, combining structured and unstructured information. They also benchmarked preliminary approaches and performed cross-level analyses.

Result: The findings include insights such as complaint-driven inspections reducing future incident likelihood by 17.3%, showcasing the utility of the dataset.

Conclusion: The dataset enables improved analyses of construction safety risks and strategies via machine learning and large language models, offering a resource for future research and applications.

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [217] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: This paper introduces MoQE, a quantization inference framework using multiple specialized experts to mitigate accuracy degradation in quantized deep learning models.


<details>
  <summary>Details</summary>
Motivation: Quantization is crucial for deploying deep learning models on resource-constrained devices, but it often reduces model accuracy.

Method: The authors leverage a Mixture-of-Experts (MoE) architecture to combine multiple quantization variants and dynamically route input data to the best-suited expert.

Result: Experiments on ResNet, LLaMA, and Qwen models with benchmark datasets show MoQE achieves comparable performance to SOTA quantization models without increasing inference latency.

Conclusion: MoQE effectively mitigates performance degradation in quantized models while maintaining efficient inference, showing promise for CV and NLP applications.

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [218] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: The paper introduces a novel repair algorithm for optimizing the transfer process in microLED fabrication, achieving significant efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and limitations in traditional and RL-based computational models used for laser-enabled selective transfer in high-throughput microLED fabrication.

Method: The paper proposes a differentiable transfer module incorporated in a repair algorithm to model discrete shifts of transfer platforms, enabling gradient-based optimization and eliminating the need for handcrafted features.

Result: The proposed method achieves a 50% reduction in transfer steps and can plan for 2000x2000 arrays in less than 2 minutes, significantly outperforming traditional approaches.

Conclusion: The presented method provides a scalable, efficient, and flexible solution for optimizing microLED repair processes, with practical applications in AR/VR and advanced display manufacturing.

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [219] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: The paper introduces Hi-Vec, a test-time adaptation method with hierarchical layers to improve robustness under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Existing test-time adaptation methods struggle with complex distribution shifts due to reliance on single-dimensional layers.

Method: Hi-Vec employs hierarchical layers, dynamic layer selection, weight merging, and linear layer agreement for adaptive test-time adaptation.

Result: Hi-Vec demonstrates improved performance on diverse target datasets, proving its robustness against challenges like uncertainty and outliers.

Conclusion: Hi-Vec enhances state-of-the-art test-time adaptation methods by dynamically responding to varied distribution complexities.

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [220] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: The paper introduces GSMT, a hybrid model for trajectory prediction of buses, blending GAT and RNN models with a task corrector to enhance accuracy, especially in urban environments using limited GPS data.


<details>
  <summary>Details</summary>
Motivation: To improve trajectory prediction for buses in urban areas with limited access to multimodal data, which impacts the efficacy of intelligent transportation systems.

Method: The method employs GSMT, integrating a Graph Attention Network and a sequence-to-sequence RNN. A task corrector refines predictions via clustering historical trajectories and detecting motion patterns. The model uses a two-stage approach: fusion of dynamic/static information followed by prediction refinement.

Result: Experiments on real-world data from Kuala Lumpur show GSMT's superior performance compared to existing methods in both short-term and long-term prediction tasks.

Conclusion: The proposed GSMT model enhances trajectory prediction for buses in dense urban environments, providing a robust solution for limited GPS-only data scenarios and outperforming existing methods in accuracy.

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [221] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: The paper proposes a Quantum Inspired Graph Neural Network (QI-GNN) combined with either QBoost or traditional models to improve blockchain anti-money laundering (AML) efforts, achieving a high F2 score.


<details>
  <summary>Details</summary>
Motivation: Fraudulent activities in blockchain networks require innovative solutions for their detection, specifically in anti-money laundering efforts.

Method: The paper combines Quantum Inspired GNNs with either QBoost or Random Forest Classifier, introducing a Canonical Polyadic (CP) decomposition layer to better analyze complex data.

Result: Their model achieved an F2 score of 74.8% in detecting fraudulent blockchain transactions, outperforming classical machine learning methods.

Conclusion: Quantum-inspired techniques, enhanced by CP structures, show strong promise in surpassing traditional methods for financial security and fraud detection in complex networks.

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [222] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: The paper proposes a novel framework using LLMs for tabular data modeling in zero- and few-shot learning scenarios by generating feature values with example-free prompts and building training-free prototypes.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of effectively using large language models for tabular data modeling in zero-shot and few-shot scenarios.

Method: A framework involving example-free prompts to generate feature values via LLM, which are used to build scalable and robust prototypes without training classifiers or finetuning models.

Result: Demonstrated effectiveness of the framework through extensive experiments in zero-shot and few-shot tabular learning tasks.

Conclusion: The proposed approach bypasses constraints of traditional example-based prompts, enabling scalable and robust tabular learning with LLMs.

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [223] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: The paper proposes a deep learning framework that detects odors using signal analysis from mouse olfactory bulbs, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Odor detection has important applications in fields like food safety and medical diagnostics, but current methods face challenges with complex mixtures and reliability.

Method: The authors developed an ensemble model combining ResCNN and AttentionCNN to decode odor presence from olfactory bulb LFPs recorded in mice.

Result: Their model achieved 86.6% mean accuracy, 81.0% F1-score, and 0.9247 AUC, outperforming previous benchmarks.

Conclusion: The work confirms that LFP features and olfactory bulb signals are sufficient for single-trial odor detection, highlighting the potential of deep learning in understanding olfactory systems.

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [224] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: The paper investigates over-squashing in message-passing Graph Neural Networks (GNNs), introduces metrics to quantify it, and examines the impact of rewiring techniques on various datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of over-squashing in GNNs, which limits their ability to handle long-range node information effectively, and evaluate the efficacy of rewiring techniques.

Method: The paper proposes metrics using decay rates of mutual sensitivity between nodes and extends them to graph-level statistics. It further employs causal experiments to measure the impacts of rewiring strategies on over-squashing across different datasets.

Result: Empirical analysis reveals substantial over-squashing in graph classification datasets mitigated by rewiring to varying extents, while node classification datasets show less over-squashing and mixed results for rewiring effectiveness.

Conclusion: Rewiring techniques are most effective when over-squashing is significant but must be applied judiciously. A diagnostic tool is proposed to guide practitioners in deciding the utility of rewiring before model training.

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [225] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: This paper introduces new adversarial attack vulnerabilities for collaborative multi-agent reinforcement learning (c-MARL) systems, tested under realistic constraints.


<details>
  <summary>Details</summary>
Motivation: To address the lack of realistic investigations into adversarial vulnerabilities in c-MARL, especially beyond training-time attacks and idealistic assumptions.

Method: Propose novel algorithms to generate adversarial perturbations that target agents' environmental perceptions, tested empirically on three benchmarks and 22 environments.

Result: Demonstrated effective and sample-efficient adversarial attacks that work across diverse c-MARL algorithms and environments, needing only 1,000 samples.

Conclusion: This research highlights significant vulnerabilities in c-MARL under realistic conditions, emphasizing the need for more robust defense mechanisms.

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [226] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: The paper introduces a novel framework for personalized learning in computer science education by automatically extracting explainable Knowledge Components (KCs) from student code using structural patterns.


<details>
  <summary>Details</summary>
Motivation: To improve personalized learning in computer science by addressing challenges like insufficient explainability of extracted KCs and diverse student coding solutions.

Method: A variational autoencoder guided by an attention-based code representation model identifies structural code patterns, which are clustered into pattern-based KCs. These are evaluated using learning curve analysis and Deep Knowledge Tracing (DKT).

Result: The proposed method demonstrates meaningful learning trajectories and improves DKT predictive performance compared to traditional methods.

Conclusion: The study presents a scalable, automated, and explainable framework for modeling student knowledge through granular and interpretable code patterns, enhancing computer science education.

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [227] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: Dataset distillation can compress reinforcement learning tasks into small supervised learning datasets, enabling efficient training in one gradient step.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency of training by compressing large datasets or complex tasks like reinforcement learning into small, easy-to-learn synthetic datasets using data distillation.

Method: The paper extends proximal policy optimization for meta-learning and applies this method to distill reinforcement learning environments into one-step supervised learning datasets. This includes distilling environments like a cart-pole problem, MuJoCo environments, and Atari games.

Result: The results demonstrate the ability of dataset distillation to compress reinforcement learning tasks into minimal synthetic datasets while showing generalizability across different learning architectures.

Conclusion: Dataset distillation not only reduces the complexity of reinforcement learning tasks but also transforms them into supervised learning problems, making learning more efficient and adaptable to diverse architectures.

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [228] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: This paper proposes a decentralized weather forecasting system using Federated Learning and blockchain to enhance privacy, security, and scalability.


<details>
  <summary>Details</summary>
Motivation: Current centralized weather forecasting systems face security, scalability, and reliability challenges.

Method: The paper integrates Federated Learning for privacy-preserving model training, Ethereum blockchain for verification, a reputation-based voting mechanism for model trustworthiness, and IPFS for efficient storage.

Result: The proposed framework improves forecasting accuracy and strengthens system resilience and scalability, as shown in experiments.

Conclusion: The system is a viable solution for secure, scalable, and real-world weather forecasting applications.

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [229] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: The paper presents GNNev, an exact verification method for graph neural networks against adversarial attacks, supporting sum, max, and mean aggregation functions. Evaluations show its superior performance on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Ensure the robustness of GNNs against adversarial attacks in sensitive applications like fraud detection and healthcare.

Method: Developed GNNev, an exact verification solver using constraint solving with bound tightening and iterative relaxation, supporting sum, max, and mean aggregation functions.

Result: GNNev outperforms existing tools in exact verification tasks, validated across multiple benchmarks and real-world fraud datasets.

Conclusion: GNNev represents a significant advancement in verifying GNNs, offering strong guarantees against adversarial perturbations, expanding support for additional aggregation functions, and demonstrating high efficiency and usability.

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [230] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: The paper proposes a biologically-inspired synaptic pruning method for neural networks using magnitude-based pruning, improving performance over dropout, particularly in financial time series forecasting.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of dropout regularization, which randomly deactivates neurons without biological inspiration, and improve performance by mimicking synaptic pruning seen in biological brains.

Method: A magnitude-based pruning method is integrated into the training loop. Connections with low importance are progressively pruned using a cubic sparsity schedule. This directly eliminates weights based on magnitudes, bypassing the traditional fine-tuning phase.

Result: The method demonstrated consistent improvements across diverse time series prediction models (RNN, LSTM, Transformer) on multiple datasets, including a reduction in financial forecasting error by up to 52% in select cases.

Conclusion: The proposed dynamic pruning mechanism is an efficient, biologically-inspired alternative to dropout, providing significant statistical improvements and easy adaptability for time series forecasting and other architectures.

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [231] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: RicciFlowRec leverages Ricci curvature and flow on dynamic graphs to improve interpretability and robustness in financial recommendations.


<details>
  <summary>Details</summary>
Motivation: Develop a framework to attribute root causes in financial systems and enhance interpretability and robustness in decision-making.

Method: Utilize Ricci curvature and flow on financial graphs to model stock interactions, measure stress, propagate shocks, and derive risk-aware ranking.

Result: Tested on S&P 500 data with FinBERT sentiment, achieving improved robustness and interpretability under synthetic perturbations.

Conclusion: RicciFlowRec establishes curvature-based financial attribution and ranking, with future plans for portfolio optimization and forecasting.

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [232] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: Restricting sparse autoencoders (SAEs) to medical text improves their ability to capture domain-specific latent features, reducing reconstruction errors and enhancing interpretability.


<details>
  <summary>Details</summary>
Motivation: Broad-domain sparse autoencoders struggle with capturing domain-specific structures, often leading to reconstruction errors and latent feature complications.

Method: The paper trains domain-confined SAEs using JumpReLU on layer-20 activations of Gemma-2 models with 195k clinical QA examples.

Result: Domain-specific SAEs demonstrated a 20% improvement in variance explanation, better loss recovery, and reduced linear residual error compared to conventional SAEs.

Conclusion: Restricting SAE training to specific domains, like medical text, enhances latent feature decomposition, improving reconstruction fidelity and interpretability while questioning general-purpose scaling approaches.

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [233] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: This paper explores the capability of text-to-image models to align dementia-related speech with generated images and achieves 75% accuracy in detecting dementia solely based on such images.


<details>
  <summary>Details</summary>
Motivation: To investigate whether text-to-image models, originally designed for natural language, can align and detect dementia-related speech information through generated images.

Method: The study generates images from pathological speech input using text-to-image models and analyzes the alignment using explainability methods to identify significant language features.

Result: Dementia detection achieved a 75% accuracy from images generated based on dementia-related speech using the ADReSS dataset.

Conclusion: Text-to-image models show promise for aligning pathological information with images and can facilitate dementia detection, offering novel interpretability techniques.

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [234] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: The paper presents a federated learning-based framework for financial risk analysis across institutions, enabling collaboration without sharing raw data.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges in financial risk analysis caused by data privacy concerns and the need for collaborative modeling across institutions.

Method: The paper proposes a distributed optimization framework where local sub-models are trained at institutions using a feature attention mechanism and temporal modeling. Differential privacy and noise injection secure model parameters, which are centrally aggregated to build a global model.

Result: The framework was validated through experiments, demonstrating superior performance over traditional centralized methods and other federated learning variants in terms of communication efficiency, model accuracy, risk detection, and cross-market generalization.

Conclusion: The proposed framework effectively enhances risk analysis with strong modeling performance and practical utility while ensuring data privacy, making it a secure and efficient solution for financial institutions.

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [235] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: An unsupervised anomaly detection method combining graph convolution and Transformers for distributed backend service systems.


<details>
  <summary>Details</summary>
Motivation: Challenges in distributed services such as complex dependencies, behavior evolution, and no labeled data.

Method: Dynamic graph construction, graph convolution for structure extraction, and Transformers for temporal modeling fused into anomaly vectors.

Result: Outperforms existing models with sensitivity analyses demonstrating robustness and effective anomaly detection.

Conclusion: Improved expressiveness and stability make the method suitable for practical deployment in cloud monitoring systems.

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [236] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: This study proposes DGS-MAML, a meta-learning algorithm enhancing robustness and adaptability through bi-level optimization, outperforming existing models in generalization.


<details>
  <summary>Details</summary>
Motivation: To address limited training data and enhance task generalization in meta-learning contexts.

Method: DGS-MAML merges gradient matching with sharpness-aware minimization using a bi-level optimization framework, supported by PAC-Bayes theoretical analysis and convergence guarantees.

Result: Experimental testing on benchmark datasets shows DGS-MAML improves accuracy and task generalization compared to established methods.

Conclusion: DGS-MAML demonstrates significant advancements in meta-learning applications, enabling effective few-shot learning and rapid adaptation with public implementation access.

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [237] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: IHGNN introduces implicit equilibrium formulations into hypergraph neural networks, avoiding deep architectures and enabling stable, efficient global propagation. It achieves superior performance and robustness on citation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current hypergraph neural networks struggle with capturing long-range dependencies and exhibit instability as the number of layers increases.

Method: The authors propose IHGNN, which computes representations using a nonlinear fixed-point equation rather than relying on deep, stacked layers. This approach ensures stable global propagation while addressing training stability with provable convergence and implicit-gradient techniques.

Result: IHGNN outperforms traditional graph/hypergraph neural network baselines in accuracy and robustness across citation benchmarks. It also demonstrates strong resilience to random initialization and changes in hyperparameters.

Conclusion: IHGNN is a practically valuable framework for higher-order relational learning on hypergraphs with its strong generalization, robustness, and efficiency.

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [238] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: The paper introduces NEXICA, an algorithm designed to identify causative locations of traffic slowdowns on highways using a novel probabilistic approach and classifier, outperforming existing solutions with higher accuracy and faster computation speeds.


<details>
  <summary>Details</summary>
Motivation: To address persistent road traffic congestion by focusing resources on pinpointing causative locations for traffic slowdowns more efficiently.

Method: Develop a novel algorithm (NEXICA) using event-driven time series analysis, maximum likelihood probabilistic modeling to compute slowdown causation probabilities, and a binary classifier trained on known causal connections between road locations.

Result: Validated on six months of data from 195 highway speed sensors in Los Angeles, yielding superior performance compared to state-of-the-art baselines in terms of accuracy and computation speed.

Conclusion: The NEXICA algorithm effectively identifies causative areas of road slowdowns, enabling targeted interventions in traffic congestion management, and presents a significant improvement over existing methods.

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [239] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: The paper introduces CoGenT, a framework unifying contrastive and generative self-supervised learning methods for multivariate time series to address their respective limitations.


<details>
  <summary>Details</summary>
Motivation: Contrastive and generative SSL methods for time series excel in different areas but have unutilized complementary strengths.

Method: CoGenT employs joint contrastive-generative optimization to combine the advantages of both paradigms.

Result: CoGenT demonstrates consistent improvements across six datasets, achieving up to 59.2% and 14.27% F1 gains compared to standalone SSL methods.

Conclusion: A hybrid SSL framework like CoGenT preserves discriminative efficacy and generative robustness, providing a strong foundation for future research in SSL for time-series data.

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [240] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: This paper proposes FGCRN, a method using advanced neural networks and clustering to improve fault diagnosis in multimode processes by handling unknown faults effectively.


<details>
  <summary>Details</summary>
Motivation: Multimode processes often exhibit complex cluster distributions within the same health state, making it hard to set precise decision boundaries.

Method: The FGCRN model integrates multiscale depthwise convolution, bidirectional gated recurrent units, and temporal attention mechanisms. Additionally, a distance-based loss function and unsupervised learning uncover fine-grained feature structures.

Result: Experiments reveal significant performance improvement of FGCRN in accurately diagnosing faults and identifying unknown fault states.

Conclusion: The FGCRN model is effective for both accurate classification of known states and identification of unknown faults, addressing challenges in multimode processes.

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [241] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: GraB-NAS introduces a graph-based hybrid Meta-NAS approach to address limitations in rapid adaptation across tasks.


<details>
  <summary>Details</summary>
Motivation: Address challenges of limited generalization, search spaces, and computational costs in Meta-NAS approaches.

Method: A graph-based representation of architectures combined with Bayesian Optimization and gradient ascent in latent space.

Result: GraB-NAS demonstrates superior generalization and search quality against state-of-the-art alternatives.

Conclusion: Using graph modeling and hybrid strategies, GraB-NAS expands search capabilities and enhances task adaptation in Meta-NAS.

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [242] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: NeuronTune optimizes both safety and utility in Large Language Models (LLMs) by leveraging fine-grained neuron-level interventions, surpassing existing methods in robustness, performance, and adaptability.


<details>
  <summary>Details</summary>
Motivation: To address intertwined challenges in LLM deployment, such as vulnerability to malicious attacks, low usability due to frequent refusals of benign queries, and degradation in text quality and general task performance.

Method: Introduced NeuronTune, a dynamic framework that uses attribution techniques to identify neurons critical for safety and utility, and employs meta-learning to modulate their activations. It allows adjustable intervention levels to prioritize either safety or utility.

Result: Extensive experiments show that NeuronTune achieves a better balance between model safety and utility compared to state-of-the-art methods, improving robustness against attacks and maintaining high task performance.

Conclusion: NeuronTune offers a fine-grained, adaptable solution for optimizing model safety and utility in LLMs, providing a significant advancement over coarse-grained, less efficient existing approaches.

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [243] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: The paper introduces DeepFeatIoT, a novel deep learning model for classifying IoT time series data, overcoming challenges like metadata loss and data inconsistency.


<details>
  <summary>Details</summary>
Motivation: IoT sensors generate massive time series data but face issues like metadata loss, data source heterogeneity, and irregularities, limiting the effectiveness of smart systems.

Method: DeepFeatIoT integrates local/global learned features with random convolutional kernel-based features and LLM-derived features to enhance time series classification.

Result: DeepFeatIoT outperformed state-of-the-art models on diverse IoT datasets across multiple critical applications, demonstrating consistent and generalized performance.

Conclusion: DeepFeatIoT shows strong potential to significantly improve IoT analytics and aid next-generation smart systems.

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [244] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: The paper introduces EGGS-PTP, a pruning method based on expander diagrams to efficiently reduce the size and computational burden of LLMs.


<details>
  <summary>Details</summary>
Motivation: The computational and memory challenges of deploying large-scale LLMs necessitate more efficient model variants.

Method: EGGS-PTP uses expander graphs to guide N:M structured post-training pruning, maintaining essential information flow.

Result: EGGS-PTP shows significant acceleration, memory savings, and improved accuracy compared to existing pruning methods across LLMs.

Conclusion: EGGS-PTP is an effective and efficient solution for reducing computational demands while maintaining LLM performance.

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [245] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: This paper addresses the challenges of Federated Continual Learning (FCL) with Foundation Models (FMs) by proposing a framework that combines small models with FMs for better adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the limitations of Foundation Models in FCL, such as suboptimal performance on private local datasets and challenges in learning new tasks without forgetting prior knowledge.

Method: The paper introduces a collaborative framework where lightweight local models continually adapt to tasks and improve the utility of large models. It incorporates 'Small Model Continual Fine-Tuning' to reduce forgetting in small models and 'One-by-One Distillation' for personalized knowledge fusion on the server.

Result: The proposed framework demonstrates superior experimental performance, even when clients use heterogeneous small models.

Conclusion: The study establishes the effectiveness of combining small models with FMs for Federated Continual Learning, paving the way for adaptable and resource-efficient solutions.

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [246] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: The paper introduces CGAD, a causal graph-based framework for detecting cyberattacks in critical infrastructure systems, providing enhanced accuracy and robustness compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Cyberattacks on critical systems like water treatment networks are increasingly complex, and existing anomaly detection methods fail under distribution shifts and class imbalance challenges in time series data.

Method: CGAD is a two-phase framework using Dynamic Bayesian Networks for causal profiling and anomaly scoring through structural divergence in graphs, detecting anomalies in evolving sensor data.

Result: CGAD significantly improves detection accuracy in non-stationary, imbalanced time series environments, outperforming baselines across datasets with higher F1 and ROC-AUC scores.

Conclusion: By leveraging causal structures, CGAD reliably detects cyberattacks, demonstrating resilience to data imbalance and distribution shifts, outperforming traditional methods.

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [247] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: The paper introduces Gauss-Tin, a hybrid model combining replay strategies and Gaussian mixture models with instructional guidance to improve large language models' (LLMs') memory retention by 6%.


<details>
  <summary>Details</summary>
Motivation: The study addresses the issue of catastrophic forgetting in large language models, where learning new information causes the loss of prior knowledge.

Method: Gauss-Tin integrates replay-based continual learning with a Gaussian mixture model and instructional guidance to optimize sample selection and improve retention in LLMs.

Result: The proposed Gauss-Tin model achieved a 6% improvement in retention metrics compared to traditional retention strategies.

Conclusion: Gauss-Tin demonstrates the effectiveness of hybrid models in mitigating catastrophic forgetting, enhancing the robustness and adaptability of LLMs in dynamic learning situations.

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>


### [248] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: This study improves Predictive Business Process Monitoring by introducing a robust GNN framework that better captures structural, temporal, and semantic dependencies in event logs and achieves competitive prediction performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based PBPM models are limited by their reliance on either local (short prefix subgraphs) or global (overlooking temporal relevance and transition semantics) approaches, resulting in underdeveloped models.

Method: The study proposes a new GNN framework combining prefix-based GCNs and full-trace GATs to optimize performance, integrates a time decay attention mechanism for emphasizing temporal relevance, and incorporates edge features for embedding transition type semantics.

Result: The proposed models achieve competitive Top-k accuracy and DL scores across five benchmark datasets without requiring per-dataset tuning.

Conclusion: The work bridges architectural, temporal, and semantic gaps in existing PBPM models, providing a generalizable and interpretable solution for next event prediction in business processes.

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [249] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: The paper develops a hierarchical federated fine-tuning framework for Internet of Vehicles (IoV) systems, offering improved efficiency and accuracy using Low-Rank Adaptation (LoRA) and a novel algorithm.


<details>
  <summary>Details</summary>
Motivation: Adapting foundation models to handle diverse tasks in Internet of Vehicles (IoV) systems is challenging due to client mobility, resource variability, and connectivity issues.

Method: A hierarchical federated fine-tuning framework coordinated between roadside units (RSUs) and vehicles, using Low-Rank Adaptation (LoRA) with a rank adaptation mechanism modeled as a constrained multi-armed bandit problem. A UCB-DUAL algorithm is introduced for resource-aware learning.

Result: The framework achieves the best trade-off in accuracy and efficiency among baselines, reducing latency by over 24% and improving accuracy by more than 2.5%.

Conclusion: The proposed approach is effective for enabling resource-aware and mobility-resilient learning in dynamic IoV environments, demonstrating significant improvements in both performance and efficiency.

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


### [250] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: This paper introduces SYNAPSE-G, a pipeline leveraging Large Language Models to generate synthetic data for rare event classification, combined with semi-supervised label propagation on similarity graphs to address data scarcity in machine learning.


<details>
  <summary>Details</summary>
Motivation: The difficulty in training machine learning models due to insufficient labeled data, especially for rare events, and cold-start issues motivates the development of better approaches.

Method: SYNAPSE-G generates synthetic data for rare events using Large Language Models, employs a similarity graph for semi-supervised label propagation to identify candidate positive examples, and uses oracle annotation to expand the dataset for classifier training.

Result: When tested on the imbalanced SST2 and MHS datasets, SYNAPSE-G effectively identifies positive labels and outperforms baseline methods, including nearest neighbor approaches.

Conclusion: SYNAPSE-G showcases a promising solution to the cold-start classification problem through synthetic data creation and semi-supervised expansion, enhancing model performance on imbalanced datasets.

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [251] [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](https://arxiv.org/abs/2508.09561)
*Changyuan Zhao,Guangyuan Liu,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Jiawen Kang,Dusit Niyato,Zan Li,Xuemin,Shen,Zhu Han,Sumei Sun,Chau Yuen,Dong In Kim*

Main category: cs.LG

TL;DR: The paper discusses Edge General Intelligence (EGI), a new evolution in edge computing that uses world models for autonomous decision-making in dynamic environments, exploring its applications and challenges.


<details>
  <summary>Details</summary>
Motivation: To explore and establish the role of world models in enabling agentic AI systems at the wireless edge, addressing the gap in their integration for proactive and efficient decision-making.

Method: The authors provide a survey that analyzes world model architectures (latent representation learning, dynamics modeling, imagination-based planning) and their applications in EGI scenarios like IoT, vehicular and UAV networks, and network functions virtualization.

Result: World models are positioned as the cognitive backbone for EGI in improving optimization under latency, energy, and privacy constraints, in synergy with foundation models and digital twins.

Conclusion: The paper outlines open challenges (safety, training, deployment constraints) and offers a comprehensive roadmap for future research and development in intelligent and autonomous edge systems.

Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge
computing, where distributed agents possess the capability to perceive, reason,
and act autonomously across diverse, dynamic environments. Central to this
vision are world models, which act as proactive internal simulators that not
only predict but also actively imagine future trajectories, reason under
uncertainty, and plan multi-step actions with foresight. This proactive nature
allows agents to anticipate potential outcomes and optimize decisions ahead of
real-world interactions. While prior works in robotics and gaming have
showcased the potential of world models, their integration into the wireless
edge for EGI remains underexplored. This survey bridges this gap by offering a
comprehensive analysis of how world models can empower agentic artificial
intelligence (AI) systems at the edge. We first examine the architectural
foundations of world models, including latent representation learning, dynamics
modeling, and imagination-based planning. Building on these core capabilities,
we illustrate their proactive applications across EGI scenarios such as
vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of
Things (IoT) systems, and network functions virtualization, thereby
highlighting how they can enhance optimization under latency, energy, and
privacy constraints. We then explore their synergy with foundation models and
digital twins, positioning world models as the cognitive backbone of EGI.
Finally, we highlight open challenges, such as safety guarantees, efficient
training, and constrained deployment, and outline future research directions.
This survey provides both a conceptual foundation and a practical roadmap for
realizing the next generation of intelligent, autonomous edge systems.

</details>


### [252] [Online Prediction with Limited Selectivity](https://arxiv.org/abs/2508.09592)
*Licheng Liu,Mingda Qiao*

Main category: cs.LG

TL;DR: The paper discusses selective prediction with a model called Prediction with Limited Selectivity (PLS), examining optimal prediction error under certain constraints.


<details>
  <summary>Details</summary>
Motivation: To address limitations in the standard selective prediction framework when the forecaster is restricted to start predictions only within a subset of the time horizon.

Method: Introduce the PLS model, analyze prediction error both instance-by-instance and on average, and develop a complexity measure to bound errors.

Result: The study provides instance-dependent bounds on optimal prediction error that align with high probability for randomly-generated PLS instances.

Conclusion: The PLS model effectively quantifies and bounds prediction error under limited selectivity, contributing insights into selective prediction frameworks.

Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster
freely decides on the prediction window that their forecast spans. Many data
statistics can be predicted to a non-trivial error rate without any
distributional assumptions or expert advice, yet these results rely on that the
forecaster may predict at any time. We introduce a model of Prediction with
Limited Selectivity (PLS) where the forecaster can start the prediction only on
a subset of the time horizon. We study the optimal prediction error both on an
instance-by-instance basis and via an average-case analysis. We introduce a
complexity measure that gives instance-dependent bounds on the optimal error.
For a randomly-generated PLS instance, these bounds match with high
probability.

</details>


### [253] [Goal Discovery with Causal Capacity for Efficient Reinforcement Learning](https://arxiv.org/abs/2508.09624)
*Yan Yu,Yaodong Yang,Zhengbo Lu,Chengdong Ma,Wengang Zhou,Houqiang Li*

Main category: cs.LG

TL;DR: This paper introduces the GDCC framework for reinforcement learning, using a novel measurement of causality to enable efficient environment exploration.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of measuring causality in complex reinforcement learning environments and improve agents' directed exploration.

Method: They propose a causal capacity metric, utilize a Monte Carlo method to identify critical state points, and optimize exploration via subgoal discovery for both discrete and continuous state spaces.

Result: Empirical tests on multi-objective tasks show that high-causal-capacity states match subgoal expectations, and GDCC significantly improves success rates over baseline methods.

Conclusion: The GDCC framework effectively enhances exploration efficiency by leveraging causality, and aligns with expected goals, offering a promising direction for reinforcement learning research.

Abstract: Causal inference is crucial for humans to explore the world, which can be
modeled to enable an agent to efficiently explore the environment in
reinforcement learning. Existing research indicates that establishing the
causality between action and state transition will enhance an agent to reason
how a policy affects its future trajectory, thereby promoting directed
exploration. However, it is challenging to measure the causality due to its
intractability in the vast state-action space of complex scenarios. In this
paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework
for efficient environment exploration. Specifically, we first derive a
measurement of causality in state space, \emph{i.e.,} causal capacity, which
represents the highest influence of an agent's behavior on future trajectories.
After that, we present a Monte Carlo based method to identify critical points
in discrete state space and further optimize this method for continuous
high-dimensional environments. Those critical points are used to uncover where
the agent makes important decisions in the environment, which are then regarded
as our subgoals to guide the agent to make exploration more purposefully and
efficiently. Empirical results from multi-objective tasks demonstrate that
states with high causal capacity align with our expected subgoals, and our GDCC
achieves significant success rate improvements compared to baselines.

</details>


### [254] [Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs](https://arxiv.org/abs/2508.09627)
*Subhankar Sarkar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: The paper introduces a novel spatio-spectral graph neural operator ($\pi$G-Sp$^2$GNO) that integrates physics and geometry to solve PDEs in complex settings efficiently, providing a simulation-free, multiscale learning solution.


<details>
  <summary>Details</summary>
Motivation: The motivation of the study lies in overcoming the challenges of efficiently and accurately solving PDEs, particularly for scenarios with complex geometries and sparse labeled data.

Method: The method involves enhancing the existing Sp$^2$GNO by incorporating geometry-awareness and physics-based learning, including a novel physics-informed loss function for time-dependent problems using advanced time-marching and stochastic projection techniques.

Result: The results demonstrate superior performance of $\pi$G-Sp$^2$GNO on benchmark problems, showcasing its ability to handle diverse geometries and outperform existing physics-informed neural operator algorithms.

Conclusion: The proposed $\pi$G-Sp$^2$GNO approach effectively integrates geometric and physical aspects for solving both time-independent and time-dependent PDEs, offering improved accuracy and generalizability over state-of-the-art methods.

Abstract: Solving partial differential equations (PDEs) efficiently and accurately
remains a cornerstone challenge in science and engineering, especially for
problems involving complex geometries and limited labeled data. We introduce a
Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator
($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and
time-dependent PDEs. The proposed approach first improves upon the recently
developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits
the governing physics to learn the underlying solution operator in a
simulation-free setup. While the spatio-spectral structure present in the
proposed architecture allows multiscale learning, two separate strategies for
enabling geometry awareness is introduced in this paper. For time dependent
problems, we also introduce a novel hybrid physics informed loss function that
combines higher-order time-marching scheme with upscaled theory inspired
stochastic projection scheme. This allows accurate integration of the
physics-information into the loss function. The performance of the proposed
approach is illustrated on number of benchmark examples involving regular and
complex domains, variation in geometry during inference, and time-independent
and time-dependent problems. The results obtained illustrate the efficacy of
the proposed approach as compared to the state-of-the-art physics-informed
neural operator algorithms in the literature.

</details>


### [255] [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)
*Yifei Sun,Junming Liu,Ding Wang,Yirong Chen,Xuefeng Yan*

Main category: cs.LG

TL;DR: This paper introduces TimeMKG, a framework leveraging variable semantics and numerical observations in multivariate time series data to enhance predictions and generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional models overlook semantic information in variable descriptions, missing critical domain knowledge for interpretable and robust time series analyses.

Method: TimeMKG integrates textual and numerical modalities via large language models to build Multivariate Knowledge Graphs, using a dual-modality encoder and cross-modality attention for alignment.

Result: Experiments show that the incorporation of semantic knowledge improves predictive accuracy and generalization across various datasets.

Conclusion: Variable-level semantic integration is crucial for advancing multivariate time series modeling, providing interpretable and knowledge-guided inference capabilities.

Abstract: Multivariate time series data typically comprises two distinct modalities:
variable semantics and sampled numerical observations. Traditional time series
models treat variables as anonymous statistical signals, overlooking the rich
semantic information embedded in variable names and data descriptions. However,
these textual descriptors often encode critical domain knowledge that is
essential for robust and interpretable modeling. Here we present TimeMKG, a
multimodal causal reasoning framework that elevates time series modeling from
low-level signal processing to knowledge informed inference. TimeMKG employs
large language models to interpret variable semantics and constructs structured
Multivariate Knowledge Graphs that capture inter-variable relationships. A
dual-modality encoder separately models the semantic prompts, generated from
knowledge graph triplets, and the statistical patterns from historical time
series. Cross-modality attention aligns and fuses these representations at the
variable level, injecting causal priors into downstream tasks such as
forecasting and classification, providing explicit and interpretable priors to
guide model reasoning. The experiment in diverse datasets demonstrates that
incorporating variable-level knowledge significantly improves both predictive
performance and generalization.

</details>


### [256] [Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments](https://arxiv.org/abs/2508.09659)
*Johannes F. Hevler,Shivam Verma,Mirat Soijtra,Carolyn R. Bertozzi*

Main category: cs.LG

TL;DR: Thermal Tracks is a Python-based tool that uses Gaussian Process models to flexibly analyze diverse protein thermal stability data, overcoming limitations of existing methodologies.


<details>
  <summary>Details</summary>
Motivation: Existing Thermal Proteome Profiling methods are limited by restrictive statistical constraints that can miss biologically significant protein changes.

Method: The framework employs Gaussian Process models with squared-exponential kernels to model various melting curve shapes and generate unbiased null distributions.

Result: Thermal Tracks provides robust, flexible analysis especially for proteins with unconventional thermal profiles, such as phase-separating and membrane proteins.

Conclusion: Thermal Tracks enhances proteome-wide thermal profiling by offering a statistically unbiased and adaptable approach, accessible via Python implementation on GitHub.

Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein
thermal stability data that overcomes key limitations of existing thermal
proteome profiling (TPP) work-flows. Unlike standard approaches that assume
sigmoidal melting curves and are constrained by empirical null distributions
(limiting significant hits to approximately 5 % of data), Thermal Tracks uses
Gaussian Process (GP) models with squared-exponential kernels to flexibly model
any melting curve shape while generating unbiased null distributions through
kernel priors. This framework is particularly valuable for analyzing
proteome-wide perturbations that significantly alter protein thermal stability,
such as pathway inhibitions, genetic modifications, or environmental stresses,
where conventional TPP methods may miss biologically relevant changes due to
their statistical constraints. Furthermore, Thermal Tracks excels at analyzing
proteins with un-conventional melting profiles, including phase-separating
proteins and membrane proteins, which often exhibit complex, non-sigmoidal
thermal stability behaviors. Thermal Tracks is freely available from GitHub and
is implemented in Python, providing an accessible and flexible tool for
proteome-wide thermal profiling studies.

</details>


### [257] [Global Convergence Analysis of Vanilla Gradient Descent for Asymmetric Matrix Completion](https://arxiv.org/abs/2508.09685)
*Xu Zhang,Shuo Chen,Jinsheng Li,Xiangying Pang,Maoguo Gong*

Main category: cs.LG

TL;DR: This paper explores unregularized gradient descent for asymmetric low-rank matrix completion, demonstrating linear convergence with reduced computation.


<details>
  <summary>Details</summary>
Motivation: To investigate if regularization terms are necessary in gradient descent algorithms for low-rank matrix completion and analyze their impact on computational efficiency and performance.

Method: The authors employ the leave-one-out technique and spectral initialization to theoretically and empirically analyze vanilla unregularized gradient descent for low-rank matrix completion.

Result: The study proves linear convergence of unregularized gradient descent with high probability and shows that it has a lower computational cost while maintaining performance comparable to regularized methods.

Conclusion: Eliminating regularization in gradient descent for asymmetric low-rank matrix completion is effective and computationally efficient, with implicit regularization occurring naturally in the algorithm.

Abstract: This paper investigates the asymmetric low-rank matrix completion problem,
which can be formulated as an unconstrained non-convex optimization problem
with a nonlinear least-squares objective function, and is solved via gradient
descent methods. Previous gradient descent approaches typically incorporate
regularization terms into the objective function to guarantee convergence.
However, numerical experiments and theoretical analysis of the gradient flow
both demonstrate that the elimination of regularization terms in gradient
descent algorithms does not adversely affect convergence performance. By
introducing the leave-one-out technique, we inductively prove that the vanilla
gradient descent with spectral initialization achieves a linear convergence
rate with high probability. Besides, we demonstrate that the balancing
regularization term exhibits a small norm during iterations, which reveals the
implicit regularization property of gradient descent. Empirical results show
that our algorithm has a lower computational cost while maintaining comparable
completion performance compared to other gradient descent algorithms.

</details>


### [258] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: The paper introduces a Dynamic Connection Masking (DCM) mechanism to improve robustness against noisy labels, showing enhanced performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Noisy labels in real-world data degrade the performance of deep neural networks, and existing methods mainly focus on robust loss functions or sample selection, leaving architectural regularization underexplored.

Method: The authors propose DCM, a mechanism that adaptively masks less important edges based on their information-carrying capacity for MLPs and KANs, reducing gradient errors and improving noise robustness.

Result: Experiments on synthetic and real-world benchmarks show that DCM outperforms state-of-the-art approaches and reveals the superior robustness of KANs as classifiers against noisy labels.

Conclusion: DCM is a promising method to enhance deep network robustness against noisy labels and can complement existing noise-robust training methods. KANs are also found to be superior to MLPs in these scenarios.

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [259] [GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation](https://arxiv.org/abs/2508.09710)
*Yitong Luo,Islem Rekik*

Main category: cs.LG

TL;DR: This paper introduces GraphTreeGen (GTG), a novel approach for efficiently generating accurate synthetic brain connectomes with minimal memory requirements, addressing limitations of current graph generative models.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and precise generative models for brain connectomes, addressing constraints like high acquisition costs, loss of structural detail, reliance on unavailable attributes, poor edge-weight prediction, and scalability.

Method: GraphTreeGen (GTG) decomposes brain connectomes into k-hop entropy-guided trees, encoding them using a shared graph convolutional network (GCN). Subtree embeddings are fused via bipartite message-passing and decoded using a dual-branch mechanism to predict edge existence and weights.

Result: The model surpasses state-of-the-art methods in generating connectomes with better structural fidelity, edge-weight precision, reduced memory usage in self-supervised tasks, and remains competitive in supervised tasks.

Conclusion: GTG offers a modular, efficient, and scalable approach for synthetic connectome generation, applicable to tasks like super-resolution and cross-modality synthesis. Its performance improvement and low memory demand make it a valuable contribution.

Abstract: Brain connectomes, representing neural connectivity as graphs, are crucial
for understanding brain organization but costly and time-consuming to acquire,
motivating generative approaches. Recent advances in graph generative modeling
offer a data-driven alternative, enabling synthetic connectome generation and
reducing dependence on large neuroimaging datasets. However, current models
face key limitations: (i) compressing the whole graph into a single latent code
(e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node
attributes rarely available in connectomes reduces reconstruction quality;
(iii) edge-centric models emphasize topology but overlook accurate edge-weight
prediction, harming quantitative fidelity; and (iv) computationally expensive
designs (e.g., edge-conditioned convolutions) impose high memory demands,
limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric
generative framework for efficient, accurate connectome synthesis. GTG
decomposes each connectome into entropy-guided k-hop trees capturing
informative local structure, encoded by a shared GCN. A bipartite
message-passing layer fuses subtree embeddings with global node features, while
a dual-branch decoder jointly predicts edge existence and weights to
reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in
self-supervised tasks and remains competitive in supervised settings,
delivering higher structural fidelity and more precise weights with far less
memory. Its modular design enables extensions to connectome super-resolution
and cross-modality synthesis. Code: https://github.com/basiralab/GTG/

</details>


### [260] [Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models](https://arxiv.org/abs/2508.09719)
*Anish Narain,Ritam Majumdar,Nikita Narayanan,Dominic Marshall,Sonali Parbhoo*

Main category: cs.LG

TL;DR: This paper introduces a new method to enhance Concept Bottleneck Models (CBMs) for disease detection, focusing on Acute Respiratory Distress Syndrome (ARDS) by incorporating contextual information from clinical notes using a Large Language Model.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the performance limitations of AI-based clinical tools when the learned concepts fail to adequately characterize the disease detection tasks, especially within incomplete and unlabeled datasets.

Method: The method involves integrating contextual information extracted from clinical notes processed by a Large Language Model to generate additional concepts for disease characterization.

Result: The inclusion of the Large Language Model improved CBM performance by 10%, enhanced concept learning, reduced information leakage, and minimized reliance on spurious shortcuts.

Conclusion: Leveraging contextual data from clinical notes using advanced language models can improve interpretability and accuracy in challenging disease identification tasks, like ARDS.

Abstract: Large, publicly available clinical datasets have emerged as a novel resource
for understanding disease heterogeneity and to explore personalization of
therapy. These datasets are derived from data not originally collected for
research purposes and, as a result, are often incomplete and lack critical
labels. Many AI tools have been developed to retrospectively label these
datasets, such as by performing disease classification; however, they often
suffer from limited interpretability. Previous work has attempted to explain
predictions using Concept Bottleneck Models (CBMs), which learn interpretable
concepts that map to higher-level clinical ideas, facilitating human
evaluation. However, these models often experience performance limitations when
the concepts fail to adequately explain or characterize the task. We use the
identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging
test case to demonstrate the value of incorporating contextual information from
clinical notes to improve CBM performance. Our approach leverages a Large
Language Model (LLM) to process clinical notes and generate additional
concepts, resulting in a 10% performance gain over existing methods.
Additionally, it facilitates the learning of more comprehensive concepts,
thereby reducing the risk of information leakage and reliance on spurious
shortcuts, thus improving the characterization of ARDS.

</details>


### [261] [Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization](https://arxiv.org/abs/2508.09730)
*Qiaolei Gu,Yu Li,DingYi Zeng,Lu Wang,Ming Pang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: The paper introduces GenCO, a novel framework combining generative modeling and multi-instance reward learning to optimize advertising creative combinations in e-commerce and improve conversions.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to evaluate combinations of creative elements in e-commerce advertising, limiting their ability to explore the vast space of possibilities effectively.

Method: A two-stage framework: Generative modeling powered by reinforcement learning to create diverse combinations, followed by multi-instance learning to attribute rewards and refine creative selections.

Result: GenCO was implemented on a major e-commerce platform, leading to a substantial increase in advertising revenue.

Conclusion: The approach proves its practicality in boosting advertising revenue and provides a dataset to support future research in creative optimization for e-commerce.

Abstract: In e-commerce advertising, selecting the most compelling combination of
creative elements -- such as titles, images, and highlights -- is critical for
capturing user attention and driving conversions. However, existing methods
often evaluate creative components individually, failing to navigate the
exponentially large search space of possible combinations. To address this
challenge, we propose a novel framework named GenCO that integrates generative
modeling with multi-instance reward learning. Our unified two-stage
architecture first employs a generative model to efficiently produce a diverse
set of creative combinations. This generative process is optimized with
reinforcement learning, enabling the model to effectively explore and refine
its selections. Next, to overcome the challenge of sparse user feedback, a
multi-instance learning model attributes combination-level rewards, such as
clicks, to the individual creative elements. This allows the reward model to
provide a more accurate feedback signal, which in turn guides the generative
model toward creating more effective combinations. Deployed on a leading
e-commerce platform, our approach has significantly increased advertising
revenue, demonstrating its practical value. Additionally, we are releasing a
large-scale industrial dataset to facilitate further research in this important
domain.

</details>


### [262] [HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks](https://arxiv.org/abs/2508.09743)
*Yanick Chistian Tchenko,Felix Mohr,Hicham Hadj Abdelkader,Hedi Tabia*

Main category: cs.LG

TL;DR: Hereditary Knowledge Transfer (HKT) significantly improves small neural model performance by selectively inheriting task-relevant features from larger pretrained networks.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to enhance the performance of smaller, deployable neural networks, addressing challenges arising from deep and large models' inefficiency in resource-constrained contexts.

Method: The HKT approach involves three biologically inspired components—Extraction, Transfer, and Mixture (ETM)—augmented by a Genetic Attention mechanism to optimize selective feature inheritance.

Result: HKT demonstrated improved performance across multiple tasks, outperforming traditional knowledge distillation methods in optical flow, image classification, and semantic segmentation.

Conclusion: HKT provides a scalable and interpretable method to develop compact and high-performance neural networks suitable for constrained environments.

Abstract: A prevailing trend in neural network research suggests that model performance
improves with increasing depth and capacity - often at the cost of
integrability and efficiency. In this paper, we propose a strategy to optimize
small, deployable models by enhancing their capabilities through structured
knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a
biologically inspired framework for modular and selective transfer of
task-relevant features from a larger, pretrained parent network to a smaller
child model. Unlike standard knowledge distillation, which enforces uniform
imitation of teacher outputs, HKT draws inspiration from biological inheritance
mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage
process of feature transfer. Neural network blocks are treated as functional
carriers, and knowledge is transmitted through three biologically motivated
components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention
(GA) mechanism governs the integration of inherited and native representations,
ensuring both alignment and selectivity. We evaluate HKT across diverse vision
tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),
and semantic segmentation (LiTS), demonstrating that it significantly improves
child model performance while preserving its compactness. The results show that
HKT consistently outperforms conventional distillation approaches, offering a
general-purpose, interpretable, and scalable solution for deploying
high-performance neural networks in resource-constrained environments.

</details>


### [263] [A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers](https://arxiv.org/abs/2508.09747)
*Nazira Dunbayeva,Yulong Li,Yutong Xie,Imran Razzak*

Main category: cs.LG

TL;DR: This paper develops a machine learning pipeline that improves the prediction of biological age by considering the rate of change of biomarkers over time, outperforming traditional models.


<details>
  <summary>Details</summary>
Motivation: Accurately predicting aging trajectories is crucial for proactive medical interventions and understanding the dynamic nature of the aging process.

Method: A machine learning pipeline, including engineered features capturing biomarker rate of change, was trained using longitudinal cohort data from two time periods.

Result: The final LightGBM model achieved strong predictive accuracy ($R^2 = 0.515$ for males, $R^2 = 0.498$ for females) and identified slope features as key contributors.

Conclusion: Dynamic health trajectories are more informative than static biomarkers for determining biological age, enabling better tools for managing age-related health risks.

Abstract: Predicting an individual's aging trajectory is a central challenge in
preventative medicine and bioinformatics. While machine learning models can
predict chronological age from biomarkers, they often fail to capture the
dynamic, longitudinal nature of the aging process. In this work, we developed
and validated a machine learning pipeline to predict age using a longitudinal
cohort with data from two distinct time periods (2019-2020 and 2021-2022). We
demonstrate that a model using only static, cross-sectional biomarkers has
limited predictive power when generalizing to future time points. However, by
engineering novel features that explicitly capture the rate of change (slope)
of key biomarkers over time, we significantly improved model performance. Our
final LightGBM model, trained on the initial wave of data, successfully
predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for
males, $R^2 = 0.498$ for females), significantly outperforming both traditional
linear models and other tree-based ensembles. SHAP analysis of our successful
model revealed that the engineered slope features were among the most important
predictors, highlighting that an individual's health trajectory, not just their
static health snapshot, is a key determinant of biological age. Our framework
paves the way for clinical tools that dynamically track patient health
trajectories, enabling early intervention and personalized prevention
strategies for age-related diseases.

</details>


### [264] [$μ$-Parametrization for Mixture of Experts](https://arxiv.org/abs/2508.09752)
*Jan Małaśnicki,Kamil Ciebiera,Mateusz Boruń,Maciej Pióro,Jan Ludziejewski,Maciej Stefaniak,Michał Krutul,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jakub Krajewski*

Main category: cs.LG

TL;DR: The paper introduces a μ-Parameterization (μP) technique for Mixture-of-Experts (MoE) models, ensuring feature learning scalability across model components. It is validated empirically, focusing on learning rate optimization with varying experts and granularity.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address a gap between two significant advancements: μTransfer for hyperparameter tuning in large-scale training, and the Mixture-of-Experts (MoE) architecture in extremely large models, by exploring their intersection.

Method: The authors propose a μ-Parameterization (μP) specifically tailored for MoE models that theoretically guarantees proper feature learning across the router and experts, and empirically validate this parameterization through experiments.

Result: The μP ensures scalable feature learning for both the router and expert modules in MoE models, with experiments showing its validity and insights into the effects of expert scaling and granularity on optimal learning rates.

Conclusion: The study successfully bridges the gap between μTransfer techniques and MoE architectures by deriving and validating μP, enabling robust feature learning and offering practical guidelines on scaling experts and tuning learning rates.

Abstract: Recent years have seen a growing interest and adoption of LLMs, with
$\mu$Transfer becoming a key technique for tuning hyperparameters in
large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a
leading architecture in extremely large models. However, the intersection of
these two advancements has remained unexplored. In this work, we derive a
$\mu$-Parameterization ($\mu$P) for MoE, providing theoretical guarantees for
feature learning across model widths in both the router and experts. We
empirically validate our parameterization and further investigate how scaling
the number of experts and granularity affects the optimal learning rate.

</details>


### [265] [TriForecaster: A Mixture of Experts Framework for Multi-Region Electric Load Forecasting with Tri-dimensional Specialization](https://arxiv.org/abs/2508.09753)
*Zhaoyang Zhu,Zhipeng Zeng,Qiming Chen,Linxiao Yang,Peiyuan Liu,Weiqi Chen,Liang Sun*

Main category: cs.LG

TL;DR: This paper introduces TriForecaster, a novel multi-task learning framework for accurate short-term electric load forecasting across multiple regions. It successfully reduces forecast errors by 22.4% and demonstrates practical applicability in eastern China.


<details>
  <summary>Details</summary>
Motivation: To enhance electric load forecasting accuracy for multiple sub-regions in a large area, leveraging detailed load data and addressing regional, contextual, and temporal variations.

Method: Proposed TriForecaster, a Mixture of Experts (MoE) framework within a Multi-Task Learning paradigm. It uses RegionMixer and CTSpecializer layers for dynamic cooperation and specialization of models across regional, contextual, and temporal dimensions.

Result: Evaluation on four real-world MRELF datasets shows TriForecaster reduces forecast errors by 22.4% compared to state-of-the-art models. It has been deployed successfully in eastern China for city-level load forecasting.

Conclusion: TriForecaster is effective in addressing MRELF challenges, demonstrating flexibility and broad applicability. Its deployment supports large-scale power system operations and planning.

Abstract: Electric load forecasting is pivotal for power system operation, planning and
decision-making. The rise of smart grids and meters has provided more detailed
and high-quality load data at multiple levels of granularity, from home to bus
and cities. Motivated by similar patterns of loads across different cities in a
province in eastern China, in this paper we focus on the Multi-Region Electric
Load Forecasting (MRELF) problem, targeting accurate short-term load
forecasting for multiple sub-regions within a large region. We identify three
challenges for MRELF, including regional variation, contextual variation, and
temporal variation. To address them, we propose TriForecaster, a new framework
leveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning
(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer
and Context-Time Specializer (CTSpecializer) layers, enabling dynamic
cooperation and specialization of expert models across regional, contextual,
and temporal dimensions. Based on evaluation on four real-world MRELF datasets
with varied granularity, TriForecaster outperforms state-of-the-art models by
achieving an average forecast error reduction of 22.4\%, thereby demonstrating
its flexibility and broad applicability. In particular, the deployment of
TriForecaster on the eForecaster platform in eastern China exemplifies its
practical utility, effectively providing city-level, short-term load forecasts
for 17 cities, supporting a population exceeding 110 million and daily
electricity usage over 100 gigawatt-hours.

</details>


### [266] [Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations](https://arxiv.org/abs/2508.09787)
*Mauro Tucci*

Main category: cs.LG

TL;DR: Proto-PINV+H is a rapid training method combining closed-form weight computation and gradient-based optimization of synthetic inputs, achieving high accuracy on MNIST and Fashion-MNIST with minimal parameters and computational cost.


<details>
  <summary>Details</summary>
Motivation: To develop a fast and efficient training paradigm with favorable accuracy-speed-size trade-offs compared to conventional methods.

Method: The method combines closed-form computation of weight matrices using ridge-regularized pseudo-inverse solves with gradient-based optimization of prototypes (inputs, labels, and activations), shifting trainable components to data/activation space and incorporating optional extensions like PCA/PLS projections.

Result: Achieved 97.8% accuracy on MNIST and 89.3% on Fashion-MNIST test sets in under 5 seconds on an RTX 5060 GPU, using around 130k trainable parameters and 250 epochs.

Conclusion: Proto-PINV+H demonstrates that a hybrid closed-form and gradient-based optimization approach can achieve competitive accuracy with significant gains in speed and efficiency, offering a promising alternative to traditional training methods.

Abstract: We present Proto-PINV+H, a fast training paradigm that combines closed-form
weight computation with gradient-based optimisation of a small set of synthetic
inputs, soft labels, and-crucially-hidden activations. At each iteration we
recompute all weight matrices in closed form via two (or more)
ridge-regularised pseudo-inverse solves, while updating only the prototypes
with Adam. The trainable degrees of freedom are thus shifted from weight space
to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k
train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the
official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k
trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a
multi-layer extension (optimised activations at each hidden stage), learnable
ridge parameters, optional PCA/PLS projections, and theory linking the
condition number of prototype matrices to generalisation. The approach yields
favourable accuracy--speed--size trade-offs against ELM, random-feature ridge,
and shallow MLPs trained by back-propagation.

</details>


### [267] [Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques](https://arxiv.org/abs/2508.09810)
*Qi Gan,Stephan Clémençon,Mounîm A. El-Yacoubi,Sao Mai Nguyen,Eric Fenaux,Ons Jelassi*

Main category: cs.LG

TL;DR: Machine learning and statistical methods were used to analyze biomechanical features affecting long jump performance in elite athletes to identify critical factors influencing results.


<details>
  <summary>Details</summary>
Motivation: Traditional methods evaluating athletes' performance struggle with the complex relationships between biomechanical features and outcomes. Machine learning offers tools for better understanding these dynamics.

Method: The researchers utilized quantile regression to evaluate the relationship between biomechanical features and effective distance, supported by SHAP, PDPs, and ICE plots for model interpretation.

Result: For male athletes, knee angle before take-off (>169°) was crucial for top 10% performance. For female athletes, landing pose and approach technique, along with velocity, were identified as key factors.

Conclusion: The study highlights the importance of integrating machine learning in sports analytics to uncover critical performance-driving features and better understand athletic techniques.

Abstract: Biomechanical features have become important indicators for evaluating
athletes' techniques. Traditionally, experts propose significant features and
evaluate them using physics equations. However, the complexity of the human
body and its movements makes it challenging to explicitly analyze the
relationships between some features and athletes' final performance. With
advancements in modern machine learning and statistics, data analytics methods
have gained increasing importance in sports analytics. In this study, we
leverage machine learning models to analyze expert-proposed biomechanical
features from the finals of long jump competitions in the World Championships.
The objectives of the analysis include identifying the most important features
contributing to top-performing jumps and exploring the combined effects of
these key features. Using quantile regression, we model the relationship
between the biomechanical feature set and the target variable (effective
distance), with a particular focus on elite-level jumps. To interpret the
model, we apply SHapley Additive exPlanations (SHAP) alongside Partial
Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The
findings reveal that, beyond the well-documented velocity-related features,
specific technical aspects also play a pivotal role. For male athletes, the
angle of the knee of the supporting leg before take-off is identified as a key
factor for achieving top 10% performance in our dataset, with angles greater
than 169{\deg}contributing significantly to jump performance. In contrast, for
female athletes, the landing pose and approach step technique emerge as the
most critical features influencing top 10% performances, alongside velocity.
This study establishes a framework for analyzing the impact of various features
on athletic performance, with a particular emphasis on top-performing events.

</details>


### [268] [Provable In-Context Vector Arithmetic via Retrieving Task Concepts](https://arxiv.org/abs/2508.09820)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Qingfu Zhang,Hau-San Wong,Taiji Suzuki*

Main category: cs.LG

TL;DR: The paper proposes a theoretical framework explaining how transformers utilize vector arithmetic for factual-recall in-context learning (ICL), achieving robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explain why transformers outperform previous models in factual-recall ICL tasks, addressing the lack of theoretical understanding despite empirical observations.

Method: They develop a hierarchical concept modeling framework and an optimization theory, proving convergence and generalization properties of nonlinear residual transformers trained via gradient descent.

Result: Key results include proofs of strong generalization capabilities, robustness to concept recombination and distribution shifts, and empirical simulations that support the theoretical model.

Conclusion: Transformers' ability to perform factual-recall ICL tasks using vector arithmetic is theoretically grounded, highlighting their advantages over static embedding methods.

Abstract: In-context learning (ICL) has garnered significant attention for its ability
to grasp functions/tasks from demonstrations. Recent studies suggest the
presence of a latent task/function vector in LLMs during ICL. Merullo et al.
(2024) showed that LLMs leverage this vector alongside the residual stream for
Word2Vec-like vector arithmetic, solving factual-recall ICL tasks.
Additionally, recent work empirically highlighted the key role of
Question-Answer data in enhancing factual-recall capabilities. Despite these
insights, a theoretical explanation remains elusive. To move one step forward,
we propose a theoretical framework building on empirically grounded
hierarchical concept modeling. We develop an optimization theory, showing how
nonlinear residual transformers trained via gradient descent on cross-entropy
loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss
convergence and show the strong generalization, including robustness to concept
recombination and distribution shifts. These results elucidate the advantages
of transformers over static embedding predecessors. Empirical simulations
corroborate our theoretical insights.

</details>


### [269] [RankList -- A Listwise Preference Learning Framework for Predicting Subjective Preferences](https://arxiv.org/abs/2508.09826)
*Abinay Reddy Naini,Fernando Diaz,Carlos Busso*

Main category: cs.LG

TL;DR: This paper introduces RankList, a preference learning framework designed for tasks like speech emotion recognition and image aesthetic assessment. It generalizes RankNet from pairwise to listwise comparisons, achieving global ranking consistency.


<details>
  <summary>Details</summary>
Motivation: Preference learning in tasks that involve subjective human judgments often struggles with global ranking consistency, especially with existing pairwise models like RankNet. The authors aim to address these limitations by improving ranking fidelity.

Method: The authors propose RankList, which incorporates list-level supervision through a probabilistic framework that handles ranking constraints. Training efficiency is improved using a log-sum-exp approximation, and skip-wise comparisons are introduced for handling complex list structures.

Result: RankList demonstrates superior performance in SER tasks (e.g., MSP-Podcast, IEMOCAP) with enhanced ranking accuracy and Kendall's Tau compared to baselines. It also generalizes well to aesthetic image ranking tasks.

Conclusion: The framework provides a unified, effective, and extensible solution for modeling ordered preferences across diverse modalities, excelling both in-domain and cross-domain applications.

Abstract: Preference learning has gained significant attention in tasks involving
subjective human judgments, such as \emph{speech emotion recognition} (SER) and
image aesthetic assessment. While pairwise frameworks such as RankNet offer
robust modeling of relative preferences, they are inherently limited to local
comparisons and struggle to capture global ranking consistency. To address
these limitations, we propose RankList, a novel listwise preference learning
framework that generalizes RankNet to structured list-level supervision. Our
formulation explicitly models local and non-local ranking constraints within a
probabilistic framework. The paper introduces a log-sum-exp approximation to
improve training efficiency. We further extend RankList with skip-wise
comparisons, enabling progressive exposure to complex list structures and
enhancing global ranking fidelity. Extensive experiments demonstrate the
superiority of our method across diverse modalities. On benchmark SER datasets
(MSP-Podcast, IEMOCAP, BIIC Podcast), RankList achieves consistent improvements
in Kendall's Tau and ranking accuracy compared to standard listwise baselines.
We also validate our approach on aesthetic image ranking using the Artistic
Image Aesthetics dataset, highlighting its broad applicability. Through
ablation and cross-domain studies, we show that RankList not only improves
in-domain ranking but also generalizes better across datasets. Our framework
offers a unified, extensible approach for modeling ordered preferences in
subjective learning scenarios.

</details>


### [270] [FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness](https://arxiv.org/abs/2508.09866)
*Siyuan Wen,Meng Zhang,Yang Yang,Ningning Ding*

Main category: cs.LG

TL;DR: The paper introduces FedShard, a federated unlearning algorithm enhancing efficiency fairness and performance fairness in removing client data from learned models.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the unexplored issues of fairness—both efficiency and performance—in federated unlearning, ensuring all clients are treated equitably during unlearning.

Method: The authors propose FedShard, a novel algorithm that balances convergence, unlearning efficiency, and fairness. They also introduce new quantitative metrics for fairness evaluation.

Result: FedShard demonstrates fairness in unlearning performance and efficiency and mitigates risks such as cascading client departures and data poisoning attacks.

Conclusion: FedShard outperforms existing unlearning methods, speeding up the process significantly while ensuring fairness and balanced unlearning costs for decentralized clients.

Abstract: To protect clients' right to be forgotten in federated learning, federated
unlearning aims to remove the data contribution of leaving clients from the
global learned model. While current studies mainly focused on enhancing
unlearning efficiency and effectiveness, the crucial aspects of efficiency
fairness and performance fairness among decentralized clients during unlearning
have remained largely unexplored. In this study, we introduce FedShard, the
first federated unlearning algorithm designed to concurrently guarantee both
efficiency fairness and performance fairness. FedShard adaptively addresses the
challenges introduced by dilemmas among convergence, unlearning efficiency, and
unlearning fairness. Furthermore, we propose two novel metrics to
quantitatively assess the fairness of unlearning algorithms, which we prove to
satisfy well-known properties in other existing fairness measurements. Our
theoretical analysis and numerical evaluation validate FedShard's fairness in
terms of both unlearning performance and efficiency. We demonstrate that
FedShard mitigates unfairness risks such as cascaded leaving and poisoning
attacks and realizes more balanced unlearning costs among clients. Experimental
results indicate that FedShard accelerates the data unlearning process 1.3-6.2
times faster than retraining from scratch and 4.9 times faster than the
state-of-the-art exact unlearning methods.

</details>


### [271] [Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning](https://arxiv.org/abs/2508.09883)
*Xiaojun Wu,Xiaoguang Jiang,Huiyang Li,Jucai Zhai,Dengfeng Liu,Qiaobo Hao,Huang Liu,Zhiguo Yang,Ji Xie,Ninglun Gu,Jin Yang,Kailai Zhang,Yelun Bao,Jun Wang*

Main category: cs.LG

TL;DR: This paper introduces a data-efficient distillation framework (DED) that enhances reasoning in LLMs using only 0.8k curated examples, achieving state-of-the-art results in mathematical reasoning and code generation tasks.


<details>
  <summary>Details</summary>
Motivation: Current reasoning improvements in LLMs often require extensive datasets and computational resources, making them less efficient and unsustainable.

Method: The paper proposes the DED framework with three key ideas: optimal teacher model selection, smaller curated corpus for balanced in-domain and out-of-domain performance, and diverse reasoning trajectories for robust skill development.

Result: DED achieves superior reasoning performance on benchmarks like AIME 2024/2025, MATH-500, and LiveCodeBench while using a much smaller corpus, proving a practical alternative to scaling.

Conclusion: DED provides a data-efficient, effective approach to enhance reasoning in LLMs while maintaining general capabilities, addressing both computational cost and dataset size limitations.

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities in
tasks such as algorithmic coding and mathematical problem-solving. Recent
methods have improved reasoning through expanded corpus and multistage training
combining reinforcement learning and supervised fine-tuning. Although some
methods suggest that small but targeted dataset can incentivize reasoning via
only distillation, a reasoning scaling laws is still taking shape, increasing
computational costs. To address this, we propose a data-efficient distillation
framework (DED) that optimizes the Pareto frontier of reasoning distillation.
Inspired by the on-policy learning and diverse roll-out strategies of
reinforcement learning, the key idea of our approach is threefold: (1) We
identify that benchmark scores alone do not determine an effective teacher
model. Through comprehensive comparisons of leading reasoning LLMs, we develop
a method to select an optimal teacher model. (2) While scaling distillation can
enhance reasoning, it often degrades out-of-domain performance. A carefully
curated, smaller corpus achieves a balanced trade-off between in-domain and
out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the
student model to develop robust reasoning skills. We validate our method
through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and
code generation (LiveCodeBench), achieving state-of-the-art results with only
0.8k carefully curated examples, bypassing the need for extensive scaling. Our
systematic analysis demonstrates that DED outperforms existing methods by
considering factors beyond superficial hardness, token length, or teacher model
capability. This work offers a practical and efficient pathway to advanced
reasoning while preserving general capabilities.

</details>


### [272] [Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?](https://arxiv.org/abs/2508.09888)
*Viacheslav Barkov,Jonas Schmidinger,Robin Gebbers,Martin Atzmueller*

Main category: cs.LG

TL;DR: This paper presents a study evaluating advanced artificial neural networks (ANNs) against classical machine learning methods for predictive soil modeling, demonstrating that ANNs consistently outperform traditional methods, with TabPFN showing the strongest performance.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the challenges of field-scale predictive soil modeling, particularly limited training samples and high feature-to-sample ratios, through modern artificial neural networks.

Method: The study constructed a benchmark that evaluates state-of-the-art ANN architectures, including MLP-based models, transformers, and retrieval-augmented approaches, using 31 field- and farm-scale datasets for soil property predictions.

Result: The results indicate that modern ANNs consistently surpassed traditional machine learning methods on predictive soil modeling tasks, with TabPFN performing robustly across diverse conditions.

Conclusion: The paper concludes by recommending the adoption of modern ANN approaches, particularly TabPFN, as the default for field-scale predictive soil modeling in pedometrics.

Abstract: In the field of pedometrics, tabular machine learning is the predominant
method for predicting soil properties from remote and proximal soil sensing
data, forming a central component of digital soil mapping. At the field-scale,
this predictive soil modeling (PSM) task is typically constrained by small
training sample sizes and high feature-to-sample ratios in soil spectroscopy.
Traditionally, these conditions have proven challenging for conventional deep
learning methods. Classical machine learning algorithms, particularly
tree-based models like Random Forest and linear models such as Partial Least
Squares Regression, have long been the default choice for field-scale PSM.
Recent advances in artificial neural networks (ANN) for tabular data challenge
this view, yet their suitability for field-scale PSM has not been proven. We
introduce a comprehensive benchmark that evaluates state-of-the-art ANN
architectures, including the latest multilayer perceptron (MLP)-based models
(TabM, RealMLP), attention-based transformer variants (FT-Transformer,
ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,
ModernNCA), and an in-context learning foundation model (TabPFN). Our
evaluation encompasses 31 field- and farm-scale datasets containing 30 to 460
samples and three critical soil properties: soil organic matter or soil organic
carbon, pH, and clay content. Our results reveal that modern ANNs consistently
outperform classical methods on the majority of tasks, demonstrating that deep
learning has matured sufficiently to overcome the long-standing dominance of
classical machine learning for PSM. Notably, TabPFN delivers the strongest
overall performance, showing robustness across varying conditions. We therefore
recommend the adoption of modern ANNs for field-scale PSM and propose TabPFN as
the new default choice in the toolkit of every pedometrician.

</details>


### [273] [Rare anomalies require large datasets: About proving the existence of anomalies](https://arxiv.org/abs/2508.09894)
*Simon Klüttermann,Emmanuel Müller*

Main category: cs.LG

TL;DR: The paper investigates when anomalies in a dataset can be conclusively detected, identifying a mathematical threshold for anomaly existence based on dataset size, contamination rate, and an algorithm constant.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored question of when anomalies can be definitively identified in datasets, which is critical for reliable anomaly detection.

Method: The study involves over three million statistical tests across different anomaly detection scenarios to establish a relationship between dataset properties and detection feasibility.

Result: The research identifies a lower bound condition \( N \ge \frac{\alpha_{\text{algo}}}{\nu^2} \) that determines the required dataset size for confirming anomaly existence.

Conclusion: The study concludes that there is a limit to detecting very rare anomalies, based on dataset size and contamination rate, which sets a feasibility threshold for anomaly detection efforts.

Abstract: Detecting whether any anomalies exist within a dataset is crucial for
effective anomaly detection, yet it remains surprisingly underexplored in
anomaly detection literature. This paper presents a comprehensive study that
addresses the fundamental question: When can we conclusively determine that
anomalies are present? Through extensive experimentation involving over three
million statistical tests across various anomaly detection tasks and
algorithms, we identify a relationship between the dataset size, contamination
rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results
demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate
$ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents
a lower bound on the number of samples required to confirm anomaly existence.
This threshold implies a limit to how rare anomalies can be before proving
their existence becomes infeasible.

</details>


### [274] [Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs](https://arxiv.org/abs/2508.09904)
*Arjun Ashok,Andrew Robert Williams,Vincent Zhihao Zheng,Irina Rish,Nicolas Chapados,Étienne Marcotte,Valentina Zantedeschi,Alexandre Drouin*

Main category: cs.LG

TL;DR: The paper explores strategies to improve large language models (LLMs) for context-aided forecasting tasks, introducing new methods that enhance interpretability, performance, and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance the integration of contextual information, often textual, with historical data in forecasting by leveraging the potential of large language models (LLMs), a capability that has been underexplored so far.

Method: The paper introduces four strategies: (1) ReDP focuses on interpretability via explicit reasoning. (2) CorDP uses LLMs to refine existing forecasts. (3) IC-DP embeds historical task examples in prompts for higher accuracy. (4) RouteDP optimizes resource use by routing complex tasks to larger, more capable LLMs.

Result: These strategies demonstrate distinct improvements across various LLMs on the CiK benchmark by improving interpretability, forecast accuracy, and efficiency over naive prompting techniques.

Conclusion: The findings highlight the potential for simple, effective refinements in LLM-based forecasting methods, paving the way for more robust and applicable real-world forecasting solutions.

Abstract: Forecasting in real-world settings requires models to integrate not only
historical data but also relevant contextual information, often available in
textual form. While recent work has shown that large language models (LLMs) can
be effective context-aided forecasters via na\"ive direct prompting, their full
potential remains underexplored. We address this gap with 4 strategies,
providing new insights into the zero-shot capabilities of LLMs in this setting.
ReDP improves interpretability by eliciting explicit reasoning traces, allowing
us to assess the model's reasoning over the context independently from its
forecast accuracy. CorDP leverages LLMs solely to refine existing forecasts
with context, enhancing their applicability in real-world forecasting
pipelines. IC-DP proposes embedding historical examples of context-aided
forecasting tasks in the prompt, substantially improving accuracy even for the
largest models. Finally, RouteDP optimizes resource efficiency by using LLMs to
estimate task difficulty, and routing the most challenging tasks to larger
models. Evaluated on different kinds of context-aided forecasting tasks from
the CiK benchmark, our strategies demonstrate distinct benefits over na\"ive
prompting across LLMs of different sizes and families. These results open the
door to further simple yet effective improvements in LLM-based context-aided
forecasting.

</details>


### [275] [Prototype-Guided Diffusion: Visual Conditioning without External Memory](https://arxiv.org/abs/2508.09922)
*Bilal Faye,Hanane Azzag,Mustapha Lebbah*

Main category: cs.LG

TL;DR: Diffusion models are good at generating images but are computationally expensive. The Prototype Diffusion Model (PDM) reduces these costs by using prototype learning instead of external memory retrieval.


<details>
  <summary>Details</summary>
Motivation: Diffusion models offer high-quality image generation but are hampered by significant computational demands, especially during denoising, necessitating more efficient solutions.

Method: The proposed Prototype Diffusion Model (PDM) replaces external memory retrieval with dynamic visual prototypes derived through contrastive learning, which are used for conditioning during the denoising process.

Result: PDM achieves efficient image generation with reduced computational and storage requirements while maintaining high-quality results.

Conclusion: PDM presents a scalable and efficient alternative to retrieval-based diffusion methods, solving key inefficiencies without compromising generation quality.

Abstract: Diffusion models have emerged as a leading framework for high-quality image
generation, offering stable training and strong performance across diverse
domains. However, they remain computationally intensive, particularly during
the iterative denoising process. Latent-space models like Stable Diffusion
alleviate some of this cost by operating in compressed representations, though
at the expense of fine-grained detail. More recent approaches such as
Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning
denoising on similar examples retrieved from large external memory banks. While
effective, these methods introduce drawbacks: they require costly storage and
retrieval infrastructure, depend on static vision-language models like CLIP for
similarity, and lack adaptability during training. We propose the Prototype
Diffusion Model (PDM), a method that integrates prototype learning directly
into the diffusion process for efficient and adaptive visual conditioning -
without external memory. Instead of retrieving reference samples, PDM
constructs a dynamic set of compact visual prototypes from clean image features
using contrastive learning. These prototypes guide the denoising steps by
aligning noisy representations with semantically relevant visual patterns,
enabling efficient generation with strong semantic grounding. Experiments show
that PDM maintains high generation quality while reducing computational and
storage overhead, offering a scalable alternative to retrieval-based
conditioning in diffusion models.

</details>


### [276] [Residual Reservoir Memory Networks](https://arxiv.org/abs/2508.09925)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: The paper introduces Residual Reservoir Memory Networks (ResRMNs), a new kind of untrained RNNs under the Reservoir Computing paradigm, which improves long-term memory and classification tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations in traditional Reservoir Computing models, particularly in maintaining long-term propagation of input information.

Method: The authors propose ResRMNs, which combine a linear memory reservoir with a non-linear one via residual orthogonal connections. They further analyze these configurations using linear stability analysis.

Result: Experimental results show that ResRMNs outperform other conventional Reservoir Computing models on time-series and pixel-level 1-D classification tasks.

Conclusion: ResRMNs provide significant improvements in handling long-term memory and performance in classification tasks, demonstrating their potential as an advanced RC model.

Abstract: We introduce a novel class of untrained Recurrent Neural Networks (RNNs)
within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory
Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear
reservoir, where the latter is based on residual orthogonal connections along
the temporal dimension for enhanced long-term propagation of the input. The
resulting reservoir state dynamics are studied through the lens of linear
stability analysis, and we investigate diverse configurations for the temporal
residual connections. The proposed approach is empirically assessed on
time-series and pixel-level 1-D classification tasks. Our experimental results
highlight the advantages of the proposed approach over other conventional RC
models.

</details>


### [277] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: This paper introduces a Noise Hypernetwork to integrate benefits of test-time scaling into models post-training, avoiding significant computation overhead.


<details>
  <summary>Details</summary>
Motivation: The current test-time scaling paradigm allows models to handle complex problems by using additional computation during inference, but it slows down the process significantly, making it less practical for many applications.

Method: The authors propose replacing test-time noise optimization in diffusion models with a Noise Hypernetwork. This Hypernetwork modulates input noise and uses a noise-space objective for learning a reward-tilted distribution that retains the base model's fidelity while optimizing specific characteristics.

Result: The proposed method recovers much of the quality improvements provided by test-time optimization while significantly reducing the computational cost.

Conclusion: The Noise Hypernetwork framework effectively integrates test-time scaling capabilities into models post-training, maintaining quality and practicality for broader applications.

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>


### [278] [Dynamic Mixture-of-Experts for Incremental Graph Learning](https://arxiv.org/abs/2508.09974)
*Lecheng Kong,Theodore Vasiloudis,Seongjun Yun,Han Xie,Xiang Song*

Main category: cs.LG

TL;DR: The paper introduces a dynamic mixture-of-experts (DyMoE) approach for graph incremental learning, addressing catastrophic forgetting and optimizing computational costs with sparse expert selection.


<details>
  <summary>Details</summary>
Motivation: To combat catastrophic forgetting in graph incremental learning and enhance adaptability of models to evolving data without full retraining.

Method: Proposes DyMoE GNN layers, introducing new expert networks for incoming data blocks, with customized regularization and sparse MoE for efficient predictions.

Result: Achieved a 4.92% relative accuracy improvement over the best baselines in class incremental learning benchmarks.

Conclusion: The DyMoE approach effectively balances learning new knowledge while retaining old patterns, providing scalable and accurate incremental learning on graphs.

Abstract: Graph incremental learning is a learning paradigm that aims to adapt trained
models to continuously incremented graphs and data over time without the need
for retraining on the full dataset. However, regular graph machine learning
methods suffer from catastrophic forgetting when applied to incremental
learning settings, where previously learned knowledge is overridden by new
knowledge. Previous approaches have tried to address this by treating the
previously trained model as an inseparable unit and using techniques to
maintain old behaviors while learning new knowledge. These approaches, however,
do not account for the fact that previously acquired knowledge at different
timestamps contributes differently to learning new tasks. Some prior patterns
can be transferred to help learn new data, while others may deviate from the
new data distribution and be detrimental. To address this, we propose a dynamic
mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a
DyMoE GNN layer adds new expert networks specialized in modeling the incoming
data blocks. We design a customized regularization loss that utilizes data
sequence information so existing experts can maintain their ability to solve
old tasks while helping the new expert learn the new data effectively. As the
number of data blocks grows over time, the computational cost of the full
mixture-of-experts (MoE) model increases. To address this, we introduce a
sparse MoE approach, where only the top-$k$ most relevant experts make
predictions, significantly reducing the computation time. Our model achieved
4.92\% relative accuracy increase compared to the best baselines on class
incremental learning, showing the model's exceptional power.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [279] [Reinforcement learning in densely recurrent biological networks](https://arxiv.org/abs/2508.09618)
*Miles Walter Churchland,Jordi Garcia-Ojalvo*

Main category: cs.NE

TL;DR: The paper introduces ENOMAD, a hybrid optimization framework combining evolutionary and direct search methods to efficiently train recurrent networks modeled on biological circuits for specific tasks.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of existing gradient-based and evolutionary search methods in training highly recurrent networks in continuous action spaces.

Method: The proposed method, ENOMAD, integrates global evolutionary exploration with local direct search exploitation and employs biologically inspired weight priors to refine neural circuits.

Result: ENOMAD, with its two algorithmic variants, outperformed both untrained neural connectomes and existing training methods in food-foraging tasks using the nematode C. elegans' neural circuitry.

Conclusion: A biologically motivated hybrid optimization approach like ENOMAD offers an efficient and effective way to adapt natural recurrent networks for specialized tasks, outperforming existing strategies.

Abstract: Training highly recurrent networks in continuous action spaces is a technical
challenge: gradient-based methods suffer from exploding or vanishing gradients,
while purely evolutionary searches converge slowly in high-dimensional weight
spaces. We introduce a hybrid, derivative-free optimization framework that
implements reinforcement learning by coupling global evolutionary exploration
with local direct search exploitation. The method, termed ENOMAD (Evolutionary
Nonlinear Optimization with Mesh Adaptive Direct search), is benchmarked on a
suite of food-foraging tasks instantiated in the fully mapped neural connectome
of the nematode \emph{Caenorhabditis elegans}. Crucially, ENOMAD leverages
biologically derived weight priors, letting it refine--rather than rebuild--the
organism's native circuitry. Two algorithmic variants of the method are
introduced, which lead to either small distributed adjustments of many weights,
or larger changes on a limited number of weights. Both variants significantly
exceed the performance of the untrained connectome (in what can be interpreted
as an example of transfer learning) and of existing training strategies. These
findings demonstrate that integrating evolutionary search with nonlinear
optimization provides an efficient, biologically grounded strategy for
specializing natural recurrent networks towards a specified set of tasks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [280] [Invertible Syntax without the Tuples (Functional Pearl)](https://arxiv.org/abs/2508.09856)
*Mathieu Boespflug,Arnaud Spiwack*

Main category: cs.PL

TL;DR: The paper revisits Danvy's original use of continuation-passing for printf-like format strings, applying it to modern combinators for invertible syntax descriptions. It argues that continuation-passing remains relevant in parsing and printing structured data.


<details>
  <summary>Details</summary>
Motivation: To highlight the importance of Danvy's continuation-passing insights for developing modern combinators that define both parsers and printers with greater expressiveness.

Method: The authors adapt continuation-passing style, proposing three solutions distinct from dependent types and monoidal aggregation, to parse and print structured data with enhanced expressive power.

Result: Continuation passing is shown to retain its utility in addressing more general and expressive syntax descriptions, including complex inductive structures like lists and trees.

Conclusion: Continuation-passing style remains a viable and effective approach in modern syntax-combinator contexts, avoiding reliance on dependent types or monoidal aggregation.

Abstract: In the seminal paper Functional unparsing, Olivier Danvy used continuation
passing to reanalyse printf-like format strings as combinators. In the
intervening decades, the conversation shifted towards a concurrent line of work
-- applicative, monadic or arrow-based combinator libraries -- in an effort to
find combinators for invertible syntax descriptions that simultaneously
determine a parser as well as a printer, and with more expressive power, able
to handle inductive structures such as lists and trees. Along the way,
continuation passing got lost. This paper argues that Danvy's insight remains
as relevant to the general setting as it was to the restricted setting of his
original paper. Like him, we present three solutions that exploit
continuation-passing style as an alternative to both dependent types and
monoidal aggregation via nested pairs, in our case to parse and print
structured data with increasing expressive power.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [281] [Decision-Making-Based Path Planning for Autonomous UAVs: A Survey](https://arxiv.org/abs/2508.09304)
*Kelen C. Teixeira Vivaldini,Robert Pěnička,Martin Saska*

Main category: cs.RO

TL;DR: This survey evaluates how decision-making contributes to path planning for autonomous UAVs, detailing techniques like Exploration Path Planning and Informative Path Planning, and highlighting associated challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance autonomous UAV functionality by understanding how decision-making during flight impacts the path planning process, especially in dealing with environmental uncertainties.

Method: The paper conducts a survey of literature on decision-making in UAV path planning, categorizing studies into Exploration Path Planning and Informative Path Planning, and analyzing data modeling approaches.

Result: The survey summarizes the current state-of-the-art, categorizing approaches and emphasizing how data influences decision-making in UAV path planning.

Conclusion: The paper concludes by identifying significant challenges and research gaps, aiming to guide future developments in autonomous UAV decision-making and path planning techniques.

Abstract: One of the most critical features for the successful operation of autonomous
UAVs is the ability to make decisions based on the information acquired from
their surroundings. Each UAV must be able to make decisions during the flight
in order to deal with uncertainties in its system and the environment, and to
further act upon the information being received. Such decisions influence the
future behavior of the UAV, which is expressed as the path plan. Thus,
decision-making in path planning is an enabling technique for deploying
autonomous UAVs in real-world applications. This survey provides an overview of
existing studies that use aspects of decision-making in path planning,
presenting the research strands for Exploration Path Planning and Informative
Path Planning, and focusing on characteristics of how data have been modeled
and understood. Finally, we highlight the existing challenges for relevant
topics in this field.

</details>


### [282] [How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy](https://arxiv.org/abs/2508.09346)
*Zhenjiang Mao,Mrinall Eashaan Umasudhan,Ivan Ruchkin*

Main category: cs.RO

TL;DR: The paper proposes a framework for safety prediction in end-to-end vision-controlled autonomous robots, addressing challenges of partial observability and distribution shift.


<details>
  <summary>Details</summary>
Motivation: To improve the safety and reliability of autonomous robots controlled by deep neural networks under uncertain conditions.

Method: Leverages world models, variational autoencoders, recurrent predictors, and unsupervised domain adaptation for safety evaluation and calibration of confidence.

Result: Demonstrates high accuracy, lower false positives under distribution shift, and improved long-horizon tasks in three benchmarks.

Conclusion: The proposed method effectively ensures robust and calibrated safety evaluation, even under challenging prediction conditions and environmental changes.

Abstract: Autonomous robots that rely on deep neural network controllers pose critical
challenges for safety prediction, especially under partial observability and
distribution shift. Traditional model-based verification techniques are limited
in scalability and require access to low-dimensional state models, while
model-free methods often lack reliability guarantees. This paper addresses
these limitations by introducing a framework for calibrated safety prediction
in end-to-end vision-controlled systems, where neither the state-transition
model nor the observation model is accessible. Building on the foundation of
world models, we leverage variational autoencoders and recurrent predictors to
forecast future latent trajectories from raw image sequences and estimate the
probability of satisfying safety properties. We distinguish between monolithic
and composite prediction pipelines and introduce a calibration mechanism to
quantify prediction confidence. In long-horizon predictions from
high-dimensional observations, the forecasted inputs to the safety evaluator
can deviate significantly from the training distribution due to compounding
prediction errors and changing environmental conditions, leading to
miscalibrated risk estimates. To address this, we incorporate unsupervised
domain adaptation to ensure robustness of safety evaluation under distribution
shift in predictions without requiring manual labels. Our formulation provides
theoretical calibration guarantees and supports practical evaluation across
long prediction horizons. Experimental results on three benchmarks show that
our UDA-equipped evaluators maintain high accuracy and substantially lower
false positive rates under distribution shift. Similarly, world model-based
composite predictors outperform their monolithic counterparts on long-horizon
tasks, and our conformal calibration provides reliable statistical bounds.

</details>


### [283] [CLF-RL: Control Lyapunov Function Guided Reinforcement Learning](https://arxiv.org/abs/2508.09354)
*Kejun Li,Zachary Olkin,Yisong Yue,Aaron D. Ames*

Main category: cs.RO

TL;DR: The paper introduces a framework combining model-based trajectory planning and control Lyapunov functions (CLFs) to improve reinforcement learning for bipedal robot locomotion policies.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in reinforcement learning for bipedal robots related to reward design and sensitivity to poorly shaped objectives.

Method: The authors propose a structured reward shaping framework using two model-based planners (LIP model and HZD gait library) for trajectory generation and CLF-based rewards to guide policy learning.

Result: CLF-RL showed superior robustness and performance compared to baseline RL approaches, demonstrated through extensive real-world experiments and simulations.

Conclusion: The proposed framework enhances reinforcement learning by providing meaningful intermediate rewards and simplifying policy deployment, proving beneficial for real-world bipedal robot locomotion.

Abstract: Reinforcement learning (RL) has shown promise in generating robust locomotion
policies for bipedal robots, but often suffers from tedious reward design and
sensitivity to poorly shaped objectives. In this work, we propose a structured
reward shaping framework that leverages model-based trajectory generation and
control Lyapunov functions (CLFs) to guide policy learning. We explore two
model-based planners for generating reference trajectories: a reduced-order
linear inverted pendulum (LIP) model for velocity-conditioned motion planning,
and a precomputed gait library based on hybrid zero dynamics (HZD) using
full-order dynamics. These planners define desired end-effector and joint
trajectories, which are used to construct CLF-based rewards that penalize
tracking error and encourage rapid convergence. This formulation provides
meaningful intermediate rewards, and is straightforward to implement once a
reference is available. Both the reference trajectories and CLF shaping are
used only during training, resulting in a lightweight policy at deployment. We
validate our method both in simulation and through extensive real-world
experiments on a Unitree G1 robot. CLF-RL demonstrates significantly improved
robustness relative to the baseline RL policy and better performance than a
classic tracking reward RL formulation.

</details>


### [284] [DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation](https://arxiv.org/abs/2508.09444)
*Haoxiang Shi,Xiang Deng,Zaijing Li,Gongwei Chen,Yaowei Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: The paper introduces DifNav, an end-to-end policy for Vision-Language Navigation tasks that eliminates the traditional two-stage framework, achieving better navigation performance without waypoint predictors.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Navigation approaches in continuous environments suffer from suboptimal frameworks and rely heavily on waypoint predictors, which limits their performance.

Method: The authors propose DifNav, a unified diffusion policy replacing waypoint generation and planning. They optimize the policy end-to-end using DAgger for online training and use aggregated data for fine-tuning.

Result: Experiments show that DifNav significantly outperforms traditional models, achieving state-of-the-art navigation performance without dependency on waypoint predictors.

Conclusion: DifNav addresses major limitations of existing methods, offering a robust, unified navigation framework that handles compounding errors and achieves superior results.

Abstract: Vision-Language Navigation in Continuous Environments (VLN-CE) requires
agents to follow natural language instructions through free-form 3D spaces.
Existing VLN-CE approaches typically use a two-stage waypoint planning
framework, where a high-level waypoint predictor generates the navigable
waypoints, and then a navigation planner suggests the intermediate goals in the
high-level action space. However, this two-stage decomposition framework
suffers from: (1) global sub-optimization due to the proxy objective in each
stage, and (2) a performance bottleneck caused by the strong reliance on the
quality of the first-stage predicted waypoints. To address these limitations,
we propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE
policy that unifies the traditional two stages, i.e. waypoint generation and
planning, into a single diffusion policy. Notably, DifNav employs a conditional
diffusion policy to directly model multi-modal action distributions over future
actions in continuous navigation space, eliminating the need for a waypoint
predictor while enabling the agent to capture multiple possible
instruction-following behaviors. To address the issues of compounding error in
imitation learning and enhance spatial reasoning in long-horizon navigation
tasks, we employ DAgger for online policy training and expert trajectory
augmentation, and use the aggregated data to further fine-tune the policy. This
approach significantly improves the policy's robustness and its ability to
recover from error states. Extensive experiments on benchmark datasets
demonstrate that, even without a waypoint predictor, the proposed method
substantially outperforms previous state-of-the-art two-stage waypoint-based
models in terms of navigation performance. Our code is available at:
https://github.com/Tokishx/DifNav.

</details>


### [285] [Reactive Model Predictive Contouring Control for Robot Manipulators](https://arxiv.org/abs/2508.09502)
*Junheon Yoon,Woo-Jeong Baek,Jaeheung Park*

Main category: cs.RO

TL;DR: A method called Reactive Model Predictive Contouring Control (RMPCC) facilitates real-time path-following of robots in dynamic environments, with quick obstacle avoidance and minimal error.


<details>
  <summary>Details</summary>
Motivation: Current path-following methods struggle with collision avoidance, singularity handling, and constraint adherence in dynamic environments, particularly during evasive maneuvers.

Method: RMPCC is employed to optimize path-following by parameterizing the reference path and combining this with Control Barrier Functions (CBFs) for obstacle and singularity avoidance. Advanced computational techniques like Jacobian-based linearization and Gauss-Newton Hessian approximation allow for fast problem-solving.

Result: The proposed framework achieves a computation speed of 100 Hz, outperforming existing methods by a factor of 10, while maintaining low contouring error and minimal robot acceleration.

Conclusion: This framework enables efficient and precise robot maneuvering in dynamic scenarios, making it a practical solution for real-world robotic applications.

Abstract: This contribution presents a robot path-following framework via Reactive
Model Predictive Contouring Control (RMPCC) that successfully avoids obstacles,
singularities and self-collisions in dynamic environments at 100 Hz. Many
path-following methods rely on the time parametrization, but struggle to handle
collision and singularity avoidance while adhering kinematic limits or other
constraints. Specifically, the error between the desired path and the actual
position can become large when executing evasive maneuvers. Thus, this paper
derives a method that parametrizes the reference path by a path parameter and
performs the optimization via RMPCC. In particular, Control Barrier Functions
(CBFs) are introduced to avoid collisions and singularities in dynamic
environments. A Jacobian-based linearization and Gauss-Newton Hessian
approximation enable solving the nonlinear RMPCC problem at 100 Hz,
outperforming state-of-the-art methods by a factor of 10. Experiments confirm
that the framework handles dynamic obstacles in real-world settings with low
contouring error and low robot acceleration.

</details>


### [286] [SMART-OC: A Real-time Time-risk Optimal Replanning Algorithm for Dynamic Obstacles and Spatio-temporally Varying Currents](https://arxiv.org/abs/2508.09508)
*Reema Raval,Shalabh Gupta*

Main category: cs.RO

TL;DR: The paper addresses the challenges of safe navigation in dynamic marine environments and proposes the SMART-OC algorithm for real-time optimal path replanning in Unmanned Surface Vehicles (USVs).


<details>
  <summary>Details</summary>
Motivation: Marine environments are complex due to spatio-temporally varying currents and dynamic obstacles, requiring USVs to continuously adapt their navigation paths for safety and efficiency.

Method: The paper introduces SMART-OC, an algorithm that integrates obstacle risks and time costs for optimal path replanning in real-time dynamic marine environments.

Result: Simulation experiments validate the effectiveness of SMART-OC, demonstrating its ability to enable USVs to avoid dynamic obstacles and exploit ocean currents for goal-oriented navigation.

Conclusion: SMART-OC allows USVs to perform real-time time-risk optimal replanning, ensuring efficient navigation in dynamic marine settings.

Abstract: Typical marine environments are highly complex with spatio-temporally varying
currents and dynamic obstacles, presenting significant challenges to Unmanned
Surface Vehicles (USVs) for safe and efficient navigation. Thus, the USVs need
to continuously adapt their paths with real-time information to avoid
collisions and follow the path of least resistance to the goal via exploiting
ocean currents. In this regard, we introduce a novel algorithm, called
Self-Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents
(SMART-OC), that facilitates real-time time-risk optimal replanning in dynamic
environments. SMART-OC integrates the obstacle risks along a path with the time
cost to reach the goal to find the time-risk optimal path. The effectiveness of
SMART-OC is validated by simulation experiments, which demonstrate that the USV
performs fast replannings to avoid dynamic obstacles and exploit ocean currents
to successfully reach the goal.

</details>


### [287] [CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail](https://arxiv.org/abs/2508.09558)
*Jiahui Zuo,Boyang Zhang,Fumin Zhang*

Main category: cs.RO

TL;DR: This paper introduces an eagle-inspired fingernail for grippers and a novel single-grasp 3D cable routing framework, enabling efficient and safer robotic cable manipulation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of cable routing in robot automation, particularly the risks associated with over-squeezing and over-tension when using common grippers.

Method: A novel eagle-inspired fingernail design for grippers is combined with a single-grasp end-to-end 3D cable routing framework. Key methods include vision-based state estimation, offline trajectory planning using motion primitives, and continuous control.

Result: The framework was evaluated with various cables and channel slots, and it significantly outperformed traditional pick-and-place manipulation processes under comparable conditions.

Conclusion: The proposed fingernail design and framework demonstrate greater efficiency and safety for robotic cable routing tasks, setting a benchmark for future 3D cable routing applications.

Abstract: The manipulation of deformable linear flexures has a wide range of
applications in industry, such as cable routing in automotive manufacturing and
textile production. Cable routing, as a complex multi-stage robot manipulation
scenario, is a challenging task for robot automation. Common parallel
two-finger grippers have the risk of over-squeezing and over-tension when
grasping and guiding cables. In this paper, a novel eagle-inspired fingernail
is designed and mounted on the gripper fingers, which helps with cable grasping
on planar surfaces and in-hand cable guiding operations. Then we present a
single-grasp end-to-end 3D cable routing framework utilizing the proposed
fingernails, instead of the common pick-and-place strategy. Continuous control
is achieved to efficiently manipulate cables through vision-based state
estimation of task configurations and offline trajectory planning based on
motion primitives. We evaluate the effectiveness of the proposed framework with
a variety of cables and channel slots, significantly outperforming the
pick-and-place manipulation process under equivalent perceptual conditions. Our
reconfigurable task setting and the proposed framework provide a reference for
future cable routing manipulations in 3D space.

</details>


### [288] [ESCoT: An Enhanced Step-based Coordinate Trajectory Planning Method for Multiple Car-like Robots](https://arxiv.org/abs/2508.09581)
*Junkai Jiang,Yihe Chen,Yibin Yang,Ruochen Li,Shaobing Xu,Jianqiang Wang*

Main category: cs.RO

TL;DR: This paper introduces ESCoT, a trajectory planning method for multiple car-like robots, focusing on improving solution quality and efficiency in both sparse and dense scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of multi-vehicle trajectory planning (MVTP) in multi-robot systems, which are essential for real-world applications.

Method: ESCoT employs two strategies: collaborative planning for local robot groups and replanning to resolve duplicate configurations, enhancing step-based trajectory planning methods.

Result: ESCoT significantly improves solution quality in sparse scenarios (70% in typical conflicts, 34% randomly) and maintains strong performance in dense scenarios with a success rate above 50%.

Conclusion: ESCoT effectively addresses MVTP challenges, extends capabilities of step-based methods, and is validated through practical robot tests.

Abstract: Multi-vehicle trajectory planning (MVTP) is one of the key challenges in
multi-robot systems (MRSs) and has broad applications across various fields.
This paper presents ESCoT, an enhanced step-based coordinate trajectory
planning method for multiple car-like robots. ESCoT incorporates two key
strategies: collaborative planning for local robot groups and replanning for
duplicate configurations. These strategies effectively enhance the performance
of step-based MVTP methods. Through extensive experiments, we show that ESCoT
1) in sparse scenarios, significantly improves solution quality compared to
baseline step-based method, achieving up to 70% improvement in typical conflict
scenarios and 34% in randomly generated scenarios, while maintaining high
solving efficiency; and 2) in dense scenarios, outperforms all baseline
methods, maintains a success rate of over 50% even in the most challenging
configurations. The results demonstrate that ESCoT effectively solves MVTP,
further extending the capabilities of step-based methods. Finally, practical
robot tests validate the algorithm's applicability in real-world scenarios.

</details>


### [289] [HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control](https://arxiv.org/abs/2508.09595)
*Michael Fennel,Markus Walker,Dominik Pikos,Uwe D. Hanebeck*

Main category: cs.RO

TL;DR: This paper introduces HapticGiant, a novel kinesthetic haptic interface that overcomes traditional limitations to enhance VR immersion.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges with current kinesthetic haptic interfaces, such as limited workspaces, insufficient degrees of freedom, and mismatched arm kinematics.

Method: The researchers developed HapticGiant, integrating an admittance-type force control scheme using hierarchical optimization, accommodating system constraints and achieving realistic feedback.

Result: Experimental results showcased the efficacy of HapticGiant and its control scheme in addressing these challenges while supporting immersive interactions.

Conclusion: The work demonstrates a step forward in improving immersive VR experiences through advanced haptic feedback technologies.

Abstract: Research in virtual reality and haptic technologies has consistently aimed to
enhance immersion. While advanced head-mounted displays are now commercially
available, kinesthetic haptic interfaces still face challenges such as limited
workspaces, insufficient degrees of freedom, and kinematics not matching the
human arm. In this paper, we present HapticGiant, a novel large-scale
kinesthetic haptic interface designed to match the properties of the human arm
as closely as possible and to facilitate natural user locomotion while
providing full haptic feedback. The interface incorporates a novel
admittance-type force control scheme, leveraging hierarchical optimization to
render both arbitrary serial kinematic chains and Cartesian admittances.
Notably, the proposed control scheme natively accounts for system limitations,
including joint and Cartesian constraints, as well as singularities.
Experimental results demonstrate the effectiveness of HapticGiant and its
control scheme, paving the way for highly immersive virtual reality
applications.

</details>


### [290] [BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots](https://arxiv.org/abs/2508.09606)
*Alejandro Posadas-Nava,Alejandro Carrasco,Richard Linares*

Main category: cs.RO

TL;DR: BEAVR is an open-source VR-based teleoperation system for controlling robots. It enables low-latency, dexterous control of various robotic systems while supporting data recording and policy learning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create a unified, versatile, and real-time teleoperation system using VR for diverse robotic platforms, addressing challenges in dexterous control, data recording, and policy learning.

Method: The system employs VR hardware for teleoperation and has a zero-copy architecture for low latency, asynchronous control loops for scalability, and a modular network API for multi-robot operation.

Result: BEAVR demonstrates low latency (≤35 ms), compatibility with modern visuomotor policies, modular integration for various robots, and successful multi-modal manipulation task benchmarks.

Conclusion: This system advances VR-based robotic teleoperation, providing open-source tools and datasets for further research in dexterous multi-robot control and policy development.

Abstract: \textbf{BEAVR} is an open-source, bimanual, multi-embodiment Virtual Reality
(VR) teleoperation system for robots, designed to unify real-time control, data
recording, and policy learning across heterogeneous robotic platforms. BEAVR
enables real-time, dexterous teleoperation using commodity VR hardware,
supports modular integration with robots ranging from 7-DoF manipulators to
full-body humanoids, and records synchronized multi-modal demonstrations
directly in the LeRobot dataset schema. Our system features a zero-copy
streaming architecture achieving $\leq$35\,ms latency, an asynchronous
``think--act'' control loop for scalable inference, and a flexible network API
optimized for real-time, multi-robot operation. We benchmark BEAVR across
diverse manipulation tasks and demonstrate its compatibility with leading
visuomotor policies such as ACT, DiffusionPolicy, and SmolVLA. All code is
publicly available, and datasets are released on Hugging Face\footnote{Code,
datasets, and VR app available at https://github.com/ARCLab-MIT/BEAVR-Bot.

</details>


### [291] [Interpretable Robot Control via Structured Behavior Trees and Large Language Models](https://arxiv.org/abs/2508.09621)
*Ingrid Maéva Chekam,Ines Pastor-Martinez,Ali Tourani,Jose Andres Millan-Romera,Laura Ribeiro,Pedro Miguel Bastos Soares,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: The paper integrates Large Language Models (LLMs) with Behavior Trees to create a framework for robots to understand human natural language and execute actions with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional robot interfaces require users to adapt to them, reducing usability in unstructured environments. Thus, a more natural and adaptable Human-Robot Interaction (HRI) interface is needed.

Method: A framework is introduced combining LLMs for language understanding with Behavior Trees for robotic execution, emphasizing modularity and scalability, particularly in perception tasks like person tracking and gesture recognition.

Result: Real-world experiments validate the framework, achieving approximately 94% accuracy from cognition to execution across varied scenarios.

Conclusion: The presented system significantly enhances HRI by enabling intuitive and reliable interaction, with its implementation and source code made publicly available to further its use and development.

Abstract: As intelligent robots become more integrated into human environments, there
is a growing need for intuitive and reliable Human-Robot Interaction (HRI)
interfaces that are adaptable and more natural to interact with. Traditional
robot control methods often require users to adapt to interfaces or memorize
predefined commands, limiting usability in dynamic, unstructured environments.
This paper presents a novel framework that bridges natural language
understanding and robotic execution by combining Large Language Models (LLMs)
with Behavior Trees. This integration enables robots to interpret natural
language instructions given by users and translate them into executable actions
by activating domain-specific plugins. The system supports scalable and modular
integration, with a primary focus on perception-based functionalities, such as
person tracking and hand gesture recognition. To evaluate the system, a series
of real-world experiments was conducted across diverse environments.
Experimental results demonstrate that the proposed approach is practical in
real-world scenarios, with an average cognition-to-execution accuracy of
approximately 94%, making a significant contribution to HRI systems and robots.
The complete source code of the framework is publicly available at
https://github.com/snt-arg/robot_suite.

</details>


### [292] [Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions](https://arxiv.org/abs/2508.09700)
*Mahdi Hejrati,Jouni Mattila*

Main category: cs.RO

TL;DR: The paper discusses teleoperation challenges for beyond-human-scale robotic manipulators and proposes strategies for scalable, safe, and effective human-robot collaboration.


<details>
  <summary>Details</summary>
Motivation: To address control and interface challenges posed by large-scale robotic manipulators, emphasizing human safety, sensorimotor alignment, and operator embodiment.

Method: Analyzed trade-offs in haptic and visual feedback mechanisms, comparing exoskeleton and joystick setups through experimental data.

Result: Identified strengths and weaknesses of different feedback and control systems, along with proposed key research directions.

Conclusion: Highlighting the need for tailored evaluation tools and human-centered models to improve teleoperation of large-scale robots in industrial applications.

Abstract: Teleoperation of beyond-human-scale robotic manipulators (BHSRMs) presents
unique challenges that differ fundamentally from conventional human-scale
systems. As these platforms gain relevance in industrial domains such as
construction, mining, and disaster response, immersive interfaces must be
rethought to support scalable, safe, and effective human-robot collaboration.
This paper investigates the control, cognitive, and interface-level challenges
of immersive teleoperation in BHSRMs, with a focus on ensuring operator safety,
minimizing sensorimotor mismatch, and enhancing the sense of embodiment. We
analyze design trade-offs in haptic and visual feedback systems, supported by
early experimental comparisons of exoskeleton- and joystick-based control
setups. Finally, we outline key research directions for developing new
evaluation tools, scaling strategies, and human-centered safety models tailored
to large-scale robotic telepresence.

</details>


### [293] [FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning](https://arxiv.org/abs/2508.09797)
*Dongcheng Cao,Jin Zhou,Xian Wang,Shuo Li*

Main category: cs.RO

TL;DR: The paper introduces FLARE, an RL framework for quadrotor cable-payload systems, overcoming computational and maneuverability limits seen in traditional methods.


<details>
  <summary>Details</summary>
Motivation: The complexity of underactuated, nonlinear, and hybrid dynamics in quadrotor cable-payload systems has presented challenges for agile flight, especially as traditional methods face computational and real-time application limitations.

Method: The authors propose FLARE, a reinforcement learning framework that learns agile navigation policies directly from high-fidelity simulations.

Result: FLARE outperformed state-of-the-art optimization methods with a 3x speedup in gate traversal. It also demonstrated zero-shot sim-to-real transfer effectiveness in real-world experiments.

Conclusion: FLARE showcases its potential for real-time, agile, and safe quadrotor navigation, addressing limitations of traditional methods in complex scenarios.

Abstract: Agile flight for the quadrotor cable-suspended payload system is a formidable
challenge due to its underactuated, highly nonlinear, and hybrid dynamics.
Traditional optimization-based methods often struggle with high computational
costs and the complexities of cable mode transitions, limiting their real-time
applicability and maneuverability exploitation. In this letter, we present
FLARE, a reinforcement learning (RL) framework that directly learns agile
navigation policy from high-fidelity simulation. Our method is validated across
three designed challenging scenarios, notably outperforming a state-of-the-art
optimization-based approach by a 3x speedup during gate traversal maneuvers.
Furthermore, the learned policies achieve successful zero-shot sim-to-real
transfer, demonstrating remarkable agility and safety in real-world
experiments, running in real time on an onboard computer.

</details>


### [294] [QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds](https://arxiv.org/abs/2504.19716)
*Navin Sriram Ravie,Keerthi Vasan M,Asokan Thondiyath,Bijo Sebastian*

Main category: cs.RO

TL;DR: The paper proposes a lightweight, analytical method for robotic grasp planning that minimizes reliance on sampling in the six-degree-of-freedom space, enhancing efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: Robotic grasp planning faces challenges in real-world applications due to poor generalization across domains, inefficiency in sampling, and lack of repeatability in existing methods.

Method: The paper formulates grasp planning as an optimization problem to estimate grasp points on an object's surface using a soft-region-growing algorithm for plane segmentation. It evaluates the quality of grasp points through an optimization-based metric ensuring indirect force closure.

Result: The proposed approach was benchmarked against the Grasp Pose Detection (GPD) framework in simulations and real-world tests, demonstrating higher effectiveness in terms of grasp planning and execution.

Conclusion: The lightweight analytical approach improves grasp planning by addressing inefficiencies in sampling and generalization, proving effective in both simulated and real-world robotics tasks.

Abstract: Grasping has been a long-standing challenge in facilitating the final
interface between a robot and the environment. As environments and tasks become
complicated, the need to embed higher intelligence to infer from the
surroundings and act on them has become necessary. Although most methods
utilize techniques to estimate grasp pose by treating the problem via pure
sampling-based approaches in the six-degree-of-freedom space or as a learning
problem, they usually fail in real-life settings owing to poor generalization
across domains. In addition, the time taken to generate the grasp plan and the
lack of repeatability, owing to sampling inefficiency and the probabilistic
nature of existing grasp planning approaches, severely limits their application
in real-world tasks. This paper presents a lightweight analytical approach
towards robotic grasp planning, particularly antipodal grasps, with little to
no sampling in the six-degree-of-freedom space. The proposed grasp planning
algorithm is formulated as an optimization problem towards estimating grasp
points on the object surface instead of directly estimating the end-effector
pose. To this extent, a soft-region-growing algorithm is presented for
effective plane segmentation, even in the case of curved surfaces. An
optimization-based quality metric is then used for the evaluation of grasp
points to ensure indirect force closure. The proposed grasp framework is
compared with the existing state-of-the-art grasp planning approach, Grasp pose
detection (GPD), as a baseline over multiple simulated objects. The
effectiveness of the proposed approach in comparison to GPD is also evaluated
in a real-world setting using image and point-cloud data, with the planned
grasps being executed using a ROBOTIQ gripper and UR5 manipulator.

</details>


### [295] [Embodied Tactile Perception of Soft Objects Properties](https://arxiv.org/abs/2508.09836)
*Anirvan Dutta,Alexis WM Devillard,Zhihuan Zhang,Xiaoxiao Cheng,Etienne Burdet*

Main category: cs.RO

TL;DR: The paper studies how compliance, sensing, and purposeful actions enable robotic tactile perception using a modular e-Skin and unsupervised learning.


<details>
  <summary>Details</summary>
Motivation: To understand human-like fine manipulation in robots by studying mechanical compliance, interaction strategies, and multi-modal tactile sensing.

Method: The authors use a modular e-Skin with tunable compliance and multi-modal sensing with controlled interaction primitives, coupled with a deep state-space unsupervised model for latent feature extraction.

Result: Multi-modal sensing demonstrated superior tactile perception versus uni-modal approaches, with nuanced environmental interactions enhancing perception accuracy.

Conclusion: Embodiment, multi-modal sensing, and interaction strategies critically shape robotic tactile perception, with temporal dynamic considerations offering deeper insights.

Abstract: To enable robots to develop human-like fine manipulation, it is essential to
understand how mechanical compliance, multi-modal sensing, and purposeful
interaction jointly shape tactile perception. In this study, we use a dedicated
modular e-Skin with tunable mechanical compliance and multi-modal sensing
(normal, shear forces and vibrations) to systematically investigate how sensing
embodiment and interaction strategies influence robotic perception of objects.
Leveraging a curated set of soft wave objects with controlled viscoelastic and
surface properties, we explore a rich set of palpation primitives-pressing,
precession, sliding that vary indentation depth, frequency, and directionality.
In addition, we propose the latent filter, an unsupervised, action-conditioned
deep state-space model of the sophisticated interaction dynamics and infer
causal mechanical properties into a structured latent space. This provides
generalizable and in-depth interpretable representation of how embodiment and
interaction determine and influence perception. Our investigation demonstrates
that multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced
interaction between the environment and mechanical properties of e-Skin, which
should be examined alongside the interaction by incorporating temporal
dynamics.

</details>


### [296] [Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation](https://arxiv.org/abs/2508.09846)
*Donghoon Baek,Amartya Purushottam,Jason J. Choi,Joao Ramos*

Main category: cs.RO

TL;DR: This paper introduces a framework for teleoperating wheeled humanoids during loco-manipulation tasks, featuring an object parameter estimator that improves dynamic handling using vision and hierarchical methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance dynamic whole-body bilateral teleoperation for wheeled humanoids by enabling real-time adjustments to interactions with objects, particularly for tasks involving significant payloads, through precise object parameter estimation.

Method: The framework incorporates a multi-stage object inertial parameter estimation strategy using a vision-based size estimator, large vision-language model (VLM) initial guesses, and a hierarchical sampling approach. The system runs in parallel with hardware and updates parameters in real-time, adapting the robot's equilibrium and haptic feedback.

Result: The system demonstrated real-time performance on a custom wheeled humanoid during various tasks, handling a payload weighting a third of the robot's body weight, with improvements in manipulation tracking and compliant behavior.

Conclusion: The proposed framework effectively enhances dynamic teleoperation of wheeled humanoids by integrating online object parameter estimation, improving haptic feedback, and enabling robust loco-manipulation under significant dynamic loads.

Abstract: This paper presents an object-aware whole-body bilateral teleoperation
framework for wheeled humanoid loco-manipulation. This framework combines
whole-body bilateral teleoperation with an online multi-stage object inertial
parameter estimation module, which is the core technical contribution of this
work. The multi-stage process sequentially integrates a vision-based object
size estimator, an initial parameter guess generated by a large vision-language
model (VLM), and a decoupled hierarchical sampling strategy. The visual size
estimate and VLM prior offer a strong initial guess of the object's inertial
parameters, significantly reducing the search space for sampling-based
refinement and improving the overall estimation speed. A hierarchical strategy
first estimates mass and center of mass, then infers inertia from object size
to ensure physically feasible parameters, while a decoupled multi-hypothesis
scheme enhances robustness to VLM prior errors. Our estimator operates in
parallel with high-fidelity simulation and hardware, enabling real-time online
updates. The estimated parameters are then used to update the wheeled
humanoid's equilibrium point, allowing the operator to focus more on locomotion
and manipulation. This integration improves the haptic force feedback for
dynamic synchronization, enabling more dynamic whole-body teleoperation. By
compensating for object dynamics using the estimated parameters, the framework
also improves manipulation tracking while preserving compliant behavior. We
validate the system on a customized wheeled humanoid with a robotic gripper and
human-machine interface, demonstrating real-time execution of lifting,
delivering, and releasing tasks with a payload weighing approximately one-third
of the robot's body weight.

</details>


### [297] [Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes](https://arxiv.org/abs/2508.09855)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: This paper proposes a method to train human-robot handover policies using RGB images and simulation, eliminating the need for real-world robot training or data collection.


<details>
  <summary>Details</summary>
Motivation: Human-robot teaming often struggles with the need for extensive real-world data for tasks like handovers, and simulation gaps hinder training efficiency.

Method: The method uses sparse-view Gaussian Splatting to reconstruct human-to-robot handover interactions, enabling training policies purely in simulation using robot-mounted camera data.

Result: The proposed method successfully generates effective policies for human-to-robot handover tasks, validated through both simulated and real-world experiments.

Conclusion: This approach provides a cost-effective and robust way to train robots for human handovers, bridging the gap between simulation and physical environments.

Abstract: Human-robot teaming (HRT) systems often rely on large-scale datasets of human
and robot interactions, especially for close-proximity collaboration tasks such
as human-robot handovers. Learning robot manipulation policies from raw,
real-world image data requires a large number of robot-action trials in the
physical environment. Although simulation training offers a cost-effective
alternative, the visual domain gap between simulation and robot workspace
remains a major limitation. We introduce a method for training HRT policies,
focusing on human-to-robot handovers, solely from RGB images without the need
for real-robot training or real-robot data collection. The goal is to enable
the robot to reliably receive objects from a human with stable grasping while
avoiding collisions with the human hand. The proposed policy learner leverages
sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes
to generate robot demonstrations containing image-action pairs captured with a
camera mounted on the robot gripper. As a result, the simulated camera pose
changes in the reconstructed scene can be directly translated into gripper pose
changes. Experiments in both Gaussian Splatting reconstructed scene and
real-world human-to-robot handover experiments demonstrate that our method
serves as a new and effective representation for the human-to-robot handover
task, contributing to more seamless and robust HRT.

</details>


### [298] [A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion](https://arxiv.org/abs/2508.09876)
*Xiaowei Tan,Weizhong Jiang,Bi Zhang,Wanxin Chen,Yiwen Zhao,Ning Li,Lianqing Liu,Xingang Zhao*

Main category: cs.RO

TL;DR: A shank angle-based control system enables real-time adaptation of exoskeleton assistance for walking, running, and stair tasks, accommodating gait phase perturbations.


<details>
  <summary>Details</summary>
Motivation: The study addresses the gap in understanding how exoskeletons can assist during non-steady locomotion and diverse activities, where gait cycles deviate from linear patterns.

Method: The system involves an online adjustable dual-Gaussian assistance profile model based on IMU data, combined with model-based feedforward control that adapts dynamically to biomechanical variability.

Result: Experiments validated the individual methods, demonstrating robust adaptation to gait perturbations and positive biomechanical and physiological responses during diverse locomotion tasks.

Conclusion: The developed control system offers effective and adaptable mechanical assistance for various locomotion activities, improving user experience and gait phase synchronization.

Abstract: Exoskeletons have been shown to effectively assist humans during steady
locomotion. However, their effects on non-steady locomotion, characterized by
nonlinear phase progression within a gait cycle, remain insufficiently
explored, particularly across diverse activities. This work presents a shank
angle-based control system that enables the exoskeleton to maintain real-time
coordination with human gait, even under phase perturbations, while dynamically
shaping assistance profiles to match the biological ankle moment patterns
across walking, running, stair negotiation tasks. The control system consists
of an assistance profile online generation method and a model-based feedforward
control method. The assistance profile is formulated as a dual-Gaussian model
with the shank angle as the independent variable. Leveraging only IMU
measurements, the model parameters are updated online each stride to adapt to
inter- and intra-individual biomechanical variability. The profile tracking
control employs a human-exoskeleton kinematics and stiffness model as a
feedforward component, reducing reliance on historical control data due to the
lack of clear and consistent periodicity in non-steady locomotion. Three
experiments were conducted using a lightweight soft exoskeleton with multiple
subjects. The results validated the effectiveness of each individual method,
demonstrated the robustness of the control system against gait perturbations
across various activities, and revealed positive biomechanical and
physiological responses of human users to the exoskeleton's mechanical
assistance.

</details>


### [299] [PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces](https://arxiv.org/abs/2508.09950)
*Bida Ma,Nuo Xu,Chenkun Qi,Xin Liu,Yule Mo,Jinkai Wang,Chunpeng Lu*

Main category: cs.RO

TL;DR: Proposes a proprioceptive locomotion learning method supervised by point clouds for legged robots in crawl spaces, enabling agile traversal without heavy reliance on exteroceptive sensors.


<details>
  <summary>Details</summary>
Motivation: Legged robots struggle in crawl spaces due to limitations in existing proprioceptive methods and sensor noise in low-visibility environments.

Method: Uses a state estimation network trained with supervised learning from polar-framed point clouds to extract ground and spatial features for proprioceptive locomotion. Also designs reward functions to guide robots through collisions.

Result: Experiments show improved locomotion agility in crawl spaces compared to current methods.

Conclusion: The method significantly improves legged robots' capability to navigate spatially constrained environments without exteroceptive sensors.

Abstract: The legged locomotion in spatially constrained structures (called crawl
spaces) is challenging. In crawl spaces, current exteroceptive locomotion
learning methods are limited by large noises and errors of the sensors in
possible low visibility conditions, and current proprioceptive locomotion
learning methods are difficult in traversing crawl spaces because only ground
features are inferred. In this study, a point cloud supervised proprioceptive
locomotion reinforcement learning method for legged robots in crawl spaces is
proposed. A state estimation network is designed to estimate the robot's
surrounding ground and spatial features as well as the robot's collision states
using historical proprioceptive sensor data. The point cloud is represented in
polar coordinate frame and a point cloud processing method is proposed to
efficiently extract the ground and spatial features that are used to supervise
the state estimation network learning. Comprehensive reward functions that
guide the robot to traverse through crawl spaces after collisions are designed.
Experiments demonstrate that, compared to existing methods, our method exhibits
more agile locomotion in crawl spaces. This study enhances the ability of
legged robots to traverse spatially constrained environments without requiring
exteroceptive sensors.

</details>


### [300] [GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation](https://arxiv.org/abs/2508.09960)
*Yifei Yao,Chengyuan Luo,Jiaheng Du,Wentao He,Jun-Guo Lu*

Main category: cs.RO

TL;DR: This paper presents Generalized Behavior Cloning (GBC), a unified framework enabling human-like motion transfer from human data to diverse humanoid robots via adaptive pipelines, novel learning algorithms, and open-source tools.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of fragmentation in humanoid robot design, where algorithms for learning and data processing are not universally applicable across different robot morphologies, limiting progress in creating human-like robots.

Method: The framework integrates three innovations: an adaptive data pipeline using a differentiable IK network for motion retargeting, a DAgger-MMPPO algorithm with an MMTransformer architecture for robust imitation learning, and an open-source platform for streamlined deployment using Isaac Lab.

Result: The approach was validated by training policies across various types of humanoid robots, achieving high performance, and successfully transferring learned behaviors to novel motions.

Conclusion: This work introduces the first practical and unified framework for developing generalized humanoid controllers, offering a complete, efficient, and open-source solution to the challenge of transferring human motion to robots.

Abstract: The creation of human-like humanoid robots is hindered by a fundamental
fragmentation: data processing and learning algorithms are rarely universal
across different robot morphologies. This paper introduces the Generalized
Behavior Cloning (GBC) framework, a comprehensive and unified solution designed
to solve this end-to-end challenge. GBC establishes a complete pathway from
human motion to robot action through three synergistic innovations. First, an
adaptive data pipeline leverages a differentiable IK network to automatically
retarget any human MoCap data to any humanoid. Building on this foundation, our
novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust,
high-fidelity imitation policies. To complete the ecosystem, the entire
framework is delivered as an efficient, open-source platform based on Isaac
Lab, empowering the community to deploy the full workflow via simple
configuration scripts. We validate the power and generality of GBC by training
policies on multiple heterogeneous humanoids, demonstrating excellent
performance and transfer to novel motions. This work establishes the first
practical and unified pathway for creating truly generalized humanoid
controllers.

</details>


### [301] [Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model](https://arxiv.org/abs/2508.09971)
*Zihan Wang,Nina Mahmoudian*

Main category: cs.RO

TL;DR: The paper focuses on developing a vision-driven method for Unmanned Aerial Vehicles (UAVs) to autonomously follow rivers, especially in GPS-unreliable areas, and frames it as a constrained reinforcement learning problem.


<details>
  <summary>Details</summary>
Motivation: Autonomous UAV river-following is essential for tasks like rescue, surveillance, and environmental monitoring in challenging environments where GPS may be unreliable.

Method: The authors propose (1) Marginal Gain Advantage Estimation (MGAE) for improved reward advantage alignment, (2) a Semantic Dynamics Model (SDM) for interpretable and efficient prediction, and (3) the Constrained Actor Dynamics Estimator architecture for integrating these approaches into a Safe Reinforcement Learning (SafeRL) framework.

Result: Simulation results show that MGAE offers quicker convergence and outperforms traditional methods, the SDM improves prediction accuracy, and the overall CADE framework effectively balances reward and safety requirements.

Conclusion: The paper successfully integrates novel techniques into a model-based RL framework, providing safety and performance benefits in the autonomous river-following task, thereby paving the way for real-world applications in challenging environments.

Abstract: Vision-driven autonomous river following by Unmanned Aerial Vehicles is
critical for applications such as rescue, surveillance, and environmental
monitoring, particularly in dense riverine environments where GPS signals are
unreliable. We formalize river following as a coverage control problem in which
the reward function is submodular, yielding diminishing returns as more unique
river segments are visited, thereby framing the task as a Submodular Markov
Decision Process. First, we introduce Marginal Gain Advantage Estimation, which
refines the reward advantage function by using a sliding window baseline
computed from historical episodic returns, thus aligning the advantage
estimation with the agent's evolving recognition of action value in
non-Markovian settings. Second, we develop a Semantic Dynamics Model based on
patchified water semantic masks that provides more interpretable and
data-efficient short-term prediction of future observations compared to latent
vision dynamics models. Third, we present the Constrained Actor Dynamics
Estimator architecture, which integrates the actor, the cost estimator, and SDM
for cost advantage estimation to form a model-based SafeRL framework capable of
solving partially observable Constrained Submodular Markov Decision Processes.
Simulation results demonstrate that MGAE achieves faster convergence and
superior performance over traditional critic-based methods like Generalized
Advantage Estimation. SDM provides more accurate short-term state predictions
that enable the cost estimator to better predict potential violations. Overall,
CADE effectively integrates safety regulation into model-based RL, with the
Lagrangian approach achieving the soft balance of reward and safety during
training, while the safety layer enhances performance during inference by hard
action overlay.

</details>


### [302] [Masquerade: Learning from In-the-wild Human Videos using Data-Editing](https://arxiv.org/abs/2508.09976)
*Marion Lepert,Jiaying Fang,Jeannette Bohg*

Main category: cs.RO

TL;DR: Masquerade utilizes edited egocentric human videos to bridge the gap between human and robot manipulation, significantly improving robot policy learning.


<details>
  <summary>Details</summary>
Motivation: Robot manipulation often suffers from limited and less diverse datasets compared to other fields like language and vision, hindering advancements.

Method: The paper proposes Masquerade, a pipeline that edits human videos by estimating 3-D hand poses, removing human arms, and overlaying rendered robotic gestures to mimic demonstrations suitable for robot learning.

Result: Using 675K edited frames for pre-training and only 50 demonstrations per task for fine-tuning, Masquerade achieved superior generalization and outperformed baselines by 5-6x in kitchen tasks across unseen environments.

Conclusion: Masquerade effectively closes the visual embodiment gap by leveraging human videos for robot policy learning, highlighting its scalability and robustness for improving manipulation tasks.

Abstract: Robot manipulation research still suffers from significant data scarcity:
even the largest robot datasets are orders of magnitude smaller and less
diverse than those that fueled recent breakthroughs in language and vision. We
introduce Masquerade, a method that edits in-the-wild egocentric human videos
to bridge the visual embodiment gap between humans and robots and then learns a
robot policy with these edited videos. Our pipeline turns each human video into
robotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the
human arms, and (iii) overlaying a rendered bimanual robot that tracks the
recovered end-effector trajectories. Pre-training a visual encoder to predict
future 2-D robot keypoints on 675K frames of these edited clips, and continuing
that auxiliary loss while fine-tuning a diffusion policy head on only 50 robot
demonstrations per task, yields policies that generalize significantly better
than prior work. On three long-horizon, bimanual kitchen tasks evaluated in
three unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations
show that both the robot overlay and co-training are indispensable, and
performance scales logarithmically with the amount of edited human video. These
results demonstrate that explicitly closing the visual embodiment gap unlocks a
vast, readily available source of data from human videos that can be used to
improve robot policies.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [303] [Teaching Code Refactoring Using LLMs](https://arxiv.org/abs/2508.09332)
*Anshul Khairnar,Aarya Rajoju,Edward F. Gehringer*

Main category: cs.SE

TL;DR: This paper examines leveraging Large Language Models (LLMs) in software engineering courses to teach code refactoring with real-time feedback, showing positive impacts on student comprehension and code quality through structured prompts.


<details>
  <summary>Details</summary>
Motivation: Teaching code refactoring is challenging, especially using complex real-world code, with traditional methods providing limited and inconsistent feedback.

Method: Integrated LLM-assisted feedback using structured prompts in a software engineering course project, focusing on refactoring code smells. Evaluations included student feedback and planned analysis of code quality.

Result: LLM-assisted refactoring enhanced student understanding of maintainability and refactoring principles, with positive feedback on learning experiences.

Conclusion: LLMs are effective tools in bridging theoretical learning and practical application in refactoring, improving code quality and student comprehension.

Abstract: This Innovative Practice full paper explores how Large Language Models (LLMs)
can enhance the teaching of code refactoring in software engineering courses
through real-time, context-aware feedback. Refactoring improves code quality
but is difficult to teach, especially with complex, real-world codebases.
Traditional methods like code reviews and static analysis tools offer limited,
inconsistent feedback. Our approach integrates LLM-assisted refactoring into a
course project using structured prompts to help students identify and address
code smells such as long methods and low cohesion. Implemented in Spring 2025
in a long-lived OSS project, the intervention is evaluated through student
feedback and planned analysis of code quality improvements. Findings suggest
that LLMs can bridge theoretical and practical learning, supporting a deeper
understanding of maintainability and refactoring principles.

</details>


### [304] [Plug it and Play on Logs: A Configuration-Free Statistic-Based Log Parser](https://arxiv.org/abs/2508.09366)
*Qiaolin Qin,Xingfang Wu,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: PIPLUP is a new statistic-based log parser that enhances accuracy and generalizability, outperforming existing parsers, while being computationally efficient and privacy-preserving.


<details>
  <summary>Details</summary>
Motivation: Current statistic-based parsers lack accuracy and generalizability, prompting the design of a new approach to match the efficiency benefits while addressing these weaknesses.

Method: PIPLUP eliminates assumptions regarding token position and uses data-insensitive parameters to achieve accuracy and generalizability.

Result: PIPLUP achieves superior performance compared to state-of-the-art statistic-based parsers and performs competitively with an advanced unsupervised semantic-based parser (LUNAR).

Conclusion: PIPLUP is a simple, accurate, and efficient parser suitable for real-world applications due to its lower computational cost and privacy-preserving design.

Abstract: Log parsing is an essential task in log analysis, and many tools have been
designed to accomplish it. Existing log parsers can be categorized into
statistic-based and semantic-based approaches. In comparison to semantic-based
parsers, existing statistic-based parsers tend to be more efficient, require
lower computational costs, and be more privacy-preserving thanks to on-premise
deployment, but often fall short in their accuracy (e.g., grouping or parsing
accuracy) and generalizability. Therefore, it became a common belief that
statistic-based parsers cannot be as effective as semantic-based parsers since
the latter could take advantage of external knowledge supported by pretrained
language models. Our work, however, challenges this belief with a novel
statistic-based parser, PIPLUP. PIPLUP eliminates the pre-assumption of the
position of constant tokens for log grouping and relies on data-insensitive
parameters to overcome the generalizability challenge, allowing "plug and play"
on given log files. According to our experiments on an open-sourced large log
dataset, PIPLUP shows promising accuracy and generalizability with the
data-insensitive default parameter set. PIPLUP not only outperforms the
state-of-the-art statistic-based log parsers, Drain and its variants, but also
obtains a competitive performance compared to the best unsupervised
semantic-based log parser (i.e., LUNAR). Further, PIPLUP exhibits low time
consumption without GPU acceleration and external API usage; our simple,
efficient, and effective approach makes it more practical in real-world
adoptions, especially when costs and privacy are of major concerns.

</details>


### [305] [Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion](https://arxiv.org/abs/2508.09537)
*Yanzhou Li,Tianlin Li,Yiran Zhang,Shangqing Liu,Aishan Liu,Yang Liu*

Main category: cs.SE

TL;DR: This paper presents a three-stage framework to improve function completion by LLMs in real-world code repositories, especially when explicit annotations like docstrings are missing.


<details>
  <summary>Details</summary>
Motivation: Performance of LLMs drops significantly in function completion tasks when explicit annotations are unavailable, necessitating methods to infer intent from context.

Method: A novel three-stage framework is introduced: (1) intent inference from preceding context via reasoning-based prompting, (2) interactive refinement through developer input, and (3) target function generation based on finalized intent. A curated dataset of 40,000 examples supports this approach.

Result: Experiments on DevEval and ComplexCodeEval demonstrate over 20% relative gains in metrics, with additional improvements from interactive refinement.

Conclusion: The proposed method enhances the performance of LLMs for repository-scale function completion tasks, overcoming challenges posed by missing explicit annotations.

Abstract: Large Language Models (LLMs) are increasingly used for function completion in
repository-scale codebases. Prior studies demonstrate that when explicit
instructions--such as docstrings--are provided, these models can generate
highly accurate implementations. However, in real-world repositories, such
annotations are frequently absent, and performance drops substantially without
them. To address this gap, we frame the task as a three-stage process. The
first stage focuses on intent inference, where the model analyzes the code
preceding the target function to uncover cues about the desired functionality.
Such preceding context often encodes subtle but critical information, and we
design a reasoning-based prompting framework to guide the LLM through
step-by-step extraction and synthesis of these signals before any code is
generated. The second stage introduces an optional interactive refinement
mechanism to handle cases where preceding context alone is insufficient for
intent recovery. In this stage, the model proposes a small set of candidate
intentions, enabling the developer to select or edit them so that the inferred
intent closely matches the actual requirement. Finally, in the third stage, the
LLM generates the target function conditioned on the finalized intent. To
support this pipeline, we curate a dataset of 40,000 examples annotated with
intermediate reasoning traces and corresponding docstrings. Extensive
experiments on DevEval and ComplexCodeEval show that our approach consistently
boosts multiple LLMs, achieving over 20\% relative gains in both
reference-based and execution-based metrics, with the interactive refinement
stage delivering additional improvements beyond these gains.

</details>


### [306] [ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation](https://arxiv.org/abs/2508.09648)
*Taohong Zhu,Lucas C. Cordeiro,Youcheng Sun*

Main category: cs.SE

TL;DR: ReqInOne is an LLM-based agent for automated SRS creation, utilizing a modular architecture to enhance the accuracy and consistency of generated documents.


<details>
  <summary>Details</summary>
Motivation: SRS is essential but manually writing it results in inefficiencies and ambiguities. Existing automated methods fall short due to substantial manual involvement or issues with LLM-based approaches such as hallucinations and lack of control.

Method: ReqInOne uses a modular approach to generate SRS through three tasks—summarization, requirement extraction, and requirement classification—supported by tailored prompts.

Result: ReqInOne outperforms existing GPT-4-based methods and entry-level engineers in producing accurate, well-structured SRS documents. Additionally, its modular design aids its success, and its requirement classification is on par or superior to the best current models.

Conclusion: ReqInOne provides significant advancements in automated SRS generation, offering a more accurate and structured solution to circumvent challenges seen in older methods and LLM-based frameworks.

Abstract: Software Requirements Specification (SRS) is one of the most important
documents in software projects, but writing it manually is time-consuming and
often leads to ambiguity. Existing automated methods rely heavily on manual
analysis, while recent Large Language Model (LLM)-based approaches suffer from
hallucinations and limited controllability. In this paper, we propose ReqInOne,
an LLM-based agent that follows the common steps taken by human requirements
engineers when writing an SRS to convert natural language into a structured
SRS. ReqInOne adopts a modular architecture by decomposing SRS generation into
three tasks: summary, requirement extraction, and requirement classification,
each supported by tailored prompt templates to improve the quality and
consistency of LLM outputs.
  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the
generated SRSs against those produced by the holistic GPT-4-based method from
prior work as well as by entry-level requirements engineers. Expert evaluations
show that ReqInOne produces more accurate and well-structured SRS documents.
The performance advantage of ReqInOne benefits from its modular design, and
experimental results further demonstrate that its requirement classification
component achieves comparable or even better results than the state-of-the-art
requirement classification model.

</details>


### [307] [DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity](https://arxiv.org/abs/2508.09676)
*Vishal Khare,Vijay Saini,Deepak Sharma,Anand Kumar,Ankit Rana,Anshul Yadav*

Main category: cs.SE

TL;DR: This study evaluates DeputyDev, an AI-powered code review assistant, revealing significant improvements in code review efficiency and reduced review durations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in the code review process, such as prolonged durations, inconsistent feedback, and interruptions leading to delays in development timelines and reduced code quality.

Method: The study implemented DeputyDev's AI-driven contextual PR review capabilities and conducted a double-controlled A/B experiment with over 200 engineers to measure its impact on review duration.

Result: DeputyDev reduced PR review times by 23.09% and average per-line-of-code review durations by 40.13%. It is now rolled out within the organisation and available as a SaaS platform.

Conclusion: AI-assisted tools like DeputyDev can significantly enhance code review efficiency, thus improving overall development timelines and code quality. It demonstrates effective usage in industry and is scalable for external organizations.

Abstract: This study investigates the implementation and efficacy of DeputyDev, an
AI-powered code review assistant developed to address inefficiencies in the
software development process. The process of code review is highly inefficient
for several reasons, such as it being a time-consuming process, inconsistent
feedback, and review quality not being at par most of the time. Using our
telemetry data, we observed that at TATA 1mg, pull request (PR) processing
exhibits significant inefficiencies, with average pick-up and review times of
73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review
cycle was marked by prolonged iterative communication between the reviewing and
submitting parties. Research from the University of California, Irvine
indicates that interruptions can lead to an average of 23 minutes of lost
focus, critically affecting code quality and timely delivery. To address these
challenges, we developed DeputyDev's PR review capabilities by providing
automated, contextual code reviews. We conducted a rigorous double-controlled
A/B experiment involving over 200 engineers to evaluate DeputyDev's impact on
review times. The results demonstrated a statistically significant reduction in
both average per PR (23.09%) and average per-line-of-code (40.13%) review
durations. After implementing safeguards to exclude outliers, DeputyDev has
been effectively rolled out across the entire organisation. Additionally, it
has been made available to external companies as a Software-as-a-Service (SaaS)
solution, currently supporting the daily work of numerous engineering
professionals. This study explores the implementation and effectiveness of
AI-assisted code reviews in improving development workflow timelines and code.

</details>


### [308] [Inclusive Employment Pathways: Career Success Factors for Autistic Individuals in Software Engineering](https://arxiv.org/abs/2508.09680)
*Orvila Sarker,Mona Jamshaid,M. Ali Babar*

Main category: cs.SE

TL;DR: The study highlights barriers and opportunities for autistic individuals in Software Engineering (SE) roles, proposing evidence-based strategies for inclusive education, training, work environments, and tailored accommodations.


<details>
  <summary>Details</summary>
Motivation: Autistic individuals have unique strengths valuable to the ICT sector but face barriers in SE roles due to non-inclusive environments and practices.

Method: Conducted a Systematic Review of 30 studies, identifying 18 success factors across Software Engineering Education, Career and Employment Training, Work Environment, and Tools.

Result: The review identified evidence-based recommendations to enhance inclusion practices, with strategies for collaboration, structured environments, and tailored accommodations.

Conclusion: The paper provides actionable insights to promote the sustainable inclusion of autistic individuals in SE, benefitting employers, institutions, and tool developers.

Abstract: Research has highlighted the valuable contributions of autistic individuals
in the Information and Communication Technology (ICT) sector, particularly in
areas such as software development, testing, and cybersecurity. Their strengths
in information processing, attention to detail, innovative thinking, and
commitment to high-quality outcomes in the ICT domain are well-documented.
However, despite their potential, autistic individuals often face barriers in
Software Engineering (SE) roles due to a lack of personalised tools, complex
work environments, non-inclusive recruitment practices, limited co-worker
support, challenging social dynamics and so on. Motivated by the ethical
framework of the neurodiversity movement and the success of pioneering
initiatives like the Dandelion program, corporate Diversity, Equity, and
Inclusion (DEI) in the ICT sector has increasingly focused on autistic talent.
This movement fundamentally reframes challenges not as individual deficits but
as failures of environments designed for a neurotypical majority. Despite this
progress, there is no synthesis of knowledge reporting the full pathway from
software engineering education through to sustainable workplace inclusion. To
address this, we conducted a Systematic Review of 30 studies and identified 18
success factors grouped into four thematic categories: (1) Software Engineering
Education, (2) Career and Employment Training, (3) Work Environment, and (4)
Tools and Assistive Technologies. Our findings offer evidence-based
recommendations for educational institutions, employers, organisations, and
tool developers to enhance the inclusion of autistic individuals in SE. These
include strategies for inclusive meeting and collaboration practices,
accessible and structured work environments, clear role and responsibility
definitions, and the provision of tailored workplace accommodations.

</details>


### [309] [LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations](https://arxiv.org/abs/2508.09791)
*Junxiao Han,Yarong Wang,Xiaodong Gu,Cuiyun Gao,Yao Wan,Song Han,David Lo,Shuiguang Deng*

Main category: cs.SE

TL;DR: This paper introduces LibRec, a framework combining LLMs with RAG for recommending library migrations, and evaluates it using LibEval, a benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Motivated by the challenge of automating library migration recommendations, this paper seeks to leverage LLMs and RAG techniques to address this issue effectively.

Method: The framework uses retrieval-augmented generation and in-context learning to extract migration intents from commit messages. It integrates evaluation via a benchmark dataset called LibEval.

Result: LibEval benchmark was designed with 2,888 migration records and used to evaluate ten popular LLMs within the framework, exploring prompt strategies, intent types, and assessing framework effectiveness.

Conclusion: LibRec demonstrates potential in automating library migration recommendations with impactful components and strategies based on detailed evaluation results from the LibEval benchmark.

Abstract: In this paper, we propose LibRec, a novel framework that integrates the
capabilities of LLMs with retrieval-augmented generation(RAG) techniques to
automate the recommendation of alternative libraries. The framework further
employs in-context learning to extract migration intents from commit messages
to enhance the accuracy of its recommendations. To evaluate the effectiveness
of LibRec, we introduce LibEval, a benchmark designed to assess the performance
in the library migration recommendation task. LibEval comprises 2,888 migration
records associated with 2,368 libraries extracted from 2,324 Python
repositories. Each migration record captures source-target library pairs, along
with their corresponding migration intents and intent types. Based on LibEval,
we evaluated the effectiveness of ten popular LLMs within our framework,
conducted an ablation study to examine the contributions of key components
within our framework, explored the impact of various prompt strategies on the
framework's performance, assessed its effectiveness across various intent
types, and performed detailed failure case analyses.

</details>


### [310] [Fast and Accurate Heuristics for Bus-Factor Estimation](https://arxiv.org/abs/2508.09828)
*Sebastiano Antonio Piccolo*

Main category: cs.SE

TL;DR: The paper introduces two scalable, accurate heuristics for estimating the bus-factor in software projects modeled as bipartite graphs, addressing the impracticality of NP-Hard computation methods.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the challenge of accurately estimating the bus-factor in large software systems, as current methods are either computationally infeasible or yield inaccurate results.

Method: The paper models software projects as developer-task bipartite graphs and proposes two iterative graph-peeling heuristics: Minimum Coverage and Maximum Coverage, as alternatives to the common degree-based heuristic.

Result: The proposed heuristics provide tighter estimates of the bus-factor and are computationally efficient, scaling to large graphs with millions of nodes and edges within minutes, as demonstrated through evaluations on synthetic power-law graphs.

Conclusion: The proposed methods outperform existing approaches in accuracy, scalability, and robustness, making them valuable for practical and research applications. The authors contribute open-source implementations to the community.

Abstract: The bus-factor is a critical risk indicator that quantifies how many key
contributors a project can afford to lose before core knowledge or
functionality is compromised. Despite its practical importance, accurately
computing the bus-factor is NP-Hard under established formalizations, making
scalable analysis infeasible for large software systems.
  In this paper, we model software projects as bipartite graphs of developers
and tasks and propose two novel approximation heuristics, Minimum Coverage and
Maximum Coverage, based on iterative graph peeling, for two influential
bus-factor formalizations. Our methods significantly outperform the widely
adopted degree-based heuristic, which we show can yield severely inflated
estimates.
  We conduct a comprehensive empirical evaluation on over $1\,000$ synthetic
power-law graphs and demonstrate that our heuristics provide tighter estimates
while scaling to graphs with millions of nodes and edges in minutes. Our
results reveal that the proposed heuristics are not only more accurate but also
robust to structural variations in developer-task assignment graph. We release
our implementation as open-source software to support future research and
practical adoption.

</details>


### [311] [Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification](https://arxiv.org/abs/2508.09832)
*Linh Nguyen,Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: The paper investigates the capability of Large Language Models (LLMs) to classify code review comments into 17 categories, outperforming current methods reliant on deep learning and limited training data.


<details>
  <summary>Details</summary>
Motivation: Identify limitations in existing methods of classifying code review comments, particularly concerning reliance on supervised learning and annotated training data.

Method: This study evaluates the classification performance of LLMs across 17 comment categories, comparing the results with a state-of-the-art deep learning model.

Result: LLMs surpass the deep learning models by achieving better accuracy, particularly for underrepresented categories and providing balanced performance across all categories.

Conclusion: LLMs can serve as a scalable and effective solution for analyzing code review comments, potentially improving the overall code review process.

Abstract: Code review is a crucial practice in software development. As code review
nowadays is lightweight, various issues can be identified, and sometimes, they
can be trivial. Research has investigated automated approaches to classify
review comments to gauge the effectiveness of code reviews. However, previous
studies have primarily relied on supervised machine learning, which requires
extensive manual annotation to train the models effectively. To address this
limitation, we explore the potential of using Large Language Models (LLMs) to
classify code review comments. We assess the performance of LLMs to classify 17
categories of code review comments. Our results show that LLMs can classify
code review comments, outperforming the state-of-the-art approach using a
trained deep learning model. In particular, LLMs achieve better accuracy in
classifying the five most useful categories, which the state-of-the-art
approach struggles with due to low training examples. Rather than relying
solely on a specific small training data distribution, our results show that
LLMs provide balanced performance across high- and low-frequency categories.
These results suggest that the LLMs could offer a scalable solution for code
review analytics to improve the effectiveness of the code review process.

</details>


### [312] [An Empirical Study of CGO Usage in Go Projects -- Distribution, Purposes, Patterns and Critical Issues](https://arxiv.org/abs/2508.09875)
*Jinbao Chen,Boyao Ding,Yu Zhang,Qingwei Li,Fugen Tang*

Main category: cs.SE

TL;DR: Study examines CGO usage in Go projects, revealing patterns, risks, and solutions for improving Go's robustness.


<details>
  <summary>Details</summary>
Motivation: Address the lack of research on CGO, an FFI in Go, aiming to understand its risks, usage, and efficiency.

Method: Analyzed 920 open-source Go projects using a custom tool, CGOAnalyzer, to identify features, patterns, and issues.

Result: Identified critical issues, usage patterns, and proposed both temporary and permanent solutions to improve Go's toolchain reliability.

Conclusion: Insights help developers and Go maintainers enhance software efficiency, stability, and optimize CGO usage.

Abstract: Multilingual software development integrates multiple languages into a single
application, with the Foreign Function Interface (FFI) enabling seamless
interaction. While FFI boosts efficiency and extensibility, it also introduces
risks. Existing studies focus on FFIs in languages like Python and Java,
neglecting CGO, the emerging FFI in Go, which poses unique risks.
  To address these concerns, we conduct an empirical study of CGO usage across
920 open-source Go projects. Our study aims to reveal the distribution,
patterns, purposes, and critical issues associated with CGO, offering insights
for developers and the Go team. We develop CGOAnalyzer, a tool to efficiently
identify and quantify CGO-related features. Our findings reveal that: (1) 11.3%
of analyzed Go projects utilize CGO, with usage concentrated in a subset of
projects; (2) CGO serves 4 primary purposes, including system-level
interactions and performance optimizations, with 15 distinct usage patterns
observed; (3) 19 types of CGO-related issues exist, including one critical
issue involving unnecessary pointer checks that pose risks of runtime crashes
due to limitations in the current Go compilation toolchain; (4) a temporary
solution reduces unnecessary pointer checks, mitigating crash risks, and (5) we
submitted a proposal to improve the Go toolchain for a permanent fix, which has
been grouped within an accepted proposal for future resolution. Our findings
provide valuable insights for developers and the Go team, enhancing development
efficiency and reliability while improving the robustness of the Go toolchain.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [313] [Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions](https://arxiv.org/abs/2508.09852)
*Baihan Lin*

Main category: q-bio.NC

TL;DR: The paper proposes the Perceptual Reality Transformer framework, using neural architectures to simulate eight neurological perception conditions by transforming natural images for others' understanding.


<details>
  <summary>Details</summary>
Motivation: To address experiential gaps between individuals with neurological conditions affecting perception and those around them, such as caregivers or medical professionals.

Method: The study designs neural networks, particularly Vision Transformer architectures, to map natural images to condition-specific perceptual simulations of eight neurological disorders, leveraging clinical literature for condition-specific transforms.

Result: Vision Transformers outperform traditional convolutional networks (CNN) and other generative approaches in simulating neurological perception across datasets such as ImageNet and CIFAR-10.

Conclusion: The framework enables applications in medical education, empathy training, and assistive technology, while establishing a benchmark for neurological perception simulation and advancing understanding of neural modeling of atypical perception.

Abstract: Neurological conditions affecting visual perception create profound
experiential divides between affected individuals and their caregivers,
families, and medical professionals. We present the Perceptual Reality
Transformer, a comprehensive framework employing six distinct neural
architectures to simulate eight neurological perception conditions with
scientifically-grounded visual transformations. Our system learns mappings from
natural images to condition-specific perceptual states, enabling others to
experience approximations of simultanagnosia, prosopagnosia, ADHD attention
deficits, visual agnosia, depression-related changes, anxiety tunnel vision,
and Alzheimer's memory effects. Through systematic evaluation across ImageNet
and CIFAR-10 datasets, we demonstrate that Vision Transformer architectures
achieve optimal performance, outperforming traditional CNN and generative
approaches. Our work establishes the first systematic benchmark for
neurological perception simulation, contributes novel condition-specific
perturbation functions grounded in clinical literature, and provides
quantitative metrics for evaluating simulation fidelity. The framework has
immediate applications in medical education, empathy training, and assistive
technology development, while advancing our fundamental understanding of how
neural networks can model atypical human perception.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [314] [Distributional Sensitivity Analysis: Enabling Differentiability in Sample-Based Inference](https://arxiv.org/abs/2508.09347)
*Pi-Yueh Chuang,Ahmed Attia,Emil Constantinescu*

Main category: stat.ML

TL;DR: The paper introduces two formulas and numerical algorithms for calculating the sensitivity (Jacobian or gradient) of random vectors with respect to distributional parameters and applies these methods in nuclear physics and deep learning frameworks.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current sensitivity estimation methods that require model fitting, sampling knowledge, and computationally expensive integrals.

Method: Present two analytical formulas for sensitivity calculation, introduce diagonal approximations for reduced costs, and develop four numerical algorithms for cases without closed forms.

Result: Verification, validation studies showcase correctness and effectiveness; a nuclear physics application demonstrates practical utility in uncertainty quantification and parameter inference.

Conclusion: The approach avoids complex model fitting and sampling requirements, and enables differentiable sampling methods compatible with deep learning frameworks, supported by open-source implementation in DistroSA.

Abstract: We present two analytical formulae for estimating the sensitivity -- namely,
the gradient or Jacobian -- at given realizations of an arbitrary-dimensional
random vector with respect to its distributional parameters. The first formula
interprets this sensitivity as partial derivatives of the inverse mapping
associated with the vector of 1-D conditional distributions. The second
formula, intended for optimization methods that tolerate inexact gradients,
introduces a diagonal approximation that reduces computational cost at the cost
of some accuracy. We additionally provide four second-order numerical
algorithms to approximate both formulae when closed forms are unavailable. We
performed verification and validation studies to demonstrate the correctness of
these numerical algorithms and the effectiveness of the proposed formulae. A
nuclear physics application showcases how our work enables uncertainty
quantification and parameter inference for quantum correlation functions. Our
approach differs from existing methods by avoiding the need for model fitting,
knowledge of sampling algorithms, and evaluation of high-dimensional integrals.
It is therefore particularly useful for sample-based inverse problems when the
sampler operates as a black box or requires expensive physics simulations.
Moreover, our method renders arbitrary sampling subroutines differentiable,
facilitating their integration into programming frameworks for deep learning
and automatic differentiation. Algorithmic details and code implementations are
provided in this paper and in our open-source software DistroSA to enable
reproducibility and further development.

</details>


### [315] [A pseudo-inverse of a line graph](https://arxiv.org/abs/2508.09412)
*Sevvandi Kandanaarachchi,Philip Kilby,Cheng Soon Ong*

Main category: stat.ML

TL;DR: The paper explores how to recover original root graphs from perturbed line graphs using a linear integer program.


<details>
  <summary>Details</summary>
Motivation: Understanding the inverse operation of line graphs is challenging because not all line graphs have a corresponding root graph.

Method: The researchers developed a linear integer programming model to edit minimal edges in a line graph for finding its corresponding root graph. They also used the spectral norm to validate their approach theoretically.

Result: Empirical experiments on Erdős-Rényi graphs demonstrate the effectiveness of the inverse operation, confirming its practical usability and theoretical underpinnings.

Conclusion: Editing perturbed line graphs can successfully restore the structure of root graphs, providing a solid framework for minimizing changes in graph operations.

Abstract: Line graphs are an alternative representation of graphs where each vertex of
the original (root) graph becomes an edge. However not all graphs have a
corresponding root graph, hence the transformation from graphs to line graphs
is not invertible. We investigate the case when there is a small perturbation
in the space of line graphs, and try to recover the corresponding root graph,
essentially defining the inverse of the line graph operation. We propose a
linear integer program that edits the smallest number of edges in the line
graph, that allow a root graph to be found. We use the spectral norm to
theoretically prove that such a pseudo-inverse operation is well behaved.
Illustrative empirical experiments on Erd\H{o}s-R\'enyi graphs show that our
theoretical results work in practice.

</details>


### [316] [Scalable h-adaptive probabilistic solver for time-independent and time-dependent systems](https://arxiv.org/abs/2508.09623)
*Akshay Thakur,Sawan Kumar,Matthew Zahr,Souvik Chakraborty*

Main category: stat.ML

TL;DR: This paper introduces scalable innovations to probabilistic approaches for solving PDEs, improving computational efficiency and adaptiveness.


<details>
  <summary>Details</summary>
Motivation: Quantifying epistemic uncertainty from discretization when solving PDEs is a challenge, especially for large-scale or high-dimensional problems.

Method: The authors propose a stochastic dual descent algorithm to reduce computational complexity and an adaptive learning strategy leveraging clustering to select collocation points efficiently.

Result: The enhanced probabilistic solver scales linearly with collocation points and adapts efficiently, demonstrated on benchmark PDE problems, including elliptic and parabolic cases.

Conclusion: The innovations enable a scalable mesh-free PDE solver that delivers efficient and adaptive solutions while addressing computational bottlenecks.

Abstract: Solving partial differential equations (PDEs) within the framework of
probabilistic numerics offers a principled approach to quantifying epistemic
uncertainty arising from discretization. By leveraging Gaussian process
regression and imposing the governing PDE as a constraint at a finite set of
collocation points, probabilistic numerics delivers mesh-free solutions at
arbitrary locations. However, the high computational cost, which scales
cubically with the number of collocation points, remains a critical bottleneck,
particularly for large-scale or high-dimensional problems. We propose a
scalable enhancement to this paradigm through two key innovations. First, we
develop a stochastic dual descent algorithm that reduces the per-iteration
complexity from cubic to linear in the number of collocation points, enabling
tractable inference. Second, we exploit a clustering-based active learning
strategy that adaptively selects collocation points to maximize information
gain while minimizing computational expense. Together, these contributions
result in an $h$-adaptive probabilistic solver that can scale to a large number
of collocation points. We demonstrate the efficacy of the proposed solver on
benchmark PDEs, including two- and three-dimensional steady-state elliptic
problems, as well as a time-dependent parabolic PDE formulated in a space-time
setting.

</details>


### [317] [Structured Kernel Regression VAE: A Computationally Efficient Surrogate for GP-VAEs in ICA](https://arxiv.org/abs/2508.09721)
*Yuan-Hao Wei,Fu-Hao Deng,Lin-Yong Cui,Yan-Jie Sun*

Main category: stat.ML

TL;DR: This paper introduces the Structured Kernel Regression VAE (SKR-VAE), which improves interpretability of generative models by achieving disentanglement in latent variable space efficiently. Unlike Gaussian Process-based VAEs, it avoids high computational costs while maintaining Independent Component Analysis (ICA) performance.


<details>
  <summary>Details</summary>
Motivation: Generative models' interpretability is crucial for assessing their effectiveness and controllability, which involves disentangling latent variables that impact generated outputs.

Method: The paper proposes SKR-VAE, which utilizes kernel functions to structure and disentangle latent variables without the high computational cost of Gaussian Process (GP)-based methods.

Result: SKR-VAE achieves equivalent ICA performance while being significantly more computationally efficient compared to GP-VAEs.

Conclusion: SKR-VAE provides an effective and efficient approach to disentangling latent variables in VAEs, reducing computational burden without compromising interpretability.

Abstract: The interpretability of generative models is considered a key factor in
demonstrating their effectiveness and controllability. The generated data are
believed to be determined by latent variables that are not directly observable.
Therefore, disentangling, decoupling, decomposing, causal inference, or
performing Independent Component Analysis (ICA) in the latent variable space
helps uncover the independent factors that influence the attributes or features
affecting the generated outputs, thereby enhancing the interpretability of
generative models. As a generative model, Variational Autoencoders (VAEs)
combine with variational Bayesian inference algorithms. Using VAEs, the inverse
process of ICA can be equivalently framed as a variational inference process.
In some studies, Gaussian processes (GPs) have been introduced as priors for
each dimension of latent variables in VAEs, structuring and separating each
dimension from temporal or spatial perspectives, and encouraging different
dimensions to control various attributes of the generated data. However, GPs
impose a significant computational burden, resulting in substantial resource
consumption when handling large datasets. Essentially, GPs model different
temporal or spatial structures through various kernel functions. Structuring
the priors of latent variables via kernel functions-so that different kernel
functions model the correlations among sequence points within different latent
dimensions-is at the core of achieving disentanglement in VAEs. The proposed
Structured Kernel Regression VAE (SKR-VAE) leverages this core idea in a more
efficient way, avoiding the costly kernel matrix inversion required in GPs.
This research demonstrates that, while maintaining ICA performance, SKR-VAE
achieves greater computational efficiency and significantly reduced
computational burden compared to GP-VAE.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [318] [Counting Short Trajectories in Elementary Cellular Automata using the Transfer Matrix Method](https://arxiv.org/abs/2508.09768)
*Cédric Koller,Barbora Hudcová*

Main category: nlin.CG

TL;DR: This study adaptively employs the Transfer Matrix Method (TMM) to compute entropy and trajectory statistics of Elementary Cellular Automata (ECA), establishing a quantitative basis related to Wolfram's classification.


<details>
  <summary>Details</summary>
Motivation: Understanding the global dynamics and quantitative behaviors of Elementary Cellular Automata (ECAs) and connecting these with Wolfram's qualitative classification.

Method: Adapting the Transfer Matrix Method (TMM) to compute exact entropy-related statistics for initial configurations converging to attractors after $p$ steps, within the thermodynamic limit.

Result: Quantitative entropy differences among ECAs were identified: Class 1 and 2 quickly reach maximal entropy, Class 3 shows low/zero entropy saturation, and Class 4 maintains finite positive entropy.

Conclusion: This framework precisely quantifies ECAs' trajectory statistics, suggesting broad applicability despite limitations due to exponential computational costs for long trajectories.

Abstract: Elementary Cellular Automata (ECAs) exhibit diverse behaviours often
categorized by Wolfram's qualitative classification. To provide a quantitative
basis for understanding these behaviours, we investigate the global dynamics of
such automata and we describe a method that allows us to compute the number of
all configurations leading to short attractors in a limited number of time
steps. This computation yields exact results in the thermodynamic limit (as the
CA grid size grows to infinity), and is based on the Transfer Matrix Method
(TMM) that we adapt for our purposes. Specifically, given two parameters $(p,
c)$ we are able to compute the entropy of all initial configurations converging
to an attractor of size $c$ after $p$ time-steps. By calculating such
statistics for various ECA rules, we establish a quantitative connection
between the entropy and the qualitative Wolfram classification scheme. Class 1
rules rapidly converge to maximal entropy for stationary states ($c=1$) as $p$
increases. Class 2 rules also approach maximal entropy quickly for appropriate
cycle lengths $c$, potentially requiring consideration of translations. Class 3
rules exhibit zero or low finite entropy that saturates after a short
transient. Class 4 rules show finite positive entropy, similar to some Class 3
rules. This method provides a precise framework for quantifying trajectory
statistics, although its exponential computational cost in $p+c$ restricts
practical analysis to short trajectories.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [319] [Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation](https://arxiv.org/abs/2508.09177)
*Xuanru Zhou,Cheng Li,Shuqiang Wang,Ye Li,Tao Tan,Hairong Zheng,Shanshan Wang*

Main category: eess.IV

TL;DR: This paper reviews the transformative role of generative AI in medical imaging and outlines its advancements, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Advances in generative AI are increasingly addressing challenges in medical imaging, including data limitations, standardization, and integration across modalities.

Method: The paper systematically reviews recent generative AI models like GANs, VAEs, and diffusion models, explores their clinical applications, and proposes a three-tiered evaluation framework.

Result: It presents a comprehensive synthesis of how generative models impact imaging stages and highlights obstacles such as generalization, hallucination risk, and regulatory concerns.

Conclusion: The paper underscores the potential of generative AI and foundation models to revolutionize medical imaging, advocating for interdisciplinary collaboration to address deployment challenges.

Abstract: Generative artificial intelligence (AI) is rapidly transforming medical
imaging by enabling capabilities such as data synthesis, image enhancement,
modality translation, and spatiotemporal modeling. This review presents a
comprehensive and forward-looking synthesis of recent advances in generative
modeling including generative adversarial networks (GANs), variational
autoencoders (VAEs), diffusion models, and emerging multimodal foundation
architectures and evaluates their expanding roles across the clinical imaging
continuum. We systematically examine how generative AI contributes to key
stages of the imaging workflow, from acquisition and reconstruction to
cross-modality synthesis, diagnostic support, and treatment planning. Emphasis
is placed on both retrospective and prospective clinical scenarios, where
generative models help address longstanding challenges such as data scarcity,
standardization, and integration across modalities. To promote rigorous
benchmarking and translational readiness, we propose a three-tiered evaluation
framework encompassing pixel-level fidelity, feature-level realism, and
task-level clinical relevance. We also identify critical obstacles to
real-world deployment, including generalization under domain shift,
hallucination risk, data privacy concerns, and regulatory hurdles. Finally, we
explore the convergence of generative AI with large-scale foundation models,
highlighting how this synergy may enable the next generation of scalable,
reliable, and clinically integrated imaging systems. By charting technical
progress and translational pathways, this review aims to guide future research
and foster interdisciplinary collaboration at the intersection of AI, medicine,
and biomedical engineering.

</details>


### [320] [Hybrid(Transformer+CNN)-based Polyp Segmentation](https://arxiv.org/abs/2508.09189)
*Madan Baduwal*

Main category: eess.IV

TL;DR: The paper introduces a hybrid Transformer-CNN model for polyp segmentation, achieving notable improvements in segmentation accuracy and artifact resilience.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the challenging problem of polyp segmentation caused by variations in their size, shape, imaging conditions, and ill-defined boundaries, which current methods struggle to address.

Method: The paper proposes a hybrid architecture combining Transformer and CNN models. Key features include boundary-aware attention mechanisms for better handling ill-defined margins and enhanced feature extraction capabilities to address endoscopic artifacts.

Result: The proposed model demonstrates improved recall (1.76% or 0.9555) and accuracy (0.07% or 0.9849) in segmentation, showcasing superior performance over existing methods in handling polyp characteristics and artifacts.

Conclusion: The hybrid approach outperforms state-of-the-art methods, effectively addressing critical challenges in polyp segmentation such as ill-defined boundaries and artifacts in endoscopic imaging.

Abstract: Colonoscopy is still the main method of detection and segmentation of colonic
polyps, and recent advancements in deep learning networks such as U-Net,
ResUNet, Swin-UNet, and PraNet have made outstanding performance in polyp
segmentation. Yet, the problem is extremely challenging due to high variation
in size, shape, endoscopy types, lighting, imaging protocols, and ill-defined
boundaries (fluid, folds) of the polyps, rendering accurate segmentation a
challenging and problematic task. To address these critical challenges in polyp
segmentation, we introduce a hybrid (Transformer + CNN) model that is crafted
to enhance robustness against evolving polyp characteristics. Our hybrid
architecture demonstrates superior performance over existing solutions,
particularly in addressing two critical challenges: (1) accurate segmentation
of polyps with ill-defined margins through boundary-aware attention mechanisms,
and (2) robust feature extraction in the presence of common endoscopic
artifacts, including specular highlights, motion blur, and fluid occlusions.
Quantitative evaluations reveal significant improvements in segmentation
accuracy (Recall improved by 1.76%, i.e., 0.9555, accuracy improved by 0.07%,
i.e., 0.9849) and artifact resilience compared to state-of-the-art polyp
segmentation methods.

</details>


### [321] [impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction](https://arxiv.org/abs/2508.09195)
*Maria Boyko,Aleksandra Beliaeva,Dmitriy Kornilov,Alexander Bernstein,Maxim Sharaev*

Main category: eess.IV

TL;DR: The paper introduces impuTMAE, a transformer-based model that addresses missing modalities in medical data and achieves state-of-the-art results in glioma survival prediction by leveraging five data modalities.


<details>
  <summary>Details</summary>
Motivation: The aim is to utilize diverse data modalities (e.g., omics, images, clinical data) to improve disease prognosis while addressing the challenges of incomplete and missing data in multimodal medical datasets.

Method: The paper proposes impuTMAE, a transformer-based model that incorporates multimodal pre-training strategies to learn both inter- and intra-modal interactions while imputing missing modalities by reconstructing masked patches. The model was pre-trained on heterogeneous, incomplete datasets and fine-tuned for glioma survival prediction.

Result: impuTMAE outperforms previous multimodal approaches and establishes a new state-of-the-art in glioma survival prediction using five data modalities.

Conclusion: impuTMAE effectively handles missing modality issues during training, enhances resource utilization, and demonstrates superior performance in predicting glioma patient survival.

Abstract: The use of diverse modalities, such as omics, medical images, and clinical
data can not only improve the performance of prognostic models but also deepen
an understanding of disease mechanisms and facilitate the development of novel
treatment approaches. However, medical data are complex, often incomplete, and
contains missing modalities, making effective handling its crucial for training
multimodal models. We introduce impuTMAE, a novel transformer-based end-to-end
approach with an efficient multimodal pre-training strategy. It learns inter-
and intra-modal interactions while simultaneously imputing missing modalities
by reconstructing masked patches. Our model is pre-trained on heterogeneous,
incomplete data and fine-tuned for glioma survival prediction using
TCGA-GBM/LGG and BraTS datasets, integrating five modalities: genetic (DNAm,
RNA-seq), imaging (MRI, WSI), and clinical data. By addressing missing data
during pre-training and enabling efficient resource utilization, impuTMAE
surpasses prior multimodal approaches, achieving state-of-the-art performance
in glioma patient survival prediction. Our code is available at
https://github.com/maryjis/mtcp

</details>


### [322] [FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation](https://arxiv.org/abs/2508.09196)
*Asim Ukaye,Numan Saeed,Karthik Nandakumar*

Main category: eess.IV

TL;DR: The paper introduces a federated learning method for universal abdominal CT segmentation using uncertainties in model training and inference.


<details>
  <summary>Details</summary>
Motivation: Segmentation datasets come from varied scanners, capture settings, and often cover limited or disjoint organ labels, posing challenges for heterogeneity handling and privacy preservation.

Method: The method estimates model parameter uncertainty during gradient descent at the client level and uses a Bayesian-inspired inverse-variance scheme for server aggregation. Predictive uncertainty is further propagated to enhance inference.

Result: The approach improves federated aggregation quality and prediction accuracy, outperforming previous baselines in experimental evaluations.

Conclusion: Using model uncertainty for aggregation and predictive uncertainty for inference enhances segmentation performance across diverse datasets. The implementation code is publicly available for further use.

Abstract: Different CT segmentation datasets are typically obtained from different
scanners under different capture settings and often provide segmentation labels
for a limited and often disjoint set of organs. Using these heterogeneous data
effectively while preserving patient privacy can be challenging. This work
presents a novel federated learning approach to achieve universal segmentation
across diverse abdominal CT datasets by utilizing model uncertainty for
aggregation and predictive uncertainty for inference. Our approach leverages
the inherent noise in stochastic mini-batch gradient descent to estimate a
distribution over the model weights to provide an on-the-go uncertainty over
the model parameters at the client level. The parameters are then aggregated at
the server using the additional uncertainty information using a
Bayesian-inspired inverse-variance aggregation scheme. Furthermore, the
proposed method quantifies prediction uncertainty by propagating the
uncertainty from the model weights, providing confidence measures essential for
clinical decision-making. In line with recent work shown, predictive
uncertainty is utilized in the inference stage to improve predictive
performance. Experimental evaluations demonstrate the effectiveness of this
approach in improving both the quality of federated aggregation and
uncertainty-weighted inference compared to previously established baselines.
The code for this work is made available at: https://github.com/asimukaye/fiva

</details>


### [323] [Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction](https://arxiv.org/abs/2508.09200)
*Jinho Kim,Marcel Dominik Nickel,Florian Knoll*

Main category: eess.IV

TL;DR: This paper explores zero-shot self-supervised learning to significantly reduce breath-hold durations in MRCP scans while achieving high-quality reconstructions.


<details>
  <summary>Details</summary>
Motivation: Long breath-hold durations required for MRCP scans pose challenges for patient comfort and clinical efficiency.

Method: Zero-shot self-supervised learning was employed for MRCP reconstruction using incoherent k-space sampling; shallow training was explored to minimize computation times.

Result: Zero-shot learning improved image quality significantly compared to compressed sensing, achieving performance comparable to longer respiratory-triggered acquisitions. Shallow training reduced training time drastically.

Conclusion: Zero-shot learning can enhance MRCP reconstructions while reducing breath-hold durations, and shallow training makes the process suitable for clinical settings.

Abstract: Purpose: To investigate the feasibility of applying zero-shot self-supervised
learning reconstruction to reduce breath-hold times in magnetic resonance
cholangiopancreatography (MRCP). Methods: Breath-hold MRCP was acquired from 11
healthy volunteers on a 3T scanner using an incoherent k-space sampling pattern
leading to a breath-hold duration of 14s. We evaluated zero-shot reconstruction
of breath-hold MRCP against parallel imaging of respiratory-triggered MRCP
acquired in 338s on average and compressed sensing reconstruction of
breath-hold MRCP. To address the long computation times of zero-shot trainings,
we used a training approach that leverages a pretrained network to reduce
backpropagation depth during training. Results: Zero-shot learning
reconstruction significantly improved visual image quality compared to
compressed sensing reconstruction, particularly in terms of signal-to-noise
ratio and ductal delineation, and reached a level of quality comparable to that
of successful respiratory-triggered acquisitions with regular breathing
patterns. Shallow training provided nearly equivalent reconstruction
performance with a training time of 11 minutes in comparison to 271 minutes for
a conventional zero-shot training. Conclusion: Zero-shot learning delivers
high-fidelity MRCP reconstructions with reduced breath-hold times, and shallow
training offers a practical solution for translation to time-constrained
clinical workflows.

</details>


### [324] [From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations](https://arxiv.org/abs/2508.09205)
*Yoni Schirris,Eric Marcus,Jonas Teuwen,Hugo Horlings,Efstratios Gavves*

Main category: eess.IV

TL;DR: The paper introduces a human-machine interaction system for explaining deep learning classifiers in computational pathology, leveraging multi-instance learning and vision-language models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current explainable AI (e.g., GradCAM) in medical image analysis, which lacks comprehensive explanatory capability for deep learning models.

Method: The authors developed an AI-integrated slide viewer for sliding-window experiments and utilized vision-language models to quantify explanation predictiveness for pathology classifiers.

Result: The system qualitatively tested claims of explanations and quantitatively distinguished competing explanations, demonstrating utility in validating AI models in digital pathology.

Conclusion: The study bridges the gap from explainable AI to explained AI, offering a practical approach for integrating deep learning explanations in computational pathology with available tools.

Abstract: Explaining deep learning models is essential for clinical integration of
medical image analysis systems. A good explanation highlights if a model
depends on spurious features that undermines generalization and harms a subset
of patients or, conversely, may present novel biological insights. Although
techniques like GradCAM can identify influential features, they are measurement
tools that do not themselves form an explanation. We propose a
human-machine-VLM interaction system tailored to explaining classifiers in
computational pathology, including multi-instance learning for whole-slide
images. Our proof of concept comprises (1) an AI-integrated slide viewer to run
sliding-window experiments to test claims of an explanation, and (2)
quantification of an explanation's predictiveness using general-purpose
vision-language models. The results demonstrate that this allows us to
qualitatively test claims of explanations and can quantifiably distinguish
competing explanations. This offers a practical path from explainable AI to
explained AI in digital pathology and beyond. Code and prompts are available at
https://github.com/nki-ai/x2x.

</details>


### [325] [AMRG: Extend Vision Language Models for Automatic Mammography Report Generation](https://arxiv.org/abs/2508.09225)
*Nak-Jun Sung,Donghyun Lee,Bo Hwa Choi,Chae Jung Park*

Main category: eess.IV

TL;DR: This paper introduces AMRG, the first end-to-end framework for automatic mammography report generation using large vision-language models, trained on a public dataset.


<details>
  <summary>Details</summary>
Motivation: Mammography report generation is important but underexplored, with challenges in reasoning across medical images and unstructured radiologic language.

Method: AMRG utilizes a vision-language model fine-tuned with Low-Rank Adaptation (LoRA) on the DMID dataset of mammograms and diagnostic reports.

Result: The framework achieved strong language generation and clinical metrics, including ROUGE-L (0.5691), METEOR (0.6152), CIDEr (0.5818), and BI-RADS accuracy (0.5582), with improved diagnostic consistency.

Conclusion: AMRG establishes a scalable methodology for mammography report generation, creating a benchmark for multimodal clinical AI and reducing diagnostic inconsistencies.

Abstract: Mammography report generation is a critical yet underexplored task in medical
AI, characterized by challenges such as multiview image reasoning,
high-resolution visual cues, and unstructured radiologic language. In this
work, we introduce AMRG (Automatic Mammography Report Generation), the first
end-to-end framework for generating narrative mammography reports using large
vision-language models (VLMs). Building upon MedGemma-4B-it-a
domain-specialized, instruction-tuned VLM-we employ a parameter-efficient
fine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling
lightweight adaptation with minimal computational overhead. We train and
evaluate AMRG on DMID, a publicly available dataset of paired high-resolution
mammograms and diagnostic reports. This work establishes the first reproducible
benchmark for mammography report generation, addressing a longstanding gap in
multimodal clinical AI. We systematically explore LoRA hyperparameter
configurations and conduct comparative experiments across multiple VLM
backbones, including both domain-specific and general-purpose models under a
unified tuning protocol. Our framework demonstrates strong performance across
both language generation and clinical metrics, achieving a ROUGE-L score of
0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582.
Qualitative analysis further highlights improved diagnostic consistency and
reduced hallucinations. AMRG offers a scalable and adaptable foundation for
radiology report generation and paves the way for future research in multimodal
medical AI.

</details>


### [326] [A Generative Imputation Method for Multimodal Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.09271)
*Reihaneh Hassanzadeh,Anees Abrol,Hamid Reza Hassanzadeh,Vince D. Calhoun*

Main category: eess.IV

TL;DR: The paper proposes a generative adversarial network (GAN) to handle incomplete multimodal neuroimaging data, achieving better reconstruction and improved Alzheimer's disease detection accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of incomplete multimodal neuroimaging data, which limits the accuracy of predictions and introduces biases with traditional imputation methods.

Method: A GAN-based method is used to reconstruct missing neuroimaging modalities, preserving disease-specific patterns. Two modalities (T1-weighted MRI and functional network connectivity) are leveraged in the study.

Result: The GAN-based method improved Alzheimer's disease classification accuracy by 9% compared to traditional subsampling or zero-filling approaches.

Conclusion: The proposed GAN model effectively reconstructs missing data, preserving disease-relevant information and enhancing diagnostic accuracy in incomplete multimodal datasets.

Abstract: Multimodal data analysis can lead to more accurate diagnoses of brain
disorders due to the complementary information that each modality adds.
However, a major challenge of using multimodal datasets in the neuroimaging
field is incomplete data, where some of the modalities are missing for certain
subjects. Hence, effective strategies are needed for completing the data.
Traditional methods, such as subsampling or zero-filling, may reduce the
accuracy of predictions or introduce unintended biases. In contrast, advanced
methods such as generative models have emerged as promising solutions without
these limitations. In this study, we proposed a generative adversarial network
method designed to reconstruct missing modalities from existing ones while
preserving the disease patterns. We used T1-weighted structural magnetic
resonance imaging and functional network connectivity as two modalities. Our
findings showed a 9% improvement in the classification accuracy for Alzheimer's
disease versus cognitive normal groups when using our generative imputation
method compared to the traditional approaches.

</details>


### [327] [HiFi-Mamba: Dual-Stream W-Laplacian Enhanced Mamba for High-Fidelity MRI Reconstruction](https://arxiv.org/abs/2508.09179)
*Hongli Chen,Pengcheng Fang,Yuxia Chen,Yingxuan Ren,Jing Hao,Fangfang Tang,Xiaohao Cai,Shanshan Shan,Feng Liu*

Main category: eess.IV

TL;DR: The paper introduces HiFi-Mamba, a novel architecture designed to enhance MRI reconstruction accuracy using dual-stream processing and efficient modeling.


<details>
  <summary>Details</summary>
Motivation: Current MRI reconstruction methods struggle with insensitivity to high-frequency anatomical details and rely on inefficient multi-directional scanning processes.

Method: The authors propose HiFi-Mamba, a dual-stream architecture with stacked WL and HiFi-Mamba blocks. The WL block separates low-frequency and high-frequency streams, while the HiFi-Mamba block selectively integrates these streams using adaptive modulation and streamlined unidirectional traversal.

Result: HiFi-Mamba outperformed state-of-the-art CNN-based, Transformer-based, and Mamba-based models in MRI reconstruction benchmarks, achieving higher accuracy and efficiency.

Conclusion: HiFi-Mamba is a compact, efficient solution for MRI reconstruction, addressing limitations of previous methods and ensuring high fidelity reconstruction of anatomical details.

Abstract: Reconstructing high-fidelity MR images from undersampled k-space data remains
a challenging problem in MRI. While Mamba variants for vision tasks offer
promising long-range modeling capabilities with linear-time complexity, their
direct application to MRI reconstruction inherits two key limitations: (1)
insensitivity to high-frequency anatomical details; and (2) reliance on
redundant multi-directional scanning. To address these limitations, we
introduce High-Fidelity Mamba (HiFi-Mamba), a novel dual-stream Mamba-based
architecture comprising stacked W-Laplacian (WL) and HiFi-Mamba blocks.
Specifically, the WL block performs fidelity-preserving spectral decoupling,
producing complementary low- and high-frequency streams. This separation
enables the HiFi-Mamba block to focus on low-frequency structures, enhancing
global feature modeling. Concurrently, the HiFi-Mamba block selectively
integrates high-frequency features through adaptive state-space modulation,
preserving comprehensive spectral details. To eliminate the scanning
redundancy, the HiFi-Mamba block adopts a streamlined unidirectional traversal
strategy that preserves long-range modeling capability with improved
computational efficiency. Extensive experiments on standard MRI reconstruction
benchmarks demonstrate that HiFi-Mamba consistently outperforms
state-of-the-art CNN-based, Transformer-based, and other Mamba-based models in
reconstruction accuracy while maintaining a compact and efficient model design.

</details>


### [328] [MedPatch: Confidence-Guided Multi-Stage Fusion for Multimodal Clinical Data](https://arxiv.org/abs/2508.09182)
*Baraa Al Jorf,Farah Shamout*

Main category: eess.IV

TL;DR: The paper introduces MedPatch, a multimodal fusion architecture for clinical prediction, achieving state-of-the-art results using MIMIC datasets.


<details>
  <summary>Details</summary>
Motivation: Clinical decision-making requires integration of heterogeneous, sparse medical data modalities for improving prediction models.

Method: MedPatch uses a multi-stage architecture with components like joint and late fusion, a missingness-aware module, and confidence-guided patch clustering.

Result: MedPatch significantly outperforms existing baselines in tasks on in-hospital mortality prediction and clinical condition classification using real-world MIMIC datasets.

Conclusion: Confidence-guided multi-stage fusion effectively addresses data heterogeneity and establishes new benchmarks in clinical prediction tasks.

Abstract: Clinical decision-making relies on the integration of information across
various data modalities, such as clinical time-series, medical images and
textual reports. Compared to other domains, real-world medical data is
heterogeneous in nature, limited in size, and sparse due to missing modalities.
This significantly limits model performance in clinical prediction tasks.
Inspired by clinical workflows, we introduce MedPatch, a multi-stage multimodal
fusion architecture, which seamlessly integrates multiple modalities via
confidence-guided patching. MedPatch comprises three main components: (i) a
multi-stage fusion strategy that leverages joint and late fusion
simultaneously, (ii) a missingness-aware module that handles sparse samples
with missing modalities, (iii) a joint fusion module that clusters latent token
patches based on calibrated unimodal token-level confidence. We evaluated
MedPatch using real-world data consisting of clinical time-series data, chest
X-ray images, radiology reports, and discharge notes extracted from the
MIMIC-IV, MIMIC-CXR, and MIMIC-Notes datasets on two benchmark tasks, namely
in-hospital mortality prediction and clinical condition classification.
Compared to existing baselines, MedPatch achieves state-of-the-art performance.
Our work highlights the effectiveness of confidence-guided multi-stage fusion
in addressing the heterogeneity of multimodal data, and establishes new
state-of-the-art benchmark results for clinical prediction tasks.

</details>


### [329] [Dynamic Survival Prediction using Longitudinal Images based on Transformer](https://arxiv.org/abs/2508.09328)
*Bingfan Liu,Haolun Shi,Jiguo Cao*

Main category: eess.IV

TL;DR: SurLonFormer is a Transformer-based model for survival analysis using longitudinal medical images, addressing censored data, temporal correlations, and interpretability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve survival analysis with longitudinal medical images by addressing the limitations of current methods, such as inadequate use of censored data and low interpretability.

Method: SurLonFormer integrates imaging and structured data, utilizing a Vision Encoder for spatial features, a Sequence Encoder for temporal information, and a Survival Encoder based on the Cox proportional hazards model.

Result: Simulations and Alzheimer's disease analysis confirm SurLonFormer’s predictive superiority and ability to identify imaging biomarkers associated with diseases.

Conclusion: SurLonFormer enhances survival analysis through improved scalability, interpretation, and predictive performance in longitudinal imaging-based diagnosis.

Abstract: Survival analysis utilizing multiple longitudinal medical images plays a
pivotal role in the early detection and prognosis of diseases by providing
insight beyond single-image evaluations. However, current methodologies often
inadequately utilize censored data, overlook correlations among longitudinal
images measured over multiple time points, and lack interpretability. We
introduce SurLonFormer, a novel Transformer-based neural network that
integrates longitudinal medical imaging with structured data for survival
prediction. Our architecture comprises three key components: a Vision Encoder
for extracting spatial features, a Sequence Encoder for aggregating temporal
information, and a Survival Encoder based on the Cox proportional hazards
model. This framework effectively incorporates censored data, addresses
scalability issues, and enhances interpretability through occlusion sensitivity
analysis and dynamic survival prediction. Extensive simulations and a
real-world application in Alzheimer's disease analysis demonstrate that
SurLonFormer achieves superior predictive performance and successfully
identifies disease-related imaging biomarkers.

</details>


### [330] [T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis](https://arxiv.org/abs/2508.09919)
*Xiaojiao Xiao,Jianfeng Zhao,Qinmin Vivian Hu,Guanghui Wang*

Main category: eess.IV

TL;DR: The T-CACE framework converts non-contrast MRI into multi-phase contrast-enhanced MRI, tackling diagnostic and practical challenges in liver cancer imaging.


<details>
  <summary>Details</summary>
Motivation: Address risks from contrast agents, manual assessment inefficiencies, and limited annotated datasets in traditional MRI.

Method: Developed T-CACE with innovations like conditional token encoding, a dynamic time-aware attention mask, and temporal classification consistency constraints.

Result: Experiments proved T-CACE's superiority in image synthesis, segmentation, and lesion classification across two liver MRI datasets.

Conclusion: T-CACE enhances diagnostic safety, efficiency, and reliability while eliminating the need for traditional contrast-enhanced imaging methods.

Abstract: Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of
liver cancer, significantly improving the classification of the lesion and
patient outcomes. However, traditional MRI faces challenges including risks
from contrast agent (CA) administration, time-consuming manual assessment, and
limited annotated datasets. To address these limitations, we propose a
Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for
synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from
non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a
conditional token encoding (CTE) mechanism that unifies anatomical priors and
temporal phase information into latent representations; and a dynamic
time-aware attention mask (DTAM) that adaptively modulates inter-phase
information flow using a Gaussian-decayed attention mechanism, ensuring smooth
and physiologically plausible transitions across phases. Furthermore, a
constraint for temporal classification consistency (TCC) aligns the lesion
classification output with the evolution of the physiological signal, further
enhancing diagnostic reliability. Extensive experiments on two independent
liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods
in image synthesis, segmentation, and lesion classification. This framework
offers a clinically relevant and efficient alternative to traditional
contrast-enhanced imaging, improving safety, diagnostic efficiency, and
reliability for the assessment of liver lesion. The implementation of T-CACE is
publicly available at: https://github.com/xiaojiao929/T-CACE.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [331] [Safety Perspective on Assisted Lane Changes: Insights from Open-Road, Live-Traffic Experiments](https://arxiv.org/abs/2508.09233)
*Konstantinos Mattas,Sandor Vass,Gergely Zachar,Junyi Ji,Derek Gloudemans,Davide Maggi,Akos Kriston,Mohamed Brahmi,Maria Christina Galassi,Daniel B Work,Biagio Ciuffo*

Main category: physics.soc-ph

TL;DR: This paper evaluates lane change behaviors of vehicles with ADAS on a Tennessee highway, finding disparities in kinematics, adherence to regulations, safety margins, and induced traffic effects.


<details>
  <summary>Details</summary>
Motivation: To scrutinize under-researched lane change behaviors of ADAS-equipped vehicles in real-world contexts, focusing on safety and performance.

Method: Experimentation on I-24 highway with multiple vehicles, analyzing kinematics and safety margins, and using simulations to classify the challenge levels induced on surrounding traffic.

Result: Four systems showed slower speeds and decelerations than humans, while one was assertive. Three systems affected following vehicle safety, triggering traffic disruptions.

Conclusion: The results highlight inconsistencies in ADAS designs, raise safety concerns, and point out potential impacts on traffic dynamics due to deceleration-induced shockwaves.

Abstract: This study investigates the assisted lane change functionality of five
different vehicles equipped with advanced driver assistance systems (ADAS). The
goal is to examine novel, under-researched features of commercially available
ADAS technologies. The experimental campaign, conducted in the I-24 highway
near Nashville, TN, US, collected data on the kinematics and safety margins of
assisted lane changes in real-world conditions. The results show that the
kinematics of assisted lane changes are consistent for each system, with four
out of five vehicles using slower speeds and decelerations than human drivers.
However, one system consistently performed more assertive lane changes,
completing the maneuver in around 5 seconds. Regarding safety margins, only
three vehicles are investigated. Those operated in the US are not restricted by
relevant UN regulations, and their designs were found not to adhere to these
regulatory requirements. A simulation method used to classify the challenge
level for the vehicle receiving the lane change, showing that these systems can
force trailing vehicles to decelerate to keep a safe gap. One assisted system
was found to have performed a maneuver that posed a hard challenge level for
the other vehicle, raising concerns about the safety of these systems in
real-world operation. All three vehicles were found to carry out lane changes
that induced decelerations to the vehicle in the target lane. Those
decelerations could affect traffic flow, inducing traffic shockwaves.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [332] [AI Blob! LLM-Driven Recontextualization of Italian Television Archives](https://arxiv.org/abs/2508.09535)
*Roberto Balestri*

Main category: cs.MM

TL;DR: AI Blob! is an experimental system leveraging semantic cataloging and large language models to recontextualize archival television footage, creating narrative sequences using dynamic, content-aware retrieval.


<details>
  <summary>Details</summary>
Motivation: To explore innovative methods for archival television footage retrieval and reinterpretation through semantic technologies and LLMs.

Method: AI Blob! transcribes and segments audio from archival videos, embeds them into a vector database, and uses thematic prompts with an LLM to retrieve and recombine fragments into narrative montages.

Result: The system creates montages using semantic querying and retrieval that mimic editorial practices, showcasing dynamic archival engagement methods.

Conclusion: AI Blob! highlights the potential of AI-driven semantic technologies for new approaches to archival research and cultural analysis, providing a conceptual framework and dataset for interdisciplinary experimentation.

Abstract: This paper introduces AI Blob!, an experimental system designed to explore
the potential of semantic cataloging and Large Language Models (LLMs) for the
retrieval and recontextualization of archival television footage. Drawing
methodological inspiration from Italian television programs such as Blob (RAI
Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic
embeddings, and retrieval-augmented generation (RAG) to organize and
reinterpret archival content. The system processes a curated dataset of 1,547
Italian television videos by transcribing audio, segmenting it into
sentence-level units, and embedding these segments into a vector database for
semantic querying. Upon user input of a thematic prompt, the LLM generates a
range of linguistically and conceptually related queries, guiding the retrieval
and recombination of audiovisual fragments. These fragments are algorithmically
selected and structured into narrative sequences producing montages that
emulate editorial practices of ironic juxtaposition and thematic coherence. By
foregrounding dynamic, content-aware retrieval over static metadata schemas, AI
Blob! demonstrates how semantic technologies can facilitate new approaches to
archival engagement, enabling novel forms of automated narrative construction
and cultural analysis. The project contributes to ongoing debates in media
historiography and AI-driven archival research, offering both a conceptual
framework and a publicly available dataset to support further interdisciplinary
experimentation.

</details>


### [333] [PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research](https://arxiv.org/abs/2508.09232)
*Nick Oh,Giorgos D. Vrakas,Siân J. M. Brooke,Sasha Morinière,Toju Duke*

Main category: cs.MM

TL;DR: The paper introduces the PETLP framework to integrate GDPR, copyright, and platform terms for AI researchers, providing a systematic compliance solution for social media data analysis.


<details>
  <summary>Details</summary>
Motivation: AI researchers face difficulties in managing overlapping legal and regulatory obligations (GDPR, copyright, platform terms) when using social media data, with no existing unified framework to guide compliance.

Method: The framework, PETLP, embeds compliance directly into ETL pipelines and uses Data Protection Impact Assessments as evolving documents. It includes a systematic analysis of Reddit data to differentiate between legal rights of research organizations and commercial entities, and evaluates GDPR applicability.

Result: The study highlights the infeasibility of true anonymization for social media data and identifies gaps in legal provisions regarding dataset creation and model distribution.

Conclusion: PETLP helps researchers integrate legal safeguards into their workflows, offering a structured approach to resolve regulatory challenges in social media data research, while simplifying institutional obligations and improving compliance.

Abstract: Social media data presents AI researchers with overlapping obligations under
the GDPR, copyright law, and platform terms -- yet existing frameworks fail to
integrate these regulatory domains, leaving researchers without unified
guidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and
Present), a compliance framework that embeds legal safeguards directly into
extended ETL pipelines. Central to PETLP is treating Data Protection Impact
Assessments as living documents that evolve from pre-registration through
dissemination. Through systematic Reddit analysis, we demonstrate how
extraction rights fundamentally differ between qualifying research
organisations (who can invoke DSM Article 3 to override platform restrictions)
and commercial entities (bound by terms of service), whilst GDPR obligations
apply universally. We reveal why true anonymisation remains unachievable for
social media data and expose the legal gap between permitted dataset creation
and uncertain model distribution. By structuring compliance decisions into
practical workflows and simplifying institutional data management plans, PETLP
enables researchers to navigate regulatory complexity with confidence, bridging
the gap between legal requirements and research practice.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [334] [A Lightweight Learned Cardinality Estimation Model](https://arxiv.org/abs/2508.09602)
*Yaoyu Zhu,Jintao Zhang,Guoliang Li,Jianhua Feng*

Main category: cs.DB

TL;DR: This study introduces CoDe, a novel data-driven method for efficient and accurate cardinality estimation in databases using covering designs and tensor decompositions.


<details>
  <summary>Details</summary>
Motivation: Accurate and efficient cardinality estimation is critical for database management systems to optimize query executions, addressing the limitations of existing methods with either low accuracy or high latency.

Method: CoDe utilizes covering design to divide tables into smaller segments and applies tensor decomposition for data distribution modeling. It combines multiple models and algorithms to optimize query-specific distribution selection.

Result: Experimental results demonstrate CoDe achieves state-of-the-art accuracy and efficiency, with absolute accuracy for more than half of the queries across different datasets.

Conclusion: CoDe represents a significant step forward in cardinality estimation techniques, successfully balancing high accuracy and computational efficiency.

Abstract: Cardinality estimation is a fundamental task in database management systems,
aiming to predict query results accurately without executing the queries.
However, existing techniques either achieve low estimation accuracy or incur
high inference latency. Simultaneously achieving high speed and accuracy
becomes critical for the cardinality estimation problem. In this paper, we
propose a novel data-driven approach called CoDe (Covering with Decompositions)
to address this problem. CoDe employs the concept of covering design, which
divides the table into multiple smaller, overlapping segments. For each
segment, CoDe utilizes tensor decomposition to accurately model its data
distribution. Moreover, CoDe introduces innovative algorithms to select the
best-fitting distributions for each query, combining them to estimate the final
result. By employing multiple models to approximate distributions, CoDe excels
in effectively modeling discrete distributions and ensuring computational
efficiency. Notably, experimental results show that our method represents a
significant advancement in cardinality estimation, achieving state-of-the-art
levels of both estimation accuracy and inference efficiency. Across various
datasets, CoDe achieves absolute accuracy in estimating more than half of the
queries.

</details>


### [335] [AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?](https://arxiv.org/abs/2508.09631)
*Yuchen Tian,Kaixin Li,Hao Chen,Ziyang Luo,Hongzhan Lin,Sebastian Schelter,Lun Du,Jing Ma*

Main category: cs.DB

TL;DR: The paper evaluates how well large language models (LLMs) handle ambiguous graph queries and introduces a benchmark called AmbiGraph-Eval.


<details>
  <summary>Details</summary>
Motivation: To address the issue of ambiguity in graph queries when using LLMs, which often results in incorrect or unintended results.

Method: The authors developed a taxonomy for graph-query ambiguities and created AmbiGraph-Eval, a benchmark set of ambiguous queries with validated graph query answers.

Result: Nine representative LLMs were tested, and it was found that even advanced models struggle with ambiguities in graph queries.

Conclusion: There is a significant need for developing specialized techniques for handling ambiguity in graph queries processed by LLMs.

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in translating natural language into database queries, especially when dealing
with complex graph-structured data. However, real-world queries often contain
inherent ambiguities, and the interconnected nature of graph structures can
amplify these challenges, leading to unintended or incorrect query results. To
systematically evaluate LLMs on this front, we propose a taxonomy of
graph-query ambiguities, comprising three primary types: Attribute Ambiguity,
Relationship Ambiguity, and Attribute-Relationship Ambiguity, each subdivided
into Same-Entity and Cross-Entity scenarios. We introduce AmbiGraph-Eval, a
novel benchmark of real-world ambiguous queries paired with expert-verified
graph query answers. Evaluating 9 representative LLMs shows that even top
models struggle with ambiguous graph queries. Our findings reveal a critical
gap in ambiguity handling and motivate future work on specialized resolution
techniques.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [336] [User-Intent-Driven Semantic Communication via Adaptive Deep Understanding](https://arxiv.org/abs/2508.05884)
*Peigen Ye,Jingpu Duan,Hongyang Du,Yulan Guo*

Main category: cs.IT

TL;DR: The paper proposes a user-intention-driven semantic communication system enhanced by a multi-modal model, mask-guided attention, and channel state awareness, achieving significant improvements in communication efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing semantic communication systems that fail to deeply understand and generalize user intentions in intent-oriented communication.

Method: The system employs a multi-modal large model as a knowledge base for generating user-intention priors, introduces a mask-guided attention module for highlighting key semantics, and incorporates a channel state awareness module for adaptive and robust transmission.

Result: Extensive experiments show the proposed system significantly outperforms the state-of-the-art DeepJSCC, with improvements of 8%, 6%, and 19% in PSNR, SSIM, and LPIPS, respectively, under Rayleigh channel conditions with SNR of 5 dB.

Conclusion: The proposed system enhances semantic communication by effectively interpreting diverse user intents and achieving robust performance across varying conditions, offering a significant advancement in the field.

Abstract: Semantic communication focuses on transmitting task-relevant semantic
information, aiming for intent-oriented communication. While existing systems
improve efficiency by extracting key semantics, they still fail to deeply
understand and generalize users' real intentions. To overcome this, we propose
a user-intention-driven semantic communication system that interprets diverse
abstract intents. First, we integrate a multi-modal large model as semantic
knowledge base to generate user-intention prior. Next, a mask-guided attention
module is proposed to effectively highlight critical semantic regions. Further,
a channel state awareness module ensures adaptive, robust transmission across
varying channel conditions. Extensive experiments demonstrate that our system
achieves deep intent understanding and outperforms DeepJSCC, e.g., under a
Rayleigh channel at an SNR of 5 dB, it achieves improvements of 8%, 6%, and 19%
in PSNR, SSIM, and LPIPS, respectively.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [337] [A Limits Study of Memory-side Tiering Telemetry](https://arxiv.org/abs/2508.09351)
*Vinicius Petrucci,Felippe Zacarias,David Roberts*

Main category: cs.OS

TL;DR: The study introduces a CXL-based memory request logger to analyze runtime memory access and proposes a Hotness Monitoring Unit (HMU) for optimizing memory tiering, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the increasing complexity of workload demands and emerging memory/storage technologies, and improve current memory tiering strategies with more accurate and scalable solutions.

Method: Development of a CXL-based Experimental Memory Request Logger to reveal memory access patterns without disrupting workloads, and using these insights to emulate future memory telemetry hardware. Proposed combining data monitoring, proactive data movement, and compiler hints into an HMU.

Result: Demonstrated that the HMU could achieve a 1.94x speedup over Linux NUMA balancing tiering and close performance to Host-DRAM allocation, while offloading over 90% of pages to CXL memory.

Conclusion: Current tiering strategies are limited in accuracy and scalability; programmable, device-level telemetry provided by innovations like the HMU can greatly enhance memory systems' efficiency and performance.

Abstract: Increasing workload demands and emerging technologies necessitate the use of
various memory and storage tiers in computing systems. This paper presents
results from a CXL-based Experimental Memory Request Logger that reveals
precise memory access patterns at runtime without interfering with the running
workloads. We use it for software emulation of future memory telemetry
hardware. By combining reactive placement based on data address monitoring,
proactive data movement, and compiler hints, a Hotness Monitoring Unit (HMU)
within memory modules can greatly improve memory tiering solutions. Analysis of
page placement using profiled access counts on a Deep Learning Recommendation
Model (DLRM) indicates a potential 1.94x speedup over Linux NUMA balancing
tiering, and only a 3% slowdown compared to Host-DRAM allocation while
offloading over 90% of pages to CXL memory. The study underscores the
limitations of existing tiering strategies in terms of coverage and accuracy,
and makes a strong case for programmable, device-level telemetry as a scalable
and efficient solution for future memory systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [338] [Collision-Free Bearing-Driven Formation Tracking for Euler-Lagrange Systems](https://arxiv.org/abs/2508.09908)
*Haoshu Cheng,Martin Guay,Shimin Wang,Yunhong Che*

Main category: eess.SY

TL;DR: The paper proposes a method to track formations in heterogeneous systems with parametric uncertainty without requiring prior system parameter knowledge, confirming collision avoidance via numerical validation.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the challenge of controlling formations with heterogeneous Euler-Lagrange systems in scenarios with uncertainties and multiple moving leaders.

Method: A distributed observer and adaptive mechanism leveraging bearing-based localization are developed to estimate leader dynamics and synthesize a novel control law.

Result: A sufficient condition for collision avoidance based on initial configurations is established, and the approach's effectiveness is validated numerically.

Conclusion: The paper ensures reliable formation tracking with collision avoidance, even under parametric uncertainty and multiple leaders.

Abstract: In this paper, we investigate the problem of tracking formations driven by
bearings for heterogeneous Euler-Lagrange systems with parametric uncertainty
in the presence of multiple moving leaders. To estimate the leaders' velocities
and accelerations, we first design a distributed observer for the leader
system, utilizing a bearing-based localization condition in place of the
conventional connectivity assumption. This observer, coupled with an adaptive
mechanism, enables the synthesis of a novel distributed control law that guides
the formation towards the target formation, without requiring prior knowledge
of the system parameters. Furthermore, we establish a sufficient condition,
dependent on the initial formation configuration, that ensures collision
avoidance throughout the formation evolution. The effectiveness of the proposed
approach is demonstrated through a numerical example.

</details>


### [339] [Online Safety under Multiple Constraints and Input Bounds using gatekeeper: Theory and Applications](https://arxiv.org/abs/2508.09963)
*Devansh R. Agrawal,Dimitra Panagou*

Main category: eess.SY

TL;DR: This paper introduces "gatekeeper," a framework ensuring online safety for cyber-physical systems under various constraints using an efficient backup controller and single-variable optimization.


<details>
  <summary>Details</summary>
Motivation: To enhance the online safety of cyber-physical systems, particularly in constrained environments like multi-agent formation flight.

Method: Developed the 'gatekeeper' framework using a backup controller and optimization over a scalar variable, with a focus on satisfying infinite-horizon trajectories under constraints.

Result: Provided theoretical insights with sub-optimality bounds and applied this framework to multi-agent formation flight, showcasing its effectiveness under nonlinear, nonconvex conditions.

Conclusion: The 'gatekeeper' ensures safety efficiently with minimal assumptions, showing practical applicability in complex, constraint-heavy scenarios like multi-agent operations.

Abstract: This letter presents an approach to guarantee online safety of a
cyber-physical system under multiple state and input constraints. Our proposed
framework, called gatekeeper, recursively guarantees the existence of an
infinite-horizon trajectory that satisfies all constraints and system dynamics.
Such trajectory is constructed using a backup controller, which we define
formally in this paper. gatekeeper relies on a small number of verifiable
assumptions, and is computationally efficient since it requires optimization
over a single scalar variable. We make two primary contributions in this
letter. (A) First, we develop the theory of gatekeeper: we derive a
sub-optimality bound relative to a full nonlinear trajectory optimization
problem, and show how this can be used in runtime to validate performance. This
also informs the design of the backup controllers and sets. (B) Second, we
demonstrate in detail an application of gatekeeper for multi-agent formation
flight, where each Dubins agent must avoid multiple obstacles and weapons
engagement zones, both of which are nonlinear, nonconvex constraints.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [340] [Distributed Diamond Formation of Sliding Squares](https://arxiv.org/abs/2508.09638)
*Irina Kostitsyna,David Liedtke,Christian Scheideler*

Main category: cs.CG

TL;DR: This paper introduces a new distributed algorithm for square-shaped robotic modules to reconfigure into a diamond shape, preserving connectivity and working under minimal assumptions. The algorithm operates in $
^2$ time, with faster parallel variants proposed.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of reconfiguring square-shaped robots into a diamond configuration while ensuring connectivity and minimal assumptions.

Method: The paper presents a sequential algorithm addressing connectivity preservation and locked configurations using the original sliding square move set. It also proposes two parallel variants using a spanning tree structure for efficiency.

Result: The sequential version achieves optimal $
^2$ time complexity. The parallel variants demonstrated significant speedups in experiments and achieved linear average runtime in the second variant.

Conclusion: The new algorithm effectively reconfigures robotic modules into diamond shapes with minimal constraints, providing optimal or near-optimal performance in both sequential and parallel setups.

Abstract: The sliding square model is a widely used abstraction for studying
self-reconfigurable robotic systems, where modules are square-shaped robots
that move by sliding or rotating over one another. In this paper, we propose a
novel distributed algorithm that allows a group of modules to reconfigure into
a diamond shape, starting from an arbitrary side-connected configuration. It is
connectivity-preserving and operates under minimal assumptions: one leader
module, common chirality, constant memory per module, and visibility and
communication restricted to immediate neighbors. Unlike prior work, which
relaxes the original sliding square move-set, our approach uses the unmodified
move-set, addressing the additional challenge of handling locked
configurations. Our algorithm is sequential in nature and operates with a
worst-case time complexity of $\mathcal{O}(n^2)$ rounds, which is optimal for
sequential algorithms. To improve runtime, we introduce two parallel variants
of the algorithm. Both rely on a spanning tree data structure, allowing modules
to make decisions based on local connectivity. Our experimental results show a
significant speedup for the first variant, and linear average runtime for the
second variant, which is worst-case optimal for parallel algorithms.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [341] [Low-latency D-MIMO Localization using Distributed Scalable Message-Passing Algorithm](https://arxiv.org/abs/2508.09546)
*Dumitra Iancu,Liang Liu,Ove Edfors,Erik Leitinger,Xuhong Li*

Main category: eess.SP

TL;DR: The paper introduces a message-passing localization method designed for distributed MIMO systems, achieving low latency localization and hardware efficiency.


<details>
  <summary>Details</summary>
Motivation: To address scalability, low-latency requirements, and dynamic scenarios in localization for distributed MIMO architectures in future wireless systems.

Method: A distributed message-passing approach combining LOS path detection from multipath measurements and dynamic agent localization, integrated with FPGA-based hardware modeling for latency evaluation.

Result: The proposed method demonstrates comparable localization accuracy to other techniques while achieving lower latency and computational complexity.

Conclusion: The method supports distributed MIMO systems effectively, optimizing localization performance with minimal latency and efficient hardware use.

Abstract: Distributed MIMO and integrated sensing and communication are expected to be
key technologies in future wireless systems, enabling reliable, low-latency
communication and accurate localization. Dedicated localization solutions must
support distributed architecture, provide scalability across different system
configurations and meet strict latency requirements. We present a scalable
message-passing localization method and architecture co-designed for a
panel-based distributed MIMO system and network topology, in which
interconnected units operate without centralized processing. This method
jointly detects line-of-sight paths to distributed units from multipath
measurements in dynamic scenarios, localizes the agent, and achieves very low
latency. Additionally, we introduce a cycle-accurate system latency model based
on implemented FPGA operations, and show important insights into processing
latency and hardware utilization and system-level trade-offs. We compare our
method to a multipath-based localization method and show that it can achieve
similar localization performance, with wide enough distribution of array
elements, while offering lower latency and computational complexity.

</details>


### [342] [Bayesian-Driven Graph Reasoning for Active Radio Map Construction](https://arxiv.org/abs/2508.09142)
*Wenlihan Lu,Shijian Gao,Miaowen Wen,Yuxuan Liang,Chan-Byoung Chae,H. Vincent Poor*

Main category: eess.SP

TL;DR: This paper presents the Uncertainty-aware Radio Map (URAM) framework for efficient and accurate radio map reconstruction using intelligent trajectory planning.


<details>
  <summary>Details</summary>
Motivation: The emergence of low-altitude economy and the necessity for reliable wireless connectivity to aerial platforms highlight the need for efficient and accurate radio map reconstruction, constrained by aerial agents' limited battery capacity.

Method: A framework combining a Bayesian neural network to estimate real-time spatial uncertainty and an attention-based reinforcement learning policy to plan energy-efficient, informative trajectories over a probabilistic roadmap.

Result: URAM achieves a 34% improvement in reconstruction accuracy compared to existing methods, based on experimental evaluations.

Conclusion: The proposed URAM framework significantly enhances radio map reconstruction by leveraging uncertainty-aware reasoning and non-myopic navigation planning, ensuring better accuracy and efficiency under battery limitations.

Abstract: With the emergence of the low-altitude economy, radio maps have become
essential for ensuring reliable wireless connectivity to aerial platforms.
Autonomous aerial agents are commonly deployed for data collection using
waypoint-based navigation; however, their limited battery capacity
significantly constrains coverage and efficiency. To address this, we propose
an uncertainty-aware radio map (URAM) reconstruction framework that explicitly
leverages graph-based reasoning tailored for waypoint navigation. Our approach
integrates two key deep learning components: (1) a Bayesian neural network that
estimates spatial uncertainty in real time, and (2) an attention-based
reinforcement learning policy that performs global reasoning over a
probabilistic roadmap, using uncertainty estimates to plan informative and
energy-efficient trajectories. This graph-based reasoning enables intelligent,
non-myopic trajectory planning, guiding agents toward the most informative
regions while satisfying safety constraints. Experimental results show that
URAM improves reconstruction accuracy by up to 34% over existing baselines.

</details>


### [343] [RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet](https://arxiv.org/abs/2508.09140)
*Honggang Jia,Nan Cheng,Xiucheng Wang,Conghao Zhou,Ruijin Sun,Xuemin,Shen*

Main category: eess.SP

TL;DR: This paper introduces RadioMamba, an efficient and accurate model for constructing radio maps for 6G services by combining global and local feature capturing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the accuracy-efficiency trade-off in constructing radio maps for real-time and accurate spatial channel information crucial for 6G services.

Method: A hybrid Mamba-UNet model is proposed, using Mamba-Convolutional blocks to capture both long-range spatial (global) dependencies and local features with linear computational complexity.

Result: RadioMamba surpasses traditional methods in accuracy, operates 20 times faster, and uses only 2.9% of their parameter counts.

Conclusion: RadioMamba provides a viable and highly efficient approach for real-time optimization in next-generation wireless systems, integrating accuracy, speed, and parameter efficiency.

Abstract: Radio map (RM) has recently attracted much attention since it can provide
real-time and accurate spatial channel information for 6G services and
applications. However, current deep learning-based methods for RM construction
exhibit well known accuracy-efficiency trade-off. In this paper, we introduce
RadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the
trade-off. Generally, accurate RM construction requires modeling long-range
spatial dependencies, reflecting the global nature of wave propagation physics.
RadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures
these global dependencies with linear complexity, while a parallel
convolutional branch extracts local features. This hybrid design generates
feature representations that capture both global context and local detail.
Experiments show that RadioMamba achieves higher accuracy than existing
methods, including diffusion models, while operating nearly 20 times faster and
using only 2.9\% of the model parameters. By improving both accuracy and
efficiency, RadioMamba presents a viable approach for real-time intelligent
optimization in next generation wireless systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [344] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: This paper introduces a model for personalized product search ranking using a multi-task learning framework that effectively combines tabular and non-tabular data. A pre-trained TinyBERT is used for embeddings, alongside a novel sampling technique and relevance labeling.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve personalized product search ranking by leveraging diverse data types and capture customer behavior using advanced embedding techniques.

Method: A multi-task learning model integrates tabular and non-tabular data with TinyBERT embeddings, a new sampling approach, and scalable relevance labeling based on user actions.

Result: Combining non-tabular data with advanced embeddings significantly improves model performance, as evidenced by experiments and ablation studies.

Conclusion: The proposed framework effectively enhances personalized product search rankings, demonstrating robustness and scalability in handling mixed data sources.

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


### [345] [On Negative-aware Preference Optimization for Recommendation](https://arxiv.org/abs/2508.09653)
*Chenlu Ding,Daoxuan Liu,Jiancan Wu,Xingyu Hu,Junkang Wu,Haitao Wang,Yongkang Wang,Xingxing Wang,Xiang Wang*

Main category: cs.IR

TL;DR: The paper introduces NAPO, a framework to improve LLM-based recommendation systems by addressing challenges with optimally utilizing negative samples.


<details>
  <summary>Details</summary>
Motivation: To tackle deficiencies in leveraging negative samples for LLM-based recommenders, such as high computational costs and suboptimal optimization from existing methods.

Method: The proposed NAPO framework employs (1) in-batch negative sharing to expand negative sample pools without extra memory costs and (2) dynamic reward margin adjustment to refine model updates based on sample confidence.

Result: Experiments using three public datasets show that NAPO improves recommendation accuracy and reduces popularity bias over current methods.

Conclusion: NAPO offers a more effective and efficient approach to preference optimization in recommendation systems, addressing key shortcomings in negative sample handling.

Abstract: Recommendation systems leverage user interaction data to suggest relevant
items while filtering out irrelevant (negative) ones. The rise of large
language models (LLMs) has garnered increasing attention for their potential in
recommendation tasks. However, existing methods for optimizing LLM-based
recommenders face challenges in effectively utilizing negative samples. Simply
integrating large numbers of negative samples can improve ranking accuracy and
mitigate popularity bias but often leads to increased computational overhead
and memory costs. Additionally, current approaches fail to account for the
varying informativeness of negative samples, leading to suboptimal optimization
performance. To address these issues, we propose NAPO
(\textbf{N}egative-\textbf{A}ware \textbf{P}reference \textbf{O}ptimization),
an enhanced framework for preference optimization in LLM-based recommendation.
NAPO introduces two key innovations: (1) in-batch negative sharing, which
expands the pool of negative samples without additional memory overhead, and
(2) dynamic reward margin adjustment, which adapts model updates based on the
confidence of negative samples. Extensive experiments on three public datasets
demonstrate that NAPO outperforms existing methods in both recommendation
accuracy and popularity bias reduction.

</details>


### [346] [Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations](https://arxiv.org/abs/2508.09789)
*Marco De Nadai,Andreas Damianou,Mounia Lalmas*

Main category: cs.IR

TL;DR: This paper proposes a framework that integrates high-level semantic understanding into video recommendation systems by using multimodal large language models (MLLM) to summarize video clips into natural language descriptions, which enhances the accuracy of recommendations.


<details>
  <summary>Details</summary>
Motivation: There is a gap in video recommendation systems where low-level features like visual and audio signals fail to capture deeper semantics such as intent, humour, or cultural references crucial for personalized recommendations.

Method: The authors use an off-the-shelf Multimodal Large Language Model (MLLM) to generate rich natural-language summaries of video clips, which are then fed into standard recommendation models via a state-of-the-art text encoder.

Result: The framework is tested on the MicroLens-100K dataset and demonstrates consistent improvement in performance across five different types of video recommendation models.

Conclusion: MLLMs can serve as effective tools to enrich video recommendation pipelines by enhancing semantic understanding, making recommendations more intent-aware and personalized.

Abstract: Existing video recommender systems rely primarily on user-defined metadata or
on low-level visual and acoustic signals extracted by specialised encoders.
These low-level features describe what appears on the screen but miss deeper
semantics such as intent, humour, and world knowledge that make clips resonate
with viewers. For example, is a 30-second clip simply a singer on a rooftop, or
an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such
distinctions are critical to personalised recommendations yet remain invisible
to traditional encoding pipelines. In this paper, we introduce a simple,
recommendation system-agnostic zero-finetuning framework that injects
high-level semantics into the recommendation pipeline by prompting an
off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip
into a rich natural-language description (e.g. "a superhero parody with
slapstick fights and orchestral stabs"), bridging the gap between raw content
and user intent. We use MLLM output with a state-of-the-art text encoder and
feed it into standard collaborative, content-based, and generative
recommenders. On the MicroLens-100K dataset, which emulates user interactions
with TikTok-style videos, our framework consistently surpasses conventional
video, audio, and metadata features in five representative models. Our findings
highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to
build more intent-aware video recommenders.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [347] [Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research](https://arxiv.org/abs/2508.09815)
*Klaudia Krawiecka,Christian Schroeder de Witt*

Main category: cs.MA

TL;DR: This paper enhances the OWASP Multi-Agentic System Threat Modeling Guide by addressing gaps in securing Large Language Model (LLM)-driven multi-agent architectures.


<details>
  <summary>Details</summary>
Motivation: Existing OWASP taxonomy does not address unique security challenges in LLM-driven multi-agent systems, such as reasoning failure and cross-agent threats, necessitating an update.

Method: The authors extend the OWASP guide with new threat classes, scenarios, and evaluation strategies focused on real-world MAS deployments.

Result: The paper identifies specific gaps in current threat modeling, proposes new scenarios and classifications, and suggests evaluation methods like robustness testing and safety enforcement.

Conclusion: By expanding OWASP's framework, this work introduces a practical guide for securing increasingly complex autonomous multi-agent systems, particularly those powered by LLMs.

Abstract: We propose an extension to the OWASP Multi-Agentic System (MAS) Threat
Modeling Guide, translating recent anticipatory research in multi-agent
security (MASEC) into practical guidance for addressing challenges unique to
large language model (LLM)-driven multi-agent architectures. Although OWASP's
existing taxonomy covers many attack vectors, our analysis identifies gaps in
modeling failures, including, but not limited to: reasoning collapse across
planner-executor chains, metric overfitting, unsafe delegation escalation,
emergent covert coordination, and heterogeneous multi-agent exploits. We
introduce additional threat classes and scenarios grounded in practical MAS
deployments, highlighting risks from benign goal drift, cross-agent
hallucination propagation, affective prompt framing, and multi-agent backdoors.
We also outline evaluation strategies, including robustness testing,
coordination assessment, safety enforcement, and emergent behavior monitoring,
to ensure complete coverage. This work complements the framework of OWASP by
expanding its applicability to increasingly complex, autonomous, and adaptive
multi-agent systems, with the goal of improving security posture and resilience
in real world deployments.

</details>


### [348] [Cowpox: Towards the Immunity of VLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.09230)
*Yutong Wu,Jie Zhang,Yiming Li,Chao Zhang,Qing Guo,Nils Lukas,Tianwei Zhang*

Main category: cs.MA

TL;DR: The paper introduces Cowpox, a defense mechanism for enhancing robustness in multi-agent systems against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems lack robustness, as adversarial attacks targeting one agent can compromise the integrity of the entire system.

Method: Cowpox is a distributed defense mechanism that uses cure samples to immunize agents preemptively and aid recovery of infected agents, limiting the spread of infections.

Result: Cowpox was shown to effectively enhance robustness empirically while providing theoretical guarantees.

Conclusion: Cowpox offers a provable and practical solution to bolster multi-agent system resilience against security threats.

Abstract: Vision Language Model (VLM)-based agents are stateful, autonomous entities
capable of perceiving and interacting with their environments through vision
and language. Multi-agent systems comprise specialized agents who collaborate
to solve a (complex) task. A core security property is robustness, stating that
the system should maintain its integrity under adversarial attacks. However,
the design of existing multi-agent systems lacks the robustness consideration,
as a successful exploit against one agent can spread and infect other agents to
undermine the entire system's assurance. To address this, we propose a new
defense approach, Cowpox, to provably enhance the robustness of multi-agent
systems. It incorporates a distributed mechanism, which improves the recovery
rate of agents by limiting the expected number of infections to other agents.
The core idea is to generate and distribute a special cure sample that
immunizes an agent against the attack before exposure and helps recover the
already infected agents. We demonstrate the effectiveness of Cowpox empirically
and provide theoretical robustness guarantees.

</details>


### [349] [Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective](https://arxiv.org/abs/2508.09541)
*Gang Chen,Guoxin Wang,Anton van Beek,Zhenjun Ming,Yan Yan*

Main category: cs.MA

TL;DR: The research investigates how dependency hierarchies emerge and evolve during task execution in multi-agent self-organizing systems (MASOS), using multi-agent reinforcement learning (MARL) in a box-pushing task.


<details>
  <summary>Details</summary>
Motivation: To understand the formation, evolution, and governing factors behind dependency hierarchies in MASOS during collaborative tasks.

Method: Multi-agent reinforcement learning (MARL) was used to train agents for a box-pushing task, analyzing inter-agent dependencies through action-state gradients and aggregating these dependencies.

Result: Dynamic hierarchies emerged organically from agents working towards a shared objective, influenced by task environment and network initialization, without pre-configured rules.

Conclusion: Dependency hierarchies in MASOS are shaped by the interplay of agents' Talent, Effort, and Environment, emphasizing organic emergence and adaptability in collaborative systems.

Abstract: Multi-agent self-organizing systems (MASOS) exhibit key characteristics
including scalability, adaptability, flexibility, and robustness, which have
contributed to their extensive application across various fields. However, the
self-organizing nature of MASOS also introduces elements of unpredictability in
their emergent behaviors. This paper focuses on the emergence of dependency
hierarchies during task execution, aiming to understand how such hierarchies
arise from agents' collective pursuit of the joint objective, how they evolve
dynamically, and what factors govern their development. To investigate this
phenomenon, multi-agent reinforcement learning (MARL) is employed to train
MASOS for a collaborative box-pushing task. By calculating the gradients of
each agent's actions in relation to the states of other agents, the inter-agent
dependencies are quantified, and the emergence of hierarchies is analyzed
through the aggregation of these dependencies. Our results demonstrate that
hierarchies emerge dynamically as agents work towards a joint objective, with
these hierarchies evolving in response to changing task requirements. Notably,
these dependency hierarchies emerge organically in response to the shared
objective, rather than being a consequence of pre-configured rules or
parameters that can be fine-tuned to achieve specific results. Furthermore, the
emergence of hierarchies is influenced by the task environment and network
initialization conditions. Additionally, hierarchies in MASOS emerge from the
dynamic interplay between agents' "Talent" and "Effort" within the
"Environment." "Talent" determines an agent's initial influence on collective
decision-making, while continuous "Effort" within the "Environment" enables
agents to shift their roles and positions within the system.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [350] [Classifying Cool Dwarfs: Comprehensive Spectral Typing of Field and Peculiar Dwarfs Using Machine Learning](https://arxiv.org/abs/2508.09370)
*Tianxing Zhou,Christopher A. Theissen,S. Jean Feeser,William M. J. Best,Adam J. Burgasser,Kelle L. Cruz,Lexu Zhao*

Main category: astro-ph.SR

TL;DR: This study employs machine learning (ML) techniques (Random Forest, SVM, KNN) for automated spectral type classification of M0–T9 dwarfs using low-resolution near-infrared spectra. The best-performing KNN model achieved 95.5% spectral type accuracy and 89.5% subclass accuracy.


<details>
  <summary>Details</summary>
Motivation: The classification of low-mass stars and brown dwarfs is crucial for understanding stellar/substellar demographics and processes, but current methods rely heavily on manual inspection. Large spectroscopic surveys are producing massive datasets, necessitating automated classification approaches.

Method: The study used low-resolution SpeX instrument spectra (R ~120) and tested ML models (Random Forest, SVM, KNN) with binned fluxes as features. Different normalizations were applied, and spectral regions were analyzed for their contribution to gravity and metallicity subclassification.

Result: The KNN model performed best, achieving 95.5% accuracy in spectral type classification (±1 SpT) and 89.5% accuracy for gravity and metallicity subclass classification. Sources with SNR ≥ 60 had classification accuracies above 95%. The zy-band and FeH/TiO features were most important in the RF model.

Conclusion: Machine learning methods, particularly KNN, are effective for automated classification of low-mass stars and brown dwarfs. They demonstrate high accuracy in both spectral typing and subclass determination, significantly aiding the analysis of extensive spectroscopic datasets.

Abstract: Low-mass stars and brown dwarfs -- spectral types (SpTs) M0 and later -- play
a significant role in studying stellar and substellar processes and
demographics, reaching down to planetary-mass objects. Currently, the
classification of these sources remains heavily reliant on visual inspection of
spectral features, equivalent width measurements, or narrow-/wide-band spectral
indices. Recent advances in machine learning (ML) methods offer automated
approaches for spectral typing, which are becoming increasingly important as
large spectroscopic surveys such as Gaia, SDSS, and SPHEREx generate datasets
containing millions of spectra. We investigate the application of ML in
spectral type classification on low-resolution (R $\sim$ 120) near-infrared
spectra of M0--T9 dwarfs obtained with the SpeX instrument on the NASA Infrared
Telescope Facility. We specifically aim to classify the gravity- and
metallicity-dependent subclasses for late-type dwarfs. We used binned fluxes as
input features and compared the efficacy of spectral type estimators built
using Random Forest (RF), Support Vector Machine (SVM), and K-Nearest Neighbor
(KNN) models. We tested the influence of different normalizations and analyzed
the relative importance of different spectral regions for surface gravity and
metallicity subclass classification. Our best-performing model (using KNN)
classifies 95.5 $\pm$ 0.6% of sources to within $\pm$1 SpT, and assigns surface
gravity and metallicity subclasses with 89.5 $\pm$ 0.9% accuracy. We test the
dependence of signal-to-noise ratio on classification accuracy and find sources
with SNR $\gtrsim$ 60 have $\gtrsim$ 95% accuracy. We also find that zy-band
plays the most prominent role in the RF model, with FeH and TiO having the
highest feature importance.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [351] [Objective Soups: Multilingual Multi-Task Modeling for Speech Processing](https://arxiv.org/abs/2508.09228)
*A F M Saif,Lisha Chen,Xiaodong Cui,Songtao Lu,Brian Kingsbury,Tianyi Chen*

Main category: eess.AS

TL;DR: The paper investigates how to efficiently train a multilingual, multi-task speech processing (MSP) model by addressing conflicting objectives using hierarchical multi-objective optimization (MOO).


<details>
  <summary>Details</summary>
Motivation: Training MSP models is challenging due to conflicting objectives, especially between tasks like speech recognition and translation. Effective optimization strategies are needed to ensure better performance and scalability.

Method: The study proposes three multi-objective MSP formulations (termed 'objective soup recipes') that apply MOO at various optimization levels. A lightweight layer-selection mechanism is introduced to minimize computational overhead by focusing on the most problematic layers.

Result: Extensive experiments show that a bi-level optimization approach, which separates recognition and translation tasks, achieves better performance compared to standard flat optimization methods.

Conclusion: Hierarchical MOO proves to be a more effective and scalable approach for multi-task MSP model training, offering better alignment of objectives and improved performance.

Abstract: Training a single model for multilingual, multi-task speech processing (MSP)
is severely hampered by conflicting objectives between tasks like speech
recognition and translation. While multi-objective optimization (MOO) aims to
align gradient updates, its effectiveness diminishes as the number of tasks
grows, making it difficult to find a common descent direction. This raises a
fundamental question: should highly conflicting objectives be optimized jointly
or separated into a hierarchical structure? To address this question, this
paper investigates three multi-objective MSP formulations, which we refer to as
\textbf{objective soup recipes}. These formulations apply multi-objective
optimization at different optimization levels to mitigate potential conflicts
among all objectives. To ensure efficiency, we introduce a lightweight
layer-selection mechanism that computes the conflict-avoiding gradient using
only the most problematic layers, minimizing computational and memory overhead.
Extensive experiments on CoVoST v2, LibriSpeech, and AISHELL-1 reveal that a
bi-level recipe separating recognition and translation tasks consistently
outperforms standard flat optimization. Our work demonstrates that hierarchical
MOO is a more effective and scalable approach for building state-of-the-art MSP
models. Our code has been released at
https://github.com/afmsaif/Objective_Soups.

</details>


### [352] [Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative](https://arxiv.org/abs/2508.09294)
*Xi Xuan,Zimo Zhu,Wenxin Zhang,Yi-Cheng Lin,Tomi Kinnunen*

Main category: eess.AS

TL;DR: This paper presents a deepfake speech detection solution, Fake-Mamba, achieving state-of-the-art results with efficient inference.


<details>
  <summary>Details</summary>
Motivation: The rise in realistic synthesized speech poses significant security risks, necessitating effective real-time detection methods.

Method: The proposed framework, Fake-Mamba, combines XLSR with three novel bidirectional Mamba-based encoders (TransBiMamba, ConBiMamba, and PN-BiMamba) to detect local and global synthetic speech artifacts.

Result: Fake-Mamba achieves 0.97% EER on ASVspoof 21 LA, 1.74% EER on 21 DF, and 5.85% EER on In-The-Wild benchmarks, surpassing state-of-the-art models.

Conclusion: The methodology demonstrates practicality for real-time deepfake detection, offering effective solutions to counter synthetic speech threats while enhancing efficiency.

Abstract: Advances in speech synthesis intensify security threats, motivating real-time
deepfake detection research. We investigate whether bidirectional Mamba can
serve as a competitive alternative to Self-Attention in detecting synthetic
speech. Our solution, Fake-Mamba, integrates an XLSR front-end with
bidirectional Mamba to capture both local and global artifacts. Our core
innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and
PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can
effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof
21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and
5.85% EER, respectively, representing substantial relative gains over SOTA
models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time
inference across utterance lengths, demonstrating strong generalization and
practical viability. The code is available at
https://github.com/xuanxixi/Fake-Mamba.

</details>


### [353] [ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs](https://arxiv.org/abs/2508.09389)
*Eray Eren,Qingju Liu,Hyeongwoo Kim,Pablo Garrido,Abeer Alwan*

Main category: eess.AS

TL;DR: The paper introduces a model to map text to prosodic features like F0 and energy, enhancing applications such as Text-To-Speech (TTS).


<details>
  <summary>Details</summary>
Motivation: To better capture prosodic features (emotion, semantics, and speaker idiosyncrasies) for improved downstream applications like TTS.

Method: ProMode is designed as a stand-alone encoder-decoder model that encodes latent prosodic features and predicts masked acoustic data. It uses audio and time-aligned text as inputs.

Result: Compared to state-of-the-art methods, the model performs consistently better in predicting F0 and energy. When integrated into TTS, it shows higher prosody preference in perceptual tests.

Conclusion: The proposed model improves prosody modeling and demonstrates its utility in tasks like TTS, outperforming current baselines in both prediction and perceptual quality.

Abstract: Prosody conveys rich emotional and semantic information of the speech signal
as well as individual idiosyncrasies. We propose a stand-alone model that maps
text-to-prosodic features such as F0 and energy and can be used in downstream
tasks such as TTS. The ProMode encoder takes as input acoustic features and
time-aligned textual content, both are partially masked, and obtains a
fixed-length latent prosodic embedding. The decoder predicts acoustics in the
masked region using both the encoded prosody input and unmasked textual
content. Trained on the GigaSpeech dataset, we compare our method with
state-of-the-art style encoders. For F0 and energy predictions, we show
consistent improvements for our model at different levels of granularity. We
also integrate these predicted prosodic features into a TTS system and conduct
perceptual tests, which show higher prosody preference compared to the
baselines, demonstrating the model's potential in tasks where prosody modeling
is important.

</details>


### [354] [Improving the Speaker Anonymization Evaluation's Robustness to Target Speakers with Adversarial Learning](https://arxiv.org/abs/2508.09803)
*Carlos Franzreb,Arnab Das,Tim Polzehl,Sebastian Möller*

Main category: eess.AS

TL;DR: The paper identifies a flaw in current privacy evaluations for speaker anonymization that overestimates privacy with same-gender target selection (TSA). It proposes adding a target classifier to improve reliability.


<details>
  <summary>Details</summary>
Motivation: The study aims to address overestimated privacy in speaker anonymization evaluations caused by same-gender TSA, which leaks gender information, making it more vulnerable.

Method: The paper introduces a target classifier to measure target speaker influence in anonymized speech and uses adversarial learning to mitigate it in the evaluation process.

Result: The proposed approach improves privacy evaluation accuracy across multiple anonymization techniques, especially when using same-gender TSA.

Conclusion: Adding a target classifier produces a more accurate and reliable assessment of privacy in speaker anonymization systems.

Abstract: The current privacy evaluation for speaker anonymization often overestimates
privacy when a same-gender target selection algorithm (TSA) is used, although
this TSA leaks the speaker's gender and should hence be more vulnerable. We
hypothesize that this occurs because the evaluation does not account for the
fact that anonymized speech contains information from both the source and
target speakers. To address this, we propose to add a target classifier that
measures the influence of target speaker information in the evaluation, which
can also be removed with adversarial learning. Experiments demonstrate that
this approach is effective for multiple anonymizers, particularly when using a
same-gender TSA, leading to a more reliable assessment.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [355] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)
*Daniel Raffini,Agnese Macori,Lorenzo Porcaro,Tiziana Catarci,Marco Angelini*

Main category: cs.HC

TL;DR: The paper studies how arguments generated by ChatGPT on ethical topics affect human opinions and perceptions.


<details>
  <summary>Details</summary>
Motivation: To understand the persuasive impact of AI-generated argumentative texts on ethically sensitive topics.

Method: Conducted a user study with 62 participants, using pre- and post-interaction surveys, coupled with linguistic and rhetorical analysis of ChatGPT's text.

Result: ChatGPT's generated arguments were coherent but formulaic, with limited stylistic variety. Ethical concerns persisted or intensified in participants post-interaction.

Conclusion: AI-generated texts effectively construct arguments but have limited persuasive power, particularly in ethically nuanced discussions. The study offers insights for understanding and enhancing AI's role in persuasive communication.

Abstract: This study examines the rhetorical and linguistic features of argumentative
texts generated by ChatGPT on ethically nuanced topics and investigates their
persuasive impact on human readers.Through a user study involving 62
participants and pre-post interaction surveys, the paper analyzes how exposure
to AI-generated arguments affects opinion change and user perception. A
linguistic and rhetorical analysis of the generated texts reveals a consistent
argumentative macrostructure, reliance on formulaic expressions, and limited
stylistic richness. While ChatGPT demonstrates proficiency in constructing
coherent argumentative texts, its persuasive efficacy appears constrained,
particularly on topics involving ethical issues.The study finds that while
participants often acknowledge the benefits highlighted by ChatGPT, ethical
concerns tend to persist or even intensify post-interaction. The results also
demonstrate a variation depending on the topic. These findings highlight new
insights on AI-generated persuasion in ethically sensitive domains and are a
basis for future research.

</details>


### [356] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)
*Daniel Raffini,Agnese Macori,Marco Angelini,Tiziana Catarci*

Main category: cs.HC

TL;DR: The paper investigates gender biases in narratives generated by ChatGPT, Gemini, and Claude, based on character roles and plot structures.


<details>
  <summary>Details</summary>
Motivation: Gender-based narrative biases can subtly influence AI-generated content, affecting equity and representation across stories.

Method: Using Propp's character classifications and Freytag's narrative framework, stories were analyzed regarding adherence to prompts, character gender distributions, descriptions, actions, and plot relationships through close reading.

Result: Implicit biases were identified in the narratives, underscoring their persistence and highlighting the importance of multi-level bias assessments.

Conclusion: An interpretative approach is essential for analyzing and addressing biases in AI-generated narratives to promote better representation.

Abstract: The paper explores the study of gender-based narrative biases in stories
generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's
character classifications and Freytag's narrative structure. The stories are
analyzed through a close reading approach, with particular attention to
adherence to the prompt, gender distribution of characters, physical and
psychological descriptions, actions, and finally, plot development and
character relationships. The results reveal the persistence of biases -
especially implicit ones - in the generated stories and highlight the
importance of assessing biases at multiple levels using an interpretative
approach.

</details>


### [357] [Based AI improves human decision-making but reduces trust](https://arxiv.org/abs/2508.09297)
*Shiyang Lai,Junsol Kim,Nadav Kunievsky,Yujin Potter,James Evans*

Main category: cs.HC

TL;DR: This study investigates the impact of culturally biased versus neutral AI systems on human decision-making, finding that biased AI can enhance performance and engagement despite trust drawbacks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore whether culturally biased AI systems can improve cognitive engagement and decision-making in humans compared to ideologically neutral AI.

Method: The researchers conducted randomized trials involving 2,500 participants who interacted with politically diverse GPT-4o variants, analyzing their performance on information evaluation tasks both with partisan and neutral AI inputs.

Result: Partisan AI assistants improved decision-making, increased user engagement, and reduced evaluative bias more effectively than neutral AI. However, participants showed less trust in biased AI systems and perceived neutral systems favorably despite performance differences.

Conclusion: The study suggests existing practices emphasizing AI neutrality might need reconsideration, highlighting the potential for incorporating diverse cultural biases to enhance human decision-making and robustness.

Abstract: Current AI systems minimize risk by enforcing ideological neutrality, yet
this may introduce automation bias by suppressing cognitive engagement in human
decision-making. We conducted randomized trials with 2,500 participants to test
whether culturally biased AI enhances human decision-making. Participants
interacted with politically diverse GPT-4o variants on information evaluation
tasks. Partisan AI assistants enhanced human performance, increased engagement,
and reduced evaluative bias compared to non-biased counterparts, with amplified
benefits when participants encountered opposing views. These gains carried a
trust penalty: participants underappreciated biased AI and overcredited neutral
systems. Exposing participants to two AIs whose biases flanked human
perspectives closed the perception-performance gap. These findings complicate
conventional wisdom about AI neutrality, suggesting that strategic integration
of diverse cultural biases may foster improved and resilient human
decision-making.

</details>


### [358] [Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis](https://arxiv.org/abs/2508.09458)
*Xi Long,Christy Boscardin,Lauren A. Maggio,Joseph A. Costello,Ralph Gonzales,Rasmyah Hammoudeh,Ki Lai,Yoon Soo Park,Brian C. Gin*

Main category: cs.HC

TL;DR: The paper evaluates the use of large language models (LLMs) for automating data extraction in health professions education (HPE) literature reviews, finding AI highly consistent with humans for explicit tasks but noting caution for subjective interpretation.


<details>
  <summary>Details</summary>
Motivation: To address the labor-intensive nature of data extraction in HPE literature reviews while exploring if AI can maintain accuracy and reliability compared to human efforts.

Method: Developed a platform using large language models (LLMs) for data extraction and analyzed AI-human, human-human, and AI-AI responses on 187 publications and 17 extraction questions. Measured consistency using interrater reliability and thematic similarity and identified errors by comparing to source texts.

Result: AI showed high consistency with humans for explicit, clearly stated questions but lower consistency for subjective interpretation or absent data. Most AI-human discrepancies arose from interpretive differences, not inaccuracies. Human errors were nearly three times more frequent than AI inaccuracies.

Conclusion: AI tools show promise in assisting data extraction by enhancing efficiency and accuracy for explicit tasks, but human oversight remains critical for subjective areas to ensure the preservation of insights and address interpretive challenges.

Abstract: Knowledge syntheses (literature reviews) are essential to health professions
education (HPE), consolidating findings to advance theory and practice.
However, they are labor-intensive, especially during data extraction.
Artificial Intelligence (AI)-assisted extraction promises efficiency but raises
concerns about accuracy, making it critical to distinguish AI 'hallucinations'
(fabricated content) from legitimate interpretive differences. We developed an
extraction platform using large language models (LLMs) to automate data
extraction and compared AI to human responses across 187 publications and 17
extraction questions from a published scoping review. AI-human, human-human,
and AI-AI consistencies were measured using interrater reliability
(categorical) and thematic similarity ratings (open-ended). Errors were
identified by comparing extracted responses to source publications. AI was
highly consistent with humans for concrete, explicitly stated questions (e.g.,
title, aims) and lower for questions requiring subjective interpretation or
absent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human
consistency was not higher than AI-human and showed the same question-dependent
variability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due
to interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while
humans were nearly three times more likely to state inaccuracies (4.37%).
Findings suggest AI accuracy depends more on interpretability than
hallucination. Repeating AI extraction can identify interpretive complexity or
ambiguity, refining processes before human review. AI can be a transparent,
trustworthy partner in knowledge synthesis, though caution is needed to
preserve critical human insights.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [359] [Forecasting Binary Economic Events in Modern Mercantilism: Traditional methodologies coupled with PCA and K-means Quantitative Analysis of Qualitative Sentimental Data](https://arxiv.org/abs/2508.09243)
*Sebastian Kot*

Main category: econ.GN

TL;DR: The paper explores Modern Mercantilism as a departure from post-1945 globalization, using PCA on SBERT-based semantic embeddings of news articles to classify and interpret events related to protectionism and geopolitical shifts.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the growing trend of economic nationalism and geopolitical fragmentation by developing a scalable, data-driven framework to analyze these emergent dynamics effectively.

Method: The authors use Principal Component Analysis (PCA) on 768-dimensional semantic embeddings generated by SBERT (Sentence-BERT) from selected news articles to extract latent factors. These factors are then used to classify events and interpret the semantic features influencing their classification.

Result: The approach successfully identified key semantic features and event classifications, enhancing both interpretability and predictive accuracy concerning protectionism, technological sovereignty, and bloc realignment.

Conclusion: The study introduces a scalable methodology leveraging high-dimensional text analytics to quantitatively track and analyze modern mercantilist dynamics, offering significant insights into economic and geopolitical trends.

Abstract: This paper examines Modern Mercantilism, characterized by rising economic
nationalism, strategic technological decoupling, and geopolitical
fragmentation, as a disruptive shift from the post-1945 globalization paradigm.
It applies Principal Component Analysis (PCA) to 768-dimensional
SBERT-generated semantic embeddings of curated news articles to extract
orthogonal latent factors that discriminate binary event outcomes linked to
protectionism, technological sovereignty, and bloc realignments. Analysis of
principal component loadings identifies key semantic features driving
classification performance, enhancing interpretability and predictive accuracy.
This methodology provides a scalable, data-driven framework for quantitatively
tracking emergent mercantilist dynamics through high-dimensional text analytics

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [360] [DeepWKB: Learning WKB Expansions of Invariant Distributions for Stochastic Systems](https://arxiv.org/abs/2508.09529)
*Yao Li,Yicheng Liu,Shirou Wang*

Main category: math.DS

TL;DR: This paper presents DeepWKB, a deep learning method to estimate invariant distributions of randomly perturbed systems using the WKB approximation, effective especially in small noise scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to compute the invariant distribution for high-dimensional, stochastic systems in the small noise regime, which is essential for studying rare events and system stability.

Method: The DeepWKB approach leverages Monte Carlo data and partial differential equations to compute components of the WKB approximation—quasi-potential and normalization factor—separately.

Result: DeepWKB successfully approximates invariant distributions in systems with high dimensions and non-trivial deterministic attractors.

Conclusion: DeepWKB is a scalable alternative for analyzing rare events and stochastic stability in complex systems, addressing challenges of prior methods in handling small noise scenarios.

Abstract: This paper introduces a novel deep learning method, called DeepWKB, for
estimating the invariant distribution of randomly perturbed systems via its
Wentzel-Kramers-Brillouin (WKB) approximation $u_\epsilon(x) = Q(\epsilon)^{-1}
Z_\epsilon(x) \exp\{-V(x)/\epsilon\}$, where $V$ is known as the
quasi-potential, $\epsilon$ denotes the noise strength, and $Q(\epsilon)$ is
the normalization factor. By utilizing both Monte Carlo data and the partial
differential equations satisfied by $V$ and $Z_\epsilon$, the DeepWKB method
computes $V$ and $Z_\epsilon$ separately. This enables an approximation of the
invariant distribution in the singular regime where $\epsilon$ is sufficiently
small, which remains a significant challenge for most existing methods.
Moreover, the DeepWKB method is applicable to higher-dimensional stochastic
systems whose deterministic counterparts admit non-trivial attractors. In
particular, it provides a scalable and flexible alternative for computing the
quasi-potential, which plays a key role in the analysis of rare events,
metastability, and the stochastic stability of complex systems.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [361] [TPTP World Infrastructure for Non-classical Logics](https://arxiv.org/abs/2508.09318)
*Alexander Steen,Geoff Sutcliffe*

Main category: cs.LO

TL;DR: The paper overviews the TPTP World infrastructure, particularly its extension for non-classical logics, including language, support tools, problems, solutions, and its application to quantified normal multi-modal logic.


<details>
  <summary>Details</summary>
Motivation: The need for a robust infrastructure to support research and development in Automated Theorem Proving (ATP) systems for non-classical logics.

Method: The paper describes extensions to the TPTP World, such as non-classical logic support, problem formulations, solution mechanisms, and associated tools.

Result: A comprehensive overview and functionality of the TPTP World infrastructure for ATP in non-classical logics, demonstrated through quantified normal multi-modal logic.

Conclusion: The extended TPTP World effectively supports Automated Theorem Proving in non-classical logics, broadening its applicability in research and development areas.

Abstract: The TPTP World is the well established infrastructure that supports research,
development, and deployment of Automated Theorem Proving (ATP) systems. The
TPTP World supports a range of classical logics, and since release v9.0.0 has
supported non-classical logics. This paper provides a self-contained
comprehensive overview of the TPTP World infrastructure for ATP in
non-classical logics: the non-classical language extension, problems and
solutions, and tool support. A detailed description of use of the
infrastructure for quantified normal multi-modal logic is given.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [362] [Robustness analysis of Deep Sky Objects detection models on HPC](https://arxiv.org/abs/2508.09831)
*Olivier Parisot,Diogo Ramalho Fernandes*

Main category: astro-ph.IM

TL;DR: This paper utilizes computer vision and deep learning models (YOLO, RET-DETR) with High-Performance Computing to enhance the detection of faint astronomical objects.


<details>
  <summary>Details</summary>
Motivation: The increasing volume of sky images from surveys and amateur astronomers requires accurate and automated processing methods for detecting faint Deep Sky Objects.

Method: The authors trained and compared computer vision models YOLO and RET-DETR on smart telescope images, leveraging High-Performance Computing to optimize computation and robustness testing.

Result: The study successfully demonstrated the implementation of detection models that mitigate the challenges of faint signals and complex backgrounds in astronomical image processing.

Conclusion: Deep learning and HPC technologies can significantly improve the detection accuracy and robustness for Deep Sky Objects in astronomical research.

Abstract: Astronomical surveys and the growing involvement of amateur astronomers are
producing more sky images than ever before, and this calls for automated
processing methods that are accurate and robust. Detecting Deep Sky Objects --
such as galaxies, nebulae, and star clusters -- remains challenging because of
their faint signals and complex backgrounds. Advances in Computer Vision and
Deep Learning now make it possible to improve and automate this process. In
this paper, we present the training and comparison of different detection
models (YOLO, RET-DETR) on smart telescope images, using High-Performance
Computing (HPC) to parallelise computations, in particular for robustness
testing.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [363] [Enhance the machine learning algorithm performance in phishing detection with keyword features](https://arxiv.org/abs/2508.09765)
*Zijiang Yang*

Main category: cs.CR

TL;DR: The paper proposes a novel feature selection method that enhances machine learning algorithms for detecting phishing websites by integrating keyword features with traditional ones, yielding improved accuracy and error reduction.


<details>
  <summary>Details</summary>
Motivation: The motivation is the increasing occurrences of phishing attacks on the Internet, which lead to sensitive information leaks and financial losses, highlighting the need for early detection of phishing URLs.

Method: The authors introduce a new feature selection technique that combines keyword features with traditional ones, applying it to various machine learning algorithms without relying on third-party information.

Result: The proposed method reduces classification errors by 30% on large datasets and shows greater enhancements on small datasets, achieving a maximum accuracy of 99.68% on certain algorithms.

Conclusion: Their approach effectively improves the detection of phishing websites, presenting a practical solution that is independent of third-party services and works on datasets of varying sizes.

Abstract: Recently, we can observe a significant increase of the phishing attacks in
the Internet. In a typical phishing attack, the attacker sets up a malicious
website that looks similar to the legitimate website in order to obtain the
end-users' information. This may cause the leakage of the sensitive information
and the financial loss for the end-users. To avoid such attacks, the early
detection of these websites' URLs is vital and necessary. Previous researchers
have proposed many machine learning algorithms to distinguish the phishing URLs
from the legitimate ones. In this paper, we would like to enhance these machine
learning algorithms from the perspective of feature selection. We propose a
novel method to incorporate the keyword features with the traditional features.
This method is applied on multiple traditional machine learning algorithms and
the experimental results have shown this method is useful and effective. On
average, this method can reduce the classification error by 30% for the large
dataset. Moreover, its enhancement is more significant for the small dataset.
In addition, this method extracts the information from the URL and does not
rely on the additional information provided by the third-part service. The best
result for the machine learning algorithm using our proposed method has
achieved the accuracy of 99.68%.

</details>


### [364] [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
*Aayush Gupta*

Main category: cs.CR

TL;DR: This paper introduces Contextual Integrity Verification (CIV), a cryptographic security architecture to protect LLMs from prompt injection attacks by enforcing deterministic trust hierarchies during inference.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are highly vulnerable to prompt injection and jailbreak attacks, necessitating robust security mechanisms beyond heuristic safeguards.

Method: The paper proposes CIV, which attaches cryptographic provenance labels to tokens and uses hard attention masking to enforce trust hierarchies among token representations during inference without the need for model fine-tuning.

Result: CIV achieves 0% attack success rate under the defined threat model, maintains 93.1% token-level similarity, does not degrade model performance on benign tasks, and operates with latency overhead due to a non-optimized data path.

Conclusion: CIV offers a lightweight and effective solution for securing frozen LLMs against prompt injection attacks, demonstrated through drop-in compatibility with existing models like Llama-3-8B and Mistral-7B, alongside tools for reproducible research.

Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

</details>


### [365] [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
*Zhifan Luo,Shuo Shao,Su Zhang,Lijing Zhou,Yuke Hu,Chenxu Zhao,Zhihao Liu,Zhan Qin*

Main category: cs.CR

TL;DR: The paper reveals privacy risks in Key-Value (KV) caches used in Large Language Models, introduces attack methods to exploit these vulnerabilities, and proposes KV-Cloak to defend against them.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored privacy risks associated with KV-caches in LLM inference, where sensitive user inputs could be reconstructed, demonstrating a significant security concern.

Method: The authors analyzed three attack vectors (Inversion, Collision, and Injection) to exploit KV-cache vulnerabilities and introduced KV-Cloak, a defense mechanism using matrix-based obfuscation and operator fusion.

Result: Experiments showed that KV-Cloak effectively blocked the proposed attacks, reduced reconstruction success to random noise, and maintained model accuracy with minimal performance trade-offs.

Conclusion: KV-Cloak is a practical, efficient, and robust mitigation for KV-cache privacy issues, making LLM deployments more secure without impacting performance.

Abstract: The Key-Value (KV) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (LLM) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the KV-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of KV-cache privacy leakage issues. To mitigate this,
we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.
KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the KV-cache. Our extensive experiments show that
KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy LLM deployment.

</details>


### [366] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CR

TL;DR: The paper introduces LoD, an unsupervised framework for detecting jailbreak attacks on Large Vision-Language Models using anomaly detection, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models (LVLMs) are vulnerable to jailbreak attacks, creating a significant safety risk. Current detection methods rely on heuristic-based internal representations, which yield suboptimal outcomes.

Method: The authors propose LoD that includes Multi-modal Safety Concept Activation Vectors (MSCAV) to extract safety representations and a Safety Pattern Auto-Encoder trained on safe inputs to identify anomalous (jailbreak) cases through reconstruction errors.

Result: LoD was tested on three LVLMs across five benchmarks and achieved a state-of-the-art AUROC of 0.9951, with up to a 38.89% improvement over baselines.

Conclusion: LoD successfully demonstrates accurate and unified detection of jailbreak attacks, showcasing its effectiveness as a robust anomaly detection framework for improving LVLM safety.

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. Although
recent detection works have shifted to internal representations due to their
rich cross-modal information, most methods rely on heuristic rules rather than
principled objectives, resulting in suboptimal performance. To address these
limitations, we propose Learning to Detect (LoD), a novel unsupervised
framework that formulates jailbreak detection as anomaly detection. LoD
introduces two key components: Multi-modal Safety Concept Activation Vectors
(MSCAV), which capture layer-wise safety-related representations across
modalities, and the Safety Pattern Auto-Encoder, which models the distribution
of MSCAV derived from safe inputs and detects anomalies via reconstruction
errors. By training the auto-encoder (AE) solely on safe samples without attack
labels, LoD naturally identifies jailbreak inputs as distributional anomalies,
enabling accurate and unified detection of jailbreak attacks. Comprehensive
experiments on three different LVLMs and five benchmarks demonstrate that LoD
achieves state-of-the-art performance, with an average AUROC of 0.9951 and an
improvement of up to 38.89% in the minimum AUROC over the strongest baselines.

</details>


### [367] [Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication](https://arxiv.org/abs/2508.09665)
*Ahmed Alharbi,Hai Dong,Xun Yi*

Main category: cs.CR

TL;DR: The paper addresses identity cloning in social-sensor clouds by introducing a novel detection and authentication method, significantly improving performance and scalability based on real-world data.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing frequency of identity cloning incidents within social-sensor cloud service providers and the shortcomings of existing detection approaches.

Method: The authors propose a technique combining a deep forest model for detecting similar identities and a cryptography-based authentication protocol to confirm if the identities originate from the same provider.

Result: Experiments on a large-scale real-world dataset demonstrate that the proposed method outperforms state-of-the-art approaches in detecting identity clones.

Conclusion: The study provides an effective and scalable solution for identity cloning detection in social-sensor clouds, setting the groundwork for improved security in such systems.

Abstract: Recent years have witnessed a rising trend in social-sensor cloud identity
cloning incidents. However, existing approaches suffer from unsatisfactory
performance, a lack of solutions for detecting duplicated accounts, and a lack
of large-scale evaluations on real-world datasets. We introduce a novel method
for detecting identity cloning in social-sensor cloud service providers. Our
proposed technique consists of two primary components: 1) a similar identity
detection method and 2) a cryptography-based authentication protocol.
Initially, we developed a weakly supervised deep forest model to identify
similar identities using non-privacy-sensitive user profile features provided
by the service. Subsequently, we designed a cryptography-based authentication
protocol to verify whether similar identities were generated by the same
provider. Our extensive experiments on a large real-world dataset demonstrate
the feasibility and superior performance of our technique compared to current
state-of-the-art identity clone detection methods.

</details>


### [368] [Demystifying the Role of Rule-based Detection in AI Systems for Windows Malware Detection](https://arxiv.org/abs/2508.09652)
*Andrea Ponte,Luca Demetrio,Luca Oneto,Ivan Tesfai Ogbu,Battista Biggio,Fabio Roli*

Main category: cs.CR

TL;DR: This paper explores integrating signature-based malware detection into machine learning training pipelines to improve robustness against adversarial samples and temporal drift.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in current AI-based malware detection systems, which are often developed in isolation and fail to exploit synergies between signature-based detection and machine learning.

Method: The researchers trained AI models on datasets excluding samples already identified by signature-based detection methods and compared the robustness of these models against comprehensive datasets.

Result: Models trained excluding signature-flagged samples exhibited enhanced resilience to adversarial samples and temporal drift but introduced a fixed false positive rate due to imperfect rule selection.

Conclusion: Integrating signature-based data in training pipelines enriches malware detection, but future works should aim to address fixed false positives and explore dynamic analysis for improved system resilience.

Abstract: Malware detection increasingly relies on AI systems that integrate
signature-based detection with machine learning. However, these components are
typically developed and combined in isolation, missing opportunities to reduce
data complexity and strengthen defenses against adversarial EXEmples, carefully
crafted programs designed to evade detection. Hence, in this work we
investigate the influence that signature-based detection exerts on model
training, when they are included inside the training pipeline. Specifically, we
compare models trained on a comprehensive dataset with an AI system whose
machine learning component is trained solely on samples not already flagged by
signatures. Our results demonstrate improved robustness to both adversarial
EXEmples and temporal data drift, although this comes at the cost of a fixed
lower bound on false positives, driven by suboptimal rule selection. We
conclude by discussing these limitations and outlining how future research
could extend AI-based malware detection to include dynamic analysis, thereby
further enhancing system resilience.

</details>


### [369] [Explainable Ensemble Learning for Graph-Based Malware Detection](https://arxiv.org/abs/2508.09801)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali A Ghorbani*

Main category: cs.CR

TL;DR: The paper introduces a stacking ensemble framework for malware detection using graph neural networks (GNNs) to improve performance and explainability in identifying malicious behavior.


<details>
  <summary>Details</summary>
Motivation: Malware detection demands models that are accurate, interpretable, and robust to evasion tactics, but existing graph-based methods often lack generalization and interpretability.

Method: The framework extracts control flow graphs (CFGs) from executables, uses diverse GNN base learners with distinct message-passing mechanisms, and aggregates predictions through an attention-based meta-learner. A post-hoc explanation approach further enhances interpretability.

Result: The proposed method showed improved malware classification performance and provided interpretable explanations of the detected malware's behavior.

Conclusion: The stacking ensemble framework advances malware detection by combining accuracy, robustness, and explainability, making it suitable for security-critical applications.

Abstract: Malware detection in modern computing environments demands models that are
not only accurate but also interpretable and robust to evasive techniques.
Graph neural networks (GNNs) have shown promise in this domain by modeling rich
structural dependencies in graph-based program representations such as control
flow graphs (CFGs). However, single-model approaches may suffer from limited
generalization and lack interpretability, especially in high-stakes security
applications. In this paper, we propose a novel stacking ensemble framework for
graph-based malware detection and explanation. Our method dynamically extracts
CFGs from portable executable (PE) files and encodes their basic blocks through
a two-step embedding strategy. A set of diverse GNN base learners, each with a
distinct message-passing mechanism, is used to capture complementary behavioral
features. Their prediction outputs are aggregated by a meta-learner implemented
as an attention-based multilayer perceptron, which both classifies malware
instances and quantifies the contribution of each base model. To enhance
explainability, we introduce an ensemble-aware post-hoc explanation technique
that leverages edge-level importance scores generated by a GNN explainer and
fuses them using the learned attention weights. This produces interpretable,
model-agnostic explanations aligned with the final ensemble decision.
Experimental results demonstrate that our framework improves classification
performance while providing insightful interpretations of malware behavior.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [370] [Deep Generative Models for Discrete Genotype Simulation](https://arxiv.org/abs/2508.09212)
*Sihan Xie,Thierry Tribout,Didier Boichard,Blaise Hanczar,Julien Chiquet,Eric Barrey*

Main category: q-bio.GN

TL;DR: The paper explores the use of generative models to simulate realistic genotype data, both unconditioned and phenotype-conditioned, focusing on the challenges posed by its discrete nature.


<details>
  <summary>Details</summary>
Motivation: To address data privacy and accessibility issues in genomic studies by creating realistic, simulated genotype data.

Method: Developed and adapted generative models like VAEs, Diffusion Models, and GANs for genotype data; conducted experiments on large-scale human and cow datasets; and evaluated models with established metrics from deep learning and genetics.

Result: The study finds that generative models can successfully capture genetic patterns and maintain genotype-phenotype associations.

Conclusion: The paper offers a comparative analysis of generative models for genotype simulation and presents practical guidelines for future research.

Abstract: Deep generative models open new avenues for simulating realistic genomic data
while preserving privacy and addressing data accessibility constraints. While
previous studies have primarily focused on generating gene expression or
haplotype data, this study explores generating genotype data in both
unconditioned and phenotype-conditioned settings, which is inherently more
challenging due to the discrete nature of genotype data. In this work, we
developed and evaluated commonly used generative models, including Variational
Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks
(GANs), and proposed adaptation tailored to discrete genotype data. We
conducted extensive experiments on large-scale datasets, including all
chromosomes from cow and multiple chromosomes from human. Model performance was
assessed using a well-established set of metrics drawn from both deep learning
and quantitative genetics literature. Our results show that these models can
effectively capture genetic patterns and preserve genotype-phenotype
association. Our findings provide a comprehensive comparison of these models
and offer practical guidelines for future research in genotype simulation. We
have made our code publicly available at
https://github.com/SihanXXX/DiscreteGenoGen.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [371] [Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery](https://arxiv.org/abs/2508.09183)
*Farzan Moosavi,Bilal Farooq*

Main category: quant-ph

TL;DR: The paper explores quantum computing to tackle the Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW) using a quantum-enhanced reinforcement learning approach, achieving improved scale and training complexity.


<details>
  <summary>Details</summary>
Motivation: Optimize last-mile on-demand delivery by overcoming the computational limitations of classical methods in solving NP-hard optimization problems, particularly for large-scale instances.

Method: Introduced a Reinforcement Learning framework integrated with a Parametrized Quantum Circuit (PQC) and designed a novel encoding quantum circuit. The study includes comparative analysis with Proximal Policy Optimization (PPO) and Quantum Singular Value Transformation (QSVT).

Result: The proposed method reduces travel time effectively and demonstrates superiority over traditional approaches in handling large-scale optimization with better training efficiency.

Conclusion: The study validates the capability of the quantum-powered RL framework in solving real-world optimization problems with enhanced performance in scale and efficiency.

Abstract: Quantum computation has demonstrated a promising alternative to solving the
NP-hard combinatorial problems. Specifically, when it comes to optimization,
classical approaches become intractable to account for large-scale solutions.
Specifically, we investigate quantum computing to solve the large-scale
Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this
regard, a Reinforcement Learning (RL) framework augmented with a Parametrized
Quantum Circuit (PQC) is designed to minimize the travel time in a realistic
last-mile on-demand delivery. A novel problem-specific encoding quantum circuit
with an entangling and variational layer is proposed. Moreover, Proximal Policy
Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are
designed for comparison through numerical experiments, highlighting the
superiority of the proposed method in terms of the scale of the solution and
training complexity while incorporating the real-world constraints.

</details>


### [372] [Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks](https://arxiv.org/abs/2508.09209)
*Kun Ming Goh*

Main category: quant-ph

TL;DR: This paper explores Hybrid Quantum-Classical GANs (HQCGANs) using noisy quantum circuits as latent priors in GANs, achieving competitive results compared to classical GANs.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations in GAN performance due to the quality of latent representations derived from classical noise distributions, by introducing quantum-generated latent vectors.

Method: The researchers implemented HQCGANs, involving quantum generators (with 3, 5, and 7 qubits) and a classical discriminator, then trained and evaluated them alongside a classical GAN on the binary MNIST dataset using realistic noise models in Qiskit's AerSimulator.

Result: The 7-qubit HQCGAN performed competitively with the classical GAN, especially in later epochs, although the classical GAN achieved the best overall scores. The 3-qubit HQCGAN faced limitations in early convergence.

Conclusion: Noisy quantum circuits can serve as feasible latent priors for GAN architectures, with moderate overhead and potential for performance enhancement in the noisy intermediate-scale quantum (NISQ) era.

Abstract: Generative adversarial networks (GANs) have emerged as a powerful paradigm
for producing high-fidelity data samples, yet their performance is constrained
by the quality of latent representations, typically sampled from classical
noise distributions. This study investigates hybrid quantum-classical GANs
(HQCGANs) in which a quantum generator, implemented via parameterised quantum
circuits, produces latent vectors for a classical discriminator. We evaluate a
classical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using
Qiskit's AerSimulator with realistic noise models to emulate near-term quantum
devices. The binary MNIST dataset (digits 0 and 1) is used to align with the
low-dimensional latent spaces imposed by current quantum hardware. Models are
trained for 150 epochs and assessed with Frechet Inception Distance (FID) and
Kernel Inception Distance (KID). Results show that while the classical GAN
achieved the best scores, the 7-qubit HQCGAN produced competitive performance,
narrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier
convergence limitations. Efficiency analysis indicates only moderate training
time increases despite quantum sampling overhead. These findings validate the
feasibility of noisy quantum circuits as latent priors in GAN architectures,
highlighting their potential to enhance generative modelling within the
constraints of the noisy intermediate-scale quantum (NISQ) era.

</details>


### [373] [On the Generalization Limits of Quantum Generative Adversarial Networks with Pure State Generators](https://arxiv.org/abs/2508.09844)
*Jasmin Frkatovic,Akash Malemath,Ivan Kankeu,Yannick Werner,Matthias Tschöpe,Vitor Fortes Rey,Sungho Suh,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: The paper examines the limitations of Quantum Generative Adversarial Networks (QGANs) in image generation tasks, highlighting challenges in generalization across datasets and providing theoretical insights.


<details>
  <summary>Details</summary>
Motivation: To explore and understand the performance and limitations of quantum generative models, particularly QGANs, in image generation.

Method: Extensive numerical testing of current QGAN architectures was conducted, alongside analytical derivation of a lower bound for discriminator quality based on the fidelity between the generator's output and target data.

Result: QGANs were found to struggle with generalizing across datasets, only converging on average training data representation. Analytical results explained limitations in discriminator quality using fidelity analysis.

Conclusion: Current quantum generative models, such as QGANs, have fundamental challenges in generalization, raising broader concerns for their application in generative tasks.

Abstract: We investigate the capabilities of Quantum Generative Adversarial Networks
(QGANs) in image generations tasks. Our analysis centers on fully quantum
implementations of both the generator and discriminator. Through extensive
numerical testing of current main architectures, we find that QGANs struggle to
generalize across datasets, converging on merely the average representation of
the training data. When the output of the generator is a pure-state, we
analytically derive a lower bound for the discriminator quality given by the
fidelity between the pure-state output of the generator and the target data
distribution, thereby providing a theoretical explanation for the limitations
observed in current models. Our findings reveal fundamental challenges in the
generalization capabilities of existing quantum generative models. While our
analysis focuses on QGANs, the results carry broader implications for the
performance of related quantum generative models.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [374] [Collective dynamics of strategic classification](https://arxiv.org/abs/2508.09340)
*Marta C. Couto,Flavia Barsotti,Fernando P. Santos*

Main category: cs.GT

TL;DR: The paper uses evolutionary game theory to study the feedback loop dynamics between users adapting their behavior to AI classifiers and institutions retraining these algorithms, particularly focusing on credit lending scenarios.


<details>
  <summary>Details</summary>
Motivation: AI classifiers are increasingly applied in high-stakes decisions, and users strategically adapting to them can create a feedback loop. Understanding this dynamic is crucial to mitigate adverse effects in areas like finance and healthcare.

Method: The paper applies evolutionary game theory to model user-institution interactions and tests scenarios such as algorithm robustness, gaming detection, and algorithmic recourse.

Result: Improved gaming detection lowers social costs; when perfect classifiers aren't feasible, algorithmic recourse leads to increased user improvement rates. Strict institutions offering actionable recourse reveal previously unexplored cycling dynamics.

Conclusion: Intervention strategies like enhanced detection and algorithmic recourse can steer system dynamics toward beneficial outcomes, highlighting the role of institutions' adaptation rates and strict recourse policies.

Abstract: Classification algorithms based on Artificial Intelligence (AI) are nowadays
applied in high-stakes decisions in finance, healthcare, criminal justice, or
education. Individuals can strategically adapt to the information gathered
about classifiers, which in turn may require algorithms to be re-trained. Which
collective dynamics will result from users' adaptation and algorithms'
retraining? We apply evolutionary game theory to address this question. Our
framework provides a mathematically rigorous way of treating the problem of
feedback loops between collectives of users and institutions, allowing to test
interventions to mitigate the adverse effects of strategic adaptation. As a
case study, we consider institutions deploying algorithms for credit lending.
We consider several scenarios, each representing different interaction
paradigms. When algorithms are not robust against strategic manipulation, we
are able to capture previous challenges discussed in the strategic
classification literature, whereby users either pay excessive costs to meet the
institutions' expectations (leading to high social costs) or game the algorithm
(e.g., provide fake information). From this baseline setting, we test the role
of improving gaming detection and providing algorithmic recourse. We show that
increased detection capabilities reduce social costs and could lead to users'
improvement; when perfect classifiers are not feasible (likely to occur in
practice), algorithmic recourse can steer the dynamics towards high users'
improvement rates. The speed at which the institutions re-adapt to the user's
population plays a role in the final outcome. Finally, we explore a scenario
where strict institutions provide actionable recourse to their unsuccessful
users and observe cycling dynamics so far unnoticed in the literature.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [375] [Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics](https://arxiv.org/abs/2508.09215)
*Kerem Delikoyun,Qianyu Chen,Liu Wei,Si Ko Myo,Johannes Krell,Martin Schlegel,Win Sen Kuan,John Tshon Yit Soong,Gerhard Schneider,Clarissa Prazeres da Costa,Percy A. Knolle,Laurent Renia,Matthew Edward Cove,Hwee Kuan Lee,Klaus Diepold,Oliver Hayden*

Main category: q-bio.QM

TL;DR: This paper introduces RT-HAD, a deep learning framework for real-time analysis of blood cell aggregates using holographic microscopy, achieving efficient and accurate detection within acceptable laboratory error rates.


<details>
  <summary>Details</summary>
Motivation: The need for improved diagnostic capabilities in haematology to identify rare blood cell aggregates without manual reviews or flagged results.

Method: RT-HAD employs deep learning to process holographic microscopy data, using physics-consistent reconstruction and graph-based cell representation.

Result: The framework processes large datasets on-the-fly (<1.5 min per 30 GB) with an 8.9% error rate for platelet aggregate detection.

Conclusion: RT-HAD offers a rapid and reliable solution to advance point-of-care diagnostics by overcoming big data challenges in haematology.

Abstract: While analysing rare blood cell aggregates remains challenging in automated
haematology, they could markedly advance label-free functional diagnostics.
Conventional flow cytometers efficiently perform cell counting with leukocyte
differentials but fail to identify aggregates with flagged results, requiring
manual reviews. Quantitative phase imaging flow cytometry captures detailed
aggregate morphologies, but clinical use is hampered by massive data storage
and offline processing. Incorporating hidden biomarkers into routine
haematology panels would significantly improve diagnostics without flagged
results. We present RT-HAD, an end-to-end deep learning-based image and data
processing framework for off-axis digital holographic microscopy (DHM), which
combines physics-consistent holographic reconstruction and detection,
representing each blood cell in a graph to recognize aggregates. RT-HAD
processes >30 GB of image data on-the-fly with turnaround time of <1.5 min and
error rate of 8.9% in platelet aggregate detection, which matches acceptable
laboratory error rates of haematology biomarkers and solves the big data
challenge for point-of-care diagnostics.

</details>


### [376] [Cross-BCI, A Cross-BCI-Paradigm Classifica-tion Model Towards Universal BCI Applications](https://arxiv.org/abs/2508.09242)
*Gaojie Zhou,Junhua Li*

Main category: q-bio.QM

TL;DR: This paper proposes a unified and lightweight decoding model for cross-BCI-paradigm classification, aiming to reduce complexity and enhance practical applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies and excessive costs in creating separate classification models for different BCI paradigms, while also aiming for models that are practical for deployment on portable devices.

Method: The method involves a three-stage approach: (1) tempo-spatial convolution to initiate feature extraction, (2) a multi-scale local feature selection module for isolating and weighting shared features across paradigms, and (3) a multi-dimensional global feature extraction module for high-level feature representation.

Result: The proposed model achieved performance metrics including 88.39% accuracy, 82.36% macro-precision, 80.01% macro-recall, and 0.8092 macro-F1-score, surpassing alternative models in cross-BCI paradigms (MI, SSVEP, and P300).

Conclusion: The study demonstrates that the model is effective for cross-BCI-paradigm classification. It offers a cost-efficient and foundational step towards advancing unified, portable, and widely applicable BCI systems.

Abstract: Classification models used in brain-computer interface (BCI) are usually
designed for a single BCI paradigm. This requires the redevelopment of the
model when applying it to a new BCI paradigm, resulting in repeated costs and
effort. Moreover, less complex deep learning models are desired for practical
usage, as well as for deployment on portable devices. In or-der to fill the
above gaps, we, in this study, proposed a light-weight and unified decoding
model for cross-BCI-paradigm classification. The proposed model starts with a
tempo-spatial convolution. It is followed by a multi-scale local feature
selec-tion module, aiming to extract local features shared across BCI paradigms
and generate weighted features. Finally, a mul-ti-dimensional global feature
extraction module is designed, in which multi-dimensional global features are
extracted from the weighted features and fused with the weighted features to
form high-level feature representations associated with BCI para-digms. The
results, evaluated on a mixture of three classical BCI paradigms (i.e., MI,
SSVEP, and P300), demon-strate that the proposed model achieves 88.39%, 82.36%,
80.01%, and 0.8092 for accuracy, macro-precision, mac-ro-recall, and
macro-F1-score, respectively, significantly out-performing the compared models.
This study pro-vides a feasible solution for cross-BCI-paradigm
classifica-tion. It lays a technological foundation for de-veloping a new
generation of unified decoding systems, paving the way for low-cost and
universal practical applications.

</details>


### [377] [Exploring Molecular Odor Taxonomies for Structure-based Odor Predictions using Machine Learning](https://arxiv.org/abs/2508.09217)
*Akshay Sajan,Stijn Sluis,Reza Haydarlou,Sanne Abeln,Pasquale Lisena,Raphael Troncy,Caro Verbeek,Inger Leemans,Halima Mouhib*

Main category: q-bio.QM

TL;DR: Predicting odor from molecular structure faces challenges due to limited understanding of odor space and complex structure-odor relationships. Machine learning models improve their predictive performance using expert and data-driven odor taxonomies.


<details>
  <summary>Details</summary>
Motivation: To address the limited knowledge of odor space and enhance machine learning predictions for odors based on molecular structure.

Method: The study uses two taxonomies—an expert taxonomy based on semantic and perceptual similarities, and a data-driven taxonomy derived from clustering co-occurrence patterns in the dataset.

Result: Both taxonomies significantly improved the predictive performance of machine learning models compared to random groupings, and inconsistencies within the taxonomies were analyzed.

Conclusion: The developed taxonomies enhance our understanding of the molecular odor space, improve predictions, and offer resources for community-driven exploration of smell mechanisms.

Abstract: One of the key challenges to predict odor from molecular structure is
unarguably our limited understanding of the odor space and the complexity of
the underlying structure-odor relationships. Here, we show that the predictive
performance of machine learning models for structure-based odor predictions can
be improved using both, an expert and a data-driven odor taxonomy. The expert
taxonomy is based on semantic and perceptual similarities, while the
data-driven taxonomy is based on clustering co-occurrence patterns of odor
descriptors directly from the prepared dataset. Both taxonomies improve the
predictions of different machine learning models and outperform random
groupings of descriptors that do not reflect existing relations between odor
descriptors. We assess the quality of both taxonomies through their predictive
performance across different odor classes and perform an in-depth error
analysis highlighting the complexity of odor-structure relationships and
identifying potential inconsistencies within the taxonomies by showcasing pear
odorants used in perfumery. The data-driven taxonomy allows us to critically
evaluate our expert taxonomy and better understand the molecular odor space.
Both taxonomies as well as a full dataset are made available to the community,
providing a stepping stone for a future community-driven exploration of the
molecular basis of smell. In addition, we provide a detailed multi-layer expert
taxonomy including a total of 777 different descriptors from the Pyrfume
repository.

</details>


### [378] [NEUBORN: The Neurodevelopmental Evolution framework Using BiOmechanical RemodelliNg](https://arxiv.org/abs/2508.09757)
*Nashira Baena,Mariana da Silva,Irina Grigorescu,Aakash Saboo,Saga Masui,Jaques-Donald Tournier,Emma C. Robinson*

Main category: q-bio.QM

TL;DR: The paper introduces a framework that models individual cortical development using biomechanically constrained, longitudinal image registration to improve biological plausibility and predictive modeling.


<details>
  <summary>Details</summary>
Motivation: Current frameworks struggle with capturing fine anatomical details in cortical development due to their reliance on population-average reference spaces.

Method: The authors propose a hierarchical network architecture utilizing biomechanically constrained, longitudinal, diffeomorphic image registration, trained on neonatal MRI data.

Result: The method generates growth trajectories that follow population-level trends, produces smoother anatomical warps, and reduces negative Jacobians compared to state-of-the-art methods.

Conclusion: The framework enhances interpretability in cortical development and opens up pathways for predicting brain maturation and identifying early malformations of cortical development.

Abstract: Understanding individual cortical development is essential for identifying
deviations linked to neurodevelopmental disorders. However, current normative
modelling frameworks struggle to capture fine-scale anatomical details due to
their reliance on modelling data within a population-average reference space.
Here, we present a novel framework for learning individual growth trajectories
from biomechanically constrained, longitudinal, diffeomorphic image
registration, implemented via a hierarchical network architecture. Trained on
neonatal MRI data from the Developing Human Connectome Project, the method
improves the biological plausibility of warps, generating growth trajectories
that better follow population-level trends while generating smoother warps,
with fewer negative Jacobians, relative to state-of-the-art baselines. The
resulting subject-specific deformations provide interpretable, biologically
grounded mappings of development. This framework opens new possibilities for
predictive modeling of brain maturation and early identification of
malformations of cortical development.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [379] [Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams](https://arxiv.org/abs/2508.09219)
*Wilder Baldwin,Sepideh Ghanavati,Manuel Woersdoerfer*

Main category: cs.CY

TL;DR: The paper studies ethical perceptions in AI roles through a survey of 414 participants, emphasizing role-sensitive approaches for inclusive solutions to AI ethics.


<details>
  <summary>Details</summary>
Motivation: To understand the varying levels of familiarity and practices related to AI ethics across roles and regions, and address the need for inclusive ethical guidelines.

Method: A mixed-method survey study combining statistical and qualitative analysis with 414 participants across 43 countries, covering diverse AI development roles.

Result: Identified differences in knowledge and practices related to AI ethics, emphasizing the importance of diverse roles and demographics in ethical decision-making.

Conclusion: The paper advocates tailored, inclusive approaches to AI ethics, promotes role-sensitive solutions, and suggests future research and educational strategies.

Abstract: Recent advances in AI applications have raised growing concerns about the
need for ethical guidelines and regulations to mitigate the risks posed by
these technologies. In this paper, we present a mixed-method survey study -
combining statistical and qualitative analyses - to examine the ethical
perceptions, practices, and knowledge of individuals involved in various AI
development roles. Our survey includes 414 participants from 43 countries,
representing roles such as AI managers, analysts, developers, quality assurance
professionals, and information security and privacy experts. The results reveal
varying degrees of familiarity and experience with AI ethics principles,
government initiatives, and risk mitigation strategies across roles, regions,
and other demographic factors. Our findings highlight the importance of a
collaborative, role-sensitive approach, involving diverse stakeholders in
ethical decision-making throughout the AI development lifecycle. We advocate
for developing tailored, inclusive solutions to address ethical challenges in
AI development, and we propose future research directions and educational
strategies to promote ethics-aware AI practices.

</details>


### [380] [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/abs/2508.09224)
*Yuan Yuan,Tina Sriskandarajah,Anna-Luisa Brakman,Alec Helyar,Alex Beutel,Andrea Vallone,Saachi Jain*

Main category: cs.CY

TL;DR: This paper discusses an approach to improve the safety training of large language models, particularly proposing 'safe-completions' over binary refusals.


<details>
  <summary>Details</summary>
Motivation: Traditional binary refusal boundaries in language models struggle with prompts that have unclear intent, especially in dual-use scenarios, leading to the need for a more nuanced approach.

Method: The paper introduces 'safe-completions,' a safety-training approach focusing on the safety of model responses instead of solely classifying user intents as malicious or benign.

Result: In applying 'safe-completions' to GPT-5, the approach improved safety, reduced residual safety failures, and increased helpfulness in both production comparisons and controlled experiments.

Conclusion: Safe-completions present a promising pathway to improving the safety and effectiveness of language models, especially for complex or dual-use prompts.

Abstract: Large Language Models used in ChatGPT have traditionally been trained to
learn a refusal boundary: depending on the user's intent, the model is taught
to either fully comply or outright refuse. While this is a strong mitigation
for explicitly malicious prompts, focusing safety training on refusals can lead
to brittleness for prompts with obscured user intent. Binary refusal boundaries
are especially ill-suited for dual-use cases (such as biology or
cybersecurity), where a user request can be answered safely at a high level,
but in some cases can lead to malicious uplift if sufficiently detailed or
actionable. As an alternative, we propose safe-completions: a safety-training
approach that centers on the safety of the assistant's output, rather than a
binary classification of the user's intent. Safe-completions seek to maximize
helpfulness within the safety policy's constraints. We incorporated this
approach into GPT-5 and find that across both production comparisons and
internally controlled experiments, safe-completion training improves safety
(especially on dual-use prompts), reduces the severity of residual safety
failures, and substantially increases model helpfulness.

</details>


### [381] [Beyond Technocratic XAI: The Who, What & How in Explanation Design](https://arxiv.org/abs/2508.09231)
*Ruchira Dhar,Stephanie Brandl,Ninell Oldenburg,Anders Søgaard*

Main category: cs.CY

TL;DR: This paper presents a framework for designing explainable AI (XAI) systems focused on contextual and ethical considerations, emphasizing explanation as a sociotechnical design process.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need to address the context-dependent challenges of generating meaningful and ethically responsible explanations for AI models.

Method: The authors propose a three-part framework based on design thinking, focusing on Who needs the explanation, What they need explained, and How to deliver it.

Result: The framework promotes a context-aware and ethically responsible approach to designing XAI systems.

Conclusion: Explanation in XAI should be treated as a sociotechnical design process to enhance communication, accountability, and transparency in ethically responsible ways.

Abstract: The field of Explainable AI (XAI) offers a wide range of techniques for
making complex models interpretable. Yet, in practice, generating meaningful
explanations is a context-dependent task that requires intentional design
choices to ensure accessibility and transparency. This paper reframes
explanation as a situated design process -- an approach particularly relevant
for practitioners involved in building and deploying explainable systems.
Drawing on prior research and principles from design thinking, we propose a
three-part framework for explanation design in XAI: asking Who needs the
explanation, What they need explained, and How that explanation should be
delivered. We also emphasize the need for ethical considerations, including
risks of epistemic inequality, reinforcing social inequities, and obscuring
accountability and governance. By treating explanation as a sociotechnical
design process, this framework encourages a context-aware approach to XAI that
supports effective communication and the development of ethically responsible
explanations.

</details>


### [382] [Ethical Medical Image Synthesis](https://arxiv.org/abs/2508.09293)
*Weina Jin,Ashish Sinha,Kumar Abhishek,Ghassan Hamarneh*

Main category: cs.CY

TL;DR: The paper focuses on ethical issues in medical image synthesis (MISyn), outlines intrinsic risks and limits, and suggests recommendations for ethical practices.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address ethical challenges in the research and development of medical image synthesis, identifying that synthetic images lack grounding in real phenomena and can introduce biases.

Method: The study includes a theoretical analysis of ethical concerns in MISyn, followed by ethical practice recommendations and oversight suggestions. Case studies are also provided to demonstrate practical application.

Result: Key ethical risks and limits are identified, recommendations for practice are developed, and gaps between current practices and the proposed ethical standards are highlighted.

Conclusion: To achieve ethical MISyn, acknowledgment of intrinsic limitations, incorporation of ethical guidelines in all phases, and collective contributions from various stakeholders are necessary.

Abstract: The task of ethical Medical Image Synthesis (MISyn) is to ensure that the
MISyn techniques are researched and developed ethically throughout their entire
lifecycle, which is essential to prevent the negative impacts of MISyn. To
address the ever-increasing needs and requirements for ethical practice of
MISyn research and development, we first conduct a theoretical analysis that
identifies the key properties of ethical MISyn and intrinsic limits of MISyn.
We identify that synthetic images lack inherent grounding in real medical
phenomena, cannot fully represent the training medical images, and inevitably
introduce new distribution shifts and biases.
  Ethical risks can arise from not acknowledging the intrinsic limits and
weaknesses of synthetic images compared to medical images, with the extreme
form manifested as misinformation of MISyn that substitutes synthetic images
for medical images without acknowledgment. The resulting ethical harms include
eroding trust in the medical imaging dataset environment and causing
algorithmic discrimination towards stakeholders and the public.
  To facilitate collective efforts towards ethical MISyn within and outside the
medical image analysis community, we then propose practical supports for
ethical practice in MISyn based on the theoretical analysis, including ethical
practice recommendations that adapt the existing technical standards, problem
formulation, design, and evaluation practice of MISyn to the ethical
challenges; and oversight recommendations to facilitate checks and balances
from stakeholders and the public. We also present two case studies that
demonstrate how to apply the ethical practice recommendations in practice, and
identify gaps between existing practice and the ethical practice
recommendations.

</details>


### [383] [STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports](https://arxiv.org/abs/2508.09853)
*Tegan McCaslin,Jide Alaga,Samira Nedungadi,Seth Donoughe,Tom Reed,Rishi Bommasani,Chris Painter,Luca Righetti*

Main category: cs.CY

TL;DR: The paper introduces STREAM, a standard for improved transparency in AI evaluation reports, focusing initially on ChemBio benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance public trust in AI by improving transparency and rigor in the reporting of dangerous AI capability evaluations.

Method: STREAM was developed in collaboration with 23 experts and offers best practices and a reporting template for clearer disclosures.

Result: STREAM provides gold-standard examples and practical tools to help AI developers and third parties improve and assess ChemBio evaluations.

Conclusion: This standard aims to foster trust and rigorous assessment in the evaluation of AI models, starting with ChemBio benchmarks.

Abstract: Evaluations of dangerous AI capabilities are important for managing
catastrophic risks. Public transparency into these evaluations - including what
they test, how they are conducted, and how their results inform decisions - is
crucial for building trust in AI development. We propose STREAM (A Standard for
Transparently Reporting Evaluations in AI Model Reports), a standard to improve
how model reports disclose evaluation results, initially focusing on chemical
and biological (ChemBio) benchmarks. Developed in consultation with 23 experts
across government, civil society, academia, and frontier AI companies, this
standard is designed to (1) be a practical resource to help AI developers
present evaluation results more clearly, and (2) help third parties identify
whether model reports provide sufficient detail to assess the rigor of the
ChemBio evaluations. We concretely demonstrate our proposed best practices with
"gold standard" examples, and also provide a three-page reporting template to
enable AI developers to implement our recommendations more easily.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [384] [Metrics for Assessing Changes in Flow-based Networks](https://arxiv.org/abs/2508.09573)
*Michał Rzepka,Piotr Chołda*

Main category: cs.NI

TL;DR: The paper introduces new metrics to evaluate network performance, focusing on flow impact and resource utilization.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of assessing network performance under fluctuating traffic while understanding peak data rate impacts on resources.

Method: Analysis of network data using percentile values, sample distributions, a new Utilization Score metric, and a Shapley value-based method.

Result: Effective metrics were identified to capture changes in network state caused by specific flows, with three being particularly insightful and easy to maintain.

Conclusion: The work offers a scalable framework for assessing network flow impact, providing valuable tools for research and practical application.

Abstract: This paper addresses the challenges of evaluating network performance in the
presence of fluctuating traffic patterns, with a particular focus on the impact
of peak data rates on network resources. We introduce a set of metrics to
quantify network load and measure the impact of individual flows on the overall
network state. By analyzing link and flow data through percentile values and
sample distributions, and introducing the Utilization Score metric, the
research provides insights into resource utilization under varying network
conditions. Furthermore, we employ a modified Shapley value-based approach to
measure the influence of individual flows on the network, offering a better
understanding of their contribution to network performance. The paper reviews
and compares 11 metrics across various network scenarios, evaluating their
practical relevance for research and development. Our evaluation demonstrates
that these metrics effectively capture changes in network state induced by
specific flows, with three of them offering a broad range of valuable insights
while remaining relatively easy to maintain. Moreover, the methodology
described in this paper serves as a framework for future research, with the
potential to expand and refine the set of metrics used to evaluate flow impact
on network performance.

</details>


### [385] [Agentic TinyML for Intent-aware Handover in 6G Wireless Networks](https://arxiv.org/abs/2508.09147)
*Alaa Saleh,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.NI

TL;DR: This paper introduces WAAN, an AI-driven framework for proactive network handovers using TinyML agents in 6G networks.


<details>
  <summary>Details</summary>
Motivation: Traditional reactive handover mechanisms in evolving 6G networks fail to effectively address issues in mobile edge computing and autonomous services.

Method: WAAN embeds lightweight TinyML agents across edge nodes for proactive handovers, employing semi-stable rendezvous points for continuity and context transfer.

Result: A case study on multimodal environmental control demonstrates WAAN's proficiency in maintaining user experience during mobility conditions.

Conclusion: The study discusses WAAN's potential in addressing mobility disruptions and outlines challenges and opportunities in its further development and deployment.

Abstract: As 6G networks evolve into increasingly AI-driven, user-centric ecosystems,
traditional reactive handover mechanisms demonstrate limitations, especially in
mobile edge computing and autonomous agent-based service scenarios. This
manuscript introduces WAAN, a cross-layer framework that enables intent-aware
and proactive handovers by embedding lightweight TinyML agents as autonomous,
negotiation-capable entities across heterogeneous edge nodes that contribute to
intent propagation and network adaptation. To ensure continuity across
mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points
that serve as coordination anchors for context transfer and state preservation.
The framework's operational capabilities are demonstrated through a multimodal
environmental control case study, highlighting its effectiveness in maintaining
user experience under mobility. Finally, the article discusses key challenges
and future opportunities associated with the deployment and evolution of WAAN.

</details>


### [386] [Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks](https://arxiv.org/abs/2508.09149)
*Seyed Hossein Ahmadpanah*

Main category: cs.NI

TL;DR: This paper introduces a Semantic-Aware Proactive LLM Orchestration (SP-LLM) framework using Predictive Digital Twin and Large Language Models for proactive vehicular edge computing management, outperforming traditional approaches in adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: Next-generation automotive applications require dynamic vehicular edge computing systems due to the limitations of fixed, reactive management methodologies in highly dynamic environments.

Method: It combines Predictive Digital Twin for forecasting network states and a Large Language Model as a cognitive orchestrator for proactive task offloading and resource allocation, based on high-level semantic commands.

Result: Extensive simulations demonstrate SP-LLM's superior scalability, robustness under volatile conditions, and adaptability compared to reactive and MARL-based methods.

Conclusion: SP-LLM provides a transformative solution for enabling intelligent, autonomous, and goal-driven vehicular networks, bridging human intent and optimal network behaviors in real-time scenarios.

Abstract: Next-generation automotive applications require vehicular edge computing
(VEC), but current management systems are essentially fixed and reactive. They
are suboptimal in extremely dynamic vehicular environments because they are
constrained to static optimization objectives and base their decisions on the
current network states. This paper presents a novel Semantic-Aware Proactive
LLM Orchestration (SP-LLM) framework to address these issues. Our method
transforms the traditional Digital Twin (DT) into a Predictive Digital Twin
(pDT) that predicts important network parameters such as task arrivals, vehicle
mobility, and channel quality. A Large Language Model (LLM) that serves as a
cognitive orchestrator is at the heart of our framework. It makes proactive,
forward-looking decisions about task offloading and resource allocation by
utilizing the pDT's forecasts. The LLM's ability to decipher high-level
semantic commands given in natural language is crucial because it enables it to
dynamically modify its optimization policy to match evolving strategic
objectives, like giving emergency services priority or optimizing energy
efficiency. We show through extensive simulations that SP-LLM performs
significantly better in terms of scalability, robustness in volatile
conditions, and adaptability than state-of-the-art reactive and MARL-based
approaches. More intelligent, autonomous, and goal-driven vehicular networks
will be possible due to our framework's outstanding capacity to convert human
intent into optimal network behavior.

</details>


### [387] [Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference](https://arxiv.org/abs/2508.09229)
*Danil Sivtsov,Aleksandr Katrutsa,Ivan Oseledets*

Main category: cs.NI

TL;DR: The paper proposes a topology-aware method for deploying Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently by using an integer linear program (ILP) to minimize network traffic during inference.


<details>
  <summary>Details</summary>
Motivation: With the rise of Mixture-of-Experts (MoE) LLMs, it is vital to address efficient deployment across many servers. The inherent model structure leads to imbalanced expert utilization, necessitating optimized placement strategies to enhance cluster utilization.

Method: The authors propose an integer linear program (ILP) to optimize the placement of experts in MoE LLMs. It minimizes network transmissions while considering network topology. The ILP can be efficiently solved using standard solvers.

Result: The ILP-based placement strategy demonstrates reduced network traffic compared to competitors, validated on two scales of models: DeepSeekMoE~16B and DeepSeek-R1~671B.

Conclusion: Topology-aware expert placement through ILP improves efficiency in deploying MoE LLMs, reducing network overhead and enhancing cluster performance during inference.

Abstract: Efficient deployment of a pre-trained LLM to a cluster with multiple servers
is a critical step for providing fast responses to users' queries. The recent
success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy
them efficiently, considering their underlying structure. During the inference
in MoE LLMs, only a small part of the experts is selected to process a given
token. Moreover, in practice, the experts' load is highly imbalanced. For
efficient deployment, one has to distribute the model across a large number of
servers using a model placement algorithm. Thus, to improve cluster
utilization, the model placement algorithm has to take into account the network
topology. This work focuses on the efficient topology-aware placement of the
pre-trained MoE LLMs in the inference stage. We propose an integer linear
program (ILP) that determines the optimal placement of experts, minimizing the
expected number of transmissions. Due to the internal structure, this
optimization problem can be solved with a standard ILP solver. We demonstrate
that ILP-based placement strategy yields lower network traffic than competitors
for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models.

</details>


### [388] [5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI](https://arxiv.org/abs/2508.09152)
*Joseph H. R. Isaac,Harish Saradagam,Nallamothu Pardhasaradhi*

Main category: cs.NI

TL;DR: The paper introduces an AI/ML-based Fault Analysis Engine to classify faults in 5G network traffic and suggest solutions using Generative AI, achieving high accuracy with reduced human intervention.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies and high human intervention in detecting faults in 5G network traffic using PCAP files.

Method: The FA Engine employs AI/ML methods, natural language processing, and Generative AI integrated with a Large Language Model trained on relevant 5G documents.

Result: High accuracy in classifying faulty and successful frames in test datasets, demonstrating improved efficiency in fault analysis.

Conclusion: The FA Engine effectively reduces time and personnel effort in analyzing 5G packet core traffic errors and suggests actionable solutions; scope for future development includes compatibility with 4G and other network types.

Abstract: With the advent of 5G networks and technologies, ensuring the integrity and
performance of packet core traffic is paramount. During network analysis, test
files such as Packet Capture (PCAP) files and log files will contain errors if
present in the system that must be resolved for better overall network
performance, such as connectivity strength and handover quality. Current
methods require numerous person-hours to sort out testing results and find the
faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine
designed to classify successful and faulty frames in PCAP files, specifically
within the 5G packet core. The FA engine analyses network traffic using natural
language processing techniques to identify anomalies and inefficiencies,
significantly reducing the effort time required and increasing efficiency. The
FA Engine also suggests steps to fix the issue using Generative AI via a Large
Language Model (LLM) trained on several 5G packet core documents. The engine
explains the details of the error from the domain perspective using documents
such as the 3GPP standards and user documents regarding the internal conditions
of the tests. Test results on the ML models show high classification accuracy
on the test dataset when trained with 80-20 splits for the successful and
failed PCAP files. Future scopes include extending the AI engine to incorporate
4G network traffic and other forms of network data, such as log text files and
multimodal systems.

</details>


### [389] [Agoran: An Agentic Open Marketplace for 6G RAN Automation](https://arxiv.org/abs/2508.09159)
*Ilias Chatzistefanidis,Navid Nikaein,Andrea Leone,Ali Maatouk,Leandros Tassioulas,Roberto Morabito,Ioannis Pitsiorlas,Marios Kountouris*

Main category: cs.NI

TL;DR: Agoran SRB introduces an AI-driven marketplace for mobile networks integrating stakeholders in decision-making, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Modern mobile networks face challenges in adjudicating conflicting goals between service owners. Current solutions lack flexibility and business context understanding.

Method: Agoran SRB employs three AI branches—Legislative, Executive, and Judicial—to ensure compliance, real-time monitoring, and trust evaluation. It includes Negotiation Agents and Mediator Agent to optimize and deploy network slice configurations.

Result: The approach leads to 37% higher throughput for eMBB slices, 73% lower latency for URLLC slices, and 8.3% reduction in PRB usage. It also demonstrates efficient performance with minimal memory and fast convergence using fine-tuned Llama AI models.

Conclusion: Agoran SRB offers a standards-aligned, stakeholder-centric strategy for managing 6G networks effectively, providing marked performance improvements and practical deployment potential.

Abstract: Next-generation mobile networks must reconcile the often-conflicting goals of
multiple service owners. However, today's network slice controllers remain
rigid, policy-bound, and unaware of the business context. We introduce Agoran
Service and Resource Broker (SRB), an agentic marketplace that brings
stakeholders directly into the operational loop. Inspired by the ancient Greek
agora, Agoran distributes authority across three autonomous AI branches: a
Legislative branch that answers compliance queries using retrieval-augmented
Large Language Models (LLMs); an Executive branch that maintains real-time
situational awareness through a watcher-updated vector database; and a Judicial
branch that evaluates each agent message with a rule-based Trust Score, while
arbitrating LLMs detect malicious behavior and apply real-time incentives to
restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator
Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective
optimizer, reaching a consensus intent in a single round, which is then
deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and
evaluated with realistic traces of vehicle mobility, Agoran achieved
significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73%
reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3%
saving in PRB usage compared to a static baseline. An 1B-parameter Llama model,
fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80%
of GPT-4.1's decision quality, while operating within 6 GiB of memory and
converging in only 1.3 seconds. These results establish Agoran as a concrete,
standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks.
A live demo is presented
https://www.youtube.com/watch?v=h7vEyMu2f5w\&ab_channel=BubbleRAN.

</details>


### [390] [webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design](https://arxiv.org/abs/2508.09171)
*D. Perera*

Main category: cs.NI

TL;DR: The paper introduces webMCP, a client-side standard for embedding structured metadata into web pages, which reduces AI agents' computational load by 67.6% while achieving high accuracy of 97.9%.


<details>
  <summary>Details</summary>
Motivation: AI-assisted web interactions are slow and expensive due to the need for extensive processing to understand web pages, creating barriers for users. This paper aims to overcome these inefficiencies.

Method: webMCP embeds interaction metadata into web pages, bypassing full HTML document processing, and provides direct mappings between page elements and user actions. A comprehensive evaluation assessed performance across online shopping, authentication, and content management scenarios.

Result: webMCP reduced processing requirements by 67.6%, achieved a task success rate of 97.9%, lowered user costs by 34-63%, and improved response times in web interactions. Statistical analysis confirmed its efficacy across multiple AI models.

Conclusion: webMCP represents an accessible, deployable solution for improving AI-assisted web interaction, effectively addressing computational inefficiencies and user needs. It requires no server-side changes, ensuring scalability across existing websites.

Abstract: Current AI agents create significant barriers for users by requiring
extensive processing to understand web pages, making AI-assisted web
interaction slow and expensive. This paper introduces webMCP (Web Machine
Context & Procedure), a client-side standard that embeds structured interaction
metadata directly into web pages, enabling more efficient human-AI
collaboration on existing websites. webMCP transforms how AI agents understand
web interfaces by providing explicit mappings between page elements and user
actions. Instead of processing entire HTML documents, agents can access
pre-structured interaction data, dramatically reducing computational overhead
while maintaining task accuracy. A comprehensive evaluation across 1,890 real
API calls spanning online shopping, authentication, and content management
scenarios demonstrates webMCP reduces processing requirements by 67.6% while
maintaining 97.9% task success rates compared to 98.8% for traditional
approaches. Users experience significantly lower costs (34-63% reduction) and
faster response times across diverse web interactions. Statistical analysis
confirms these improvements are highly significant across multiple AI models.
An independent WordPress deployment study validates practical applicability,
showing consistent improvements across real-world content management workflows.
webMCP requires no server-side modifications, making it deployable across
millions of existing websites without technical barriers. These results
establish webMCP as a viable solution for making AI web assistance more
accessible and sustainable, addressing the critical gap between user
interaction needs and AI computational requirements in production environments.

</details>


### [391] [HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting](https://arxiv.org/abs/2508.09184)
*Zineddine Bettouche,Khalid Ali,Andreas Fischer,Andreas Kassler*

Main category: cs.NI

TL;DR: The paper introduces HiSTM, a hierarchical spatiotemporal model for cellular traffic forecasting, achieving significant accuracy improvements and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate cellular traffic forecasting is challenging but crucial for network planning and resource allocation due to complex user mobility patterns.

Method: The authors developed HiSTM, which integrates a dual spatial encoder with a Mamba-based temporal module and employs state space methods for capturing spatial and temporal traffic patterns.

Result: HiSTM demonstrates a 29.4% improvement in Mean Absolute Error (MAE) compared to the STN baseline while utilizing 94% fewer parameters across evaluations using real-world datasets.

Conclusion: HiSTM offers an effective balance between accuracy and efficiency, generalizing well across datasets and showing superior long-term forecasting capabilities.

Abstract: Cellular traffic forecasting is essential for network planning, resource
allocation, or load-balancing traffic across cells. However, accurate
forecasting is difficult due to intricate spatial and temporal patterns that
exist due to the mobility of users. Existing AI-based traffic forecasting
models often trade-off accuracy and computational efficiency. We present
Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial
encoder with a Mamba-based temporal module and attention mechanism. HiSTM
employs selective state space methods to capture spatial and temporal patterns
in network traffic. In our evaluation, we use a real-world dataset to compare
HiSTM against several baselines, showing a 29.4% MAE improvement over the STN
baseline while using 94% fewer parameters. We show that the HiSTM generalizes
well across different datasets and improves in accuracy over longer
time-horizons.

</details>


### [392] [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
*Zainab Khan,Ahmed Hussain,Mukesh Thakur,Arto Hellas,Panos Papadimitratos*

Main category: cs.NI

TL;DR: The NEFMind framework uses fine-tuned LLMs to reduce API discovery complexities in telecommunications systems, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The exponential increase in APIs and NFs within 5G architectures creates operational challenges in service discovery and management.

Method: NEFMind uses synthetic dataset generation, Quantized-Low-Rank Adaptation for model optimization, and evaluates performance via metrics like GPT-4 Ref Score and BertScore.

Result: This approach reduced communication overhead by 85% and achieved 98-100% accuracy in identifying API calls using the Phi-2 model.

Conclusion: NEFMind demonstrates that parameter-efficient LLMs are effective for domain-specific API management, outperforming larger models in efficiency and accuracy for telecom networks.

Abstract: The use of Service-Based Architecture in modern telecommunications has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
communication overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for telecommunications infrastructure
deployment. These findings validate domain-specific, parameter-efficient LLM
strategies for managing complex API ecosystems in next-generation
telecommunications networks.

</details>


### [393] [MX-AI: Agentic Observability and Control Platform for Open and AI-RAN](https://arxiv.org/abs/2508.09197)
*Ilias Chatzistefanidis,Andrea Leone,Ali Yaghoubian,Mikel Irazabal,Sehad Nassim,Lina Bariah,Merouane Debbah,Navid Nikaein*

Main category: cs.NI

TL;DR: The paper introduces MX-AI, an AI-powered system for managing 6G RAN resources, demonstrating its efficiency in real-world scenarios with human-like expert performance.


<details>
  <summary>Details</summary>
Motivation: Enhance automation and intelligence in 6G RAN systems by leveraging AI technologies for observability and control.

Method: MX-AI utilizes a testbed based on OAI and FlexRIC, integrates LLM-powered agents into the SMO layer, and facilitates natural-language interaction for resource management.

Result: MX-AI achieves high-quality answers (mean score of 4.1/5.0), perfect decision-action accuracy (100%) across operational queries, and low latency (8.8 seconds).

Conclusion: The system proves practical for real-world applications, matching human expert capabilities and advancing research on AI-native RANs.

Abstract: Future 6G radio access networks (RANs) will be artificial intelligence
(AI)-native: observed, reasoned about, and re-configured by autonomous agents
cooperating across the cloud-edge continuum. We introduce MX-AI, the first
end-to-end agentic system that (i) instruments a live 5G Open RAN testbed based
on OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of
Large-Language-Model (LLM)-powered agents inside the Service Management and
Orchestration (SMO) layer, and (iii) exposes both observability and control
functions for 6G RAN resources through natural-language intents. On 50
realistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0
and 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end
latency when backed by GPT-4.1. Thus, it matches human-expert performance,
validating its practicality in real settings. We publicly release the agent
graph, prompts, and evaluation harness to accelerate open research on AI-native
RANs. A live demo is presented here:
https://www.youtube.com/watch?v=CEIya7988Ug&t=285s&ab_channel=BubbleRAN

</details>


### [394] [CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge](https://arxiv.org/abs/2508.09208)
*Muqing Li,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.NI

TL;DR: The paper introduces CoMoE, a framework to optimize the deployment of Mixture-of-Experts (MoE) language models in mobile edge computing environments, reducing memory usage and inference latency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) require significant memory and computational resources, making it challenging to deploy them in resource-constrained mobile edge computing environments. Mixture-of-Experts (MoE) models offer scalability but introduce issues like high memory demands and dynamic expert activation. Thus, an efficient deployment framework is needed.

Method: The proposed framework, CoMoE, dynamically optimizes expert aggregation granularity and offloading strategies based on real-time factors such as device resource states, network conditions, and input characteristics. It incorporates expert parameter techniques (merging, distillation, sharing decomposition) and expert offloading strategies (prediction, prefetching, caching, scheduling) to handle MoE models in edge environments.

Result: CoMoE achieves significant improvements in deployment efficiency, reducing memory usage by approximately 70%, lowering inference latency by 10.5%, and enabling the deployment of large-scale MoE models in mobile edge scenarios. For example, memory requirements for a 7.4B-parameter model are reduced from 15.6GB to 4.7GB.

Conclusion: CoMoE successfully addresses the resource constraints of mobile edge devices, enabling the deployment of large-scale MoE models without compromising performance. It demonstrates the practicality of resource-aware strategies in dynamic environments for efficient and scalable model usage.

Abstract: The proliferation of large language models (LLMs) has driven the adoption of
Mixture-of-Experts (MoE) architectures as a promising solution to scale model
capacity while controlling computational costs. However, deploying MoE models
in resource-constrained mobile edge computing environments presents significant
challenges due to their large memory footprint and dynamic expert activation
patterns. To address these challenges, we propose a novel dynamic
resource-aware collaborative optimization framework that jointly optimizes
expert aggregation granularity and offloading strategies based on real-time
device resource states, network conditions, and input characteristics in mobile
edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze
existing expert aggregation techniques, including expert parameter
merging,knowledge distillation,and parameter sharing decomposition, identifying
their limitations in dynamic mobile environments.We then investigate expert
offloading strategies encompassing expert prediction and prefetching, expert
caching and scheduling, and multi-tier storage architectures, revealing the
interdependencies between routing decisions and offloading performance.The
CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility
and varying network conditions, enabling efficient MoE deployment across
heterogeneous edge devices. Extensive experiments on real mobile edge testbeds
demonstrate that CoMoE achieves approximately 70% reduction in memory usage
compared to baseline methods, 10.5% lower inference latency than existing
expert offloading techniques, while maintaining model performance stability.
For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE
reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on
resource-constrained mobile edge devices that previously could only support
much smaller models.

</details>


### [395] [Anomaly Detection for IoT Global Connectivity](https://arxiv.org/abs/2508.09660)
*Jesus Omaña Iglesias,Carlos Segura Perales,Stefan Geißler,Diego Perino,Andra Lutu*

Main category: cs.NI

TL;DR: The paper introduces ANCHOR, an unsupervised anomaly detection solution aimed at enhancing IoT service reliability by proactively identifying connectivity issues.


<details>
  <summary>Details</summary>
Motivation: IoT service providers face challenges with guaranteeing reliable communication across complex ecosystems and often rely on reactive approaches to connectivity issues, leading to compromised service quality.

Method: The paper outlines statistical rules, machine-learning models, and deep-learning approaches for detecting anomalies based on passive signaling traffic, tailored to IoT verticals.

Result: The solution, ANCHOR, was evaluated on operational IoT customers, showing its capability to identify problematic clients proactively before service degradation occurs.

Conclusion: ANCHOR assists network engineers in ensuring consistent IoT service quality by proactively detecting and mitigating connectivity anomalies, proving its value for operational platforms.

Abstract: Internet of Things (IoT) application providers rely on Mobile Network
Operators (MNOs) and roaming infrastructures to deliver their services
globally. In this complex ecosystem, where the end-to-end communication path
traverses multiple entities, it has become increasingly challenging to
guarantee communication availability and reliability. Further, most platform
operators use a reactive approach to communication issues, responding to user
complaints only after incidents have become severe, compromising service
quality. This paper presents our experience in the design and deployment of
ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity
service of a large global roaming platform. ANCHOR assists engineers by
filtering vast amounts of data to identify potential problematic clients (i.e.,
those with connectivity issues affecting several of their IoT devices),
enabling proactive issue resolution before the service is critically impacted.
We first describe the IoT service, infrastructure, and network visibility of
the IoT connectivity provider we operate. Second, we describe the main
challenges and operational requirements for designing an unsupervised anomaly
detection solution on this platform. Following these guidelines, we propose
different statistical rules, and machine- and deep-learning models for IoT
verticals anomaly detection based on passive signaling traffic. We describe the
steps we followed working with the operational teams on the design and
evaluation of our solution on the operational platform, and report an
evaluation on operational IoT customers.

</details>
