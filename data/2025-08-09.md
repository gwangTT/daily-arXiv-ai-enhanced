<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 18]
- [cs.CV](#cs.CV) [Total: 18]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.LG](#cs.LG) [Total: 26]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 18]
- [cs.SE](#cs.SE) [Total: 12]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.SI](#cs.SI) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prescriptive Agents based on Rag for Automated Maintenance (PARAM)](https://arxiv.org/abs/2508.04714)
*Chitranshu Harbola,Anupam Purwar*

Main category: cs.AI

TL;DR: This paper introduces an intelligent prescriptive maintenance system using Large Language Models (LLMs) for analyzing bearing vibration data and generating actionable maintenance recommendations beyond anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To prevent machinery failures, optimize operational efficiency, and address the gap between condition monitoring and actionable maintenance planning.

Method: The proposed system uses LLMs to process serialized bearing vibration data for anomaly detection and fault classification. Also, it utilizes multi-agentic mechanisms for analyzing maintenance manuals and web retrieval, generating actionable and structured maintenance recommendations.

Result: Experimental validation shows effective anomaly detection and maintenance guidance for bearings, leveraging advanced integration of LLMs and contextual data processing.

Conclusion: The approach provides intelligent decision support that scales across industrial components and sectors, advancing the use of LLMs in industrial maintenance.

Abstract: Industrial machinery maintenance requires timely intervention to prevent
catastrophic failures and optimize operational efficiency. This paper presents
an integrated Large Language Model (LLM)-based intelligent system for
prescriptive maintenance that extends beyond traditional anomaly detection to
provide actionable maintenance recommendations. Building upon our prior LAMP
framework for numerical data analysis, we develop a comprehensive solution that
combines bearing vibration frequency analysis with multi agentic generation for
intelligent maintenance planning. Our approach serializes bearing vibration
data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM
processing, enabling few-shot anomaly detection with high accuracy. The system
classifies fault types (inner race, outer race, ball/roller, cage faults) and
assesses severity levels. A multi-agentic component processes maintenance
manuals using vector embeddings and semantic search, while also conducting web
searches to retrieve comprehensive procedural knowledge and access up-to-date
maintenance practices for more accurate and in-depth recommendations. The
Gemini model then generates structured maintenance recommendations includes
immediate actions, inspection checklists, corrective measures, parts
requirements, and timeline specifications. Experimental validation in bearing
vibration datasets demonstrates effective anomaly detection and contextually
relevant maintenance guidance. The system successfully bridges the gap between
condition monitoring and actionable maintenance planning, providing industrial
practitioners with intelligent decision support. This work advances the
application of LLMs in industrial maintenance, offering a scalable framework
for prescriptive maintenance across machinery components and industrial
sectors.

</details>


### [2] [GeoFlow: Agentic Workflow Automation for Geospatial Tasks](https://arxiv.org/abs/2508.04719)
*Amulya Bhattaram,Justin Chung,Stanley Chung,Ranit Gupta,Janani Ramamoorthy,Kartikeya Gullapalli,Diana Marculescu,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: GeoFlow is a method designed to enhance agent-driven workflows for geospatial tasks by optimizing API invocation and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in geospatial tasks, specifically improving detailed tool-calling for APIs to enhance agent success.

Method: GeoFlow provides agents with explicit tool-calling objectives for invoking geospatial APIs, moving beyond reasoning decomposition approaches.

Result: GeoFlow achieves a 6.8% increase in agentic success and significantly lowers token usage by up to fourfold compared to leading methods.

Conclusion: GeoFlow demonstrates superior performance in geospatial task workflows, making it a more efficient and effective approach compared to state-of-the-art techniques.

Abstract: We present GeoFlow, a method that automatically generates agentic workflows
for geospatial tasks. Unlike prior work that focuses on reasoning decomposition
and leaves API selection implicit, our method provides each agent with detailed
tool-calling objectives to guide geospatial API invocation at runtime. GeoFlow
increases agentic success by 6.8% and reduces token usage by up to fourfold
across major LLM families compared to state-of-the-art approaches.

</details>


### [3] [Who is a Better Player: LLM against LLM](https://arxiv.org/abs/2508.04720)
*Yingjie Zhou,Jiezhang Cao,Farong Wen,Li Xu,Yanwei Jiang,Jun Jia,Ronghui Li,Xiaohong Liu,Yu Zhou,Xiongkuo Min,Jie Guo,Zicheng Zhang,Guangtao Zhai*

Main category: cs.AI

TL;DR: This paper introduces Qi Town, an adversarial evaluation framework that assesses the performance of Large Language Models (LLMs) through competitive board games, using metrics like Elo rating, Performance Loop Graph (PLG), and Positive Sentiment Score (PSS).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional Q&A benchmarks for assessing LLMs by offering a more comprehensive evaluation through the framework of adversarial board games.

Method: The authors developed Qi Town, a platform supporting 5 board games and 20 LLM-driven players. They utilize Elo ratings for skill evaluation, PLG for analyzing cyclic skill fluctuations, and PSS for gauging mental fitness during competition.

Result: The study found that LLMs generally exhibit optimism in high-stress environments but demonstrate instability in gameplay skills, as revealed by PLG analyses.

Conclusion: While LLMs show adaptability and optimism during adversarial challenges, the inconsistencies in gameplay performance highlight areas that need further investigation for enhancing strategic reasoning.

Abstract: Adversarial board games, as a paradigmatic domain of strategic reasoning and
intelligence, have long served as both a popular competitive activity and a
benchmark for evaluating artificial intelligence (AI) systems. Building on this
foundation, we propose an adversarial benchmarking framework to assess the
comprehensive performance of Large Language Models (LLMs) through board games
competition, compensating the limitation of data dependency of the mainstream
Question-and-Answer (Q&A) based benchmark method. We introduce Qi Town, a
specialized evaluation platform that supports 5 widely played games and
involves 20 LLM-driven players. The platform employs both the Elo rating system
and a novel Performance Loop Graph (PLG) to quantitatively evaluate the
technical capabilities of LLMs, while also capturing Positive Sentiment Score
(PSS) throughout gameplay to assess mental fitness. The evaluation is
structured as a round-robin tournament, enabling systematic comparison across
players. Experimental results indicate that, despite technical differences,
most LLMs remain optimistic about winning and losing, demonstrating greater
adaptability to high-stress adversarial environments than humans. On the other
hand, the complex relationship between cyclic wins and losses in PLGs exposes
the instability of LLMs' skill play during games, warranting further
explanation and exploration.

</details>


### [4] [Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)](https://arxiv.org/abs/2508.04846)
*Mahdi Nazari Ashani,Ali Asghar Alesheikh,Saba Kazemi,Kimya Kheirkhah,Yasin Mohammadi,Fatemeh Rezaie,Amir Mahdi Manafi,Hedieh Zarkesh*

Main category: cs.AI

TL;DR: This paper evaluates three methods for enabling autonomous web-based GIS, focusing on client-side small language models, achieving high accuracy and eliminating server reliance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for intuitive, private, and scalable autonomous geographical information systems by overcoming reliance on cloud-based models.

Method: Three approaches are compared: cloud-based LLMs, semi-automated classical machine learning classifiers, and offline client-side small language models fine-tuned for use in browsers.

Result: Client-side small language models, specifically a fine-tuned T5-small model, demonstrated the highest accuracy (e.g., exact matching: 0.93, ROUGE-1: 0.98) and eliminated backend dependencies.

Conclusion: Client-side browser-executable models are a viable and efficient solution for AWebGIS, providing high accuracy while addressing privacy and scalability issues.

Abstract: Autonomous web-based geographical information systems (AWebGIS) aim to
perform geospatial operations from natural language input, providing intuitive,
intelligent, and hands-free interaction. However, most current solutions rely
on cloud-based large language models (LLMs), which require continuous internet
access and raise users' privacy and scalability issues due to centralized
server processing. This study compares three approaches to enabling AWebGIS:
(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)
a semi-automated offline method using classical machine learning classifiers
such as support vector machine and random forest; and (3) a fully autonomous
offline (client-side) method based on a fine-tuned small language model (SLM),
specifically T5-small model, executed in the client's web browser. The third
approach, which leverages SLMs, achieved the highest accuracy among all
methods, with an exact matching accuracy of 0.93, Levenshtein similarity of
0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L
scores of 0.98. Crucially, this client-side computation strategy reduces the
load on backend servers by offloading processing to the user's device,
eliminating the need for server-based inference. These results highlight the
feasibility of browser-executable models for AWebGIS solutions.

</details>


### [5] [Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning](https://arxiv.org/abs/2508.04848)
*Chang Tian,Matthew B. Blaschko,Mingzhe Xing,Xiuxing Li,Yinliang Yue,Marie-Francine Moens*

Main category: cs.AI

TL;DR: While reinforcement learning enhances reasoning abilities of large language models in ideal scenarios, their performance drops significantly in non-ideal ones such as summary inference, noise suppression, and contextual filtering. Current methods remain insufficient for resolving these deficits.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in evaluating the reasoning abilities of large language models under realistic, non-ideal scenarios, reflecting practical challenges.

Method: The authors formally define and evaluate non-ideal reasoning scenarios, fine-tune three large language models and one large vision-language model using a policy-gradient algorithm, and test them across eight datasets.

Result: Fine-tuned models improved baseline reasoning in idealized settings but displayed significant performance declines under non-ideal scenarios, despite attempts at scenario-specific remediation.

Conclusion: The current methods overstate the reasoning abilities of large models, emphasizing the necessity of evaluating models under real-world, non-ideal conditions to uncover critical limitations.

Abstract: Reinforcement learning (RL) has become a key technique for enhancing the
reasoning abilities of large language models (LLMs), with policy-gradient
algorithms dominating the post-training stage because of their efficiency and
effectiveness. However, most existing benchmarks evaluate large-language-model
reasoning under idealized settings, overlooking performance in realistic,
non-ideal scenarios. We identify three representative non-ideal scenarios with
practical relevance: summary inference, fine-grained noise suppression, and
contextual filtering. We introduce a new research direction guided by
brain-science findings that human reasoning remains reliable under imperfect
inputs. We formally define and evaluate these challenging scenarios. We
fine-tune three LLMs and a state-of-the-art large vision-language model (LVLM)
using RL with a representative policy-gradient algorithm and then test their
performance on eight public datasets. Our results reveal that while RL
fine-tuning improves baseline reasoning under idealized settings, performance
declines significantly across all three non-ideal scenarios, exposing critical
limitations in advanced reasoning capabilities. Although we propose a
scenario-specific remediation method, our results suggest current methods leave
these reasoning deficits largely unresolved. This work highlights that the
reasoning abilities of large models are often overstated and underscores the
importance of evaluating models under non-ideal scenarios. The code and data
will be released at XXXX.

</details>


### [6] [ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis](https://arxiv.org/abs/2508.04915)
*Huiya Zhao,Yinghao Zhu,Zixiang Wang,Yasha Wang,Junyi Gao,Liantao Ma*

Main category: cs.AI

TL;DR: The paper introduces HealthFlow, a self-evolving AI agent for healthcare problem-solving, along with a benchmark called EHRFlowBench. Experiments show HealthFlow significantly outperforms current frameworks.


<details>
  <summary>Details</summary>
Motivation: Current AI agents in healthcare rely on static strategies, limiting their ability to develop high-level strategic planning skills for complex domains.

Method: The authors propose HealthFlow, an AI agent that uses a meta-level evolution mechanism to refine strategic policies based on successes and failures. They also introduce EHRFlowBench for evaluation.

Result: Experimental findings show HealthFlow's self-evolving approach surpasses state-of-the-art agent frameworks in tasks derived from realistic health data.

Conclusion: HealthFlow represents a shift toward creating smarter, autonomous AI agents that can independently improve their high-level task-management skills, enhancing scientific discovery in healthcare.

Abstract: The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.

</details>


### [7] [The Docking Game: Loop Self-Play for Fast, Dynamic, and Accurate Prediction of Flexible Protein--Ligand Binding](https://arxiv.org/abs/2508.05006)
*Youzhi Zhang,Yufei Li,Gaofeng Meng,Hongbin Liu,Jiebo Luo*

Main category: cs.AI

TL;DR: The paper presents Loop Self-Play (LoopPlay), a game-theoretic algorithm that enhances molecular docking prediction by treating it as a Docking Game, achieving a 10% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: Current multi-task learning models underperform in ligand docking compared to protein pocket docking due to structural differences, necessitating new approaches.

Method: Proposes a game-theoretic Docking Game framework, where ligand and protein docking modules are modeled as players, optimized using LoopPlay that alternates between mutual adaptation and individual refinement loops.

Result: LoopPlay achieves about 10% improvement in predicting accurate binding modes over state-of-the-art methods, as validated on benchmark datasets.

Conclusion: LoopPlay demonstrates strong potential in advancing molecular docking, making it valuable for drug discovery applications.

Abstract: Molecular docking is a crucial aspect of drug discovery, as it predicts the
binding interactions between small-molecule ligands and protein pockets.
However, current multi-task learning models for docking often show inferior
performance in ligand docking compared to protein pocket docking. This
disparity arises largely due to the distinct structural complexities of ligands
and proteins. To address this issue, we propose a novel game-theoretic
framework that models the protein-ligand interaction as a two-player game
called the Docking Game, with the ligand docking module acting as the ligand
player and the protein pocket docking module as the protein player. To solve
this game, we develop a novel Loop Self-Play (LoopPlay) algorithm, which
alternately trains these players through a two-level loop. In the outer loop,
the players exchange predicted poses, allowing each to incorporate the other's
structural predictions, which fosters mutual adaptation over multiple
iterations. In the inner loop, each player dynamically refines its predictions
by incorporating its own predicted ligand or pocket poses back into its model.
We theoretically show the convergence of LoopPlay, ensuring stable
optimization. Extensive experiments conducted on public benchmark datasets
demonstrate that LoopPlay achieves approximately a 10\% improvement in
predicting accurate binding modes compared to previous state-of-the-art
methods. This highlights its potential to enhance the accuracy of molecular
docking in drug discovery.

</details>


### [8] [Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses](https://arxiv.org/abs/2508.05009)
*Bin Han,Robert Wolfe,Anat Caspi,Bill Howe*

Main category: cs.AI

TL;DR: The paper investigates the use of large language models (LLMs) for integrating noisy urban spatial datasets, comparing effectiveness to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for spatial data integration struggle with edge cases and require manual intervention, while machine learning demands extensive labeled data. This paper aims to test LLMs as a potential solution.

Method: The authors analyzed LLMs' reasoning about spatial relationships, tested how reducing dependence on spatial reasoning impacts performance, and applied a review-and-refine approach for improving responses.

Result: LLMs exhibit spatial reasoning but fail on computational geometry tasks; however, refining their input features significantly boosts their performance.

Conclusion: LLMs emerge as a promising alternative to rule-based heuristics for spatial data integration, with scope for enhancement through post-training and multi-modal methods.

Abstract: We explore the application of large language models (LLMs) to empower domain
experts in integrating large, heterogeneous, and noisy urban spatial datasets.
Traditional rule-based integration methods are unable to cover all edge cases,
requiring manual verification and repair. Machine learning approaches require
collecting and labeling of large numbers of task-specific samples. In this
study, we investigate the potential of LLMs for spatial data integration. Our
analysis first considers how LLMs reason about environmental spatial
relationships mediated by human experience, such as between roads and
sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they
struggle to connect the macro-scale environment with the relevant computational
geometry tasks, often producing logically incoherent responses. But when
provided relevant features, thereby reducing dependence on spatial reasoning,
LLMs are able to generate high-performing results. We then adapt a
review-and-refine method, which proves remarkably effective in correcting
erroneous initial responses while preserving accurate responses. We discuss
practical implications of employing LLMs for spatial data integration in
real-world contexts and outline future research directions, including
post-training, multi-modal integration methods, and support for diverse data
formats. Our findings position LLMs as a promising and flexible alternative to
traditional rule-based heuristics, advancing the capabilities of adaptive
spatial data integration.

</details>


### [9] [Cognitive Duality for Adaptive Web Agents](https://arxiv.org/abs/2508.05081)
*Jiarun Liu,Chunhong Zhang,Zheng Hu*

Main category: cs.AI

TL;DR: The paper introduces CogniWeb, an autonomous web navigation agent inspired by dual-process theory, combining fast intuitive processing and slow deliberative reasoning to achieve competitive success rates and higher efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of building autonomous web agents capable of effective decision-making in complex, dynamic environments, and integrates offline imitation learning with online exploration.

Method: The method involves using a dual-process cognitive framework to design CogniWeb, which toggles between fast intuitive processing (System 1) and slow deliberative reasoning (System 2) based on task complexity.

Result: CogniWeb achieved a 43.96% success rate and a 75% reduction in token usage during evaluation on WebArena, demonstrating both competitive performance and efficiency.

Conclusion: The dual-process framework implemented in CogniWeb provides an effective approach for web navigation tasks, combining intuitive offline behaviors with deliberative online planning for optimal efficiency and performance.

Abstract: Web navigation represents a critical and challenging domain for evaluating
artificial general intelligence (AGI), demanding complex decision-making within
high-entropy, dynamic environments with combinatorially explosive action
spaces. Current approaches to building autonomous web agents either focus on
offline imitation learning or online exploration, but rarely integrate both
paradigms effectively. Inspired by the dual-process theory of human cognition,
we derive a principled decomposition into fast System 1 and slow System 2
cognitive processes. This decomposition provides a unifying perspective on
existing web agent methodologies, bridging the gap between offline learning of
intuitive reactive behaviors and online acquisition of deliberative planning
capabilities. We implement this framework in CogniWeb, a modular agent
architecture that adaptively toggles between fast intuitive processing and
deliberate reasoning based on task complexity. Our evaluation on WebArena
demonstrates that CogniWeb achieves competitive performance (43.96% success
rate) while maintaining significantly higher efficiency (75% reduction in token
usage).

</details>


### [10] [MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.05083)
*Dexuan Xu,Jieyi Wang,Zhongyan Chai,Yongzhi Cao,Hanpin Wang,Huamin Zhang,Yu Huang*

Main category: cs.AI

TL;DR: The paper introduces MedMKEB, a benchmark for evaluating medical multimodal language model editing capabilities across various criteria and editing tasks.


<details>
  <summary>Details</summary>
Motivation: To address the evolving nature of medical knowledge and the need for efficient updates in multimodal medical AI models, without full model retraining.

Method: MedMKEB consists of carefully constructed editing tasks and validations like counterfactual correction, semantic generalization, and adversarial robustness, tested with expert feedback and experimentation on existing models.

Result: Experiments reveal significant limitations in current knowledge-editing methods for medical multimodal models, highlighting areas for improvement.

Conclusion: MedMKEB offers a standardized measurement tool to drive progress in developing efficient and trustworthy editing strategies for multimodal medical knowledge systems.

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly improved medical AI, enabling it to unify the understanding of
visual and textual information. However, as medical knowledge continues to
evolve, it is critical to allow these models to efficiently update outdated or
incorrect information without retraining from scratch. Although textual
knowledge editing has been widely studied, there is still a lack of systematic
benchmarks for multimodal medical knowledge editing involving image and text
modalities. To fill this gap, we present MedMKEB, the first comprehensive
benchmark designed to evaluate the reliability, generality, locality,
portability, and robustness of knowledge editing in medical multimodal large
language models. MedMKEB is built on a high-quality medical visual
question-answering dataset and enriched with carefully constructed editing
tasks, including counterfactual correction, semantic generalization, knowledge
transfer, and adversarial robustness. We incorporate human expert validation to
ensure the accuracy and reliability of the benchmark. Extensive single editing
and sequential editing experiments on state-of-the-art general and medical
MLLMs demonstrate the limitations of existing knowledge-based editing
approaches in medicine, highlighting the need to develop specialized editing
strategies. MedMKEB will serve as a standard benchmark to promote the
development of trustworthy and efficient medical knowledge editing algorithms.

</details>


### [11] [EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search](https://arxiv.org/abs/2508.05113)
*Xinyue Wu,Fan Hu,Shaik Jani Babu,Yi Zhao,Xinfei Guo*

Main category: cs.AI

TL;DR: EasySize introduces a lightweight gate sizing framework applying a finetuned Qwen3-8B model, leveraging efficient heuristics to automate analog circuit design across technology nodes with reduced computational demands.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of slow, experience-driven analog circuit design and the limitations of current AI-based methods, such as dependence on large models and lack of adaptability across technology nodes.

Method: The paper introduces the EasySize framework, using a finetuned Qwen3-8B model combined with heuristic optimization techniques like Differential Evolution and Particle Swarm Optimization, along with dynamic loss functions based on Ease of Attainability (EOA).

Result: EasySize demonstrates universal applicability, achieving strong performance across multiple technology nodes (350nm to 22nm) without targeted retraining and surpasses AutoCkt in efficiency on over 86% of tasks, with over 96% simulation resource savings.

Conclusion: EasySize reduces reliance on human expertise, accelerates analog circuit design, and significantly lowers computational resource needs, providing an open-sourced solution for universal gate sizing.

Abstract: Analog circuit design is a time-consuming, experience-driven task in chip
development. Despite advances in AI, developing universal, fast, and stable
gate sizing methods for analog circuits remains a significant challenge. Recent
approaches combine Large Language Models (LLMs) with heuristic search
techniques to enhance generalizability, but they often depend on large model
sizes and lack portability across different technology nodes. To overcome these
limitations, we propose EasySize, the first lightweight gate sizing framework
based on a finetuned Qwen3-8B model, designed for universal applicability
across process nodes, design specifications, and circuit topologies. EasySize
exploits the varying Ease of Attainability (EOA) of performance metrics to
dynamically construct task-specific loss functions, enabling efficient
heuristic search through global Differential Evolution (DE) and local Particle
Swarm Optimization (PSO) within a feedback-enhanced flow. Although finetuned
solely on 350nm node data, EasySize achieves strong performance on 5
operational amplifier (Op-Amp) netlists across 180nm, 45nm, and 22nm technology
nodes without additional targeted training, and outperforms AutoCkt, a
widely-used Reinforcement Learning based sizing framework, on 86.67\% of tasks
with more than 96.67\% of simulation resources reduction. We argue that
EasySize can significantly reduce the reliance on human expertise and
computational resources in gate sizing, thereby accelerating and simplifying
the analog circuit design process. EasySize will be open-sourced at a later
date.

</details>


### [12] [Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures](https://arxiv.org/abs/2508.05116)
*Peer-Benedikt Degen,Igor Asanov*

Main category: cs.AI

TL;DR: This study explores the application of a Socratic AI Tutor to enhance student research question development compared to basic AI chatbots, proposing a broader pedagogical shift towards orchestrated multi-agent systems in education.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the evolving role of generative AI in higher education by examining how structured AI dialogue can enhance critical thinking and metacognitive engagement, countering concerns about de-skilling.

Method: A controlled experiment was conducted with 65 pre-service teacher students in Germany, comparing interactions with the Socratic AI Tutor and an uninstructed AI chatbot, grounded in constructivist educational theory.

Result: Students reported better support for critical, independent, and reflective thinking with the Socratic AI Tutor, demonstrating its effectiveness in fostering metacognitive engagement.

Conclusion: The study highlights the potential of dialogic AI systems for stimulating deeper learning and introduces the concept of orchestrated multi-agent systems (MAS) curated by educators, presenting implications for education systems regarding scalability, costs, and faculty roles.

Abstract: Generative AI is no longer a peripheral tool in higher education. It is
rapidly evolving into a general-purpose infrastructure that reshapes how
knowledge is generated, mediated, and validated. This paper presents findings
from a controlled experiment evaluating a Socratic AI Tutor, a large language
model designed to scaffold student research question development through
structured dialogue grounded in constructivist theory. Conducted with 65
pre-service teacher students in Germany, the study compares interaction with
the Socratic Tutor to engagement with an uninstructed AI chatbot. Students
using the Socratic Tutor reported significantly greater support for critical,
independent, and reflective thinking, suggesting that dialogic AI can stimulate
metacognitive engagement and challenging recent narratives of de-skilling due
to generative AI usage. These findings serve as a proof of concept for a
broader pedagogical shift: the use of multi-agent systems (MAS) composed of
specialised AI agents. To conceptualise this, we introduce the notion of
orchestrated MAS, modular, pedagogically aligned agent constellations, curated
by educators, that support diverse learning trajectories through differentiated
roles and coordinated interaction. To anchor this shift, we propose an adapted
offer-and-use model, in which students appropriate instructional offers from
these agents. Beyond technical feasibility, we examine system-level
implications for higher education institutions and students, including funding
necessities, changes to faculty roles, curriculars, competencies and assessment
practices. We conclude with a comparative cost-effectiveness analysis
highlighting the scalability of such systems. In sum, this study contributes
both empirical evidence and a conceptual roadmap for hybrid learning ecosystems
that embed human-AI co-agency and pedagogical alignment.

</details>


### [13] [Graph-based Event Log Repair](https://arxiv.org/abs/2508.05145)
*Sebastiano Dissegna,Chiara Di Francescomarino,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: This paper develops a Heterogeneous Graph Neural Network model to reconstruct missing event attributes in process mining traces, outperforming state-of-the-art autoencoder-based approaches in various scenarios.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of missing information in real-world event logs, which affects the reliability of process mining analysis.

Method: A Heterogeneous Graph Neural Network model is proposed to reconstruct missing event attributes by effectively representing and leveraging multi-modal execution trace data.

Result: Experimental evaluation on synthetic and real-world logs demonstrates superior performance of the proposed model in reconstructing all types of missing event attributes compared to existing state-of-the-art techniques.

Conclusion: The study illustrates the potential of Heterogeneous Graph Neural Networks for addressing missing data in complex event logs, suggesting broader applicability in process mining scenarios.

Abstract: The quality of event logs in Process Mining is crucial when applying any form
of analysis to them. In real-world event logs, the acquisition of data can be
non-trivial (e.g., due to the execution of manual activities and related manual
recording or to issues in collecting, for each event, all its attributes), and
often may end up with events recorded with some missing information. Standard
approaches to the problem of trace (or log) reconstruction either require the
availability of a process model that is used to fill missing values by
leveraging different reasoning techniques or employ a Machine Learning/Deep
Learning model to restore the missing values by learning from similar cases. In
recent years, a new type of Deep Learning model that is capable of handling
input data encoded as graphs has emerged, namely Graph Neural Networks. Graph
Neural Network models, and even more so Heterogeneous Graph Neural Networks,
offer the advantage of working with a more natural representation of complex
multi-modal sequences like the execution traces in Process Mining, allowing for
more expressive and semantically rich encodings.
  In this work, we focus on the development of a Heterogeneous Graph Neural
Network model that, given a trace containing some incomplete events, will
return the full set of attributes missing from those events. We evaluate our
work against a state-of-the-art approach leveraging autoencoders on two
synthetic logs and four real event logs, on different types of missing values.
Different from state-of-the-art model-free approaches, which mainly focus on
repairing a subset of event attributes, the proposed approach shows very good
performance in reconstructing all different event attributes.

</details>


### [14] [QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering](https://arxiv.org/abs/2508.05197)
*Zhuohang Jiang,Pangjing Wu,Xu Yuan,Wenqi Fan,Qing Li*

Main category: cs.AI

TL;DR: QA-Dragon proposes a novel approach to enhance multimodal VQA by utilizing a dynamic retrieval system combining both text and image-based knowledge sources, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Retrieval-Augmented Generation methods for Multimodal VQA struggle with complex queries requiring multi-hop reasoning or recent factual knowledge due to isolated text or image retrieval.

Method: QA-Dragon employs a query-aware dynamic system using domain and search routers to identify the query domain and dynamically select retrieval strategies, combining both text and image sources for multimodal and multi-turn reasoning.

Result: QA-Dragon significantly improves reasoning performance, achieving superior answer accuracy and knowledge overlap scores, outperforming baselines by 5.06% on single-source, 6.35% on multi-source, and 5.03% on multi-turn tasks.

Conclusion: The proposed framework effectively tackles knowledge-intensive and complex VQA tasks, showcasing its capability in multimodal, multi-hop, and multi-turn reasoning through dynamic retrieval strategies.

Abstract: Retrieval-Augmented Generation (RAG) has been introduced to mitigate
hallucinations in Multimodal Large Language Models (MLLMs) by incorporating
external knowledge into the generation process, and it has become a widely
adopted approach for knowledge-intensive Visual Question Answering (VQA).
However, existing RAG methods typically retrieve from either text or images in
isolation, limiting their ability to address complex queries that require
multi-hop reasoning or up-to-date factual knowledge. To address this
limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for
Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to
identify the query's subject domain for domain-specific reasoning, along with a
search router that dynamically selects optimal retrieval strategies. By
orchestrating both text and image search agents in a hybrid setup, our system
supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle
complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM
Challenge at KDD Cup 2025, where it significantly enhances the reasoning
performance of base models under challenging scenarios. Our framework achieves
substantial improvements in both answer accuracy and knowledge overlap scores,
outperforming baselines by 5.06% on the single-source task, 6.35% on the
multi-source task, and 5.03% on the multi-turn task.

</details>


### [15] [An Explainable Natural Language Framework for Identifying and Notifying Target Audiences In Enterprise Communication](https://arxiv.org/abs/2508.05267)
*Vítor N. Lourenço,Mohnish Dubey,Yunfei Bai,Audrey Depeige,Vivek Jain*

Main category: cs.AI

TL;DR: This paper presents a framework combining RDF graph databases and LLMs to enhance audience targeting and improve communication efficiency in large-scale maintenance organizations.


<details>
  <summary>Details</summary>
Motivation: Traditional communication approaches are ineffective in addressing the information overload and response time issues caused by identifying experts and complex relationships in maintenance organizations.

Method: The proposed framework integrates RDF graph databases and Large Language Models (LLMs) to process natural language queries for precise audience targeting. It employs a planning-orchestration architecture for reasoning.

Result: The framework improves communication efficiency, enables intuitive query formulation, and delivers explainable results that foster user trust.

Conclusion: This approach effectively addresses communication challenges in large-scale maintenance organizations through advanced audience targeting and transparent reasoning.

Abstract: In large-scale maintenance organizations, identifying subject matter experts
and managing communications across complex entities relationships poses
significant challenges -- including information overload and longer response
times -- that traditional communication approaches fail to address effectively.
We propose a novel framework that combines RDF graph databases with LLMs to
process natural language queries for precise audience targeting, while
providing transparent reasoning through a planning-orchestration architecture.
Our solution enables communication owners to formulate intuitive queries
combining concepts such as equipment, manufacturers, maintenance engineers, and
facilities, delivering explainable results that maintain trust in the system
while improving communication efficiency across the organization.

</details>


### [16] [A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents](https://arxiv.org/abs/2508.05311)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: This paper introduces a hybrid system that integrates decision trees and large language models (LLMs) into a unified multi-agent framework for improved reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve limitations in prior neuro-symbolic systems by tightly integrating interpretable decision trees with the generative and reasoning capabilities of LLMs, addressing both structured and unstructured reasoning challenges.

Method: The method involves embedding decision trees and random forests as callable oracles within a multi-agent framework. An orchestrator coordinates agents, maintains belief state consistency, and mediates communication, while decision trees enhance interpretability and causal reasoning, and LLMs handle abductive reasoning and generalization.

Result: The proposed system outperforms baselines, showing +7.2% entailment consistency on ProofWriter, +5.3% accuracy gains on GSM8k mathematical problems, and +6.0% abstraction accuracy on ARC benchmarks. It also demonstrates applications in clinical decision-making and scientific research.

Conclusion: The paper concludes that this hybrid architecture provides a robust, interpretable, and extensible framework for tackling general-purpose reasoning tasks by combining symbolic and neural approaches effectively.

Abstract: We propose a hybrid architecture that integrates decision tree-based symbolic
reasoning with the generative capabilities of large language models (LLMs)
within a coordinated multi-agent framework. Unlike prior approaches that
loosely couple symbolic and neural modules, our design embeds decision trees
and random forests as callable oracles within a unified reasoning system.
Tree-based modules enable interpretable rule inference and causal logic, while
LLM agents handle abductive reasoning, generalization, and interactive
planning. A central orchestrator maintains belief state consistency and
mediates communication across agents and external tools, enabling reasoning
over both structured and unstructured inputs.
  The system achieves strong performance on reasoning benchmarks. On
\textit{ProofWriter}, it improves entailment consistency by +7.2\% through
logic-grounded tree validation. On GSM8k, it achieves +5.3\% accuracy gains in
multistep mathematical problems via symbolic augmentation. On \textit{ARC}, it
boosts abstraction accuracy by +6.0\% through integration of symbolic oracles.
Applications in clinical decision support and scientific discovery show how the
system encodes domain rules symbolically while leveraging LLMs for contextual
inference and hypothesis generation. This architecture offers a robust,
interpretable, and extensible solution for general-purpose neuro-symbolic
reasoning.

</details>


### [17] [The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition](https://arxiv.org/abs/2508.05338)
*Brinnae Bent*

Main category: cs.AI

TL;DR: The term 'agent' in AI is ambiguous and needs redefining to improve research clarity, policy development, and evaluation practices.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address ambiguity in the definition of 'agent' in AI, which affects research communication, reproducibility, system evaluation, and policymaking.

Method: The authors propose redefining 'agent' using a framework based on historical and contemporary usage, outlining requirements and characterizing systems along key interaction dimensions.

Result: The paper provides a structured framework to clarify 'agent' terminology and offers practical recommendations for standardization and adoption.

Conclusion: The redefinition enhances system descriptions while preserving the term's multifaceted nature, benefiting research clarity and policy advancement.

Abstract: The term 'agent' in artificial intelligence has long carried multiple
interpretations across different subfields. Recent developments in AI
capabilities, particularly in large language model systems, have amplified this
ambiguity, creating significant challenges in research communication, system
evaluation and reproducibility, and policy development. This paper argues that
the term 'agent' requires redefinition. Drawing from historical analysis and
contemporary usage patterns, we propose a framework that defines clear minimum
requirements for a system to be considered an agent while characterizing
systems along a multidimensional spectrum of environmental interaction,
learning and adaptation, autonomy, goal complexity, and temporal coherence.
This approach provides precise vocabulary for system description while
preserving the term's historically multifaceted nature. After examining
potential counterarguments and implementation challenges, we provide specific
recommendations for moving forward as a field, including suggestions for
terminology standardization and framework adoption. The proposed approach
offers practical tools for improving research clarity and reproducibility while
supporting more effective policy development.

</details>


### [18] [NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making](https://arxiv.org/abs/2508.05344)
*Asutosh Hota,Jussi P. P. Jokinen*

Main category: cs.AI

TL;DR: NomicLaw demonstrates how LLMs engage in collaborative law-making, revealing latent social reasoning and persuasive capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand LLM behavior in multi-agent, deliberative legal settings.

Method: NomicLaw simulation with structured legal vignettes, rule proposals, justification, and voting.

Result: LLMs exhibited strategic behavior such as alliance formation, trust betrayal, and adaptive rhetoric.

Conclusion: Findings offer insights for designing AI systems for autonomous legal negotiation and legislation drafting.

Abstract: Recent advancements in large language models (LLMs) have extended their
capabilities from basic text processing to complex reasoning tasks, including
legal interpretation, argumentation, and strategic interaction. However,
empirical understanding of LLM behavior in open-ended, multi-agent settings
especially those involving deliberation over legal and ethical dilemmas remains
limited. We introduce NomicLaw, a structured multi-agent simulation where LLMs
engage in collaborative law-making, responding to complex legal vignettes by
proposing rules, justifying them, and voting on peer proposals. We
quantitatively measure trust and reciprocity via voting patterns and
qualitatively assess how agents use strategic language to justify proposals and
influence outcomes. Experiments involving homogeneous and heterogeneous LLM
groups demonstrate how agents spontaneously form alliances, betray trust, and
adapt their rhetoric to shape collective decisions. Our results highlight the
latent social reasoning and persuasive capabilities of ten open-source LLMs and
provide insights into the design of future AI systems capable of autonomous
negotiation, coordination and drafting legislation in legal settings.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [19] [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
*Jiazheng Zhang,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: This paper analyzes errors in LLM-based RTL code generation and proposes targeted correction techniques, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: LLM-based RTL code generation faces low success rates due to insufficient understanding of error causes, necessitating a detailed analysis and targeted solutions.

Method: The authors conduct comprehensive error analysis and categorization, use in-context learning, and integrate strategies like retrieval-augmented generation, design rules, meta-format conversion, and iterative debugging to correct errors.

Result: The proposed framework achieves 91.0% accuracy on the VerilogEval benchmark, representing a 32.7% performance improvement over the baseline method.

Conclusion: The integration of error correction techniques into an LLM-based RTL framework greatly enhances its performance, highlighting the value of addressing specific error types systematically.

Abstract: Despite the promising potential of large language model (LLM) based
register-transfer-level (RTL) code generation, the overall success rate remains
unsatisfactory. Errors arise from various factors, with limited understanding
of specific failure causes hindering improvement. To address this, we conduct a
comprehensive error analysis and manual categorization. Our findings reveal
that most errors stem not from LLM reasoning limitations, but from insufficient
RTL programming knowledge, poor understanding of circuit concepts, ambiguous
design descriptions, or misinterpretation of complex multimodal inputs.
Leveraging in-context learning, we propose targeted error correction
techniques. Specifically, we construct a domain-specific knowledge base and
employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge.
To mitigate ambiguity errors, we introduce design description rules and
implement a rule-checking mechanism. For multimodal misinterpretation, we
integrate external tools to convert inputs into LLM-compatible meta-formats.
For remaining errors, we adopt an iterative debugging loop (simulation-error
localization-correction). Integrating these techniques into an LLM-based
framework significantly improves performance. We incorporate these error
correction techniques into a foundational LLM-based RTL code generation
framework, resulting in significantly improved performance. Experimental
results show that our enhanced framework achieves 91.0\% accuracy on the
VerilogEval benchmark, surpassing the baseline code generation approach by
32.7\%, demonstrating the effectiveness of our methods.

</details>


### [20] [relOBI: A Reliable Low-latency Interconnect for Tightly-Coupled On-chip Communication](https://arxiv.org/abs/2508.05354)
*Michael Rogenmoser,Angelo Garofalo,Luca Benini*

Main category: cs.AR

TL;DR: The paper introduces relOBI, improving fault tolerance in SoC interconnects using Triple Modular Redundancy and Error Correction Codes, achieving zero vulnerability with moderate area and timing overhead.


<details>
  <summary>Details</summary>
Motivation: To ensure reliable on-chip communication in radiation-heavy environments, preventing functional failures in SoCs due to interconnect errors.

Method: The approach integrates Triple Modular Redundancy (TMR) for handshake signals and Error Correction Codes (ECC) for other signals into Open Bus Interface (OBI). A fully reliable crossbar is implemented and tested.

Result: Fault vulnerability reduced from 34.85% to 0% compared to the reference design. Area increased by 2.6x and timing impact by 1.4x; area overhead was notably lower than fine-grained triplication reported in literature.

Conclusion: RelOBI effectively enhances reliability in critical interconnects, offering a balanced trade-off between robustness and hardware overhead.

Abstract: On-chip communication is a critical element of modern systems-on-chip (SoCs),
allowing processor cores to interact with memory and peripherals. Interconnects
require special care in radiation-heavy environments, as any soft error within
the SoC interconnect is likely to cause a functional failure of the whole SoC.
This work proposes relOBI, an extension to Open Bus Interface (OBI) combining
triple modular redundancy (TMR) for critical handshake signals with error
correction codes (ECC) protection on other signals for complete reliability.
Implementing and testing a fully reliable crossbar shows improved reliability
to injected faults from a vulnerability of 34.85 % to 0 % compared to a
reference design, with an area increase of 2.6x and 1.4x timing impact. The
area overhead is 1.8x lower than that reported in the literature for
fine-grained triplication and voting.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

TL;DR: This paper explores enriching dialogue transcriptions by tagging speaker traits such as age, gender, and emotion using frozen audio models and an LLAMA model, without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To enhance the utility of dialogue transcripts by not just improving their readability but also adding valuable metadata about speaker characteristics.

Method: The approach uses frozen audio foundation models (e.g., Whisper, WavLM) and a frozen LLAMA language model connected via lightweight connectors to infer speaker attributes.

Result: The proposed method achieves competitive performance in speaker profiling without losing modularity or speed, and also enables comparing x-vectors with an Equal Error Rate of 8.8% in some cases.

Conclusion: Metadata tagging for speaker profiles can complement traditional transcription pipelines, achieved efficiently without task-specific fine-tuning of models.

Abstract: In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [22] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

TL;DR: The paper introduces Parity-aware Byte Pair Encoding (BPE) to improve tokenization fairness across languages.


<details>
  <summary>Details</summary>
Motivation: Standard tokenization methods disproportionately disadvantage low-resource languages, leading to computational and financial inequalities.

Method: Parity-aware BPE adjusts tokenization by maximizing compression gain for the worst-compressed language at each merge step, balancing cross-lingual parity.

Result: Parity-aware BPE results in more equitable token counts without significantly impacting global compression or language model performance.

Conclusion: The proposed method effectively promotes fairness in tokenization across languages with minimal trade-offs in performance or compression.

Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


### [23] [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)
*David Sasu,Natalie Schluter*

Main category: cs.CL

TL;DR: The study improves ASR systems by integrating a pitch accent detection module, showing significant enhancements in both tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance ASR performance by leveraging prosodic cues, such as pitch accent, which are not fully utilized by current pre-trained speech models.

Method: Introduced a joint model combining ASR and pitch accent detection, improving both tasks via semi-supervised speech representations and fine-tuning.

Result: The model achieved a 41% improvement in F1-score for pitch accent detection and a 28.3% WER reduction for ASR on the LibriSpeech dataset.

Conclusion: Extending pretrained speech models to include prosodic features like pitch accent significantly boosts both ASR and pitch detection capabilities.

Abstract: We show the performance of Automatic Speech Recognition (ASR) systems that
use semi-supervised speech representations can be boosted by a complimentary
pitch accent detection module, by introducing a joint ASR and pitch accent
detection model. The pitch accent detection component of our model achieves a
significant improvement on the state-of-the-art for the task, closing the gap
in F1-score by 41%. Additionally, the ASR performance in joint training
decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With
these results, we show the importance of extending pretrained speech models to
retain or re-learn important prosodic cues such as pitch accent.

</details>


### [24] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
*Tommaso Tosato,Saskia Helbling,Yorguin-Jose Mantilla-Ramos,Mahmood Hegazy,Alberto Tosato,David John Lemay,Irina Rish,Guillaume Dumas*

Main category: cs.CL

TL;DR: Current large language models are unstable in personality-like traits, even with interventions, and lack consistent behavior required for safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: Understanding personality-like traits in large language models to ensure safe deployment and behavioral consistency.

Method: Developed a framework (PERSIST) testing over 25 models with 500,000+ responses using traditional and LLM-specific personality tests, varying prompts and reasoning setups.

Result: Identified substantial variabilities in model responses, challenges in stabilizing behavior, and limitations in existing personality alignment strategies.

Conclusion: Current large language models are not suitable for applications requiring consistent, predictable behavior due to inherent architectural limitations.

Abstract: Large language models require consistent behavioral patterns for safe
deployment, yet their personality-like traits remain poorly understood. We
present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive
evaluation framework testing 25+ open-source models (1B-671B parameters) across
500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted
personality instruments, we systematically vary question order, paraphrasing,
personas, and reasoning modes. Our findings challenge fundamental deployment
assumptions: (1) Even 400B+ models exhibit substantial response variability (SD
> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up
to 20%; (3) Interventions expected to stabilize behavior, such as
chain-of-thought reasoning, detailed personas instruction, inclusion of
conversation history, can paradoxically increase variability; (4) LLM-adapted
instruments show equal instability to human-centric versions, confirming
architectural rather than translational limitations. This persistent
instability across scales and mitigation strategies suggests current LLMs lack
the foundations for genuine behavioral consistency. For safety-critical
applications requiring predictable behavior, these findings indicate that
personality-based alignment strategies may be fundamentally inadequate.

</details>


### [25] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
*Jun Liu,Zhenglun Kong,Changdi Yang,Fan Yang,Tianqi Li,Peiyan Dong,Joannah Nanjekye,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.CL

TL;DR: RCR-Router is a context-routing framework for multi-agent LLMs that reduces token usage while improving answer quality.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent LLM coordination schemes often rely on inefficient static or full-context routing, consuming excessive tokens and limiting adaptability.

Method: RCR-Router dynamically routes semantically relevant memory subsets based on agent roles and task stages. It uses a scoring policy for memory selection and integrates outputs iteratively into shared memory while adhering to token budgets.

Result: The framework reduced token use by up to 30% and improved or maintained answer quality on HotPotQA, MuSiQue, and 2WikiMultihop benchmarks.

Conclusion: Adaptive and structured memory routing enables efficient and scalable collaboration in multi-agent LLMs while maintaining high-quality outputs.

Abstract: Multi-agent large language model (LLM) systems have shown strong potential in
complex reasoning and collaborative decision-making tasks. However, most
existing coordination schemes rely on static or full-context routing
strategies, which lead to excessive token consumption, redundant memory
exposure, and limited adaptability across interaction rounds. We introduce
RCR-Router, a modular and role-aware context routing framework designed to
enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,
this is the first routing approach that dynamically selects semantically
relevant memory subsets for each agent based on its role and task stage, while
adhering to a strict token budget. A lightweight scoring policy guides memory
selection, and agent outputs are iteratively integrated into a shared memory
store to facilitate progressive context refinement. To better evaluate model
behavior, we further propose an Answer Quality Score metric that captures
LLM-generated explanations beyond standard QA accuracy. Experiments on three
multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate
that RCR-Router reduces token usage (up to 30%) while improving or maintaining
answer quality. These results highlight the importance of structured memory
routing and output-aware evaluation in advancing scalable multi-agent LLM
systems.

</details>


### [26] [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939)
*Julia Kharchenko,Tanya Roosta,Aman Chadha,Chirag Shah*

Main category: cs.CL

TL;DR: This paper introduces a benchmark for identifying demographic biases in Large Language Models (LLMs) based on linguistic patterns.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how subtle linguistic markers may reveal demographic attributes and lead to biases in AI systems.

Method: The researchers constructed interview simulations using 100 validated question-response pairs with controlled linguistic variations while maintaining semantic equivalence.

Result: Their benchmark showed that hedged responses were systematically penalized with 25.6% lower ratings, revealing biases in LLMs.

Conclusion: The paper provides a foundational framework for measuring linguistic discrimination in AI systems, crucial for ensuring fairness in automated decision-making.

Abstract: This paper introduces a comprehensive benchmark for evaluating how Large
Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic
markers that can inadvertently reveal demographic attributes such as gender,
social class, or regional background. Through carefully constructed interview
simulations using 100 validated question-response pairs, we demonstrate how
LLMs systematically penalize certain linguistic patterns, particularly hedging
language, despite equivalent content quality. Our benchmark generates
controlled linguistic variations that isolate specific phenomena while
maintaining semantic equivalence, which enables the precise measurement of
demographic bias in automated evaluation systems. We validate our approach
along multiple linguistic dimensions, showing that hedged responses receive
25.6% lower ratings on average, and demonstrate the benchmark's effectiveness
in identifying model-specific biases. This work establishes a foundational
framework for detecting and measuring linguistic discrimination in AI systems,
with broad applications to fairness in automated decision-making contexts.

</details>


### [27] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
*Shuhai Zhang,Zeng You,Yaofo Chen,Zhiquan Wen,Qianyue Wang,Zhijie Qiu,Yuanqing Li,Mingkui Tan*

Main category: cs.CL

TL;DR: The paper introduces Dynamic Group Attention (DGA), which reduces redundancy in attention computation for large language models, decreasing computational costs without compromising performance.


<details>
  <summary>Details</summary>
Motivation: Current transformer-based large language models (LLMs) face inefficiencies in long-context modeling due to redundant attention computations, as all tokens consume equal computational resources despite sparseness in attention weights.

Method: The authors reformulate probabilistic sequence modeling into a supervised learning task, analyze attention sparsity to identify significant tokens, and introduce a group coding strategy that aggregates less important tokens to optimize attention computation. They then propose Dynamic Group Attention (DGA) based on this framework.

Result: The proposed DGA approach reduces computational costs significantly while maintaining competitive performance in comparison to traditional methods.

Conclusion: Dynamic Group Attention (DGA) enhances the efficiency of attention computations in LLMs by reducing redundancy, which can contribute to more resource-efficient natural language processing tasks.

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


### [28] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

TL;DR: This paper highlights the limitations of existing evaluation methods for visual activity recognition systems and introduces a clustering framework to account for verb ambiguities and perspectives.


<details>
  <summary>Details</summary>
Motivation: Understanding and addressing ambiguities in verb semantics and image interpretation within visual activity recognition systems.

Method: A novel vision-language clustering framework that categorizes verb sense clusters for evaluating model accuracy, considering human-like perspectives.

Result: Analysis of the imSitu dataset confirms an average mapping of 2.8 sense clusters per image; evaluations show improved alignment with human judgments compared to standard methods.

Conclusion: Cluster-based evaluation offers a nuanced and robust alternative for assessing visual activity recognition models, aligning more closely with human interpretations.

Abstract: Evaluating visual activity recognition systems is challenging due to inherent
ambiguities in verb semantics and image interpretation. When describing actions
in images, synonymous verbs can refer to the same event (e.g., brushing vs.
grooming), while different perspectives can lead to equally valid but distinct
verb choices (e.g., piloting vs. operating). Standard exact-match evaluation,
which relies on a single gold answer, fails to capture these ambiguities,
resulting in an incomplete assessment of model performance. To address this, we
propose a vision-language clustering framework that constructs verb sense
clusters, providing a more robust evaluation. Our analysis of the imSitu
dataset shows that each image maps to an average of 2.8 sense clusters, with
each cluster representing a distinct perspective of the image. We evaluate
multiple activity recognition models and compare our cluster-based evaluation
with standard evaluation methods. Additionally, our human alignment analysis
suggests that the cluster-based evaluation better aligns with human judgements,
offering a more nuanced assessment of model performance.

</details>


### [29] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
*Song Wang,Yishu Wei,Haotian Ma,Max Lovitt,Kelly Deng,Yuan Meng,Zihan Xu,Jingze Zhang,Yunyu Xiao,Ying Ding,Xuhai Xu,Joydeep Ghosh,Yifan Peng*

Main category: cs.CL

TL;DR: The study develops a multi-stage language model framework to improve the extraction of social determinants of health (SDoH) related to suicide incidents, achieving better accuracy, explainability, and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in data-driven analysis of suicide incidents, particularly the difficulty in handling long-tailed SDoH factor distributions, identifying pivotal stressors, and enhancing model explainability.

Method: A multi-stage large language model was developed and tested against state-of-the-art language models like BioBERT and GPT-3.5-turbo and reasoning models like DeepSeek-R1. The study also conducted a pilot user study to evaluate annotation speed and accuracy with model explanations.

Result: The proposed framework outperformed existing models in extracting SDoH factors and retrieving relevant contexts. Fine-tuning smaller, task-specific models delivered comparable or better results with reduced costs. The framework also improved model explainability through intermediate explanations.

Conclusion: The approach advances both the precision and transparency of SDoH extraction related to suicide, enabling more accurate identification of at-risk individuals and potential improvements in suicide prevention strategies.

Abstract: Background: Understanding social determinants of health (SDoH) factors
contributing to suicide incidents is crucial for early intervention and
prevention. However, data-driven approaches to this goal face challenges such
as long-tailed factor distributions, analyzing pivotal stressors preceding
suicide incidents, and limited model explainability. Methods: We present a
multi-stage large language model framework to enhance SDoH factor extraction
from unstructured text. Our approach was compared to other state-of-the-art
language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning
models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help
people annotate SDoH factors more quickly and accurately. The analysis included
both automated comparisons and a pilot user study. Results: We show that our
proposed framework demonstrated performance boosts in the overarching task of
extracting SDoH factors and in the finer-grained tasks of retrieving relevant
context. Additionally, we show that fine-tuning a smaller, task-specific model
achieves comparable or better performance with reduced inference costs. The
multi-stage design not only enhances extraction but also provides intermediate
explanations, improving model explainability. Conclusions: Our approach
improves both the accuracy and transparency of extracting suicide-related SDoH
from unstructured texts. These advancements have the potential to support early
identification of individuals at risk and inform more effective prevention
strategies.

</details>


### [30] [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)
*Kun Peng,Cong Cao,Hao Peng,Zhifeng Hao,Lei Jiang,Kongjing Gu,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: The paper presents a novel approach for Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ), focusing on partitioning dialogues into semantically independent sub-dialogues, significantly improving extraction accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods assume uniform distribution of sentiment elements across dialogues, which is ineffective due to the presence of multiple semantically independent sub-dialogues. This results in noise and inefficiencies in extraction.

Method: The paper proposes a method involving structural entropy minimization to partition dialogues into independent sub-dialogues, coupled with a two-step framework: extract sentiment elements at the utterance level and match quadruples at the sub-dialogue level.

Result: Experiments show the proposed method achieves state-of-the-art performance in DiaASQ tasks, with significantly lower computational costs compared to existing techniques.

Conclusion: Partitioning dialogues into semantically independent sub-dialogues eliminates noise and improves accuracy, marking a substantial improvement over previous methods in sentiment quadruple extraction tasks.

Abstract: Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to
extract all target-aspect-opinion-sentiment quadruples from a given
multi-round, multi-participant dialogue. Existing methods typically learn word
relations across entire dialogues, assuming a uniform distribution of sentiment
elements. However, we find that dialogues often contain multiple semantically
independent sub-dialogues without clear dependencies between them. Therefore,
learning word relationships across the entire dialogue inevitably introduces
additional noise into the extraction process. To address this, our method
focuses on partitioning dialogues into semantically independent sub-dialogues.
Achieving completeness while minimizing these sub-dialogues presents a
significant challenge. Simply partitioning based on reply relationships is
ineffective. Instead, we propose utilizing a structural entropy minimization
algorithm to partition the dialogues. This approach aims to preserve relevant
utterances while distinguishing irrelevant ones as much as possible.
Furthermore, we introduce a two-step framework for quadruple extraction: first
extracting individual sentiment elements at the utterance level, then matching
quadruples at the sub-dialogue level. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in DiaASQ with much lower
computational costs.

</details>


### [31] [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)
*Shu Han Ho*

Main category: cs.CL

TL;DR: The paper evaluates the effectiveness of finetuning Large Language Models (LLMs) for Abstract Meaning Representation (AMR) parsing, demonstrating that straightforward finetuning achieves performance comparable to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Developing simpler yet effective alternatives to complex state-of-the-art methods for AMR parsing, leveraging LLMs to reduce methodological complexity.

Method: The study finetunes four different LLM architectures (Phi 3.5, Gemma 2, LLaMA 3.2, DeepSeek R1 LLaMA Distilled) using the LDC2020T02 Gold AMR3.0 test set and compares their performance with existing AMR parsers.

Result: Straightforward finetuning of decoder-only LLMs yielded competitive results, with LLaMA 3.2 achieving an SMATCH F1 score of 0.804, comparable to SOTA parsers such as APT+Silver (IBM) and approaching Graphene Smatch (MBSE).

Conclusion: Finetuning decoder-only LLMs is a promising alternative to complex methods for AMR parsing, with distinct strengths observed in different architectures (e.g., semantic performance by LLaMA 3.2, structural validity by Phi 3.5).

Abstract: Meaning Representation (AMR) is a semantic formalism that encodes sentence
meaning as rooted, directed, acyclic graphs, where nodes represent concepts and
edges denote semantic relations. Finetuning decoder only Large Language Models
(LLMs) represent a promising novel straightfoward direction for AMR parsing.
This paper presents a comprehensive evaluation of finetuning four distinct LLM
architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled
using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that
straightfoward finetuning of decoder only LLMs can achieve comparable
performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2
demonstrates competitive performance against SOTA AMR parsers given a
straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full
LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching
Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a
consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5
excels in structural validity.

</details>


### [32] [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)
*Jinda Liu,Bo Cheng,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: The paper challenges the trend of using complex multi-adapter approaches in multi-task learning for adapting LLMs, proposing instead simpler architectures with high inter-head similarity. It introduces Align-LoRA, which aligns task representations and achieves superior performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the adaptation of LLMs for diverse tasks by questioning the structural complexity of existing PEFT approaches in multi-task learning and emphasizing the need for robust shared representations.

Method: The authors propose Align-LoRA, a simple fine-tuning method utilizing a loss function to align task representations within a shared adapter space across tasks.

Result: Align-LoRA outperformed complex multi-adapter and multi-head systems in experiments, demonstrating competitive results using standard single-adapter systems with increased rank.

Conclusion: Simpler architectures focusing on aligning shared representations are more efficient and effective for adapting LLMs to multiple tasks than complex multi-component systems.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large
Language Models (LLMs). In practice, LLMs are often required to handle a
diverse set of tasks from multiple domains, a scenario naturally addressed by
multi-task learning (MTL). Within this MTL context, a prevailing trend involves
LoRA variants with multiple adapters or heads, which advocate for structural
diversity to capture task-specific knowledge. Our findings present a direct
challenge to this paradigm. We first show that a simplified multi-head
architecture with high inter-head similarity substantially outperforms complex
multi-adapter and multi-head systems. This leads us to question the
multi-component paradigm itself, and we further demonstrate that a standard
single-adapter LoRA, with a sufficiently increased rank, also achieves highly
competitive performance. These results lead us to a new hypothesis: effective
MTL generalization hinges on learning robust shared representations, not
isolating task-specific features. To validate this, we propose Align-LoRA,
which incorporates an explicit loss to align task representations within the
shared adapter space. Experiments confirm that Align-LoRA significantly
surpasses all baselines, establishing a simpler yet more effective paradigm for
adapting LLMs to multiple tasks. The code is available at
https://github.com/jinda-liu/Align-LoRA.

</details>


### [33] [Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations](https://arxiv.org/abs/2508.05097)
*Aditya Kishore,Gaurav Kumar,Jasabanta Patro*

Main category: cs.CL

TL;DR: The paper introduces "MultiCheck," a unified framework aimed at verifying multimodal misinformation using text-visual reasoning. Tests on Factify 2 data achieved superior fact-checking performance.


<details>
  <summary>Details</summary>
Motivation: The study targets the challenge of tackling multimodal misinformation, emphasizing the need for systems capable of verifying claims involving both textual and visual components.

Method: The "MultiCheck" framework utilizes encoders for text and images, a fusion module for cross-modal relationships, and contrastive learning to predict claim veracity by aligning semantic meanings in a shared embedding space.

Result: Evaluations on the Factify 2 dataset revealed a weighted F1 score of 0.84, outperforming benchmark methods and confirming the efficacy of explicit multimodal reasoning.

Conclusion: The findings underscore the importance of multimodal reasoning in enhancing fact-checking accuracy, promoting scalable solutions for real-world misinformation scenarios.

Abstract: The growing rate of multimodal misinformation, where claims are supported by
both text and images, poses significant challenges to fact-checking systems
that rely primarily on textual evidence. In this work, we have proposed a
unified framework for fine-grained multimodal fact verification called
"MultiCheck", designed to reason over structured textual and visual signals.
Our architecture combines dedicated encoders for text and images with a fusion
module that captures cross-modal relationships using element-wise interactions.
A classification head then predicts the veracity of a claim, supported by a
contrastive learning objective that encourages semantic alignment between
claim-evidence pairs in a shared latent space. We evaluate our approach on the
Factify 2 dataset, achieving a weighted F1 score of 0.84, substantially
outperforming the baseline. These results highlight the effectiveness of
explicit multimodal reasoning and demonstrate the potential of our approach for
scalable and interpretable fact-checking in complex, real-world scenarios.

</details>


### [34] [BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05100)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Jing Liu,Wayne Xin Zhao,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: The paper addresses performance issues in retrieval-augmented generation (RAG) caused by long context lengths, presenting the BEE-RAG framework to stabilize entropy dynamics and improve adaptability.


<details>
  <summary>Details</summary>
Motivation: RAG frameworks face performance limitations due to unconstrained entropy growth and diluted attention when processing lengthy retrieval contexts.

Method: The proposed BEE-RAG framework employs entropy invariance principles to stabilize entropy levels, introduces a zero-shot inference strategy for context importance estimation, and combines adaptive, parameter-efficient fine-tuning to optimize performance.

Result: Experiments confirm that BEE-RAG enhances RAG tasks' effectiveness across various applications through improved attention dynamics and entropy management.

Conclusion: BEE-RAG successfully improves adaptability and performance in RAG systems, addressing challenges imposed by long retrieval contexts.

Abstract: With the rapid advancement of large language models (LLMs),
retrieval-augmented generation (RAG) has emerged as a critical approach to
supplement the inherent knowledge limitations of LLMs. However, due to the
typically large volume of retrieved information, RAG tends to operate with long
context lengths. From the perspective of entropy engineering, we identify
unconstrained entropy growth and attention dilution due to long retrieval
context as significant factors affecting RAG performance. In this paper, we
propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves
the adaptability of RAG systems to varying context lengths through the
principle of entropy invariance. By leveraging balanced context entropy to
reformulate attention dynamics, BEE-RAG separates attention sensitivity from
context length, ensuring a stable entropy level. Building upon this, we
introduce a zero-shot inference strategy for multi-importance estimation and a
parameter-efficient adaptive fine-tuning mechanism to obtain the optimal
balancing factor for different settings. Extensive experiments across multiple
RAG tasks demonstrate the effectiveness of BEE-RAG.

</details>


### [35] [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)
*Zihao Yi,Delong Zeng,Zhenqing Ling,Haohao Luo,Zhe Xu,Wei Liu,Jian Luan,Wanxia Cao,Ying Shen*

Main category: cs.CL

TL;DR: LLMs show 'attention basin' behavior, focusing more on the start and end of input sequences, neglecting the middle; the proposed AttnRank method effectively reorders input to enhance model output.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the positional bias in LLMs, where attention is unevenly distributed across input sequences, focusing primarily on the initial and final items.

Method: The study introduces AttnRank, a framework that uses a small calibration set to determine a model's attention preferences and reorders input sequences to optimize attention distribution.

Result: AttnRank significantly improves performance across 10 large language models by realigning key information without altering model training or parameters.

Conclusion: Redistributing input to align with LLMs' natural attention patterns enhances their capabilities in multi-hop QA and few-shot tasks, providing a scalable and efficient solution.

Abstract: The performance of Large Language Models (LLMs) is significantly sensitive to
the contextual position of information in the input. To investigate the
mechanism behind this positional bias, our extensive experiments reveal a
consistent phenomenon we term the attention basin: when presented with a
sequence of structured items (e.g., retrieved documents or few-shot examples),
models systematically assign higher attention to the items at the beginning and
end of the sequence, while neglecting those in the middle. Crucially, our
analysis further reveals that allocating higher attention to critical
information is key to enhancing model performance. Based on these insights, we
introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)
estimates a model's intrinsic positional attention preferences using a small
calibration set, and (ii) reorders retrieved documents or few-shot examples to
align the most salient content with these high-attention positions. AttnRank is
a model-agnostic, training-free, and plug-and-play method with minimal
computational overhead. Experiments on multi-hop QA and few-shot in-context
learning tasks demonstrate that AttnRank achieves substantial improvements
across 10 large language models of varying architectures and scales, without
modifying model parameters or training procedures.

</details>


### [36] [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)
*Chang Hong,Minghao Wu,Qingying Xiao,Yuchi Wang,Xiang Wan,Guangjun Yu,Benyou Wang,Yan Hu*

Main category: cs.CL

TL;DR: The paper introduces PrinciplismQA, a benchmark to evaluate large language models' (LLMs) ethical reasoning in healthcare. It highlights gaps in ethical application and provides metrics for improvement.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of systematic benchmarks for assessing the ethical reasoning capabilities of LLMs in healthcare contexts.

Method: Researchers developed PrinciplismQA, comprising 3,648 questions based on medical ethics textbooks and case literature, validated by experts, for evaluating LLM alignment with medical ethical principles.

Result: Experiments showed that LLMs often fail to apply ethical principles effectively, particularly in dilemmas involving Beneficence. Closed-source models outperform others, and fine-tuning improves ethical competence.

Conclusion: PrinciplismQA provides a robust tool to identify ethical reasoning gaps in LLMs and emphasizes the need for better alignment between medical ethical knowledge and AI capabilities for responsible healthcare applications.

Abstract: The integration of large language models into healthcare necessitates a
rigorous evaluation of their ethical reasoning, an area current benchmarks
often overlook. We introduce PrinciplismQA, a comprehensive benchmark with
3,648 questions designed to systematically assess LLMs' alignment with core
medical ethics. Grounded in Principlism, our benchmark features a high-quality
dataset. This includes multiple-choice questions curated from authoritative
textbooks and open-ended questions sourced from authoritative medical ethics
case study literature, all validated by medical experts. Our experiments reveal
a significant gap between models' ethical knowledge and their practical
application, especially in dynamically applying ethical principles to
real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,
often over-emphasizing other principles. Frontier closed-source models, driven
by strong general capabilities, currently lead the benchmark. Notably, medical
domain fine-tuning can enhance models' overall ethical competence, but further
progress requires better alignment with medical ethical knowledge.
PrinciplismQA offers a scalable framework to diagnose these specific ethical
weaknesses, paving the way for more balanced and responsible medical AI.

</details>


### [37] [ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering](https://arxiv.org/abs/2508.05179)
*Catherine Kobus,François Lancelot,Marion-Cécile Martin,Nawal Ould Amer*

Main category: cs.CL

TL;DR: This paper tackles hallucinated text detection in QA systems using various methods, achieving top performance in multiple languages.


<details>
  <summary>Details</summary>
Motivation: LLMs, despite advancements in NLG, often generate hallucinated content, necessitating effective detection methods.

Method: The authors used few-shot prompting with LLMs, token-level classification, and fine-tuned LLMs on synthetic data.

Result: Their approaches yielded top rankings in Spanish and competitive results in English and German, effectively detecting hallucinated spans.

Conclusion: Integrating relevant context and employing fine-tuned models with prompt engineering are key strategies to mitigate hallucinations in LLMs.

Abstract: This paper presents the contributions of the ATLANTIS team to SemEval-2025
Task 3, focusing on detecting hallucinated text spans in question answering
systems. Large Language Models (LLMs) have significantly advanced Natural
Language Generation (NLG) but remain susceptible to hallucinations, generating
incorrect or misleading content. To address this, we explored methods both with
and without external context, utilizing few-shot prompting with a LLM,
token-level classification or LLM fine-tuned on synthetic data. Notably, our
approaches achieved top rankings in Spanish and competitive placements in
English and German. This work highlights the importance of integrating relevant
context to mitigate hallucinations and demonstrate the potential of fine-tuned
models and prompt engineering.

</details>


### [38] [Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation](https://arxiv.org/abs/2508.05234)
*Haonan Shangguan,Xiaocui Yang,Shi Feng,Daling Wang,Yifei Zhang,Ge Yu*

Main category: cs.CL

TL;DR: This paper introduces a lightweight model to perform multimodal sentiment reasoning and classification effectively using a novel distillation approach.


<details>
  <summary>Details</summary>
Motivation: To address the lack of autonomous multimodal sentiment reasoning and classification in resource-constrained environments.

Method: The paper proposes a "Teacher-Assistant-Student" distillation paradigm, where a large LLM generates reasoning data, a medium assistant model learns via multi-task learning, and a lightweight student model is jointly trained for sentiment tasks.

Result: MulCoT-RD achieves strong performance on the JMSRC task using only 3B parameters, with robust generalization and improved interpretability across four datasets.

Conclusion: The approach successfully integrates efficient reasoning and classification capabilities into a compact model suitable for resource-limited environments.

Abstract: The surge in rich multimodal content on social media platforms has greatly
advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)
further accelerating progress in this field. Current approaches primarily
leverage the knowledge and reasoning capabilities of parameter-heavy
(Multimodal) LLMs for sentiment classification, overlooking autonomous
multimodal sentiment reasoning generation in resource-constrained environments.
Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment
Reasoning and Classification task, JMSRC, which simultaneously performs
multimodal sentiment reasoning chain generation and sentiment classification
only with a lightweight model. We propose a Multimodal Chain-of-Thought
Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a
"Teacher-Assistant-Student" distillation paradigm to address deployment
constraints in resource-limited environments. We first leverage a
high-performance Multimodal Large Language Model (MLLM) to generate the initial
reasoning dataset and train a medium-sized assistant model with a multi-task
learning mechanism. A lightweight student model is jointly trained to perform
efficient multimodal sentiment reasoning generation and classification.
Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B
parameters achieves strong performance on JMSRC, while exhibiting robust
generalization and enhanced interpretability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [39] [RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration](https://arxiv.org/abs/2508.04797)
*Mohab Kishawy,Ali Abdellatif Hussein,Jun Chen*

Main category: cs.CV

TL;DR: RetinexDual is a novel framework based on Retinex theory for Ultra-High-Definition Image Restoration (UHD IR), utilizing SAMBA and FIA sub-networks to address spatial artifacts and global contexts respectively, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional UHD IR methods suffer from irreversible information loss and inefficacy in handling spatially confined image artifacts. This paper introduces RetinexDual to address these challenges and enhance restoration quality.

Method: RetinexDual comprises SAMBA and FIA sub-networks. SAMBA adopts a coarse-to-fine approach for reflectance correction, while FIA operates in the frequency domain to adjust color and illumination focusing on global context.

Result: RetinexDual outperformed recent methods qualitatively and quantitatively across four UHD IR tasks: deraining, deblurring, dehazing, and Low-Light Image Enhancement.

Conclusion: This study establishes RetinexDual as an effective framework for UHD IR tasks, validating its distinct design and components through ablation studies.

Abstract: Advancements in image sensing have elevated the importance of
Ultra-High-Definition Image Restoration (UHD IR). Traditional methods, such as
extreme downsampling or transformation from the spatial to the frequency
domain, encounter significant drawbacks: downsampling induces irreversible
information loss in UHD images, while our frequency analysis reveals that pure
frequency-domain approaches are ineffective for spatially confined image
artifacts, primarily due to the loss of degradation locality. To overcome these
limitations, we present RetinexDual, a novel Retinex theory-based framework
designed for generalized UHD IR tasks. RetinexDual leverages two complementary
sub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination
Adaptor (FIA). SAMBA, responsible for correcting the reflectance component,
utilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba,
which effectively reduces artifacts and restores intricate details. On the
other hand, FIA ensures precise correction of color and illumination
distortions by operating in the frequency domain and leveraging the global
context provided by it. Evaluating RetinexDual on four UHD IR tasks, namely
deraining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows
that it outperforms recent methods qualitatively and quantitatively. Ablation
studies demonstrate the importance of employing distinct designs for each
branch in RetinexDual, as well as the effectiveness of its various components.

</details>


### [40] [ACM Multimedia Grand Challenge on ENT Endoscopy Analysis](https://arxiv.org/abs/2508.04801)
*Trong-Thuan Nguyen,Viet-Tham Huynh,Thao Thi Phuong Dao,Ha Nguyen Thi,Tien To Vu Thuy,Uyen Hanh Tran,Tam V. Nguyen,Thanh Dinh Le,Minh-Triet Tran*

Main category: cs.CV

TL;DR: The paper introduces ENTRap, a benchmark for automated analysis of ENT endoscopy images, supporting bilingual classification and retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Automated analysis for ENT imagery is underdeveloped due to variability, subtle findings, and fine-grained distinctions. Clinicians need tools for reliable case retrieval and concise descriptions.

Method: The authors introduce a dataset with expert annotations, dual-language descriptions, and propose benchmarks for anatomical classification and bilingual image and text retrieval.

Result: They created standards for submissions, evaluated systems with public/private tests, and analyzed top-performing team results.

Conclusion: ENTRap aims to advance ENT endoscopy analysis by integrating classification and retrieval tasks, enhancing standardization and bilingual accessibility.

Abstract: Automated analysis of endoscopic imagery is a critical yet underdeveloped
component of ENT (ear, nose, and throat) care, hindered by variability in
devices and operators, subtle and localized findings, and fine-grained
distinctions such as laterality and vocal-fold state. In addition to
classification, clinicians require reliable retrieval of similar cases, both
visually and through concise textual descriptions. These capabilities are
rarely supported by existing public benchmarks. To this end, we introduce
ENTRep, the ACM Multimedia 2025 Grand Challenge on ENT endoscopy analysis,
which integrates fine-grained anatomical classification with image-to-image and
text-to-image retrieval under bilingual (Vietnamese and English) clinical
supervision. Specifically, the dataset comprises expert-annotated images,
labeled for anatomical region and normal or abnormal status, and accompanied by
dual-language narrative descriptions. In addition, we define three benchmark
tasks, standardize the submission protocol, and evaluate performance on public
and private test splits using server-side scoring. Moreover, we report results
from the top-performing teams and provide an insight discussion.

</details>


### [41] [CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework](https://arxiv.org/abs/2508.04816)
*Sriram Mandalika,Lalitha V*

Main category: cs.CV

TL;DR: CoMAD is a lightweight framework unifying knowledge from multiple self-supervised ViT embeddings into a compact student model, achieving state-of-the-art performance in compact distillation.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning methods like Masked Image Modeling and Contrastive Learning produce powerful representations but are computationally expensive and trained in isolation, limiting their practical use in resource-constrained environments.

Method: CoMAD unifies knowledge from three pretrained ViT-Base teachers using asymmetric patch masking, linear adapter alignment, consensus gating, and KL divergence training for effective distillation into a compact student network.

Result: CoMAD's ViT-Tiny achieves 75.4 percent Top-1 accuracy on ImageNet-1K, outperforming prior compact SSL distillation methods. It also sets benchmarks in dense-prediction tasks like ADE20K and MS-COCO.

Conclusion: CoMAD demonstrates effective integration of complementary insights from self-supervised ViT models, offering resource-efficient solutions without sacrificing performance in both classification and dense-prediction tasks.

Abstract: Numerous self-supervised learning paradigms, such as contrastive learning and
masked image modeling, learn powerful representations from unlabeled data but
are typically pretrained in isolation, overlooking complementary insights and
yielding large models that are impractical for resource-constrained deployment.
To overcome these challenges, we introduce Consensus-oriented Masked
Distillation (CoMAD), a lightweight, parameter-free framework that unifies
knowledge from multiple current state-of-the-art self-supervised Vision
Transformers into a compact student network. CoMAD distills from three
pretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct
semantic and contextual priors. Rather than naively averaging teacher outputs,
we apply asymmetric masking: the student sees only 25 percent of patches while
each teacher receives a progressively lighter, unique mask, forcing the student
to interpolate missing features under richer contexts. Teacher embeddings are
aligned to the student's space via a linear adapter and layer normalization,
then fused through our joint consensus gating, which weights each token by
combining cosine affinity with inter-teacher agreement. The student is trained
with dual-level KL divergence on visible tokens and reconstructed feature maps,
capturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny
achieves 75.4 percent Top-1, an increment of 0.4 percent over the previous
state-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU
on ADE20K, and 44.5 percent box average precision and 40.5 percent mask average
precision on MS-COCO, establishing a new state-of-the-art in compact SSL
distillation.

</details>


### [42] [Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models](https://arxiv.org/abs/2508.04818)
*Mehrdad Moradi,Marco Grasso,Bianca Maria Colosimo,Kamran Paynabar*

Main category: cs.CV

TL;DR: RADAR outperforms existing anomaly detection methods by utilizing attention-based diffusion models for direct anomaly mapping, achieving better accuracy, computational efficiency, and F1 scores.


<details>
  <summary>Details</summary>
Motivation: Existing reconstruction-based anomaly detection methods using diffusion models are computationally expensive, struggle with subtle patterns, and lack adaptability to application-specific noise levels.

Method: RADAR uses attention-based diffusion models to directly generate anomaly maps, eliminating the need for an iterative reconstruction process and leveraging attention mechanisms for improved accuracy.

Result: RADAR surpasses state-of-the-art methods in accuracy, precision, recall, and F1 score, with a 7% improvement in F1 score for MVTec-AD and 13% for 3D-printed materials datasets.

Conclusion: RADAR presents a significant advancement in real-time anomaly detection, addressing limitations in reconstruction-based approaches and delivering superior performance metrics.

Abstract: Generative models have demonstrated significant success in anomaly detection
and segmentation over the past decade. Recently, diffusion models have emerged
as a powerful alternative, outperforming previous approaches such as GANs and
VAEs. In typical diffusion-based anomaly detection, a model is trained on
normal data, and during inference, anomalous images are perturbed to a
predefined intermediate step in the forward diffusion process. The
corresponding normal image is then reconstructed through iterative reverse
sampling.
  However, reconstruction-based approaches present three major challenges: (1)
the reconstruction process is computationally expensive due to multiple
sampling steps, making real-time applications impractical; (2) for complex or
subtle patterns, the reconstructed image may correspond to a different normal
pattern rather than the original input; and (3) Choosing an appropriate
intermediate noise level is challenging because it is application-dependent and
often assumes prior knowledge of anomalies, an assumption that does not hold in
unsupervised settings.
  We introduce Reconstruction-free Anomaly Detection with Attention-based
diffusion models in Real-time (RADAR), which overcomes the limitations of
reconstruction-based anomaly detection. Unlike current SOTA methods that
reconstruct the input image, RADAR directly produces anomaly maps from the
diffusion model, improving both detection accuracy and computational
efficiency. We evaluate RADAR on real-world 3D-printed material and the
MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and
statistical machine learning models across all key metrics, including accuracy,
precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on
MVTec-AD and 13% on the 3D-printed material dataset compared to the next best
model.
  Code available at: https://github.com/mehrdadmoradi124/RADAR

</details>


### [43] [A deep learning approach to track eye movements based on events](https://arxiv.org/abs/2508.04827)
*Chirag Seth,Divya Naiken,Keyan Lin*

Main category: cs.CV

TL;DR: This paper proposes a cost-effective deep learning algorithm to locate eye center positions using event cameras for applications in VR/AR, achieving 81% accuracy with a CNN_LSTM model.


<details>
  <summary>Details</summary>
Motivation: Precise eye tracking is crucial for VR/AR applications but traditionally requires expensive equipment. The study seeks a cost-effective solution leveraging event cameras and deep learning.

Method: The research tested various algorithms and identified the CNN_LSTM model as the most effective. It aims to improve interpretability through methods like Layer-wise Relevance Propagation (LRP).

Result: The CNN_LSTM model achieved approximately 81% accuracy in predicting eye center positions, showing promise for practical applications.

Conclusion: The study demonstrates the feasibility of using event cameras and deep learning for accurate and cost-effective eye tracking, hinting at future work for better interpretability and performance.

Abstract: This research project addresses the challenge of accurately tracking eye
movements during specific events by leveraging previous research. Given the
rapid movements of human eyes, which can reach speeds of 300{\deg}/s, precise
eye tracking typically requires expensive and high-speed cameras. Our primary
objective is to locate the eye center position (x, y) using inputs from an
event camera. Eye movement analysis has extensive applications in consumer
electronics, especially in VR and AR product development. Therefore, our
ultimate goal is to develop an interpretable and cost-effective algorithm using
deep learning methods to predict human attention, thereby improving device
comfort and enhancing overall user experience. To achieve this goal, we
explored various approaches, with the CNN\_LSTM model proving most effective,
achieving approximately 81\% accuracy. Additionally, we propose future work
focusing on Layer-wise Relevance Propagation (LRP) to further enhance the
model's interpretability and predictive performance.

</details>


### [44] [LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction](https://arxiv.org/abs/2508.04847)
*Md Zahidul Hasan,A. Ben Hamza,Nizar Bouguila*

Main category: cs.CV

TL;DR: Presented LuKAN, a 3D human motion prediction model that balances accuracy and efficiency using Kolmogorov-Arnold Networks and Lucas polynomials.


<details>
  <summary>Details</summary>
Motivation: To address limitations in balancing prediction accuracy and computational efficiency in 3D human motion prediction.

Method: Utilizes discrete wavelet transform for temporal encoding, a spatial projection layer for inter-joint dependency, and a KAN layer with Lucas polynomials for efficient function approximation.

Result: Extensive experiments on three benchmark datasets show competitive performance against strong baselines, both quantitatively and qualitatively.

Conclusion: LuKAN achieves temporally coherent predictions with a compact architecture, offering computational efficiency and robust prediction capabilities.

Abstract: The goal of 3D human motion prediction is to forecast future 3D poses of the
human body based on historical motion data. Existing methods often face
limitations in achieving a balance between prediction accuracy and
computational efficiency. In this paper, we present LuKAN, an effective model
based on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations.
Our model first applies the discrete wavelet transform to encode temporal
information in the input motion sequence. Then, a spatial projection layer is
used to capture inter-joint dependencies, ensuring structural consistency of
the human body. At the core of LuKAN is the Temporal Dependency Learner, which
employs a KAN layer parameterized by Lucas polynomials for efficient function
approximation. These polynomials provide computational efficiency and an
enhanced capability to handle oscillatory behaviors. Finally, the inverse
discrete wavelet transform reconstructs motion sequences in the time domain,
generating temporally coherent predictions. Extensive experiments on three
benchmark datasets demonstrate the competitive performance of our model
compared to strong baselines, as evidenced by both quantitative and qualitative
evaluations. Moreover, its compact architecture coupled with the linear
recurrence of Lucas polynomials, ensures computational efficiency.

</details>


### [45] [VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence](https://arxiv.org/abs/2508.04852)
*Chenhui Qiang,Zhaoyang Wei,Xumeng Han Zipeng Wang,Siyao Li,Xiangyuan Lan,Jianbin Jiao,Zhenjun Han*

Main category: cs.CV

TL;DR: This paper introduces VER-Bench, a new benchmark designed to evaluate MLLMs' capability to identify subtle visual clues and perform complex reasoning, addressing the limitations of existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current MLLM benchmarks either focus on basic perception or prominent image elements but fail to evaluate models' ability to interpret subtle, intricate details critical for profound visual understanding and reasoning.

Method: The paper presents VER-Bench, a framework with 374 tailored questions on six reasoning categories (Geospatial, Temporal, Situational, Intent, System State, Symbolic), along with structured evidence, designed to assess models' abilities in fine-grained clue identification and reasoning.

Result: VER-Bench demonstrates that current models have limited capabilities in extracting subtle visual evidence and constructing evidence-based reasoning, exposing a gap in their visual understanding skills.

Conclusion: The study highlights the need for improved MLLMs with robust capabilities in subtle evidence extraction and complex reasoning for better visual understanding and human-like analysis.

Abstract: With the rapid development of MLLMs, evaluating their visual capabilities has
become increasingly crucial. Current benchmarks primarily fall into two main
types: basic perception benchmarks, which focus on local details but lack deep
reasoning (e.g., "what is in the image?"), and mainstream reasoning benchmarks,
which concentrate on prominent image elements but may fail to assess subtle
clues requiring intricate analysis. However, profound visual understanding and
complex reasoning depend more on interpreting subtle, inconspicuous local
details than on perceiving salient, macro-level objects. These details, though
occupying minimal image area, often contain richer, more critical information
for robust analysis. To bridge this gap, we introduce the VER-Bench, a novel
framework to evaluate MLLMs' ability to: 1) identify fine-grained visual clues,
often occupying on average just 0.25% of the image area; 2) integrate these
clues with world knowledge for complex reasoning. Comprising 374 carefully
designed questions across Geospatial, Temporal, Situational, Intent, System
State, and Symbolic reasoning, each question in VER-Bench is accompanied by
structured evidence: visual clues and question-related reasoning derived from
them. VER-Bench reveals current models' limitations in extracting subtle visual
evidence and constructing evidence-based arguments, highlighting the need to
enhance models's capabilities in fine-grained visual evidence extraction,
integration, and reasoning for genuine visual understanding and human-like
analysis. Dataset and additional materials are available
https://github.com/verbta/ACMMM-25-Materials.

</details>


### [46] [RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation](https://arxiv.org/abs/2508.02903)
*Mehrdad Moradi,Kamran Paynabar*

Main category: cs.CV

TL;DR: The paper introduces robust denoising diffusion models designed to handle contaminated data, improving unsupervised anomaly segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models require access to purely normal data for training, which is unrealistic in many practical applications dealing with contaminated datasets.

Method: The paper reinterprets denoising diffusion probabilistic models as a nonlinear regression problem, applying robust regression techniques to derive robust diffusion models capable of handling contaminated data.

Result: The proposed method achieves significant improvements over existing diffusion models, with up to 8.08% higher AUROC and 10.37% higher AUPRC performance on the MVTec dataset.

Conclusion: The introduced framework broadens the applicability of diffusion models to scenarios without pure normal data, marking progress in unsupervised anomaly segmentation and enhancing real-world utility.

Abstract: Recent advancements in diffusion models have demonstrated significant success
in unsupervised anomaly segmentation. For anomaly segmentation, these models
are first trained on normal data; then, an anomalous image is noised to an
intermediate step, and the normal image is reconstructed through backward
diffusion. Unlike traditional statistical methods, diffusion models do not rely
on specific assumptions about the data or target anomalies, making them
versatile for use across different domains. However, diffusion models typically
assume access to normal data for training, limiting their applicability in
realistic settings. In this paper, we propose novel robust denoising diffusion
models for scenarios where only contaminated (i.e., a mix of normal and
anomalous) unlabeled data is available. By casting maximum likelihood
estimation of the data as a nonlinear regression problem, we reinterpret the
denoising diffusion probabilistic model through a regression lens. Using robust
regression, we derive a robust version of denoising diffusion probabilistic
models. Our novel framework offers flexibility in constructing various robust
diffusion models. Our experiments show that our approach outperforms current
state of the art diffusion models, for unsupervised anomaly segmentation when
only contaminated data is available. Our method outperforms existing
diffusion-based approaches, achieving up to 8.08\% higher AUROC and 10.37\%
higher AUPRC on MVTec datasets. The implementation code is available at:
https://github.com/mehrdadmoradi124/RDDPM

</details>


### [47] [Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications](https://arxiv.org/abs/2508.04868)
*Noreen Anwar,Guillaume-Alexandre Bilodeau,Wassim Bouachir*

Main category: cs.CV

TL;DR: The paper introduces DAMM, an improved object detection framework addressing occlusions, localization, and efficiency issues by using multi-modal queries and dual-stream cross-attention.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with occlusions, fine-grained localization, and inefficiency due to fixed queries and dense attention in object detection tasks.

Method: The paper proposes DAMM, a framework utilizing appearance-based, positional, and random queries, alongside a dual-stream cross-attention module, to improve detection accuracy and efficiency.

Result: DAMM achieved state-of-the-art performance in average precision (AP) and recall on four challenging benchmarks.

Conclusion: The multi-modal query adaptation and dual-stream attention strategy employed by DAMM effectively enhance object detection under challenging scenarios.

Abstract: Transformer-based object detectors often struggle with occlusions,
fine-grained localization, and computational inefficiency caused by fixed
queries and dense attention. We propose DAMM, Dual-stream Attention with
Multi-Modal queries, a novel framework introducing both query adaptation and
structured cross-attention for improved accuracy and efficiency. DAMM
capitalizes on three types of queries: appearance-based queries from
vision-language models, positional queries using polygonal embeddings, and
random learned queries for general scene coverage. Furthermore, a dual-stream
cross-attention module separately refines semantic and spatial features,
boosting localization precision in cluttered scenes. We evaluated DAMM on four
challenging benchmarks, and it achieved state-of-the-art performance in average
precision (AP) and recall, demonstrating the effectiveness of multi-modal query
adaptation and dual-stream attention. Source code is at:
\href{https://github.com/DET-LIP/DAMM}{GitHub}.

</details>


### [48] [Revealing Temporal Label Noise in Multimodal Hateful Video Classification](https://arxiv.org/abs/2508.04900)
*Shuonan Yang,Tailin Chen,Rahul Singh,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: This paper highlights the issue of coarse video-level annotations in multimodal hateful video detection, emphasizing the importance of temporally fine-grained approaches.


<details>
  <summary>Details</summary>
Motivation: The proliferation of hateful content in online multimedia necessitates robust detection methods. Current techniques using coarse video-level annotations fail to address the temporal granularity, leading to label noise and reduced model performance.

Method: The authors trim hateful videos from two datasets using annotated timestamps to isolate explicitly hateful segments. They then conduct exploratory analyses and perform controlled experiments to investigate the impact of temporal noise on model performance.

Result: Their analysis shows significant semantic overlap and confusion due to coarse annotations. Experiments reveal that timestamp noise alters decision boundaries and reduces classification confidence, highlighting the importance of temporal context.

Conclusion: The study underscores the need for temporally aware models and benchmarks to improve the robustness and interpretability of hateful video detection systems.

Abstract: The rapid proliferation of online multimedia content has intensified the
spread of hate speech, presenting critical societal and regulatory challenges.
While recent work has advanced multimodal hateful video detection, most
approaches rely on coarse, video-level annotations that overlook the temporal
granularity of hateful content. This introduces substantial label noise, as
videos annotated as hateful often contain long non-hateful segments. In this
paper, we investigate the impact of such label ambiguity through a fine-grained
approach. Specifically, we trim hateful videos from the HateMM and
MultiHateClip English datasets using annotated timestamps to isolate explicitly
hateful segments. We then conduct an exploratory analysis of these trimmed
segments to examine the distribution and characteristics of both hateful and
non-hateful content. This analysis highlights the degree of semantic overlap
and the confusion introduced by coarse, video-level annotations. Finally,
controlled experiments demonstrated that time-stamp noise fundamentally alters
model decision boundaries and weakens classification confidence, highlighting
the inherent context dependency and temporal continuity of hate speech
expression. Our findings provide new insights into the temporal dynamics of
multimodal hateful videos and highlight the need for temporally aware models
and benchmarks for improved robustness and interpretability. Code and data are
available at
https://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise.

</details>


### [49] [Test-Time Adaptation for Video Highlight Detection Using Meta-Auxiliary Learning and Cross-Modality Hallucinations](https://arxiv.org/abs/2508.04924)
*Zahidul Islam,Sujoy Paul,Mrigank Rochan*

Main category: cs.CV

TL;DR: Highlight-TTA is proposed as a test-time adaptation framework to enhance video highlight detection by dynamically adapting models to the specific characteristics of each test video.


<details>
  <summary>Details</summary>
Motivation: Current video highlight detection methods fail to account for unique features of individual test videos, leading to limited generalization and suboptimal performance.

Method: The framework uses a cross-modality hallucination auxiliary task alongside the highlight detection task, with meta-auxiliary training enabling dynamic adaptation during testing.

Result: Highlight-TTA improves performance in video highlight detection across three state-of-the-art models and benchmark datasets.

Conclusion: The proposed framework addresses existing limitations and enhances video highlight detection through test-time dynamic model adaptation.

Abstract: Existing video highlight detection methods, although advanced, struggle to
generalize well to all test videos. These methods typically employ a generic
highlight detection model for each test video, which is suboptimal as it fails
to account for the unique characteristics and variations of individual test
videos. Such fixed models do not adapt to the diverse content, styles, or audio
and visual qualities present in new, unseen test videos, leading to reduced
highlight detection performance. In this paper, we propose Highlight-TTA, a
test-time adaptation framework for video highlight detection that addresses
this limitation by dynamically adapting the model during testing to better
align with the specific characteristics of each test video, thereby improving
generalization and highlight detection performance. Highlight-TTA is jointly
optimized with an auxiliary task, cross-modality hallucinations, alongside the
primary highlight detection task. We utilize a meta-auxiliary training scheme
to enable effective adaptation through the auxiliary task while enhancing the
primary task. During testing, we adapt the trained model using the auxiliary
task on the test video to further enhance its highlight detection performance.
Extensive experiments with three state-of-the-art highlight detection models
and three benchmark datasets show that the introduction of Highlight-TTA to
these models improves their performance, yielding superior results.

</details>


### [50] [Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens](https://arxiv.org/abs/2508.04928)
*Suchisrit Gangopadhyay,Jung-Hee Kim,Xien Chen,Patrick Rim,Hyoungseob Park,Alex Wong*

Main category: cs.CV

TL;DR: A method to adapt monocular depth estimators to fisheye images, using calibration tokens for latent embedding alignment.


<details>
  <summary>Details</summary>
Motivation: Monocular depth estimators struggle with covariate shifts introduced by fisheye cameras, which leads to inaccurate depth predictions.

Method: The approach introduces Calibration Tokens to align fisheye image embeddings with perspective embeddings, leveraging existing datasets and employing self-supervised training.

Result: The method improves depth estimation accuracy for fisheye images compared to state-of-the-art techniques without the need for retraining or finetuning.

Conclusion: Calibration Tokens enable effective adaptation of FMDEs to fisheye cameras, addressing covariate shift while reducing artifacts compared to traditional methods.

Abstract: We propose a method to extend foundational monocular depth estimators
(FMDEs), trained on perspective images, to fisheye images. Despite being
trained on tens of millions of images, FMDEs are susceptible to the covariate
shift introduced by changes in camera calibration (intrinsic, distortion)
parameters, leading to erroneous depth estimates. Our method aligns the
distribution of latent embeddings encoding fisheye images to those of
perspective images, enabling the reuse of FMDEs for fisheye cameras without
retraining or finetuning. To this end, we introduce a set of Calibration Tokens
as a light-weight adaptation mechanism that modulates the latent embeddings for
alignment. By exploiting the already expressive latent space of FMDEs, we posit
that modulating their embeddings avoids the negative impact of artifacts and
loss introduced in conventional recalibration or map projection to a canonical
reference frame in the image space. Our method is self-supervised and does not
require fisheye images but leverages publicly available large-scale perspective
image datasets. This is done by recalibrating perspective images to fisheye
images, and enforcing consistency between their estimates during training. We
evaluate our approach with several FMDEs, on both indoors and outdoors, where
we consistently improve over state-of-the-art methods using a single set of
tokens for both. Code available at:
https://github.com/JungHeeKim29/calibration-token.

</details>


### [51] [Toward Errorless Training ImageNet-1k](https://arxiv.org/abs/2508.04941)
*Bo Deng,Levi Heath*

Main category: cs.CV

TL;DR: This paper presents a high-performance feedforward neural network trained on the ImageNet 2012 dataset, reaching near-perfect accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to achieve high accuracy on image classification tasks using the ImageNet 2012 dataset and explore potential reasons for any performance limitations.

Method: A feedforward artificial neural network with 322,430,160 parameters and four-decimal precision was employed. It was trained using a specific method referenced from another work.

Result: The model achieved 98.3% accuracy, 99.69% Top-1 accuracy, and averaged 285.9 perfectly classified labels per batch despite a double-labeling issue in the dataset.

Conclusion: The model performs exceptionally well, but a double-labeling issue in the dataset might prevent achieving absolute 100% accuracy.

Abstract: In this paper, we describe a feedforward artificial neural network trained on
the ImageNet 2012 contest dataset [7] with the new method of [5] to an accuracy
rate of 98.3% with a 99.69 Top-1 rate, and an average of 285.9 labels that are
perfectly classified over the 10 batch partitions of the dataset. The best
performing model uses 322,430,160 parameters, with 4 decimal places precision.
We conjecture that the reason our model does not achieve a 100% accuracy rate
is due to a double-labeling problem, by which there are duplicate images in the
dataset with different labels.

</details>


### [52] [Accelerating Conditional Prompt Learning via Masked Image Modeling for Vision-Language Models](https://arxiv.org/abs/2508.04942)
*Phuoc-Nguyen Bui,Khanh-Binh Nguyen,Hyunseung Choo*

Main category: cs.CV

TL;DR: ProMIM is a framework that enhances prompt learning in vision-language models by using masked image modeling to improve generalization while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: The need for better generalization in VLMs using lightweight methods, as existing approaches like CoOp and CoCoOp suffer from overfitting to known classes.

Method: ProMIM integrates masked image modeling into VLM pipelines, masking visible image patches to generate instance-conditioned prompts, improving feature robustness without modifying core architectures.

Result: ProMIM improves generalization performance in both zero-shot and few-shot classification tasks while adding minimal computational cost to existing prompt learning approaches.

Conclusion: ProMIM offers a simple yet effective plug-and-play enhancement for VLM prompt learning, addressing overfitting and poor generalization in a lightweight manner.

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning but often
require resource-intensive training to adapt to new tasks. Prompt learning
techniques, such as CoOp and CoCoOp, offer efficient adaptation but tend to
overfit to known classes, limiting generalization to unseen categories. We
introduce ProMIM, a plug-and-play framework that enhances conditional prompt
learning by integrating masked image modeling (MIM) into existing VLM
pipelines. ProMIM leverages a simple yet effective masking strategy to generate
robust, instance-conditioned prompts, seamlessly augmenting methods like CoOp
and CoCoOp without altering their core architectures. By masking only visible
image patches and using these representations to guide prompt generation,
ProMIM improves feature robustness and mitigates overfitting, all while
introducing negligible additional computational cost. Extensive experiments
across zero-shot and few-shot classification tasks demonstrate that ProMIM
consistently boosts generalization performance when plugged into existing
approaches, providing a practical, lightweight solution for real-world
vision-language applications.

</details>


### [53] [TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring](https://arxiv.org/abs/2508.04943)
*Zhu Xu,Ting Lei,Zhimin Li,Guan Wang,Qingchao Chen,Yuxin Peng,Yang liu*

Main category: cs.CV

TL;DR: The paper introduces "Temporal-enhanced Relation-aware Knowledge Transferring" (TRKT) to tackle issues in Weakly Supervised Dynamic Scene Graph Generation (WS-DSGG), improving object detection and relationship prediction in videos by leveraging relational and motion-aware features.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing WS-DSGG methods that rely on object detectors trained on static images, which fail to localize objects effectively and predict relationships accurately in dynamic video frames.

Method: The authors propose the TRKT method, consisting of two key components: (1) Relation-aware knowledge mining using category-specific attention maps augmented by optical flow for motion-aware processing. (2) A Dual-stream Fusion Module that integrates these attention maps with external detector outputs to refine localization and confidence.

Result: TRKT achieves state-of-the-art performance on the Action Genome dataset, showing enhanced accuracy in object and relationship detection across video frames.

Conclusion: The proposed TRKT method significantly improves WS-DSGG tasks by addressing challenges of dynamic scenarios and elevating detection quality without heavy reliance on static pretrained detectors.

Abstract: Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each
video frame by detecting objects and predicting their relationships. Weakly
Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized
scene graph from a single frame per video for training. Existing WS-DSGG
methods depend on an off-the-shelf external object detector to generate pseudo
labels for subsequent DSGG training. However, detectors trained on static,
object-centric images struggle in dynamic, relation-aware scenarios required
for DSGG, leading to inaccurate localization and low-confidence proposals. To
address the challenges posed by external object detectors in WS-DSGG, we
propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT)
method, which leverages knowledge to enhance detection in relation-aware
dynamic scenarios. TRKT is built on two key components:(1)Relation-aware
knowledge mining: we first employ object and relation class decoders that
generate category-specific attention maps to highlight both object regions and
interactive areas. Then we propose an Inter-frame Attention Augmentation
strategy that exploits optical flow for neighboring frames to enhance the
attention maps, making them motion-aware and robust to motion blur. This step
yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we
introduce a Dual-stream Fusion Module that integrates category-specific
attention maps into external detections to refine object localization and boost
confidence scores for object proposals. Extensive experiments demonstrate that
TRKT achieves state-of-the-art performance on Action Genome dataset. Our code
is avaliable at https://github.com/XZPKU/TRKT.git.

</details>


### [54] [AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics](https://arxiv.org/abs/2508.04955)
*Stella Su,Marc Harary,Scott J. Rodig,William Lotter*

Main category: cs.CV

TL;DR: This paper introduces AdvDINO, a domain-adversarial self-supervised learning method to enhance domain-invariant feature extraction in biomedical imaging, addressing domain shifts and batch effects.


<details>
  <summary>Details</summary>
Motivation: Standard self-supervised learning struggles with domain shifts, which are prevalent in biomedical imaging and obscure true biological signals.

Method: AdvDINO integrates a gradient reversal layer into the DINOv2 architecture to promote domain-invariant feature learning.

Result: AdvDINO mitigates slide-specific biases, uncovers biologically significant phenotype clusters, enhances survival prediction, and demonstrates broad applicability beyond biomedical imaging.

Conclusion: AdvDINO improves robustness and generalization in imaging domains affected by domain shift, making it valuable for applications like radiology and autonomous driving.

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach for
learning visual representations without manual annotations. However, the
robustness of standard SSL methods to domain shift -- systematic differences
across data sources -- remains uncertain, posing an especially critical
challenge in biomedical imaging where batch effects can obscure true biological
signals. We present AdvDINO, a domain-adversarial self-supervised learning
framework that integrates a gradient reversal layer into the DINOv2
architecture to promote domain-invariant feature learning. Applied to a
real-world cohort of six-channel multiplex immunofluorescence (mIF) whole slide
images from non-small cell lung cancer patients, AdvDINO mitigates
slide-specific biases to learn more robust and biologically meaningful
representations than non-adversarial baselines. Across $>5.46$ million mIF
image tiles, the model uncovers phenotype clusters with distinct proteomic
profiles and prognostic significance, and improves survival prediction in
attention-based multiple instance learning. While demonstrated on mIF data,
AdvDINO is broadly applicable to other imaging domains -- including radiology,
remote sensing, and autonomous driving -- where domain shift and limited
annotated data hinder model generalization and interpretability.

</details>


### [55] [Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework](https://arxiv.org/abs/2508.04962)
*Peng Zhang,Songru Yang,Jinsheng Sun,Weiqing Li,Zhiyong Su*

Main category: cs.CV

TL;DR: HOW-Seg is a human-in-the-loop framework for open-world point cloud segmentation that refines class prototypes with sparse annotations, achieving state-of-the-art performance in generalized few-shot segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing OW-Seg methods rely on resource-intensive training or heavily annotated data, which limits scalability and practicality. The goal is to develop an efficient approach that alleviates these constraints.

Method: HOW-Seg constructs class prototypes directly from the query dataset, refines initial prototypes using a hierarchical prototype disambiguation mechanism, and applies a dense conditional random field for better contextual understanding. Iterative human feedback further enhances its predictions.

Result: HOW-Seg outperforms state-of-the-art generalized few-shot segmentation methods. It achieves 85.27% mIoU on the S3DIS dataset and 66.37% mIoU on the ScanNetv2 dataset with sparse human annotations and advanced network backbones.

Conclusion: The proposed HOW-Seg framework is efficient and scalable, demonstrating superior performance in OW-Seg tasks with minimal human intervention and achieving better results than existing methods.

Abstract: Open-world point cloud semantic segmentation (OW-Seg) aims to predict point
labels of both base and novel classes in real-world scenarios. However,
existing methods rely on resource-intensive offline incremental learning or
densely annotated support data, limiting their practicality. To address these
limitations, we propose HOW-Seg, the first human-in-the-loop framework for
OW-Seg. Specifically, we construct class prototypes, the fundamental
segmentation units, directly on the query data, avoiding the prototype bias
caused by intra-class distribution shifts between the support and query data.
By leveraging sparse human annotations as guidance, HOW-Seg enables
prototype-based segmentation for both base and novel classes. Considering the
lack of granularity of initial prototypes, we introduce a hierarchical
prototype disambiguation mechanism to refine ambiguous prototypes, which
correspond to annotations of different classes. To further enrich contextual
awareness, we employ a dense conditional random field (CRF) upon the refined
prototypes to optimize their label assignments. Through iterative human
feedback, HOW-Seg dynamically improves its predictions, achieving high-quality
segmentation for both base and novel classes. Experiments demonstrate that with
sparse annotations (e.g., one-novel-class-one-click), HOW-Seg matches or
surpasses the state-of-the-art generalized few-shot segmentation (GFS-Seg)
method under the 5-shot setting. When using advanced backbones (e.g.,
Stratified Transformer) and denser annotations (e.g., 10 clicks per sub-scene),
HOW-Seg achieves 85.27% mIoU on S3DIS and 66.37% mIoU on ScanNetv2,
significantly outperforming alternatives.

</details>


### [56] [UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS](https://arxiv.org/abs/2508.04968)
*Zhihao Guo,Peng Wang,Zidong Chen,Xiangyu Kong,Yan Lyu,Guanyu Gao,Liangxiu Han*

Main category: cs.CV

TL;DR: This paper introduces a method to enhance 3D Gaussian Splatting for novel view synthesis by applying adaptive Gaussian weighting through learned uncertainties, improving rendering quality, especially for sparse-view scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of traditional 3D Gaussian Splatting methods, which tend to overfit in sparse-view settings due to treating all Gaussians equally during rendering.

Method: The proposed method involves learned uncertainties for adaptive weighting of Gaussians, differentiable opacity adjustment, and soft dropout regularisation to manage Gaussian projections and blending effectively.

Result: Extensive experiments show significant improvement in sparse-view 3D synthesis, achieving higher reconstruction quality and using fewer Gaussians. The method demonstrates a 3.27% PSNR improvement on the MipNeRF 360 dataset compared to previous methods such as DropGaussian.

Conclusion: The paper concludes that employing adaptive Gaussian weighting through learned uncertainties enhances rendering quality in sparse-view scenarios, establishing a more efficient and accurate method for 3D Gaussian Splatting.

Abstract: 3D Gaussian Splatting (3DGS) has become a competitive approach for novel view
synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian
projection and blending. However, Gaussians are treated equally weighted for
rendering in most 3DGS methods, making them prone to overfitting, which is
particularly the case in sparse-view scenarios. To address this, we investigate
how adaptive weighting of Gaussians affects rendering quality, which is
characterised by learned uncertainties proposed. This learned uncertainty
serves two key purposes: first, it guides the differentiable update of Gaussian
opacity while preserving the 3DGS pipeline integrity; second, the uncertainty
undergoes soft differentiable dropout regularisation, which strategically
transforms the original uncertainty into continuous drop probabilities that
govern the final Gaussian projection and blending process for rendering.
Extensive experimental results over widely adopted datasets demonstrate that
our method outperforms rivals in sparse-view 3D synthesis, achieving higher
quality reconstruction with fewer Gaussians in most datasets compared to
existing sparse-view approaches, e.g., compared to DropGaussian, our method
achieves 3.27\% PSNR improvements on the MipNeRF 360 dataset.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [57] [OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks](https://arxiv.org/abs/2508.04833)
*Nicolas Nicolaou,Onyeka Obi,Aayush Rajasekaran,Alejandro Bergasov,Aleksandr Bezobchuk,Kishori M. Konwar,Michael Meier,Santiago Paiva,Har Preet Singh,Swarnabha Sinha*

Main category: cs.DC

TL;DR: The paper introduces OPTIMUMP2P, a gossip algorithm utilizing Random Linear Network Coding (RLNC) to enhance performance and reliability in decentralized systems, outperforming existing libp2p protocols.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and reliability of information dissemination in decentralized systems, particularly blockchain protocols.

Method: OPTIMUMP2P employs Random Linear Network Coding (RLNC) for faster and reliable message propagation in peer-to-peer networks, overcoming challenges from malicious data corruption.

Result: Evaluations, both simulated and real-world, indicate that OPTIMUMP2P delivers superior performance compared to the Gossipsub protocol.

Conclusion: OPTIMUMP2P effectively improves the performance and reliability of libp2p by leveraging RLNC, proving significant potential in decentralized systems.

Abstract: Gossip algorithms are pivotal in the dissemination of information within
decentralized systems. Consequently, numerous gossip libraries have been
developed and widely utilized especially in blockchain protocols for the
propagation of blocks and transactions. A well-established library is libp2p,
which provides two gossip algorithms: floodsup and gossibsup. These algorithms
enable the delivery of published messages to a set of peers. In this work we
aim to enhance the performance and reliability of libp2p by introducing
OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random
Linear Network Coding (RLNC) to expedite the dissemination of information in a
peer-to-peer (P2P) network while ensuring reliable delivery, even in the
presence of malicious actors capable of corrupting the transmitted data.
Preliminary research from the Ethereum Foundation has demonstrated the use of
RLNC in the significant improvement in the block propagation time [14]. Here we
present extensive evaluation results both in simulation and real-world
environments that demonstrate the performance gains of OPTIMUMP2P over the
Gossipsub protocol.

</details>


### [58] [Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model](https://arxiv.org/abs/2508.04870)
*Khaled Jawhar,Evangelos Kranakis*

Main category: cs.DC

TL;DR: The paper studies how two robots with asymmetric communication capabilities work together to capture a mobile target moving on a linear path using newly designed search algorithms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the role of asymmetric communication capabilities in improving the efficiency of robotic linear search to capture a mobile target.

Method: The authors develop linear search algorithms customized for two robots—Sender and Receiver—with distinct communication modes, accounting for different target speeds and movement models.

Result: Competitive ratios of the time to capture the target under varying scenarios (e.g., target speed, direction) are analyzed, demonstrating how the robots' asymmetric communication impacts efficiency.

Conclusion: The analysis underscores that asymmetric communication capabilities between robots influence the effectiveness of linear search strategies to capture moving targets.

Abstract: We consider linear search for capturing an oblivious moving target by two
autonomous robots with different communicating abilities. Both robots can
communicate Face-to-Face (F2F) when co-located but in addition one robot is a
Sender (can also send messages wirelessly) and the other also a Receiver (can
also receive messages wirelessly). This is known as Sender/Receiver (S/R, for
short) communication model. The robots can move with max speed $1$. The moving
target starts at distance $d$ from the origin and can move either with speed
$v<1$ away from the origin in the ``away'' model or with speed $v \geq 0$
toward the origin in the ``toward'' model. We assume that the direction of
motion of the target (i.e., whether it is the away or toward model) is known to
the robots in advance. To capture the target the two robots must be co-located
with it.
  We design new linear search algorithms and analyze the competitive ratio of
the time required to capture the target. The approach takes into account
various scenarios related to what the robots know about the search environment
(e.g., starting distance or speed of the mobile, away or toward model, or a
combination thereof). Our study contributes to understanding how asymmetric
communication affects the competitive ratio of linear search.

</details>


### [59] [Managing, Analyzing and Sharing Research Data with Gen3 Data Commons](https://arxiv.org/abs/2508.04944)
*Craig Barnes,Kyle Burton,Michael S. Fitzsimons,Hara Prasad Juvvala,Brienna Larrick,Christopher Meyer,Pauline Ribeyre,Ao Liu,Clint Malson,Noah Metoki-Shlubsky,Andrii Prokhorenkov,Jawad Qureshi,Radhika Reddy,L. Philip Schumm,Mingfei Shao,Trevar Simmons,Alexander VanTol,Peter Vassilatos,Aarti Venkat,Robert L. Grossman*

Main category: cs.DC

TL;DR: Gen3 is an open-source platform for building data commons, providing features for managing, analyzing, and sharing research data, equipped with autogenerated tools for portals and APIs.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to develop a scalable and efficient platform to manage, analyze, and share research data within communities, supporting FAIR principles.

Method: The Gen3 platform implements a defined data model to automatically generate tools like data portals for exploration and submission, and FAIR APIs for programmatic data access.

Result: Gen3 has successfully been used to create over a dozen data commons aggregating 28 PB of data and 64 million FAIR data objects.

Conclusion: Gen3 establishes itself as a scalable and interoperable solution for organizing and sharing research data while aligning with standards to ensure future compatibility with external platforms.

Abstract: Gen3 is an open-source data platform for building data commons. A data
commons is a cloud-based data platform for managing, analyzing, and sharing
data with a research community. Gen3 has been used to build over a dozen data
commons that in aggregate contain over 28 PB of data and 64 million FAIR data
objects. To set up a Gen3 data commons, you first define a data model. Gen3
then autogenerates 1) a data portal for searching and exploring data in the
commons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs
for accessing the data programmatically. Gen3 is built over a small number of
standards-based software services, which are designed to support current and
future Gen3 components so that Gen3 can interoperate with other data platforms
and data ecosystems.

</details>


### [60] [Tesserae: Scalable Placement Policies for Deep Learning Workloads](https://arxiv.org/abs/2508.04953)
*Song Bian,Saurabh Agarwal,Md. Tareq Mahmood,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: The paper introduces Tesserae, a scalable and efficient GPU cluster scheduler that leverages graph matching-based placement policies, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Improving resource utilization in data centers, particularly in DL cluster schedulers, through better job placement policies.

Method: The paper proposes designing placement policies based on graph matching problems to optimize job migration overheads and job packing, integrating these policies into Tesserae.

Result: Tesserae demonstrated improved performance metrics, achieving up to 1.62x better average Job Completion Time (JCT) and up to 1.15x improvement in Makespan compared to existing schedulers.

Conclusion: Graph matching-based placement policies are effective and scalable, and their integration into Tesserae significantly enhances GPU cluster scheduling performance.

Abstract: Training deep learning (DL) models has become a dominant workload in
data-centers and improving resource utilization is a key goal of DL cluster
schedulers. In order to do this, schedulers typically incorporate placement
policies that govern where jobs are placed on the cluster. Existing placement
policies are either designed as ad-hoc heuristics or incorporated as
constraints within a complex optimization problem and thus either suffer from
suboptimal performance or poor scalability. Our key insight is that many
placement constraints can be formulated as graph matching problems and based on
that we design novel placement policies for minimizing job migration overheads
and job packing. We integrate these policies into Tesserae and describe how our
design leads to a scalable and effective GPU cluster scheduler. Our
experimental results show that Tesserae improves average JCT by up to 1.62x and
the Makespan by up to 1.15x compared with the existing schedulers.

</details>


### [61] [Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations](https://arxiv.org/abs/2508.05020)
*Anjiang Wei,Hang Song,Mert Hidayetoglu,Elliott Slaughter,Sanjiva K. Lele,Alex Aiken*

Main category: cs.DC

TL;DR: This paper presents an AMR-based numerical solver for compressible flows in Regent, achieving significant speedups through task fusion and GPU kernel optimization.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency in high-order solvers for compressible flows using AMR while integrating with the Regent language and overcoming dynamic data structure, mesh validity, and performance bottleneck challenges.

Method: Developed an AMR-based solver implemented in Regent, tackled challenges like patch refinement, mesh validity, and task launch overhead through task fusion, and used simple annotations for GPU kernel generation.

Result: Task fusion achieved 18x speedup, and GPU kernel annotations delivered 9.7x speedup, validated through compressible flow Euler equation simulations.

Conclusion: The proposed solver is effective in leveraging AMR and Regent capabilities for efficiency in compressible flow simulations, showcasing substantial computational improvements.

Abstract: High-order solvers for compressible flows are vital in scientific
applications. Adaptive mesh refinement (AMR) is a key technique for reducing
computational cost by concentrating resolution in regions of interest. In this
work, we develop an AMR-based numerical solver using Regent, a high-level
programming language for the Legion programming model. We address several
challenges associated with implementing AMR in Regent. These include dynamic
data structures for patch refinement/coarsening, mesh validity enforcement, and
reducing task launch overhead via task fusion. Experimental results show that
task fusion achieves 18x speedup, while automated GPU kernel generation via
simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate
our approach through simulations of two canonical compressible flow problems
governed by the Euler equations.

</details>


### [62] [Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement](https://arxiv.org/abs/2508.05029)
*Felipe Aramburú,William Malpica,Kaouther Abrougui,Amin Aramoon,Romulo Auccapuclla,Claude Brisson,Matthijs Brobbel,Colby Farrell,Pradeep Garigipati,Joost Hoozemans,Supun Kamburugamuve,Akhil Nair,Alexander Ocsa,Johan Peltenburg,Rubén Quesada López,Deepak Sihag,Ahmet Uyar,Dhruv Vats,Michael Wendt,Jignesh M. Patel,Rodrigo Aramburú*

Main category: cs.DC

TL;DR: Theseus is a specialized, GPU-focused, distributed query engine that optimizes data movement, memory use, and computation, achieving significant performance gains on large-scale benchmarks.


<details>
  <summary>Details</summary>
Motivation: The need to handle terabyte-scale datasets efficiently with reduced costs, leveraging ubiquitous GPU accelerators despite the challenges of data movement and system optimization.

Method: Theseus employs asynchronous control mechanisms for network communication, data pre-loading, spilling, and GPU compute tasks, with customized memory allocation techniques to enhance throughput and reduce fragmentation.

Result: Theseus achieves up to 4x performance improvement over Databricks Photon with cost parity on TPC-H benchmarks, and efficiently handles 100TB scale datasets with minimal hardware.

Conclusion: Theseus establishes itself as a high-performance, cost-effective, GPU-optimized query engine capable of addressing enterprise-large-scale analytics needs.

Abstract: Online analytical processing of queries on datasets in the many-terabyte
range is only possible with costly distributed computing systems. To decrease
the cost and increase the throughput, systems can leverage accelerators such as
GPUs, which are now ubiquitous in the compute infrastructure. This introduces
many challenges, the majority of which are related to when, where, and how to
best move data around the system. We present Theseus -- a production-ready
enterprise-scale distributed accelerator-native query engine designed to
balance data movement, memory utilization, and computation in an
accelerator-based system context. Specialized asynchronous control mechanisms
are tightly coupled to the hardware resources for the purpose of network
communication, data pre-loading, data spilling across memories and storage, and
GPU compute tasks. The memory subsystem contains a mechanism for fixed-size
page-locked host memory allocations to increase throughput and reduce memory
fragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k
on cloud infrastructure, Theseus outperforms Databricks Photon by up to
$4\times$ at cost parity. Theseus is capable of processing all queries of the
TPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as
2 DGX A100 640GB nodes.

</details>


### [63] [Simulating LLM training workloads for heterogeneous compute and network infrastructure](https://arxiv.org/abs/2508.05370)
*Sumit Kumar,Arjun Temura,Naman Sharma,Ramanjeet Singh,Meet Dadhania,Praveen Tammana,Satananda Burla,Abed Mohammad Kamaluddin,Rinku Shah*

Main category: cs.DC

TL;DR: This paper introduces a heterogeneity-aware distributed LLM training simulator to estimate training time in settings with non-uniform compute and network infrastructure, addressing the gap between existing simulators and practical needs.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for efficient and realistic LLM training simulation due to hindrances like GPU cluster heterogeneity in modern distributed training, which current simulators overlook.

Method: The authors built a simulator incorporating abstractions for custom configurations in device groups and mapping, along with non-uniform workload partitioning, to account for heterogeneous infrastructures.

Result: Initial simulations showed the influence of heterogeneity on both computation and communication times in distributed LLM training.

Conclusion: The proposed simulator highlights how device heterogeneity impacts training performance and provides a valuable tool for optimizing models and systems in practical, varied environments.

Abstract: The growing demand for large-scale GPU clusters in distributed model training
presents a significant barrier to innovation, particularly in model
optimization, performance tuning, and system-level enhancements. To address
this challenge, LLM training simulators are employed to estimate training time
and guide design decisions. However, the state-of-the-art LLM training
simulators assume homogeneous compute and network infrastructure. In practice,
device heterogeneity is inevitable due to resource sharing in cloud
environments, frequent shifts in device generations, and inherent intra-chip
interconnect heterogeneity. To address the gap between state-of-the-art and
practical requirements, we propose the design of a heterogeneity-aware
distributed LLM simulator capable of predicting training time while enabling
abstractions to specify custom configurations for device groups and
device-to-parallelism mapping. We present the design requirements and
challenges in building a heterogeneity-aware distributed ML training simulator,
and design components such as non-uniform workload partitioning. Our initial
simulation results demonstrate the impact of heterogeneity on the model
computation and communication time.

</details>


### [64] [Adaptive Parallel Downloader for Large Genomic Datasets](https://arxiv.org/abs/2508.05511)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: FastBioDL is a parallel downloader for large biological datasets that uses adaptive concurrency to improve download speed, achieving up to 4x faster performance compared to existing tools.


<details>
  <summary>Details</summary>
Motivation: Researchers need efficient tools to download large-scale genomic datasets from public repositories, as current tools are unable to adapt to changing network conditions, leading to slow performance.

Method: FastBioDL models the download as an online optimization problem, using a utility function and gradient descent to dynamically adjust concurrent socket streams for maximum throughput.

Result: FastBioDL achieved up to a 4x speedup compared to existing tools on public genomic datasets and showed a 2.1x improvement even in high-speed networks.

Conclusion: FastBioDL democratizes high-performance genomic data downloading by providing a fast, adaptive, and resource-efficient solution using standard protocols, eliminating the need for specialized software or hardware.

Abstract: Modern next-generation sequencing (NGS) projects routinely generate terabytes
of data, which researchers commonly download from public repositories such as
SRA or ENA. Existing download tools often employ static concurrency settings,
leading to inefficient bandwidth utilization and prolonged download times due
to their inability to adapt to dynamic network conditions. We introduce
FastBioDL, a parallel file downloader designed for large biological datasets,
featuring an adaptive concurrency controller. FastBioDL frames the download
process as an online optimization problem, utilizing a utility function and
gradient descent to adjust the number of concurrent socket streams in real-time
dynamically. This approach maximizes download throughput while minimizing
resource overhead. Comprehensive evaluations on public genomic datasets
demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art
tools. Moreover, in high-speed network experiments, its adaptive design was up
to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP
or FTP downloads on the client side, FastBioDL provides a robust and efficient
solution for large-scale genomic data acquisition, democratizing
high-performance data retrieval for researchers without requiring specialized
commercial software or protocols.

</details>


### [65] [Modular Architecture for High-Performance and Low Overhead Data Transfers](https://arxiv.org/abs/2508.05546)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: AutoMDT is a modular architecture for efficient data transfer using deep reinforcement learning, reducing completion times by 68%.


<details>
  <summary>Details</summary>
Motivation: Traditional file transfer tools are inefficient due to fixed configurations and monolithic optimization approaches, causing resource underutilization.

Method: AutoMDT employs a modular design with a deep reinforcement learning agent using Proximal Policy Optimization for offline training in a lightweight simulator.

Result: AutoMDT achieves up to 8x faster convergence and reduces transfer completion times by 68% compared to existing solutions.

Conclusion: The modular architecture and adaptive capabilities of AutoMDT significantly enhance data transfer performance across geographically dispersed systems.

Abstract: High-performance applications necessitate rapid and dependable transfer of
massive datasets across geographically dispersed locations. Traditional file
transfer tools often suffer from resource underutilization and instability
because of fixed configurations or monolithic optimization methods. We propose
AutoMDT, a novel modular data transfer architecture that employs a deep
reinforcement learning based agent to simultaneously optimize concurrency
levels for read, network, and write operations. Our solution incorporates a
lightweight network-system simulator, enabling offline training of a Proximal
Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby
overcoming the impracticality of lengthy online training in production
networks. AutoMDT's modular design decouples I/O and network tasks, allowing
the agent to capture complex buffer dynamics precisely and to adapt quickly to
changing system and network conditions. Evaluations on production-grade
testbeds show that AutoMDT achieves up to 8x faster convergence and a 68%
reduction in transfer completion times compared with state-of-the-art
solutions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [NAEx: A Plug-and-Play Framework for Explaining Network Alignment](https://arxiv.org/abs/2508.04731)
*Shruti Saxena,Arijit Khan,Joydeep Chandra*

Main category: cs.LG

TL;DR: The paper introduces NAEx, a framework for explaining network alignment decisions by focusing on key subgraphs and features, improving interpretability without being model-specific.


<details>
  <summary>Details</summary>
Motivation: To enhance the interpretability of network alignment models, which is crucial in high-stakes domains where understanding alignment decisions is critical.

Method: NAEx uses learnable edge and feature masks to jointly parameterize graph structures and feature spaces, combined with a tailored optimization objective to ensure faithful and meaningful explanations.

Result: NAEx generates efficient, interpretable explanations for network alignment models on benchmark datasets, as validated by newly introduced evaluation metrics.

Conclusion: NAEx successfully addresses the challenge of explaining network alignment models in a model-agnostic and efficient manner, fostering trust and understanding in diverse applications.

Abstract: Network alignment (NA) identifies corresponding nodes across multiple
networks, with applications in domains like social networks, co-authorship, and
biology. Despite advances in alignment models, their interpretability remains
limited, making it difficult to understand alignment decisions and posing
challenges in building trust, particularly in high-stakes domains. To address
this, we introduce NAEx, a plug-and-play, model-agnostic framework that
explains alignment models by identifying key subgraphs and features influencing
predictions. NAEx addresses the key challenge of preserving the joint
cross-network dependencies on alignment decisions by: (1) jointly
parameterizing graph structures and feature spaces through learnable edge and
feature masks, and (2) introducing an optimization objective that ensures
explanations are both faithful to the original predictions and enable
meaningful comparisons of structural and feature-based similarities between
networks. NAEx is an inductive framework that efficiently generates NA
explanations for previously unseen data. We introduce evaluation metrics
tailored to alignment explainability and demonstrate NAEx's effectiveness and
efficiency on benchmark datasets by integrating it with four representative NA
models.

</details>


### [67] [Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment](https://arxiv.org/abs/2508.04865)
*Aleksander Boruch-Gruszecki,Yangtian Zi,Zixuan Wu,Tejas Oberoi,Carolyn Jane Anderson,Joydeep Biswas,Arjun Guha*

Main category: cs.LG

TL;DR: The paper introduces Agnostics, a language-agnostic post-training pipeline for improving large language models' performance in low-resource programming languages using reinforcement learning and universal verifiers.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with low-resource programming languages, which are crucial for science and engineering, due to limited pre-training data and per-language post-training requirements.

Method: Agnostics uses a pipeline involving I/O-based unit test conversion, language compilation configuration, and reinforcement learning with verifiable rewards to evaluate code based on externally observable behavior.

Result: Agnostics enhances the performance of LLMs like Qwen-3, achieving state-of-the-art results in several low-resource languages and enabling scalable adaptations across different models.

Conclusion: Agnostics simplifies RL post-training for any programming language, providing datasets, codes, and configurations for broader accessibility and efficiency.

Abstract: Large language models (LLMs) already excel at writing code in high-resource
languages such as Python and JavaScript, yet stumble on low-resource languages
that remain essential to science and engineering. Besides the obvious shortage
of pre-training data, post-training itself is a bottleneck: every new language
seems to require new datasets, test harnesses, and reinforcement-learning (RL)
infrastructure.
  We introduce Agnostics, a language-agnostic post-training pipeline that
eliminates this per-language engineering. The key idea is to judge code solely
by its externally observable behavior, so a single verifier can test solutions
written in any language. Concretely, we (i) use an LLM to rewrite existing
unit-test datasets into an I/O format, (ii) supply a short configuration that
tells the verifier how to compile and run a target language, and (iii) apply
reinforcement learning with verifiable rewards (RLVR) in a robust code
execution environment.
  Applied to five low-resource languages--Lua, Julia, R, OCaml, and
Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other
16B-70B open-weight models; (2) scales cleanly to larger and diverse model
families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for
${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on
MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
  We will release the language-agnostic training datasets (Ag-MBPP-X,
Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use
configurations, making RL post-training in any programming language as simple
as editing a short YAML file.

</details>


### [68] [LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation](https://arxiv.org/abs/2508.04732)
*Xiaoqi Dong,Xiangyu Zhou,Nicholas Evans,Yujia Lin*

Main category: cs.LG

TL;DR: LumiGen improves Text-to-Image (T2I) generation through Vision-Language Model (LVLM) integration for control and quality, featuring iterative refinement mechanisms.


<details>
  <summary>Details</summary>
Motivation: Enhance T2I generation to address limitations in content control, semantic consistency, and handling intricate instructions.

Method: Proposed LumiGen, combining Intelligent Prompt Parsing & Augmentation (IPPA) and Iterative Visual Feedback & Refinement (IVFR) modules for iterative improvement.

Result: LumiGen achieves state-of-the-art performance on LongBench-T2I Benchmark with notable advancements in text rendering and pose expression.

Conclusion: LVLM integration in LumiGen is effective for achieving controllable, high-quality T2I generation and addressing existing challenges.

Abstract: Text-to-Image (T2I) generation has made significant advancements with
diffusion models, yet challenges persist in handling complex instructions,
ensuring fine-grained content control, and maintaining deep semantic
consistency. Existing T2I models often struggle with tasks like accurate text
rendering, precise pose generation, or intricate compositional coherence.
Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in cross-modal understanding and instruction following. We propose
LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I
model performance, particularly in areas requiring fine-grained control,
through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an
Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt
enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which
acts as a "visual critic" to iteratively correct and optimize generated images.
Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a
superior average score of 3.08, outperforming state-of-the-art baselines.
Notably, our framework demonstrates significant improvements in critical
dimensions such as text rendering and pose expression, validating the
effectiveness of LVLM integration for more controllable and higher-quality
image generation.

</details>


### [69] [MissMecha: An All-in-One Python Package for Studying Missing Data Mechanisms](https://arxiv.org/abs/2508.04740)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: MissMecha is a Python toolkit addressing missing data in tabular datasets by simulating and evaluating under different assumptions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of incomplete data in real-world datasets and provide better tools for understanding and analyzing its impact.

Method: MissMecha uses simulation, visual diagnostics, MCAR testing, and type-aware evaluation to handle missing data under MCAR, MAR, and MNAR assumptions.

Result: MissMecha supports numerical and categorical data, provides easy diagnostics, and improves researchers' ability to evaluate and benchmark missing data studies.

Conclusion: MissMecha is positioned as a comprehensive tool for researchers and practitioners dealing with heterogeneous tabular datasets containing missing data.

Abstract: Incomplete data is a persistent challenge in real-world datasets, often
governed by complex and unobservable missing mechanisms. Simulating missingness
has become a standard approach for understanding its impact on learning and
analysis. However, existing tools are fragmented, mechanism-limited, and
typically focus only on numerical variables, overlooking the heterogeneous
nature of real-world tabular data. We present MissMecha, an open-source Python
toolkit for simulating, visualizing, and evaluating missing data under MCAR,
MAR, and MNAR assumptions. MissMecha supports both numerical and categorical
features, enabling mechanism-aware studies across mixed-type tabular datasets.
It includes visual diagnostics, MCAR testing utilities, and type-aware
imputation evaluation metrics. Designed to support data quality research,
benchmarking, and education,MissMecha offers a unified platform for researchers
and practitioners working with incomplete data.

</details>


### [70] [Echo State Networks for Bitcoin Time Series Prediction](https://arxiv.org/abs/2508.05416)
*Mansi Sharma,Enrico Sartor,Marc Cavazza,Helmut Prendinger*

Main category: cs.LG

TL;DR: The paper explores using Echo State Networks (ESNs) for forecasting stock and cryptocurrency prices, especially during volatile, chaotic market conditions, and shows superior performance compared to other methods.


<details>
  <summary>Details</summary>
Motivation: The highly volatile and non-stationary nature of stock and cryptocurrency prices makes forecasting challenging, requiring advanced techniques to handle such complexity.

Method: The authors applied Echo State Networks (ESNs) and used chaos analysis via the Lyapunov exponent to examine dynamics during chaotic periods. They compared ESNs' performance against existing machine learning methods like Boosting and Naive methods.

Result: The study found that ESNs outperform other machine learning models during chaotic market conditions, demonstrating robustness and accuracy under high chaos.

Conclusion: ESNs are effective tools for forecasting in volatile and chaotic markets, outperforming traditional methods and exhibiting robustness that aligns with chaos analysis findings.

Abstract: Forecasting stock and cryptocurrency prices is challenging due to high
volatility and non-stationarity, influenced by factors like economic changes
and market sentiment. Previous research shows that Echo State Networks (ESNs)
can effectively model short-term stock market movements, capturing nonlinear
patterns in dynamic data. To the best of our knowledge, this work is among the
first to explore ESNs for cryptocurrency forecasting, especially during extreme
volatility. We also conduct chaos analysis through the Lyapunov exponent in
chaotic periods and show that our approach outperforms existing machine
learning methods by a significant margin. Our findings are consistent with the
Lyapunov exponent analysis, showing that ESNs are robust during chaotic periods
and excel under high chaos compared to Boosting and Na\"ive methods.

</details>


### [71] [Edge-Assisted Collaborative Fine-Tuning for Multi-User Personalized Artificial Intelligence Generated Content (AIGC)](https://arxiv.org/abs/2508.04745)
*Nan Li,Wanting Yang,Marie Siew,Zehui Xiong,Binbin Chen,Shiwen Mao,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: The paper proposes a federated learning framework to enable efficient, privacy-conscious, and scalable diffusion models for generating personalized content on resource-limited edge devices.


<details>
  <summary>Details</summary>
Motivation: High-quality content generation using diffusion models is hindered by their high computational requirements, which are unsuitable for edge devices. Current cloud-based solutions compromise on privacy, scalability, and communication in multi-user scenarios.

Method: The paper introduces a hierarchical federated learning framework incorporating Low-Rank Adaptation (LoRA). It clusters clients based on task similarities for intra-cluster aggregation and facilitates inter-cluster interactions for hybrid content generation. Prompts are securely encoded to ensure privacy.

Result: The suggested framework achieves faster convergence and is viable for multi-user personalized content generation on edge devices, addressing efficiency and scalability under resource constraints.

Conclusion: The proposed approach successfully bridges the efficiency, privacy, and scalability gaps in personalized AIGC applications for edge devices, making it practical for real-world scenarios.

Abstract: Diffusion models (DMs) have emerged as powerful tools for high-quality
content generation, yet their intensive computational requirements for
inference pose challenges for resource-constrained edge devices. Cloud-based
solutions aid in computation but often fall short in addressing privacy risks,
personalization efficiency, and communication costs in multi-user edge-AIGC
scenarios. To bridge this gap, we first analyze existing edge-AIGC applications
in personalized content synthesis, revealing their limitations in efficiency
and scalability. We then propose a novel cluster-aware hierarchical federated
aggregation framework. Based on parameter-efficient local fine-tuning via
Low-Rank Adaptation (LoRA), the framework first clusters clients based on the
similarity of their uploaded task requirements, followed by an intra-cluster
aggregation for enhanced personalization at the server-side. Subsequently, an
inter-cluster knowledge interaction paradigm is implemented to enable
hybrid-style content generation across diverse clusters.Building upon federated
learning (FL) collaboration, our framework simultaneously trains personalized
models for individual users at the devices and a shared global model enhanced
with multiple LoRA adapters on the server,enabling efficient edge inference;
meanwhile, all prompts for clustering and inference are encoded prior to
transmission, thereby further mitigating the risk of plaintext leakage. Our
evaluations demonstrate that the framework achieves accelerated convergence
while maintaining practical viability for scalable multi-user personalized AIGC
services under edge constraints.

</details>


### [72] [Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search](https://arxiv.org/abs/2508.05433)
*Qinglong Hu,Xialiang Tong,Mingxuan Yuan,Fei Liu,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: This paper introduces MLES (Multimodal Large Language Model-assisted Evolutionary Search), a novel interpretable and high-performing method for policy discovery combining large language models and evolutionary mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in deep reinforcement learning, which hinders trust and real-world deployment, while retaining high performance in control tasks.

Method: The authors propose MLES, which combines multimodal large language models with evolutionary search for policy optimization. It also incorporates visual feedback-driven behavior analysis to identify failure patterns and improve policies.

Result: Experimental results show that MLES achieves efficiency and policy discovery capabilities comparable to Proximal Policy Optimization (PPO) across two control tasks, while providing transparent and traceable processes.

Conclusion: MLES overcomes predefined language limitations, enables reusable knowledge transfer, and scales across control tasks, presenting itself as a promising approach for interpretable and adaptable control policy discovery.

Abstract: Interpretability and high performance are essential goals in designing
control policies, particularly for safety-critical tasks. Deep reinforcement
learning has greatly enhanced performance, yet its inherent lack of
interpretability often undermines trust and hinders real-world deployment. This
work addresses these dual challenges by introducing a novel approach for
programmatic policy discovery, called Multimodal Large Language Model-assisted
Evolutionary Search (MLES). MLES utilizes multimodal large language models as
policy generators, combining them with evolutionary mechanisms for automatic
policy optimization. It integrates visual feedback-driven behavior analysis
within the policy generation process to identify failure patterns and
facilitate targeted improvements, enhancing the efficiency of policy discovery
and producing adaptable, human-aligned policies. Experimental results show that
MLES achieves policy discovery capabilities and efficiency comparable to
Proximal Policy Optimization (PPO) across two control tasks, while offering
transparent control logic and traceable design processes. This paradigm
overcomes the limitations of predefined domain-specific languages, facilitates
knowledge transfer and reuse, and is scalable across various control tasks.
MLES shows promise as a leading approach for the next generation of
interpretable control policy discovery.

</details>


### [73] [A Foundational Multi-Modal Model for Few-Shot Learning](https://arxiv.org/abs/2508.04746)
*Pengtao Dang,Tingbo Guo,Sha Cao,Chi Zhang*

Main category: cs.LG

TL;DR: The paper introduces a novel Few-shot Learning (FSL) approach using a Large Multi-Modal Model (LMMM), demonstrating superior generalization on data-scarce scientific tasks through a newly proposed dataset (M3FD) and framework (M3F).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in data-scarce domains, especially in sciences like biomedical and environmental fields, where data collection is costly, time-intensive, and sometimes ethically challenging.

Method: The method involves building a Multi-Modal Model Few-shot Dataset (M3FD) containing diverse data types and tasks, and developing the M3F framework, which uses a modular pipeline to fine-tune an LMMM for improved performance on Few-shot Learning tasks.

Result: The proposed approach significantly improves generalization and outperforms traditional meta-learning models on few-shot tasks, making Large Multi-Modal Models (LMMMs) effective in real-world applications.

Conclusion: The study offers a scalable and accessible solution for deploying LMMMs in FSL across data-constrained domains through its novel dataset (M3FD) and framework (M3F), lowering barriers and promoting reproducibility.

Abstract: Few-shot learning (FSL) is a machine learning paradigm that aims to
generalize models from a small number of labeled examples, typically fewer than
10 per class. FSL is particularly crucial in biomedical, environmental,
materials, and mechanical sciences, where samples are limited and data
collection is often prohibitively costly, time-consuming, or ethically
constrained. In this study, we present an innovative approach to FSL by
demonstrating that a Large Multi-Modal Model (LMMM), trained on a set of
independent tasks spanning diverse domains, task types, and input modalities,
can substantially improve the generalization of FSL models, outperforming
models based on conventional meta-learning on tasks of the same type. To
support this, we first constructed a Multi-Modal Model Few-shot Dataset (M3FD,
over 10K+ few-shot samples), which includes 2D RGB images, 2D/3D medical scans,
tabular and time-course datasets, from which we manually curated FSL tasks such
as classification. We further introduced M3F (Multi-Modal Model for Few-shot
learning framework), a novel Large Multi-Modal Model framework tailored for
data-constrained scientific applications. M3F supports a wide range of
scientific data types through a modular pipeline. By fine-tuning the model on
M3FD, M3F improves model performance, making LMMM feasible for real-world FSL
deployment. The source code is located at https://github.com/ptdang1001/M3F. To
democratize access to complex FSL data and promote reproducibility for public
usage, M3FD is paired with a flexible and user-friendly tool that enables
efficient querying, task-specific sampling, and preprocessing. Together, our
dataset and framework offer a unified, scalable solution that significantly
lowers the barrier to applying LMMMs in data-scarce scientific domains.

</details>


### [74] [TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution](https://arxiv.org/abs/2508.05616)
*Zhikai Zhao,Chuanbo Hua,Federico Berto,Kanghoon Lee,Zihan Ma,Jiachen Li,Jinkyoo Park*

Main category: cs.LG

TL;DR: Introduction of TrajEvo, a framework that leverages Large Language Models (LLMs) and evolutionary algorithms for better trajectory prediction. It surpasses other methods in generalization to Out-of-Distribution (OOD) datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional handcrafted heuristics struggle with accuracy and adaptability, while deep learning offers better results but faces issues like high computational demands and poor generalization in Out-of-Distribution (OOD) scenarios.

Method: TrajEvo utilizes an evolutionary algorithm supported by LLMs. Key features include Cross-Generation Elite Sampling to ensure population diversity and a Statistics Feedback Loop for iterative refinement of trajectory prediction heuristics.

Result: TrajEvo demonstrated superior performance compared to traditional heuristics and deep learning approaches in trajectory prediction across multiple datasets, especially in OOD scenarios.

Conclusion: TrajEvo offers a promising method for creating fast, explainable, and generalizable trajectory prediction heuristics, showcasing its potential for applications in safety-critical domains.

Abstract: Trajectory prediction is a critical task in modeling human behavior,
especially in safety-critical domains such as social robotics and autonomous
vehicle navigation. Traditional heuristics based on handcrafted rules often
lack accuracy and generalizability. Although deep learning approaches offer
improved performance, they typically suffer from high computational cost,
limited explainability, and, importantly, poor generalization to
out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a
framework that leverages Large Language Models (LLMs) to automatically design
trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to
generate and refine prediction heuristics from past trajectory data. We propose
two key innovations: Cross-Generation Elite Sampling to encourage population
diversity, and a Statistics Feedback Loop that enables the LLM to analyze and
improve alternative predictions. Our evaluations demonstrate that TrajEvo
outperforms existing heuristic methods across multiple real-world datasets, and
notably surpasses both heuristic and deep learning methods in generalizing to
an unseen OOD real-world dataset. TrajEvo marks a promising step toward the
automated design of fast, explainable, and generalizable trajectory prediction
heuristics. We release our source code to facilitate future research at
https://github.com/ai4co/trajevo.

</details>


### [75] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: AttriLens-Mol proposes an attribute-guided reinforcement learning framework that improves molecular property prediction using LLMs without highly verbose or irrelevant reasoning.


<details>
  <summary>Details</summary>
Motivation: Current LLMs used for molecular property prediction often rely on human-made prompts and generate verbose, less relevant reasoning.

Method: AttriLens-Mol uses reinforcement learning rewards: format reward, count reward, and rationality reward to guide LLMs’ reasoning towards structured and relevant molecular attributes.

Result: Models trained with AttriLens-Mol outperformed both fine-tuned and advanced models in prediction performance using attributes as features for decision tree models.

Conclusion: AttriLens-Mol enhances performance and interpretability in molecular property prediction by eliciting more relevant and predictive molecular attributes, proving to be an effective alternative to existing methods.

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [76] [PA-RNet: Perturbation-Aware Reasoning Network for Multimodal Time Series Forecasting](https://arxiv.org/abs/2508.04750)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.LG

TL;DR: This paper introduces PA-RNet, a framework to improve multimodal time series forecasting by addressing textual noise and interference.


<details>
  <summary>Details</summary>
Motivation: Existing methods often neglect the impact of noisy or irrelevant textual data on multimodal time series forecasting, which significantly degrades model performance.

Method: PA-RNet leverages a perturbation-aware projection module and cross-modal attention mechanism to separate noise from textual embeddings while retaining meaningful semantic representations.

Result: PA-RNet demonstrated superior performance over state-of-the-art baselines in various domains and conditions, with theoretical guarantees of stability and reduced prediction error under noisy inputs.

Conclusion: PA-RNet effectively improves the robustness and generalization in multimodal forecasting tasks by addressing textual perturbations and can be seamlessly integrated into existing workflows.

Abstract: In real-world applications, multimodal time series data often suffer from
interference, especially in the textual modality. Existing methods for
multimodal time series forecasting often neglect the inherent perturbations
within textual data, where irrelevant, noisy, or ambiguous content can
significantly degrade model performance, particularly when the noise exhibits
varying intensity or stems from structural inconsistencies. To address this
challenge, we propose PA-RNet (Perturbation-Aware Reasoning Network for
Multimodal Time Series Forecasting), a robust multimodal forecasting framework.
PA-RNet features a perturbation-aware projection module and a cross-modal
attention mechanism to effectively separate noise from the textual embeddings
while maintaining semantically meaningful representations, thereby enhancing
the model's generalization ability. Theoretically, we establish the Lipschitz
continuity of PA-RNet with respect to textual inputs and prove that the
proposed perturbation module can reduce expected prediction error, offering
strong guarantees of stability under noisy conditions. Furthermore, we
introduce a textual perturbation pipeline that can be seamlessly incorporated
into existing multimodal time series forecasting tasks, allowing for systematic
evaluation of the model's robustness in the presence of varying levels of
textual noise. Extensive experiments across diverse domains and temporal
settings demonstrate that PA-RNet consistently outperforms state-of-the-art
baselines.

</details>


### [77] [InfoQ: Mixed-Precision Quantization via Global Information Flow](https://arxiv.org/abs/2508.04753)
*Mehmet Emre Akbulut,Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Manuel Roveri*

Main category: cs.LG

TL;DR: The paper introduces a training-free framework (InfoQ) for mixed-precision quantization (MPQ) that improves network quantization by focusing on the information flow disruption caused in layers and achieves better efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the computational complexity and inefficiency of finding the optimal bit-width for neural network layers, highlighting the limitations of current methods which use expensive searches or local sensitivity assessments that overlook global network effects.

Method: The authors propose InfoQ, a method that quantizes each layer at various bit-widths and measures mutual information changes through a single forward pass. These scores are used to solve bit-width allocation as an integer linear programming problem under a resource budget.

Result: InfoQ provides a significantly better search-time/accuracy trade-off compared to state-of-the-art methods (e.g., using two orders of magnitude less data than LIMPQ) and achieves up to a 1% accuracy improvement for compressed MobileNetV2 and ResNet18 models.

Conclusion: InfoQ reframes sensitivity measurement to account for global network effects, offering an efficient and retraining-free MPQ framework. This approach achieves better performance with lower computational costs, making it practical for resource-limited environments.

Abstract: Mixed-precision quantization (MPQ) is crucial for deploying deep neural
networks on resource-constrained devices, but finding the optimal bit-width for
each layer represents a complex combinatorial optimization problem. Current
state-of-the-art methods rely on computationally expensive search algorithms or
local sensitivity heuristic proxies like the Hessian, which fail to capture the
cascading global effects of quantization error. In this work, we argue that the
quantization sensitivity of a layer should not be measured by its local
properties, but by its impact on the information flow throughout the entire
network. We introduce InfoQ, a novel framework for MPQ that is training-free in
the bit-width search phase. InfoQ assesses layer sensitivity by quantizing each
layer at different bit-widths and measuring, through a single forward pass, the
resulting change in mutual information in the subsequent layers. This
quantifies how much each layer quantization impacts the network information
flow. The resulting scores are used to formulate bit-width allocation as an
integer linear programming problem, which is solved efficiently to minimize
total sensitivity under a given budget (e.g., model size or BitOps). Our
retraining-free search phase provides a superior search-time/accuracy trade-off
(using two orders of magnitude less data compared to state-of-the-art methods
such as LIMPQ), while yielding up to a 1% accuracy improvement for MobileNetV2
and ResNet18 on ImageNet at high compression rates (14X and 10.66X).

</details>


### [78] [Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle](https://arxiv.org/abs/2508.04755)
*Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: The paper explores the use of large language models (LLMs) as dynamic insulin dosing agents, showing promise in certain scenarios but highlighting their limitations in complex clinical inference.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in deploying reinforcement-learning-based dynamic treatment regimes for clinical decision-making, leveraging LLMs to embed clinical heuristics without intensive engineering.

Method: The authors conducted a rigorous evaluation by comparing zero-shot inference performance of open-source LLMs to trained small neural network-based RL agents in a Type 1 diabetes simulator.

Result: LLMs demonstrated competitive clinical performance with well-designed prompts but showed significant limitations like aggressive dosing and reasoning failures in complex cases.

Conclusion: Cautious adoption of LLMs in clinical workflows is recommended, integrating prompt engineering and hybrid approaches for safe and effective solutions.

Abstract: Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold
promise for automating complex clinical decision-making, yet their practical
deployment remains hindered by the intensive engineering required to inject
clinical knowledge and ensure patient safety. Recent advancements in large
language models (LLMs) suggest a complementary approach, where implicit prior
knowledge and clinical heuristics are naturally embedded through linguistic
prompts without requiring environment-specific training. In this study, we
rigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in
silico Type 1 diabetes simulator, comparing their zero-shot inference
performance against small neural network-based RL agents (SRAs) explicitly
trained for the task. Our results indicate that carefully designed zero-shot
prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or
superior clinical performance relative to extensively trained SRAs,
particularly in stable patient cohorts. However, LLMs exhibit notable
limitations, such as overly aggressive insulin dosing when prompted with
chain-of-thought (CoT) reasoning, highlighting critical failure modes including
arithmetic hallucination, temporal misinterpretation, and inconsistent clinical
logic. Incorporating explicit reasoning about latent clinical states (e.g.,
meals) yielded minimal performance gains, underscoring the current model's
limitations in capturing complex, hidden physiological dynamics solely through
textual inference. Our findings advocate for cautious yet optimistic
integration of LLMs into clinical workflows, emphasising the necessity of
targeted prompt engineering, careful validation, and potentially hybrid
approaches that combine linguistic reasoning with structured physiological
modelling to achieve safe, robust, and clinically effective decision-support
systems.

</details>


### [79] [HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation](https://arxiv.org/abs/2508.05135)
*Thinh Nguyen,Trung Phan,Binh T. Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: The paper introduces HFedATM, a method to improve robustness against domain shifts in Hierarchical Federated Learning by aligning and merging models more effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the challenges of domain shifts in Hierarchical Federated Learning frameworks, which negatively affect model performance across different data distributions.

Method: The proposed HFedATM method involves aligning convolutional filters using Filter-wise Optimal Transport Alignment and merging models through Shrinkage-aware Regularized Mean Aggregation.

Result: Experimental results show HFedATM significantly improves Federated Domain Generalization performance across datasets while being computationally and communication-efficient. Theoretical analysis confirms improved generalization error bounds and faster convergence.

Conclusion: HFedATM enhances performance and robustness in Hierarchical Federated Learning under domain shift conditions, showing potential for real-world distributed systems with diverse data distributions.

Abstract: Federated Learning (FL) is a decentralized approach where multiple clients
collaboratively train a shared global model without sharing their raw data.
Despite its effectiveness, conventional FL faces scalability challenges due to
excessive computational and communication demands placed on a single central
server as the number of participating devices grows. Hierarchical Federated
Learning (HFL) addresses these issues by distributing model aggregation tasks
across intermediate nodes (stations), thereby enhancing system scalability and
robustness against single points of failure. However, HFL still suffers from a
critical yet often overlooked limitation: domain shift, where data
distributions vary significantly across different clients and stations,
reducing model performance on unseen target domains. While Federated Domain
Generalization (FedDG) methods have emerged to improve robustness to domain
shifts, their integration into HFL frameworks remains largely unexplored. In
this paper, we formally introduce Hierarchical Federated Domain Generalization
(HFedDG), a novel scenario designed to investigate domain shift within
hierarchical architectures. Specifically, we propose HFedATM, a hierarchical
aggregation method that first aligns the convolutional filters of models from
different stations through Filter-wise Optimal Transport Alignment and
subsequently merges aligned models using a Shrinkage-aware Regularized Mean
Aggregation. Our extensive experimental evaluations demonstrate that HFedATM
significantly boosts the performance of existing FedDG baselines across
multiple datasets and maintains computational and communication efficiency.
Moreover, theoretical analyses indicate that HFedATM achieves tighter
generalization error bounds compared to standard hierarchical averaging,
resulting in faster convergence and stable training behavior.

</details>


### [80] [Gaussian mixture layers for neural networks](https://arxiv.org/abs/2508.04883)
*Sinho Chewi,Philippe Rigollet,Yuling Yan*

Main category: cs.LG

TL;DR: The paper introduces a Gaussian Mixture (GM) layer and explores direct training dynamics for networks using probability measures, validating its efficacy through experiments.


<details>
  <summary>Details</summary>
Motivation: To investigate whether training dynamics can be directly implemented over probability measures and potentially explore alternatives to conventional neural network training paradigms.

Method: The authors employ Gaussian mixture models and leverage Wasserstein gradient flows to derive training dynamics and introduce the Gaussian Mixture layer into neural network architectures.

Result: GM layers achieve performance comparable to two-layer fully connected networks on simple classification tasks and exhibit distinct behavior when compared to classical layers in mean-field regimes.

Conclusion: The GM layer is a promising addition to neural networks, offering a different conceptual and operational approach, with results suggesting it could serve as a viable alternative to classical network models.

Abstract: The mean-field theory for two-layer neural networks considers infinitely wide
networks that are linearly parameterized by a probability measure over the
parameter space. This nonparametric perspective has significantly advanced both
the theoretical and conceptual understanding of neural networks, with
substantial efforts made to validate its applicability to networks of moderate
width. In this work, we explore the opposite direction, investigating whether
dynamics can be directly implemented over probability measures. Specifically,
we employ Gaussian mixture models as a flexible and expressive parametric
family of distributions together with the theory of Wasserstein gradient flows
to derive training dynamics for such measures. Our approach introduces a new
type of layer -- the Gaussian mixture (GM) layer -- that can be integrated into
neural network architectures. As a proof of concept, we validate our proposal
through experiments on simple classification tasks, where a GM layer achieves
test performance comparable to that of a two-layer fully connected network.
Furthermore, we examine the behavior of these dynamics and demonstrate
numerically that GM layers exhibit markedly different behavior compared to
classical fully connected layers, even when the latter are large enough to be
considered in the mean-field regime.

</details>


### [81] [Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration](https://arxiv.org/abs/2508.04780)
*Lin Jiang,Dahai Yu,Rongchao Xu,Tian Tang,Guang Wang*

Main category: cs.LG

TL;DR: This paper identifies inequities in power restoration and proposes an equity-aware solution using a novel framework, EPOPR, which reduces outage duration and improves equity.


<details>
  <summary>Details</summary>
Motivation: The study aims to address disparities in power restoration decisions that disproportionately disadvantage certain communities, particularly in the wake of extreme weather events.

Method: The researchers propose a predict-then-optimize framework called EPOPR, combining Equity-Conformalized Quantile Regression for improved uncertainty prediction and Spatial-Temporal Attentional RL for equitable restoration decisions.

Result: The proposed EPOPR framework reduces average power outage duration by 3.60% and decreases inequity across communities by 14.19%, outperforming standard restoration methods.

Conclusion: EPOPR demonstrates that incorporating equity-aware mechanisms in power restoration strategies can significantly improve both efficiency and fairness, benefiting underserved communities.

Abstract: The increasing frequency of extreme weather events, such as hurricanes,
highlights the urgent need for efficient and equitable power system
restoration. Many electricity providers make restoration decisions primarily
based on the volume of power restoration requests from each region. However,
our data-driven analysis reveals significant disparities in request submission
volume, as disadvantaged communities tend to submit fewer restoration requests.
This disparity makes the current restoration solution inequitable, leaving
these communities vulnerable to extended power outages. To address this, we aim
to propose an equity-aware power restoration strategy that balances both
restoration efficiency and equity across communities. However, achieving this
goal is challenging for two reasons: the difficulty of predicting repair
durations under dataset heteroscedasticity, and the tendency of reinforcement
learning agents to favor low-uncertainty actions, which potentially undermine
equity. To overcome these challenges, we design a predict-then-optimize
framework called EPOPR with two key components: (1) Equity-Conformalized
Quantile Regression for uncertainty-aware repair duration prediction, and (2)
Spatial-Temporal Attentional RL that adapts to varying uncertainty levels
across regions for equitable decision-making. Experimental results show that
our EPOPR effectively reduces the average power outage duration by 3.60% and
decreases inequity between different communities by 14.19% compared to
state-of-the-art baselines.

</details>


### [82] [X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment](https://arxiv.org/abs/2508.05568)
*Qinghua Yao,Xiangrui Xu,Zhize Li*

Main category: cs.LG

TL;DR: X-VFL addresses challenges in Vertical Federated Learning (VFL) by enabling collaborative learning with non-aligned data samples and locally independent inference.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the limitations of VFL that include strict requirements for aligned data samples and the inability to support locally independent inference.

Method: Introducing X-VFL framework with two new modules – Cross Completion (XCom) for reconstructing missing features, and Decision Subspace Alignment (DS-Align) for feature alignment within the decision subspace.

Result: X-VFL demonstrates a 15% accuracy improvement on CIFAR-10 and 43% on MIMIC-III datasets compared to existing methods.

Conclusion: X-VFL proves its effectiveness and superiority for collaborative learning, particularly in cases of data samples with missing features and local inference needs.

Abstract: Vertical Federated Learning (VFL) enables collaborative learning by
integrating disjoint feature subsets from multiple clients/parties. However,
VFL typically faces two key challenges: i) the requirement for perfectly
aligned data samples across all clients (missing features are not allowed); ii)
the requirement for joint collaborative inference/prediction involving all
clients (it does not support locally independent inference on a single client).
To address these challenges, we propose X-VFL, a new VFL framework designed to
deal with the non-aligned data samples with (partially) missing features and to
support locally independent inference of new data samples for each client. In
particular, we design two novel modules in X-VFL: Cross Completion (XCom) and
Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing
features for non-aligned data samples by leveraging information from other
clients. DS-Align aligns local features with completed and global features
across all clients within the decision subspace, thus enabling locally
independent inference at each client. Moreover, we provide convergence theorems
for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$
convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type
algorithms, where $T$ denotes the number of training update steps. Extensive
experiments on real-world datasets demonstrate that X-VFL significantly
outperforms existing methods, e.g., achieving a 15% improvement in accuracy on
the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III
dataset. These results validate the practical effectiveness and superiority of
X-VFL, particularly in scenarios involving partially missing features and
locally independent inference.

</details>


### [83] [RCUKF: Data-Driven Modeling Meets Bayesian Estimation](https://arxiv.org/abs/2508.04985)
*Kumar Anurag,Kasra Azizi,Francesco Sorrentino,Wenbin Wan*

Main category: cs.LG

TL;DR: The paper introduces RCUKF, a framework combining reservoir computing (RC) and unscented Kalman filtering (UKF) for accurate state estimation in complex systems.


<details>
  <summary>Details</summary>
Motivation: Accurate process modeling for complex systems is often difficult, and existing mathematical models may fail in high-dimensional or chaotic scenarios.

Method: RCUKF leverages reservoir computing to learn system dynamics from data and integrates it with the unscented Kalman filter to estimate and correct system states using real-time sensor input.

Result: RCUKF showed strong performance on benchmark problems and in a simulated real-time vehicle trajectory estimation task.

Conclusion: RCUKF provides an effective hybrid modeling framework for complex systems, addressing challenges where standard models are unreliable.

Abstract: Accurate modeling is crucial in many engineering and scientific applications,
yet obtaining a reliable process model for complex systems is often
challenging. To address this challenge, we propose a novel framework, reservoir
computing with unscented Kalman filtering (RCUKF), which integrates data-driven
modeling via reservoir computing (RC) with Bayesian estimation through the
unscented Kalman filter (UKF). The RC component learns the nonlinear system
dynamics directly from data, serving as a surrogate process model in the UKF
prediction step to generate state estimates in high-dimensional or chaotic
regimes where nominal mathematical models may fail. Meanwhile, the UKF
measurement update integrates real-time sensor data to correct potential drift
in the data-driven model. We demonstrate RCUKF effectiveness on well-known
benchmark problems and a real-time vehicle trajectory estimation task in a
high-fidelity simulation environment.

</details>


### [84] [Federated Continual Recommendation](https://arxiv.org/abs/2508.04792)
*Jaehyung Lim,Wonbin Kweon,Woojoo Kim,Junyoung Kim,Seongjin Choi,Dongha Kim,Hwanjo Yu*

Main category: cs.LG

TL;DR: The paper introduces F3CRec, a framework combining Federated Learning (FL) and Continual Learning for privacy-preserving, evolving user preference recommendations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Federated Recommendation systems with non-stationary data streams while maintaining privacy.

Method: F3CRec utilizes Adaptive Replay Memory on clients for selective retention of past data and Item-wise Temporal Mean on servers to balance new and old information.

Result: F3CRec outperformed existing approaches in maintaining consistent recommendation quality over time in a federated setup.

Conclusion: Integrating FL and continual learning through F3CRec resolves privacy and non-stationary data challenges, ensuring long-term recommendation efficiency.

Abstract: The increasing emphasis on privacy in recommendation systems has led to the
adoption of Federated Learning (FL) as a privacy-preserving solution, enabling
collaborative training without sharing user data. While Federated
Recommendation (FedRec) effectively protects privacy, existing methods struggle
with non-stationary data streams, failing to maintain consistent recommendation
quality over time. On the other hand, Continual Learning Recommendation (CLRec)
methods address evolving user preferences but typically assume centralized data
access, making them incompatible with FL constraints. To bridge this gap, we
introduce Federated Continual Recommendation (FCRec), a novel task that
integrates FedRec and CLRec, requiring models to learn from streaming data
while preserving privacy. As a solution, we propose F3CRec, a framework
designed to balance knowledge retention and adaptation under the strict
constraints of FCRec. F3CRec introduces two key components: Adaptive Replay
Memory on the client side, which selectively retains past preferences based on
user-specific shifts, and Item-wise Temporal Mean on the server side, which
integrates new knowledge while preserving prior information. Extensive
experiments demonstrate that F3CRec outperforms existing approaches in
maintaining recommendation quality over time in a federated environment.

</details>


### [85] [Near Optimal Inference for the Best-Performing Algorithm](https://arxiv.org/abs/2508.05173)
*Amichai Painsky*

Main category: cs.LG

TL;DR: The paper introduces a novel method for subset selection to identify the best algorithm from a benchmark, improving upon existing methods with both asymptotic and finite-sample guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting the best-performing machine learning algorithm on unseen datasets when performance differences are marginal.

Method: A new framework for subset selection of multinomial distributions, which includes improved asymptotic and finite-sample schemes.

Result: The proposed schemes outperform existing ones and are supported by matching lower bounds.

Conclusion: The framework is effective and reliable for identifying high-performing candidates from benchmarks.

Abstract: Consider a collection of competing machine learning algorithms. Given their
performance on a benchmark of datasets, we would like to identify the best
performing algorithm. Specifically, which algorithm is most likely to rank
highest on a future, unseen dataset. A natural approach is to select the
algorithm that demonstrates the best performance on the benchmark. However, in
many cases the performance differences are marginal and additional candidates
may also be considered. This problem is formulated as subset selection for
multinomial distributions. Formally, given a sample from a countable alphabet,
our goal is to identify a minimal subset of symbols that includes the most
frequent symbol in the population with high confidence. In this work, we
introduce a novel framework for the subset selection problem. We provide both
asymptotic and finite-sample schemes that significantly improve upon currently
known methods. In addition, we provide matching lower bounds, demonstrating the
favorable performance of our proposed schemes.

</details>


### [86] [HCRide: Harmonizing Passenger Fairness and Driver Preference for Human-Centered Ride-Hailing](https://arxiv.org/abs/2508.04811)
*Lin Jiang,Yu Yang,Guang Wang*

Main category: cs.LG

TL;DR: The paper introduces HCRide, a human-centered ride-hailing system that balances system efficiency, passenger fairness, and driver preference using a novel multi-agent reinforcement learning approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the shortcomings of current ride-hailing systems that prioritize operator revenue at the expense of passenger and driver experiences.

Method: The authors propose HCRide, a system built on the Harmonization-oriented Actor-Bi-Critic (Habic) algorithm with a multi-agent competition mechanism, dynamic Actor network, and Bi-Critic network.

Result: HCRide improves system efficiency by 2.02%, passenger fairness by 5.39%, and driver preference by 10.21%, as tested on datasets from Shenzhen and New York City.

Conclusion: HCRide successfully balances passenger fairness and driver preference while maintaining overall system efficiency, improving ride-hailing services for all stakeholders.

Abstract: Order dispatch systems play a vital role in ride-hailing services, which
directly influence operator revenue, driver profit, and passenger experience.
Most existing work focuses on improving system efficiency in terms of operator
revenue, which may cause a bad experience for both passengers and drivers.
Hence, in this work, we aim to design a human-centered ride-hailing system by
considering both passenger fairness and driver preference without compromising
the overall system efficiency. However, it is nontrivial to achieve this target
due to the potential conflicts between passenger fairness and driver preference
since optimizing one may sacrifice the other. To address this challenge, we
design HCRide, a Human-Centered Ride-hailing system based on a novel
multi-agent reinforcement learning algorithm called Harmonization-oriented
Actor-Bi-Critic (Habic), which includes three major components (i.e., a
multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic
network) to optimize system efficiency and passenger fairness with driver
preference consideration. We extensively evaluate our HCRide using two
real-world ride-hailing datasets from Shenzhen and New York City. Experimental
results show our HCRide effectively improves system efficiency by 2.02%,
fairness by 5.39%, and driver preference by 10.21% compared to state-of-the-art
baselines.

</details>


### [87] [Unified Flow Matching for Long Horizon Event Forecasting](https://arxiv.org/abs/2508.04843)
*Xiao Shou*

Main category: cs.LG

TL;DR: The paper introduces a unified flow matching framework for modeling marked temporal point processes, enabling accurate and efficient long horizon predictions without autoregressive decoding.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of autoregressive neural temporal point process models, which struggle with efficiency and error accumulation in long-range forecasting.

Method: Proposes a non-autoregressive flow matching framework using continuous and discrete flow matching to jointly model inter-event times and event types, avoiding sequential decoding.

Result: The model achieves substantial performance improvements in accuracy and generation efficiency compared to existing baseline methods across six real-world datasets.

Conclusion: The unified flow matching approach successfully enhances long-horizon event sequence modeling, offering a more coherent and computationally efficient solution.

Abstract: Modeling long horizon marked event sequences is a fundamental challenge in
many real-world applications, including healthcare, finance, and user behavior
modeling. Existing neural temporal point process models are typically
autoregressive, predicting the next event one step at a time, which limits
their efficiency and leads to error accumulation in long-range forecasting. In
this work, we propose a unified flow matching framework for marked temporal
point processes that enables non-autoregressive, joint modeling of inter-event
times and event types, via continuous and discrete flow matching. By learning
continuous-time flows for both components, our method generates coherent long
horizon event trajectories without sequential decoding. We evaluate our model
on six real-world benchmarks and demonstrate significant improvements over
autoregressive and diffusion-based baselines in both accuracy and generation
efficiency.

</details>


### [88] [Negative Binomial Variational Autoencoders for Overdispersed Latent Modeling](https://arxiv.org/abs/2508.05423)
*Yixuan Zhang,Wenxin Zhang,Hua Jiang,Quyu Kong,Feng Zhou*

Main category: cs.LG

TL;DR: The paper introduces NegBio-VAE, a novel extension to VAEs that models neural spike counts using the negative binomial distribution instead of the Poisson distribution, offering improved reconstruction fidelity by accommodating the stochastic nature of neural activity.


<details>
  <summary>Details</summary>
Motivation: Current VAEs and Poisson-VAE fail to effectively capture the high variability of biological neuron spike trains due to limitations such as the rigid mean-variance constraint imposed by the Poisson distribution.

Method: NegBio-VAE uses the negative binomial distribution to model spike counts, providing explicit control over dispersion. It introduces tailored ELBO optimization schemes and reparameterization strategies to adapt to this new model.

Result: Empirical studies demonstrate that NegBio-VAE achieves superior reconstruction fidelity compared to Poisson-VAE, proving the importance of explicitly modeling overdispersion.

Conclusion: NegBio-VAE expands the VAE framework to better represent neural activity, showing that incorporating dispersion control improves the accuracy of neural representations and reconstruction performance.

Abstract: Biological neurons communicate through spike trains, discrete, irregular
bursts of activity that exhibit variability far beyond the modeling capacity of
conventional variational autoencoders (VAEs). Recent work, such as the
Poisson-VAE, makes a biologically inspired move by modeling spike counts using
the Poisson distribution. However, they impose a rigid constraint: equal mean
and variance, which fails to reflect the true stochastic nature of neural
activity. In this work, we challenge this constraint and introduce NegBio-VAE,
a principled extension of the VAE framework that models spike counts using the
negative binomial distribution. This shift grants explicit control over
dispersion, unlocking a broader and more accurate family of neural
representations. We further develop two ELBO optimization schemes and two
differentiable reparameterization strategies tailored to the negative binomial
setting. By introducing one additional dispersion parameter, NegBio-VAE
generalizes the Poisson latent model to a negative binomial formulation.
Empirical results demonstrate this minor yet impactful change leads to
significant gains in reconstruction fidelity, highlighting the importance of
explicitly modeling overdispersion in spike-like activations.

</details>


### [89] [Multi-Stage Knowledge-Distilled VGAE and GAT for Robust Controller-Area-Network Intrusion Detection](https://arxiv.org/abs/2508.04845)
*Robert Frenken,Sidra Ghayour Bhatti,Hanqin Zhang,Qadeer Ahmed*

Main category: cs.LG

TL;DR: This paper proposes a novel intrusion detection framework for CAN protocol traffic using unsupervised anomaly detection and supervised graph learning methods.


<details>
  <summary>Details</summary>
Motivation: The CAN protocol is widely used in vehicles but lacks inherent security measures, making it vulnerable to cyber attacks.

Method: The framework utilizes Variational Graph Autoencoder (VGAE) for anomaly detection, Knowledge-Distilled Graph Attention Network (KD-GAT) for attack classification, and encodes CAN bus activity as graph sequences.

Result: The compact student GAT model significantly reduces parameters by 96%, achieves competitive accuracy, and improves F1-scores by an average of 16.2% across six public CAN intrusion datasets.

Conclusion: The presented approach demonstrates superior performance for identifying and classifying attacks in imbalanced CAN datasets, thereby enhancing automotive cyber security.

Abstract: The Controller Area Network (CAN) protocol is a standard for in-vehicle
communication but remains susceptible to cyber-attacks due to its lack of
built-in security. This paper presents a multi-stage intrusion detection
framework leveraging unsupervised anomaly detection and supervised graph
learning tailored for automotive CAN traffic. Our architecture combines a
Variational Graph Autoencoder (VGAE) for structural anomaly detection with a
Knowledge-Distilled Graph Attention Network (KD-GAT) for robust attack
classification. CAN bus activity is encoded as graph sequences to model
temporal and relational dependencies. The pipeline applies VGAE-based selective
undersampling to address class imbalance, followed by GAT classification with
optional score-level fusion. The compact student GAT achieves 96% parameter
reduction compared to the teacher model while maintaining strong predictive
performance. Experiments on six public CAN intrusion datasets--Car-Hacking,
Car-Survival, and can-train-and-test--demonstrate competitive accuracy and
efficiency, with average improvements of 16.2% in F1-score over existing
methods, particularly excelling on highly imbalanced datasets with up to 55%
F1-score improvements.

</details>


### [90] [Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos](https://arxiv.org/abs/2508.04853)
*Haoyu Zhang,Shihao Zhang,Ian Colbert,Rayan Saab*

Main category: cs.LG

TL;DR: The paper provides the first mathematical error bounds for the widely used Post-training quantization methods OPTQ and Qronos, improving understanding and user guidance.


<details>
  <summary>Details</summary>
Motivation: The need to assess the theoretical reliability of OPTQ and Qronos PTQ methods, which are widely utilized for reducing neural network costs but lack quantitative guarantees.

Method: The authors derive non-asymptotic error bounds for deterministic and stochastic variants of OPTQ and Qronos by analyzing their quantization procedures against calibration data and regularization parameters.

Result: They established 2-norm and infinity-norm error bounds for OPTQ and Qronos, offering insights on calibration heuristics and parameter selection to improve performance.

Conclusion: The analysis provides theoretical justification for OPTQ design choices, guidance for parameterization, and new guarantees explaining Qronos's empirical performance advantages.

Abstract: Post-training quantization (PTQ) has become a crucial tool for reducing the
memory and compute costs of modern deep neural networks, including large
language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as
GPTQ-has emerged as a leading method due to its computational efficiency and
strong empirical performance. Despite its widespread adoption, however, OPTQ
lacks rigorous quantitative theoretical guarantees. This paper presents the
first quantitative error bounds for both deterministic and stochastic variants
of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ
algorithm. We analyze how OPTQ's iterative procedure induces quantization error
and derive non-asymptotic 2-norm error bounds that depend explicitly on the
calibration data and a regularization parameter that OPTQ uses. Our analysis
provides theoretical justification for several practical design choices,
including the widely used heuristic of ordering features by decreasing norm, as
well as guidance for selecting the regularization parameter. For the stochastic
variant, we establish stronger infinity-norm error bounds, which enable control
over the required quantization alphabet and are particularly useful for
downstream layers and nonlinearities. Finally, we extend our analysis to
Qronos, providing new theoretical bounds, for both its deterministic and
stochastic variants, that help explain its empirical advantages.

</details>


### [91] [Hilbert Neural Operator: Operator Learning in the Analytic Signal Domain](https://arxiv.org/abs/2508.04882)
*Saman Pordanesh,Pejman Shahsavari,Hossein Ghadjari*

Main category: cs.LG

TL;DR: The paper introduces the Hilbert Neural Operator (HNO), a neural operator architecture based on the Hilbert transform, designed to overcome limitations of Fourier-based operators such as periodicity assumptions.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing neural operators, particularly the Fourier Neural Operator (FNO), like its reliance on periodicity and lack of other useful signal representations.

Method: The architecture employs a Hilbert transform to create an analytic signal representation, explicitly capturing amplitude and phase as features. Spectral convolution is applied to this transformed representation.

Result: HNO is hypothesized to perform better in handling causal, phase-sensitive, and non-stationary systems due to its design rooted in analytic signal theory.

Conclusion: HNO presents an innovative use of signal processing principles in operator learning, offering a promising alternative to Fourier-based methods for certain types of systems.

Abstract: Neural operators have emerged as a powerful, data-driven paradigm for
learning solution operators of partial differential equations (PDEs).
State-of-the-art architectures, such as the Fourier Neural Operator (FNO), have
achieved remarkable success by performing convolutions in the frequency domain,
making them highly effective for a wide range of problems. However, this method
has some limitations, including the periodicity assumption of the Fourier
transform. In addition, there are other methods of analysing a signal, beyond
phase and amplitude perspective, and provide us with other useful information
to learn an effective network. We introduce the \textbf{Hilbert Neural Operator
(HNO)}, a new neural operator architecture to address some advantages by
incorporating a strong inductive bias from signal processing. HNO operates by
first mapping the input signal to its analytic representation via the Hilbert
transform, thereby making instantaneous amplitude and phase information
explicit features for the learning process. The core learnable operation -- a
spectral convolution -- is then applied to this Hilbert-transformed
representation. We hypothesize that this architecture enables HNO to model
operators more effectively for causal, phase-sensitive, and non-stationary
systems. We formalize the HNO architecture and provide the theoretical
motivation for its design, rooted in analytic signal theory.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [92] [Optimality Principles and Neural Ordinary Differential Equations-based Process Modeling for Distributed Control](https://arxiv.org/abs/2508.04799)
*Michael R. Wartmann,B. Erik Ydstie*

Main category: cs.NE

TL;DR: The paper introduces a framework that blends classical process modeling with data-driven methods, focusing on conservation properties and dynamic relations learned via sparse neural networks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in integrating recent machine learning advances into classical process models and control strategies.

Method: A framework is developed utilizing topology and conservation properties with neural ordinary differential equations for modeling process dynamics, demonstrated with inventory control systems.

Result: The framework successfully integrates sparse neural networks with process topology to create a state-space model suitable for model predictive control.

Conclusion: The proposed approach enables a consistent and scalable integration of data-driven algorithms with classical process models while maintaining fundamental conservation laws.

Abstract: Most recent advances in machine learning and analytics for process control
pose the question of how to naturally integrate new data-driven methods with
classical process models and control. We propose a process modeling framework
enabling integration of data-driven algorithms through consistent topological
properties and conservation of extensive quantities. Interconnections among
process network units are represented through connectivity matrices and network
graphs. We derive the system's natural objective function equivalent to the
non-equilibrium entropy production in a steady state system as a driving force
for the process dynamics. We illustrate how distributed control and
optimization can be implemented into process network structures and how control
laws and algorithms alter the system's natural equilibrium towards engineered
objectives. The basic requirement is that the flow conditions can be expressed
in terms of conic sector (passivity) conditions. Our formalism allows
integration of fundamental conservation properties from topology with learned
dynamic relations from data through sparse deep neural networks.
  We demonstrate in a practical example of a simple inventory control system
how to integrate the basic topology of a process with a neural network ordinary
differential equation model. The system specific constitutive equations are
left undescribed and learned by the neural ordinary differential equation
algorithm using the adjoint method in combination with an adaptive ODE solver
from synthetic time-series data. The resulting neural network forms a state
space model for use in e.g. a model predictive control algorithm.

</details>


### [93] [Modelling the emergence of open-ended technological evolution](https://arxiv.org/abs/2508.04828)
*James Winters,Mathieu Charbonneau*

Main category: cs.NE

TL;DR: The paper explores how open-ended technological evolution is influenced by the interaction between technological systems and societal needs/problems, highlighting the rarity and dependence on co-evolutionary dynamics.


<details>
  <summary>Details</summary>
Motivation: Open-ended technological advancement enables societies to continually expand resources and collective information-processing capabilities, defining human progress.

Method: The authors develop a macro-level model of cultural evolutionary dynamics involving co-evolving technological systems and societal needs, manipulated by stochastic and selection-like processes.

Result: Open-ended growth in technological evolution is rare and contingent upon strong stochastic perturbations and selection-like processes to maintain effective dynamics and resource production.

Conclusion: Co-evolutionary dynamics between technological systems and search spaces are crucial for sustained open-ended technological evolution and resource expansion.

Abstract: Humans stand alone in terms of their potential to collectively and
cumulatively improve technologies in an open-ended manner. This open-endedness
provides societies with the ability to continually expand their resources and
to increase their capacity to store, transmit and process information at a
collective-level. Here, we propose that the production of resources arises from
the interaction between technological systems (a society's repertoire of
interdependent skills, techniques and artifacts) and search spaces (the
aggregate collection of needs, problems and goals within a society). Starting
from this premise we develop a macro-level model wherein both technological
systems and search spaces are subject to cultural evolutionary dynamics. By
manipulating the extent to which these dynamics are characterised by stochastic
or selection-like processes, we demonstrate that open-ended growth is extremely
rare, historically contingent and only possible when technological systems and
search spaces co-evolve. Here, stochastic factors must be strong enough to
continually perturb the dynamics into a far-from-equilibrium state, whereas
selection-like factors help maintain effectiveness and ensure the sustained
production of resources. Only when this co-evolutionary dynamic maintains
effective technological systems, supports the ongoing expansion of the search
space and leads to an increased provision of resources do we observe open-ended
technological evolution.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [94] [Mapping Sparse Triangular Solves to GPUs via Fine-grained Domain Decomposition](https://arxiv.org/abs/2508.04917)
*Atharva Gondhalekar,Kjetil Haugen,Thomas Gibson,Wu-chun Feng*

Main category: cs.PF

TL;DR: This paper focuses on optimizing sparse linear systems on GPUs by proposing a fine-grained domain decomposition method which enhances parallelism and memory efficiency, yielding significant speedups.


<details>
  <summary>Details</summary>
Motivation: Sparse linear systems often encounter bottlenecks when using preconditioners due to irregular memory access and data dependency during triangular solves. The goal is to address these limitations in GPU architectures.

Method: A fine-grained domain decomposition strategy assigns non-overlapping subdomains to thread blocks, with subdomains designed to fit in GPU shared memory, minimizing synchronization issues and irregular global memory accesses.

Result: The proposed method shows a 10.7× speedup for triangular solves and a 3.2× speedup for BiCGSTAB solver with ILU0-preconditioners on AMD Instinct MI210 GPU compared to ROCm-state techniques.

Conclusion: The approach successfully adapts triangular solves for GPUs, boosting solver performance while tolerating a modest increase in convergence iteration count.

Abstract: Sparse linear systems are typically solved using preconditioned iterative
methods, but applying preconditioners via sparse triangular solves introduces
bottlenecks due to irregular memory accesses and data dependencies. This work
leverages fine-grained domain decomposition to adapt triangular solves to the
GPU architecture. We develop a fine-grained domain decomposition strategy that
generates non-overlapping subdomains, increasing parallelism in the application
of preconditioner at the expense of a modest increase in the iteration count
for convergence. Each subdomain is assigned to a thread block and is sized such
that the subdomain vector fits in the GPU shared memory, eliminating the need
for inter-block synchronization and reducing irregular global memory accesses.
Compared to other state-of-the-art implementations using the ROCm$^{\text{TM}}$
software stack, we achieve a 10.7$\times$ speedup for triangular solves and a
3.2$\times$ speedup for the ILU0-preconditioned biconjugate gradient stabilized
(BiCGSTAB) solver on the AMD Instinct$^{\text{TM}}$ MI210 GPU.

</details>


### [95] [Back to Bits: Extending Shannon's communication performance framework to computing](https://arxiv.org/abs/2508.05621)
*Max Hawkins,Richard Vuduc*

Main category: cs.PF

TL;DR: The paper introduces a new performance metric based on information theory for diverse modern computing systems, replacing outdated measures like flops.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics for computing systems, such as flops, fail to capture the complexity of emerging computing paradigms like analog, quantum, and reversible logic.

Method: Performance is redefined as mutual information between inputs and outputs, viewing computing as a channel for transforming information.

Result: The authors provide a framework that evaluates computing systems by measuring meaningful information encoded and manipulated during computation.

Conclusion: This novel, implementation-agnostic framework serves as a more accurate and principled method to evaluate computing performance in increasingly diverse systems.

Abstract: This work proposes a novel computing performance unit grounded in information
theory. Modern computing systems are increasingly diverse, supporting
low-precision formats, hardware specialization, and emerging paradigms such as
analog, quantum, and reversible logic. Traditional metrics like floating-point
operations (flops) no longer accurately capture this complexity. We frame
computing as the transformation of information through a channel and define
performance in terms of the mutual information between a system's inputs and
outputs. This approach measures not just the quantity of data processed, but
the amount of meaningful information encoded, manipulated, and retained through
computation. Our framework provides a principled, implementation-agnostic
foundation for evaluating performance.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [96] [Consistent Updates for Scalable Microservices](https://arxiv.org/abs/2508.04829)
*Devora Chait-Roth,Kedar S. Namjoshi,Thomas Wies*

Main category: cs.PL

TL;DR: The paper addresses the challenge of maintaining service consistency during mixed-mode updates in scalable microservice architectures by resolving inconsistencies through novel semantics-based algorithms.


<details>
  <summary>Details</summary>
Motivation: Service updates in microservice architectures must occur dynamically without disrupting existing client requests. However, mixed-mode operations—where different service versions interact—can cause inconsistencies or inefficiencies if not handled properly.

Method: The proposed approach introduces algorithms that leverage semantic properties of service actions, such as commutativity. The authors prove that semantic-awareness is essential for achieving consistency and develop a formal framework to analyze these updates theoretically, ensuring their correctness.

Result: The paper establishes foundational theory for consistent mixed-mode updates and derives algorithms guaranteed to handle updates without inconsistencies.

Conclusion: Semantic-aware update methods are necessary and sufficient to avoid inconsistencies during mixed-mode updates in microservice architectures, offering a more reliable and efficient alternative to existing approaches.

Abstract: Online services are commonly implemented with a scalable microservice
architecture, where isomorphic worker processes service client requests,
recording persistent state in a backend data store. To maintain service, any
modifications to the service functionality must be made on the fly -- i.e., as
the service continues to process client requests -- but doing so is
challenging. The central difficulty is that of avoiding potential
inconsistencies caused by ''mixed mode'' operation, where workers of current
and new versions are concurrently active and interact via the data store. Some
update methods avoid mixed mode altogether, but only at the cost of substantial
inefficiency -- by doubling resources (memory and compute), or by halving
throughput. The alternative is a so-called ''rolling'' update, which is
uncontrolled and runs the risk of serious service failures arising from
inconsistent mixed-mode behavior.
  In this paper, we present the first algorithms that guarantee consistency for
mixed mode updates. The algorithms rely on semantic properties of service
actions, such as commutativity. We show that semantic awareness is required, by
proving that any semantically oblivious, mixed-mode update method cannot avoid
inconsistencies. Ideally, it should appear to every client that a service
update takes effect atomically; this ensures that a client is not exposed to
inconsistent mixed-mode behavior. We introduce a framework that formalizes this
intuition and develop foundational theory for reasoning about the consistency
of mixed-mode updates, applying that theory to derive the new algorithms and
establish their correctness.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [97] [On the causality between affective impact and coordinated human-robot reactions](https://arxiv.org/abs/2508.04834)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: This paper explores how a robot's shared reactions to events and human-like reaction timing affect human perception of the robot's emotional impact.


<details>
  <summary>Details</summary>
Motivation: To enhance robot functionalities in social contexts by investigating the perceived affective impact of shared reactions and precise timing during human-robot interactions.

Method: Two experiments: (1) testing shared reaction impact (n=84) and (2) evaluating reaction timing effects with progressive delays (n=110).

Result: Robots that react to events shared with humans rather than randomly create significant affective impact; human-like reaction times (~200ms) are most engaging.

Conclusion: Around 200ms delays enhance perceived robot affective impact, while 100ms delays maximize perceived human influence on robots in small non-humanoid models.

Abstract: In an effort to improve how robots function in social contexts, this paper
investigates if a robot that actively shares a reaction to an event with a
human alters how the human perceives the robot's affective impact. To verify
this, we created two different test setups. One to highlight and isolate the
reaction element of affective robot expressions, and one to investigate the
effects of applying specific timing delays to a robot reacting to a physical
encounter with a human. The first test was conducted with two different groups
(n=84) of human observers, a test group and a control group both interacting
with the robot. The second test was performed with 110 participants using
increasingly longer reaction delays for the robot with every ten participants.
The results show a statistically significant change (p$<$.05) in perceived
affective impact for the robots when they react to an event shared with a human
observer rather than reacting at random. The result also shows for shared
physical interaction, the near-human reaction times from the robot are most
appropriate for the scenario. The paper concludes that a delay time around
200ms may render the biggest impact on human observers for small-sized
non-humanoid robots. It further concludes that a slightly shorter reaction time
around 100ms is most effective when the goal is to make the human observers
feel they made the biggest impact on the robot.

</details>


### [98] [INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM](https://arxiv.org/abs/2508.04931)
*Jin Wang,Weijie Wang,Boyuan Deng,Heng Zhang,Rui Dai,Nikos Tsagarakis*

Main category: cs.RO

TL;DR: The paper introduces INTENTION, a robotics framework combining Vision-Language Models and interaction-driven memory to enable autonomous and adaptable manipulation across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of robotics relying on precise physical models which fail in real-world scenarios and lack adaptability compared to humans' intuitive interaction abilities.

Method: The framework integrates Vision-Language Models for scene reasoning and a Memory Graph for recalling previous task interactions. It also includes an Intuitive Perceptor for extracting physical relations and affordances.

Result: The proposed framework enables robots to infer appropriate behaviors in novel scenes without repetitive instructions.

Conclusion: INTENTION demonstrates human-like understanding and adaptability in robotic manipulation, paving the way for more versatile and autonomous robotics.

Abstract: Traditional control and planning for robotic manipulation heavily rely on
precise physical models and predefined action sequences. While effective in
structured environments, such approaches often fail in real-world scenarios due
to modeling inaccuracies and struggle to generalize to novel tasks. In
contrast, humans intuitively interact with their surroundings, demonstrating
remarkable adaptability, making efficient decisions through implicit physical
understanding. In this work, we propose INTENTION, a novel framework enabling
robots with learned interactive intuition and autonomous manipulation in
diverse scenarios, by integrating Vision-Language Models (VLMs) based scene
reasoning with interaction-driven memory. We introduce Memory Graph to record
scenes from previous task interactions which embodies human-like understanding
and decision-making about different tasks in real world. Meanwhile, we design
an Intuitive Perceptor that extracts physical relations and affordances from
visual scenes. Together, these components empower robots to infer appropriate
interaction behaviors in new scenes without relying on repetitive instructions.
Videos: https://robo-intention.github.io

</details>


### [99] [Optimal Planning for Multi-Robot Simultaneous Area and Line Coverage Using Hierarchical Cyclic Merging Regulation](https://arxiv.org/abs/2508.04981)
*Tianyuan Zheng,Jingang Yi,Kaiyan Yu*

Main category: cs.RO

TL;DR: The paper addresses the challenge of creating efficient, collision-free robot routes for covering both linear features and larger areas, proposing the HCMR algorithm.


<details>
  <summary>Details</summary>
Motivation: To develop an optimal planning solution for robots tasked with simultaneous linear feature coverage and area exploration in known environments.

Method: A hierarchical cyclic merging regulation (HCMR) algorithm is introduced, supported by a Morse theory-based analysis to regulate traversal behavior and generate optimal routes.

Result: Simulation results indicate at least a 10.0% improvement in path length, a 16.9% reduction in task time, and successful conflict-free operations compared to existing methods.

Conclusion: The HCMR algorithm effectively achieves optimal, cost-efficient, and collision-free robot operation, surpassing existing planning methods in performance.

Abstract: The double coverage problem focuses on determining efficient, collision-free
routes for multiple robots to simultaneously cover linear features (e.g.,
surface cracks or road routes) and survey areas (e.g., parking lots or local
regions) in known environments. In these problems, each robot carries two
functional roles: service (linear feature footprint coverage) and exploration
(complete area coverage). Service has a smaller operational footprint but
incurs higher costs (e.g., time) compared to exploration. We present optimal
planning algorithms for the double coverage problems using hierarchical cyclic
merging regulation (HCMR). To reduce the complexity for optimal planning
solutions, we analyze the manifold attachment process during graph traversal
from a Morse theory perspective. We show that solutions satisfying minimum path
length and collision-free constraints must belong to a Morse-bounded
collection. To identify this collection, we introduce the HCMR algorithm. In
HCMR, cyclic merging search regulates traversal behavior, while edge sequence
back propagation converts these regulations into graph edge traversal
sequences. Incorporating balanced partitioning, the optimal sequence is
selected to generate routes for each robot. We prove the optimality of the HCMR
algorithm under a fixed sweep direction. The multi-robot simulation results
demonstrate that the HCMR algorithm significantly improves planned path length
by at least 10.0%, reduces task time by at least 16.9% in average, and ensures
conflict-free operation compared to other state-of-the-art planning methods.

</details>


### [100] [Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots](https://arxiv.org/abs/2508.04994)
*Wenjie Hu,Ye Zhou,Hann Woei Ho*

Main category: cs.RO

TL;DR: The paper introduces a Hierarchical DDPG (HDDPG) algorithm to address challenges in maze navigation, notably sparse rewards and inefficient exploration.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of the DDPG algorithm for autonomous maze navigation, addressing issues like sparse rewards and long-horizon planning difficulties.

Method: The HDDPG algorithm employs high-level subgoal generation and low-level primitive action planning, using refined experience relabeling, adaptive parameter space noise, and a reshaped reward function for enhanced stability and exploration.

Result: HDDPG improved maze navigation success rates by at least 56.59% and average rewards by a minimum of 519.03 compared to baseline approaches.

Conclusion: The HDDPG algorithm significantly enhances the capabilities of DDPG for complex maze navigation tasks, presenting a robust solution to its limitations.

Abstract: Maze navigation is a fundamental challenge in robotics, requiring agents to
traverse complex environments efficiently. While the Deep Deterministic Policy
Gradient (DDPG) algorithm excels in control tasks, its performance in maze
navigation suffers from sparse rewards, inefficient exploration, and
long-horizon planning difficulties, often leading to low success rates and
average rewards, sometimes even failing to achieve effective navigation. To
address these limitations, this paper proposes an efficient Hierarchical DDPG
(HDDPG) algorithm, which includes high-level and low-level policies. The
high-level policy employs an advanced DDPG framework to generate intermediate
subgoals from a long-term perspective and on a higher temporal scale. The
low-level policy, also powered by the improved DDPG algorithm, generates
primitive actions by observing current states and following the subgoal
assigned by the high-level policy. The proposed method enhances stability with
off-policy correction, refining subgoal assignments by relabeling historical
experiences. Additionally, adaptive parameter space noise is utilized to
improve exploration, and a reshaped intrinsic-extrinsic reward function is
employed to boost learning efficiency. Further optimizations, including
gradient clipping and Xavier initialization, are employed to improve
robustness. The proposed algorithm is rigorously evaluated through numerical
simulation experiments executed using the Robot Operating System (ROS) and
Gazebo. Regarding the three distinct final targets in autonomous maze
navigation tasks, HDDPG significantly overcomes the limitations of standard
DDPG and its variants, improving the success rate by at least 56.59% and
boosting the average reward by a minimum of 519.03 compared to baseline
algorithms.

</details>


### [101] [MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding](https://arxiv.org/abs/2508.05021)
*Weifan Zhang,Tingguang Li,Yuzhen Liu*

Main category: cs.RO

TL;DR: The paper introduces a zero-shot navigation framework using off-the-shelf Visual Language Models enhanced by active grounding and historical memory for robots in complex environments.


<details>
  <summary>Details</summary>
Motivation: Enable robots to navigate complex unknown environments using only natural language descriptions.

Method: Uses Visual Language Models with active grounding for viewpoint adjustment and historical memory for re-evaluating uncertain observations.

Result: Achieves state-of-the-art performance in language-driven navigation in both simulated (HM3D) and real-world settings.

Conclusion: Demonstrates effective zero-shot generalization and robust robot navigation, not requiring labeled data or fine-tuning.

Abstract: Visual navigation in unknown environments based solely on natural language
descriptions is a key capability for intelligent robots. In this work, we
propose a navigation framework built upon off-the-shelf Visual Language Models
(VLMs), enhanced with two human-inspired mechanisms: perspective-based active
grounding, which dynamically adjusts the robot's viewpoint for improved visual
inspection, and historical memory backtracking, which enables the system to
retain and re-evaluate uncertain observations over time. Unlike existing
approaches that passively rely on incidental visual inputs, our method actively
optimizes perception and leverages memory to resolve ambiguity, significantly
improving vision-language grounding in complex, unseen environments. Our
framework operates in a zero-shot manner, achieving strong generalization to
diverse and open-ended language descriptions without requiring labeled data or
model fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show
that our method outperforms state-of-the-art approaches in language-driven
object navigation. We further demonstrate its practicality through real-world
deployment on a quadruped robot, achieving robust and effective navigation
performance.

</details>


### [102] [Benchmarking Shortcutting Techniques for Multi-Robot-Arm Motion Planning](https://arxiv.org/abs/2508.05027)
*Philip Huang,Yorai Shaoul,Jiaoyang Li*

Main category: cs.RO

TL;DR: This paper studies and compares shortcutting techniques for motion planning in multi-robot arm systems, proposing strategies for better performance-runtime tradeoffs.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty in generating high-quality, collision-free, and smooth motion plans for multi-arm robotic systems, and to improve upon traditional methods using shortcutting techniques.

Method: The study performs a quantitative analysis of existing shortcutting methods across diverse simulated scenarios, examining their pros and cons. It also proposes two strategies for combining these methods to optimize performance and efficiency.

Result: The study identifies the strengths and weaknesses of existing shortcutting techniques and demonstrates how the proposed combination strategies improve performance and runtime in multi-arm motion planning.

Conclusion: Effective shortcutting is critical for high-quality multi-arm trajectory planning, and the proposed combination strategies enhance execution quality and efficiency.

Abstract: Generating high-quality motion plans for multiple robot arms is challenging
due to the high dimensionality of the system and the potential for inter-arm
collisions. Traditional motion planning methods often produce motions that are
suboptimal in terms of smoothness and execution time for multi-arm systems.
Post-processing via shortcutting is a common approach to improve motion quality
for efficient and smooth execution. However, in multi-arm scenarios, optimizing
one arm's motion must not introduce collisions with other arms. Although
existing multi-arm planning works often use some form of shortcutting
techniques, their exact methodology and impact on performance are often vaguely
described. In this work, we present a comprehensive study quantitatively
comparing existing shortcutting methods for multi-arm trajectories across
diverse simulated scenarios. We carefully analyze the pros and cons of each
shortcutting method and propose two simple strategies for combining these
methods to achieve the best performance-runtime tradeoff. Video, code, and
dataset are available at https://philip-huang.github.io/mr-shortcut/.

</details>


### [103] [A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System](https://arxiv.org/abs/2508.05040)
*Boyang Zhang,Jiahui Zuo,Zeyu Duan,Fumin Zhang*

Main category: cs.RO

TL;DR: The study develops a vision-based sensing system for soft grippers to detect and respond to external collisions, enhancing grasp stability.


<details>
  <summary>Details</summary>
Motivation: Ensure stable grasping of circular objects in the presence of external collisions, which pose risks to robotic systems.

Method: The system integrates an eye-in-palm camera with a wide field of view and introduces a collision-rich grasping strategy. A physical soft gripper was created and used on a robotic arm for testing.

Result: Experiments validated that the system reacts to collisions instantaneously and can accurately determine the direction and scale of collisions.

Conclusion: The presented vision-based sensing approach ensures dynamic stability and security in robotic grasping amidst external disturbances.

Abstract: External collisions to robot actuators typically pose risks to grasping
circular objects. This work presents a vision-based sensing module capable of
detecting collisions to maintain stable grasping with a soft gripper system.
The system employs an eye-in-palm camera with a broad field of view to
simultaneously monitor the motion of fingers and the grasped object.
Furthermore, we have developed a collision-rich grasping strategy to ensure the
stability and security of the entire dynamic grasping process. A physical soft
gripper was manufactured and affixed to a collaborative robotic arm to evaluate
the performance of the collision detection mechanism. An experiment regarding
testing the response time of the mechanism confirmed the system has the
capability to react to the collision instantaneously. A dodging test was
conducted to demonstrate the gripper can detect the direction and scale of
external collisions precisely.

</details>


### [104] [Examining the legibility of humanoid robot arm movements in a pointing task](https://arxiv.org/abs/2508.05104)
*Andrej Lúčny,Matilde Antonj,Carlo Mazzola,Hana Hornáčková,Ana Farić,Kristína Malinovská,Michal Vavrecka,Igor Farkaš*

Main category: cs.RO

TL;DR: This study examines how humans interpret humanoid robot arm movements during pointing tasks by varying cues like gaze and stopping arm motions early, finding support for multimodal superiority and ocular primacy hypotheses.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve human-robot interaction by exploring how humanoid robots can make their actions more legible and predictable to humans, ensuring safety and effective collaboration.

Method: Participants observed the NICO humanoid robot performing pointing tasks using different cues (gaze and arm movements). The robot's arm trajectories were truncated, and participants predicted target destinations.

Result: Results supported the multimodal superiority hypothesis (cues are better combined) and ocular primacy hypothesis (gaze plays a key role in predicting intentions).

Conclusion: Multimodal robot cues, especially gaze, enhance understanding and prediction of robot actions, facilitating safer and better human-robot interaction.

Abstract: Human--robot interaction requires robots whose actions are legible, allowing
humans to interpret, predict, and feel safe around them. This study
investigates the legibility of humanoid robot arm movements in a pointing task,
aiming to understand how humans predict robot intentions from truncated
movements and bodily cues. We designed an experiment using the NICO humanoid
robot, where participants observed its arm movements towards targets on a
touchscreen. Robot cues varied across conditions: gaze, pointing, and pointing
with congruent or incongruent gaze. Arm trajectories were stopped at 60\% or
80\% of their full length, and participants predicted the final target. We
tested the multimodal superiority and ocular primacy hypotheses, both of which
were supported by the experiment.

</details>


### [105] [From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation](https://arxiv.org/abs/2508.05143)
*Siméon Capy,Thomas M. Kwok,Kevin Joseph,Yuichiro Kawasumi,Koichi Nagashima,Tomoya Sasaki,Yue Hu,Eiichi Yoshida*

Main category: cs.RO

TL;DR: The study investigates user perception of long-distance versus local robot teleoperation, finding no major differences between the two.


<details>
  <summary>Details</summary>
Motivation: Explore the utility of robot teleoperation for tasks requiring human intervention, particularly in elder care.

Method: Develop a protocol with questionnaires and implement software using ROS and Unity to evaluate user perception.

Result: Statistical analysis showed no significant difference in perception between local and remote robot operation.

Conclusion: Robot teleoperation can be as effective as local control, potentially enabling broader applications in elder care.

Abstract: Robot teleoperation (RTo) has emerged as a viable alternative to local
control, particularly when human intervention is still necessary. This research
aims to study the distance effect on user perception in RTo, exploring the
potential of teleoperated robots for older adult care. We propose an evaluation
of non-expert users' perception of long-distance RTo, examining how their
perception changes before and after interaction, as well as comparing it to
that of locally operated robots. We have designed a specific protocol
consisting of multiple questionnaires, along with a dedicated software
architecture using the Robotics Operating System (ROS) and Unity. The results
revealed no statistically significant differences between the local and remote
robot conditions, suggesting that robots may be a viable alternative to
traditional local control.

</details>


### [106] [Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories](https://arxiv.org/abs/2508.05148)
*Francisco Munguia-Galeano,Zhengxue Zhou,Satheeshkumar Veeramani,Hatem Fakhruldeen,Louis Longley,Rob Clowes,Andrew I. Cooper*

Main category: cs.RO

TL;DR: The paper introduces Chemist Eye, an AI-driven safety monitoring system for self-driving laboratories that achieves high accuracy in hazard detection and decision-making.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the additional safety risks posed by integrating robotics, such as fire hazards from lithium batteries and the need for PPE compliance, into self-driving laboratories.

Method: The system uses RGB, depth, and infrared cameras along with a vision-language model (VLM) to detect fire hazards, PPE compliance, and worker emergencies in real time.

Result: Chemist Eye was tested in a self-driving lab with three mobile robots, achieving a 97% accuracy in hazard detection and 95% in decision-making.

Conclusion: Chemist Eye effectively enhances safety in self-driving labs by integrating real-time monitoring, decision-making, and notification mechanisms.

Abstract: The integration of robotics and automation into self-driving laboratories
(SDLs) can introduce additional safety complexities, in addition to those that
already apply to conventional research laboratories. Personal protective
equipment (PPE) is an essential requirement for ensuring the safety and
well-being of workers in laboratories, self-driving or otherwise. Fires are
another important risk factor in chemical laboratories. In SDLs, fires that
occur close to mobile robots, which use flammable lithium batteries, could have
increased severity. Here, we present Chemist Eye, a distributed safety
monitoring system designed to enhance situational awareness in SDLs. The system
integrates multiple stations equipped with RGB, depth, and infrared cameras,
designed to monitor incidents in SDLs. Chemist Eye is also designed to spot
workers who have suffered a potential accident or medical emergency, PPE
compliance and fire hazards. To do this, Chemist Eye uses decision-making
driven by a vision-language model (VLM). Chemist Eye is designed for seamless
integration, enabling real-time communication with robots. Based on the VLM
recommendations, the system attempts to drive mobile robots away from potential
fire locations, exits, or individuals not wearing PPE, and issues audible
warnings where necessary. It also integrates with third-party messaging
platforms to provide instant notifications to lab personnel. We tested Chemist
Eye with real-world data from an SDL equipped with three mobile robots and
found that the spotting of possible safety hazards and decision-making
performances reached 97 % and 95 %, respectively.

</details>


### [107] [FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction](https://arxiv.org/abs/2508.05153)
*Mohammed Daba,Jing Qiu*

Main category: cs.RO

TL;DR: The paper introduces FCBV-Net for robotic garment manipulation, enhancing category-level generalization using pre-trained 3D geometric features.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in robotic garment manipulation due to high dimensionality, complex dynamics, and intra-category variations.

Method: FCBV-Net employs pre-trained frozen 3D geometric features for bimanual value prediction and trains downstream components for specific garment smoothing tasks.

Result: In simulated GarmentLab experiments, FCBV-Net showed superior performance over benchmarks, with minimal efficiency drop and improved coverage.

Conclusion: FCBV-Net's approach of separating geometric understanding from bimanual action learning improves generalization in garment manipulation.

Abstract: Category-level generalization for robotic garment manipulation, such as
bimanual smoothing, remains a significant hurdle due to high dimensionality,
complex dynamics, and intra-category variations. Current approaches often
struggle, either overfitting with concurrently learned visual features for a
specific instance or, despite category-level perceptual generalization, failing
to predict the value of synergistic bimanual actions. We propose the
Feature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point
clouds to specifically enhance category-level policy generalization for garment
smoothing. FCBV-Net conditions bimanual action value prediction on pre-trained,
frozen dense geometric features, ensuring robustness to intra-category garment
variations. Trainable downstream components then learn a task-specific policy
using these static features. In simulated GarmentLab experiments with the
CLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization.
It exhibited only an 11.5% efficiency drop (Steps80) on unseen garments
compared to 96.2% for a 2D image-based baseline, and achieved 89% final
coverage, outperforming an 83% coverage from a 3D correspondence-based baseline
that uses identical per-point geometric features but a fixed primitive. These
results highlight that the decoupling of geometric understanding from bimanual
action value learning enables better category-level generalization.

</details>


### [108] [Learning to See and Act: Task-Aware View Planning for Robotic Manipulation](https://arxiv.org/abs/2508.05186)
*Yongjie Bai,Zhouxia Wang,Yang Liu,Weixing Chen,Ziliang Chen,Mingtong Dai,Yongsen Zheng,Lingbo Liu,Guanbin Li,Liang Lin*

Main category: cs.RO

TL;DR: The paper introduces TAVP, a framework addressing limitations in multi-task robotic manipulation using active view planning and task-specific representation learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome issues like static viewpoints and shared visual encoders in VLA models, which limit 3D perception and cause task interference.

Method: TAVP integrates an exploration policy with a pseudo-environment for active view acquisition and employs a Mixture-of-Experts visual encoder for task-specific feature representation.

Result: Experiments show that TAVP significantly improves action prediction and surpasses fixed-view methods in multi-task robotic manipulation.

Conclusion: Task-Aware View Planning provides more comprehensive and discriminative visual representations, which enhance robustness and generalization in robotic manipulation tasks.

Abstract: Recent vision-language-action (VLA) models for multi-task robotic
manipulation commonly rely on static viewpoints and shared visual encoders,
which limit 3D perception and cause task interference, hindering robustness and
generalization. In this work, we propose Task-Aware View Planning (TAVP), a
framework designed to overcome these challenges by integrating active view
planning with task-specific representation learning. TAVP employs an efficient
exploration policy, accelerated by a novel pseudo-environment, to actively
acquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)
visual encoder to disentangle features across different tasks, boosting both
representation fidelity and task generalization. By learning to see the world
in a task-aware way, TAVP generates more complete and discriminative visual
representations, demonstrating significantly enhanced action prediction across
a wide array of manipulation challenges. Extensive experiments on RLBench tasks
show that our proposed TAVP model achieves superior performance over
state-of-the-art fixed-view approaches. Visual results and code are provided
at: https://hcplab-sysu.github.io/TAVP.

</details>


### [109] [Dancing with a Robot: An Experimental Study of Child-Robot Interaction in a Performative Art Setting](https://arxiv.org/abs/2508.05208)
*Victor Ngo,Rachel,Ramchurn,Roma Patel,Alan Chamberlain,Ayse Kucukyilmaz*

Main category: cs.RO

TL;DR: The paper examines children's interactions with an autonomous robot arm, NED, in an art installation.


<details>
  <summary>Details</summary>
Motivation: Understand the complexities of children's engagement with robotic performers in interactive arts.

Method: Analysis of 18 children's interactions with NED in a real-world setting across the UK.

Result: Identified challenges in engagement, expressivity, reciprocity, and expectation management.

Conclusion: By optimizing HRI systems to match audience capabilities and perceptions, meaningful experiences for children can be fostered in performative art contexts.

Abstract: This paper presents an evaluation of 18 children's in-the-wild experiences
with the autonomous robot arm performer NED (Never-Ending Dancer) within the
Thingamabobas installation, showcased across the UK. We detail NED's design,
including costume, behaviour, and human interactions, all integral to the
installation. Our observational analysis revealed three key challenges in
child-robot interactions: 1) Initiating and maintaining engagement, 2) Lack of
robot expressivity and reciprocity, and 3) Unmet expectations. Our findings
show that children are naturally curious, and adept at interacting with a
robotic art performer. However, our observations emphasise the critical need to
optimise human-robot interaction (HRI) systems through careful consideration of
audience's capabilities, perceptions, and expectations, within the performative
arts context, to enable engaging and meaningful experiences, especially for
young audiences.

</details>


### [110] [Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction](https://arxiv.org/abs/2508.05294)
*Sahar Salimpour,Lei Fu,Farhad Keramat,Leonardo Militano,Giovanni Toffetti,Harry Edelman,Jorge Peña Queralta*

Main category: cs.RO

TL;DR: The paper surveys foundation models enabling advanced robotics, focusing on agentic applications of language and behavior models, taxonomy, and trends.


<details>
  <summary>Details</summary>
Motivation: Foundation models like LLMs and VLMs are transforming robot autonomy and human-robot interfaces. This study examines emerging agentic architectures for robotics.

Method: The paper proposes a taxonomy, reviews both peer-reviewed and community-driven projects, and provides a comparative analysis of agents' roles.

Result: Emerging trends in GPT-style interfaces, robot reasoning, API invocation, and task assistance are highlighted, along with a taxonomy categorizing model integrations.

Conclusion: Agentic architectures in robotics offer revolutionary ways for task handling, diagnostics, and interaction, and their development is rapidly evolving.

Abstract: Foundation models, including large language models (LLMs) and vision-language
models (VLMs), have recently enabled novel approaches to robot autonomy and
human-robot interfaces. In parallel, vision-language-action models (VLAs) or
large behavior models (BLMs) are increasing the dexterity and capabilities of
robotic systems. This survey paper focuses on those words advancing towards
agentic applications and architectures. This includes initial efforts exploring
GPT-style interfaces to tooling, as well as more complex system where AI agents
are coordinators, planners, perception actors, or generalist interfaces. Such
agentic architectures allow robots to reason over natural language
instructions, invoke APIs, plan task sequences, or assist in operations and
diagnostics. In addition to peer-reviewed research, due to the fast-evolving
nature of the field, we highlight and include community-driven projects, ROS
packages, and industrial frameworks that show emerging trends. We propose a
taxonomy for classifying model integration approaches and present a comparative
analysis of the role that agents play in different solutions in today's
literature.

</details>


### [111] [GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming](https://arxiv.org/abs/2508.05298)
*Jian Gong,Youwei Huang,Bo Yuan,Ming Zhu,Juncheng Zhan,Jinke Wang,Hang Shu,Mingyue Xiong,Yanjun Ye,Yufan Zu,Yang Zhou,Yihan Ding,Xuannian Chen,Xingyu Lu,Runjie Ban,Bingchao Huang,Fusen Liu*

Main category: cs.RO

TL;DR: GhostShell is a novel LLM-enabled framework for real-time behavioral programming in robots, outperforming traditional methods in speed, accuracy, and adaptability during various tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of conventional robotic behavioral programming methods, which depend on pre-scripted action sequences or behavior trees, and explores real-time decision-making through LLMs.

Method: The researchers developed GhostShell, featuring a streaming XML function parser, dynamic function interface mapper, and a multi-channel scheduler to enable on-the-fly robotic decision-making based on streamed LLM outputs.

Result: Experiments on the COCO robot prototype demonstrated a Behavioral Correctness Metric of 0.85 with Claude-4 Sonnet, significant speed-ups (up to 66X), and strong performance on long-horizon multimodal tasks.

Conclusion: GhostShell achieves state-of-the-art results in both speed and behavioral accuracy and is robust enough to support complex interaction tasks, representing a groundbreaking approach for embodied systems.

Abstract: We present GhostShell, a novel approach that leverages Large Language Models
(LLMs) to enable streaming and concurrent behavioral programming for embodied
systems. In contrast to conventional methods that rely on pre-scheduled action
sequences or behavior trees, GhostShell drives embodied systems to act
on-the-fly by issuing function calls incrementally as tokens are streamed from
the LLM. GhostShell features a streaming XML function token parser, a dynamic
function interface mapper, and a multi-channel scheduler that orchestrates
intra-channel synchronous and inter-channel asynchronous function calls,
thereby coordinating serial-parallel embodied actions across multiple robotic
components as directed by the LLM. We evaluate GhostShell on our robot
prototype COCO through comprehensive grounded experiments across 34 real-world
interaction tasks and multiple LLMs. The results demonstrate that our approach
achieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4
Sonnet and up to 66X faster response times compared to LLM native function
calling APIs. GhostShell also proves effective in long-horizon multimodal
tasks, demonstrating strong robustness and generalization.

</details>


### [112] [Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control](https://arxiv.org/abs/2508.05342)
*Shunlei Li,Longsen Gao,Jin Wang,Chang Che,Xi Xiao,Jiuwen Cao,Yingbai Hu,Hamid Reza Karimi*

Main category: cs.RO

TL;DR: The paper introduces GF-VLA, a framework for dual-arm robots to adeptly interpret and replicate human tasks using RGB and Depth data, with notable accuracy in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Developing robotic systems capable of task-level generalization by understanding human demonstrations without the limiting factors of simple trajectory imitation.

Method: The method involves extracting key hand-object and object-object interactions to form scene graphs, integrating these with language-conditioned transformers to derive task policies, and employing a cross-hand selection policy for better dual-arm coordination.

Result: GF-VLA achieved over 95% scene graph accuracy, 93% task segmentation, and 90% overall task success in challenging dual-arm robotic tasks, proving reliable across spatial and semantic variations.

Conclusion: GF-VLA demonstrates the ability of robots to achieve high generalization and task success in structured manipulations, broadening the possibilities for practical robotic applications.

Abstract: Teaching robots dexterous skills from human videos remains challenging due to
the reliance on low-level trajectory imitation, which fails to generalize
across object types, spatial layouts, and manipulator configurations. We
propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables
dual-arm robotic systems to perform task-level reasoning and execution directly
from RGB and Depth human demonstrations. GF-VLA first extracts
Shannon-information-based cues to identify hands and objects with the highest
task relevance, then encodes these cues into temporally ordered scene graphs
that capture both hand-object and object-object interactions. These graphs are
fused with a language-conditioned transformer that generates hierarchical
behavior trees and interpretable Cartesian motion commands. To improve
execution efficiency in bimanual settings, we further introduce a cross-hand
selection policy that infers optimal gripper assignment without explicit
geometric reasoning. We evaluate GF-VLA on four structured dual-arm block
assembly tasks involving symbolic shape construction and spatial
generalization. Experimental results show that the information-theoretic scene
representation achieves over 95 percent graph accuracy and 93 percent subtask
segmentation, supporting the LLM planner in generating reliable and
human-readable task policies. When executed by the dual-arm robot, these
policies yield 94 percent grasp success, 89 percent placement accuracy, and 90
percent overall task success across stacking, letter-building, and geometric
reconfiguration scenarios, demonstrating strong generalization and robustness
across diverse spatial and semantic variations.

</details>


### [113] [Affecta-Context: The Context-Guided Behavior Adaptation Framework](https://arxiv.org/abs/2508.05359)
*Morten Roed Frederiksen,Kasper Støy*

Main category: cs.RO

TL;DR: The paper introduces Affecta-context, a framework enabling social robots to adapt their behaviors based on physical contexts and user preferences.


<details>
  <summary>Details</summary>
Motivation: To improve human-robot interaction by creating robots that can autonomously adapt to varying physical environments and user preferences.

Method: The framework learns and clusters physical contexts through their measured properties and prioritizes robot behaviors to match these contexts using data from interactions with humans.

Result: Through 72 interactions in two physical contexts involving 6 human participants, the framework demonstrated its ability to adapt behaviors and generalize to new physical contexts.

Conclusion: The Affecta-context framework shows promise in enabling social robots to adapt behaviors autonomously across varying physical contexts, enhancing human-robot interaction experiences.

Abstract: This paper presents Affecta-context, a general framework to facilitate
behavior adaptation for social robots. The framework uses information about the
physical context to guide its behaviors in human-robot interactions. It
consists of two parts: one that represents encountered contexts and one that
learns to prioritize between behaviors through human-robot interactions. As
physical contexts are encountered the framework clusters them by their measured
physical properties. In each context, the framework learns to prioritize
between behaviors to optimize the physical attributes of the robot's behavior
in line with its current environment and the preferences of the users it
interacts with. This paper illlustrates the abilities of the Affecta-context
framework by enabling a robot to autonomously learn the prioritization of
discrete behaviors. This was achieved by training across 72 interactions in two
different physical contexts with 6 different human test participants. The paper
demonstrates the trained Affecta-context framework by verifying the robot's
ability to generalize over the input and to match its behaviors to a previously
unvisited physical context.

</details>


### [114] [A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry](https://arxiv.org/abs/2508.05368)
*Tong Hua,Jiale Han,Wei Ouyang*

Main category: cs.RO

TL;DR: The paper introduces a pose-only estimation approach to optimize the efficiency of GNSS-Visual-Inertial Odometry (GVIO) systems, addressing the computational burden of IEKF.


<details>
  <summary>Details</summary>
Motivation: To address the high computational burden inherent in IEKF when jointly optimizing camera poses and landmarks, and enhance the applicability in multi-sensor fusion scenarios.

Method: Proposed a visual measurement model that directly links landmark representation to multiple camera poses and observations, allowing pose-only estimation. Applied the model within a filter-based GVIO framework, introducing a novel feature management strategy.

Result: Simulation tests and real-world experiments validated the improved efficiency and accuracy of the proposed method.

Conclusion: The approach is effective in reducing computational complexity while maintaining accuracy, offering improvements for GVIO systems in multi-sensor fusion applications.

Abstract: Invariant Extended Kalman Filter (IEKF) has been a significant technique in
vision-aided sensor fusion. However, it usually suffers from high computational
burden when jointly optimizing camera poses and the landmarks. To improve its
efficiency and applicability for multi-sensor fusion, we present a multi-view
pose-only estimation approach with its application to GNSS-Visual-Inertial
Odometry (GVIO) in this paper. Our main contribution is deriving a visual
measurement model which directly associates landmark representation with
multiple camera poses and observations. Such a pose-only measurement is proven
to be tightly-coupled between landmarks and poses, and maintain a perfect null
space that is independent of estimated poses. Finally, we apply the proposed
approach to a filter based GVIO with a novel feature management strategy. Both
simulation tests and real-world experiments are conducted to demonstrate the
superiority of the proposed method in terms of efficiency and accuracy.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [115] [Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini](https://arxiv.org/abs/2508.04820)
*Mayra Sofia Ruiz Rodriguez,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: The study evaluates GPT-4o mini's ability to generate file-level log statements in ML projects, finding high overlogging and challenges with alignment to human conventions.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the importance of logging in software development, particularly in ML applications, and the underexplored potential of LLMs for file-level log generation.

Method: The researchers analyzed 4,073 Python files from 171 ML repositories, removed original logs, prompted GPT-4o mini to generate new logs, and compared its outputs with human-written logs based on location, content, and quality. Additionally, a manual review identified patterns and challenges.

Result: The LLM introduced logs in the same locations as humans 63.91% of the time but exhibited a high overlogging rate of 82.66%. Challenges include excessive logging at function boundaries, difficulty with large code blocks, and poor adaptation to project-specific conventions.

Conclusion: GPT-4o mini shows potential in logging for complete files but faces significant challenges in practical applications due to issues like overlogging and misalignment with human practices.

Abstract: Logging is essential in software development, helping developers monitor
system behavior and aiding in debugging applications. Given the ability of
large language models (LLMs) to generate natural language and code, researchers
are exploring their potential to generate log statements. However, prior work
focuses on evaluating logs introduced in code functions, leaving file-level log
generation underexplored -- especially in machine learning (ML) applications,
where comprehensive logging can enhance reliability. In this study, we evaluate
the capacity of GPT-4o mini as a case study to generate log statements for ML
projects at file level. We gathered a set of 171 ML repositories containing
4,073 Python files with at least one log statement. We identified and removed
the original logs from the files, prompted the LLM to generate logs for them,
and evaluated both the position of the logs and log level, variables, and text
quality of the generated logs compared to human-written logs. In addition, we
manually analyzed a representative sample of generated logs to identify common
patterns and challenges. We find that the LLM introduces logs in the same place
as humans in 63.91% of cases, but at the cost of a high overlogging rate of
82.66%. Furthermore, our manual analysis reveals challenges for file-level
logging, which shows overlogging at the beginning or end of a function,
difficulty logging within large code blocks, and misalignment with
project-specific logging conventions. While the LLM shows promise for
generating logs for complete files, these limitations remain to be addressed
for practical implementation.

</details>


### [116] [Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models](https://arxiv.org/abs/2508.04895)
*Wentao Lu,Alexander Senchenko,Abram Hindle,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: The paper proposes an automated system to extract single frames from gameplay videos that match textual bug descriptions for faster bug triage in game development.


<details>
  <summary>Details</summary>
Motivation: The manual process of reviewing gameplay videos for bug verification is labor-intensive, slow, and hard to scale. This creates challenges in rapidly identifying issues reported in bug submissions.

Method: The pipeline uses FFmpeg for extracting keyframes and employs a vision-language model (GPT-4o) to rank frames based on textual bug descriptions, selecting the most representative frame for developers.

Result: The system captures bug moments in 98.79% of cases, achieves an F1 score of 0.79 and an accuracy of 0.89 for top-1 frame retrieval, and performs well in most bug categories, with lower performance for Animation & VFX bugs.

Conclusion: The approach significantly reduces manual effort by providing developers with a single, most relevant frame representative of the bug description, facilitating faster regression checks and triage for game development QA teams.

Abstract: Modern game studios deliver new builds and patches at a rapid pace,
generating thousands of bug reports, many of which embed gameplay videos. To
verify and triage these bug reports, developers must watch the submitted
videos. This manual review is labour-intensive, slow, and hard to scale. In
this paper, we introduce an automated pipeline that reduces each video to a
single frame that best matches the reported bug description, giving developers
instant visual evidence that pinpoints the bug.
  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video
to a median of just 1.90% of its original frames while still capturing bug
moments in 98.79 of cases. These keyframes are then evaluated by a
vision--language model (GPT-4o), which ranks them based on how well they match
the textual bug description and selects the most representative frame. We
evaluated this approach using real-world developer-submitted gameplay videos
and JIRA bug reports from a popular First-Person Shooter (FPS) game. The
pipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the
top-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =
0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and
lowest for Animation & VFX (0.51).
  By replacing video viewing with an immediately informative image, our
approach dramatically reduces manual effort and speeds up triage and regression
checks, offering practical benefits to quality assurance (QA) teams and
developers across the game industry.

</details>


### [117] [Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities](https://arxiv.org/abs/2508.04921)
*Zixuan Feng,Reed Milewicz,Emerson Murphy-Hill,Tyler Menezes,Alexander Serebrenik,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: The paper explores how Generative AI impacts open-source software (OSS) through risks and opportunities in areas like development, documentation, engagement, and governance.


<details>
  <summary>Details</summary>
Motivation: To address the uncertainty and potential risks that Generative AI introduces to Open Source Software (OSS) and provide a way for communities to adapt constructively.

Method: A conceptual exploration using McLuhan's Tetrad socio-technical framework, applied to scenario-driven analysis across four OSS domains: practices, documentation, governance, and community engagement.

Result: This approach identified risks and opportunities in OSS development and highlighted strategies for community resilience to GenAI-driven disruptions.

Conclusion: By applying the socio-technical framework proactively, OSS leaders can better manage and adapt their ecosystems amidst Generative AI-induced changes.

Abstract: Open Source Software communities face a wave of uncertainty as Generative AI
rapidly transforms how software is created, maintained, and governed. Without
clear frameworks, communities risk being overwhelmed by the complexity and
ambiguity introduced by GenAI, threatening the collaborative ethos that
underpins OSS. We conduct a scenario-driven, conceptual exploration using a
socio-technical framework inspired by McLuhan's Tetrad to surface both risks
and opportunities for community resilience amid GenAI-driven disruption of OSS
development across four domains: software practices, documentation, community
engagement, and governance. By adopting this lens, OSS leaders and researchers
can proactively shape the future of their ecosystems, rather than simply
reacting to technological upheaval.

</details>


### [118] [Taxonomy of Faults in Attention-Based Neural Networks](https://arxiv.org/abs/2508.04925)
*Sigma Jahan,Saurabh Singh Rajput,Tushar Sharma,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: This paper investigates faults in attention-based neural networks (ABNNs), introduces a novel taxonomy of seven fault categories, and provides diagnostic heuristics for practitioners.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding and diagnosing the unique failures arising in attention mechanisms, as existing fault taxonomies fall short.

Method: A systematic analysis of 555 real-world faults in ABNNs from 96 projects across ten frameworks, followed by the development of a fault taxonomy and evidence-based heuristics.

Result: The study identifies seven unique fault categories specific to attention mechanisms, links root causes to symptoms, and suggests four diagnostic heuristics explaining 33% of these faults.

Conclusion: This work fills a critical gap by providing actionable diagnostic guidance for attention-based models, enhancing reliability and understanding of attention mechanisms.

Abstract: Attention mechanisms are at the core of modern neural architectures, powering
systems ranging from ChatGPT to autonomous vehicles and driving a major
economic impact. However, high-profile failures, such as ChatGPT's nonsensical
outputs or Google's suspension of Gemini's image generation due to attention
weight errors, highlight a critical gap: existing deep learning fault
taxonomies might not adequately capture the unique failures introduced by
attention mechanisms. This gap leaves practitioners without actionable
diagnostic guidance. To address this gap, we present the first comprehensive
empirical study of faults in attention-based neural networks (ABNNs). Our work
is based on a systematic analysis of 555 real-world faults collected from 96
projects across ten frameworks, including GitHub, Hugging Face, and Stack
Overflow. Through our analysis, we develop a novel taxonomy comprising seven
attention-specific fault categories, not captured by existing work. Our results
show that over half of the ABNN faults arise from mechanisms unique to
attention architectures. We further analyze the root causes and manifestations
of these faults through various symptoms. Finally, by analyzing symptom-root
cause associations, we identify four evidence-based diagnostic heuristics that
explain 33.0% of attention-specific faults, offering the first systematic
diagnostic guidance for attention-based models.

</details>


### [119] [Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic](https://arxiv.org/abs/2508.05005)
*Gang Xu,Airong Wang,Yushan Pan*

Main category: cs.SE

TL;DR: The paper explores how Large Language Models (LLMs) can be integrated into Object-Oriented Programming (OOP) workflows to improve learning and coding outcomes.


<details>
  <summary>Details</summary>
Motivation: The intersection of LLMs with OOP methods and workflows is underexamined, and this paper aims to explore how LLMs can enhance OOP learning and code-writing processes.

Method: The authors identify critical moments in coding workflows where LLM integration could provide substantial benefits and propose strategies to improve logical reasoning and code-writing using LLMs.

Result: Critical coding workflows were mapped and opportunities for LLM-supported interventions were suggested to improve OOP learning and code writing.

Conclusion: Integrating LLMs into OOP workflows can significantly aid programmers by boosting reasoning and writing in coding tasks, thereby enriching the programming experience.

Abstract: We find ourselves in the midst of an explosion in artificial intelligence
research, particularly with large language models (LLMs). These models have
diverse applications spanning finance, commonsense knowledge graphs, medicine,
and visual analysis. In the world of Object-Oriented Programming(OOP), a robust
body of knowledge and methods has been developed for managing complex tasks
through object-oriented thinking. However, the intersection of LLMs with OOP
remains an underexplored territory. Empirically, we currently possess limited
understanding of how LLMs can enhance the effectiveness of OOP learning and
code writing, as well as how we can evaluate such AI-powered tools. Our work
aims to address this gap by presenting a vision from the perspectives of key
stakeholders involved in an OOP task: programmers, mariners, and experienced
programmers. We identify critical junctures within typical coding workflows
where the integration of LLMs can offer significant benefits. Furthermore, we
propose ways to augment existing logical reasoning and code writing, ultimately
enhancing the programming experience.

</details>


### [120] [An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack](https://arxiv.org/abs/2508.05034)
*Arabat,Ali,Sayagh,Mohammed,Hassine,Jameleddine*

Main category: cs.SE

TL;DR: The paper addresses dependency management in complex software systems, focusing on OpenStack. It proposes ML models for proactively identifying dependencies, achieving promising accuracy metrics.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of software systems necessitates better management of dependencies among changes to avoid operational inefficiencies and deployment issues.

Method: The study examines dependency detection in OpenStack over a 10-year period and introduces two machine learning models: one to predict the likelihood of dependencies and another to identify specific pairs of dependent changes.

Result: The ML models show strong performance, with AUC scores of 79.33% and 91.89% and Brier scores of 0.11 and 0.014, respectively. Dependency detection during code reviews, however, remains delayed.

Conclusion: Proactive dependency management using ML models offers a significant improvement compared to current manual detection methods, although further refinement in precision is required.

Abstract: As software systems grow in complexity, accurately identifying and managing
dependencies among changes becomes increasingly critical. For instance, a
change that leverages a function must depend on the change that introduces it.
Establishing such dependencies allows CI/CD pipelines to build and orchestrate
changes effectively, preventing build failures and incomplete feature
deployments. In modern software systems, dependencies often span multiple
components across teams, creating challenges for development and deployment.
They serve various purposes, from enabling new features to managing
configurations, and can even involve traditionally independent changes like
documentation updates. To address these challenges, we conducted a preliminary
study on dependency management in OpenStack, a large-scale software system. Our
study revealed that a substantial portion of software changes in OpenStack over
the past 10 years are interdependent. Surprisingly, 51.08% of these
dependencies are identified during the code review phase-after a median delay
of 5.06 hours-rather than at the time of change creation. Developers often
spend a median of 57.12 hours identifying dependencies, searching among a
median of 463 other changes. To help developers proactively identify
dependencies, we propose a semi-automated approach that leverages two ML
models. The first model predicts the likelihood of dependencies among changes,
while the second identifies the exact pairs of dependent changes. Our proposed
models demonstrate strong performance, achieving average AUC scores of 79.33%
and 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the
second model has a good top-k recall across all types of pairs, while the top-k
precision has room for improvement.

</details>


### [121] [LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps](https://arxiv.org/abs/2508.05085)
*Junayed Mahmud,James Chen,Terry Achille,Camilo Alvarez-Velez,Darren Dean Bansil,Patrick Ijieh,Samar Karanch,Nadeeshan De Silva,Oscar Chaparro,Andrian Marcus,Kevin Moran*

Main category: cs.SE

TL;DR: LadyBug is an open-source GitHub bot that identifies and ranks files likely containing bugs in Android apps by combining text and UI interaction data, outperforming text-retrieval-only methods.


<details>
  <summary>Details</summary>
Motivation: Developers often face challenges in accurately localizing bugs in Android apps, leading to inefficiencies in debugging. This paper seeks to improve bug localization accuracy.

Method: LadyBug leverages a combination of textual bug descriptions and UI reproduction traces uploaded through GitHub issue trackers to determine the most probable files containing bugs. It uses automation tools and a benchmark dataset, RedWing, for empirical evaluation.

Result: Experiments with LadyBug on the RedWing benchmark demonstrated enhanced bug localization accuracy, surpassing text-retrieval-based approaches by incorporating UI interaction data.

Conclusion: LadyBug effectively combines UI and text retrieval data for improved bug localization in Android apps and is publicly accessible as an open-source tool.

Abstract: This paper introduces LadyBug, a GitHub bot that automatically localizes bugs
for Android apps by combining UI interaction information with text retrieval.
LadyBug connects to an Android app's GitHub repository, and is triggered when a
bug is reported in the corresponding issue tracker. Developers can then record
a reproduction trace for the bug on a device or emulator and upload the trace
to LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both
the text from the original bug description, and UI information from the
reproduction trace to accurately retrieve a ranked list of files from the
project that most likely contain the reported bug.
  We empirically evaluated LadyBug using an automated testing pipeline and
benchmark called RedWing that contains 80 fully-localized and reproducible bug
reports from 39 Android apps. Our results illustrate that LadyBug outperforms
text-retrieval-based baselines and that the utilization of UI information leads
to a substantial increase in localization accuracy. LadyBug is an open-source
tool, available at https://github.com/LadyBugML/ladybug.
  A video showing the capabilities of Ladybug can be viewed here:
https://youtu.be/hI3tzbRK0Cw

</details>


### [122] [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
*Lishui Fan,Yu Zhang,Mouxiang Chen,Zhongxin Liu*

Main category: cs.SE

TL;DR: The paper introduces methods to enhance reinforcement learning for code generation in large language models by focusing on reasoning process quality through new benchmarks, reward modeling techniques, and RL methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL paradigms for code generation neglect the intermediate reasoning process, which is crucial for high-quality outcomes and lead to reward hacking.

Method: The authors developed the LCB-RB benchmark for reasoning evaluation, an OD-based reward model training method, and the Posterior-GRPO algorithm that applies reasoning-based rewards selectively on successful outcomes.

Result: A reward model trained with the OD-based method achieved state-of-the-art performance on reasoning benchmarks. Models utilizing P-GRPO outperformed baselines by 4.5% and performed comparably to GPT-4-Turbo.

Conclusion: Focusing on reasoning process quality significantly improves code generation RL models’ performance and generalizability across diverse tasks, including mathematical reasoning.

Abstract: Reinforcement learning (RL) has significantly advanced code generation for
large language models (LLMs). However, current paradigms rely on outcome-based
rewards from test cases, neglecting the quality of the intermediate reasoning
process. While supervising the reasoning process directly is a promising
direction, it is highly susceptible to reward hacking, where the policy model
learns to exploit the reasoning reward signal without improving final outcomes.
To address this, we introduce a unified framework that can effectively
incorporate the quality of the reasoning process during RL. First, to enable
reasoning evaluation, we develop LCB-RB, a benchmark comprising preference
pairs of superior and inferior reasoning processes. Second, to accurately score
reasoning quality, we introduce an Optimized-Degraded based (OD-based) method
for reward model training. This method generates high-quality preference pairs
by systematically optimizing and degrading initial reasoning paths along
curated dimensions of reasoning quality, such as factual accuracy, logical
rigor, and coherence. A 7B parameter reward model with this method achieves
state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other
benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method
that conditions process-based rewards on task success. By selectively applying
rewards to the reasoning processes of only successful outcomes, P-GRPO
effectively mitigates reward hacking and aligns the model's internal reasoning
with final code correctness. A 7B parameter model with P-GRPO achieves superior
performance across diverse code generation tasks, outperforming outcome-only
baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further
demonstrate the generalizability of our approach by extending it to
mathematical tasks. Our models, dataset, and code are publicly available.

</details>


### [123] [AI-assisted JSON Schema Creation and Mapping](https://arxiv.org/abs/2508.05192)
*Felix Neubauer,Jürgen Pleiss,Benjamin Uekermann*

Main category: cs.SE

TL;DR: The paper introduces a hybrid approach combining large language models (LLMs) and deterministic techniques for JSON Schema creation and data integration using natural language inputs, integrated into the MetaConfigurator tool.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized models across various domains and reduce the barriers for non-experts in structured data modeling and data integration.

Method: The approach utilizes a combination of LLMs and deterministic techniques, integrated into the MetaConfigurator tool, for creating, modifying, and mapping JSON Schemas based on user inputs. The deterministic safeguards ensure scalability and reliability.

Result: The solution provides automated JSON Schema creation and mappings across heterogeneous data formats like JSON, CSV, XML, and YAML, demonstrated in an example from the field of chemistry.

Conclusion: This hybrid approach enhances accessibility to structured data modeling and data integration for non-experts, bridging gaps with natural language interaction and reliable execution.

Abstract: Model-Driven Engineering (MDE) places models at the core of system and data
engineering processes. In the context of research data, these models are
typically expressed as schemas that define the structure and semantics of
datasets. However, many domains still lack standardized models, and creating
them remains a significant barrier, especially for non-experts. We present a
hybrid approach that combines large language models (LLMs) with deterministic
techniques to enable JSON Schema creation, modification, and schema mapping
based on natural language inputs by the user. These capabilities are integrated
into the open-source tool MetaConfigurator, which already provides visual model
editing, validation, code generation, and form generation from models. For data
integration, we generate schema mappings from heterogeneous JSON, CSV, XML, and
YAML data using LLMs, while ensuring scalability and reliability through
deterministic execution of generated mapping rules. The applicability of our
work is demonstrated in an application example in the field of chemistry. By
combining natural language interaction with deterministic safeguards, this work
significantly lowers the barrier to structured data modeling and data
integration for non-experts.

</details>


### [124] [STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning](https://arxiv.org/abs/2508.05193)
*Kaiwen Yan,Yuhang Chang,Zirui Guo,Yaling Mou,Jiang Ming,Jingwei Sun*

Main category: cs.SE

TL;DR: STEPWISE-CODEX-Bench (SX-Bench) is a new benchmark addressing the limitations of current code reasoning evaluations by focusing on complex multi-function scenarios and fine-grained execution reasoning.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to adequately assess large language models in complex reasoning scenarios, leading to nearly saturated scores for advanced models.

Method: SX-Bench employs tasks with multi-function collaboration and evaluates models' understanding using computation steps, incorporating dynamic reasoning beyond simple input/output assessments.

Result: Evaluation of over 20 models reveals SX-Bench's high discrimination power, exposing weaknesses in current state-of-the-art systems like OpenAI-O3 on complex reasoning.

Conclusion: SX-Bench enhances code intelligence evaluation by shifting focus to multi-function dynamic reasoning, addressing bottlenecks in existing benchmarks and improving assessment of advanced models.

Abstract: In recent years, large language models (LLMs) have made significant progress
in code intelligence, yet systematically evaluating their code understanding
and reasoning abilities remains challenging. Mainstream benchmarks such as
HumanEval and MBPP primarily assess functional correctness, while reasoning
benchmarks like CRUXEVAL are limited to single-function, low-complexity
scenarios. As a result, advanced models achieve nearly saturated scores,
limiting their discriminative power. To address this, we present
STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex
multi-function understanding and fine-grained execution reasoning. SX-Bench
features tasks involving collaboration among multiple sub-functions (e.g.,
chained calls, nested loops), shifting evaluation towards overall control and
data flow modeling. It defines "computation steps" as the minimal execution
unit and requires models to predict the total number of steps in reasoning
tasks, thereby assessing a model's in-depth understanding of dynamic execution
beyond simple I/O matching. Evaluation on over 20 mainstream models (including
14 reasoning-enhanced models) demonstrates that SX-Bench is highly
discriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent
accuracy on Hard-Reasoning tasks, much lower than its saturated scores on
previous benchmarks, thereby revealing bottlenecks in complex and fine-grained
reasoning. We also release an automated pipeline combining program synthesis,
symbolic execution, and LLM-aided validation for efficient benchmark generation
and quality assurance. SX-Bench advances code evaluation from "single-function
verification" to "multi-function dynamic reasoning," providing a key tool for
the in-depth assessment of advanced code intelligence models.

</details>


### [125] [EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0](https://arxiv.org/abs/2508.05199)
*Igor Costa,Christopher Baran*

Main category: cs.SE

TL;DR: EvoGraph is a software framework that advances self-evolving systems, achieving automation in code evolution and significant functional equivalencies across languages while reducing costs.


<details>
  <summary>Details</summary>
Motivation: To create a framework where software systems can autonomously and efficiently evolve, modernize, and manage their codebase and related artifacts.

Method: EvoGraph employs a typed directed graph to represent software artifacts, uses specialized small language models for mutation, and applies a multi-objective fitness selection mechanism.

Result: EvoGraph demonstrates the ability to fix security vulnerabilities, achieve high functional equivalence in code translations, and maintain documentation freshness with significant reductions in latency and cost.

Conclusion: This framework presents a tangible method for continuous software evolution, paving the way for highly adaptive systems within measurable constraints, termed as Software 3.0.

Abstract: We introduce **EvoGraph**, a framework that enables software systems to
evolve their own source code, build pipelines, documentation, and tickets.
EvoGraph represents every artefact in a typed directed graph, applies learned
mutation operators driven by specialized small language models (SLMs), and
selects survivors with a multi-objective fitness. On three benchmarks, EvoGraph
fixes 83% of known security vulnerabilities, translates COBOL to Java with 93%
functional equivalence (test verified), and maintains documentation freshness
within two minutes. Experiments show a 40% latency reduction and a sevenfold
drop in feature lead time compared with strong baselines. We extend our
approach to **evoGraph**, leveraging language-specific SLMs for modernizing
.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%
semantic equivalence across languages while reducing computational costs by 90%
compared to large language models. EvoGraph's design responds to empirical
failure modes in legacy modernization, such as implicit contracts, performance
preservation, and integration evolution. Our results suggest a practical path
toward Software 3.0, where systems adapt continuously yet remain under
measurable control.

</details>


### [126] [A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes](https://arxiv.org/abs/2508.05301)
*Victoria Torres Bosch,Ronny Seiger,Manuela Albert Albiol,Antoni Mestre Gascon,Pedro Jose Valderas Aranda*

Main category: cs.SE

TL;DR: The paper proposes a conceptual model and methodology to enhance Business Processes (BPs) sustainability using Internet of Things (IoT), extending sustainability considerations beyond environmental dimensions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address sustainability in Business Processes comprehensively by integrating IoT capabilities, beyond the usual focus on environmental concerns in Business Process Management (BPM) research.

Method: The authors developed a conceptual model linking BPM and IoT through key sustainability concepts, coupled with a methodology to systematically analyze and redesign BPs for IoT-enhanced sustainability.

Result: The model and methodology were applied to examples in tourism and healthcare, demonstrating their practicality and effectiveness in identifying and implementing sustainability-aware BPs.

Conclusion: The paper concludes that IoT technology can significantly contribute to the multidimensional sustainability of Business Processes, offering structured tools for systematic improvement beyond environmental factors.

Abstract: The real-time data collection and automation capabilities offered by the
Internet of Things (IoT) are revolutionizing and transforming Business
Processes (BPs) into IoT-enhanced BPs, showing high potential for improving
sustainability. Although already studied in Business Process Management (BPM),
sustainability research has primarily focused on environmental concerns.
However, achieving a holistic and lasting impact requires a systematic approach
to address sustainability beyond the environmental dimension. This work
proposes a conceptual model and a structured methodology with the goal of
analyzing the potential of IoT to measure and improve the sustainability of
BPs. The conceptual model formally represents key sustainability concepts,
linking BPM and IoT by highlighting how IoT devices support and contribute to
sustainability. The methodology guides the systematic analysis of existing BPs,
identifies opportunities, and implements sustainability-aware, IoT-enhanced
BPs. The approach is illustrated through a running example from the tourism
domain and a case study in healthcare.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [127] [Bridging Brains and Models: MoE-Based Functional Lesions for Simulating and Rehabilitating Aphasia](https://arxiv.org/abs/2508.04749)
*Yifan Wang,Jingyuan Sun,Jichen Zheng,Yunhao Zhang,Chunyu Ye,Jixing Li,Chengqing Zong,Shaonan Wang*

Main category: q-bio.NC

TL;DR: This paper uses a modular language model to simulate and study language disorders like aphasia by selectively disabling components. They validate these simulations against real patient data and explore the potential for rehabilitation via retraining.


<details>
  <summary>Details</summary>
Motivation: To determine if large language models, which align with human brain activity, can simulate linguistic deficits caused by brain injuries, and to assess their potential for exploring therapeutic pathways.

Method: The authors selectively disabled components in a modular Mixture-of-Experts language model to simulate subtypes of aphasia, validated the outputs against patient speech data, and tested recovery by retraining intact parts of the model.

Result: Lesioning functionally-specialized experts produced linguistic impairments mimicking Broca's and Wernicke's aphasia. Retraining intact components on conversational data partially restored language function.

Conclusion: Modular large language models can effectively simulate language deficits and recovery, offering a useful framework for studying language disorders and potential therapeutic interventions.

Abstract: The striking alignment between large language models (LLMs) and human brain
activity positions them as powerful models of healthy cognition. This parallel
raises a fundamental question: if LLMs can model the intact brain, can we
lesion them to simulate the linguistic deficits of the injured brain? In this
work, we introduce a methodology to model aphasia - a complex language disorder
caused by neural injury - by selectively disabling components in a modular
Mixture-of-Experts (MoE) language model. We simulate distinct aphasia subtypes,
validate their linguistic outputs against real patient speech, and then
investigate functional recovery by retraining the model's remaining healthy
experts. Our results demonstrate that lesioning functionally-specialized
experts for syntax or semantics induces distinct impairments that closely
resemble Broca's and Wernicke's aphasia, respectively. Crucially, we show that
freezing the damaged experts and retraining the intact ones on conversational
data restores significant linguistic function, demonstrating a computational
analogue for rehabilitation. These findings establish modular LLMs as a
powerful and clinically-relevant potential framework for modeling the
mechanisms of language disorders and for computationally exploring novel
pathways for therapy.

</details>


### [128] [Delay-constrained re-entry governs large-scale brain seizures and other network pathologies](https://arxiv.org/abs/2508.04824)
*Paul Triebkorn,Huifang E. Wang,Marmaduke Woodman,Maxime Guye,Fabrice Bartolomei,Viktor Jirsa*

Main category: q-bio.NC

TL;DR: The paper investigates how re-entry of excitation loops contributes to seizures using a virtual brain model built from patient data, and identifies strategies for disrupting these loops.


<details>
  <summary>Details</summary>
Motivation: To understand how re-entry of traveling excitation loops arises in patient brain networks during seizures, and their susceptibility to disruption.

Method: The researchers created a millimetre-scale virtual brain based on diffusion MRI data, embedded neural field models, conducted parameter sweeps, and tested interventions like stimuli and virtual lesions.

Result: The study identified narrow delay-coupling windows predicting oscillation frequencies and seizure durations, and found phase-dependent interventions that effectively aborted re-entry loops in simulations.

Conclusion: Delay-constrained re-entry is a fundamental mechanism in large-scale brain synchrony and could guide precision interventions for drug-resistant epilepsy.

Abstract: Re-entry of travelling excitation loops is a long-suspected driver of human
seizures, yet how such loops arise in patient brain networks -- and how
susceptible they are to targeted disruption -- remains unclear. We reconstruct
a millimetre-scale virtual brain from diffusion MRI of a drug-resistant
epilepsy patient, embed excitable Epileptor neural fields, and show that
realistic cortico-cortical delays are sufficient to generate self-sustaining
re-entry. Systematic parameter sweeps reveal a narrow delay-coupling window
that predicts oscillation frequency and seizure duration across 184 recorded
seizures. Precisely timed biphasic stimuli or sub-millimetre virtual lesions
abort re-entry in silico, yielding phase-dependent termination rules validated
in intracranial recordings. Our framework exposes delay-constrained re-entry as
a generic dynamical mechanism for large-scale brain synchrony and provides a
patient-specific testbed for precision neuromodulation and minimally invasive
disconnection.

</details>


### [129] [Harmonic fractal transformation for modeling complex neuronal effects: from bursting and noise shaping to waveform sensitivity and noise-induced subthreshold spiking](https://arxiv.org/abs/2508.05341)
*Mariia Sorokina*

Main category: q-bio.NC

TL;DR: The paper introduces a fractal frequency mapping method that replicates complex neuronal effects, differing from conventional filters.


<details>
  <summary>Details</summary>
Motivation: The motivation of the study is to better understand complex neuronal functionalities using a novel spectral approach that excites new frequency components, instead of merely suppressing or amplifying them.

Method: The authors propose a fractal recomposition of input spectra, forming spikes at resonant frequencies, which provides a nonlinear transformation of the frequency domain.

Result: The method achieves high sensitivity detection and robustness to noise while amplifying signals induced by noise.

Conclusion: The study suggests neuronal functionalities can be viewed as the linear summation of spectrum over nonlinearly transformed frequency domains, demonstrating the impact of fractal frequency mapping.

Abstract: We propose the first fractal frequency mapping, which in a simple form
enables to replicate complex neuronal effects. Unlike the conventional filters,
which suppress or amplify the input spectral components according to the filter
weights, the transformation excites novel components by a fractal recomposition
of the input spectra resulting in a formation of spikes at resonant frequencies
that are optimal for sampling. This enables high sensitivity detection,
robustness to noise and noise-induced signal amplification. The proposed model
illustrates that a neuronal functionality can be viewed as a linear summation
of spectrum over nonlinearly transformed frequency domain.

</details>


### [130] [Covariance spectrum in nonlinear recurrent neural networks](https://arxiv.org/abs/2508.05288)
*Xuanyu Shen,Yu Hu*

Main category: q-bio.NC

TL;DR: The paper explores covariance spectrum as a tool to understand nonlinear neural population dynamics across various dynamical regimes, extending previous theoretical insights achieved with minimal linear neuronal models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theory regarding covariance eigenvalue distributions in nonlinear neural networks and chaotic regimes, since previous approaches relied on minimal linear models.

Method: The authors extend the analysis of covariance spectra, incorporating nonlinear neurons and broader dynamical regimes like chaos, while using an effective recurrent connection strength parameter.

Result: The covariance spectrum can be precisely described using equations similar to linear theory but adjusted with an effective connection strength, offering unified insights into dynamical changes and maintaining a critical value without fine-tuning in chaotic regimes.

Conclusion: The findings reinforce the applicability of covariance spectrum analysis to biological circuits, advancing theoretical understanding of nonlinear neural population dynamics.

Abstract: Advances in simultaneous recordings of large numbers of neurons have driven
significant interest in the structure of neural population activity such as
dimension. A key question is how these dynamic features arise mechanistically
and their relationship to circuit connectivity. It was previously proposed to
use the covariance eigenvalue distribution, or spectrum, which can be
analytically derived in random recurrent networks, as a robust measure to
describe the shape of neural population activity beyond the dimension (Hu and
Sompolinsky 2022). Applications of the theoretical spectrum have broadly found
accurate matches to experimental data across brain areas providing mechanistic
insights into the observed low dimensional population dynamics (Morales et al.
2023). However, the empirical success highlights a gap in theory, as the neural
network model used to derive the spectrum was minimal with linear neurons. In
this work, we aim to close this gap by studying the covariance spectrum in
networks with nonlinear neurons and under broader dynamical regimes including
chaos. Surprisingly, we found that the spectrum can be precisely understood by
equations analogous to the linear theory substituted with an effective
recurrent connection strength parameter, that reflects both the connection
weights and the nonlinearity of neurons. Across dynamical regimes, this
effective connection strength provides a unified interpretation for the
spectrum and dimension changes, and stays near the critical value in the
chaotic regime without fine-tuning. These results further our understanding of
nonlinear neural population dynamics and provide additional theoretical support
for applying the covariance spectrum analysis in biological circuits.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [131] [Differentially Private Model-X Knockoffs via Johnson-Lindenstrauss Transform](https://arxiv.org/abs/2508.04800)
*Yuxuan Tao,Adel Javanmard*

Main category: stat.ML

TL;DR: The paper introduces a novel framework for variable selection that combines False Discovery Rate (FDR) control with differential privacy. It utilizes Gaussian Johnson-Lindenstrauss Transformation (JLT) for data privatization, bypassing limitations of traditional privacy mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of rigorous False Discovery Rate (FDR) control in high-dimensional settings while adhering to differential privacy constraints.

Method: The Gaussian Johnson-Lindenstrauss Transformation (JLT) is used to reduce dimensions and privatize data while preserving covariate relationships. A debiasing technique is applied for the private knockoff procedure.

Result: The proposed framework provides theoretical guarantees for controlling FDR and maintaining statistical power. Conditions under which the method achieves high power are derived.

Conclusion: The work bridges knockoff-based FDR control and differential privacy, showing the effectiveness of structural privacy preservation techniques over traditional noise addition mechanisms.

Abstract: We introduce a novel privatization framework for high-dimensional controlled
variable selection. Our framework enables rigorous False Discovery Rate (FDR)
control under differential privacy constraints. While the Model-X knockoff
procedure provides FDR guarantees by constructing provably exchangeable
``negative control" features, existing privacy mechanisms like Laplace or
Gaussian noise injection disrupt its core exchangeability conditions. Our key
innovation lies in privatizing the data knockoff matrix through the Gaussian
Johnson-Lindenstrauss Transformation (JLT), a dimension reduction technique
that simultaneously preserves covariate relationships through approximate
isometry for $(\epsilon,\delta)$-differential privacy.
  We theoretically characterize both FDR and the power of the proposed private
variable selection procedure, in an asymptotic regime. Our theoretical analysis
characterizes the role of different factors, such as the JLT's dimension
reduction ratio, signal-to-noise ratio, differential privacy parameters, sample
size and feature dimension, in shaping the privacy-power trade-off. Our
analysis is based on a novel `debiasing technique' for high-dimensional private
knockoff procedure. We further establish sufficient conditions under which the
power of the proposed procedure converges to one. This work bridges two
critical paradigms -- knockoff-based FDR control and private data release --
enabling reliable variable selection in sensitive domains. Our analysis
demonstrates that structural privacy preservation through random projections
outperforms the classical noise addition mechanism, maintaining statistical
power even under strict privacy budgets.

</details>


### [132] [The Cosine Schedule is Fisher-Rao-Optimal for Masked Discrete Diffusion Models](https://arxiv.org/abs/2508.04884)
*Leo Zhang*

Main category: stat.ML

TL;DR: The research gives theoretical backing to the use of cosine scheduling for discrete diffusion models using information geometry principles.


<details>
  <summary>Details</summary>
Motivation: To optimize discretization schedules for sampling from masked discrete diffusion models through a geometric perspective.

Method: Investigating discretization schedules using the Fisher-Rao geometry of the probability path.

Result: Demonstrated that the cosine schedule aligns with the optimal schedule derived from the Fisher-Rao geometry.

Conclusion: Validates the cosine schedule as optimal for masked discrete diffusion models under Fisher-Rao geometry, providing theoretical support for its widespread use.

Abstract: In this work, we study the problem of choosing the discretisation schedule
for sampling from masked discrete diffusion models in terms of the information
geometry of the induced probability path. Specifically, we show that the
optimal schedule under the Fisher-Rao geometry recovers the popularly-used
cosine schedule.

</details>


### [133] [High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference](https://arxiv.org/abs/2508.05212)
*Ziliang Shen,Caixing Wang,Shaoli Wang,Yibo Yan*

Main category: stat.ML

TL;DR: The paper introduces a differentially private quantile regression method for high-dimensional distributed data, addressing privacy concerns while enabling statistical analysis.


<details>
  <summary>Details</summary>
Motivation: There is a need to ensure privacy in machine learning models, especially when analyzing heterogeneous datasets containing sensitive personal information.

Method: The paper reformulates the quantile regression problem using a Newton-type transformation and develops iterative estimation and inference techniques. It also includes communication-efficient bootstrapping for distributed settings.

Result: The proposed methods ensure near-optimal statistical accuracy, formal privacy guarantees, and effective hypothesis testing in high-dimensional data settings.

Conclusion: The techniques presented are robust, efficient, and practically applicable, balancing privacy and accuracy in distributed data analysis.

Abstract: With the development of big data and machine learning, privacy concerns have
become increasingly critical, especially when handling heterogeneous datasets
containing sensitive personal information. Differential privacy provides a
rigorous framework for safeguarding individual privacy while enabling
meaningful statistical analysis. In this paper, we propose a differentially
private quantile regression method for high-dimensional data in a distributed
setting. Quantile regression is a powerful and robust tool for modeling the
relationships between the covariates and responses in the presence of outliers
or heavy-tailed distributions. To address the computational challenges due to
the non-smoothness of the quantile loss function, we introduce a Newton-type
transformation that reformulates the quantile regression task into an ordinary
least squares problem. Building on this, we develop a differentially private
estimation algorithm with iterative updates, ensuring both near-optimal
statistical accuracy and formal privacy guarantees. For inference, we further
propose a differentially private debiased estimator, which enables valid
confidence interval construction and hypothesis testing. Additionally, we
propose a communication-efficient and differentially private bootstrap for
simultaneous hypothesis testing in high-dimensional quantile regression,
suitable for distributed settings with both small and abundant local data.
Extensive simulations demonstrate the robustness and effectiveness of our
methods in practical scenarios.

</details>


### [134] [L1-Regularized Functional Support Vector Machine](https://arxiv.org/abs/2508.05567)
*Bingfan Liu,Peijun Sang*

Main category: stat.ML

TL;DR: The paper proposes an $L_1$-regularized functional support vector machine for binary classification with multivariate functional covariates.


<details>
  <summary>Details</summary>
Motivation: There is a gap in binary classification research focused on functional data when dealing with multiple covariates.

Method: An $L_1$-regularized functional support vector machine is developed alongside an algorithm for classification and covariate selection.

Result: Simulations and real-world application validate its effectiveness in prediction and feature selection.

Conclusion: The proposed approach improves classification accuracy while identifying relevant covariates.

Abstract: In functional data analysis, binary classification with one functional
covariate has been extensively studied. We aim to fill in the gap of
considering multivariate functional covariates in classification. In
particular, we propose an $L_1$-regularized functional support vector machine
for binary classification. An accompanying algorithm is developed to fit the
classifier. By imposing an $L_1$ penalty, the algorithm enables us to identify
relevant functional covariates of the binary response. Numerical results from
simulations and one real-world application demonstrate that the proposed
classifier enjoys good performance in both prediction and feature selection.

</details>


### [135] [High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation](https://arxiv.org/abs/2508.05570)
*Ilya Levin,Alexey Naumov,Sergey Samsonov*

Main category: stat.ML

TL;DR: The paper examines the Linear Stochastic Approximation (LSA) algorithm with Polyak-Ruppert averaging under Markovian noise, identifies bias challenges, and uses Richardson-Romberg extrapolation to mitigate leading bias terms.


<details>
  <summary>Details</summary>
Motivation: To address bias and error bounds in the LSA algorithm with PR averaging under Markovian noise and identify ways to improve it.

Method: A novel linearization technique is employed to decompose bias, followed by applying Richardson-Romberg extrapolation to cancel leading bias terms. High-order moment bounds for iterates are derived.

Result: The leading-order bias term is linear in the step size and cannot be removed by PR averaging, but it can be effectively mitigated through Richardson-Romberg extrapolation.

Conclusion: Richardson-Romberg extrapolation improves the performance of the LSA algorithm by addressing its leading bias term and aligning the error with the asymptotically optimal covariance matrix.

Abstract: In this paper, we study the bias and high-order error bounds of the Linear
Stochastic Approximation (LSA) algorithm with Polyak-Ruppert (PR) averaging
under Markovian noise. We focus on the version of the algorithm with constant
step size $\alpha$ and propose a novel decomposition of the bias via a
linearization technique. We analyze the structure of the bias and show that the
leading-order term is linear in $\alpha$ and cannot be eliminated by PR
averaging. To address this, we apply the Richardson-Romberg (RR) extrapolation
procedure, which effectively cancels the leading bias term. We derive
high-order moment bounds for the RR iterates and show that the leading error
term aligns with the asymptotically optimal covariance matrix of the vanilla
averaged LSA iterates.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [136] [Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications](https://arxiv.org/abs/2508.04889)
*Theia Henderson,David R. Karger,David D. Clark*

Main category: cs.SI

TL;DR: Graffiti is a system that enables personalized social apps to be created easily and to interoperate while retaining user data and connections.


<details>
  <summary>Details</summary>
Motivation: Current social platforms have rigid designs, making it hard to build new apps that connect with existing communities. There’s a need for systems allowing flexible and interoperable social tools.

Method: The authors propose Graffiti, which relies on a client-side API with concepts like 'total reification' for conflicting design interoperability and 'channels' to avoid context collapse. Case studies are built using a Vue.js plugin.

Result: Graffiti can support decentralized apps resembling Twitter, Messenger, and Wikipedia, showcasing its ability to create diverse and interoperable social platforms.

Conclusion: Graffiti enables a dynamic and interoperable ecosystem for personalized social applications while maintaining user connections and preventing context collapse.

Abstract: Most social applications, from Twitter to Wikipedia, have rigid
one-size-fits-all designs, but building new social applications is both
technically challenging and results in applications that are siloed away from
existing communities. We present Graffiti, a system that can be used to build a
wide variety of personalized social applications with relative ease that also
interoperate with each other. People can freely move between a plurality of
designs -- each with its own aesthetic, feature set, and moderation -- all
without losing their friends or data.
  Our concept of total reification makes it possible for seemingly
contradictory designs, including conflicting moderation rules, to interoperate.
Conversely, our concept of channels prevents interoperation from occurring by
accident, avoiding context collapse.
  Graffiti applications interact through a minimal client-side API, which we
show admits at least two decentralized implementations. Above the API, we built
a Vue.js plugin, which we use to develop applications similar to Twitter,
Messenger, and Wikipedia using only client-side code. Our case studies explore
how these and other novel applications interoperate, as well as the broader
ecosystem that Graffiti enables.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [137] [Kinetic energy in random recurrent neural networks](https://arxiv.org/abs/2508.04983)
*Li-Ru Zhang,Haiping Huang*

Main category: cond-mat.stat-mech

TL;DR: This paper investigates the kinetic energy distribution in random recurrent neural networks, identifying a critical behavior linked to synaptic gain, with a continuous shift near the critical point.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between high-dimensional neural dynamics, specifically kinetic energy distribution, and chaotic behavior to understand phase space structures.

Method: The researchers employ dynamical mean-field theory in conjunction with extensive numerical simulations to analyze kinetic energy and steady-state activity distribution.

Result: The study finds a continuous shift of average kinetic energy at a critical synaptic gain, revealing power-law dynamics near the critical point, and validates the theoretical results with finite-size simulations.

Conclusion: The work initiates a deeper understanding of kinetic energy landscapes in neural dynamics and their implications for non-equilibrium phase space structure.

Abstract: The relationship between unstable fixed points and chaotic dynamics in
high-dimensional neural dynamics remains elusive. In this work, we investigate
the kinetic energy distribution of random recurrent neural networks by
combining dynamical mean-field theory with extensive numerical simulations. We
find that the average kinetic energy shifts continuously from zero to a
positive value at a critical value of coupling variance (synaptic gain), with a
power-law behavior close to the critical point. The steady-state activity
distribution is further calculated by the theory and compared with simulations
on finite-size systems. This study provides a first step toward understanding
the landscape of kinetic energy, which may reflect the structure of phase space
for the non-equilibrium dynamics.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [138] [Periodic evaluation of defined-contribution pension fund: A dynamic risk measure approach](https://arxiv.org/abs/2508.05241)
*Wanting He,Wenyuan Li,Yunran Wei*

Main category: q-fin.RM

TL;DR: The paper proposes a dynamic evaluation framework for pension funds using reinforcement learning, integrating periodic assessments and dynamic risk management.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance pension fund evaluation by addressing interim performance, dynamic risk management, and mortality projections.

Method: They utilize a model-free reinforcement learning algorithm, dynamic risk measures, and calibrate U.S. mortality rates with the Lee-Carter model.

Result: Periodic evaluations foster risk-averse strategies, whereas improved mortality projections prompt risk-seeking behaviors.

Conclusion: Dynamic approaches to pension fund assessments provide nuanced strategies influenced by mortality dynamics and risk evaluations.

Abstract: This paper introduces an innovative framework for the periodic evaluation of
defined-contribution pension funds. The performance of the pension fund is
evaluated not only at retirement, but also within the interim periods. In
contrast to the traditional literature, we set the dynamic risk measure as the
criterion and manage the tail risk of the pension fund dynamically. To
effectively interact with the stochastic environment, a model-free
reinforcement learning algorithm is proposed to search for optimal investment
and insurance strategies. Using U.S. data, we calibrate pension members'
mortality rates and enhance mortality projections through a Lee-Carter model.
Our numerical results indicate that periodic evaluations lead to more
risk-averse strategies, while mortality improvements encourage more
risk-seeking behaviors.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [139] [Federal Reserve Communication and the COVID-19 Pandemic](https://arxiv.org/abs/2508.04830)
*Jonathan Benchimol,Sophia Kazinnik,Yossi Saadon*

Main category: econ.GN

TL;DR: This paper analyzes the Federal Reserve's evolving communication strategies during COVID-19 compared to previous crises, revealing a reactive approach and adaptation to unconventional monetary policy (UMP).


<details>
  <summary>Details</summary>
Motivation: To understand how central banks adapt communication strategies during exceptional economic circumstances like crises.

Method: The study utilizes specialized dictionaries, sentiment analysis, and topic modeling to analyze Fed communication content, sentiment, and timing during different economic crises.

Result: Findings show that Fed communication during COVID-19 prioritized financial stability and unconventional monetary policies, with reactiveness surpassing other crises like the dot-com and global financial crises.

Conclusion: Fed communication has evolved, institutionalizing unconventional monetary policy discussion as a standard post-crisis adaptation, contributing to effective crisis management.

Abstract: In this study, we examine the Federal Reserve's communication strategies
during the COVID-19 pandemic, comparing them with communication during previous
periods of economic stress. Using specialized dictionaries tailored to
COVID-19, unconventional monetary policy (UMP), and financial stability,
combined with sentiment analysis and topic modeling techniques, we identify a
distinct focus in Fed communication during the pandemic on financial stability,
market volatility, social welfare, and UMP, characterized by notable contextual
uncertainty. Through comparative analysis, we juxtapose the Fed's communication
during the COVID-19 crisis with its responses during the dot-com and global
financial crises, examining content, sentiment, and timing dimensions. Our
findings reveal that Fed communication and policy actions were more reactive to
the COVID-19 crisis than to previous crises. Additionally, declining sentiment
related to financial stability in interest rate announcements and minutes
anticipated subsequent accommodative monetary policy decisions. We further
document that communicating about UMP has become the "new normal" for the Fed's
Federal Open Market Committee meeting minutes and Chairman's speeches since the
Global Financial Crisis, reflecting an institutional adaptation in
communication strategy following periods of economic distress. These findings
contribute to our understanding of how central bank communication evolves
during crises and how communication strategies adapt to exceptional economic
circumstances.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [140] [Everything You Need to Know About CS Education: Open Results from a Survey of More Than 18,000 Participants](https://arxiv.org/abs/2508.05286)
*Katsiaryna Dzialets,Aleksandra Makeeva,Ilya Vlasov,Anna Potriasaeva,Aleksei Rostovskii,Yaroslav Golubev,Anastasiia Birillo*

Main category: cs.CY

TL;DR: A large-scale survey of 18,032 computer science learners globally, presenting a dataset covering diverse education topics like AI usage and learning formats.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for an updated comprehensive study to understand trends, formats, and challenges in computer science education across diverse learners.

Method: The authors conducted a survey of 18,032 participants from 173 countries, ensuring diverse representation to explore various learning topics and methodologies.

Result: The survey results are compiled into an open dataset, encompassing topics such as formal education, AI usage, challenges, motivation, and emerging formats.

Conclusion: This dataset provides opportunities for further research and advancement in computer science education by uncovering challenges and emerging educational trends.

Abstract: Computer science education is a dynamic field with many aspects that
influence the learner's path. While these aspects are usually studied in depth
separately, it is also important to carry out broader large-scale studies that
touch on many topics, because they allow us to put different results into each
other's perspective. Past large-scale surveys have provided valuable insights,
however, the emergence of new trends (e.g., AI), new learning formats (e.g.,
in-IDE learning), and the increasing learner diversity highlight the need for
an updated comprehensive study. To address this, we conducted a survey with
18,032 learners from 173 countries, ensuring diverse representation and
exploring a wide range of topics - formal education, learning formats, AI
usage, challenges, motivation, and more. This paper introduces the results of
this survey as an open dataset, describes our methodology and the survey
questions, and highlights, as a motivating example, three possible research
directions within this data: challenges in learning, emerging formats, and
insights into the in-IDE format. The dataset aims to support further research
and foster advancements in computer education.

</details>
